<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9094082">
Treating Coordination in Logic Grammars
Veronica Dahll
Computing Sciences Department
Simon Fraser University
Burnaby, B.C. V5A 1S6
</title>
<author confidence="0.800552">
Michael C. McCord2
</author>
<affiliation confidence="0.719712666666667">
Computer Science Department
University of Kentucky
Lexington, KY 40506
</affiliation>
<bodyText confidence="0.99300025">
Logic grammars are grammars expressible in predicate logic. Implemented in the
programming language Prolog, logic grammar systems have proved to be a good basis for
natural language processing. One of the most difficult constructions for natural language
grammars to treat is coordination (construction with conjunctions like &apos;and&apos;). This paper
describes a logic grammar formalism, modifier structure grammars (MSGs), together with an
interpreter written in Prolog, which can handle coordination (and other natural language
constructions) in a reasonable and general way. The system produces both syntactic
analyses and logical forms, and problems of scoping for coordination and quantifiers are
dealt with. The MSG formalism seems of interest in its own right (perhaps even outside
natural language processing) because the notions of syntactic structure and semantic
interpretation are more constrained than in many previous systems (made more implicit in
the formalism itself), so that less burden is put on the grammar writer.
</bodyText>
<sectionHeader confidence="0.992093" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.948228069767442">
Since the development of the Prolog programming
language (Colmerauer 1973; Roussel 1975), logic
programming (Kowalski 1974, 1979; Van Emden
1975) has been applied in many different fields. In
natural language processing, useful grammar formal-
isms have been developed and incorporated in Prolog:
metamorphosis grammars, due to Colmerauer (1978),
and extraposition grammars, defined by F. Pereira
(1981); definite clause grammars (Pereira and Warren
1980) are a special case of metamorphosis grammars.
The first sizable application of logic grammars was a
Spanish/French-consultable data base system by Dahl
(1977, 1981), which was later adapted to Portuguese
Visiting in the Computer Science Department, University of
Kentucky, during part of this research. Work partially supported by
Canadian NSERC Grant A2436 and Simon Fraser P.R. Grant
42406.
2 Current address: IBM Thomas J. Watson Research Center,
P.O. Box 218, Yorktown Heights, NY 10598.
by L. Pereira and H. Coelho and to English by F.
Pereira and D. Warren. Coelho (1979) developed a
consulting system in Portuguese for library service,
and F. Pereira and D. Warren (1980) developed a
sizable English data base query system with facilities
for query optimization. McCord (1982, 1981) pres-
ented ideas for syntactic analysis and semantic inter-
pretation in logic grammars, with application to Eng-
lish grammar; some of these ideas are used in our
work described here.
Coordination (grammatical construction with the
conjunctions &apos;and&apos;, &apos;or&apos;, &apos;but&apos;) has long been one of
the most difficult natural language phenomena to han-
dle, because it can involve such a wide range of gram-
matical constituents (or non-constituent fragments),
and ellipsis (or reduction) can occur in the items con-
joined. In most grammatical frameworks, the grammar
writer desiring to handle coordination can get by rea-
sonably well by writing enough specific rules involving
particular grammatical categories; but it appears that a
proper and general treatment must recognize coordina-
Copyright 1983 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted
provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on
the first page. To copy otherwise, or to republish, requires a fee and/or specific permission.
</bodyText>
<page confidence="0.400161">
0362-613X/83/020069-23$03.00
</page>
<note confidence="0.66884">
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 69
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
</note>
<bodyText confidence="0.997497510638298">
tion as a &amp;quot;metagrammatical&amp;quot; construction, in the sense
that metarules, general system operations, or &amp;quot;second-
pass&amp;quot; operations such as transformations, are needed
for its formulation.
Perhaps the most general and powerful metagram-
matical device for handling coordination in computa-
tional linguistics has been the SYSCONJ facility for
augmented transition networks (ATNs) (Woods 1973;
Bates 1978). The ATN interpreter with this facility
built into it can take an ATN that does not itself men-
tion conjunctions at all, and will parse reduced coordi-
nate constructions, which are of the form
A X and Y B,
for example,
John drove his car through and
A X
completely demolished a plate glass window.
where the unreduced deep structure corresponds to
AXB and A Y B.
The result of the parse is this unreduced structure.
SYSCONJ accomplishes this by treating the conjunc-
tion as an interruption which causes the parser to back
up in its history of the parse. Before backing up, the
current configuration (immediately before the inter-
ruption) is suspended for later merging. The backing
up is done to a point when the string X was being
parsed (this defines X), and with this configuration the
string Y is parsed. The parsing of Y stops when a
configuration is reached that can be merged with the
suspended configuration, whereupon B is parsed. The
choices made in this process can be deterministic or
non-deterministic, and can be guided by syntactic or
semantic heuristics.
There are some problems with SYSCONJ, however.
It suffers from inefficiency, due to the combinatorial
explosion from all the choices it makes. Because of
this inefficiency, it in fact has not been used to a great
extent in ATN parsing. Another problem is that it
does not handle embedded complex structures. Fur-
thermore, it is not clear to us that SYSCONJ offers a
good basis for handling the scoping problems that arise
for semantic interpretation when conjunctions interact
with quantifiers (and other modifiers) in the sentence.
This latter problem is discussed in detail below.
In this paper we present a system for handling co-
ordination in logic grammars. The system consists of
three things•
</bodyText>
<listItem confidence="0.997387">
(1) a new formalism for logic grammars, which we
call modifier structure grammars (MSGs),
(2) an interpreter (or parser) for MSGs that takes all
the responsibility for the syntactic aspects of co-
ordination (as with SYSCONJ), and
(3) a semantic interpretation component that prod-
</listItem>
<bodyText confidence="0.995199910714286">
uces logical forms from the output of the parser
and deals with scoping problems for coordination.
The whole system is implemented in Prolog-10
(Pereira, Pereira, and Warren 1978).
Coordination has of course received some treat-
ment in standard logic grammars by the writing of
specific grammar rules. The most extensive treatment
of this sort that we know of is in Pereira et al. (1982),
which also deals with ellipsis. However, we are aware
of no general, metagrammatical treatment of coordina-
tion in logic grammars previous to ours.
Modifier structure grammars, described in detail in
Section 2, are true logic grammars, in that they can be
translated (compiled) directly into Horn clause sys-
tems, the program format for Prolog. In fact, the
treatment of extraposition in MSGs is based on F.
Pereira&apos;s (1981) extraposition grammars (XGs), and
MSGs can be compiled into XGs (which in turn can be
compiled into Horn clause systems). A new element
in MSGs is that the formation of analysis structures of
sentences has been made largely implicit in the gram-
mar formalism. For previous logic grammar formal-
isms, the formation of analyses is entirely the responsi-
bility of the grammar writer. Compiling MSGs into
XGs consists in making this formation of analyses
explicit.
Although MSGs can be compiled into XGs, it seems
difficult to do this in a way that treats coordination
automatically (it appears to require more metalogical
facilities than are currently available in Prolog sys-
tems). Therefore, we are using an interpreter for MSGs
(written in Prolog).
For MSGs, the analysis structure associated (by the
system) with a sentence is called the modifier structure
(MS) of the sentence. This structure can be consid-
ered an annotated phrase structure tree, and in fact
the name &amp;quot;modifier structure grammar&amp;quot; is intended to
be parallel to &amp;quot;phrase structure grammar&amp;quot;. If extrapo-
sition and coordination are neglected, there is a
context-free phrase structure grammar underlying an
MSG; and the MS trees are indeed derivation trees for
this underlying grammar, but with extra information
attached to the nodes.
In an MS tree, each node contains not only syntac-
tic information but also a term called a semantic item
(supplied in the grammar), which determines the
node&apos;s contribution to the logical form of the sentence.
This contribution is for the node alone, and does not
refer to the daughters of the node, as in the approach
of Gazdar (1981). Through their semantic items, the
daughters of a node act as modifiers of the node, in a
fairly traditional sense made precise below — hence the
term &amp;quot;modifier structure&amp;quot;.
The notion of modifier structure used here and the
semantic interpretation component, which depends on
it, are much the same as in previous work by McCord
</bodyText>
<note confidence="0.562873">
70 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
</note>
<bodyText confidence="0.92701">
(1982, 1981), especially the latter paper. But new
elements are the notion of MSG (making modifier
structure implicit in the grammar), the MSG interpret-
er, with its treatment of coordination, and the specific
rules for semantic interpretation of coordination.
The MSG interpreter is described in Section 3. As
indicated above, the interpreter completely handles the
syntax of coordination. The MSG grammar itself
should not mention conjunctions at all. The interpret-
er has a general facility for treating certain words as
demons (cf. Winograd 1972), and conjunctions are
handled in this way. When a conjunction demon ap-
pears in a sentence
A X conj Y B,
a process is set off which in outline is like SYSCONJ,
in that backing up is done in the parse history in order
to parse Y parallel to X, and B is parsed by merger
with the state interrupted by the conjunction. Howev-
er, our system has the following interesting features:
(1) The MSG interpreter manipulates stacks in
such a way that embedded coordination (and coordi-
nation of more than two elements) and interactions
with extraposition are handled. (Examples are given
in the Appendix.)
(2) The interpreter produces a modifier structure
for the sentence
AX conj Y B
which remains close to the surface form, as opposed to
the unreduced structure
</bodyText>
<sectionHeader confidence="0.878191" genericHeader="method">
AXB conj AYB
</sectionHeader>
<bodyText confidence="0.984279789473684">
(but it does show all the pertinent semantic relations
through unification of variables). Not expanding to
the unreduced form is important for keeping the modi-
fier relationships straight, in particular, getting the
right quantifier scoping. Our system analyzes the sen-
tence
Each man drove a car through and
completely demolished a glass window,
producing the logical form
each(X,man(X),exists(Y,car(Y),
exists(Z,glass(Z)&amp;window(Z),
drove through(X,Y,Z)
&amp;completely(demolished(X,Z)) )))
This logical form would be difficult to recover from
the unreduced structure, because the quantified noun
phrases are repeated in the unreduced structure, and
the logical form that corresponds most naturally to the
unreduced structure is not logically equivalent to the
above logical form.
</bodyText>
<listItem confidence="0.967986842105263">
(3) In general, the use of modifier structures and
the associated semantic interpretation component per-
mits a good treatment of scoping problems involving
coordination. Examples are given below.
(4) The system seems reasonably efficient. For
example, the analysis of the example sentence under
(2) above (including syntactic analysis and semantic
interpretation) was done in 177 milliseconds. The
reader can examine analysis times for other examples
in the Appendix. One reason for the efficiency is just
that the system is formulated as a logic programming
system, and especially that it uses Prolog-10, with its
compiler. Another reason presumably lies in the de-
tails of the MSG interpreter. For example, the inter-
preter does not save the complete history of the parse,
so that the backing up necessary for coordination does
not examine as much.
(5) The code for the system seems short, and
most of it is listed in this paper.
</listItem>
<bodyText confidence="0.97450676923077">
The semantic interpretation component is described
in Section 4, but not in complete detail since it is tak-
en in the main from McCord (1982, 1981). Emphasis
is on the new aspects involving semantic interpretation
of coordinate modifiers.
Semantic interpretation of a modifier structure tree
is done in two stages. The first stage, called reshaping,
deals heuristically with the well-known scoping prob-
lem, which arises because of the discrepancies that can
exist between (surface) syntactic relations and intend-
ed semantic relations. Reshaping is a transformation
of the syntactic MS tree into another MS tree with the
(hopefully) correct modifier relations. The second
stage takes the reshaped tree and translates it into
logical form. The modifiers actually do their work of
modification in this second stage, through their seman-
tic items.
As an example of the effects of reshaping on coor-
dinate structures involving quantifiers, the sentence
Each man and each woman ate an apple
is given the logical form
each(X,man(X),exists(Y,apple(Y),ate(X,Y)))
&amp; each(X,woman(X),exists(Y,apple(Y),ate(X,Y))),
whereas the sentence
A man and a woman sat at each table
is given the form
</bodyText>
<equation confidence="0.5995465">
each(Y,table(Y), exists(X,man(X),sat_at(X,Y))
&amp; exists(X,woman(X),sat_at(X,Y))).
</equation>
<bodyText confidence="0.913005375">
Section 5 of the paper presents a short discussion
of possible improvements for the system, and Section
6 consists of concluding remarks. The Appendix to
the paper contains a listing of most of the system, a
sample MSG, and sample parses. The reader may wish
to examine the sample parses at this point.
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 71
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
</bodyText>
<subsectionHeader confidence="0.52266">
2. Modifier Structure Grammars
</subsectionHeader>
<bodyText confidence="0.99507350877193">
The most fundamental type of logic grammar is
Colmerauer&apos;s (1978) metamorphosis grammar (MG).
Grammars of this type can be viewed as generalized
type-0 phrase structure grammars in which the gram-
mar symbols (terminals and non-terminals) are terms
from predicate logic. In derivations, the rewriting of
symbol strings involves unification (Robinson 1965),
instead of simple replacement.
F. Pereira&apos;s (1981) extraposition grammars (XGs)
are essentially generalizations of MGs designed to
handle (left) extraposition. In the left-hand side of an
XG rule, grammar symbols can be connected by the
infix operator indicating a gap. When such a rule
is used in rewriting, the gaps appearing in the left-
hand side may match arbitrary strings of grammar
symbols, and then the left-hand side is replaced by the
right-hand side followed by the symbol strings
matched by the gaps (in the same order). For exam-
ple, the XG rule
a,b...c...d --&gt; e,f
is really a rule schema
a,b,X,c,Y,d --&gt; e,f,X,Y
where X and Y stand for arbitrary grammar symbol
strings. There is a constraint on the use of gaps in
rewriting called the bracketing constraint, for which we
refer to F. Pereira (1981). However, our MSG inter-
preter includes XG interpretation, so the use of gaps is
in fact completely specified below.
In XG rules, symbols on the left-hand side follow-
ing gaps represent left-extraposed elements. For ex-
ample, the extraposition of noun phrases to the front
of relative clauses (with replacement by relative pro-
nouns) can be handled by the XG rules:
relative clause --&gt; rel marker, sentence.
rel marker...trace --&gt; rel pronoun.
noun phrase --&gt; trace.
where &apos;trace&apos; marks the position out of which the noun
phrase is being moved, and is used by the second rule
above in conjunction with a relative marker to produce
(or analyze) a relative pronoun.
Pereira&apos;s implementation of XGs is a Prolog pro-
gram that compiles XGs to Horn clause systems, which
in turn can be run by Prolog for parsing sentences. In
the compiled systems, extraposition is handled by the
manipulation of a stack called the extraposition list,
which is similar to the HOLD list for ATN&apos;s (Woods
1973). Elements (like &apos;trace&apos; above) on the left-hand
sides of XG rules following the initial symbol are in
effect put on the extraposition list during parsing, and
can be taken off when they are required later by the
right-hand side of another rule. Our MSG interpreter
uses a reformulation of this same method.
Since the grammar symbols in XGs (and MGs) can
be arbitrary terms from predicate logic, they can con-
tain arguments. These arguments can be used to hold
useful information such as selectional restrictions and
analysis structures. For example, in the rule
</bodyText>
<equation confidence="0.9820115">
sentence(s(Subj,Pred)) --&gt;
noun_phrase(Subj),verb_phrase(Pred)
</equation>
<bodyText confidence="0.991388302325581">
each non-terminal is augmented with an argument
representing a syntactic structure. (Here, following
Prolog-10 syntax, the capitalized items are variables.)
Manipulating such arguments is the only way of get-
ting analysis structures in XGs. As indicated in the
Introduction, a new ingredient in MSGs over XGs is to
automate this process, or to make it implicit in the
grammar.
MSG rules are of two forms. The basic form is
A:Sem --&gt; B.
where A--&gt;B is an XG rule and Sem is a term called a
semantic item, which plays a role in the semantic inter-
pretation of a phrase analyzed by application of the
rule. The semantic item is (as in McCord 1981) of
the form
Operator—LogicalForm
where, roughly, LogicalForm is the part of the logical
form of the sentence contributed by the rule, and Op-
erator determines the way this partial structure com-
bines with others. Details on semantic items are post-
poned to Section 4 (on semantic interpretation). Ac-
tually, the current section and Section 3 deal mainly
with syntactic constructions which are independent of
the form of semantic items.
The second type of MSG rule looks exactly like an
XG rule (no Sem is exhibited), but the system takes
care of inserting a special &amp;quot;trivial&amp;quot; Sem, 8-true. (Here
the &apos;8&apos; is the operator for left-conjoining, described in
Section 4.) Most MSG rules for higher (non-lexical)
types of phrases are of this type, but not all of them
are.
A simple example of an MSG is shown in Figure 1.
Following the notational conventions of XGs (as well
as the simpler definite clause grammars built into
Prolog-10), we indicate terminal symbols by enclosing
them in brackets [ ]. Rules can contain tests on their
right-hand sides, enclosed in braces 11, which are Pro-
log goals. In this example, the tests are calls to the
lexicon, shown after the grammar rules, which consists
of assertions (non-conditional Horn clauses).
This grammar, together with the semantic interpre-
tation component, will handle sentences like the fol-
lowing, producing the indicated logical forms:
</bodyText>
<listItem confidence="0.675450636363636">
72 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
sent --&gt; nounph(X),verbph(X).
nounph(X) --&gt; det(X),noun(X).
nounph(X) --&gt; proper_noun(X).
verbph(X) --&gt; verb(X,Y),nounph(Y).
det(X):Sem --&gt; [D],{d(D,X,Sem)].
noun(X): P-Pred --&gt; [N],{n(N,X,Pred) }.
proper_noun(N) --&gt; [N],{npr(N)}.
verb (X,Y): P-Pred --&gt; [V], fv(V,X,Y,Pred)1.
/* Lexical entries */
</listItem>
<bodyText confidence="0.9549146">
n(man,X,man(X)). n(woman,X,woman(X)).
npr(john). npr(mary).
v(saw,X,Y,saw(X,Y)). v(heard,X,Y,heard(X,Y)).
d(each,X,P/Q-each(X,Q,P)).
d(a,X,P/Q-exists(X,Q,P)).
</bodyText>
<figureCaption confidence="0.999205">
Figure 1. A simple MSG with lexicon.
</figureCaption>
<bodyText confidence="0.984022205882353">
John saw Mary.
saw(john,mary).
John heard each woman.
each(Y,woman(Y),heard (john,Y)).
Each man saw a woman.
each(X,man(X),exists(Y,woman(Y),saw(X,Y))).
A larger example MSG is listed in the Appendix.
This grammar includes rules dealing with extraposition,
and the lexicon contains rules used by the MSG inter-
preter in handling coordination.
Now let us look at the formation of syntactic struc-
tures by the MSG system. As stated in the Introduc-
tion, syntactic structures are trees called modifier
structure (MS) trees.
Suppose that a phrase is analyzed by application of
an MSG rule
A:Sem --&gt; B.
and further rule applications in an MSG. (The Sem
may be explicit or supplied by the system for the sec-
ond type of rule.) Then the modifier structure of the
phrase is a term of the form
syn(NT,Sem,Mods)
where NT is the leading symbol (a non-terminal) in A
and where Mods is the list of modifier structures of
the subphrases analyzed with the right-hand side B of
the rule. The &apos;syn&apos; structure is considered a tree node,
labeled with the two items NT and Sem, and having
daughter list Mods.
As an example, the MS tree for the sentence &amp;quot;Each
man saw a woman&amp;quot; produced by the grammar in Fig-
ure 1 is shown in Figure 2. This tree is printed by
displaying the first two fields of a &apos;syn&apos; on one line
and then recursively displaying the daughters, indented
a fixed amount.
</bodyText>
<equation confidence="0.995543777777778">
sent 8-true
nounph(X) 8-true
det(X) P/Q-each(X,Q,P)
noun(X) 8-man(X)
verbph(X) 8-true
verb(X,Y) 8-saw(X,Y)
nounph(Y) 8-true
det(Y) R/S-exists(Y,S,R)
noun(Y) 8-woman(Y)
</equation>
<figureCaption confidence="0.993655">
Figure 2. MS tree for &amp;quot;Each man saw a woman.&amp;quot;.
</figureCaption>
<bodyText confidence="0.995001666666667">
Let us now indicate briefly how MSGs can be com-
piled into XGs so that these MS trees are produced as
analyses. This method of compiling does not handle
coordination metagrammatically (as does the interpret-
er), but it does seem to be of general interest for
MSGs.
In the compiled XG version of an MSG, each non-
terminal is given two additional arguments, added, say,
at the end. Each argument holds a list of modifiers.
If the original non-terminal is nt(X1,...,Xn), the new
non-terminal will look like
nt (X 1 ,...,Xn,Mods 1 ,Mods2).
When this non-terminal is expanded by a non-trivial
rule, then Modsl will differ from Mods2 by having
one additional modifier on the front, namely the modi-
fier contributed by the rule application. A rule is
trivial if its right-hand side is empty. When a trivial
rule is used to expand `ne above, Modsl will equal
Mods2.
As an example of rule translation, the first rule in
Figure 1 is translated to
</bodyText>
<equation confidence="0.905756">
sentasyn(sent,8-true,Mods1) j Mods],Mods) --&gt;
nounph(X,Mods 1,Mods2),verbph(X,Mods2,1 1) .
(Here [X I Y] denotes the list with first member X and
remainder Y.)
</equation>
<bodyText confidence="0.919274666666667">
Any non-terminal on the left-hand side of an MSG
besides the leading non-terminal is given a pair of
identical Mods arguments (because it contributes no
modifier by itself). For example, the MSG rule
rel mk(X)...trace(X) --&gt; rel_pron.
would translate to
rel mk(X,[syn(rel mk(X),8-true,Mods1) I Mods],Mods)
...trace(X,Mods-2,Mods2) --&gt; rel_pron(Mods1,[ 1).
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 73
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
For parsing a sentence with respect to the grammar
in Figure 1, one would use
</bodyText>
<equation confidence="0.758466">
sent([MST],[ 1)
</equation>
<bodyText confidence="0.999959266666667">
as start symbol (with MST unknown) and the parse
would bind MST to the modifier structure tree of the
sentence.
Pairs of list arguments manipulated in the way just
outlined are called &amp;quot;difference lists&amp;quot;, and the tech-
nique is common in logic programming. One part of
compiling MGs to Horn clauses is the addition to each
non-terminal of an argument pair for the terminal
strings being analyzed. Pereira&apos;s compilation of XGs
to Horn clauses involves one more argument pair for
extraposition lists. So the compilation of MSGs to
Horn clauses involves three argument pairs totally. In
the MSG interpreter, described in the next section,
only a single argument (not a pair) is needed for each
of these three lists.
</bodyText>
<sectionHeader confidence="0.660668" genericHeader="method">
3. The MSG Interpreter and the Syntax of
Coordination
</sectionHeader>
<bodyText confidence="0.984554024691358">
Our MSG processor actually has a bit of compiler in it,
because there is a preprocessor that translates MSG
rules into a form more convenient for the interpreter
to use.
An MSG rule
A:Sem --&gt; B
is preprocessed into a term
rule(NT,Ext,Sem,B1)
where NT is the leading non-terminal in A, Ext is the
conversion of the remainder of A into an extraposition
list, and B1 is the conversion of B to list form.
The notion and representation of extraposition lists
used here are just the same as F. Pereira&apos;s (1981). A
node in such a list is of the form
x(Context,Type,Symbol,Ext)
where Context is either &apos;gap&apos; or `nogap&apos;, Type is either
&apos;terminal&apos; or `nonterminal&apos;, Symbol is a grammar sym-
bol, and Ext is the remainder of the list. We denote
the empty extraposition list by `nil&apos; (Pereira used [ ]).
The &amp;quot;left-hand side remainder&amp;quot; in a grammar rule
(the part after the leading symbol) is converted to an
extraposition list in a straightforward way, with one
node for each symbol in the remainder. The Context
says whether the symbol has a gap preceding it, and
the remaining fields of an &apos;x&apos; node have the obvious
meaning. For the rule
afb]...c --&gt; d
the extraposition list would be
x(nogap,terminal,b,x(gap,nonterminal,c,ni1)).
The right-hand side of an MSG rule is preprocessed
to a (simple) list form in the obvious way. Thus, a
right-hand side (d,e,f) is converted to the list [d,e,f],
and a right-hand side with a single element d is con-
verted to the list [d].
As a complete example, the MSG rule
a...b:P-p --&gt; [c],{(1},e
is converted to
rule (a,x(gap,nonterminal,b,nil),e-pd[c],[d} ,e] ) .
If the MSG rule exhibits no semantic item, then the
preprocessor supplies the trivial item P-true.
The &apos;rule&apos; forms of the rules in an MSG are stored
as assertions in the Prolog data base, to be used by the
interpreter. One can understand
rule (NT,Ext,Sem,B1)
as the assertion: &amp;quot;There is a rule for the non-terminal
NT with extraposition list Ext, etc.&amp;quot;
The rule preprocessor is listed at the beginning of
the Appendix.
Now let us look at the interpreter itself, which is
listed after the preprocessor in the Appendix.
The top-level procedure is
parse (String ,NonTerminal,Syn)
which takes a String of terminals and attempts to parse
it as a phrase of type NonTerminal, with the syntactic
structure Syn. We should comment that &apos;parse&apos; de-
fines a top-down parser.
This procedure calls the main working procedure
prs (BodyList,String,Mods,Par,Mer,Ext)
which parses String against a list BodyList of goals of
the type that can appear in the right-hand side (the
body) of a rule. The list of resulting syntactic struc-
tures is Mods (one modifier for each non-trivial ex-
pansion of a non-terminal in BodyList). The remain-
ing three arguments of &apos;prs&apos; are for stacks called the
parent stack, the merge stack, and the extraposition
list. These are initialized to `nil&apos; in the call of &apos;parse&apos;
to &apos;prs&apos;.
The parent stack serves two purposes. One is to
control the recursion in the normal working of the
parser. (The parser is much like an interpreter for a
programming language — in fact, for a specialized ver-
sion of Prolog itself.) The other purpose is to provide
information for the coordination demon, when it backs
up in (part of) the parse history.
A non-empty parent stack is a term of the form
parent(BodyList,Mods,Par)
where BodyList is a body list, Mods is a modifier list,
and Par is again a parent stack. A new level gets
pushed onto the parent stack by the sixth rule for &apos;prs&apos;
and the ancillary procedure &apos;prspush&apos;. This happens
74 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
when `prs&apos; is looking at a body list of the form
[NT I BL], where the initial element NT is a non-
terminal that can be expanded by a &apos;rule&apos; entry. If
that rule is trivial (if its own body is empty), then no
actual push is done, and `prs&apos; continues with the re-
maining current body list BL. Otherwise, `prspush&apos;
goes to a lower level, to parse against the body of the
expanding rule. The items [NT I BL] and Mods from
the higher level are saved on the parent stack (Mods is
a variable for the remaining modifiers to be found on
the higher level).
Note that the body list [NT I BL] saved in the first
field of the &apos;parent&apos; term is more than is needed for
managing the recursive return to the higher level.
Only the remainder, BL, is needed for this, because
NT has already been used in the parse. In fact, the
rule that pops to the higher level (the eighth rule for
`prs&apos;) does ignore NT in doing the pop. The extra
information, NT, is saved for the second purpose of
the parent stack, the backing up by the coordination
demon.
Before going into the details of coordination,
though, let us continue with the description of the
&amp;quot;normal&amp;quot; working of the parser.
In normal parsing, there is exactly one place where
a new &apos;syn&apos; node is added to the MS trees being built.
This is in the second rule for `prspush&apos;, which handles
non-trivial rule expansions. The addition of this node
is in accordance with the definition of modifier struc-
ture given in the preceding section.
The pushing rule of `prs&apos; (the sixth rule) also ma-
nipulates the extraposition stack. The extraposition
component of the expanding rule is concatenated onto
the front of the main extraposition list (being carried
in the last argument of `prs&apos;). This is analogous to a
HOLD operation in ATNs. Of course, if no extraposi-
tion is shown in the rule, the extraposition list will
remain the same.
The third and fourth rules for `prs&apos; handle terminals
in the body list. The first of these tries to remove the
terminal from the string argument, and the second
tries to remove it from the extraposition list (as in a
VIR arc for ATNs).
The seventh `prs&apos; rule tries to remove a non-
terminal from the extraposition list (again, like a VIR
arc).
The last `prs&apos; rule is the termination condition for
the parse. It just requires that all arguments be null.
Now we can discuss coordination demons. All the
rest of the interpreter rules deal with these.
The first `prs&apos; rule is the one that notices demon
words D. It calls a procedure &apos;demon&apos;, passing D as
the first argument and all the rest of the information it
has in other arguments. &apos;demon&apos; takes control of the
rest of the parse. In the listed interpreter there is only
one &apos;demon&apos; rule, one that tests whether D is a con-
junction. It does this with the goal
conjunction(D,Cat,Sem),
which gives the syntactic category Cat for the con-
junction D, and the semantic item Sem for a new mod-
ifier node to be constructed for the right conjunct.
The lexicon contains &apos;conjunction&apos; entries as asser-
tions.
For understanding what the conjunction demon
does, it is best to look at an example, as it would be
parsed for the grammar in the Appendix. We will use
the specific notation (for variables, etc.) given in the
demon rule, and the reader should refer to that rule in
the Appendix. It should be borne in mind that Prolog
is non-deterministic; we will only state what happens
on the successful path through the choices made.
The example is
John saw and Mary heard the train.
When the demon for &apos;and&apos; is called, the current body
list is
BL= [comps( [obj-Y])].
The non-terminal comps(Comps) looks for a list
Comps of complements, and in this case there is to be
one complement, an object noun phrase. The MS tree
constructed so far is
</bodyText>
<equation confidence="0.9867965">
sent 1&apos;-true
nounph(X,def) @P-def(X,X=john,P)
verbph(X) e-true
verb(X,[obj-Y]) e-saw(X,Y)
I Mods
Mods2
</equation>
<bodyText confidence="0.986206852941177">
Here the entry I Mods in the last daughter position for
the verb phrase indicates further modifiers on that
level to be put in the unbound variable Mods. (This is
explicitly the same variable &apos;Mods&apos; used in the demon
rule.) Similarly, I Mods2 represents the remaining
modifiers for &apos;sent&apos; node. The variable Mods2 does
not appear in the &apos;demon&apos; rule, but will be referred to
below.
The parent stack Par available to the demon has
two levels, and the two body lists are
[verbph(X)],
[sent].
(Recall that we are describing the state of affairs in
the successful path through the search space.) The
recursive procedure &apos;backup&apos; is called, which can look
any number of levels through the parent stack. It goes
to the second level, where the body list is [sent].
(Choosing the first level with [verbph(X)] would be
appropriate for the sentence &amp;quot;John saw and barely
heard the train&amp;quot;) In passing up a level, &apos;backup&apos; re-
quires that the body list skipped over must be
&apos;satisfied&apos;, which means that any pending goals in the
body list (members of its tail) are satisfiable by trivial
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 75
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
rules. When &apos;backup&apos; does pass up a level, the modifi-
er list for that level is closed off. Thus Mods in the
tree displayed above will be bound to the empty list.
(There are no more modifiers for that &apos;verbph&apos; node.)
As a single remaining daughter for the level backed
up to, a new `syn&apos; node for the right conjunct is at-
tached by the demon. This means binding the variable
Mods2 in the above tree to the list consisting of this
node. Now our tree looks like
</bodyText>
<equation confidence="0.986379833333333">
sent 8-true
nounph(X,def) @P-def(X,X=john,P)
verbph(X) 8-true
verb(X,[obj-Y]) 8-saw(X,Y)
conj(and) Q*R-(Q&amp;R)
I Mods°
</equation>
<bodyText confidence="0.999496073170732">
The variable Mods0 is to contain the list of modifiers
for the conjunction node. This list will turn out to
have a single element, a new &apos;sent&apos; node for the re-
mainder of the sentence, &amp;quot;Mary heard the train&amp;quot;.
Backing up to the [sent] level makes the non-
terminal NT=sent available to the demon, and the
parent stack Par 1 at the [sent] level. The demon then
continues the parse by calling &apos;prs&apos; with body list
[NT]=[sent], but with information pushed onto the
merge stack. The main item stored on the merge stack
is the body list BL=[comps([obj-Y])], which was
pending at the time of interruption by the conjunction.
The items Pan, Ext, and of course the old merge
stack Mer are also pushed on.
So now we continue parsing &amp;quot;Mary heard the
train&amp;quot;, but with another kind of demon lurking, the
interrupted goal BL. The second rule for &apos;prs&apos; notices
this demon. When we are parsing and come to a goal
that can be unified with BL, then we can try merging.
This happens when we are looking for the comple-
ments of &amp;quot;heard&amp;quot;. This unification includes the unifi-
cation of the object variable Y of &amp;quot;saw&amp;quot; with the ob-
ject variable of &amp;quot;heard&amp;quot;, so that &amp;quot;the train&amp;quot; will logi-
cally be the object of &amp;quot;saw&amp;quot; as well as &amp;quot;heard&amp;quot;.
The procedure &apos;cutoff&apos; called by the second &apos;prs&apos;
rule requires that no new unsatisfied goals have devel-
oped in parsing the right conjunct (aside from the goal
BL to be merged) and also closes off modifier lists in
the local parent stack Par for the right conjunct.
Then the merged parse is continued by a call to
&apos;prs&apos;, with BL as goal and with the parent stack, merge
stack, and extraposition list popped from the merge
stack. When this is completed, our MS tree is as
shown in Figure 3.
The meanings of the semantic items used in this MS
tree, and their use in producing the logical form, will
be explained in the next section; but it is worth stating
now what the resulting logical form is:
def(Y,train(Y),saw(john,Y)&amp;heard(mary,Y)).
The reader may examine the analyses produced for
other examples listed in the Appendix.
</bodyText>
<equation confidence="0.995171615384615">
sent 8-true
nounph(X,def) @P-def(X,X=john,P)
verbph(X) 8-true
verb(X,[obj-Y]) 8-saw(X,Y)
conj(and) Q*R-(Q&amp;R)
sent 8-true
nounph(Z,def) @S-def(Z,Z=mary,S)
verbph(Z) 8-true
compsgobj-YD 8-true
comp(obj-Y) 8-true
nounph(Y,def) 19-true
det(Y,def) T/U-def(Y,U,T)
noun(Y,[ ]) 8-train(Y)
</equation>
<figureCaption confidence="0.855919">
Figure 3. MS tree for
&amp;quot;John saw and Mary heard the train.&amp;quot;
</figureCaption>
<sectionHeader confidence="0.470169" genericHeader="method">
4. Semantic Interpretation and Coordination
</sectionHeader>
<bodyText confidence="0.9785168">
The overall idea of the semantic interpretation
component was given in the Introduction. The rule
system is listed completely in the Appendix. This
system is taken essentially from McCord (1981), with
some rules deleted (rules dealing with focus), and
some rules added for coordination.
For a discussion of MS tree reshaping as a means of
handling scoping of modifiers, we refer to McCord
(1982, 1981). Also, the reader may examine the ex-
amples of reshaped trees given in the Appendix.
We will, however, review the second stage of se-
mantic interpretation, because the new rules for coor-
dination are added here and because it is more central
for understanding modifier structure. In this stage, the
reshaped MS tree is translated to logical form, and the
top-level procedure for this is &apos;translate&apos;. This proce-
dure actually works only with the semantic-item com-
ponents of MS tree nodes. (Reshaping uses the first,
syntactic components.)
One semantic item can combine with (or modify) a
second semantic item to produce a third semantic item.
&apos;translate&apos; uses these combining operations in a
straightforward recursive fashion to produce the logi-
cal form of an MS tree. The ancillary procedure
(&apos;transmod&apos;) that actually does the recursion produces
complete semantic items as translations, not just logi-
cal forms. For the top-level result, the operator com-
ponent is thrown away. `transmod&apos; works simply as
follows: The daughters (modifiers) of a tree node N
are translated recursively (to semantic items) and
these items cumulatively modify the semantic item of
N, the leftmost acting as the outermost modifier, etc.
So the heart of the translation process is in the
rules that say how semantic items can combine with
76 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
</bodyText>
<subsectionHeader confidence="0.745427">
Veronica Dahl and Michael C. McCord
</subsectionHeader>
<bodyText confidence="0.9964105">
other semantic items. These are rules for the proce-
dure
</bodyText>
<equation confidence="0.623566">
trans( SemO,Seml ,Sem2)
</equation>
<bodyText confidence="0.992101392156863">
which says that Sem() combines with (modifies) Semi
to produce Sem2. In the typical case, this combination
depends only on the Operator component of Sem0;
but there are exceptional cases where it depends as
well on the operator in Semi. Furthermore, &apos;trans&apos; is
free to create a new operator for the result, Sem2,
which can affect later operations. This happens with
coordinate modifiers. We often speak of Sem()
&amp;quot;operating on&amp;quot; Semi, but &amp;quot;combining with&amp;quot; is the
more accurate term generally.
The only operators appearing in the small sample
grammar in the Appendix are of the form 8, @P, P/Q,
and P*Q. Here P and Q are variables standing for
logical forms. The listing for &apos;trans&apos; in the Appendix
includes only rules for these operators and their auxili-
aries, although larger grammars involve other opera-
tors. We will elucidate the effects of these four opera-
tors with examples. The last one, P*Q, is used for
coordination.
The operator &apos;8&apos; is for left-conjoining. When
8-man(X) operates on 8-see(X,Y), the result is
8-man(X)&amp;see(X,Y).
The operator @P is used for substitutions in its
associated logical form. When @P-not(P) operates on
8-laugh(X), the result is e-not(laugh(X)).
The operator P/Q is used for forms that require
two substitutions. When
P/Q-each(X,Q,P)
operates on 8-man(X), the result is
@P-each(X,man(X),P),
which in turn can operate by substituting for P.
Notice that @p and P/Q are similar to lambda(P)
and lambda(Q)lambda(P) respectively. But they also
interact with other operators in the system in specific
ways.
To show these first three operators working togeth-
er, let us look at the MS tree that would be produced
for the sentence &amp;quot;Each man laughed&amp;quot;. (Reshaping
leaves this tree unaltered.) We throw away the syn-
tactic fields in the tree nodes (working only with the
semantic items), and show the successive stages in
producing the logical form in Figure 4. In following
the steps in Figure 4, the reader should refer to the
&apos;trans&apos; rules in the Appendix, which are numbered for
reference here. In each step of the translation, a node
combines with its parent, and the &apos;trans&apos; rule used to
do this is indicated.
The operator P*Q appears in coordinate modifiers.
The first four &apos;trans&apos; rules deal with it, and they create
auxiliary operators. The following example will make
clear how these are manipulated. The sentence is
</bodyText>
<table confidence="0.992540090909091">
Treating Coordination in Logic Grammars
8-true
8-true
P/Q-each(X,Q,P)
8-man(X)
8-true
8-laughed(X) (Rule 7 applies)
8-true
8-true
P/Q-each(X,Q,P)
8-man(X)
8-laughed(X) (Rule 7)
8-laughed(X)
8-true
P/Q-each(X,Q,P)
8-man(X) (Rule 7)
8-laughed(X)
8-man(X)
P/Q-each(X,Q,P) Rule 5)
8-laughed(X)
@P-each(X,man(X),P) (Rule 6)
8-each(X,man(X),laughed(X)).
</table>
<figureCaption confidence="0.99843">
Figure 4. The working of &apos;translate&apos;.
</figureCaption>
<bodyText confidence="0.988932166666667">
&amp;quot;Each man ate an apple and a pear.&amp;quot;
This example is shown in the Appendix, with the ini-
tial syntactic analysis and the reshaped tree. In the
reshaped tree, the &apos;sent&apos; node has three daughters, the
first being for the simple noun phrase &amp;quot;each man&amp;quot;, the
second for the conjoined noun phrase &amp;quot;an apple and a
pear&amp;quot;, and the third for the verb phrase with the ob-
ject removed.
If we perform all the modifications that are possible
in this tree without involving the coordination opera-
tor, and if we remove the syntactic fields, then the tree
looks like the following:
</bodyText>
<equation confidence="0.976277857142857">
8-ate(X,Y)
@P-each(X,man(X),P)
8-true
Q /R-exists (Y,R,Q)
8-apple(Y)
S*T-(S&amp;T)
@U-exists(Y,pear(Y),U)
</equation>
<bodyText confidence="0.9926945">
Now the first &apos;trans&apos; rule can apply to the lowest pair
of nodes, and the tree becomes:
</bodyText>
<equation confidence="0.954805833333333">
8-ate(X,Y)
@P-each(X,man(X),P)
8-true
Q/R-exists(Y,R,Q)
8-apple(Y)
cbase 1 ( @U-exists(Y,pear(Y),U),S,T)-(S&amp;T)
</equation>
<bodyText confidence="0.930900333333333">
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 77
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
We have saved the modifier for &amp;quot;a pear&amp;quot; in the first
argument of the `cbase l&apos; operator. Next, this item
operates on the 8-true node, by application of the
second &apos;trans&apos; rule, and we get the tree
</bodyText>
<equation confidence="0.9813838">
8-ate(X,Y)
@P-each(X,man(X),P)
cbase2(8,@U-exists(Y,pear(Y),U),S,T,S&amp;T)-true
Q/R-exists(Y,R,Q)
8-apple(Y)
</equation>
<bodyText confidence="0.9946555">
Now, the third &apos;trans&apos; rule is applied twice, to the two
daughters of the `cbase2&apos; node, and we get
</bodyText>
<equation confidence="0.991511">
8-ate(X,Y)
@P-each(X,man(X),P)
cbase2( @Q,@U-exists(Y,pear(Y),U),S,T,S&amp;T)
-exists(Y,apple(Y),Q)
</equation>
<bodyText confidence="0.996756111111111">
Then, as the last step with coordination operators, the
fourth &apos;trans&apos; rule is applied to let the `cbase2&apos; node
operate on the top node of the tree. This involves two
recursive calls to &apos;trans&apos;, in which the two conjunct
noun phrases operate on the material in the scope of
the coordinate node. (In this case, the material in the
scope is ate(X,Y).) This material gets duplicated, be-
cause of the double application to it. The resulting
tree now is
</bodyText>
<equation confidence="0.947714333333333">
8-exists(Y,apple(Y),ate(X,Y))&amp;exists(Y,pear(Y),
ate(X,Y))
@P-each(X,man(X),P)
</equation>
<bodyText confidence="0.999684">
Finally, the @P node modifies the top node, and after
discarding the operator (an &apos;8&apos;) in the resulting item,
we get the logical form
</bodyText>
<equation confidence="0.5122895">
each(X,man(X),exists(Y,apple(Y),ate(X,Y))
&amp;exists(Y),pear(Y),ate(X,Y)) )
</equation>
<bodyText confidence="0.981563764705882">
Near the end of the Introduction, examples were
given of two syntactically similar sentences with coor-
dination, for which the produced logical forms are
quite different. For the sentence
&amp;quot;Each man and each woman ate an apple&amp;quot;,
the reshaping stage produces a tree that in outline
looks like the following:
sent
nounph &amp;quot;each man&amp;quot;
conj(and)
nounph &amp;quot;each woman&amp;quot;
nounph &amp;quot;an apple&amp;quot;
verbph &amp;quot;ate&amp;quot;
Then, the material for &amp;quot;ate an apple&amp;quot; will be in the
scope of the conjoined noun phrase and this material
gets duplicated, with the resulting logical form being
each(X,man(X),exists(Y,apple(Y),ate(X,Y)))
&amp;each(X,woman(X),exists(Y,apple(Y),ate(X,Y))).
On the other hand, for the sentence
&amp;quot;A man and a woman sat at each table&amp;quot;,
reshaping moves the universally quantified noun
phrase to the left of the existentially quantified con-
joined noun phrase, and the tree is as follows:
sent
nounph &amp;quot;each table&amp;quot;
nounph &amp;quot;a man&amp;quot;
conj (and)
nounph &amp;quot;a woman&amp;quot;
verbph &amp;quot;sat at&amp;quot;
Then the only material in the scope of the conjoined
noun phrase is for &amp;quot;sat at&amp;quot;, and only this gets dupli-
cated. (In fact, the scoping is like that for our earlier
example, &amp;quot;Each man ate an apple and a pear.) The
complete logical form is
</bodyText>
<equation confidence="0.8875065">
each(Y,table(Y), exists(X,man(X),sat_at(X,Y))
&amp; exists(X,woman(X),sat_at(X,Y)) ).
</equation>
<bodyText confidence="0.965753947368421">
Notice that the logical forms for conjoined phrases
in the above analyses share variables. For instance,
the same variable X is used in both man(X) and
woman(X) in the last analysis. This sharing of varia-
bles arises naturally because of the unification of body
lists that is performed during parsing by the &apos;merge&apos;
demon. It keeps things straight very nicely, because
the shared variables may appear in another predica-
tion, like sat_at(X,Y) above, which occurs only once,
outside the conjoined phrase, but is related logically to
both conjuncts.
This sharing of variables presents no problems as
long as the variables are quantified over (as they are
by the existential in the preceding example). But it
makes proper nouns less convenient to treat. If coor-
dination were not being considered, it would be con-
venient to parse proper nouns by the sort of rule listed
in Figure 1 in Section 2, where the proper noun gets
immediately unified with the variable X appearing in
nounph(X). But if such a rule is used with the MSG
parser, then a sentence as simple as &amp;quot;John and Mary
laughed&amp;quot; will not parse, because the parser attempts to
unify the logical subject variable with both `john&apos; and
`mary&apos;.
Therefore, as the semantic item for a proper noun
N, we use a quantified form, specifically
@P-def(X,X=N,P),
and this is carried through in most of the processing.
However, the procedure &apos;translate&apos;, after it has carried
out all the modification, calls a procedure &apos;simplify&apos;
which simplifies the logical form. This gets rid of
some unnecessary &apos;true&apos;s and it carries out the substi-
tutions implicit in the proper noun forms, by doing
some copying of structures and renaming of variables.
78 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
For example, the logical form for &amp;quot;John and Mary
laughed&amp;quot; prior to simplification is essentially
</bodyText>
<equation confidence="0.6281785">
def (X,X= john,laughed( X) )
&amp;def (X,X=mary,laughed(X))
</equation>
<bodyText confidence="0.9858557">
But after simplification, it is
laughed(john)&amp;laughed(mary).
In the sample analyses in the Appendix, we give in
some cases only the logical form and in other cases the
intermediate structures also (the syntactic analysis tree
and the reshaped tree). Analysis times are in milli-
seconds. These do not include times for I/O and con-
version of character strings to word lists. Variables
are printed by Prolog-10 in the form _n, where n is an
integer.
</bodyText>
<sectionHeader confidence="0.844078" genericHeader="method">
5. Possible Extensions
</sectionHeader>
<bodyText confidence="0.99860242">
The main advantages of the formalism presented
here are:
■ automating the treatment of coordination,
■ freeing the user of concern with structure-building,
and
■ providing a modular treatment of semantics, based
upon information given locally in each rule.
While making a reasonable compromise between
power and elegance on the one hand, and efficiency
on the other, our present implementation could be
improved in several ways. For instance, because the
parsing history is kept in a stack that is regularly pop-
ped — the Parent stack — some parsing states are no
longer available for backing up to, so the possibility
exists for some acceptable sentences not to be recog-
nized.
We have experimented with modifications of the
MSG interpreter in which more of the parse history is
saved, and have also considered compiling MSGs into
Prolog and using a general &apos;state&apos; predicate which re-
turns the proof history, but we have not as yet ob-
tained satisfactory results along these lines.
Another possible improvement is to use some se-
mantic guidance for the (at present blind) backing up
through parsing states. The parser already carries
along semantic information (in semantic items) to be
used later on. Some of this information could perhaps
also be used during parsing, in order to improve the
backup. Research along these lines may well provide
some more insight into the dilemma of whether syntax
and semantics should be kept separate or intermingled.
It would also be interesting to include collective
and respective readings of coordinated noun phrases,
perhaps along the lines proposed in Dahl (1981).
We do not presume that our general treatment of
coordination will work for all possible MSG grammars.
Care is necessary in writing an MSG, as with any other
formalism. What we do provide are enough elements
to arrive at a grammar definition that can treat most
structure-building and coordination problems in a
modular and largely automated manner.
We have also investigated an alternative approach
to coordination, which is not metagrammatical but is
nevertheless more flexible than previous approaches,
and involves still another grammar formalism we be-
lieve worth studying in itself. We have named it the
gapping grammar (GG) formalism, as its main feature
is that it allows a grammar rule to rearrange gaps in a
fairly arbitrary fashion. This will be the subject of a
forthcoming article.
</bodyText>
<sectionHeader confidence="0.60799" genericHeader="method">
6. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.995126951219512">
We have described a new logic grammar system for
handling coordination metagrammatically, which also
automatically builds up the modifier structure of a
sentence during parsing. This structure, as we have
seen, can be considered an annotated phrase structure
tree, but the underlying grammar — unlike other recent
approaches to NL processing — is not necessarily
context-free. The rules accepted are generalized
type-0 rules that may include gaps (in view, for in-
stance, of left extraposition), and semantic interpreta-
tion, as we have seen, is guided through the semantic
items, local to each rule, which help resolve scoping
problems. The system&apos;s semantic interpretation com-
ponent can in particular deal with scoping problems
involving coordination.
While the treatment of coordination is the main
motivation for developing still another logic grammar
formalism, we believe our notion of modifier structure
grammar to be particularly attractive for allowing the
user to write grammars in a more straightforward man-
ner and more clearly. Also, because the semantic
information about the structure being built up is de-
scribed modularly in the grammar rules, it becomes
easier to adapt the parser to alternative domains of
application: modifying the logical representation ob-
tained need only involve the semantic items in each
rule. A related but less flexible idea was independent-
ly developed for Restriction Grammars by Hirshman
and Puder (1982). RGs are also logic grammars in the
sense that they are based on Prolog, but they deal only
with context-free definitions augmented by restrictions
(which are procedures attached to the rules). In RGs,
a tree record of the context-free rules applied is auto-
matically generated during the parse. More evolved
representations for the sentence, however, are again
the user&apos;s responsibility and require processing this
automatically generated parse tree.
Another important point, in our view, is the fact
that our system does not preclude context-sensitive
rules, transformations, or gaps. This is contrary to
what seems to be the general tendency today, both in
theoretical linguistics (for example, Gazdar 1981) and
in computational linguistics (for example, Hirshman
and Puder 1982, Joshi and Levy 1982, Robinson
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 79
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
1982, Schubert and Pelletier 1982), towards using
context-free grammars (which, however, are often
augmented in some way — through restrictions, local
constraints, rule schemata, metarules, etc. — compen-
sating for the lack of expressiveness in simple context-
free grammars). This approach was largely motivated
by the need to provide alternatives to transformational
grammar, which on the one hand was felt by Al re-
searchers to deal insufficiently with semantics and with
sentence analysis, and on the other hand, as observed
by Gazdar (1981), could not offer linguistically ade-
quate explanations for important constructs, such as
coordination and unbounded dependencies. Further
arguments supporting this approach include claims of
more efficient parsability, simplicity, and modularity.
From the particular point of view of logic gram-
mars, more evolved grammar formalisms make a great
deal of sense for various reasons. In the first place,
they provide various advantages that have been illus-
trated in Dahl (1981), namely modularity and concise-
ness, clarity and efficiency. A detailed discussion of
these advantages with respect to augmented transition
networks can be found in Pereira and Warren (1980).
Furthermore, they include lower-level grammars as
a special case. In particular, context-free rules aug-
mented with procedures may be written, since even the
simplest logic grammar defined to date (DCGs) allows
Prolog calls to be interspersed with the rules. The
greater expressive power allowed by more evolved
formalisms, then, can only represent a gain, since it
does not preclude more elementary approaches. Logic
grammars, in short, seem to be developing — like other
computer formalisms — into higher-level tools that
allow the user to avoid mechanizable effort in order to
concentrate on as yet unmechanizable, creative tasks.
MSGs are intended as a contribution in this direction.
</bodyText>
<sectionHeader confidence="0.681392" genericHeader="method">
7. References
</sectionHeader>
<reference confidence="0.810147746268657">
Bates, M. 1978 The Theory and Practice of Augmented Transition
Network Grammars. In Bolc, L., Ed., Natural Language Com-
munication with Computers. Springer-Verlag, New York: 191-
259.
Coelho, H.M.F. 1979 A Program Conversing in Portuguese Provid-
ing a Library Service. Ph.D. thesis, University of Edinburgh.
Colmerauer, A. 1973 Un systeme de communication homme-machine
in francais. Groupe d&apos; Intelligence Artificielle, Universite d&apos;Aix-
Marseille.
Colmerauer, A. 1978 Metamorphosis Grammars. In Bole, L., Ed.,
Natural Language Communication with Computers. Springer-
Verlag, New York: 133-189.
Dahl, V. 1977 Un Systeme Deductif d&apos;Interrogation de Banques de
Donnees en Espagnol. These de Doctorat de Specialite,
Universite d&apos;Aix-Marsielle.
Dahl, V. 1981 Translating Spanish into Logic through Logic.
American Journal of Computational Linguistics 13: 149-164.
Gazdar, G. 1981 Unbounded Dependencies and Coordinate Struc-
ture. Linguistic Inquiry 12(2): 155-184.
Hirshman, L. and Puder, K. 1982 Restriction Grammar in Prolog.
Proc. First International Logic Programming Conference. Mars-
eille, France: 85-90.
Joshi, A. and Levy, L.S. 1982 Phrase Structure Trees Bear More
Fruit than You Would Have Thought. American Journal of
Computational Linguistics 8: 1-11.
Kowalski, R.A. 1974 Predicate Logic as a Programming Language.
Proc. IFIP 74. North-Holland, Amsterdam, The Netherlands:
569-574.
Kowalski, R.A. 1979 Logic for Problem Solving. North-Holland,
New York, New York.
McCord, M.C. 1982 Using Slots and Modifiers in Logic Grammars
for Natural Language. Artificial Intelligence 18: 327-367.
McCord, M.C. 1981 Focalizers, the Scoping Problem, and Seman-
tic Interpretation Rules in Logic Grammars. Technical Report,
University of Kentucky. To appear in Warren, D. and van
Caneghem, M., Eds., Logic Programming and its Applications.
Pereira, F. 1981 Extraposition Grammars. American Journal of
Computational Linguistics 7: 243-256.
Pereira, F. and Warren, D. 1980 Definite Clause Grammars for
Language Analysis — a Survey of the Formalism and a Compari-
son with Transition Networks. Artificial Intelligence 13: 231-
278.
Pereira, F. and Warren, D. 1982 An Efficient Easily Adaptable
System for Interpreting Natural Language Queries. American
Journal of Computational Linguistics 8: 110-122.
Pereira, L.M. et al. 1982 ORBI — An Expert System for Environ-
mental Resource Evaluation through Natural Language. Univer-
sidade Nova de Lisboa.
Pereira, L.; Pereira, F.; and Warren, D. 1978 User&apos;s Guide to DEC
System-I0 Prolog. Department of Artificial Intelligence, Univer-
sity of Edinburgh.
Robinson, J. 1982 Diagram: a Grammar for Dialogues. Comm.
ACM 25: 27-47.
Robinson, J.A. 1965 A Machine-Oriented Logic Based on the
Resolution Principle. J. ACM 12: 23-41.
Roussel, P.L. 1975 Prolog Manuel de Reference et d&apos;Utilisation.
Universite d&apos;Aix-Marseille.
Schubert, L. and Pelletier, F. 1982 From English to Logic:
Context-Free Computation of &apos;Conventional&apos; Logical Transla-
tion. American Journal of Computational Linguistics 8(1): 27-44.
Van Emden, M.H. 1975 Programming with Resolution Logic.
Machine Intelligence, 8. John Wiley, New York, New York.
Winograd, T. 1972. Understanding Natural Language. Academic
Press, New York, New York.
Woods, W.A. 1973 An Experimental Parsing System for Transition
Network Grammars. In Rustin, R., Ed., Natural Language
Processing. Algorithmics Press, New York, New York: 145-149.
</reference>
<page confidence="0.753295">
80 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
</page>
<table confidence="0.697319384615385">
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
APPENDIX
/* Grammar rule preprocessor. */
:- public readRules/0,parse/ 3,go/O.
:- op (1000,xfy,(...)).
op(1100,xfy,:).
readRules :-
tell(metgrr),
repeat,
read(Rule),
process (Rule).
process (endRules) :-!,told.
process (Rule) :-!,
</table>
<equation confidence="0.963507225806452">
parts (Rule,Head,Sem,Body),
makex(Head,NT,Ext),
makelist(Body,Body 1),
write(rule(NT,Ext,Sem,Body1)),write(Y),
nl,n1,
fail.
process (Clause) :-
assertz (Clause),
fail.
parts( (Head:Sem &gt; Body),Head,Sem,Body)
parts((Head &gt; Body),Head,f -true,Body).
makelist((X,L),[X I I-11) :-!,makelist(L,L1).
makelist([ ],[ 1) :-!.
makelist(XJX]).
makex((NT,L),NT,Ext) !,makex 1 (nogap,L,Ext).
makex((NT...L),NT,Ext) !,makex 1 (gap,L,Ext).
makex(NT,NT,ni1).
makex1(CT,(S,L),x(CT,Type,S1,X)) :-!,
type (S,S 1,Type),
makexl (nogap,L,X).
makex1 (CT,(S...L),x(CT,Type,S1,X)) :-!,
type (S,S 1,Type),
makex 1 (gap,L,X).
makex 1 (CT,S,x(CT,Type,S1,ni1)) :-
type(S,S1,Type).
type ([S],S,terminal) :-!.
type (S,S,nonterminal).
/* Parser */
parse(String,NonTerminal,Syn) :-
prsallonTerminal],String,[Syn],nil,nil,nil),!.
prs(BLJD I XLMods,Par,Mer,Ext) :-
</equation>
<bodyText confidence="0.6582846">
demon(D,BL,X,Mods,Par,Mer,Ext).
prs(BL,X,Mods,Par,merge(BL,Par 1 ,Mer,Ext),nil) :-
cutoff (Par),
prs(BL,X,Mods,Parl,Mer,Ext).
prs ([[W] I BL],[W I X],Mods,Par,Mer,Ext)
gap (Ext),prs(BL,X,Mods,Par,Mer,Ext).
prsa[W] I BLLX,Mods,Par,Mer,x(_,terminal,W,Ext)):-
prs(BL,X,Mods,Par,Mer,Ext).
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 81
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
</bodyText>
<equation confidence="0.973275290322581">
prsa{B} I BL],X,Mods,Par,Mer,Ext) :-!,
B,prs(BL,X,Mods,Par,Mer,Ext).
prs([NT I BLLX,Mods,Par,Mer,Ext) :-
rule(NT,ExtO,Sem,Body),
stack(Ext0,Ext,Ext1),
prspush(Body,NT,BL,Sem,X,Mods,Par,Mer,Ext1).
prs([NT I BL],X,Mods,Par,Mer,x(_,nonterminal,NT,Ext)):-
prs(BL,X,Mods,Par,Mer,Ext).
prs([ ],X,[ ],parent([_ I BL],Mods,Par),Mer,Ext) :-
prs(BL,X,Mods,Par,Mer,Ext).
prs([ ],[ ],[ 1,nil,nil,ni1).
prspush([ 1,_,BL,_,X,Mods,Par,Mer,Ext) :-!,
prs(BL,X,Mods,Par,Mer,Ext).
prspush(Body,NT,BL,Sem,X,[syn(NT,Sem,Mods1) I Mods],Par,Mer,Ext) :-
prs(Body,X,Mods1,parent([NT I BL],Mods,Par),Mer,Ext).
gap(x(gap,_,_,_)).
gap(ni1).
stack(nil,X,X).
stack(x(C,T,S,X1),X2,x(C,T,S,X3)) :- stack(X1,X2,X3).
cutoff(parenta_ I BL],[ ],Par)) :- satisfied(BL), cutoff(Par).
cutoff (nil).
demon(D,BL,X,Mods,Par,Mer,Ext) :-
conjunction(D,Cat,Sem),
backup(Par,Mods,[syn(Cat,Sem,Mods0)]1NT I _],Parl ),
prs([NT],X,Mods0,nil,merge(BL,Par 1 ,Mer,Ext),ni1).
backup(parent(BL,Mods,Par),Mods0,Mods0,BL,parent(BL,Mods,Par)).
backup(parent([_ I BL],Mods,Par),[ i,Mods0,BL1,Par 1 ) :-
satisfied(BL),
backup(Par,Mods,Mods0,BL1,Parl).
satisfied([ ]) :-!.
satisfied([NT I BL]) :- rule(NT,_,_,[ ]),!,satisfied(BL).
</equation>
<subsectionHeader confidence="0.622228">
/* Semantic Interpretation Rules */
</subsectionHeader>
<bodyText confidence="0.942697222222222">
:- op(400,xfy,&amp;).
:- op(300,fx,@).
/* Reshaping Rules */
reshape(Tree,Sisters,Treel) :-
daughters(Tree,Daus),
reshapelist(Daus,Daus1),
reorder (Daus 1 ,Daus2),
raise(Daus2,Tree,Sisters,Daus3),
newdaus(Tree,Daus3,Tree1).
reshapelist([Tree I Trees],Trees2) :-!,
reshapelist(Trees,Trees1),
reshape (Tree,Sisters,Tree 1 ),
concat(Sisters,[Treel I Trees 1 liTrees2).
reshapelist([ ],[ ]).
reorderaA I LIM) :-
reorder(L,L1),
insert(A,L1,M).
reorder([ ],[ ]).
</bodyText>
<page confidence="0.692536">
82 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
</page>
<note confidence="0.475057">
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
</note>
<equation confidence="0.931180454545455">
insert(A,[B j L],M) :-
prec(A,PA),prec(B,PB),
(PB&gt;PA,!,M=[B I L1],insert(A,L,L1) I
m=[A,B I L]).
insert(AJ ],[A]).
raise([Dau I Daus],Tree,[Dau I Sisters],Dausl) :-
above(Dau,Tree),!,
raise (Daus,Tree,Sisters,Daus1).
raise([Dau I Daus],Tree,Sisters,[Dau I Dausl]) :-
raise(Daus,Tree,Sisters,Daus1).
raise([ ],Tree,[ ],[ ]).
</equation>
<bodyText confidence="0.890962291666667">
daughters (syn(_,_,Daus),Daus).
newdaus(syn(NT,Sem,_),Daus,syn(NT,Sem,Daus)).
cat(syn(NT,_,_),NT).
prec(Syn,P) stype(Syn,S),prec 1 (S,P),!.
prec(_,0).
stype(syn(nounph(_,Stype),_,_),Stype).
precl(def,6).
precl(a11,6).
precl(indef,4).
above(Synl ,Syn2)
cat (Synl,nounph(_,_)),cat(Syn2,Cat),
(Cat=relative(_) I Cat= conj(_)).
/* Translation Rules */
translate(Syn,LogForm 1 ) :-
transmod(Syn,8-true,P-LogForm),
simplify(LogForm,[ ],LogForm 1 ).
transmods([Mod I Mods],Sem 1 ,Sem3) :-
transmods(Mods,Seml,Sem2),
transmod(Mod,Sem2,Sem3).
transmods([ ],Sem,Sem).
transmod(syn(_,Sem,Mods),Sem 1 ,Sem2) :-
transmods(Mods,Sem,Sem0),
trans (SemO,Seml,Sem2).
/* Rules for &apos;trans&apos; are numbered for convenient reference in the text. */
</bodyText>
<equation confidence="0.692991555555556">
/*1*/ trans(Sem,C*D-P,cbase 1 (Sem,C,D)-P) :-!.
/*2*/ trans(cbasel(Sem,C,D)-P,Op-Q,
cbase2(0p,Sem,C,D,R)-true) :-!,
conj(P,Q,R).
/*3*/ trans(Op-P,cbase2(0p1,Sem,C,D,B)-P1,
cbase2(0p2,Sem,C,D,B)-P2) :-!,
trans(Op-P,Opl-P1,0p2-P2).
/*4*/ trans(cbase2(0p,Sem 1 ,C,D,B)-P,Sem2,0p1-B) :-!,
trans(Op-P,Sem2,0pl-C),
</equation>
<footnote confidence="0.242166428571429">
trans(Seml ,Sem2,0p1 -D).
/*5*/ trans(P/Q-R,Op-Q,@P-R).
/*6*/ trans(@P-Q,Op-P,Op-Q).
/*7*/ trans(f -P,Op-Q,Op-R) conj(P,Q,R).
/*8*/ trans(r-P,Op-Q,Op-R) conj(Q,P,R).
/*9*/ trans(subst(X)-X,Sem,Sem).
/*10*/ trans(id-P,Sem,Sem).
</footnote>
<note confidence="0.3653325">
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 83
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
</note>
<equation confidence="0.849485488888889">
conj(true,Q,Q)
conj(P,true,P)
conj(P,Q,P&amp;Q).
/* Simplification Rules */
simplify(X,D,Y) var(X),!,find(X,D,Y).
simplify(E,_,E) atomic(E),!.
simplify(def(X,X=Y,E),D,E1) :-!, simplify(E,[X=Y I D],E1).
simplify(true&amp;E,D,E1) :-!, simplify(E,D,E1).
simplify(E&amp;true,D,E1) :-!, simplify(E,D,E1).
simplify(E,D,E1) :-
E...[P 1 Args],
simplist(Args,D,Args1),
El=..[P 1 Argsl].
find(XJX1=Y 1 _],Y) :- X= =X1,!.
find(Xt 1 D],Y) find(X,D,Y),!.
find(X,[ ],X).
simplistUE 1 LLDJE1 1 Li]) :-
simplify(E,D,E1),
simplist(L,D,L1).
simplist([ ],_,[ ]).
/* Syntax */
readRules.
sent --&gt; nounph(X,_),verbph(X).
nounph(X,def):@P-def(X,X=N,P) --&gt; [N],[prop(N)1.
nounph(X,Stype) --&gt;
det(X,Stype),adj(X),noun(X,Comps),comps(Comps),relative(X).
nounph(X,_) --&gt; trace(X).
det(X,Stype):Sem --&gt; [D],ideter(D,X,Stype,Sem)1.
adj(_) --&gt; [ ].
adj(X):Sem --&gt; [Adj],fadjec(Adj,X,Sem)1.
noun(X,Comps):€-Pred --&gt; [N],{n(N,X,Comps,Pred)}.
relative(X) --&gt; [ ].
relative(X) --&gt; open,rel_mk(X),sent,close.
open...close --&gt; [ ].
rel_mk(X)...trace(X) --&gt; [N],[rel_pro(N)1.
rel mk(X)...[P],trace(X) --&gt; [P],{prep(P)},[N],frel_pro(N)1.
verbph(X) --&gt; advl,verb(X,Comps),comps(Comps).
advl --&gt; [ ].
advl:Sem --&gt; [Adv],[adverb(Adv,Sem)1.
verb(X,Comps):8-Pred --&gt; [V],{v(V,Pred,X,Comps)1.
comps([ ]) --&gt; [ ].
comps([X 1 LI) --&gt; comp(X),comps(L).
comp(obj-X) --&gt; nounph(X,_).
comp(pobj(Prep)-X) --&gt; [Prep],nounph(X,_).
endRules.
</equation>
<page confidence="0.749719">
84 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
</page>
<note confidence="0.609643">
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
</note>
<sectionHeader confidence="0.416511" genericHeader="method">
/* Lexicon */
</sectionHeader>
<bodyText confidence="0.998965734693878">
conjunction(and,conj(and),P*Q-(P&amp;Q)).
conjunction(or,conj(or),P*Q-(P;Q)).
conjunction(but,conj(but),P*Q-but(P,Q)).
prop (john).
prop (bill).
prop(mary).
deter(a,X,indef,Q/P-exists(X,P,Q)).
deter(an,X,indef,Q/P-exists(X,P,Q)).
deter(the,X,def,Q/P-def(X,P,Q)).
deter(each,X,a11,Q/P-each(X,P,Q)).
deter(every,X,a11,Q/P-every(X,P,Q)).
adjec(red,X,e-red(X)).
adjec(blue,X,e-blue(X)).
adjec(glass,X,e-glass(X)).
n(man,X,[ ],man(X)).
n(woman,X,[ ],woman(X)).
n(car,X,[ ],car(X)).
n(train,XJ ],train(X)).
n(book,X,[ ],book(X)).
n(pencil,X,[ ],pencil(X)).
n(table,X,[ ],table(X)).
n(window,X,[ ],window(X)).
n(father,X,[pobj(of)-Y],father(X,Y)).
n(friend,X,[pobj(of)-Y],friend(X,Y)).
n(apple,X,[ ],apple(X)).
n(pear,X,[ ],pear(X)).
v(saw,saw(X,Y),X,[obj-Y]).
v(heard,heard(X,Y),X,[obj-Y]).
v(demolished,demolished(X,Y),X,[obj-Y]).
v(laughed,laughed(X),X,[ ]).
v(drove,drove_through(X,Y,Z),X,[obj-Y,pobj(through)-Z]).
v(gave,gave(X,Y,Z),X,[obj-Y,pobj(to)-Z]).
v(ate,ate(X,Y),X,[obj-Y]).
v(sat,sat_at(X,Y),X,[pobj(at)-Y]).
adverb(completely,@P-completely(P)).
adverb(finally,@P-finally(P)).
rel pro(who).
_
rel pro (whom).
_
rel pro(that).
_
rel pro (which).
_
prep(to).
prep(from).
prep(with).
prep(of).
prep(through).
</bodyText>
<figure confidence="0.699593571428571">
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 85
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
EXAMPLES
Prolog-10 version 3
Input sentence:
: each man ate an apple and a pear.
Syntactic analysis, time = 143 msec.
sent 8-true
nounph(_1,a11) 8-true
det(_1,a11)2/3-each(_1,_3,2)
noun( 1,[ ]) 8-man(1)
verbph( _-1) 8-true
verb( 1,[obj- 4]) 8-ate(_1,_4)
comps([obj-_4]) 8-true
comp(obj-_4) 8-true
nounph(_4,indef) 8-true
det(_4,indef)_5/_6-exists(_4,_6,_5)
noun(_4,[ ]) 8-apple( 4)
conj(and) _7*_8-_7&amp;:8
nounph(_4,indef) 8-true
det(_4,indef) _9/_10-exists(_4,_10,_9)
noun(_4,[ ]) 8-pear(_4)
Reshaping tree, time = 16 msec.
sent 8-true
nounph( 1,a11) 8-true
det( -1,a11) 2/ 3-each( 1,_3,_2)
noun-( 1,[ -f) 8-man(1)--
nounph(_4,indef) 8-true
</figure>
<equation confidence="0.8503881">
det( 4,indef) 5/_6-exists(_4,_6,5)
noui7(_4,[ ]) T-apple( 4)
conj(and) 7* 8- 7&amp;
nounph(_4,indef) 8-true
det( ,indef) 9/ 10-exists(_4,_10,_9)
nouri-(_4,[ ]) 8-pear(_4)
verbph( 1) 8-true
verb( 1,[obj- 4]) 8-ate(_1,_4)
comp-s-([obj- _-47]) 8-true
comp(obj-_4) 8-true
</equation>
<bodyText confidence="0.9509205">
Semantic analysis, time = 22 msec.
each(_1,man(_1),exists(_2,apple(_2),ate(_1,2))
&amp;exists(_2,pear(_2),ate(_1,_2)))
Input sentence:
: john ate an apple and a pear.
Syntactic analysis, time = 144 msec.
Reshaping tree, time = 35 msec.
Semantic analysis, time = 11 msec.
exists (_1,apple(_1),ate(john,_1))&amp;exists(_1,pear(_1),ate( john,_1))
Input sentence:
a man and a woman saw each train.
Syntactic analysis, time = 94 msec.
sent 8-true
nounph(_1,indef) 8-true
</bodyText>
<page confidence="0.725383">
86 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
</page>
<figure confidence="0.909369526946108">
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
det(_ I ,indef) _2/_3-exists(_1,_3,_2)
noun(_1,[ ]) 8-man( 1)
conj(and)_4*_5-_4&amp;_5
nounph(_1,indef) 8-true
det(_1,indef) _6/_7-exists(_1,_7,_6)
noun(_1,[ ]) 8-woman(1)
verbph(_1) 8-true
verb(_1,[obj-_8]) 8-saw(_1,_8)
comps([obj-_8]) 8-true
comp(obj-_8) 8-true
nounph(_8,a11) 8-true
det(_8,a11)_9/_10-each(_8,_10,9)
noun(_8,[ ]) 8-train(_8)
Reshaping tree, time = 24 msec.
sent 8-true
nounph(_1,a11) 8-true
det(_1,a11)_2/_3-each(_1,_3,_2)
noun( 1,[ ]) 8-train(1)
nounph( —4,indef) 8-true
det(_4,indef) 5/_6-exists(_4,_6,5)
noun(_4,[ ]) 8-man(4)
conj(and)_7*_8-_7&amp;_8
nounph(_4,indef) 8-true
det(_4,indef) 9/_10-exists(_4,_10,9)
noun(_4,[ ]) 8-woman(4)
verbph(_4) 8-true
verb(_4,[obj-_11) 8-saw(_4,_1)
comps([obj-_1]) 8-true
comp(obj-_1) 8-true
Semantic analysis, time = 16 msec.
each(_1,train(_1),exists(_2,man(_2),saw(_2,_1))
&amp;exists(_2,woman(_2),saw(_2,_1)))
Input sentence:
I : each man and each woman ate an apple.
Syntactic analysis, time = 78 msec.
sent 8-true
nounph(_1,a11) 8-true
det(_1,a11) _2/3-each(_1,_3,_2)
noun(_1,[ ]) 8-man(1)
conj(and)_4*_5-_4&amp;_5
nounph(_1,a11) 8-true
det(_1,a11)_6/_7-each(_1,_7,_6)
noun(_1,[ ]) 8-woman(1)
verbph(_1) 8-true
verb(_1,[obj-_8]) 8-ate(_1,_8)
comps([obj-_8]) 8-true
comp(obj-_8) 8-true
nounph(_8,indef) 8-true
det(_8,indef) 9/_10-exists(_8,_10,9)
noun(_8,[ ]) 8-apple(8)
Reshaping tree, time = 14 msec.
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 87
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
sent 8-true
nounph(_1,a11) 8-true
det(_1,a11) 2/_3-each(_1,_3,_2)
noun(_1,[ 1) 8-man(1)
conj(and)_4*_5-_4&amp;_5
nounph(_1,a11) 8-true
det(_1,a11) _6/_7-each(_1,_7,_6)
noun(_1,[ ]) 8-woman(1)
nounph(_8,indef) 8-true
det(_8,indef) 9/_10-exists(_8,_10,9)
noun( 8,[ ]) 8-apple(8)
verbph(_1) 8-true
verb(_1,[obj-_8]) 8-ate(_1,_8)
comps([obj-_8]) 8-true
comp(obj-_8) 8-true
Semantic analysis, time = 20 msec.
each(_1,man(_1),exists(_2,apple(_2),ate(_1,_2)))
&amp;each(_1,woman(_1),exists(_2,apple(2),ate(_1,_2)))
Input sentence:
I john saw and the woman heard a man that laughed.
Syntactic analysis, time = 182 msec.
sent 8-true
nounph(_1,def) @_2-def(_1,_1=john,_2)
verbph(_1) 8-true
verb(_1,[obj-_3]) 8-saw(_1,_3)
conj(and)_4*_5-_4&amp;_5
sent 8-true
nounph(_6,def) 8-true
det(_6,def)_7/_8-def(_6,_8,_7)
noun( 6,[ ]) 8-woman(6)
verbph(_6) -true
verb(_6,[obj-_3]) 8-heard(_6,_3)
comps([obj-_3]) 8-true
comp(obj-_3) 8-true
nounph(_3,indef) 8-true
det(_3,indef)9/_10-exists(_3,_10,9)
noun(_3,[ ]) 8-man(_3)
relative(3) 8-true
rel mk( 3) 8-true
_ _
sent 8-true
nounph( 3, 11) 8-true
verbph( —3) -i-true
verb(_3,[ ]) 8-laughed(3)
Reshaping tree, time = 24 msec.
sent 8-true
nounph(_1,def) @_2-def(_1,_1=john,_2)
verbph(_1) 8-true
verb(_1,[obj-_3]) 8-saw(_1,_3)
conj(and)_4*_5-_4&amp;_5
nounph(_6,def) -true
det(_6,def) _7/_8-def(_6,_8,_7)
noun( 6,[ ]) 8-woman(6)
nounph( —3,indef) 8-true
88 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
det(_3,indef)_9/_10-exists(_3,_10,_9)
noun(_3,[ ]) 8-man(3)
relative( 3) 8-true
nounp—h(_3,wh) 8-true
rel mk( 3) 8-true
_ _
sent -true
verbph(_3) 8-true
verb(_3,[ ]) 8-laughed(3)
sent 8-true
verbph( 6) 8-true
verb( 6,[obj- 3]) 8-heard(_6,_3)
compsgobj- j]) 8-true
comp(obj-_3) 8-true
Semantic analysis, time = 18 msec.
def (_1,woman(_1),exists(_2,man(_2)&amp;laughed(_2),
saw(john,_2)&amp;heard(_1,_2)))
Input sentence:
I john drove the car through and completely demolished a window.
Syntactic analysis, time = 80 msec.
sent 8-true
nounph(_1,def) @_2-def(_1,_1=john,_2)
verbph(_1) 8-true
verb( 1,[obj- 3,pobj(through)- 4]) 8-drove_through(_1,_3,_4)
compsg1
obj-,pobj(through)-_41) 8-true
comp(obj-_3) 8-true
nounph(_3,def) 8-true
det(_3,def) _5/_6-def(_3,_6,_5)
noun(_3,[ ]) 8-car(3)
comps([pobj(through)-_4]) 8-true
comp(pobj(through)-_4) 8-true
conj(and)_7*_8-_7&amp;_8
verbph(_1) 8-true
advl @_9-completely(_9)
verb( 1,[obj- 4]) e-demolished(_1,_4)
comps([obj- j]) 8-true
comp(obj-_4) 8-true
nounph(_4,indef) 8-true
det(_4,indef) _10/_11-exists(_4,_11,_10)
noun(_4,[ ]) 8-window(4)
Reshaping tree, time = 22 msec.
sent -true
nounph(_1,def) @_2-def(_1,_1=john,_2)
nounph(_3,def) 8-true
det(_3,def) _4/5-def(_3,_5,_4)
noun( 3,[ ]) 8-car(_3)
verbph(I) 8-true
verb(_1,[obj-_3,pobj(through)-_6]) 8-drove_through(_1,_3,_6)
comps([obj-_3,pobj(through)-_6]) 8-true
comp(obj-_3) 8-true
compsapobj(through)-_61) 8-true
comp(pobj(through)-_6) 8-true
conj(and)_7*_8-_7&amp;_8
nounph(_6,indef) 8-true
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 89
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
</figure>
<equation confidence="0.975649714285714">
det(_6,indef) _9/_10-exists(_6,_10,_9)
noun( 6,[ ]) 8-window( 6)
verbph( _I) 8-true
advl @_11-completely(_11)
verb( 1,[obj- 6]) 8-demolished(_1,_6)
comps([obj-_6]) 8-true
comp(obj-_6) 8-true
</equation>
<bodyText confidence="0.945244444444444">
Semantic analysis, time = 9 msec.
def (_1,car(_1),exists(_2,window(_2),
drove through(john,_1,_2)&amp;completely(demolished(john,_2))))
Input sentence:
: the woman who gave a book to john and
drove a car through a window laughed.
Syntactic analysis, time = 250 msec.
Reshaping tree, time = 108 msec.
Semantic analysis, time = 28 msec.
def (_1,woman(_1 )&amp;exists(_2,book(.2),
exists(_3,car(_3),exists(_4,window(_4),gave(_1,_2,john)&amp;
drove through(_1,_3,_4)))),laughed(_1))
Input sentence:
: john saw the man that mary saw and bill heard.
Syntactic analysis, time = 87 msec.
Reshaping tree, time = 27 msec.
Semantic analysis, time = 25 msec.
def (_1,man(_1)&amp;saw(mary,_1)&amp;heard(bill,_1),saw(john,_1))
Input sentence:
: john saw the man that heard the woman that laughed and saw bill.
Syntactic analysis, time = 174 msec.
Reshaping tree, time = 101 msec.
Semantic analysis, time = 23 msec.
def (_1,man(_1)&amp;def (_2,woman(_2)&amp;laughed(_2)&amp;
saw (_2,bill),heard(_1,_2)),saw(john,_1))
Input sentence:
: the man that mary saw and john heard and bill gave a book to laughed.
Syntactic analysis, time = 1199 msec.
Reshaping tree, time = 106 msec.
Semantic analysis, time = 40 msec.
def(_1,man(_1)&amp;saw(mary,_1)&amp;exists(_2,book(_2),
heard( john,_1)&amp;gave(bill,_2,_1)),laughed(_1))
Input sentence:
: the man that mary saw and heard gave an apple to each woman.
Syntactic analysis, time = 144 msec.
sent 8-true
</bodyText>
<equation confidence="0.4699392">
nounph(_1,def) 8-true
det(_1,def) _2/_3-def(_1,_3,_2)
noun(_1,[ ]) 8-man(1)
relative(1) 8-true
rel_mk(_1) 8-true
</equation>
<page confidence="0.865111">
90 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983
</page>
<figure confidence="0.945582407407407">
Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars
sent P-true
nounph(_4,def) @_5-def(_4,_4=mary,_5)
verbph(_4) e-true
verb(_4,[obj-_1]) P-saw(_4,_1)
conj(and)_6*_7-_6&amp;_7
verb(_4,[obj-_1]) e-heard(_4,_1)
comps([obj-_1]) e-true
comp(obj-_1) P-true
nounph(_1,_8) P-true
verbph(_1) e-true
verb(_1,[obj-_9,pobj(to)-_10]) P-gave(_1,_9,_10)
compsgobj-_9,pobj(to)-_101) e-true
comp(obj-_9) e-true
nounph(_9,indef) e-true
det(_9,indef) _11/_12-exists(_9,_12,_11)
noun(_9,[ ]) P-apple(_9)
compsapobj(to)-_10]) e-true
comp(pobj(to)-_10) P-true
nounph(_10,a11) e-true
det( 10,a11) 13/ 14-each( 10,_14,_13)
noun(_10,[ -]-) e-woman(_10)
Reshaping tree, time = 26 msec.
sent P-true
nounph(_1,def) e-true
det(_1,def)_2/_3-def(_1,_3,_2)
noun(_1,[ ]) e-man(_1)
relative(1) P-true
nounph(_1,wh) P-true
nounph(_4,def) @_5-def(_4,_4=mary,_5)
rel mk(_1) e-true
sent P-true
verbph(_4) e-true
verb(_4,[obj-_1]) e-saw(_4,_1)
conj(and)_6*_7-_6&amp;_7
verb(_4,[obj-_11) e-heard(_4,_1)
comps([obj-_1]) e-true
comp(obj-_1) e-true
nounph(_8,a11) e-true
det(_8,a11)_9/_10-each(_8,_10,_9)
noun( 8,[ ]) e-woman(_8)
nounph( —11,indef) f-true
det(_11,indef) _12/_13-exists(_11,_13,_12)
noun( 11,[ ]) P-apple(_11)
verbph( _T) e-true
verb(_1,[obj-_11,pobj(to)-_8]) P-gave(_1,_11,_8)
comps([obj-_11,pobj(to)-_8]) e-true
comp(obj-_11) f-true
compsapobj(to)-_8]) P-true
comp(pobj(to)-_8) e-true
Semantic analysis, time = 22 msec.
def(_1,man(_1)&amp;heard(mary,_1)&amp;saw(mary,_1),
each(_2,woman(_2),exists(_3,apple(_3),gave(_1,_3,_2))))
American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 91
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.303986">
<title confidence="0.999976">Treating Coordination in Logic Grammars</title>
<author confidence="0.88595">Veronica Dahll</author>
<affiliation confidence="0.819767">Computing Sciences</affiliation>
<address confidence="0.757391">Simon Fraser Burnaby, B.C. V5A 1S6</address>
<email confidence="0.593112">C.</email>
<affiliation confidence="0.9997185">Computer Science University of</affiliation>
<address confidence="0.988845">Lexington, KY 40506</address>
<abstract confidence="0.993872666666667">Logic grammars are grammars expressible in predicate logic. Implemented in the programming language Prolog, logic grammar systems have proved to be a good basis for natural language processing. One of the most difficult constructions for natural language grammars to treat is coordination (construction with conjunctions like &apos;and&apos;). This paper a logic grammar formalism, structure grammars together with an interpreter written in Prolog, which can handle coordination (and other natural language constructions) in a reasonable and general way. The system produces both syntactic analyses and logical forms, and problems of scoping for coordination and quantifiers are dealt with. The MSG formalism seems of interest in its own right (perhaps even outside natural language processing) because the notions of syntactic structure and semantic interpretation are more constrained than in many previous systems (made more implicit in the formalism itself), so that less burden is put on the grammar writer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bates</author>
</authors>
<title>The Theory and Practice of Augmented Transition Network Grammars.</title>
<date>1978</date>
<booktitle>In Bolc, L., Ed., Natural Language Communication with Computers.</booktitle>
<pages>191--259</pages>
<publisher>Springer-Verlag,</publisher>
<location>New York:</location>
<contexts>
<context position="4228" citStr="Bates 1978" startWordPosition="619" endWordPosition="620">nd/or specific permission. 0362-613X/83/020069-23$03.00 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 69 Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars tion as a &amp;quot;metagrammatical&amp;quot; construction, in the sense that metarules, general system operations, or &amp;quot;secondpass&amp;quot; operations such as transformations, are needed for its formulation. Perhaps the most general and powerful metagrammatical device for handling coordination in computational linguistics has been the SYSCONJ facility for augmented transition networks (ATNs) (Woods 1973; Bates 1978). The ATN interpreter with this facility built into it can take an ATN that does not itself mention conjunctions at all, and will parse reduced coordinate constructions, which are of the form A X and Y B, for example, John drove his car through and A X completely demolished a plate glass window. where the unreduced deep structure corresponds to AXB and A Y B. The result of the parse is this unreduced structure. SYSCONJ accomplishes this by treating the conjunction as an interruption which causes the parser to back up in its history of the parse. Before backing up, the current configuration (im</context>
</contexts>
<marker>Bates, 1978</marker>
<rawString>Bates, M. 1978 The Theory and Practice of Augmented Transition Network Grammars. In Bolc, L., Ed., Natural Language Communication with Computers. Springer-Verlag, New York: 191-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M F Coelho</author>
</authors>
<title>A Program Conversing in Portuguese Providing a Library Service.</title>
<date>1979</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="2298" citStr="Coelho (1979)" startWordPosition="333" endWordPosition="334"> grammars (Pereira and Warren 1980) are a special case of metamorphosis grammars. The first sizable application of logic grammars was a Spanish/French-consultable data base system by Dahl (1977, 1981), which was later adapted to Portuguese Visiting in the Computer Science Department, University of Kentucky, during part of this research. Work partially supported by Canadian NSERC Grant A2436 and Simon Fraser P.R. Grant 42406. 2 Current address: IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598. by L. Pereira and H. Coelho and to English by F. Pereira and D. Warren. Coelho (1979) developed a consulting system in Portuguese for library service, and F. Pereira and D. Warren (1980) developed a sizable English data base query system with facilities for query optimization. McCord (1982, 1981) presented ideas for syntactic analysis and semantic interpretation in logic grammars, with application to English grammar; some of these ideas are used in our work described here. Coordination (grammatical construction with the conjunctions &apos;and&apos;, &apos;or&apos;, &apos;but&apos;) has long been one of the most difficult natural language phenomena to handle, because it can involve such a wide range of gram</context>
</contexts>
<marker>Coelho, 1979</marker>
<rawString>Coelho, H.M.F. 1979 A Program Conversing in Portuguese Providing a Library Service. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Colmerauer</author>
</authors>
<title>Un systeme de communication homme-machine in francais. Groupe d&apos; Intelligence Artificielle, Universite d&apos;AixMarseille.</title>
<date>1973</date>
<contexts>
<context position="1340" citStr="Colmerauer 1973" startWordPosition="189" endWordPosition="190">er natural language constructions) in a reasonable and general way. The system produces both syntactic analyses and logical forms, and problems of scoping for coordination and quantifiers are dealt with. The MSG formalism seems of interest in its own right (perhaps even outside natural language processing) because the notions of syntactic structure and semantic interpretation are more constrained than in many previous systems (made more implicit in the formalism itself), so that less burden is put on the grammar writer. 1. Introduction Since the development of the Prolog programming language (Colmerauer 1973; Roussel 1975), logic programming (Kowalski 1974, 1979; Van Emden 1975) has been applied in many different fields. In natural language processing, useful grammar formalisms have been developed and incorporated in Prolog: metamorphosis grammars, due to Colmerauer (1978), and extraposition grammars, defined by F. Pereira (1981); definite clause grammars (Pereira and Warren 1980) are a special case of metamorphosis grammars. The first sizable application of logic grammars was a Spanish/French-consultable data base system by Dahl (1977, 1981), which was later adapted to Portuguese Visiting in the</context>
</contexts>
<marker>Colmerauer, 1973</marker>
<rawString>Colmerauer, A. 1973 Un systeme de communication homme-machine in francais. Groupe d&apos; Intelligence Artificielle, Universite d&apos;AixMarseille.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Colmerauer</author>
</authors>
<title>Metamorphosis Grammars. In</title>
<date>1978</date>
<pages>133--189</pages>
<publisher>SpringerVerlag,</publisher>
<location>New York:</location>
<contexts>
<context position="1610" citStr="Colmerauer (1978)" startWordPosition="227" endWordPosition="228">en outside natural language processing) because the notions of syntactic structure and semantic interpretation are more constrained than in many previous systems (made more implicit in the formalism itself), so that less burden is put on the grammar writer. 1. Introduction Since the development of the Prolog programming language (Colmerauer 1973; Roussel 1975), logic programming (Kowalski 1974, 1979; Van Emden 1975) has been applied in many different fields. In natural language processing, useful grammar formalisms have been developed and incorporated in Prolog: metamorphosis grammars, due to Colmerauer (1978), and extraposition grammars, defined by F. Pereira (1981); definite clause grammars (Pereira and Warren 1980) are a special case of metamorphosis grammars. The first sizable application of logic grammars was a Spanish/French-consultable data base system by Dahl (1977, 1981), which was later adapted to Portuguese Visiting in the Computer Science Department, University of Kentucky, during part of this research. Work partially supported by Canadian NSERC Grant A2436 and Simon Fraser P.R. Grant 42406. 2 Current address: IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 1059</context>
</contexts>
<marker>Colmerauer, 1978</marker>
<rawString>Colmerauer, A. 1978 Metamorphosis Grammars. In Bole, L., Ed., Natural Language Communication with Computers. SpringerVerlag, New York: 133-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Dahl</author>
</authors>
<title>Un Systeme Deductif d&apos;Interrogation de Banques de Donnees en Espagnol. These de Doctorat de Specialite, Universite d&apos;Aix-Marsielle.</title>
<date>1977</date>
<contexts>
<context position="1878" citStr="Dahl (1977" startWordPosition="265" endWordPosition="266">ince the development of the Prolog programming language (Colmerauer 1973; Roussel 1975), logic programming (Kowalski 1974, 1979; Van Emden 1975) has been applied in many different fields. In natural language processing, useful grammar formalisms have been developed and incorporated in Prolog: metamorphosis grammars, due to Colmerauer (1978), and extraposition grammars, defined by F. Pereira (1981); definite clause grammars (Pereira and Warren 1980) are a special case of metamorphosis grammars. The first sizable application of logic grammars was a Spanish/French-consultable data base system by Dahl (1977, 1981), which was later adapted to Portuguese Visiting in the Computer Science Department, University of Kentucky, during part of this research. Work partially supported by Canadian NSERC Grant A2436 and Simon Fraser P.R. Grant 42406. 2 Current address: IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598. by L. Pereira and H. Coelho and to English by F. Pereira and D. Warren. Coelho (1979) developed a consulting system in Portuguese for library service, and F. Pereira and D. Warren (1980) developed a sizable English data base query system with facilities for query o</context>
</contexts>
<marker>Dahl, 1977</marker>
<rawString>Dahl, V. 1977 Un Systeme Deductif d&apos;Interrogation de Banques de Donnees en Espagnol. These de Doctorat de Specialite, Universite d&apos;Aix-Marsielle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Dahl</author>
</authors>
<title>Translating Spanish into Logic through Logic.</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>13</volume>
<pages>149--164</pages>
<contexts>
<context position="47437" citStr="Dahl (1981)" startWordPosition="7659" endWordPosition="7660">sible improvement is to use some semantic guidance for the (at present blind) backing up through parsing states. The parser already carries along semantic information (in semantic items) to be used later on. Some of this information could perhaps also be used during parsing, in order to improve the backup. Research along these lines may well provide some more insight into the dilemma of whether syntax and semantics should be kept separate or intermingled. It would also be interesting to include collective and respective readings of coordinated noun phrases, perhaps along the lines proposed in Dahl (1981). We do not presume that our general treatment of coordination will work for all possible MSG grammars. Care is necessary in writing an MSG, as with any other formalism. What we do provide are enough elements to arrive at a grammar definition that can treat most structure-building and coordination problems in a modular and largely automated manner. We have also investigated an alternative approach to coordination, which is not metagrammatical but is nevertheless more flexible than previous approaches, and involves still another grammar formalism we believe worth studying in itself. We have nam</context>
<context position="51688" citStr="Dahl (1981)" startWordPosition="8309" endWordPosition="8310">lt by Al researchers to deal insufficiently with semantics and with sentence analysis, and on the other hand, as observed by Gazdar (1981), could not offer linguistically adequate explanations for important constructs, such as coordination and unbounded dependencies. Further arguments supporting this approach include claims of more efficient parsability, simplicity, and modularity. From the particular point of view of logic grammars, more evolved grammar formalisms make a great deal of sense for various reasons. In the first place, they provide various advantages that have been illustrated in Dahl (1981), namely modularity and conciseness, clarity and efficiency. A detailed discussion of these advantages with respect to augmented transition networks can be found in Pereira and Warren (1980). Furthermore, they include lower-level grammars as a special case. In particular, context-free rules augmented with procedures may be written, since even the simplest logic grammar defined to date (DCGs) allows Prolog calls to be interspersed with the rules. The greater expressive power allowed by more evolved formalisms, then, can only represent a gain, since it does not preclude more elementary approache</context>
</contexts>
<marker>Dahl, 1981</marker>
<rawString>Dahl, V. 1981 Translating Spanish into Logic through Logic. American Journal of Computational Linguistics 13: 149-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Unbounded Dependencies and Coordinate Structure.</title>
<date>1981</date>
<journal>Linguistic Inquiry</journal>
<volume>12</volume>
<issue>2</issue>
<pages>155--184</pages>
<contexts>
<context position="8711" citStr="Gazdar (1981)" startWordPosition="1361" endWordPosition="1362">rallel to &amp;quot;phrase structure grammar&amp;quot;. If extraposition and coordination are neglected, there is a context-free phrase structure grammar underlying an MSG; and the MS trees are indeed derivation trees for this underlying grammar, but with extra information attached to the nodes. In an MS tree, each node contains not only syntactic information but also a term called a semantic item (supplied in the grammar), which determines the node&apos;s contribution to the logical form of the sentence. This contribution is for the node alone, and does not refer to the daughters of the node, as in the approach of Gazdar (1981). Through their semantic items, the daughters of a node act as modifiers of the node, in a fairly traditional sense made precise below — hence the term &amp;quot;modifier structure&amp;quot;. The notion of modifier structure used here and the semantic interpretation component, which depends on it, are much the same as in previous work by McCord 70 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars (1982, 1981), especially the latter paper. But new elements are the notion of MSG (making modifier structure </context>
<context position="50409" citStr="Gazdar 1981" startWordPosition="8119" endWordPosition="8120">context-free definitions augmented by restrictions (which are procedures attached to the rules). In RGs, a tree record of the context-free rules applied is automatically generated during the parse. More evolved representations for the sentence, however, are again the user&apos;s responsibility and require processing this automatically generated parse tree. Another important point, in our view, is the fact that our system does not preclude context-sensitive rules, transformations, or gaps. This is contrary to what seems to be the general tendency today, both in theoretical linguistics (for example, Gazdar 1981) and in computational linguistics (for example, Hirshman and Puder 1982, Joshi and Levy 1982, Robinson American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 79 Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars 1982, Schubert and Pelletier 1982), towards using context-free grammars (which, however, are often augmented in some way — through restrictions, local constraints, rule schemata, metarules, etc. — compensating for the lack of expressiveness in simple contextfree grammars). This approach was largely motivated by the need to provide al</context>
</contexts>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, G. 1981 Unbounded Dependencies and Coordinate Structure. Linguistic Inquiry 12(2): 155-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirshman</author>
<author>K Puder</author>
</authors>
<title>Restriction Grammar in Prolog.</title>
<date>1982</date>
<booktitle>Proc. First International Logic Programming Conference.</booktitle>
<pages>85--90</pages>
<location>Marseille, France:</location>
<contexts>
<context position="49699" citStr="Hirshman and Puder (1982)" startWordPosition="8009" endWordPosition="8012">oping still another logic grammar formalism, we believe our notion of modifier structure grammar to be particularly attractive for allowing the user to write grammars in a more straightforward manner and more clearly. Also, because the semantic information about the structure being built up is described modularly in the grammar rules, it becomes easier to adapt the parser to alternative domains of application: modifying the logical representation obtained need only involve the semantic items in each rule. A related but less flexible idea was independently developed for Restriction Grammars by Hirshman and Puder (1982). RGs are also logic grammars in the sense that they are based on Prolog, but they deal only with context-free definitions augmented by restrictions (which are procedures attached to the rules). In RGs, a tree record of the context-free rules applied is automatically generated during the parse. More evolved representations for the sentence, however, are again the user&apos;s responsibility and require processing this automatically generated parse tree. Another important point, in our view, is the fact that our system does not preclude context-sensitive rules, transformations, or gaps. This is contr</context>
</contexts>
<marker>Hirshman, Puder, 1982</marker>
<rawString>Hirshman, L. and Puder, K. 1982 Restriction Grammar in Prolog. Proc. First International Logic Programming Conference. Marseille, France: 85-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>L S Levy</author>
</authors>
<title>Phrase Structure Trees Bear More Fruit than You Would Have Thought.</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>8</volume>
<pages>1--11</pages>
<contexts>
<context position="50501" citStr="Joshi and Levy 1982" startWordPosition="8131" endWordPosition="8134">the rules). In RGs, a tree record of the context-free rules applied is automatically generated during the parse. More evolved representations for the sentence, however, are again the user&apos;s responsibility and require processing this automatically generated parse tree. Another important point, in our view, is the fact that our system does not preclude context-sensitive rules, transformations, or gaps. This is contrary to what seems to be the general tendency today, both in theoretical linguistics (for example, Gazdar 1981) and in computational linguistics (for example, Hirshman and Puder 1982, Joshi and Levy 1982, Robinson American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 79 Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars 1982, Schubert and Pelletier 1982), towards using context-free grammars (which, however, are often augmented in some way — through restrictions, local constraints, rule schemata, metarules, etc. — compensating for the lack of expressiveness in simple contextfree grammars). This approach was largely motivated by the need to provide alternatives to transformational grammar, which on the one hand was felt by Al researchers to </context>
</contexts>
<marker>Joshi, Levy, 1982</marker>
<rawString>Joshi, A. and Levy, L.S. 1982 Phrase Structure Trees Bear More Fruit than You Would Have Thought. American Journal of Computational Linguistics 8: 1-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Kowalski</author>
</authors>
<title>Predicate Logic as a Programming Language.</title>
<date>1974</date>
<booktitle>Proc. IFIP 74.</booktitle>
<pages>569--574</pages>
<publisher>North-Holland,</publisher>
<location>Amsterdam, The Netherlands:</location>
<contexts>
<context position="1389" citStr="Kowalski 1974" startWordPosition="195" endWordPosition="196">and general way. The system produces both syntactic analyses and logical forms, and problems of scoping for coordination and quantifiers are dealt with. The MSG formalism seems of interest in its own right (perhaps even outside natural language processing) because the notions of syntactic structure and semantic interpretation are more constrained than in many previous systems (made more implicit in the formalism itself), so that less burden is put on the grammar writer. 1. Introduction Since the development of the Prolog programming language (Colmerauer 1973; Roussel 1975), logic programming (Kowalski 1974, 1979; Van Emden 1975) has been applied in many different fields. In natural language processing, useful grammar formalisms have been developed and incorporated in Prolog: metamorphosis grammars, due to Colmerauer (1978), and extraposition grammars, defined by F. Pereira (1981); definite clause grammars (Pereira and Warren 1980) are a special case of metamorphosis grammars. The first sizable application of logic grammars was a Spanish/French-consultable data base system by Dahl (1977, 1981), which was later adapted to Portuguese Visiting in the Computer Science Department, University of Kentu</context>
</contexts>
<marker>Kowalski, 1974</marker>
<rawString>Kowalski, R.A. 1974 Predicate Logic as a Programming Language. Proc. IFIP 74. North-Holland, Amsterdam, The Netherlands: 569-574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Kowalski</author>
</authors>
<title>Logic for Problem Solving.</title>
<date>1979</date>
<publisher>North-Holland,</publisher>
<location>New York, New York.</location>
<marker>Kowalski, 1979</marker>
<rawString>Kowalski, R.A. 1979 Logic for Problem Solving. North-Holland, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>Using Slots and Modifiers in Logic Grammars for Natural Language.</title>
<date>1982</date>
<journal>Artificial Intelligence</journal>
<volume>18</volume>
<pages>327--367</pages>
<contexts>
<context position="2503" citStr="McCord (1982" startWordPosition="364" endWordPosition="365">h was later adapted to Portuguese Visiting in the Computer Science Department, University of Kentucky, during part of this research. Work partially supported by Canadian NSERC Grant A2436 and Simon Fraser P.R. Grant 42406. 2 Current address: IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598. by L. Pereira and H. Coelho and to English by F. Pereira and D. Warren. Coelho (1979) developed a consulting system in Portuguese for library service, and F. Pereira and D. Warren (1980) developed a sizable English data base query system with facilities for query optimization. McCord (1982, 1981) presented ideas for syntactic analysis and semantic interpretation in logic grammars, with application to English grammar; some of these ideas are used in our work described here. Coordination (grammatical construction with the conjunctions &apos;and&apos;, &apos;or&apos;, &apos;but&apos;) has long been one of the most difficult natural language phenomena to handle, because it can involve such a wide range of grammatical constituents (or non-constituent fragments), and ellipsis (or reduction) can occur in the items conjoined. In most grammatical frameworks, the grammar writer desiring to handle coordination can get</context>
<context position="12357" citStr="McCord (1982" startWordPosition="1940" endWordPosition="1941"> Appendix. One reason for the efficiency is just that the system is formulated as a logic programming system, and especially that it uses Prolog-10, with its compiler. Another reason presumably lies in the details of the MSG interpreter. For example, the interpreter does not save the complete history of the parse, so that the backing up necessary for coordination does not examine as much. (5) The code for the system seems short, and most of it is listed in this paper. The semantic interpretation component is described in Section 4, but not in complete detail since it is taken in the main from McCord (1982, 1981). Emphasis is on the new aspects involving semantic interpretation of coordinate modifiers. Semantic interpretation of a modifier structure tree is done in two stages. The first stage, called reshaping, deals heuristically with the well-known scoping problem, which arises because of the discrepancies that can exist between (surface) syntactic relations and intended semantic relations. Reshaping is a transformation of the syntactic MS tree into another MS tree with the (hopefully) correct modifier relations. The second stage takes the reshaped tree and translates it into logical form. Th</context>
<context position="35589" citStr="McCord (1982" startWordPosition="5790" endWordPosition="5791">true compsgobj-YD 8-true comp(obj-Y) 8-true nounph(Y,def) 19-true det(Y,def) T/U-def(Y,U,T) noun(Y,[ ]) 8-train(Y) Figure 3. MS tree for &amp;quot;John saw and Mary heard the train.&amp;quot; 4. Semantic Interpretation and Coordination The overall idea of the semantic interpretation component was given in the Introduction. The rule system is listed completely in the Appendix. This system is taken essentially from McCord (1981), with some rules deleted (rules dealing with focus), and some rules added for coordination. For a discussion of MS tree reshaping as a means of handling scoping of modifiers, we refer to McCord (1982, 1981). Also, the reader may examine the examples of reshaped trees given in the Appendix. We will, however, review the second stage of semantic interpretation, because the new rules for coordination are added here and because it is more central for understanding modifier structure. In this stage, the reshaped MS tree is translated to logical form, and the top-level procedure for this is &apos;translate&apos;. This procedure actually works only with the semantic-item components of MS tree nodes. (Reshaping uses the first, syntactic components.) One semantic item can combine with (or modify) a second se</context>
</contexts>
<marker>McCord, 1982</marker>
<rawString>McCord, M.C. 1982 Using Slots and Modifiers in Logic Grammars for Natural Language. Artificial Intelligence 18: 327-367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>Focalizers, the Scoping Problem, and Semantic Interpretation Rules in Logic Grammars.</title>
<date>1981</date>
<tech>Technical Report,</tech>
<institution>University of Kentucky.</institution>
<note>To appear in</note>
<contexts>
<context position="17382" citStr="McCord 1981" startWordPosition="2739" endWordPosition="2740"> with an argument representing a syntactic structure. (Here, following Prolog-10 syntax, the capitalized items are variables.) Manipulating such arguments is the only way of getting analysis structures in XGs. As indicated in the Introduction, a new ingredient in MSGs over XGs is to automate this process, or to make it implicit in the grammar. MSG rules are of two forms. The basic form is A:Sem --&gt; B. where A--&gt;B is an XG rule and Sem is a term called a semantic item, which plays a role in the semantic interpretation of a phrase analyzed by application of the rule. The semantic item is (as in McCord 1981) of the form Operator—LogicalForm where, roughly, LogicalForm is the part of the logical form of the sentence contributed by the rule, and Operator determines the way this partial structure combines with others. Details on semantic items are postponed to Section 4 (on semantic interpretation). Actually, the current section and Section 3 deal mainly with syntactic constructions which are independent of the form of semantic items. The second type of MSG rule looks exactly like an XG rule (no Sem is exhibited), but the system takes care of inserting a special &amp;quot;trivial&amp;quot; Sem, 8-true. (Here the &apos;8&apos; </context>
<context position="35389" citStr="McCord (1981)" startWordPosition="5756" endWordPosition="5757">r examples listed in the Appendix. sent 8-true nounph(X,def) @P-def(X,X=john,P) verbph(X) 8-true verb(X,[obj-Y]) 8-saw(X,Y) conj(and) Q*R-(Q&amp;R) sent 8-true nounph(Z,def) @S-def(Z,Z=mary,S) verbph(Z) 8-true compsgobj-YD 8-true comp(obj-Y) 8-true nounph(Y,def) 19-true det(Y,def) T/U-def(Y,U,T) noun(Y,[ ]) 8-train(Y) Figure 3. MS tree for &amp;quot;John saw and Mary heard the train.&amp;quot; 4. Semantic Interpretation and Coordination The overall idea of the semantic interpretation component was given in the Introduction. The rule system is listed completely in the Appendix. This system is taken essentially from McCord (1981), with some rules deleted (rules dealing with focus), and some rules added for coordination. For a discussion of MS tree reshaping as a means of handling scoping of modifiers, we refer to McCord (1982, 1981). Also, the reader may examine the examples of reshaped trees given in the Appendix. We will, however, review the second stage of semantic interpretation, because the new rules for coordination are added here and because it is more central for understanding modifier structure. In this stage, the reshaped MS tree is translated to logical form, and the top-level procedure for this is &apos;transla</context>
</contexts>
<marker>McCord, 1981</marker>
<rawString>McCord, M.C. 1981 Focalizers, the Scoping Problem, and Semantic Interpretation Rules in Logic Grammars. Technical Report, University of Kentucky. To appear in Warren, D. and van Caneghem, M., Eds., Logic Programming and its Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Extraposition Grammars.</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>7</volume>
<pages>243--256</pages>
<contexts>
<context position="1668" citStr="Pereira (1981)" startWordPosition="235" endWordPosition="236">f syntactic structure and semantic interpretation are more constrained than in many previous systems (made more implicit in the formalism itself), so that less burden is put on the grammar writer. 1. Introduction Since the development of the Prolog programming language (Colmerauer 1973; Roussel 1975), logic programming (Kowalski 1974, 1979; Van Emden 1975) has been applied in many different fields. In natural language processing, useful grammar formalisms have been developed and incorporated in Prolog: metamorphosis grammars, due to Colmerauer (1978), and extraposition grammars, defined by F. Pereira (1981); definite clause grammars (Pereira and Warren 1980) are a special case of metamorphosis grammars. The first sizable application of logic grammars was a Spanish/French-consultable data base system by Dahl (1977, 1981), which was later adapted to Portuguese Visiting in the Computer Science Department, University of Kentucky, during part of this research. Work partially supported by Canadian NSERC Grant A2436 and Simon Fraser P.R. Grant 42406. 2 Current address: IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598. by L. Pereira and H. Coelho and to English by F. Pereir</context>
<context position="15134" citStr="Pereira (1981)" startWordPosition="2365" endWordPosition="2366"> grammar symbols can be connected by the infix operator indicating a gap. When such a rule is used in rewriting, the gaps appearing in the lefthand side may match arbitrary strings of grammar symbols, and then the left-hand side is replaced by the right-hand side followed by the symbol strings matched by the gaps (in the same order). For example, the XG rule a,b...c...d --&gt; e,f is really a rule schema a,b,X,c,Y,d --&gt; e,f,X,Y where X and Y stand for arbitrary grammar symbol strings. There is a constraint on the use of gaps in rewriting called the bracketing constraint, for which we refer to F. Pereira (1981). However, our MSG interpreter includes XG interpretation, so the use of gaps is in fact completely specified below. In XG rules, symbols on the left-hand side following gaps represent left-extraposed elements. For example, the extraposition of noun phrases to the front of relative clauses (with replacement by relative pronouns) can be handled by the XG rules: relative clause --&gt; rel marker, sentence. rel marker...trace --&gt; rel pronoun. noun phrase --&gt; trace. where &apos;trace&apos; marks the position out of which the noun phrase is being moved, and is used by the second rule above in conjunction with a</context>
</contexts>
<marker>Pereira, 1981</marker>
<rawString>Pereira, F. 1981 Extraposition Grammars. American Journal of Computational Linguistics 7: 243-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D Warren</author>
</authors>
<title>Definite Clause Grammars for Language Analysis — a Survey of the Formalism and a Comparison with Transition Networks.</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="1720" citStr="Pereira and Warren 1980" startWordPosition="240" endWordPosition="243">etation are more constrained than in many previous systems (made more implicit in the formalism itself), so that less burden is put on the grammar writer. 1. Introduction Since the development of the Prolog programming language (Colmerauer 1973; Roussel 1975), logic programming (Kowalski 1974, 1979; Van Emden 1975) has been applied in many different fields. In natural language processing, useful grammar formalisms have been developed and incorporated in Prolog: metamorphosis grammars, due to Colmerauer (1978), and extraposition grammars, defined by F. Pereira (1981); definite clause grammars (Pereira and Warren 1980) are a special case of metamorphosis grammars. The first sizable application of logic grammars was a Spanish/French-consultable data base system by Dahl (1977, 1981), which was later adapted to Portuguese Visiting in the Computer Science Department, University of Kentucky, during part of this research. Work partially supported by Canadian NSERC Grant A2436 and Simon Fraser P.R. Grant 42406. 2 Current address: IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598. by L. Pereira and H. Coelho and to English by F. Pereira and D. Warren. Coelho (1979) developed a consultin</context>
<context position="51878" citStr="Pereira and Warren (1980)" startWordPosition="8335" endWordPosition="8338">ate explanations for important constructs, such as coordination and unbounded dependencies. Further arguments supporting this approach include claims of more efficient parsability, simplicity, and modularity. From the particular point of view of logic grammars, more evolved grammar formalisms make a great deal of sense for various reasons. In the first place, they provide various advantages that have been illustrated in Dahl (1981), namely modularity and conciseness, clarity and efficiency. A detailed discussion of these advantages with respect to augmented transition networks can be found in Pereira and Warren (1980). Furthermore, they include lower-level grammars as a special case. In particular, context-free rules augmented with procedures may be written, since even the simplest logic grammar defined to date (DCGs) allows Prolog calls to be interspersed with the rules. The greater expressive power allowed by more evolved formalisms, then, can only represent a gain, since it does not preclude more elementary approaches. Logic grammars, in short, seem to be developing — like other computer formalisms — into higher-level tools that allow the user to avoid mechanizable effort in order to concentrate on as y</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, F. and Warren, D. 1980 Definite Clause Grammars for Language Analysis — a Survey of the Formalism and a Comparison with Transition Networks. Artificial Intelligence 13: 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D Warren</author>
</authors>
<title>An Efficient Easily Adaptable System for Interpreting Natural Language Queries.</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>8</volume>
<pages>110--122</pages>
<marker>Pereira, Warren, 1982</marker>
<rawString>Pereira, F. and Warren, D. 1982 An Efficient Easily Adaptable System for Interpreting Natural Language Queries. American Journal of Computational Linguistics 8: 110-122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M Pereira</author>
</authors>
<title>ORBI — An Expert System for Environmental Resource Evaluation through Natural Language. Universidade Nova de Lisboa.</title>
<date>1982</date>
<marker>Pereira, 1982</marker>
<rawString>Pereira, L.M. et al. 1982 ORBI — An Expert System for Environmental Resource Evaluation through Natural Language. Universidade Nova de Lisboa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pereira</author>
<author>F Pereira</author>
<author>D Warren</author>
</authors>
<date>1978</date>
<booktitle>User&apos;s Guide to DEC System-I0 Prolog.</booktitle>
<institution>Department of Artificial Intelligence, University of Edinburgh.</institution>
<marker>Pereira, Pereira, Warren, 1978</marker>
<rawString>Pereira, L.; Pereira, F.; and Warren, D. 1978 User&apos;s Guide to DEC System-I0 Prolog. Department of Artificial Intelligence, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robinson</author>
</authors>
<title>Diagram: a Grammar for Dialogues.</title>
<date>1982</date>
<journal>Comm. ACM</journal>
<volume>25</volume>
<pages>27--47</pages>
<marker>Robinson, 1982</marker>
<rawString>Robinson, J. 1982 Diagram: a Grammar for Dialogues. Comm. ACM 25: 27-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Robinson</author>
</authors>
<title>A Machine-Oriented Logic Based on the Resolution Principle.</title>
<date>1965</date>
<journal>J. ACM</journal>
<volume>12</volume>
<pages>23--41</pages>
<contexts>
<context position="14322" citStr="Robinson 1965" startWordPosition="2228" endWordPosition="2229">es. The reader may wish to examine the sample parses at this point. American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 71 Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars 2. Modifier Structure Grammars The most fundamental type of logic grammar is Colmerauer&apos;s (1978) metamorphosis grammar (MG). Grammars of this type can be viewed as generalized type-0 phrase structure grammars in which the grammar symbols (terminals and non-terminals) are terms from predicate logic. In derivations, the rewriting of symbol strings involves unification (Robinson 1965), instead of simple replacement. F. Pereira&apos;s (1981) extraposition grammars (XGs) are essentially generalizations of MGs designed to handle (left) extraposition. In the left-hand side of an XG rule, grammar symbols can be connected by the infix operator indicating a gap. When such a rule is used in rewriting, the gaps appearing in the lefthand side may match arbitrary strings of grammar symbols, and then the left-hand side is replaced by the right-hand side followed by the symbol strings matched by the gaps (in the same order). For example, the XG rule a,b...c...d --&gt; e,f is really a rule sche</context>
</contexts>
<marker>Robinson, 1965</marker>
<rawString>Robinson, J.A. 1965 A Machine-Oriented Logic Based on the Resolution Principle. J. ACM 12: 23-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P L Roussel</author>
</authors>
<title>Prolog Manuel de Reference et d&apos;Utilisation. Universite d&apos;Aix-Marseille.</title>
<date>1975</date>
<contexts>
<context position="1355" citStr="Roussel 1975" startWordPosition="191" endWordPosition="192">ge constructions) in a reasonable and general way. The system produces both syntactic analyses and logical forms, and problems of scoping for coordination and quantifiers are dealt with. The MSG formalism seems of interest in its own right (perhaps even outside natural language processing) because the notions of syntactic structure and semantic interpretation are more constrained than in many previous systems (made more implicit in the formalism itself), so that less burden is put on the grammar writer. 1. Introduction Since the development of the Prolog programming language (Colmerauer 1973; Roussel 1975), logic programming (Kowalski 1974, 1979; Van Emden 1975) has been applied in many different fields. In natural language processing, useful grammar formalisms have been developed and incorporated in Prolog: metamorphosis grammars, due to Colmerauer (1978), and extraposition grammars, defined by F. Pereira (1981); definite clause grammars (Pereira and Warren 1980) are a special case of metamorphosis grammars. The first sizable application of logic grammars was a Spanish/French-consultable data base system by Dahl (1977, 1981), which was later adapted to Portuguese Visiting in the Computer Scien</context>
</contexts>
<marker>Roussel, 1975</marker>
<rawString>Roussel, P.L. 1975 Prolog Manuel de Reference et d&apos;Utilisation. Universite d&apos;Aix-Marseille.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Schubert</author>
<author>F Pelletier</author>
</authors>
<title>From English to Logic: Context-Free Computation of &apos;Conventional&apos; Logical Translation.</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>8</volume>
<issue>1</issue>
<pages>27--44</pages>
<contexts>
<context position="50708" citStr="Schubert and Pelletier 1982" startWordPosition="8160" endWordPosition="8163">bility and require processing this automatically generated parse tree. Another important point, in our view, is the fact that our system does not preclude context-sensitive rules, transformations, or gaps. This is contrary to what seems to be the general tendency today, both in theoretical linguistics (for example, Gazdar 1981) and in computational linguistics (for example, Hirshman and Puder 1982, Joshi and Levy 1982, Robinson American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 79 Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars 1982, Schubert and Pelletier 1982), towards using context-free grammars (which, however, are often augmented in some way — through restrictions, local constraints, rule schemata, metarules, etc. — compensating for the lack of expressiveness in simple contextfree grammars). This approach was largely motivated by the need to provide alternatives to transformational grammar, which on the one hand was felt by Al researchers to deal insufficiently with semantics and with sentence analysis, and on the other hand, as observed by Gazdar (1981), could not offer linguistically adequate explanations for important constructs, such as coor</context>
</contexts>
<marker>Schubert, Pelletier, 1982</marker>
<rawString>Schubert, L. and Pelletier, F. 1982 From English to Logic: Context-Free Computation of &apos;Conventional&apos; Logical Translation. American Journal of Computational Linguistics 8(1): 27-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Van Emden</author>
</authors>
<title>Programming with Resolution Logic.</title>
<date>1975</date>
<journal>Machine Intelligence,</journal>
<volume>8</volume>
<publisher>John Wiley,</publisher>
<location>New York, New York.</location>
<marker>Van Emden, 1975</marker>
<rawString>Van Emden, M.H. 1975 Programming with Resolution Logic. Machine Intelligence, 8. John Wiley, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="9750" citStr="Winograd 1972" startWordPosition="1523" endWordPosition="1524">ahl and Michael C. McCord Treating Coordination in Logic Grammars (1982, 1981), especially the latter paper. But new elements are the notion of MSG (making modifier structure implicit in the grammar), the MSG interpreter, with its treatment of coordination, and the specific rules for semantic interpretation of coordination. The MSG interpreter is described in Section 3. As indicated above, the interpreter completely handles the syntax of coordination. The MSG grammar itself should not mention conjunctions at all. The interpreter has a general facility for treating certain words as demons (cf. Winograd 1972), and conjunctions are handled in this way. When a conjunction demon appears in a sentence A X conj Y B, a process is set off which in outline is like SYSCONJ, in that backing up is done in the parse history in order to parse Y parallel to X, and B is parsed by merger with the state interrupted by the conjunction. However, our system has the following interesting features: (1) The MSG interpreter manipulates stacks in such a way that embedded coordination (and coordination of more than two elements) and interactions with extraposition are handled. (Examples are given in the Appendix.) (2) The </context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Winograd, T. 1972. Understanding Natural Language. Academic Press, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>An Experimental Parsing System for Transition Network Grammars. In</title>
<date>1973</date>
<pages>145--149</pages>
<publisher>Algorithmics Press,</publisher>
<location>New York, New York:</location>
<contexts>
<context position="4215" citStr="Woods 1973" startWordPosition="617" endWordPosition="618">ires a fee and/or specific permission. 0362-613X/83/020069-23$03.00 American Journal of Computational Linguistics, Volume 9, Number 2, April-June 1983 69 Veronica Dahl and Michael C. McCord Treating Coordination in Logic Grammars tion as a &amp;quot;metagrammatical&amp;quot; construction, in the sense that metarules, general system operations, or &amp;quot;secondpass&amp;quot; operations such as transformations, are needed for its formulation. Perhaps the most general and powerful metagrammatical device for handling coordination in computational linguistics has been the SYSCONJ facility for augmented transition networks (ATNs) (Woods 1973; Bates 1978). The ATN interpreter with this facility built into it can take an ATN that does not itself mention conjunctions at all, and will parse reduced coordinate constructions, which are of the form A X and Y B, for example, John drove his car through and A X completely demolished a plate glass window. where the unreduced deep structure corresponds to AXB and A Y B. The result of the parse is this unreduced structure. SYSCONJ accomplishes this by treating the conjunction as an interruption which causes the parser to back up in its history of the parse. Before backing up, the current conf</context>
<context position="16115" citStr="Woods 1973" startWordPosition="2530" endWordPosition="2531">e clause --&gt; rel marker, sentence. rel marker...trace --&gt; rel pronoun. noun phrase --&gt; trace. where &apos;trace&apos; marks the position out of which the noun phrase is being moved, and is used by the second rule above in conjunction with a relative marker to produce (or analyze) a relative pronoun. Pereira&apos;s implementation of XGs is a Prolog program that compiles XGs to Horn clause systems, which in turn can be run by Prolog for parsing sentences. In the compiled systems, extraposition is handled by the manipulation of a stack called the extraposition list, which is similar to the HOLD list for ATN&apos;s (Woods 1973). Elements (like &apos;trace&apos; above) on the left-hand sides of XG rules following the initial symbol are in effect put on the extraposition list during parsing, and can be taken off when they are required later by the right-hand side of another rule. Our MSG interpreter uses a reformulation of this same method. Since the grammar symbols in XGs (and MGs) can be arbitrary terms from predicate logic, they can contain arguments. These arguments can be used to hold useful information such as selectional restrictions and analysis structures. For example, in the rule sentence(s(Subj,Pred)) --&gt; noun_phrase</context>
</contexts>
<marker>Woods, 1973</marker>
<rawString>Woods, W.A. 1973 An Experimental Parsing System for Transition Network Grammars. In Rustin, R., Ed., Natural Language Processing. Algorithmics Press, New York, New York: 145-149.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>