<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.965796">
Learning General Connotation of Words using Graph-based Algorithms
</title>
<author confidence="0.996347">
Song Feng Ritwik Bose Yejin Choi
</author>
<affiliation confidence="0.964062">
Department of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.914913">
NY 11794, USA
</address>
<email confidence="0.968486">
songfeng, rbose, ychoi@cs.stonybrook.edu
</email>
<sectionHeader confidence="0.994801" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990436">
In this paper, we introduce a connotation lex-
icon, a new type of lexicon that lists words
with connotative polarity, i.e., words with pos-
itive connotation (e.g., award, promotion) and
words with negative connotation (e.g., cancer,
war). Connotation lexicons differ from much
studied sentiment lexicons: the latter concerns
words that express sentiment, while the former
concerns words that evoke or associate with
a specific polarity of sentiment. Understand-
ing the connotation of words would seem to
require common sense and world knowledge.
However, we demonstrate that much of the
connotative polarity of words can be inferred
from natural language text in a nearly unsu-
pervised manner. The key linguistic insight
behind our approach is selectional preference
of connotative predicates. We present graph-
based algorithms using PageRank and HITS
that collectively learn connotation lexicon to-
gether with connotative predicates. Our em-
pirical study demonstrates that the resulting
connotation lexicon is of great value for sen-
timent analysis complementing existing senti-
ment lexicons.
</bodyText>
<sectionHeader confidence="0.998129" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999674">
In this paper, we introduce a connotation lexicon,
a new type of lexicon that lists words with conno-
tative polarity, i.e., words with positive connotation
(e.g., award, promotion) and words with negative
connotation (e.g., cancer, war). Connotation lexi-
cons differ from sentiment lexicons that are studied
in much of previous research (e.g., Esuli and Sebas-
tiani (2006), Wilson et al. (2005a)): the latter con-
cerns words that express sentiment either explicitly
or implicitly, while the former concerns words that
evoke or even simply associate with a specific polar-
ity of sentiment. To our knowledge, there has been
no previous research that investigates polarized con-
notation lexicons.
Understanding the connotation of words would
seem to require common sense and world knowl-
edge at first glance, which in turn might seem to re-
quire human encoding of knowledge base. However,
we demonstrate that much of the connotative polar-
ity of words can be inferred from natural language
text in a nearly unsupervised manner.
The key linguistic insight behind our approach is
selectional preference of connotative predicates. We
define a connotative predicate as a predicate that
has selectional preference on the connotative polar-
ity of some of its semantic arguments. For instance,
in the case of the connotative predicate “prevent”,
there is strong selectional preference on negative
connotation with respect to the thematic role (se-
mantic role) “THEME”. That is, statistically speak-
ing, people tend to associate negative connotation
with the THEME of “prevent”, e.g., “prevent can-
cer” or “prevent war”, rather than positive conno-
tation, e.g., “prevent promotion”. In other words,
even though it is perfectly valid to use words with
positive connotation in the THEME role of “pre-
vent”, statistically more dominant connotative po-
larity is negative. Similarly, the THEME of “con-
gratulate” or “praise” has strong selectional prefer-
ence on positive connotation.
The theoretical concept supporting the selective
</bodyText>
<page confidence="0.36955">
1092
</page>
<note confidence="0.993537">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1092–1103,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<keyword confidence="0.62230125">
accomplish, achieve, advance, advocate, admire,
applaud, appreciate, compliment, congratulate,
develop, desire, enhance, enjoy, improve, praise,
promote, respect, save, support, win
</keyword>
<tableCaption confidence="0.994481">
Table 1: Positively Connotative Predicates w.r.t. THEME
</tableCaption>
<bodyText confidence="0.811891">
alleviate, accuse, avert, avoid, cause, complain,
condemn, criticize, detect, eliminate, eradicate,
mitigate, overcome, prevent, prohibit, protest, re-
frain, suffer, tolerate, withstand
</bodyText>
<tableCaption confidence="0.954916">
Table 2: Negatively Connotative Predicates w.r.t. THEME
</tableCaption>
<bodyText confidence="0.999903666666667">
preference of connotative predicates is that of se-
mantic prosody in corpus linguistics. Semantic
prosody describes how some of the seemingly neu-
tral words (e.g., “cause”) can be perceived with pos-
itive or negative polarity because they tend to col-
locate with words with corresponding polarity (e.g.,
Sinclair (1991), Louw et al. (1993), Stubbs (1995),
Stefanowitsch and Gries (2003)). In this work, we
demonstrate that statistical approaches that exploit
this very concept of semantic prosody can success-
fully infer connotative polarity of words.
Having described the key linguistic insight, we
now illustrate our graph-based algorithms. Figure 1
depicts the mutually reinforcing relation between
connotative predicates (nodes on the left-hand side)
and words with connotative polarity (node on the
right-hand side). The thickness of edges represents
the strength of the association between predicates
and arguments. For brevity, we only consider conno-
tation of words that appear in the THEME thematic
role.
We expect that words that appear often in the
THEME role of various positively (or negatively)
connotative predicates are likely to be words with
positive (or negative) connotation. Likewise, pred-
icates whose THEME contains words with mostly
positive (or negative) connotation are likely to be
positively (or negatively) connotative predicates. In
short, we can induce the connotative polarity of
words using connotative predicates, and inversely,
we can learn new connotative predicates based on
words with connotative polarity.
We hypothesize that this mutually reinforcing re-
</bodyText>
<figureCaption confidence="0.997288666666667">
Figure 1: Bipartite graph of connotative predicates and
arguments. Edge weights are proportionate to the associ-
ation strength.
</figureCaption>
<bodyText confidence="0.999961944444445">
lation between connotative predicates and their ar-
guments can be captured via graph centrality in
graph-based algorithms. Given a small set of seed
words for connotative predicates, our algorithms
collectively learn connotation lexicon together with
connotative predicates in a nearly unsupervised
manner. A number of different graph representa-
tions are explored using both PageRank (Page et al.,
1999) and HITS (Kleinberg, 1999) algorithms. Em-
pirical study demonstrates that our graph based al-
gorithms are highly effective in learning both con-
notation lexicon and connotative predicates.
Finally, we quantify the practical value of our
connotation lexicon in concrete sentiment analysis
applications, and demonstrate that the connotation
lexicon is of great value for sentiment classification
tasks complementing conventional sentiment lexi-
cons.
</bodyText>
<sectionHeader confidence="0.82287" genericHeader="introduction">
2 Connotation Lexicon &amp; Connotative
Predicate
</sectionHeader>
<bodyText confidence="0.999908">
In this section, we define connotation lexicon and
connotative predicates more formally, and contrast
them against words in conventional sentiment lexi-
cons.
</bodyText>
<subsectionHeader confidence="0.997818">
2.1 Connotation Lexicon
</subsectionHeader>
<bodyText confidence="0.974730875">
This lexicon lists words with positive and negative
connotation, as defined below.
• Words with positive connotation: In this
work, we define words with positive connota-
tion as those that describe physical objects or
abstract concepts that people generally value,
cherish or care about. For instance, we regard
words such as “freedom”, “life”, or “health” as
</bodyText>
<figure confidence="0.972954222222222">
Overcome
Alleviate
Prevent
Avoid
Promotion
Tragedy
Incident
Cancer
1093
</figure>
<bodyText confidence="0.991698">
words with positive connotation. Some of these
words may express subjectivity either explic-
itly or implicitly, e.g., “joy” or “satisfaction”.
However, a substantial number of words with
positive connotation are purely objective, such
as “life”, “health”, “tenure”, or “scientific”.
</bodyText>
<listItem confidence="0.923604125">
• Words with negative connotation: We define
words with negative connotation as those that
describe physical objects or abstract concepts
that people generally disvalue or avoid. Sim-
ilarly as before, some of these words may ex-
press subjectivity (e.g., “disappointment”, “hu-
miliation”), while many other are purely objec-
tive (e.g., “bedbug”, “arthritis, “funeral”).
</listItem>
<bodyText confidence="0.999762571428571">
Note that this explicit and intentional inclusion of
objective terms makes connotation lexicons differ
from sentiment lexicons: most conventional senti-
ment lexicons have focused on subjective words by
definition (e.g., Wilson et al. (2005b)), as many re-
searchers use the term sentiment and subjectivity in-
terchangeably (e.g., Wiebe et al. (2005)).
</bodyText>
<subsectionHeader confidence="0.998813">
2.2 Connotative Predicate
</subsectionHeader>
<bodyText confidence="0.999734">
In this work, connotative predicates are those that
exhibit selectional preference on the connotative po-
larity of some of their arguments. We emphasize that
the polarity of connotative predicates does not coin-
cide with the polarity of sentiment in conventional
sentiment lexicons, as will be elaborated below.
</bodyText>
<listItem confidence="0.817214352941177">
• Positively connotative predicate: In this
work, we define positively connotative predi-
cates as those that expect positive connotation
in some arguments. For example, “congratu-
late” or “save” are positively connotative pred-
icates that expect words with positive conno-
tation in the THEME argument: people typi-
cally congratulate something positive, and save
something people care about. More examples
are shown in Table 1.
• Negatively connotative predicate: In this
work, we define negatively connotative predi-
cates as those that expect negative connotation
in some arguments. For instance, predicates
such as “prevent” or “suffer” tend to project
negative connotation in the THEME argument.
More examples are shown in Table 2.
</listItem>
<bodyText confidence="0.999931">
Note that positively connotative predicates are not
necessarily positive sentiment words. For instance
“save” is not a positive sentiment word in the
lexicon published by Wilson et al. (2005b). In-
versely, (strongly) positive sentiment words are not
necessarily (strongly) positively connotative predi-
cates, e.g., “illuminate”, “agree”. Likewise, neg-
atively connotative predicates are not necessarily
negative sentiment words. For instance, predicates
such as “prevent”, “detect”, or “cause” are not
negative sentiment words, but they tend to corre-
late with negative connotation in the THEME argu-
ment. Inversely, (strongly) negative sentiment words
are not necessarily (strongly) negatively connotative
predicates, e.g., “abandon” (“abandoned [something
valuable]”).
</bodyText>
<sectionHeader confidence="0.982261" genericHeader="method">
3 Graph Representation
</sectionHeader>
<bodyText confidence="0.999525551724138">
In this section, we explore the graphical representa-
tion of our task. Figure 1 depicts the key intuition as
a bipartite graph, where the nodes on the left-hand
side correspond to connotative predicates, and the
nodes on the right-hand side correspond to words in
the THEME argument. There is an edge between a
predicate p and an argument a, if the argument a
appears in the THEME role of the predicate p. For
brevity, we explore only verbs as the predicates, and
words in the THEME role of the predicates as argu-
ments. Our work can be readily extended to exploit
other predicate-argument relations however.
Note that there are many sources of noise in the
construction of graph. For instance, some of the
predicates might be negated, changing the semantic
dynamics between the predicate and the argument.
In addition, there might be many unusual combina-
tions of predicates and arguments, either due to data
processing errors or due to idiosyncratic use of lan-
guage. Some of such combinations can be valid ones
(e.g., “prevent promotion”), challenging the learning
algorithm with confusing evidence.
We hypothesize that by focusing on the important
part of the graph via centrality analysis, it is possible
to infer connotative polarity of words despite various
noise introduced in the graph structure. This implies
that it is important to construct the graph structure so
as to capture important linguistic relations between
predicates and arguments. With this goal in mind,
</bodyText>
<page confidence="0.686878">
1094
</page>
<bodyText confidence="0.9965335">
we next explore the directionality of the edges and
different strategies to assign weights to them.
</bodyText>
<subsectionHeader confidence="0.997724">
3.1 Undirected (Symmetric) Graph
</subsectionHeader>
<bodyText confidence="0.999985666666667">
First we explore undirected edges. In this case,
we assign weight for each undirected edge between
a predicate p and an argument a. Intuitively, the
weight should correspond to the strength of relat-
edness or association between the predicate p and
the argument a. We use Pointwise Mutual Infor-
mation (PMI), as it has been used by many pre-
vious research to quantify the association between
two words (e.g., Turney (2001), Church and Hanks
(1990)). The PMI score between p and a is defined
as follows:
The log of the ratio is positive when the pair of
words tends to co-occur and negative when the pres-
ence of one word correlates with the absence of the
other word.
</bodyText>
<subsectionHeader confidence="0.997138">
3.2 Directed (Asymmetric) Graph
</subsectionHeader>
<bodyText confidence="0.9998835">
Next we explore directed edges. That is, for each
connected pair of a predicate p and an argument a,
there are two edges in opposite directions: e(p → a)
and e(a → p). In this case, we explore the use
of asymmetric weights using conditional probabil-
ity. In particular, we define weights as follows:
</bodyText>
<equation confidence="0.9995645">
P(p)
P(p, a)
w(a → p) := P(p|a) =
P(a)
</equation>
<bodyText confidence="0.99781925">
Having defined the graph structure, next we explore
algorithms that analyze graph centrality via random
walks. In particular, we investigate the use of HITS
algorithm (Section 4), and PageRank (Section 5).
</bodyText>
<sectionHeader confidence="0.978143" genericHeader="method">
4 Lexicon Induction using HITS
</sectionHeader>
<bodyText confidence="0.999980875">
The graph representation described thus far (Sec-
tion 3) captures general semantic relations between
predicates and arguments, rather than those specific
to connotative predicates and arguments. Therefore
in this section, we explore techniques to augment
the graph representation so as to bias the centrality
of the graph toward connotative predicates and argu-
ments.
In order to establish a learning bias, we start with
a small set of seed words for just connotative predi-
cates. We use 20 words for each polarity, as listed in
Table 1 and Table 2. These seed words act as prior
knowledge in our learning. We explore two different
techniques to incorporate prior knowledge into ran-
dom walk, as will be elaborated in Section 4.2 &amp; 4.3,
followed by brief description of HITS in Section 4.1.
</bodyText>
<subsectionHeader confidence="0.998585">
4.1 Hyperlink-Induced Topic Search (HITS)
</subsectionHeader>
<bodyText confidence="0.9999198">
HITS (Hyperlink-Induced Topic Search) algorithm
(Kleinberg, 1999), also known as Hubs and author-
ities, is a link analysis algorithm that is particularly
suitable to model mutual reinforcement between two
different types of nodes: hubs and authorities. The
definitions of hubs and authorities are given recur-
sively. A (good) hub is a node that points to many
(good) authorities, and a (good) authority is a node
pointed by many (good) hubs.
Notice that the mutually reinforcing relation-
ship is precisely what we intend to model between
connotative predicates and arguments. Let G =
(P, A, E) be the bipartite graph, where P is the set
of nodes corresponding to connotative predicates, A
is the set of nodes corresponding to arguments, and
E is the set of edges among nodes. (Pi, Aj) ∈ E
if and only if the predicate Pi and the argument Ai
occur together as a predicate – argument pair in the
corpus. The co-occurrence matrix derived from our
corpus is denoted as L, where
</bodyText>
<equation confidence="0.95712325">
�
w(i, j) if(Pi, Aj) ∈ E
Lij =
0 otherwise
</equation>
<bodyText confidence="0.999790428571429">
The value of w(i, j) is set to w(i − j) as defined
in Section 3.1 for undirected graphs, and w(i → j)
defined in Section 3.2 for directed graphs.
Let a(Ai) and h(Ai) be the authority and hub
score respectively, for a given node Ai ∈ A. Then
we compute the authority and hub score recursively
as follows:
</bodyText>
<equation confidence="0.993429428571429">
E E
a(Ai) = w(i, j)h(Aj) +
Pi,AjEE Pj,AiEE
E E
h(Ai) = w(i, j)a(Aj) +
Pi,AjEE Pj,AiEE
P(p, a)
w(p − a) := PMI(p, a) = log
P(p)P(a)
P(p,a)
w(p → a) := P(a|p) =
h(Pj)w(j, i)
a(Pj)w(j, i)
1095
</equation>
<bodyText confidence="0.999892166666667">
The scores a(Pi) and h(Pi) for Pi E P are defined
similarly as above.
In what follows, we describe two different tech-
niques to incorporate prior knowledge. Note that it
is possible to apply each of the following techniques
to both directed and undirected graph representa-
tions introduced in Section 3. Also note that for each
technique, we construct two separate graphs G+ and
G− corresponding to positive and negative polarity
respectively. That is, G+ learns positively connota-
tive predicates and arguments, while G− learns neg-
atively connotative predicates and arguments.
</bodyText>
<subsectionHeader confidence="0.988455">
4.2 Prior Knowledge via Truncated Graph
</subsectionHeader>
<bodyText confidence="0.999273771428572">
First we introduce a method based on graph trunca-
tion. In this method, when constructing the bipartite
graph, we limit the set of predicates P to only those
words in the seed set, instead of including all words
that can be predicates. In a way, the truncated graph
representation can be viewed as the query induced
graph on which the original HITS algorithm was in-
vented (Kleinberg, 1999).
The truncated graph is very effective in reducing
the level of noise that can be introduced by predi-
cates of the opposite polarity. It may seem like we
cannot discover new connotative predicates in the
truncated graph however, as the graph structure is
limited only to the seed predicates. We address this
issue by alternating truncation to different side of the
graph, i.e., left (predicates) or right (arguments), as
illustrated in Figure 1, through multiple rounds of
HITS.
For instance, we start with the graph G =
(Po, A, E(Po)) that is truncated only on the left-
hand side, with the seed predicates Po. Here, E(Po)
denotes the reduced set of edges discarding those
edges that connect to predicates not in Po. Then, we
apply HITS algorithm until convergence to discover
new words with connotation, and this completes the
first round of HITS.
Next we begin the second round. Let Ao be the
new words with connotation that are found in the
first round. We now set Ao as seed words for the
second phase of HITS, where we construct a new
graph G = (P, Ao, E(Ao)) that is truncated only
on the right-hand side, with full candidate words for
predicates included on the left-hand side. This al-
ternation can be repeated multiple times to discover
many new connotative predicates and arguments.
</bodyText>
<subsectionHeader confidence="0.918574">
4.3 Prior Knowledge via Focussed Graph
</subsectionHeader>
<bodyText confidence="0.980306608695652">
In the truncated graph described above, one poten-
tial concern is that the discovery of new words with
connotation is limited to those that happen to corre-
late well with the seed predicates. To mitigate this
problem, we explore an alternative technique based
on the full graph, which we will name as focussed
graph.
In this method, instead of truncating the graph, we
simply emphasize the important portion of the graph
via edge weights. That is, we assign high weights to
those edges that connect a seed predicate with an ar-
gument, while assigning low weights for those edges
that connect to a predicate outside the seed set. This
way, we allow predicates not in the seed set to par-
ticipate in hubs and authority scores, but in a much
suppressed way. This method can be interpreted as
a smoothed version of the truncated graph described
in Section 4.2.
More formally, if the node Ai is connected to
the seed predicate Pj, the value of co-occurrence
matrix Lij is defined by prior knowledge(e.g.
PMI(Ai, Pj) or P(Ai Pj) ), otherwise a small con-
stant E is assigned to the edge.
</bodyText>
<equation confidence="0.94916825">
�
Lij =
w(i, j) ifPj E Eo
E otherwise
</equation>
<bodyText confidence="0.999596333333333">
Similarly to the truncated graph, we proceed with
multiple rounds of HITS, focusing different part of
the bipartite graph alternately.
</bodyText>
<sectionHeader confidence="0.986305" genericHeader="method">
5 Lexicon Induction using PageRank
</sectionHeader>
<bodyText confidence="0.9999825">
In this section, we explore the use of another popu-
lar approach for link analysis: PageRank (Page et
al., 1999). We first describe PageRank algorithm
briefly in Section 5.1, then introduce two different
techniques to incorporate prior knowledge in Sec-
tion 5.2 and 5.3.
</bodyText>
<subsectionHeader confidence="0.9404">
5.1 PageRank
</subsectionHeader>
<bodyText confidence="0.933405888888889">
Let G = (V, E) be the graph, where vi E V =
P U A are nodes (words) for the disjunctive set of
predicates (P) and arguments (A), and e(i,j) E E
are edges. Let In(i) be the set of nodes with an
edge leading to ni and similarly, Out(i) be the set
1096
of nodes that ni has an edge leading to. At a given
iteration of the algorithm, we update the score of ni
as follows:
</bodyText>
<equation confidence="0.998667333333333">
w(i,j)
S(j) X + (1 − α) (1)
 |Out(i)|
</equation>
<bodyText confidence="0.999992285714286">
where the value α is constant damping factor. The
value of α is typically set to 0.85. The value of
w(i, j) is set to w(i−j) as defined in Section 3.1 for
undirected graphs, and w(i -+ j) as defined in Sec-
tion 3.2 for directed graphs. As before, we will con-
sider two different techniques to incorporate prior
knowledge into the graph analysis as follows.
</bodyText>
<subsectionHeader confidence="0.987725">
5.2 Prior Knowledge via Truncated Graph
</subsectionHeader>
<bodyText confidence="0.999989142857143">
Unlike HITS, which was originally invented for a
query-induced graph, PageRank is typically applied
to the full graph. However, we can still apply the
truncation technique introduced in Section 4.2 to
PageRank as well. To do so, when constructing the
bipartite graph, we limit the set of predicates P to
only those words in the seed set, instead of including
all words that can be predicates. Graph truncation
eliminates the noise that can be introduced by pred-
icates of the opposite polarity. However, in order to
learn new predicates, we need to perform multiple
rounds of PageRank, truncating different side of the
bipartite graph alternately. Refer to Section 4.2 for
futher details.
</bodyText>
<subsectionHeader confidence="0.982225">
5.3 Prior Knowledge via Teleportation
</subsectionHeader>
<bodyText confidence="0.99988375">
We next explore what is known as teleportation
technique for topic sensitive PageRank (Haveliwala,
2002). For this, we use the following equation that
is slightly augmented from Equation 1.
</bodyText>
<equation confidence="0.999125333333333">
w(i,j)
S(j) X + (1 − α) Ei (2)
|Out(i)
</equation>
<bodyText confidence="0.999975333333333">
Here, the new term Ei is a smoothing factor that pre-
vents cliques in the graph from garnering reputation
through feedback (Bianchini et al. (2005)). In or-
der to emphasize important portion of the graph, i.e.,
subgraphs connected to the seed set, we assign non-
zero E scores to only those important nodes, i.e., the
seed set. Intuitively, this will cause the random walk
to restart from the seed set with (1−α) = 0.15 prob-
ability for each step.
</bodyText>
<sectionHeader confidence="0.974212" genericHeader="method">
6 The Use of Google Web 1T Data
</sectionHeader>
<bodyText confidence="0.988050533333333">
In order to implement the network of connotative
predicates and arguments, we need a substantially
large amount of documents. The quality of the co-
occurrence statistics is expected to be proportionate
to the size of corpus, but collecting and process-
ing such a large amount of data is not trivial. We
therefore resort to the Google Web 1T data (Brants
and Franz., 2006), which consists of Google n-gram
counts (frequency of occurrence of each n-gram) for
1 &lt; n &lt; 5. The use of Web 1T data will lessen the
challenge with respect to data acquisition, while still
allowing us to enjoy the co-occurrence statistics of
web-scale data. Because Web 1T data is just n-gram
statistics, rather than a collection of normal docu-
ments, it does not provide co-occurrence statistics of
any random word pairs. However, it provides a nice
approximation to the particular co-occurrence statis-
tics we are interested in, which are, predicate – ar-
gument pairs. This is because the THEME argument
of a verb predicate is typically on the right hand side
of the predicate, and the argument is within the close
range of the predicate.
We now describe how to derive co-occurrence
statistics of each predicate – argument pair using the
Web 1T data. For a given predicate p and an argu-
ment a, we add up the count (frequency) of all n-
grams (2 &lt; n &lt; 5) that match the following pattern:
[p] [*]n−2 [a]
where p must be the first word (head), a must be the
last word (tail), and [*]n−2 matches any n − 2 num-
ber of words between p and a. Note that this rule
enforces the argument a to be on the right hand side
of the predicate p. To reduce the level of noise, we
do not allow the wildcard [*] to match any punctu-
ation mark, as such n-grams are likely to cross sen-
tence boundaries representing invalid predicate – ar-
gument relations. We consider a word as a predicate
if it is tagged as a verb by a Part-of-Speech tagger
(Toutanova and Manning, 2000). For argument [a],
we only consider content-words.
The use of web n-gram statistics necessarily in-
vites certain kinds of noise. For instance, some of
the [p] [*]n−2 [a] patterns might not correspond to
a valid predicate – argument relation. However, we
expect that our graph-based algorithms — HITS and
</bodyText>
<equation confidence="0.979973833333333">
�
S(i) = α
jEIn(i)
�
S(i) = α
jEIn(i)
</equation>
<table confidence="0.9007404">
1097
Lexicon FREQ HITS-sT HITS-aT HITS-sF HITS-aF Page-aT Page-aF
Top 100 73.6 67.8 77.7 67.8 48.4 76.3 77.0
Top 1000 67.8 60.6 68.8 60.6 38.0 68.4 68.5
Top MAX 65.8 57.6 66.5 57.6 39.1 65.5 65.7
</table>
<tableCaption confidence="0.998976">
Table 3: Comparison Result with General Inquirer Lexicon(%)
</tableCaption>
<table confidence="0.99992475">
Lexicon FREQ HITS-sT HITS-aT HITS-sF HITS-aF Page-aT Page-aF
Top 100 83.0 79.3 86.3 79.3 55.8 86.3 87.2
Top 1000 80.3 67.3 81.3 67.3 46.5 80.7 80.3
Top MAX 71.5 62.7 72.2 62.7 45.4 71.1 72.3
</table>
<tableCaption confidence="0.999962">
Table 4: Comparison Result with OpinionFinder (%)
</tableCaption>
<bodyText confidence="0.99975">
PageRank — will be able to discern valid relations
from noise, by focusing on the important part of the
graph. In other words, we expect that good predi-
cates will be supported by good arguments, and vice
versa, thereby resulting in a reliable set of predicates
and arguments that are mutually supported by each
other.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999976625">
As a baseline, we use a simple method dubbed
FREQ, which uses co-occurrence frequency with
respect to the seed predicates. Using the pattern
[p] [*]n−2 [a] (see Section 6), we collect two sets
of n-gram records: one set using the positive con-
notative predicates, and the other using the negative
connotative predicates. With respect to each set, we
calculate the following for each word a,
</bodyText>
<listItem confidence="0.9990535">
• Given [a], the number of unique [p] as f1
• Given [a], the number of unique phrases [*]n−2
as f2
• The number of occurrences of [a] as f3
</listItem>
<bodyText confidence="0.9708948">
We then obtain the score σa+ for positive connota-
tion and σa_ for negative connotation using the fol-
lowing equations that take a linear combination of
f1, f2, and f3 that we computed above with respect
to each polarity.
</bodyText>
<equation confidence="0.999995">
σa+ = α x σf1+ + 0 x σf2+ + γ x σf3+ (3)
σa_ = α x σf1_ + 0 x σf2_ + γ x σf3_ (4)
</equation>
<bodyText confidence="0.999775">
Note that the coefficients α, 0 and γ are determined
experimentally. We assign positive polarity to the
word a, if σa+ &gt;&gt; σa_ and vice versa.
</bodyText>
<subsectionHeader confidence="0.991649">
7.1 Comparison against Sentiment Lexicon
</subsectionHeader>
<bodyText confidence="0.99995785">
The polarity defined in the connotation lexicon dif-
fers from that of conventional sentiment lexicons in
which we aim to recognize more subtle sentiment
that correlates with words. Nevertheless, we provide
agreement statistics between our connotation lexi-
con and conventional sentiment lexicons for com-
parison purposes. We collect statistics with respect
to the following two resources: General Inquirer
(Stone and Hunt, 1963) and Opinion Finder (Wilson
et al., 2005b).
For polarity A E {+, −1, let countsentlex(λ) denote
the total number of words labeled as A in a given
sentiment lexicon, and let countagreement(λ) denote
the total number of words labeled as A by both the
given sentiment lexicon and our connotation lexi-
con. In addition, let countoverlap(λ) denote the total
number of words that are labeled as A by our conno-
tation lexicon that are also included in the reference
lexicon with or without the same polarity. Then we
compute precλ as follows:
</bodyText>
<equation confidence="0.994835">
countagreement(λ) x 100
countoverlap(λ)
</equation>
<bodyText confidence="0.9993765">
We compare precλ % for three different segments
of our lexicon: the top 100, top 1000, and the entire
lexicon. We compare the lexicons provided by the
seven variations of our algorithm. Results are shown
in Table 3 &amp; 4.
The acronym of each different method is defined
as follows: HITS-sT &amp; HITS-aT correspond to
the Symmetric (undirected) and Asymmetric (di-
rected) version of the Truncated method respec-
tively. HITS-sF &amp; HITS-aF correspond to the
</bodyText>
<equation confidence="0.834224">
precλ % =
1098
</equation>
<bodyText confidence="0.997722714285714">
Positive: include, offer, obtain, allow, build, in-
crease, ensure, contain, pursue, fulfill, maintain,
recommend, represent, require, respect
Negative: abate, die, condemn, deduce, investi-
gate, commit, correct, apologize, debilitate, dis-
pel, endure, exacerbate, indicate, induce, mini-
mize
</bodyText>
<tableCaption confidence="0.9638515">
Table 5: Examples of newly discovered connotative pred-
icates
</tableCaption>
<bodyText confidence="0.756447333333333">
Positive: boogie, housewarming, persuasiveness,
kickoff, playhouse, diploma, intuitively, monu-
ment, inaugurate, troubleshooter, accompanist
Negative: seasickness, overleap, gangrenous,
suppressing, fetishist, unspeakably, doubter,
bloodmobile, bureaucratized
</bodyText>
<tableCaption confidence="0.943576">
Table 6: Examples of newly discovered words with con-
notations: these words are treated as neutral in some con-
ventional sentiment lexicons.
</tableCaption>
<bodyText confidence="0.802228">
sentiment classification tasks in two different ways:
• Unsupervised classification by voting. We de-
fine r as the ratio of positive polarity words to
negative polarity words in the lexicon. In our
experiment, penalty is 0 for positive and −0.5
for negative.
</bodyText>
<equation confidence="0.999128">
score(x+) = 1 + penalty+(r, #positive)
score(x−) = −1 + penalty−(r, #negative)
</equation>
<listItem confidence="0.9879206">
• Supervised classification using SVM. We use
bag-of-words features for baseline. In order
to quantify the effect of different lexicons, we
add additional features based on the following
scores as defined below:
</listItem>
<equation confidence="0.9858435">
�scoreraw(x) = s(w)
wex
</equation>
<bodyText confidence="0.999889894736842">
symmetric and asymmetric version of the Focused
method. Finally, Page-aT &amp; Page-aF correspond to
the Truncation and teleportation (Focused) respec-
tively.
Asymmetric HITS on a directed truncated graph
(HITS-aT) and topic-sensitive PageRank (Page-aF)
achieve the best performance in most cases, espe-
cially for top ranked words which have a higher
average frequency. The difference between these
two top performers is not large, but statistically
significant using wilcoxon test with p &lt; 0.03.
Standard PageRank (Page-aT) achieves the third
best performance overall. All these top performing
ones (HITS-aT, Page-aF, Page-aT) outperform the
baseline approach (FREQ) statistically significantly
with p &lt; 0.001. For brevity, we omit the PageRank
results based on the undirected graphs, as the perfor-
mance of those was not as good as that of directed
ones.
</bodyText>
<subsectionHeader confidence="0.9938085">
7.2 Extrinsic Evaluation via Sentiment
Analysis
</subsectionHeader>
<bodyText confidence="0.99995575">
Next we perform extrinsic evaluation to quantify the
practical value of our connotation lexicon in con-
crete sentiment analysis applications. In particular,
we make use of our connotation lexicon for binary
</bodyText>
<equation confidence="0.998248">
scoreraw(x)
scorepurity(x) = �wex abs(s(w))
</equation>
<bodyText confidence="0.999691666666667">
The two corpora we use are SemEval2007 (Strap-
parava and Mihalcea, 2007) and Sentiment Twitter.1
The Twitter dataset consists of tweets containing ei-
ther a smiley emoticon (representing positive senti-
ment) or a frowny emoticon (representing negative
sentiment), we randomly select 50000 smiley tweets
and 50000 frowny tweets.2 We perform a 5-fold
cross validation.
In Table 8, we find very promising results, partic-
ularly for Twitter dataset, which is known to be very
noisy. Notice that the use of Top 6k words from
our connotation lexicon along with OpinionFinder
lexicon boost the performance up to 78.0%, which
is significantly better than than 71.4% using only
the conventional OpinionFinder lexicon. This result
shows that our connotation lexicon nicely comple-
ments existing sentiment lexicon, improving practi-
cal sentiment analysis tasks.
</bodyText>
<footnote confidence="0.930205">
1http://www.stanford.edu/˜alecmgo/cs224n/twitterdata.
2009.05.25.c.zip
</footnote>
<table confidence="0.898024133333333">
2We filter out stop-words and words appearing less than 3
times. For Twitter, we also remove usernames of the format
@username occurring within tweet bodies.
1099
Algorithm 1st Round 2nd Round
Acc. F-val Acc. F-val
Voting 68.7 65.4 71.0 68.5
Bag of Words 69.9 65.1 69.9 65.1
(00) + OpFinder 74.7 75.0 74.7 75.0
BoW + Top 2k 73.3 74.5 73.7 75.4
(00) + OpFinder 72.8 73.5 75.0 77.6
BoW + Top 6k 76.6 77.1 74.5 75.3
(00) + OpFinder 74.1 73.5 75.2 76.0
BoW + Top 10k 74,1 73.5 74.2 73.8
(00) + OpFinder 73.5 74.3 74.7 75.1
</table>
<tableCaption confidence="0.8577885">
Table 7: SemEval Classification Result(%) — (00) denotes
that all features in the previous row are copied over.
</tableCaption>
<table confidence="0.999868181818182">
Algorithm 1st Round 2nd Round
Acc. F-val Acc. F-val
Voting 60.4 59.1 62.6 61.3
Bag of Words 69.9 72.1 69.9 72.1
(00) + OpFinder 70.3 71.4 70.3 71.4
BoW + Top 2k 71.3 65.4 72.7 73.3
(00) + OpFinder 69.4 63.1 73.1 74.6
BoW + Top 6k 77.2 69.0 76.4 77.6
(00) + OpFinder 76.4 72.0 76.8 78.0
BoW + Top 10k 73.3 73.5 73.7 74.1
(00) + OpFinder 74.1 69.5 73.5 74.2
</table>
<tableCaption confidence="0.994014">
Table 8: Twitter Classification Result(%) — (00) denotes
that all features in the previous row are copied over.
</tableCaption>
<subsectionHeader confidence="0.903377">
7.3 Intrinsic Evaluation via Human Judgment
</subsectionHeader>
<bodyText confidence="0.999997132075472">
In order to measure the quality of the connotation
lexicon, we also perform human judgment study on
a subset of the lexicon. Human judges are asked to
quantify the degree of connotative polarity of each
given word using an integer value between 1 and 5,
where 1 and 5 correspond to the most negative and
positive connotation respectively. When computing
the annotator agreement score or evaluating our con-
notation lexicon against human judgment, we con-
solidate 1 and 2 into a single negative class and 4
and 5 into a single positive class. The Kappa score
between two human annotators is 0.78.
As a control set, we also include 100 words taken
from the General Inquirer lexicon: 50 words with
positive sentiment, and 50 words with negative sen-
timent. These words are included so as to mea-
sure the quality of human judgment against a well-
established sentiment lexicon. The words were pre-
sented in a random order so that the human judges
will not know which words are from the General In-
quirer lexicon and which are from our connotative
lexicon. For the words in the control set, the anno-
tators achieved 94% (97% lenient) accuracy on the
positive set and 97% on the negative set.
Note that some words appear in both positive and
negative connotation graphs, while others appear in
only one of them. For instance, if a given word x
appears as an argument for only positive connotative
predicates, but never for negative ones, then x would
appear only in the positive connotation graph. This
means that for such word, we can assume the conno-
tative polarity even without applying the algorithms
for graph centrality. Therefore, we first evaluate the
accuracy of the polarity of such words that appear
only in one of the connotation graphs. We discard
words with low frequency (300 in terms of Google
n-gram frequency), and randomly select 50 words
from each polarity. The accuracy of such words is
88% by strict evaluation and 94.5% by lenient eval-
uation, where lenient evaluation counts words in our
polarized connotation lexicon to be correct if the hu-
man judges assign non-conflicting polarities, i.e., ei-
ther neutral or identical polarity.
For words that appear in both positive and nega-
tive connotation graphs, we determine the final po-
larity of such words as one with higher scores given
by HITS or PageRank. We randomly select words
that rank at 5% of top 100, top 1000, top 2000, and
top 5000 by each algorithm for human judgment.
We only evaluate the top performing algorithms –
HITS-aT and Page-aF – and FREQ baseline. The
stratified performance for each of these methods is
given in Table 9.
</bodyText>
<sectionHeader confidence="0.999911" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999772777777778">
Graph based approaches have been used in many
previous research for lexicon induction. A tech-
nique named label propagation (Zhu and Ghahra-
mani, 2002) has been used by Rao and Ravichan-
dran (2009) and Velikovich et al. (2010), while ran-
dom walk based approaches, PageRank in particular,
have been used by Esuli and Sebastiani (2007). In
our work, we explore the use of both HITS (Klein-
berg, 1999) and PageRank (Page et al., 1999) and
</bodyText>
<table confidence="0.988906111111111">
1100
Top # Average Positive Negative
Str. Len. Str. Len. Str. Len.
FREQ
@100 73.5 87.3 72.2 91.1 74.7 83.5
@1000 51.8 78.6 44.4 75.6 81.8 90.9
@2000 66.9 74.7 73.1 84.2 57.3 60.0
@5000 61.5 81.3 61.4 84.1 62.0 70.0
HITS-aT
@100 61.3 79.8 74.4 93.3 47.0 65.1
@1000 39.6 75.5 48.1 77.8 30.8 73.1
@2000 57.7 72.1 78.0 86.0 41.0 60.7
@5000 55.6 73.5 69.7 85.7 44.3 63.8
Page-aF
@100 63.0 78.6 74.7 91.2 50.0 64.6
@1000 53.7 72.2 54.5 72.7 53.1 71.9
@2000 56.5 79.6 67.2 91.8 42.6 63.8
@5000 57.1 76.2 75.7 91.0 43.3 65.3
</table>
<tableCaption confidence="0.9958665">
Table 9: Human Annotation Accuracies(%) – Str. de-
notes strict evaluation &amp; Len. denotes lenient evaluation.
</tableCaption>
<bodyText confidence="0.999981882352941">
present systematic comparison of various options for
graph representation and encoding of prior knowl-
edge. We are not aware of any previous research
that made use of HITS algorithm for connotation or
sentiment lexicon induction.
Much of previous research investigated the use of
dictionary network (e.g., WordNet) for lexicon in-
duction (e.g., Kamps et al. (2004), Takamura et al.
(2005), Adreevskaia and Bergler (2006), Esuli and
Sebastiani (2006), Su and Markert (2009), Moham-
mad et al. (2009)), while relatively less research in-
vestigated the use of web documents (e.g., Kaji and
Kitsuregawa (2007), Velikovich et al. (2010))).
Wilson et al. (2005b) first introduced the sen-
timent lexicon, spawning a great deal of research
thereafter. At the beginning, sentiment lexicons
were designed to include only those words that ex-
press sentiment, that is, subjective words. However
in recent years, sentiment lexicons started expand-
ing to include some of those words that simply asso-
ciate with sentiment, even if those words are purely
objective (e.g., Velikovich et al. (2010), Baccianella
et al. (2010)). This trend applies even to the most re-
cent version of the lexicon of Wilson et al. (2005b).
We conjecture that this trend of broader coverage
suggests that such lexicons are practically more use-
ful than sentiment lexicons that include only those
words that are strictly subjective. In this work, we
make this transition more explicit and intentional,
by introducing a novel connotation lexicon.
Mohammad and Turney (2010) focussed on emo-
tion evoked by common words and phrases. The
spirit of their work shares some similarity with ours
in that it aims to find the emotion evoked by words,
as opposed to expressed. Two main differences are:
(1) our work aims to discover even more subtle asso-
ciation of words with sentiment, and (2) we present
a nearly unsupervised approach, while Mohammad
and Turney (2010) explored the use of Mechanical
Turk to build the lexicon based on human judgment.
In the work of Osgood et al. (1957), it has been
discussed that connotative meaning of words can
be measured in multiple scales of semantic differ-
ential, for example, the degree of “goodness” and
“badness”. Our work presents statistical approaches
that measure one such semantic differential auto-
matically. Our graph construction to capture word-
to-word relation is analogous to that of Collins-
Thompson and Callan (2007), where the graph rep-
resentation was used to model more general defini-
tions of words.
</bodyText>
<sectionHeader confidence="0.995518" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999990272727273">
We introduced the connotation lexicon, a novel lex-
icon that list words with connotative polarity, which
will be made publically available. We also pre-
sented graph-based algorithms for learning conno-
tation lexicon together with connotative predicates
in a nearly unsupervised manner. Our approaches
are grounded on the linguistic insight with respect to
the selectional preference of connotative predicates.
Empirical study demonstrates the practical value of
the connotation lexicon for sentiment analysis en-
couraging further research in this direction.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999912">
We wholeheartedly thank the reviewers for very
helpful and insightful comments.
</bodyText>
<sectionHeader confidence="0.995289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994940350318472">
Alina Adreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 209–216.
1101
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Monica Bianchini, Marco Gori, and Franco Scarselli.
2005. Inside pagerank. ACM Trans. Internet Technol.,
5:92–128, February.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16:22–29, March.
K. Collins-Thompson and J. Callan. 2007. Automatic
and human scoring of word definition responses. In
Proceedings of NAACL HLT, pages 476–483.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC06, pages
417–422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pagerank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 424–
431. Association for Computational Linguistics.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
Proceedings of the Eleventh International World Wide
Web Conference, Honolulu, Hawaii.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive collec-
tion of HTML documents. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1075–1083.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to mea-
sure semantic orientation of adjectives. In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC), pages 1115–1118.
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. JOURNAL OF THE ACM,
46(5):604–632.
B. Louw, M. Baker, G. Francis, and E. Tognini-Bonelli.
1993. Irony in the text or insincerity in the writer?
the diagnostic potential of semantic prosodies. TEXT
AND TECHNOLOGY IN HONOUR OF JOHN SIN-
CLAIR, pages 157–176.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using me-
chanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 26–34, Los Angeles, CA, June.
Association for Computational Linguistics.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
599–608, Singapore, August. Association for Compu-
tational Linguistics.
C. E. Osgood, G. Suci, and P. Tannenbaum. 1957. The
measurement of meaning. University of Illinois Press,
Urbana, IL.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-66,
Stanford InfoLab, November.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In EACL ’09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 675–682, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
John Sinclair. 1991. Corpus, concordance, colloca-
tion. Describing English language. Oxford University
Press.
A. Stefanowitsch and S.T. Gries. 2003. Collostructions:
Investigating the interaction of words and construc-
tions. International Journal of Corpus Linguistics,
8(2):209–243.
Philip J. Stone and Earl B. Hunt. 1963. A computer ap-
proach to content analysis: studies using the general
inquirer system. In Proceedings of the May 21-23,
1963, spring joint computer conference, AFIPS ’63
(Spring), pages 241–256, New York, NY, USA. ACM.
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: affective text. In SemEval ’07: Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, pages 70–74, Morristown, NJ, USA.
Association for Computational Linguistics.
M. Stubbs. 1995. Collocations and semantic profiles:
on the cause of the trouble with quantitative studies.
Functions of language, 2(1):23–55.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 1–9. Association for Computational
Linguistics.
1102
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL-05, 43rd Annual
Meeting of the Association for Computational Linguis-
tics, Ann Arbor, US. Association for Computational
Linguistics.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In In EMNLP/VLC
2000, pages 63–70.
Peter Turney. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164–
210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: a system for subjectivity anal-
ysis. In Proceedings of HLT/EMNLP on Interactive
Demonstrations, pages 34–35, Morristown, NJ, USA.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ’05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347–354, Morristown, NJ, USA. Association for
Computational Linguistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
</reference>
<page confidence="0.647999">
1103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.326111">
<title confidence="0.99995">Learning General Connotation of Words using Graph-based Algorithms</title>
<author confidence="0.999585">Song Feng Ritwik Bose Yejin Choi</author>
<affiliation confidence="0.669996">Department of Computer Stony Brook</affiliation>
<address confidence="0.927767">NY 11794,</address>
<email confidence="0.999218">songfeng,rbose,ychoi@cs.stonybrook.edu</email>
<abstract confidence="0.998748076923077">this paper, we introduce a lexa new type of lexicon that lists words with connotative polarity, i.e., words with positive connotation (e.g., award, promotion) and words with negative connotation (e.g., cancer, war). Connotation lexicons differ from much studied sentiment lexicons: the latter concerns that while the former words that with a specific polarity of sentiment. Understanding the connotation of words would seem to require common sense and world knowledge. However, we demonstrate that much of the connotative polarity of words can be inferred from natural language text in a nearly unsupervised manner. The key linguistic insight our approach is preference We present graphbased algorithms using PageRank and HITS that collectively learn connotation lexicon together with connotative predicates. Our empirical study demonstrates that the resulting connotation lexicon is of great value for sentiment analysis complementing existing sentiment lexicons.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alina Adreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>Mining wordnet for fuzzy sentiment: Sentiment tag extraction from wordnet glosses.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>209--216</pages>
<contexts>
<context position="35594" citStr="Adreevskaia and Bergler (2006)" startWordPosition="5792" endWordPosition="5795">0 53.7 72.2 54.5 72.7 53.1 71.9 @2000 56.5 79.6 67.2 91.8 42.6 63.8 @5000 57.1 76.2 75.7 91.0 43.3 65.3 Table 9: Human Annotation Accuracies(%) – Str. denotes strict evaluation &amp; Len. denotes lenient evaluation. present systematic comparison of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words</context>
</contexts>
<marker>Adreevskaia, Bergler, 2006</marker>
<rawString>Alina Adreevskaia and Sabine Bergler. 2006. Mining wordnet for fuzzy sentiment: Sentiment tag extraction from wordnet glosses. In 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 209–216.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="36274" citStr="Baccianella et al. (2010)" startWordPosition="5899" endWordPosition="5902">ohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2010)). This trend applies even to the most recent version of the lexicon of Wilson et al. (2005b). We conjecture that this trend of broader coverage suggests that such lexicons are practically more useful than sentiment lexicons that include only those words that are strictly subjective. In this work, we make this transition more explicit and intentional, by introducing a novel connotation lexicon. Mohammad and Turney (2010) focussed on emotion evoked by common words and phrases. The spirit of their work shares some similarity with ours in that it aims to find the emotion evoked by words, as oppos</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monica Bianchini</author>
<author>Marco Gori</author>
<author>Franco Scarselli</author>
</authors>
<title>Inside pagerank.</title>
<date>2005</date>
<journal>ACM Trans. Internet Technol.,</journal>
<pages>5--92</pages>
<contexts>
<context position="21086" citStr="Bianchini et al. (2005)" startWordPosition="3352" endWordPosition="3355">osite polarity. However, in order to learn new predicates, we need to perform multiple rounds of PageRank, truncating different side of the bipartite graph alternately. Refer to Section 4.2 for futher details. 5.3 Prior Knowledge via Teleportation We next explore what is known as teleportation technique for topic sensitive PageRank (Haveliwala, 2002). For this, we use the following equation that is slightly augmented from Equation 1. w(i,j) S(j) X + (1 − α) Ei (2) |Out(i) Here, the new term Ei is a smoothing factor that prevents cliques in the graph from garnering reputation through feedback (Bianchini et al. (2005)). In order to emphasize important portion of the graph, i.e., subgraphs connected to the seed set, we assign nonzero E scores to only those important nodes, i.e., the seed set. Intuitively, this will cause the random walk to restart from the seed set with (1−α) = 0.15 probability for each step. 6 The Use of Google Web 1T Data In order to implement the network of connotative predicates and arguments, we need a substantially large amount of documents. The quality of the cooccurrence statistics is expected to be proportionate to the size of corpus, but collecting and processing such a large amou</context>
</contexts>
<marker>Bianchini, Gori, Scarselli, 2005</marker>
<rawString>Monica Bianchini, Marco Gori, and Franco Scarselli. 2005. Inside pagerank. ACM Trans. Internet Technol., 5:92–128, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram version 1.</title>
<date>2006</date>
<booktitle>In Linguistic Data Consortium, ISBN:</booktitle>
<pages>1--58563</pages>
<location>Philadelphia.</location>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram version 1. In Linguistic Data Consortium, ISBN: 1-58563-397-6, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<pages>16--22</pages>
<contexts>
<context position="12122" citStr="Church and Hanks (1990)" startWordPosition="1787" endWordPosition="1790">s and arguments. With this goal in mind, 1094 we next explore the directionality of the edges and different strategies to assign weights to them. 3.1 Undirected (Symmetric) Graph First we explore undirected edges. In this case, we assign weight for each undirected edge between a predicate p and an argument a. Intuitively, the weight should correspond to the strength of relatedness or association between the predicate p and the argument a. We use Pointwise Mutual Information (PMI), as it has been used by many previous research to quantify the association between two words (e.g., Turney (2001), Church and Hanks (1990)). The PMI score between p and a is defined as follows: The log of the ratio is positive when the pair of words tends to co-occur and negative when the presence of one word correlates with the absence of the other word. 3.2 Directed (Asymmetric) Graph Next we explore directed edges. That is, for each connected pair of a predicate p and an argument a, there are two edges in opposite directions: e(p → a) and e(a → p). In this case, we explore the use of asymmetric weights using conditional probability. In particular, we define weights as follows: P(p) P(p, a) w(a → p) := P(p|a) = P(a) Having def</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16:22–29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
</authors>
<title>Automatic and human scoring of word definition responses.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT,</booktitle>
<pages>476--483</pages>
<marker>Collins-Thompson, Callan, 2007</marker>
<rawString>K. Collins-Thompson and J. Callan. 2007. Automatic and human scoring of word definition responses. In Proceedings of NAACL HLT, pages 476–483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="1687" citStr="Esuli and Sebastiani (2006)" startWordPosition="243" endWordPosition="247">HITS that collectively learn connotation lexicon together with connotative predicates. Our empirical study demonstrates that the resulting connotation lexicon is of great value for sentiment analysis complementing existing sentiment lexicons. 1 Introduction In this paper, we introduce a connotation lexicon, a new type of lexicon that lists words with connotative polarity, i.e., words with positive connotation (e.g., award, promotion) and words with negative connotation (e.g., cancer, war). Connotation lexicons differ from sentiment lexicons that are studied in much of previous research (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a)): the latter concerns words that express sentiment either explicitly or implicitly, while the former concerns words that evoke or even simply associate with a specific polarity of sentiment. To our knowledge, there has been no previous research that investigates polarized connotation lexicons. Understanding the connotation of words would seem to require common sense and world knowledge at first glance, which in turn might seem to require human encoding of knowledge base. However, we demonstrate that much of the connotative polarity of words can be inferred from natural </context>
<context position="35623" citStr="Esuli and Sebastiani (2006)" startWordPosition="5796" endWordPosition="5799">@2000 56.5 79.6 67.2 91.8 42.6 63.8 @5000 57.1 76.2 75.7 91.0 43.3 65.3 Table 9: Human Annotation Accuracies(%) – Str. denotes strict evaluation &amp; Len. denotes lenient evaluation. present systematic comparison of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., </context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Pageranking wordnet synsets: An application to opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>424--431</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="34449" citStr="Esuli and Sebastiani (2007)" startWordPosition="5593" endWordPosition="5596">domly select words that rank at 5% of top 100, top 1000, top 2000, and top 5000 by each algorithm for human judgment. We only evaluate the top performing algorithms – HITS-aT and Page-aF – and FREQ baseline. The stratified performance for each of these methods is given in Table 9. 8 Related Work Graph based approaches have been used in many previous research for lexicon induction. A technique named label propagation (Zhu and Ghahramani, 2002) has been used by Rao and Ravichandran (2009) and Velikovich et al. (2010), while random walk based approaches, PageRank in particular, have been used by Esuli and Sebastiani (2007). In our work, we explore the use of both HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) and 1100 Top # Average Positive Negative Str. Len. Str. Len. Str. Len. FREQ @100 73.5 87.3 72.2 91.1 74.7 83.5 @1000 51.8 78.6 44.4 75.6 81.8 90.9 @2000 66.9 74.7 73.1 84.2 57.3 60.0 @5000 61.5 81.3 61.4 84.1 62.0 70.0 HITS-aT @100 61.3 79.8 74.4 93.3 47.0 65.1 @1000 39.6 75.5 48.1 77.8 30.8 73.1 @2000 57.7 72.1 78.0 86.0 41.0 60.7 @5000 55.6 73.5 69.7 85.7 44.3 63.8 Page-aF @100 63.0 78.6 74.7 91.2 50.0 64.6 @1000 53.7 72.2 54.5 72.7 53.1 71.9 @2000 56.5 79.6 67.2 91.8 42.6 63.8 @5000 57.1 76.2 7</context>
</contexts>
<marker>Esuli, Sebastiani, 2007</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2007. Pageranking wordnet synsets: An application to opinion mining. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 424– 431. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eleventh International World Wide Web Conference,</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="20815" citStr="Haveliwala, 2002" startWordPosition="3305" endWordPosition="3306">well. To do so, when constructing the bipartite graph, we limit the set of predicates P to only those words in the seed set, instead of including all words that can be predicates. Graph truncation eliminates the noise that can be introduced by predicates of the opposite polarity. However, in order to learn new predicates, we need to perform multiple rounds of PageRank, truncating different side of the bipartite graph alternately. Refer to Section 4.2 for futher details. 5.3 Prior Knowledge via Teleportation We next explore what is known as teleportation technique for topic sensitive PageRank (Haveliwala, 2002). For this, we use the following equation that is slightly augmented from Equation 1. w(i,j) S(j) X + (1 − α) Ei (2) |Out(i) Here, the new term Ei is a smoothing factor that prevents cliques in the graph from garnering reputation through feedback (Bianchini et al. (2005)). In order to emphasize important portion of the graph, i.e., subgraphs connected to the seed set, we assign nonzero E scores to only those important nodes, i.e., the seed set. Intuitively, this will cause the random walk to restart from the seed set with (1−α) = 0.15 probability for each step. 6 The Use of Google Web 1T Data </context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In Proceedings of the Eleventh International World Wide Web Conference, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building lexicon for sentiment analysis from massive collection of HTML documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1075--1083</pages>
<contexts>
<context position="35776" citStr="Kaji and Kitsuregawa (2007)" startWordPosition="5821" endWordPosition="5824">enotes lenient evaluation. present systematic comparison of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2010)). This trend applies even to the most recent version of the lexicon of Wilson et al. (2005b). We conj</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of HTML documents. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1075–1083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>Robert J Mokken</author>
<author>Maarten De Rijke</author>
</authors>
<title>Using wordnet to measure semantic orientation of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1115--1118</pages>
<marker>Kamps, Marx, Mokken, De Rijke, 2004</marker>
<rawString>Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten De Rijke. 2004. Using wordnet to measure semantic orientation of adjectives. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC), pages 1115–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>JOURNAL OF THE ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<contexts>
<context position="6141" citStr="Kleinberg, 1999" startWordPosition="892" endWordPosition="893">e polarity. We hypothesize that this mutually reinforcing reFigure 1: Bipartite graph of connotative predicates and arguments. Edge weights are proportionate to the association strength. lation between connotative predicates and their arguments can be captured via graph centrality in graph-based algorithms. Given a small set of seed words for connotative predicates, our algorithms collectively learn connotation lexicon together with connotative predicates in a nearly unsupervised manner. A number of different graph representations are explored using both PageRank (Page et al., 1999) and HITS (Kleinberg, 1999) algorithms. Empirical study demonstrates that our graph based algorithms are highly effective in learning both connotation lexicon and connotative predicates. Finally, we quantify the practical value of our connotation lexicon in concrete sentiment analysis applications, and demonstrate that the connotation lexicon is of great value for sentiment classification tasks complementing conventional sentiment lexicons. 2 Connotation Lexicon &amp; Connotative Predicate In this section, we define connotation lexicon and connotative predicates more formally, and contrast them against words in conventional</context>
<context position="13843" citStr="Kleinberg, 1999" startWordPosition="2078" endWordPosition="2079"> so as to bias the centrality of the graph toward connotative predicates and arguments. In order to establish a learning bias, we start with a small set of seed words for just connotative predicates. We use 20 words for each polarity, as listed in Table 1 and Table 2. These seed words act as prior knowledge in our learning. We explore two different techniques to incorporate prior knowledge into random walk, as will be elaborated in Section 4.2 &amp; 4.3, followed by brief description of HITS in Section 4.1. 4.1 Hyperlink-Induced Topic Search (HITS) HITS (Hyperlink-Induced Topic Search) algorithm (Kleinberg, 1999), also known as Hubs and authorities, is a link analysis algorithm that is particularly suitable to model mutual reinforcement between two different types of nodes: hubs and authorities. The definitions of hubs and authorities are given recursively. A (good) hub is a node that points to many (good) authorities, and a (good) authority is a node pointed by many (good) hubs. Notice that the mutually reinforcing relationship is precisely what we intend to model between connotative predicates and arguments. Let G = (P, A, E) be the bipartite graph, where P is the set of nodes corresponding to conno</context>
<context position="16289" citStr="Kleinberg, 1999" startWordPosition="2513" endWordPosition="2514">onding to positive and negative polarity respectively. That is, G+ learns positively connotative predicates and arguments, while G− learns negatively connotative predicates and arguments. 4.2 Prior Knowledge via Truncated Graph First we introduce a method based on graph truncation. In this method, when constructing the bipartite graph, we limit the set of predicates P to only those words in the seed set, instead of including all words that can be predicates. In a way, the truncated graph representation can be viewed as the query induced graph on which the original HITS algorithm was invented (Kleinberg, 1999). The truncated graph is very effective in reducing the level of noise that can be introduced by predicates of the opposite polarity. It may seem like we cannot discover new connotative predicates in the truncated graph however, as the graph structure is limited only to the seed predicates. We address this issue by alternating truncation to different side of the graph, i.e., left (predicates) or right (arguments), as illustrated in Figure 1, through multiple rounds of HITS. For instance, we start with the graph G = (Po, A, E(Po)) that is truncated only on the lefthand side, with the seed predi</context>
<context position="34513" citStr="Kleinberg, 1999" startWordPosition="5607" endWordPosition="5609">5000 by each algorithm for human judgment. We only evaluate the top performing algorithms – HITS-aT and Page-aF – and FREQ baseline. The stratified performance for each of these methods is given in Table 9. 8 Related Work Graph based approaches have been used in many previous research for lexicon induction. A technique named label propagation (Zhu and Ghahramani, 2002) has been used by Rao and Ravichandran (2009) and Velikovich et al. (2010), while random walk based approaches, PageRank in particular, have been used by Esuli and Sebastiani (2007). In our work, we explore the use of both HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) and 1100 Top # Average Positive Negative Str. Len. Str. Len. Str. Len. FREQ @100 73.5 87.3 72.2 91.1 74.7 83.5 @1000 51.8 78.6 44.4 75.6 81.8 90.9 @2000 66.9 74.7 73.1 84.2 57.3 60.0 @5000 61.5 81.3 61.4 84.1 62.0 70.0 HITS-aT @100 61.3 79.8 74.4 93.3 47.0 65.1 @1000 39.6 75.5 48.1 77.8 30.8 73.1 @2000 57.7 72.1 78.0 86.0 41.0 60.7 @5000 55.6 73.5 69.7 85.7 44.3 63.8 Page-aF @100 63.0 78.6 74.7 91.2 50.0 64.6 @1000 53.7 72.2 54.5 72.7 53.1 71.9 @2000 56.5 79.6 67.2 91.8 42.6 63.8 @5000 57.1 76.2 75.7 91.0 43.3 65.3 Table 9: Human Annotation Accuracies(%) – Str</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. JOURNAL OF THE ACM, 46(5):604–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Louw</author>
<author>M Baker</author>
<author>G Francis</author>
<author>E Tognini-Bonelli</author>
</authors>
<title>Irony in the text or insincerity in the writer? the diagnostic potential of semantic prosodies.</title>
<date>1993</date>
<journal>TEXT AND TECHNOLOGY IN HONOUR OF JOHN SINCLAIR,</journal>
<pages>157--176</pages>
<contexts>
<context position="4334" citStr="Louw et al. (1993)" startWordPosition="629" endWordPosition="632">ively Connotative Predicates w.r.t. THEME alleviate, accuse, avert, avoid, cause, complain, condemn, criticize, detect, eliminate, eradicate, mitigate, overcome, prevent, prohibit, protest, refrain, suffer, tolerate, withstand Table 2: Negatively Connotative Predicates w.r.t. THEME preference of connotative predicates is that of semantic prosody in corpus linguistics. Semantic prosody describes how some of the seemingly neutral words (e.g., “cause”) can be perceived with positive or negative polarity because they tend to collocate with words with corresponding polarity (e.g., Sinclair (1991), Louw et al. (1993), Stubbs (1995), Stefanowitsch and Gries (2003)). In this work, we demonstrate that statistical approaches that exploit this very concept of semantic prosody can successfully infer connotative polarity of words. Having described the key linguistic insight, we now illustrate our graph-based algorithms. Figure 1 depicts the mutually reinforcing relation between connotative predicates (nodes on the left-hand side) and words with connotative polarity (node on the right-hand side). The thickness of edges represents the strength of the association between predicates and arguments. For brevity, we on</context>
</contexts>
<marker>Louw, Baker, Francis, Tognini-Bonelli, 1993</marker>
<rawString>B. Louw, M. Baker, G. Francis, and E. Tognini-Bonelli. 1993. Irony in the text or insincerity in the writer? the diagnostic potential of semantic prosodies. TEXT AND TECHNOLOGY IN HONOUR OF JOHN SINCLAIR, pages 157–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Peter Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>26--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, CA,</location>
<contexts>
<context position="36698" citStr="Mohammad and Turney (2010)" startWordPosition="5967" endWordPosition="5970">timent lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2010)). This trend applies even to the most recent version of the lexicon of Wilson et al. (2005b). We conjecture that this trend of broader coverage suggests that such lexicons are practically more useful than sentiment lexicons that include only those words that are strictly subjective. In this work, we make this transition more explicit and intentional, by introducing a novel connotation lexicon. Mohammad and Turney (2010) focussed on emotion evoked by common words and phrases. The spirit of their work shares some similarity with ours in that it aims to find the emotion evoked by words, as opposed to expressed. Two main differences are: (1) our work aims to discover even more subtle association of words with sentiment, and (2) we present a nearly unsupervised approach, while Mohammad and Turney (2010) explored the use of Mechanical Turk to build the lexicon based on human judgment. In the work of Osgood et al. (1957), it has been discussed that connotative meaning of words can be measured in multiple scales of </context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif Mohammad and Peter Turney. 2010. Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 26–34, Los Angeles, CA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Cody Dunne</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>599--608</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="35670" citStr="Mohammad et al. (2009)" startWordPosition="5804" endWordPosition="5808">75.7 91.0 43.3 65.3 Table 9: Human Annotation Accuracies(%) – Str. denotes strict evaluation &amp; Len. denotes lenient evaluation. present systematic comparison of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2</context>
</contexts>
<marker>Mohammad, Dunne, Dorr, 2009</marker>
<rawString>Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009. Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Osgood</author>
<author>G Suci</author>
<author>P Tannenbaum</author>
</authors>
<title>The measurement of meaning.</title>
<date>1957</date>
<publisher>University of Illinois Press,</publisher>
<location>Urbana, IL.</location>
<contexts>
<context position="37202" citStr="Osgood et al. (1957)" startWordPosition="6056" endWordPosition="6059">his transition more explicit and intentional, by introducing a novel connotation lexicon. Mohammad and Turney (2010) focussed on emotion evoked by common words and phrases. The spirit of their work shares some similarity with ours in that it aims to find the emotion evoked by words, as opposed to expressed. Two main differences are: (1) our work aims to discover even more subtle association of words with sentiment, and (2) we present a nearly unsupervised approach, while Mohammad and Turney (2010) explored the use of Mechanical Turk to build the lexicon based on human judgment. In the work of Osgood et al. (1957), it has been discussed that connotative meaning of words can be measured in multiple scales of semantic differential, for example, the degree of “goodness” and “badness”. Our work presents statistical approaches that measure one such semantic differential automatically. Our graph construction to capture wordto-word relation is analogous to that of CollinsThompson and Callan (2007), where the graph representation was used to model more general definitions of words. 9 Conclusion We introduced the connotation lexicon, a novel lexicon that list words with connotative polarity, which will be made </context>
</contexts>
<marker>Osgood, Suci, Tannenbaum, 1957</marker>
<rawString>C. E. Osgood, G. Suci, and P. Tannenbaum. 1957. The measurement of meaning. University of Illinois Press, Urbana, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<tech>Technical Report 1999-66,</tech>
<institution>Stanford InfoLab,</institution>
<contexts>
<context position="6114" citStr="Page et al., 1999" startWordPosition="886" endWordPosition="889">ased on words with connotative polarity. We hypothesize that this mutually reinforcing reFigure 1: Bipartite graph of connotative predicates and arguments. Edge weights are proportionate to the association strength. lation between connotative predicates and their arguments can be captured via graph centrality in graph-based algorithms. Given a small set of seed words for connotative predicates, our algorithms collectively learn connotation lexicon together with connotative predicates in a nearly unsupervised manner. A number of different graph representations are explored using both PageRank (Page et al., 1999) and HITS (Kleinberg, 1999) algorithms. Empirical study demonstrates that our graph based algorithms are highly effective in learning both connotation lexicon and connotative predicates. Finally, we quantify the practical value of our connotation lexicon in concrete sentiment analysis applications, and demonstrate that the connotation lexicon is of great value for sentiment classification tasks complementing conventional sentiment lexicons. 2 Connotation Lexicon &amp; Connotative Predicate In this section, we define connotation lexicon and connotative predicates more formally, and contrast them ag</context>
<context position="19014" citStr="Page et al., 1999" startWordPosition="2985" endWordPosition="2988">eted as a smoothed version of the truncated graph described in Section 4.2. More formally, if the node Ai is connected to the seed predicate Pj, the value of co-occurrence matrix Lij is defined by prior knowledge(e.g. PMI(Ai, Pj) or P(Ai Pj) ), otherwise a small constant E is assigned to the edge. � Lij = w(i, j) ifPj E Eo E otherwise Similarly to the truncated graph, we proceed with multiple rounds of HITS, focusing different part of the bipartite graph alternately. 5 Lexicon Induction using PageRank In this section, we explore the use of another popular approach for link analysis: PageRank (Page et al., 1999). We first describe PageRank algorithm briefly in Section 5.1, then introduce two different techniques to incorporate prior knowledge in Section 5.2 and 5.3. 5.1 PageRank Let G = (V, E) be the graph, where vi E V = P U A are nodes (words) for the disjunctive set of predicates (P) and arguments (A), and e(i,j) E E are edges. Let In(i) be the set of nodes with an edge leading to ni and similarly, Out(i) be the set 1096 of nodes that ni has an edge leading to. At a given iteration of the algorithm, we update the score of ni as follows: w(i,j) S(j) X + (1 − α) (1) |Out(i)| where the value α is con</context>
<context position="34546" citStr="Page et al., 1999" startWordPosition="5612" endWordPosition="5615">n judgment. We only evaluate the top performing algorithms – HITS-aT and Page-aF – and FREQ baseline. The stratified performance for each of these methods is given in Table 9. 8 Related Work Graph based approaches have been used in many previous research for lexicon induction. A technique named label propagation (Zhu and Ghahramani, 2002) has been used by Rao and Ravichandran (2009) and Velikovich et al. (2010), while random walk based approaches, PageRank in particular, have been used by Esuli and Sebastiani (2007). In our work, we explore the use of both HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) and 1100 Top # Average Positive Negative Str. Len. Str. Len. Str. Len. FREQ @100 73.5 87.3 72.2 91.1 74.7 83.5 @1000 51.8 78.6 44.4 75.6 81.8 90.9 @2000 66.9 74.7 73.1 84.2 57.3 60.0 @5000 61.5 81.3 61.4 84.1 62.0 70.0 HITS-aT @100 61.3 79.8 74.4 93.3 47.0 65.1 @1000 39.6 75.5 48.1 77.8 30.8 73.1 @2000 57.7 72.1 78.0 86.0 41.0 60.7 @5000 55.6 73.5 69.7 85.7 44.3 63.8 Page-aF @100 63.0 78.6 74.7 91.2 50.0 64.6 @1000 53.7 72.2 54.5 72.7 53.1 71.9 @2000 56.5 79.6 67.2 91.8 42.6 63.8 @5000 57.1 76.2 75.7 91.0 43.3 65.3 Table 9: Human Annotation Accuracies(%) – Str. denotes strict evaluation &amp; Len</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semisupervised polarity lexicon induction.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>675--682</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="34313" citStr="Rao and Ravichandran (2009)" startWordPosition="5570" endWordPosition="5574">d negative connotation graphs, we determine the final polarity of such words as one with higher scores given by HITS or PageRank. We randomly select words that rank at 5% of top 100, top 1000, top 2000, and top 5000 by each algorithm for human judgment. We only evaluate the top performing algorithms – HITS-aT and Page-aF – and FREQ baseline. The stratified performance for each of these methods is given in Table 9. 8 Related Work Graph based approaches have been used in many previous research for lexicon induction. A technique named label propagation (Zhu and Ghahramani, 2002) has been used by Rao and Ravichandran (2009) and Velikovich et al. (2010), while random walk based approaches, PageRank in particular, have been used by Esuli and Sebastiani (2007). In our work, we explore the use of both HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) and 1100 Top # Average Positive Negative Str. Len. Str. Len. Str. Len. FREQ @100 73.5 87.3 72.2 91.1 74.7 83.5 @1000 51.8 78.6 44.4 75.6 81.8 90.9 @2000 66.9 74.7 73.1 84.2 57.3 60.0 @5000 61.5 81.3 61.4 84.1 62.0 70.0 HITS-aT @100 61.3 79.8 74.4 93.3 47.0 65.1 @1000 39.6 75.5 48.1 77.8 30.8 73.1 @2000 57.7 72.1 78.0 86.0 41.0 60.7 @5000 55.6 73.5 69.7 85.7 44.3 6</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. Semisupervised polarity lexicon induction. In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 675–682, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>Corpus, concordance, collocation. Describing English language.</title>
<date>1991</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="4314" citStr="Sinclair (1991)" startWordPosition="627" endWordPosition="628">in Table 1: Positively Connotative Predicates w.r.t. THEME alleviate, accuse, avert, avoid, cause, complain, condemn, criticize, detect, eliminate, eradicate, mitigate, overcome, prevent, prohibit, protest, refrain, suffer, tolerate, withstand Table 2: Negatively Connotative Predicates w.r.t. THEME preference of connotative predicates is that of semantic prosody in corpus linguistics. Semantic prosody describes how some of the seemingly neutral words (e.g., “cause”) can be perceived with positive or negative polarity because they tend to collocate with words with corresponding polarity (e.g., Sinclair (1991), Louw et al. (1993), Stubbs (1995), Stefanowitsch and Gries (2003)). In this work, we demonstrate that statistical approaches that exploit this very concept of semantic prosody can successfully infer connotative polarity of words. Having described the key linguistic insight, we now illustrate our graph-based algorithms. Figure 1 depicts the mutually reinforcing relation between connotative predicates (nodes on the left-hand side) and words with connotative polarity (node on the right-hand side). The thickness of edges represents the strength of the association between predicates and arguments</context>
</contexts>
<marker>Sinclair, 1991</marker>
<rawString>John Sinclair. 1991. Corpus, concordance, collocation. Describing English language. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stefanowitsch</author>
<author>S T Gries</author>
</authors>
<title>Collostructions: Investigating the interaction of words and constructions.</title>
<date>2003</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="4381" citStr="Stefanowitsch and Gries (2003)" startWordPosition="635" endWordPosition="638"> THEME alleviate, accuse, avert, avoid, cause, complain, condemn, criticize, detect, eliminate, eradicate, mitigate, overcome, prevent, prohibit, protest, refrain, suffer, tolerate, withstand Table 2: Negatively Connotative Predicates w.r.t. THEME preference of connotative predicates is that of semantic prosody in corpus linguistics. Semantic prosody describes how some of the seemingly neutral words (e.g., “cause”) can be perceived with positive or negative polarity because they tend to collocate with words with corresponding polarity (e.g., Sinclair (1991), Louw et al. (1993), Stubbs (1995), Stefanowitsch and Gries (2003)). In this work, we demonstrate that statistical approaches that exploit this very concept of semantic prosody can successfully infer connotative polarity of words. Having described the key linguistic insight, we now illustrate our graph-based algorithms. Figure 1 depicts the mutually reinforcing relation between connotative predicates (nodes on the left-hand side) and words with connotative polarity (node on the right-hand side). The thickness of edges represents the strength of the association between predicates and arguments. For brevity, we only consider connotation of words that appear in</context>
</contexts>
<marker>Stefanowitsch, Gries, 2003</marker>
<rawString>A. Stefanowitsch and S.T. Gries. 2003. Collostructions: Investigating the interaction of words and constructions. International Journal of Corpus Linguistics, 8(2):209–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Earl B Hunt</author>
</authors>
<title>A computer approach to content analysis: studies using the general inquirer system.</title>
<date>1963</date>
<journal>AFIPS</journal>
<booktitle>In Proceedings of the</booktitle>
<volume>63</volume>
<pages>241--256</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="25943" citStr="Stone and Hunt, 1963" startWordPosition="4219" endWordPosition="4222">γ x σf3_ (4) Note that the coefficients α, 0 and γ are determined experimentally. We assign positive polarity to the word a, if σa+ &gt;&gt; σa_ and vice versa. 7.1 Comparison against Sentiment Lexicon The polarity defined in the connotation lexicon differs from that of conventional sentiment lexicons in which we aim to recognize more subtle sentiment that correlates with words. Nevertheless, we provide agreement statistics between our connotation lexicon and conventional sentiment lexicons for comparison purposes. We collect statistics with respect to the following two resources: General Inquirer (Stone and Hunt, 1963) and Opinion Finder (Wilson et al., 2005b). For polarity A E {+, −1, let countsentlex(λ) denote the total number of words labeled as A in a given sentiment lexicon, and let countagreement(λ) denote the total number of words labeled as A by both the given sentiment lexicon and our connotation lexicon. In addition, let countoverlap(λ) denote the total number of words that are labeled as A by our connotation lexicon that are also included in the reference lexicon with or without the same polarity. Then we compute precλ as follows: countagreement(λ) x 100 countoverlap(λ) We compare precλ % for thr</context>
</contexts>
<marker>Stone, Hunt, 1963</marker>
<rawString>Philip J. Stone and Earl B. Hunt. 1963. A computer approach to content analysis: studies using the general inquirer system. In Proceedings of the May 21-23, 1963, spring joint computer conference, AFIPS ’63 (Spring), pages 241–256, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Rada Mihalcea</author>
</authors>
<title>Semeval2007 task 14: affective text.</title>
<date>2007</date>
<booktitle>In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>70--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="29518" citStr="Strapparava and Mihalcea, 2007" startWordPosition="4754" endWordPosition="4758"> ones (HITS-aT, Page-aF, Page-aT) outperform the baseline approach (FREQ) statistically significantly with p &lt; 0.001. For brevity, we omit the PageRank results based on the undirected graphs, as the performance of those was not as good as that of directed ones. 7.2 Extrinsic Evaluation via Sentiment Analysis Next we perform extrinsic evaluation to quantify the practical value of our connotation lexicon in concrete sentiment analysis applications. In particular, we make use of our connotation lexicon for binary scoreraw(x) scorepurity(x) = �wex abs(s(w)) The two corpora we use are SemEval2007 (Strapparava and Mihalcea, 2007) and Sentiment Twitter.1 The Twitter dataset consists of tweets containing either a smiley emoticon (representing positive sentiment) or a frowny emoticon (representing negative sentiment), we randomly select 50000 smiley tweets and 50000 frowny tweets.2 We perform a 5-fold cross validation. In Table 8, we find very promising results, particularly for Twitter dataset, which is known to be very noisy. Notice that the use of Top 6k words from our connotation lexicon along with OpinionFinder lexicon boost the performance up to 78.0%, which is significantly better than than 71.4% using only the co</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>Carlo Strapparava and Rada Mihalcea. 2007. Semeval2007 task 14: affective text. In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations, pages 70–74, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stubbs</author>
</authors>
<title>Collocations and semantic profiles: on the cause of the trouble with quantitative studies. Functions of language,</title>
<date>1995</date>
<pages>2--1</pages>
<contexts>
<context position="4349" citStr="Stubbs (1995)" startWordPosition="633" endWordPosition="634">edicates w.r.t. THEME alleviate, accuse, avert, avoid, cause, complain, condemn, criticize, detect, eliminate, eradicate, mitigate, overcome, prevent, prohibit, protest, refrain, suffer, tolerate, withstand Table 2: Negatively Connotative Predicates w.r.t. THEME preference of connotative predicates is that of semantic prosody in corpus linguistics. Semantic prosody describes how some of the seemingly neutral words (e.g., “cause”) can be perceived with positive or negative polarity because they tend to collocate with words with corresponding polarity (e.g., Sinclair (1991), Louw et al. (1993), Stubbs (1995), Stefanowitsch and Gries (2003)). In this work, we demonstrate that statistical approaches that exploit this very concept of semantic prosody can successfully infer connotative polarity of words. Having described the key linguistic insight, we now illustrate our graph-based algorithms. Figure 1 depicts the mutually reinforcing relation between connotative predicates (nodes on the left-hand side) and words with connotative polarity (node on the right-hand side). The thickness of edges represents the strength of the association between predicates and arguments. For brevity, we only consider con</context>
</contexts>
<marker>Stubbs, 1995</marker>
<rawString>M. Stubbs. 1995. Collocations and semantic profiles: on the cause of the trouble with quantitative studies. Functions of language, 2(1):23–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangzhong Su</author>
<author>Katja Markert</author>
</authors>
<title>Subjectivity recognition on word senses via semi-supervised mincuts.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="35646" citStr="Su and Markert (2009)" startWordPosition="5800" endWordPosition="5803">6 63.8 @5000 57.1 76.2 75.7 91.0 43.3 65.3 Table 9: Human Annotation Accuracies(%) – Str. denotes strict evaluation &amp; Len. denotes lenient evaluation. present systematic comparison of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010</context>
</contexts>
<marker>Su, Markert, 2009</marker>
<rawString>Fangzhong Su and Katja Markert. 2009. Subjectivity recognition on word senses via semi-supervised mincuts. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05, 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, US.</location>
<contexts>
<context position="35562" citStr="Takamura et al. (2005)" startWordPosition="5788" endWordPosition="5791">74.7 91.2 50.0 64.6 @1000 53.7 72.2 54.5 72.7 53.1 71.9 @2000 56.5 79.6 67.2 91.8 42.6 63.8 @5000 57.1 76.2 75.7 91.0 43.3 65.3 Table 9: Human Annotation Accuracies(%) – Str. denotes strict evaluation &amp; Len. denotes lenient evaluation. present systematic comparison of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate wit</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In Proceedings of ACL-05, 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, US. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In In EMNLP/VLC</booktitle>
<pages>63--70</pages>
<contexts>
<context position="23332" citStr="Toutanova and Manning, 2000" startWordPosition="3762" endWordPosition="3765">quency) of all ngrams (2 &lt; n &lt; 5) that match the following pattern: [p] [*]n−2 [a] where p must be the first word (head), a must be the last word (tail), and [*]n−2 matches any n − 2 number of words between p and a. Note that this rule enforces the argument a to be on the right hand side of the predicate p. To reduce the level of noise, we do not allow the wildcard [*] to match any punctuation mark, as such n-grams are likely to cross sentence boundaries representing invalid predicate – argument relations. We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). For argument [a], we only consider content-words. The use of web n-gram statistics necessarily invites certain kinds of noise. For instance, some of the [p] [*]n−2 [a] patterns might not correspond to a valid predicate – argument relation. However, we expect that our graph-based algorithms — HITS and � S(i) = α jEIn(i) � S(i) = α jEIn(i) 1097 Lexicon FREQ HITS-sT HITS-aT HITS-sF HITS-aF Page-aT Page-aF Top 100 73.6 67.8 77.7 67.8 48.4 76.3 77.0 Top 1000 67.8 60.6 68.8 60.6 38.0 68.4 68.5 Top MAX 65.8 57.6 66.5 57.6 39.1 65.5 65.7 Table 3: Comparison Result with General Inquirer Lexicon(%) Le</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In In EMNLP/VLC 2000, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: Pmiir versus lsa on toefl.</title>
<date>2001</date>
<contexts>
<context position="12097" citStr="Turney (2001)" startWordPosition="1785" endWordPosition="1786">tween predicates and arguments. With this goal in mind, 1094 we next explore the directionality of the edges and different strategies to assign weights to them. 3.1 Undirected (Symmetric) Graph First we explore undirected edges. In this case, we assign weight for each undirected edge between a predicate p and an argument a. Intuitively, the weight should correspond to the strength of relatedness or association between the predicate p and the argument a. We use Pointwise Mutual Information (PMI), as it has been used by many previous research to quantify the association between two words (e.g., Turney (2001), Church and Hanks (1990)). The PMI score between p and a is defined as follows: The log of the ratio is positive when the pair of words tends to co-occur and negative when the presence of one word correlates with the absence of the other word. 3.2 Directed (Asymmetric) Graph Next we explore directed edges. That is, for each connected pair of a predicate p and an argument a, there are two edges in opposite directions: e(p → a) and e(a → p). In this case, we explore the use of asymmetric weights using conditional probability. In particular, we define weights as follows: P(p) P(p, a) w(a → p) :=</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the web for synonyms: Pmiir versus lsa on toefl.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of webderived polarity lexicons.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="34342" citStr="Velikovich et al. (2010)" startWordPosition="5576" endWordPosition="5579">e determine the final polarity of such words as one with higher scores given by HITS or PageRank. We randomly select words that rank at 5% of top 100, top 1000, top 2000, and top 5000 by each algorithm for human judgment. We only evaluate the top performing algorithms – HITS-aT and Page-aF – and FREQ baseline. The stratified performance for each of these methods is given in Table 9. 8 Related Work Graph based approaches have been used in many previous research for lexicon induction. A technique named label propagation (Zhu and Ghahramani, 2002) has been used by Rao and Ravichandran (2009) and Velikovich et al. (2010), while random walk based approaches, PageRank in particular, have been used by Esuli and Sebastiani (2007). In our work, we explore the use of both HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) and 1100 Top # Average Positive Negative Str. Len. Str. Len. Str. Len. FREQ @100 73.5 87.3 72.2 91.1 74.7 83.5 @1000 51.8 78.6 44.4 75.6 81.8 90.9 @2000 66.9 74.7 73.1 84.2 57.3 60.0 @5000 61.5 81.3 61.4 84.1 62.0 70.0 HITS-aT @100 61.3 79.8 74.4 93.3 47.0 65.1 @1000 39.6 75.5 48.1 77.8 30.8 73.1 @2000 57.7 72.1 78.0 86.0 41.0 60.7 @5000 55.6 73.5 69.7 85.7 44.3 63.8 Page-aF @100 63.0 78.6 74</context>
<context position="35802" citStr="Velikovich et al. (2010)" startWordPosition="5825" endWordPosition="5828">esent systematic comparison of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2010)). This trend applies even to the most recent version of the lexicon of Wilson et al. (2005b). We conjecture that this trend of </context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of webderived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<booktitle>Language Resources and Evaluation (formerly Computers and the Humanities),</booktitle>
<pages>39--2</pages>
<contexts>
<context position="8209" citStr="Wiebe et al. (2005)" startWordPosition="1186" endWordPosition="1189">describe physical objects or abstract concepts that people generally disvalue or avoid. Similarly as before, some of these words may express subjectivity (e.g., “disappointment”, “humiliation”), while many other are purely objective (e.g., “bedbug”, “arthritis, “funeral”). Note that this explicit and intentional inclusion of objective terms makes connotation lexicons differ from sentiment lexicons: most conventional sentiment lexicons have focused on subjective words by definition (e.g., Wilson et al. (2005b)), as many researchers use the term sentiment and subjectivity interchangeably (e.g., Wiebe et al. (2005)). 2.2 Connotative Predicate In this work, connotative predicates are those that exhibit selectional preference on the connotative polarity of some of their arguments. We emphasize that the polarity of connotative predicates does not coincide with the polarity of sentiment in conventional sentiment lexicons, as will be elaborated below. • Positively connotative predicate: In this work, we define positively connotative predicates as those that expect positive connotation in some arguments. For example, “congratulate” or “save” are positively connotative predicates that expect words with positiv</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation (formerly Computers and the Humanities), 39(2/3):164– 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: a system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on Interactive Demonstrations,</booktitle>
<pages>34--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1708" citStr="Wilson et al. (2005" startWordPosition="248" endWordPosition="251">connotation lexicon together with connotative predicates. Our empirical study demonstrates that the resulting connotation lexicon is of great value for sentiment analysis complementing existing sentiment lexicons. 1 Introduction In this paper, we introduce a connotation lexicon, a new type of lexicon that lists words with connotative polarity, i.e., words with positive connotation (e.g., award, promotion) and words with negative connotation (e.g., cancer, war). Connotation lexicons differ from sentiment lexicons that are studied in much of previous research (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a)): the latter concerns words that express sentiment either explicitly or implicitly, while the former concerns words that evoke or even simply associate with a specific polarity of sentiment. To our knowledge, there has been no previous research that investigates polarized connotation lexicons. Understanding the connotation of words would seem to require common sense and world knowledge at first glance, which in turn might seem to require human encoding of knowledge base. However, we demonstrate that much of the connotative polarity of words can be inferred from natural language text in a ne</context>
<context position="8102" citStr="Wilson et al. (2005" startWordPosition="1169" endWordPosition="1172">r “scientific”. • Words with negative connotation: We define words with negative connotation as those that describe physical objects or abstract concepts that people generally disvalue or avoid. Similarly as before, some of these words may express subjectivity (e.g., “disappointment”, “humiliation”), while many other are purely objective (e.g., “bedbug”, “arthritis, “funeral”). Note that this explicit and intentional inclusion of objective terms makes connotation lexicons differ from sentiment lexicons: most conventional sentiment lexicons have focused on subjective words by definition (e.g., Wilson et al. (2005b)), as many researchers use the term sentiment and subjectivity interchangeably (e.g., Wiebe et al. (2005)). 2.2 Connotative Predicate In this work, connotative predicates are those that exhibit selectional preference on the connotative polarity of some of their arguments. We emphasize that the polarity of connotative predicates does not coincide with the polarity of sentiment in conventional sentiment lexicons, as will be elaborated below. • Positively connotative predicate: In this work, we define positively connotative predicates as those that expect positive connotation in some arguments.</context>
<context position="9466" citStr="Wilson et al. (2005" startWordPosition="1374" endWordPosition="1377">eople typically congratulate something positive, and save something people care about. More examples are shown in Table 1. • Negatively connotative predicate: In this work, we define negatively connotative predicates as those that expect negative connotation in some arguments. For instance, predicates such as “prevent” or “suffer” tend to project negative connotation in the THEME argument. More examples are shown in Table 2. Note that positively connotative predicates are not necessarily positive sentiment words. For instance “save” is not a positive sentiment word in the lexicon published by Wilson et al. (2005b). Inversely, (strongly) positive sentiment words are not necessarily (strongly) positively connotative predicates, e.g., “illuminate”, “agree”. Likewise, negatively connotative predicates are not necessarily negative sentiment words. For instance, predicates such as “prevent”, “detect”, or “cause” are not negative sentiment words, but they tend to correlate with negative connotation in the THEME argument. Inversely, (strongly) negative sentiment words are not necessarily (strongly) negatively connotative predicates, e.g., “abandon” (“abandoned [something valuable]”). 3 Graph Representation I</context>
<context position="25983" citStr="Wilson et al., 2005" startWordPosition="4226" endWordPosition="4229"> 0 and γ are determined experimentally. We assign positive polarity to the word a, if σa+ &gt;&gt; σa_ and vice versa. 7.1 Comparison against Sentiment Lexicon The polarity defined in the connotation lexicon differs from that of conventional sentiment lexicons in which we aim to recognize more subtle sentiment that correlates with words. Nevertheless, we provide agreement statistics between our connotation lexicon and conventional sentiment lexicons for comparison purposes. We collect statistics with respect to the following two resources: General Inquirer (Stone and Hunt, 1963) and Opinion Finder (Wilson et al., 2005b). For polarity A E {+, −1, let countsentlex(λ) denote the total number of words labeled as A in a given sentiment lexicon, and let countagreement(λ) denote the total number of words labeled as A by both the given sentiment lexicon and our connotation lexicon. In addition, let countoverlap(λ) denote the total number of words that are labeled as A by our connotation lexicon that are also included in the reference lexicon with or without the same polarity. Then we compute precλ as follows: countagreement(λ) x 100 countoverlap(λ) We compare precλ % for three different segments of our lexicon: th</context>
<context position="35825" citStr="Wilson et al. (2005" startWordPosition="5829" endWordPosition="5832">of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2010)). This trend applies even to the most recent version of the lexicon of Wilson et al. (2005b). We conjecture that this trend of broader coverage sugges</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinionfinder: a system for subjectivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, pages 34–35, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1708" citStr="Wilson et al. (2005" startWordPosition="248" endWordPosition="251">connotation lexicon together with connotative predicates. Our empirical study demonstrates that the resulting connotation lexicon is of great value for sentiment analysis complementing existing sentiment lexicons. 1 Introduction In this paper, we introduce a connotation lexicon, a new type of lexicon that lists words with connotative polarity, i.e., words with positive connotation (e.g., award, promotion) and words with negative connotation (e.g., cancer, war). Connotation lexicons differ from sentiment lexicons that are studied in much of previous research (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a)): the latter concerns words that express sentiment either explicitly or implicitly, while the former concerns words that evoke or even simply associate with a specific polarity of sentiment. To our knowledge, there has been no previous research that investigates polarized connotation lexicons. Understanding the connotation of words would seem to require common sense and world knowledge at first glance, which in turn might seem to require human encoding of knowledge base. However, we demonstrate that much of the connotative polarity of words can be inferred from natural language text in a ne</context>
<context position="8102" citStr="Wilson et al. (2005" startWordPosition="1169" endWordPosition="1172">r “scientific”. • Words with negative connotation: We define words with negative connotation as those that describe physical objects or abstract concepts that people generally disvalue or avoid. Similarly as before, some of these words may express subjectivity (e.g., “disappointment”, “humiliation”), while many other are purely objective (e.g., “bedbug”, “arthritis, “funeral”). Note that this explicit and intentional inclusion of objective terms makes connotation lexicons differ from sentiment lexicons: most conventional sentiment lexicons have focused on subjective words by definition (e.g., Wilson et al. (2005b)), as many researchers use the term sentiment and subjectivity interchangeably (e.g., Wiebe et al. (2005)). 2.2 Connotative Predicate In this work, connotative predicates are those that exhibit selectional preference on the connotative polarity of some of their arguments. We emphasize that the polarity of connotative predicates does not coincide with the polarity of sentiment in conventional sentiment lexicons, as will be elaborated below. • Positively connotative predicate: In this work, we define positively connotative predicates as those that expect positive connotation in some arguments.</context>
<context position="9466" citStr="Wilson et al. (2005" startWordPosition="1374" endWordPosition="1377">eople typically congratulate something positive, and save something people care about. More examples are shown in Table 1. • Negatively connotative predicate: In this work, we define negatively connotative predicates as those that expect negative connotation in some arguments. For instance, predicates such as “prevent” or “suffer” tend to project negative connotation in the THEME argument. More examples are shown in Table 2. Note that positively connotative predicates are not necessarily positive sentiment words. For instance “save” is not a positive sentiment word in the lexicon published by Wilson et al. (2005b). Inversely, (strongly) positive sentiment words are not necessarily (strongly) positively connotative predicates, e.g., “illuminate”, “agree”. Likewise, negatively connotative predicates are not necessarily negative sentiment words. For instance, predicates such as “prevent”, “detect”, or “cause” are not negative sentiment words, but they tend to correlate with negative connotation in the THEME argument. Inversely, (strongly) negative sentiment words are not necessarily (strongly) negatively connotative predicates, e.g., “abandon” (“abandoned [something valuable]”). 3 Graph Representation I</context>
<context position="25983" citStr="Wilson et al., 2005" startWordPosition="4226" endWordPosition="4229"> 0 and γ are determined experimentally. We assign positive polarity to the word a, if σa+ &gt;&gt; σa_ and vice versa. 7.1 Comparison against Sentiment Lexicon The polarity defined in the connotation lexicon differs from that of conventional sentiment lexicons in which we aim to recognize more subtle sentiment that correlates with words. Nevertheless, we provide agreement statistics between our connotation lexicon and conventional sentiment lexicons for comparison purposes. We collect statistics with respect to the following two resources: General Inquirer (Stone and Hunt, 1963) and Opinion Finder (Wilson et al., 2005b). For polarity A E {+, −1, let countsentlex(λ) denote the total number of words labeled as A in a given sentiment lexicon, and let countagreement(λ) denote the total number of words labeled as A by both the given sentiment lexicon and our connotation lexicon. In addition, let countoverlap(λ) denote the total number of words that are labeled as A by our connotation lexicon that are also included in the reference lexicon with or without the same polarity. Then we compute precλ as follows: countagreement(λ) x 100 countoverlap(λ) We compare precλ % for three different segments of our lexicon: th</context>
<context position="35825" citStr="Wilson et al. (2005" startWordPosition="5829" endWordPosition="5832">of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2010)). This trend applies even to the most recent version of the lexicon of Wilson et al. (2005b). We conjecture that this trend of broader coverage sugges</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b. Recognizing contextual polarity in phraselevel sentiment analysis. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation. In</title>
<date>2002</date>
<tech>Technical Report CMU-CALD-02-107.</tech>
<institution>CarnegieMellon University.</institution>
<contexts>
<context position="34268" citStr="Zhu and Ghahramani, 2002" startWordPosition="5561" endWordPosition="5565">. For words that appear in both positive and negative connotation graphs, we determine the final polarity of such words as one with higher scores given by HITS or PageRank. We randomly select words that rank at 5% of top 100, top 1000, top 2000, and top 5000 by each algorithm for human judgment. We only evaluate the top performing algorithms – HITS-aT and Page-aF – and FREQ baseline. The stratified performance for each of these methods is given in Table 9. 8 Related Work Graph based approaches have been used in many previous research for lexicon induction. A technique named label propagation (Zhu and Ghahramani, 2002) has been used by Rao and Ravichandran (2009) and Velikovich et al. (2010), while random walk based approaches, PageRank in particular, have been used by Esuli and Sebastiani (2007). In our work, we explore the use of both HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) and 1100 Top # Average Positive Negative Str. Len. Str. Len. Str. Len. FREQ @100 73.5 87.3 72.2 91.1 74.7 83.5 @1000 51.8 78.6 44.4 75.6 81.8 90.9 @2000 66.9 74.7 73.1 84.2 57.3 60.0 @5000 61.5 81.3 61.4 84.1 62.0 70.0 HITS-aT @100 61.3 79.8 74.4 93.3 47.0 65.1 @1000 39.6 75.5 48.1 77.8 30.8 73.1 @2000 57.7 72.1 78.0 86</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. In Technical Report CMU-CALD-02-107. CarnegieMellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>