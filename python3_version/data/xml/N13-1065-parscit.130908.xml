<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002381">
<title confidence="0.99844">
Overcoming the Memory Bottleneck in Distributed Training of
Latent Variable Models of Text
</title>
<author confidence="0.999599">
Yi Yang Alexander Yates Doug Downey
</author>
<affiliation confidence="0.849378">
Northwestern University Temple University Northwestern University
Evanston, IL Philadelphia, PA Evanston, IL
</affiliation>
<email confidence="0.999411">
yiyang@eecs.northwestern.edu yates@temple.edu ddowney@eecs.northwestern.edu
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999465055555556">
Large unsupervised latent variable models
(LVMs) of text, such as Latent Dirichlet Al-
location models or Hidden Markov Models
(HMMs), are constructed using parallel train-
ing algorithms on computational clusters. The
memory required to hold LVM parameters
forms a bottleneck in training more powerful
models. In this paper, we show how the mem-
ory required for parallel LVM training can
be reduced by partitioning the training corpus
to minimize the number of unique words on
any computational node. We present a greedy
document partitioning technique for the task.
For large corpora, our approach reduces mem-
ory consumption by over 50%, and trains the
same models up to three times faster, when
compared with existing approaches for paral-
lel LVM training.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999856125">
Unsupervised latent variable models (LVMs) of text
are utilized extensively in natural language process-
ing (Griffiths and Steyvers, 2004; Ritter et al., 2010;
Downey et al., 2007; Huang and Yates, 2009; Li and
McCallum, 2005). LVM techniques include Latent
Dirichlet Allocation (LDA) (Blei et al., 2003), Hid-
den Markov Models (HMMs) (Rabiner, 1989), and
Probabilistic Latent Semantic Analysis (Hofmann,
1999), among others.
LVMs become more predictive as they are trained
on more text. However, training LVMs on mas-
sive corpora introduces computational challenges, in
terms of both time and space complexity. The time
complexity of LVM training has been addressed
through parallel training algorithms (Wolfe et al.,
2008; Chu et al., 2006; Das et al., 2007; Newman
et al., 2009; Ahmed et al., 2012; Asuncion et al.,
2011), which reduce LVM training time through the
use of large computational clusters.
However, the memory cost for training LVMs re-
mains a bottleneck. While LVM training makes se-
quential scans of the corpus (which can be stored on
disk), it requires consistent random access to model
parameters. Thus, the model parameters must be
stored in memory on each node. Because LVMs in-
clude a multinomial distribution over words for each
latent variable value, the model parameter space in-
creases with the number of latent variable values
times the vocabulary size. For large models (i.e.,
with many latent variable values) and large cor-
pora (with large vocabularies), the memory required
for training can exceed the limits of the commod-
ity servers comprising modern computational clus-
ters. Because model accuracy tends to increase with
both corpus size and model size (Ahuja and Downey,
2010; Huang and Yates, 2010), training accurate lan-
guage models requires that we overcome the mem-
ory bottleneck.
We present a simple technique for mitigating the
memory bottleneck in parallel LVM training. Ex-
isting parallelization schemes begin by partitioning
the training corpus arbitrarily across computational
nodes. In this paper, we show how to reduce mem-
ory footprint by instead partitioning the corpus to
minimize the number of unique words on each node
(and thereby minimize the number of parameters the
node must store). Because corpus partitioning is
a pre-processing step in parallel LVM training, our
</bodyText>
<page confidence="0.977805">
579
</page>
<subsectionHeader confidence="0.295328">
Proceedings of NAACL-HLT 2013, pages 579–584,
</subsectionHeader>
<bodyText confidence="0.984476304347826">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
technique can be applied to reduce the memory foot-
print of essentially any existing LVM or training ap-
proach. The accuracy of LVM training for a fixed
model size and corpus remains unchanged, but in-
telligent corpus partitioning allows us to train larger
and typically more accurate models using the same
memory capacity.
While the general minimization problem we en-
counter is NP-hard, we develop greedy approxima-
tions that work well. In experiments with both
HMM and LDA models, we show that our technique
offers large advantages over existing approaches in
terms of both memory footprint and execution time.
On a large corpus using 50 nodes in parallel, our best
partitioning method can reduce the memory required
per node to less than 1/10th that when training with-
out corpus partitioning, and to half that of a random
partitioning. Further, our approach reduces the train-
ing time of an existing parallel HMM codebase by
3x. Our work includes the release of our partitioning
codebase, and an associated codebase for the paral-
lel training of HMMs.1
</bodyText>
<sectionHeader confidence="0.955313" genericHeader="method">
2 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999507238095238">
In a distributed LVM system, a training corpus D =
{d1, d2, ... , dN} of documents is distributed across
T computational nodes. We first formalize the mem-
ory footprint on each node nt, where t = {1, ..., T}.
Let Dt C D denote the document collection on node
nt, and Vt be the number of word types (i.e., the
number of unique words) in Dt. Let K be the num-
ber of latent variable values in the LVM.
With these quantities, we can express how many
parameters must be held in memory on each com-
putational node for training LVMs in a distributed
environment. In practice, the LVM parameter space
is dominated by an observation model: a condi-
tional distribution over words given the latent vari-
able value. Thus, the observation model includes
K(Vt − 1) parameters. Different LVMs include var-
ious other parameters to specify the complete model.
For example, a first-order HMM includes additional
distributions for the initial latent variable and latent
variable transitions, for a total of K(Vt − 1) + K2
parameters. LDA, on the other hand, includes just a
</bodyText>
<footnote confidence="0.9572475">
1https://code.google.com/p/
corpus-partition/
</footnote>
<bodyText confidence="0.99151644">
single multinomial over the latent variables, making
a total of K(Vt − 1) + K − 1 parameters.
The LVM parameters comprise almost all of the
memory footprint for LVM training. Further, as the
examples above illustrate, the number of parame-
ters on each node tends to vary almost linearly with
Vt (in practice, Vt is typically larger than K by an
order of magnitude or more). Thus, in this paper
we attempt to minimize memory footprint by lim-
iting Vt on each computational node. We assume
the typical case in a distributed environment where
nodes are homogeneous, and thus our goal is to par-
tition the corpus such that the maximum vocabulary
size Vmax = maxTt�1Vt on any single node is mini-
mized. We define this task formally as follows.
Definition CORPUSPART: Given a corpus of
N documents D = {d1, d2,. . . , dN}, and T nodes,
partition D into T subsets D1, D2, ... , DT, such
that Vmax is minimized.
For illustration, consider the following small ex-
ample. Let corpus C contain three short docu-
ments {c1=“I live in Chicago”, c2=“I am studying
physics”, c3=“Chicago is a city in Illinois”}, and
consider partitioning C into 2 non-empty subsets,
i.e., T = 2. There are a total of three possibilities:
</bodyText>
<listItem confidence="0.999430666666667">
• {{c1, c2}, {c3}}. Vmax = 7
• {{c1, c3}, {c2}}. Vmax = 8
• {{c2, c3}, {c1}}. Vmax = 10
</listItem>
<bodyText confidence="0.985648555555555">
The decision problem version of
CORPUSPART is NP-Complete, by a re-
duction from independent task scheduling (Zhu and
Ibarra, 1999). In this paper, we develop greedy
algorithms for the task that are effective in practice.
We note that CORPUSPART has a submodu-
lar problem structure, where greedy algorithms are
often effective. Specifically, let |S |denote the vo-
cabulary size of a set of documents S, and let S&apos; C_
S. Then for any document c the following inequality
holds.
|S&apos; U c |− |S&apos; |&gt; |S U c |− |S|
That is, adding a document c to the subset S&apos; in-
creases vocabulary size at least as much as adding
c to S; the vocabulary size function is submodular.
The CORPUSPART task thus seeks a partition
of the data that minimizes the maximum of a set of
submodular functions. While formal approximation
</bodyText>
<page confidence="0.973876">
580
</page>
<bodyText confidence="0.999963857142857">
guarantees exist for similar problems, to our knowl-
edge none apply directly in our case. For example,
(Krause et al., 2007) considers maximizing the mini-
mum over a set of monotonic submodular functions,
which is the opposite of our problem. The distinct
task of minimizing a single submodular function has
been investigated in e.g. (Iwata et al., 2001).
It is important to emphasize that data partition-
ing is a pre-processing step, after which we can em-
ploy precisely the same Expectation-Maximization
(EM), sampling, or variational parameter learning
techniques as utilized in previous work. In fact,
for popular learning techniques including EM for
HMMs (Rabiner, 1989) and variational EM for LDA
(Wolfe et al., 2008), it can be shown that the param-
eter updates are independent of how the corpus is
partitioned. Thus, for those approaches our parti-
tioning is guaranteed to produce the same models as
any other partitioning method; i.e., model accuracy
is unchanged.
Lastly, we note that we target synchronized LVM
training, in which all nodes must finish each train-
ing iteration before any node can proceed to the
next iteration. Thus, we desire balanced partitions to
help ensure iterations have similar durations across
nodes. We achieve this in practice by constraining
each node to hold at most 3% more than Z/T to-
kens, where Z is the corpus size in tokens.
</bodyText>
<sectionHeader confidence="0.979286" genericHeader="method">
3 Corpus Partitioning Methods
</sectionHeader>
<bodyText confidence="0.999952833333333">
Our high-level greedy partitioning framework is
given in Algorithm 1. The algorithm requires an-
swering two key questions: How do we select which
document to allocate next? And, given a document,
on which node should it be placed? We present al-
ternative approaches to each question below.
</bodyText>
<table confidence="0.650506333333333">
Algorithm 1 Greedy Partitioning Framework
INPUT: {D, T}
OUTPUT: {D1, ... , DT}
Objective: Minimize Uma,
Initialize each subset Dt = 0 for T nodes
repeat
document selection:Select document d from D
node selection: Select node nt, and add d to Dt
Remove d from D
</table>
<tableCaption confidence="0.14557">
until all documents are allocated
</tableCaption>
<bodyText confidence="0.999525333333333">
A baseline partitioning method commonly used
in practice simply distributes documents across
nodes randomly. As our experiments show, this
baseline approach can be improved significantly.
In the following, set operations are interpreted as
applying to the set of unique words in a document.
For example, |dUDt |indicates the number of unique
word types in node nt after document d is added to
its document collection Dt.
</bodyText>
<subsectionHeader confidence="0.998161">
3.1 Document Selection
</subsectionHeader>
<bodyText confidence="0.999903459459459">
For document selection, previous work (Zhu and
Ibarra, 1999) proposed a heuristic DISSIMILARITY
method that selects the document d that is least sim-
ilar to any of the node document collections Dt,
where the similarity of d and Dt is calculated as:
Sim(d, DT) = |d n Dt|. The intuition behind the
heuristic is that dissimilar documents are more likely
to impact future node selection decisions. Assigning
the dissimilar documents earlier helps ensure that
more greedy node selections are informed by these
impactful assignments.
However, DISSIMILARITY has a prohibitive time
complexity of O(TN2), because we must compare
T nodes to an order of N documents for a total of
N iterations. To scale to large corpora, we propose
a novel BATCH DISSIMILARITY method. In BATCH
DISSIMILARITY, we select the top L most dissim-
ilar documents in each iteration, instead of just the
most dissimilar. Importantly, L is altered dynami-
cally: we begin with L = 1, and then increase L by
one for iteration i+1 iff using a batch size of L+1 in
iteration i would not have altered the algorithm’s ul-
timate selections (that is, if the most dissimilar doc-
ument in iteration i + 1 is in fact the L + 1st most
dissimilar in iteration i). In the ideal case where L
is incremented each iteration, BATCH DISSIMILAR
will have a reduced time complexity of O(TN3/2).
Our experiments revealed two key findings re-
garding document selection. First, BATCH DISSIM-
ILARITY provides a memory reduction within 0.1%
of that of DISSIMILARITY (on small corpora where
running DISSIMILARITY is tractable), but partitions
an estimated 2,600 times faster on our largest eval-
uation corpus. Second, we found that document se-
lection has relatively minor impact on memory foot-
print, providing a roughly 5% incremental benefit
over random document selection. Thus, although
</bodyText>
<page confidence="0.990864">
581
</page>
<bodyText confidence="0.999551">
we utilize BATCH DISSIMILARITY in the final sys-
tem we evaluate, simple random document selection
may be preferable in some practical settings.
</bodyText>
<subsectionHeader confidence="0.997468">
3.2 Node Selection
</subsectionHeader>
<bodyText confidence="0.999949705882353">
Given a selected document d, the MINIMUM
method proposed in previous work selects node nt
having the minimum number of word types after al-
location of d to nt (Zhu and Ibarra, 1999). That is,
MINIMUM minimizes |d U Dt|. Here, we introduce
an alternative node selection method JACCARD that
selects node nt maximizing the Jaccard index, de-
fined here as |d n Dt|/|d U Dt|.
Our experiments showed that our JACCARD node
selection method outperforms the MINIMUM selec-
tion method. In fact, for the largest corpora used
in our experiments, JACCARD offered an 12.9%
larger reduction in Vmax than MINIMUM. Our
proposed system, referred to as BJAC, utilizes
our best-performing strategies for document selec-
tion (BATCH DISSIMILARITY) and node selection
(JACCARD).
</bodyText>
<sectionHeader confidence="0.952481" genericHeader="method">
4 Evaluation of Partitioning Methods
</sectionHeader>
<bodyText confidence="0.9899279">
We evaluate our partitioning method against the
baseline and Z&amp;I, the best performing scalable
method from previous work, which uses random
document selection and MINIMUM node selection
(Zhu and Ibarra, 1999). We evaluate on three cor-
pora (Table 1): the Brown corpus of newswire text
(Kucera and Francis, 1967), the Reuters Corpus Vol-
ume1 (RCV1) (Lewis et al., 2004), and a larger Web-
Sent corpus of sentences gathered from the Web
(Downey et al., 2007).
</bodyText>
<table confidence="0.99442825">
Corpus N V Z
Brown 57339 56058 1161183
RCV1 804414 288062 99702278
Web-Sent 2747282 214588 58666983
</table>
<tableCaption confidence="0.997342">
Table 1: Characteristics of the three corpora. N = #
</tableCaption>
<bodyText confidence="0.667399666666667">
of documents, V = # of word types, Z = # of tokens.
We treat each sentence as a document in the Brown
and Web-Sent corpora.
Table 2 shows how the maximum word type size
Vmax varies for each method and corpus, for T = 50
nodes. BJAC significantly decreases Vmax over the
</bodyText>
<table confidence="0.99637075">
Corpus baseline Z&amp;I BJAC
Brown 6368 5714 4369
RCV1 49344 32136 24923
Web-Sent 72626 45989 34754
</table>
<tableCaption confidence="0.992681">
Table 2: Maximum word type size Vmax for each
</tableCaption>
<bodyText confidence="0.968618333333333">
partitioning method, for each corpus. For the larger
corpora, BJAC reduces Vmax by over 50% compared
to the baseline, and by 23% compared to Z&amp;I.
random partitioning baseline typically employed in
practice. Furthermore, the advantage of BJAC over
the baseline is maintained as more computational
nodes are utilized, as illustrated in Figure 1. BJac
reduces Vmax by a larger factor over the baseline as
more computational nodes are employed.
</bodyText>
<figureCaption confidence="0.896707">
Figure 1: Effects of partitioning as the number of
computational nodes increases (Web-Sent corpus).
With 100 nodes, BJac’s Vmax is half that of the base-
line, and 1/10th of the full corpus vocabulary size.
</figureCaption>
<sectionHeader confidence="0.947043" genericHeader="method">
5 Evaluation in Parallel LVM Systems
</sectionHeader>
<bodyText confidence="0.999909714285714">
We now turn to an evaluation of our corpus parti-
tioning within parallel LVM training systems.
Table 3 shows the memory footprint required for
HMM and LDA training for three different partition-
ing methods. We compare BJAC with the random
partitioning baseline, Zhu’s method, and with all-
words, the straightforward approach of simply stor-
ing parameters for the entire corpus vocabulary on
every node (Ahuja and Downey, 2010; Asuncion et
al., 2011). All-words has the same memory footprint
as when training on a single node.
For large corpora, BJAC reduces memory size
per node by approximately a factor of two over the
random baseline, and by a factor of 8-11 over all-
</bodyText>
<figure confidence="0.999270307692308">
10 20 30 40 50 60 70 80 90 100
number of nodes
Vmax by baseline
Vmax by BJac
140,000
number of word types
120,000
00,000
80,000
60,000
40,000
20,000
0
</figure>
<page confidence="0.989878">
582
</page>
<table confidence="0.999487857142857">
LVM Corpus all-words baseline BJAC
HMM Brown 435.3 56.2 40.9
RCV1 2205.4 384.1 197.8
Web-Sent 1644.8 561.7 269.7
LDA Brown 427.7 48.6 33.3
RCV1 2197.7 376.5 190.1
Web-Sent 1637.2 554.1 262.1
</table>
<tableCaption confidence="0.822611">
Table 3: Memory footprint of computational nodes
in megabytes(MB), using 50 computational nodes.
Both models utilize 1000 latent variable values.
</tableCaption>
<bodyText confidence="0.999699161290323">
words. The results demonstrate that in addition to
the well-known savings in computation time offered
by parallel LVM training, distributed computation
also significantly reduces the memory footprint on
each node. In fact, for the RCV1 corpus, BJAC re-
duces memory footprint to less than 1/10th that of
training with all words on each computational node.
We next evaluate the execution time for an itera-
tion of model training. Here, we use a parallel im-
plementation of HMMs, and measure iteration time
for training on the Web-sent corpus with 50 hidden
states as the number of computational nodes varies.
We compare against the random baseline and against
the all-words approach utilized in an existing paral-
lel HMM codebase (Ahuja and Downey, 2010). The
results are shown in Table 4. Moving beyond the all-
words method to exploit corpus partitioning reduces
training iteration time, by a factor of two to three.
However, differences in partitioning methods have
only small effects in iteration time: BJAC has essen-
tially the same iteration time as the random baseline
in this experiment.
It is also important to consider the additional time
required to execute the partitioning methods them-
selves. However, in practice this additional time
is negligible. For example, BJAC can partition the
Web-sent corpus in 368 seconds, using a single com-
putational node. By contrast, training a 200-state
HMM on the same corpus requires over a hundred
CPU-days. Thus, BJAC’s time to partition has a neg-
ligible impact on total training time.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9988725">
The CORPUSPART task has some similarities
to the graph partitioning task investigated in other
</bodyText>
<table confidence="0.998115">
T all-words baseline BJAC
25 4510 1295 1289
50 2248 740 735
100 1104 365 364
200 394 196 192
</table>
<tableCaption confidence="0.9096215">
Table 4: Average iteration time(sec) for training an
HMM with 50 hidden states on Web-Sent. Partition-
ing with BJAC outperforms all-words, which stores
parameters for all word types on each node.
</tableCaption>
<bodyText confidence="0.999106631578947">
parallelization research (Hendrickson and Kolda,
2000). However, our LVM training task differs sig-
nificantly from those in which graph partitioning is
typically employed. Specifically, graph partitioning
tends to be used for scientific computing applica-
tions where communication is the bottleneck. The
graph algorithms focus on creating balanced parti-
tions that minimize the cut edge weight, because
edge weights represent communication costs to be
minimized. By contrast, in our LVM training task,
memory consumption is the bottleneck and commu-
nication costs are less significant.
Zhu &amp; Ibarra (1999) present theoretical results
and propose techniques for the general partitioning
task we address. In contrast to that work, we fo-
cus on the case where the data to be partitioned is a
large corpus of text. In this setting, we show that our
heuristics partition faster and provide smaller mem-
ory footprint than those of (Zhu and Ibarra, 1999).
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999961625">
We presented a general corpus partitioning tech-
nique which can be exploited in LVM training to re-
duce memory footprint and training time. We eval-
uated the partitioning method’s performance, and
showed that for large corpora, our approach reduces
memory consumption by over 50% and learns mod-
els up to three times faster when compared with ex-
isting implementations for parallel LVM training.
</bodyText>
<sectionHeader confidence="0.998299" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994208333333333">
This work was supported in part by NSF Grants
IIS-101675 and IIS-1065397, and DARPA contract
D11AP00268.
</bodyText>
<page confidence="0.998374">
583
</page>
<sectionHeader confidence="0.989105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999322319587629">
Amr Ahmed, Moahmed Aly, Joseph Gonzalez, Shra-
van Narayanamurthy, and Alexander J. Smola. 2012.
Scalable inference in latent variable models. In Pro-
ceedings of the fifth ACM international conference on
Web search and data mining, WSDM ’12, pages 123–
132, New York, NY, USA. ACM.
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Human Language Technologies: Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL HLT).
Arthur U. Asuncion, Padhraic Smyth, and Max Welling.
2011. Asynchronous distributed estimation of topic
models for document analysis. Statistical Methodol-
ogy, 8(1):3 – 17. Advances in Data Mining and Statis-
tical Learning.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Olukotun.
2006. Map-Reduce for machine learning on multicore.
In Bernhard Sch¨olkopf, John C. Platt, and Thomas
Hoffman, editors, NIPS, pages 281–288. MIT Press.
Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personaliza-
tion: scalable online collaborative filtering. In Pro-
ceedings of the 16th international conference on World
Wide Web, WWW ’07, pages 271–280, New York, NY,
USA. ACM.
D. Downey, S. Schoenmackers, and O. Etzioni. 2007.
Sparse information extraction: Unsupervised language
models to the rescue. In Proc. of ACL.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235, April.
Bruce Hendrickson and Tamara G Kolda. 2000. Graph
partitioning models for parallel computing. Parallel
computing, 26(12):1519–1534.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ’99, pages
50–57, New York, NY, USA. ACM.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Satoru Iwata, Lisa Fleischer, and Satoru Fujishige. 2001.
A combinatorial strongly polynomial algorithm for
minimizing submodular functions. J. ACM, 48:761–
777.
Andreas Krause, H. Brendan Mcmahan, Google Inc, Car-
los Guestrin, and Anupam Gupta. 2007. Selecting
observations against adversarial objectives. Technical
report, In NIPS, 2007a.
H. Kucera and W. N. Francis. 1967. Computational
analysis of present-day American English. Brown
University Press, Providence, RI.
David D. Lewis, Yiming Yang, Tony G. Rose, Fan Li,
G. Dietterich, and Fan Li. 2004. Rcv1: A new bench-
mark collection for text categorization research. Jour-
nal of Machine Learning Research, 5:361–397.
Wei Li and Andrew McCallum. 2005. Semi-supervised
sequence modeling with syntactic topic models. In
Proceedings of the 20th national conference on Artifi-
cial intelligence - Volume 2, AAAI’05, pages 813–818.
AAAI Press.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801–1828.
L. R. Rabiner. 1989. A tutorial on Hidden Markov
Models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257–286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ’10, pages 172–180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Jason Wolfe, Aria Haghighi, and Dan Klein. 2008. Fully
distributed EM for very large datasets. In Proceed-
ings of the 25th international conference on Machine
learning, ICML ’08, pages 1184–1191, New York,
NY, USA. ACM.
Huican Zhu and Oscar H. Ibarra. 1999. On some ap-
proximation algorithms for the set partition problem.
In Proceedings of the 15th Triennial Conf. of Int. Fed-
eration of Operations Research Society.
</reference>
<page confidence="0.998599">
584
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.677054">
<title confidence="0.994414">Overcoming the Memory Bottleneck in Distributed Training Latent Variable Models of Text</title>
<author confidence="0.999355">Yi Yang Alexander Yates Doug Downey</author>
<affiliation confidence="0.999993">Northwestern University Temple University Northwestern University</affiliation>
<address confidence="0.829837">Evanston, IL Philadelphia, PA Evanston, IL</address>
<email confidence="0.999113">yiyang@eecs.northwestern.eduyates@temple.eduddowney@eecs.northwestern.edu</email>
<abstract confidence="0.990441947368421">Large unsupervised latent variable models (LVMs) of text, such as Latent Dirichlet Allocation models or Hidden Markov Models (HMMs), are constructed using parallel training algorithms on computational clusters. The memory required to hold LVM parameters forms a bottleneck in training more powerful models. In this paper, we show how the memory required for parallel LVM training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node. We present a greedy document partitioning technique for the task. For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Moahmed Aly</author>
<author>Joseph Gonzalez</author>
<author>Shravan Narayanamurthy</author>
<author>Alexander J Smola</author>
</authors>
<title>Scalable inference in latent variable models.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12,</booktitle>
<pages>123--132</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1887" citStr="Ahmed et al., 2012" startWordPosition="277" endWordPosition="280"> Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), which reduce LVM training time through the use of large computational clusters. However, the memory cost for training LVMs remains a bottleneck. While LVM training makes sequential scans of the corpus (which can be stored on disk), it requires consistent random access to model parameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variable values times the vocabulary size. For large models (i.e.</context>
</contexts>
<marker>Ahmed, Aly, Gonzalez, Narayanamurthy, Smola, 2012</marker>
<rawString>Amr Ahmed, Moahmed Aly, Joseph Gonzalez, Shravan Narayanamurthy, and Alexander J. Smola. 2012. Scalable inference in latent variable models. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12, pages 123– 132, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arun Ahuja</author>
<author>Doug Downey</author>
</authors>
<title>Improved extraction assessment through better language models.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT).</booktitle>
<contexts>
<context position="2791" citStr="Ahuja and Downey, 2010" startWordPosition="423" endWordPosition="426">andom access to model parameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variable values times the vocabulary size. For large models (i.e., with many latent variable values) and large corpora (with large vocabularies), the memory required for training can exceed the limits of the commodity servers comprising modern computational clusters. Because model accuracy tends to increase with both corpus size and model size (Ahuja and Downey, 2010; Huang and Yates, 2010), training accurate language models requires that we overcome the memory bottleneck. We present a simple technique for mitigating the memory bottleneck in parallel LVM training. Existing parallelization schemes begin by partitioning the training corpus arbitrarily across computational nodes. In this paper, we show how to reduce memory footprint by instead partitioning the corpus to minimize the number of unique words on each node (and thereby minimize the number of parameters the node must store). Because corpus partitioning is a pre-processing step in parallel LVM trai</context>
<context position="15107" citStr="Ahuja and Downey, 2010" startWordPosition="2482" endWordPosition="2485">number of computational nodes increases (Web-Sent corpus). With 100 nodes, BJac’s Vmax is half that of the baseline, and 1/10th of the full corpus vocabulary size. 5 Evaluation in Parallel LVM Systems We now turn to an evaluation of our corpus partitioning within parallel LVM training systems. Table 3 shows the memory footprint required for HMM and LDA training for three different partitioning methods. We compare BJAC with the random partitioning baseline, Zhu’s method, and with allwords, the straightforward approach of simply storing parameters for the entire corpus vocabulary on every node (Ahuja and Downey, 2010; Asuncion et al., 2011). All-words has the same memory footprint as when training on a single node. For large corpora, BJAC reduces memory size per node by approximately a factor of two over the random baseline, and by a factor of 8-11 over all10 20 30 40 50 60 70 80 90 100 number of nodes Vmax by baseline Vmax by BJac 140,000 number of word types 120,000 00,000 80,000 60,000 40,000 20,000 0 582 LVM Corpus all-words baseline BJAC HMM Brown 435.3 56.2 40.9 RCV1 2205.4 384.1 197.8 Web-Sent 1644.8 561.7 269.7 LDA Brown 427.7 48.6 33.3 RCV1 2197.7 376.5 190.1 Web-Sent 1637.2 554.1 262.1 Table 3: </context>
<context position="16592" citStr="Ahuja and Downey, 2010" startWordPosition="2730" endWordPosition="2733">g, distributed computation also significantly reduces the memory footprint on each node. In fact, for the RCV1 corpus, BJAC reduces memory footprint to less than 1/10th that of training with all words on each computational node. We next evaluate the execution time for an iteration of model training. Here, we use a parallel implementation of HMMs, and measure iteration time for training on the Web-sent corpus with 50 hidden states as the number of computational nodes varies. We compare against the random baseline and against the all-words approach utilized in an existing parallel HMM codebase (Ahuja and Downey, 2010). The results are shown in Table 4. Moving beyond the allwords method to exploit corpus partitioning reduces training iteration time, by a factor of two to three. However, differences in partitioning methods have only small effects in iteration time: BJAC has essentially the same iteration time as the random baseline in this experiment. It is also important to consider the additional time required to execute the partitioning methods themselves. However, in practice this additional time is negligible. For example, BJAC can partition the Web-sent corpus in 368 seconds, using a single computation</context>
</contexts>
<marker>Ahuja, Downey, 2010</marker>
<rawString>Arun Ahuja and Doug Downey. 2010. Improved extraction assessment through better language models. In Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur U Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Asynchronous distributed estimation of topic models for document analysis.</title>
<date>2011</date>
<booktitle>Statistical Methodology, 8(1):3 – 17. Advances in Data Mining and Statistical Learning.</booktitle>
<contexts>
<context position="1911" citStr="Asuncion et al., 2011" startWordPosition="281" endWordPosition="284">09; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), which reduce LVM training time through the use of large computational clusters. However, the memory cost for training LVMs remains a bottleneck. While LVM training makes sequential scans of the corpus (which can be stored on disk), it requires consistent random access to model parameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variable values times the vocabulary size. For large models (i.e., with many latent varia</context>
<context position="15131" citStr="Asuncion et al., 2011" startWordPosition="2486" endWordPosition="2489">nodes increases (Web-Sent corpus). With 100 nodes, BJac’s Vmax is half that of the baseline, and 1/10th of the full corpus vocabulary size. 5 Evaluation in Parallel LVM Systems We now turn to an evaluation of our corpus partitioning within parallel LVM training systems. Table 3 shows the memory footprint required for HMM and LDA training for three different partitioning methods. We compare BJAC with the random partitioning baseline, Zhu’s method, and with allwords, the straightforward approach of simply storing parameters for the entire corpus vocabulary on every node (Ahuja and Downey, 2010; Asuncion et al., 2011). All-words has the same memory footprint as when training on a single node. For large corpora, BJAC reduces memory size per node by approximately a factor of two over the random baseline, and by a factor of 8-11 over all10 20 30 40 50 60 70 80 90 100 number of nodes Vmax by baseline Vmax by BJac 140,000 number of word types 120,000 00,000 80,000 60,000 40,000 20,000 0 582 LVM Corpus all-words baseline BJAC HMM Brown 435.3 56.2 40.9 RCV1 2205.4 384.1 197.8 Web-Sent 1644.8 561.7 269.7 LDA Brown 427.7 48.6 33.3 RCV1 2197.7 376.5 190.1 Web-Sent 1637.2 554.1 262.1 Table 3: Memory footprint of comp</context>
</contexts>
<marker>Asuncion, Smyth, Welling, 2011</marker>
<rawString>Arthur U. Asuncion, Padhraic Smyth, and Max Welling. 2011. Asynchronous distributed estimation of topic models for document analysis. Statistical Methodology, 8(1):3 – 17. Advances in Data Mining and Statistical Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="1393" citStr="Blei et al., 2003" startWordPosition="199" endWordPosition="202">of unique words on any computational node. We present a greedy document partitioning technique for the task. For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training. 1 Introduction Unsupervised latent variable models (LVMs) of text are utilized extensively in natural language processing (Griffiths and Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), which reduce LVM training time through the use of large computational clusters. </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheng T Chu</author>
<author>Sang K Kim</author>
<author>Yi A Lin</author>
<author>Yuanyuan Yu</author>
<author>Gary R Bradski</author>
<author>Andrew Y Ng</author>
<author>Kunle Olukotun</author>
</authors>
<title>Map-Reduce for machine learning on multicore.</title>
<date>2006</date>
<pages>281--288</pages>
<editor>In Bernhard Sch¨olkopf, John C. Platt, and Thomas Hoffman, editors, NIPS,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1828" citStr="Chu et al., 2006" startWordPosition="265" endWordPosition="268">Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), which reduce LVM training time through the use of large computational clusters. However, the memory cost for training LVMs remains a bottleneck. While LVM training makes sequential scans of the corpus (which can be stored on disk), it requires consistent random access to model parameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variab</context>
</contexts>
<marker>Chu, Kim, Lin, Yu, Bradski, Ng, Olukotun, 2006</marker>
<rawString>Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu, Gary R. Bradski, Andrew Y. Ng, and Kunle Olukotun. 2006. Map-Reduce for machine learning on multicore. In Bernhard Sch¨olkopf, John C. Platt, and Thomas Hoffman, editors, NIPS, pages 281–288. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhinandan S Das</author>
<author>Mayur Datar</author>
<author>Ashutosh Garg</author>
<author>Shyam Rajaram</author>
</authors>
<title>Google news personalization: scalable online collaborative filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web, WWW ’07,</booktitle>
<pages>271--280</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1846" citStr="Das et al., 2007" startWordPosition="269" endWordPosition="272">tter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), which reduce LVM training time through the use of large computational clusters. However, the memory cost for training LVMs remains a bottleneck. While LVM training makes sequential scans of the corpus (which can be stored on disk), it requires consistent random access to model parameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variable values times th</context>
</contexts>
<marker>Das, Datar, Garg, Rajaram, 2007</marker>
<rawString>Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google news personalization: scalable online collaborative filtering. In Proceedings of the 16th international conference on World Wide Web, WWW ’07, pages 271–280, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>S Schoenmackers</author>
<author>O Etzioni</author>
</authors>
<title>Sparse information extraction: Unsupervised language models to the rescue.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1268" citStr="Downey et al., 2007" startWordPosition="180" endWordPosition="183">w how the memory required for parallel LVM training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node. We present a greedy document partitioning technique for the task. For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training. 1 Introduction Unsupervised latent variable models (LVMs) of text are utilized extensively in natural language processing (Griffiths and Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009;</context>
<context position="13436" citStr="Downey et al., 2007" startWordPosition="2201" endWordPosition="2204">red to as BJAC, utilizes our best-performing strategies for document selection (BATCH DISSIMILARITY) and node selection (JACCARD). 4 Evaluation of Partitioning Methods We evaluate our partitioning method against the baseline and Z&amp;I, the best performing scalable method from previous work, which uses random document selection and MINIMUM node selection (Zhu and Ibarra, 1999). We evaluate on three corpora (Table 1): the Brown corpus of newswire text (Kucera and Francis, 1967), the Reuters Corpus Volume1 (RCV1) (Lewis et al., 2004), and a larger WebSent corpus of sentences gathered from the Web (Downey et al., 2007). Corpus N V Z Brown 57339 56058 1161183 RCV1 804414 288062 99702278 Web-Sent 2747282 214588 58666983 Table 1: Characteristics of the three corpora. N = # of documents, V = # of word types, Z = # of tokens. We treat each sentence as a document in the Brown and Web-Sent corpora. Table 2 shows how the maximum word type size Vmax varies for each method and corpus, for T = 50 nodes. BJAC significantly decreases Vmax over the Corpus baseline Z&amp;I BJAC Brown 6368 5714 4369 RCV1 49344 32136 24923 Web-Sent 72626 45989 34754 Table 2: Maximum word type size Vmax for each partitioning method, for each cor</context>
</contexts>
<marker>Downey, Schoenmackers, Etzioni, 2007</marker>
<rawString>D. Downey, S. Schoenmackers, and O. Etzioni. 2007. Sparse information extraction: Unsupervised language models to the rescue. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="1226" citStr="Griffiths and Steyvers, 2004" startWordPosition="172" endWordPosition="175">raining more powerful models. In this paper, we show how the memory required for parallel LVM training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node. We present a greedy document partitioning technique for the task. For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training. 1 Introduction Unsupervised latent variable models (LVMs) of text are utilized extensively in natural language processing (Griffiths and Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 20</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hendrickson</author>
<author>Tamara G Kolda</author>
</authors>
<title>Graph partitioning models for parallel computing.</title>
<date>2000</date>
<booktitle>Parallel computing,</booktitle>
<pages>26--12</pages>
<contexts>
<context position="17823" citStr="Hendrickson and Kolda, 2000" startWordPosition="2930" endWordPosition="2933">node. By contrast, training a 200-state HMM on the same corpus requires over a hundred CPU-days. Thus, BJAC’s time to partition has a negligible impact on total training time. 6 Related Work The CORPUSPART task has some similarities to the graph partitioning task investigated in other T all-words baseline BJAC 25 4510 1295 1289 50 2248 740 735 100 1104 365 364 200 394 196 192 Table 4: Average iteration time(sec) for training an HMM with 50 hidden states on Web-Sent. Partitioning with BJAC outperforms all-words, which stores parameters for all word types on each node. parallelization research (Hendrickson and Kolda, 2000). However, our LVM training task differs significantly from those in which graph partitioning is typically employed. Specifically, graph partitioning tends to be used for scientific computing applications where communication is the bottleneck. The graph algorithms focus on creating balanced partitions that minimize the cut edge weight, because edge weights represent communication costs to be minimized. By contrast, in our LVM training task, memory consumption is the bottleneck and communication costs are less significant. Zhu &amp; Ibarra (1999) present theoretical results and propose techniques f</context>
</contexts>
<marker>Hendrickson, Kolda, 2000</marker>
<rawString>Bruce Hendrickson and Tamara G Kolda. 2000. Graph partitioning models for parallel computing. Parallel computing, 26(12):1519–1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99,</booktitle>
<pages>50--57</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1498" citStr="Hofmann, 1999" startWordPosition="215" endWordPosition="216">For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training. 1 Introduction Unsupervised latent variable models (LVMs) of text are utilized extensively in natural language processing (Griffiths and Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), which reduce LVM training time through the use of large computational clusters. However, the memory cost for training LVMs remains a bottleneck. While LVM training makes sequential scan</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99, pages 50–57, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1291" citStr="Huang and Yates, 2009" startWordPosition="184" endWordPosition="187">ired for parallel LVM training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node. We present a greedy document partitioning technique for the task. For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training. 1 Introduction Unsupervised latent variable models (LVMs) of text are utilized extensively in natural language processing (Griffiths and Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; As</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence labeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Exploring representation-learning approaches to domain adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural Language Processing (DANLP).</booktitle>
<contexts>
<context position="2815" citStr="Huang and Yates, 2010" startWordPosition="427" endWordPosition="430">rameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variable values times the vocabulary size. For large models (i.e., with many latent variable values) and large corpora (with large vocabularies), the memory required for training can exceed the limits of the commodity servers comprising modern computational clusters. Because model accuracy tends to increase with both corpus size and model size (Ahuja and Downey, 2010; Huang and Yates, 2010), training accurate language models requires that we overcome the memory bottleneck. We present a simple technique for mitigating the memory bottleneck in parallel LVM training. Existing parallelization schemes begin by partitioning the training corpus arbitrarily across computational nodes. In this paper, we show how to reduce memory footprint by instead partitioning the corpus to minimize the number of unique words on each node (and thereby minimize the number of parameters the node must store). Because corpus partitioning is a pre-processing step in parallel LVM training, our 579 Proceeding</context>
</contexts>
<marker>Huang, Yates, 2010</marker>
<rawString>Fei Huang and Alexander Yates. 2010. Exploring representation-learning approaches to domain adaptation. In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural Language Processing (DANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Iwata</author>
<author>Lisa Fleischer</author>
<author>Satoru Fujishige</author>
</authors>
<title>A combinatorial strongly polynomial algorithm for minimizing submodular functions.</title>
<date>2001</date>
<journal>J. ACM,</journal>
<pages>48--761</pages>
<contexts>
<context position="8141" citStr="Iwata et al., 2001" startWordPosition="1335" endWordPosition="1338"> the subset S&apos; increases vocabulary size at least as much as adding c to S; the vocabulary size function is submodular. The CORPUSPART task thus seeks a partition of the data that minimizes the maximum of a set of submodular functions. While formal approximation 580 guarantees exist for similar problems, to our knowledge none apply directly in our case. For example, (Krause et al., 2007) considers maximizing the minimum over a set of monotonic submodular functions, which is the opposite of our problem. The distinct task of minimizing a single submodular function has been investigated in e.g. (Iwata et al., 2001). It is important to emphasize that data partitioning is a pre-processing step, after which we can employ precisely the same Expectation-Maximization (EM), sampling, or variational parameter learning techniques as utilized in previous work. In fact, for popular learning techniques including EM for HMMs (Rabiner, 1989) and variational EM for LDA (Wolfe et al., 2008), it can be shown that the parameter updates are independent of how the corpus is partitioned. Thus, for those approaches our partitioning is guaranteed to produce the same models as any other partitioning method; i.e., model accurac</context>
</contexts>
<marker>Iwata, Fleischer, Fujishige, 2001</marker>
<rawString>Satoru Iwata, Lisa Fleischer, and Satoru Fujishige. 2001. A combinatorial strongly polynomial algorithm for minimizing submodular functions. J. ACM, 48:761– 777.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Krause</author>
<author>H Brendan Mcmahan</author>
<author>Google Inc</author>
<author>Carlos Guestrin</author>
<author>Anupam Gupta</author>
</authors>
<title>Selecting observations against adversarial objectives. Technical report,</title>
<date>2007</date>
<booktitle>In NIPS, 2007a.</booktitle>
<contexts>
<context position="7912" citStr="Krause et al., 2007" startWordPosition="1298" endWordPosition="1301">re often effective. Specifically, let |S |denote the vocabulary size of a set of documents S, and let S&apos; C_ S. Then for any document c the following inequality holds. |S&apos; U c |− |S&apos; |&gt; |S U c |− |S| That is, adding a document c to the subset S&apos; increases vocabulary size at least as much as adding c to S; the vocabulary size function is submodular. The CORPUSPART task thus seeks a partition of the data that minimizes the maximum of a set of submodular functions. While formal approximation 580 guarantees exist for similar problems, to our knowledge none apply directly in our case. For example, (Krause et al., 2007) considers maximizing the minimum over a set of monotonic submodular functions, which is the opposite of our problem. The distinct task of minimizing a single submodular function has been investigated in e.g. (Iwata et al., 2001). It is important to emphasize that data partitioning is a pre-processing step, after which we can employ precisely the same Expectation-Maximization (EM), sampling, or variational parameter learning techniques as utilized in previous work. In fact, for popular learning techniques including EM for HMMs (Rabiner, 1989) and variational EM for LDA (Wolfe et al., 2008), it</context>
</contexts>
<marker>Krause, Mcmahan, Inc, Guestrin, Gupta, 2007</marker>
<rawString>Andreas Krause, H. Brendan Mcmahan, Google Inc, Carlos Guestrin, and Anupam Gupta. 2007. Selecting observations against adversarial objectives. Technical report, In NIPS, 2007a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kucera</author>
<author>W N Francis</author>
</authors>
<title>Computational analysis of present-day American English.</title>
<date>1967</date>
<publisher>Brown University Press,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="13294" citStr="Kucera and Francis, 1967" startWordPosition="2175" endWordPosition="2178">n fact, for the largest corpora used in our experiments, JACCARD offered an 12.9% larger reduction in Vmax than MINIMUM. Our proposed system, referred to as BJAC, utilizes our best-performing strategies for document selection (BATCH DISSIMILARITY) and node selection (JACCARD). 4 Evaluation of Partitioning Methods We evaluate our partitioning method against the baseline and Z&amp;I, the best performing scalable method from previous work, which uses random document selection and MINIMUM node selection (Zhu and Ibarra, 1999). We evaluate on three corpora (Table 1): the Brown corpus of newswire text (Kucera and Francis, 1967), the Reuters Corpus Volume1 (RCV1) (Lewis et al., 2004), and a larger WebSent corpus of sentences gathered from the Web (Downey et al., 2007). Corpus N V Z Brown 57339 56058 1161183 RCV1 804414 288062 99702278 Web-Sent 2747282 214588 58666983 Table 1: Characteristics of the three corpora. N = # of documents, V = # of word types, Z = # of tokens. We treat each sentence as a document in the Brown and Web-Sent corpora. Table 2 shows how the maximum word type size Vmax varies for each method and corpus, for T = 50 nodes. BJAC significantly decreases Vmax over the Corpus baseline Z&amp;I BJAC Brown 63</context>
</contexts>
<marker>Kucera, Francis, 1967</marker>
<rawString>H. Kucera and W. N. Francis. 1967. Computational analysis of present-day American English. Brown University Press, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
<author>G Dietterich</author>
<author>Fan Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="13350" citStr="Lewis et al., 2004" startWordPosition="2185" endWordPosition="2188">RD offered an 12.9% larger reduction in Vmax than MINIMUM. Our proposed system, referred to as BJAC, utilizes our best-performing strategies for document selection (BATCH DISSIMILARITY) and node selection (JACCARD). 4 Evaluation of Partitioning Methods We evaluate our partitioning method against the baseline and Z&amp;I, the best performing scalable method from previous work, which uses random document selection and MINIMUM node selection (Zhu and Ibarra, 1999). We evaluate on three corpora (Table 1): the Brown corpus of newswire text (Kucera and Francis, 1967), the Reuters Corpus Volume1 (RCV1) (Lewis et al., 2004), and a larger WebSent corpus of sentences gathered from the Web (Downey et al., 2007). Corpus N V Z Brown 57339 56058 1161183 RCV1 804414 288062 99702278 Web-Sent 2747282 214588 58666983 Table 1: Characteristics of the three corpora. N = # of documents, V = # of word types, Z = # of tokens. We treat each sentence as a document in the Brown and Web-Sent corpora. Table 2 shows how the maximum word type size Vmax varies for each method and corpus, for T = 50 nodes. BJAC significantly decreases Vmax over the Corpus baseline Z&amp;I BJAC Brown 6368 5714 4369 RCV1 49344 32136 24923 Web-Sent 72626 45989</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, Dietterich, Li, 2004</marker>
<rawString>David D. Lewis, Yiming Yang, Tony G. Rose, Fan Li, G. Dietterich, and Fan Li. 2004. Rcv1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Semi-supervised sequence modeling with syntactic topic models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th national conference on Artificial intelligence - Volume 2, AAAI’05,</booktitle>
<pages>813--818</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1315" citStr="Li and McCallum, 2005" startWordPosition="188" endWordPosition="191">raining can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node. We present a greedy document partitioning technique for the task. For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training. 1 Introduction Unsupervised latent variable models (LVMs) of text are utilized extensively in natural language processing (Griffiths and Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), wh</context>
</contexts>
<marker>Li, McCallum, 2005</marker>
<rawString>Wei Li and Andrew McCallum. 2005. Semi-supervised sequence modeling with syntactic topic models. In Proceedings of the 20th national conference on Artificial intelligence - Volume 2, AAAI’05, pages 813–818. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--1801</pages>
<contexts>
<context position="1867" citStr="Newman et al., 2009" startWordPosition="273" endWordPosition="276"> Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), which reduce LVM training time through the use of large computational clusters. However, the memory cost for training LVMs remains a bottleneck. While LVM training makes sequential scans of the corpus (which can be stored on disk), it requires consistent random access to model parameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variable values times the vocabulary size. Fo</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>David Newman, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2009. Distributed algorithms for topic models. Journal of Machine Learning Research, 10:1801–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on Hidden Markov Models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="1438" citStr="Rabiner, 1989" startWordPosition="208" endWordPosition="209">sent a greedy document partitioning technique for the task. For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training. 1 Introduction Unsupervised latent variable models (LVMs) of text are utilized extensively in natural language processing (Griffiths and Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), which reduce LVM training time through the use of large computational clusters. However, the memory cost for training LVMs re</context>
<context position="8460" citStr="Rabiner, 1989" startWordPosition="1384" endWordPosition="1385">one apply directly in our case. For example, (Krause et al., 2007) considers maximizing the minimum over a set of monotonic submodular functions, which is the opposite of our problem. The distinct task of minimizing a single submodular function has been investigated in e.g. (Iwata et al., 2001). It is important to emphasize that data partitioning is a pre-processing step, after which we can employ precisely the same Expectation-Maximization (EM), sampling, or variational parameter learning techniques as utilized in previous work. In fact, for popular learning techniques including EM for HMMs (Rabiner, 1989) and variational EM for LDA (Wolfe et al., 2008), it can be shown that the parameter updates are independent of how the corpus is partitioned. Thus, for those approaches our partitioning is guaranteed to produce the same models as any other partitioning method; i.e., model accuracy is unchanged. Lastly, we note that we target synchronized LVM training, in which all nodes must finish each training iteration before any node can proceed to the next iteration. Thus, we desire balanced partitions to help ensure iterations have similar durations across nodes. We achieve this in practice by constrain</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>172--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1247" citStr="Ritter et al., 2010" startWordPosition="176" endWordPosition="179">In this paper, we show how the memory required for parallel LVM training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node. We present a greedy document partitioning technique for the task. For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training. 1 Introduction Unsupervised latent variable models (LVMs) of text are utilized extensively in natural language processing (Griffiths and Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007;</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 172–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Wolfe</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Fully distributed EM for very large datasets.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning, ICML ’08,</booktitle>
<pages>1184--1191</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1810" citStr="Wolfe et al., 2008" startWordPosition="261" endWordPosition="264">sing (Griffiths and Steyvers, 2004; Ritter et al., 2010; Downey et al., 2007; Huang and Yates, 2009; Li and McCallum, 2005). LVM techniques include Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Hidden Markov Models (HMMs) (Rabiner, 1989), and Probabilistic Latent Semantic Analysis (Hofmann, 1999), among others. LVMs become more predictive as they are trained on more text. However, training LVMs on massive corpora introduces computational challenges, in terms of both time and space complexity. The time complexity of LVM training has been addressed through parallel training algorithms (Wolfe et al., 2008; Chu et al., 2006; Das et al., 2007; Newman et al., 2009; Ahmed et al., 2012; Asuncion et al., 2011), which reduce LVM training time through the use of large computational clusters. However, the memory cost for training LVMs remains a bottleneck. While LVM training makes sequential scans of the corpus (which can be stored on disk), it requires consistent random access to model parameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the numbe</context>
<context position="8508" citStr="Wolfe et al., 2008" startWordPosition="1391" endWordPosition="1394">, (Krause et al., 2007) considers maximizing the minimum over a set of monotonic submodular functions, which is the opposite of our problem. The distinct task of minimizing a single submodular function has been investigated in e.g. (Iwata et al., 2001). It is important to emphasize that data partitioning is a pre-processing step, after which we can employ precisely the same Expectation-Maximization (EM), sampling, or variational parameter learning techniques as utilized in previous work. In fact, for popular learning techniques including EM for HMMs (Rabiner, 1989) and variational EM for LDA (Wolfe et al., 2008), it can be shown that the parameter updates are independent of how the corpus is partitioned. Thus, for those approaches our partitioning is guaranteed to produce the same models as any other partitioning method; i.e., model accuracy is unchanged. Lastly, we note that we target synchronized LVM training, in which all nodes must finish each training iteration before any node can proceed to the next iteration. Thus, we desire balanced partitions to help ensure iterations have similar durations across nodes. We achieve this in practice by constraining each node to hold at most 3% more than Z/T t</context>
</contexts>
<marker>Wolfe, Haghighi, Klein, 2008</marker>
<rawString>Jason Wolfe, Aria Haghighi, and Dan Klein. 2008. Fully distributed EM for very large datasets. In Proceedings of the 25th international conference on Machine learning, ICML ’08, pages 1184–1191, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huican Zhu</author>
<author>Oscar H Ibarra</author>
</authors>
<title>On some approximation algorithms for the set partition problem.</title>
<date>1999</date>
<booktitle>In Proceedings of the 15th Triennial Conf. of Int. Federation of Operations Research Society.</booktitle>
<contexts>
<context position="7116" citStr="Zhu and Ibarra, 1999" startWordPosition="1154" endWordPosition="1157">1, d2,. . . , dN}, and T nodes, partition D into T subsets D1, D2, ... , DT, such that Vmax is minimized. For illustration, consider the following small example. Let corpus C contain three short documents {c1=“I live in Chicago”, c2=“I am studying physics”, c3=“Chicago is a city in Illinois”}, and consider partitioning C into 2 non-empty subsets, i.e., T = 2. There are a total of three possibilities: • {{c1, c2}, {c3}}. Vmax = 7 • {{c1, c3}, {c2}}. Vmax = 8 • {{c2, c3}, {c1}}. Vmax = 10 The decision problem version of CORPUSPART is NP-Complete, by a reduction from independent task scheduling (Zhu and Ibarra, 1999). In this paper, we develop greedy algorithms for the task that are effective in practice. We note that CORPUSPART has a submodular problem structure, where greedy algorithms are often effective. Specifically, let |S |denote the vocabulary size of a set of documents S, and let S&apos; C_ S. Then for any document c the following inequality holds. |S&apos; U c |− |S&apos; |&gt; |S U c |− |S| That is, adding a document c to the subset S&apos; increases vocabulary size at least as much as adding c to S; the vocabulary size function is submodular. The CORPUSPART task thus seeks a partition of the data that minimizes the </context>
<context position="10270" citStr="Zhu and Ibarra, 1999" startWordPosition="1681" endWordPosition="1684"> document d from D node selection: Select node nt, and add d to Dt Remove d from D until all documents are allocated A baseline partitioning method commonly used in practice simply distributes documents across nodes randomly. As our experiments show, this baseline approach can be improved significantly. In the following, set operations are interpreted as applying to the set of unique words in a document. For example, |dUDt |indicates the number of unique word types in node nt after document d is added to its document collection Dt. 3.1 Document Selection For document selection, previous work (Zhu and Ibarra, 1999) proposed a heuristic DISSIMILARITY method that selects the document d that is least similar to any of the node document collections Dt, where the similarity of d and Dt is calculated as: Sim(d, DT) = |d n Dt|. The intuition behind the heuristic is that dissimilar documents are more likely to impact future node selection decisions. Assigning the dissimilar documents earlier helps ensure that more greedy node selections are informed by these impactful assignments. However, DISSIMILARITY has a prohibitive time complexity of O(TN2), because we must compare T nodes to an order of N documents for a</context>
<context position="12375" citStr="Zhu and Ibarra, 1999" startWordPosition="2032" endWordPosition="2035">s tractable), but partitions an estimated 2,600 times faster on our largest evaluation corpus. Second, we found that document selection has relatively minor impact on memory footprint, providing a roughly 5% incremental benefit over random document selection. Thus, although 581 we utilize BATCH DISSIMILARITY in the final system we evaluate, simple random document selection may be preferable in some practical settings. 3.2 Node Selection Given a selected document d, the MINIMUM method proposed in previous work selects node nt having the minimum number of word types after allocation of d to nt (Zhu and Ibarra, 1999). That is, MINIMUM minimizes |d U Dt|. Here, we introduce an alternative node selection method JACCARD that selects node nt maximizing the Jaccard index, defined here as |d n Dt|/|d U Dt|. Our experiments showed that our JACCARD node selection method outperforms the MINIMUM selection method. In fact, for the largest corpora used in our experiments, JACCARD offered an 12.9% larger reduction in Vmax than MINIMUM. Our proposed system, referred to as BJAC, utilizes our best-performing strategies for document selection (BATCH DISSIMILARITY) and node selection (JACCARD). 4 Evaluation of Partitioning</context>
<context position="18370" citStr="Zhu &amp; Ibarra (1999)" startWordPosition="3011" endWordPosition="3014">pes on each node. parallelization research (Hendrickson and Kolda, 2000). However, our LVM training task differs significantly from those in which graph partitioning is typically employed. Specifically, graph partitioning tends to be used for scientific computing applications where communication is the bottleneck. The graph algorithms focus on creating balanced partitions that minimize the cut edge weight, because edge weights represent communication costs to be minimized. By contrast, in our LVM training task, memory consumption is the bottleneck and communication costs are less significant. Zhu &amp; Ibarra (1999) present theoretical results and propose techniques for the general partitioning task we address. In contrast to that work, we focus on the case where the data to be partitioned is a large corpus of text. In this setting, we show that our heuristics partition faster and provide smaller memory footprint than those of (Zhu and Ibarra, 1999). 7 Conclusion We presented a general corpus partitioning technique which can be exploited in LVM training to reduce memory footprint and training time. We evaluated the partitioning method’s performance, and showed that for large corpora, our approach reduces</context>
</contexts>
<marker>Zhu, Ibarra, 1999</marker>
<rawString>Huican Zhu and Oscar H. Ibarra. 1999. On some approximation algorithms for the set partition problem. In Proceedings of the 15th Triennial Conf. of Int. Federation of Operations Research Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>