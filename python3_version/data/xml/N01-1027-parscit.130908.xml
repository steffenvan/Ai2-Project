<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.9147595">
Identifying User Corrections Automatically in Spoken Dialogue
Systems
</title>
<author confidence="0.7356985">
Julia Hirschberg&apos; and Diane Litman&apos; and Marc Swerts2
&apos;AT&amp;T Labs-Research, Florham Park, NJ, USA
</author>
<affiliation confidence="0.811342">
2IP0, Eindhoven, The Netherlands, and CNTS, Antwerp, Belgium
</affiliation>
<email confidence="0.989369">
fju1ia/dianel@research.att.com,m.g.j.swertAtue.n1
</email>
<sectionHeader confidence="0.995353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998765">
We present results of machine learning experiments
designed to identify user corrections of speech recog-
nition errors in a corpus collected from a train in-
formation spoken dialogue system. We investigate
the predictive power of features automatically com-
putable from the prosody of the turn, the speech
recognition process, experimental conditions, and
the dialogue history. Our best performing features
reduce classification error from baselines of 25.70-
28.99% to 15.72%.
</bodyText>
<sectionHeader confidence="0.997889" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958009345795">
Users&apos; evaluations of spoken dialogue systems largely
depend on the number of errors the system
makes (Walker et al., 2000a) and how easy it is for
the user and system to correct them. Poor auto-
matic speech recognition (ASR) accuracy is the pri-
mary source of error in most systems, compounded
by user behavior when confronted by such error.
Studies have shown that speakers tend to switch to
a prosodically &apos;marked&apos; speaking style hyperar-
ticulated speech, e.g. I said BAL-TI-MORE, not
Boston after ASR errors (Wade et al., 1992; Ovi-
att et al., 1996; Levow, 1998; Bell and Gustafson,
1999). While possibly effective in human-human
communicative settings, such correction behavior
appears to lead to further errors in human-machine
interactions (Levow, 1998; Soltau and Waibel, 2000),
perhaps because it differs from the speech most rec-
ognizers are trained on. In addition, system re-
sponses indicating incorrect beliefs, such as an im-
plicit verification containing mistaken information
(e.g. Where do you want to go from Boston when
the user has said she wants to depart from Balti-
more), cause considerable difficulties for users faced
with the need to correct a misconception and also
answer a new question (Krahmer et al., 1999).
To date, attempts to improve system performance
have largely focussed on improving ASR accuracy,
or simplifying the task, either by further constrain-
ing the domain and functionality of the system or
further restricting the vocabulary the system must
recognize. However, as ASR accuracy improves, di-
alogue systems will be called upon to handle ever
more complex tasks and ever less restricted vocabu-
laries. So, it seems likely that spoken dialogue sys-
tems will, for the foreseeable future, always require
effective error detection and repair strategies.
Ideally, such repair strategies, would consist of
steps to immediately detect an error when it occurs
and to interact with the user to correct the error
in subsequent exchanges. In previous research (Lit-
man et al., 2000; Hirschberg et al., 2000), we iden-
tified new procedures to detect recognition errors,
which perform well when tested on two different cor-
pora, the TOOT and W99 corpora (train informa-
tion and conference registration dialogues) collected
using two different ASR systems). We found that
prosodic features, in combination with information
already available to the recognizer, such as acoustic
confidence scores, grammar and recognized string,
can distinguish speaker turns that are misrecognized
far better than traditional methods for ASR rejec-
tion (the system decision that its hypothesis is so
weak that it should reprompt for fresh input), which
use acoustic confidence scores alone.
Now we have turned to the question of how users
correct such errors. In a descriptive analysis of
user corrections in the TOOT corpus (Swerts et al.,
2000), we found that corrections differ significantly
from non-corrections prosodically, being higher in
pitch, louder, longer, with longer pauses preceding
them and less internal silence. They are also mis-
recognized more frequently than non-corrections
though they are no more likely to be rejected by
the system. And corrections more distant from the
error they correct tend to exhibit greater prosodic
differences and are recognized more poorly, suggest-
ing that users are not learning to modify their own
behavior to improve system performance. So, deal-
ing with corrections is a particularly difficult task
for both users and systems. We also found that sys-
tem dialogue strategy the amount of initiative
users are allowed to exercise in controlling the flow
of the dialogue and the type of confirmation strategy
the system adopts affects users&apos; choice of correc-
tion type (e.g., directly repeating versus paraphras-
ing misrecognized information).
These findings suggest a number of possible
courses of action. System strategy might be cho-
sen to favor the type(s) of correction the system can
most easily process. Or, having chosen a particu-
lar interaction strategy, the system repair strategy
might be tuned to handle the correction types which
that strategy is likely to produce. Alternatively, the
system&apos;s dialogue manager might use the detection
of corrections as a signal that it should modify its
interaction strategy, either locally, by beginning a
subdialogue for faster error recovery, or globally, by
changing its initiative or confirmation strategies, or
even directing the user to a human operator. Or,
since corrections are often hyperarticulated, detec-
tion of a correction could serve as a signal to the
ASR engine to run a recognizer trained on hyper-
articulated speech in parallel with its normal pro-
cessor, to better transcribe the speech. All of these
possibilities, however, assume that user corrections
can be detected by the system reliably during the
dialogue.
In the current study, we turn to the question
of identifying user corrections automatically, from
prosodic features as well as other features that are
readily available to a spoken dialogue system. Our
domain is the TOOT spoken dialogue corpus, which
we describe in Section 2. In Section 3, we describe
the features we use for our machine learning experi-
ments. Section 4 presents the results of those exper-
iments. In Section 5 we summarize our conclusions
and describe future research directions.
</bodyText>
<sectionHeader confidence="0.917076" genericHeader="method">
2 The TOOT Corpus
</sectionHeader>
<bodyText confidence="0.999963379310345">
Our corpus consists of 152 dialogues between human
subjects and TOOT, a spoken dialogue system that
allowed users access to train information from the
web via telephone. The TOOT corpus was collected
in a laboratory setting, to study variations in dia-
logue strategy and in user-adapted interaction (Lit-
man and Pan, 1999). TOOT was implemented using
an interactive voice response platform developed at
AT&amp;T, combining ASR and text-to-speech with a
phone interface (Kamm et al., 1997). The system&apos;s
speech recognizer was a speaker-independent hid-
den Markov model system, with context-dependent
phone models for telephone speech and constrained
grammars defining the vocabulary allowed at any di-
alogue state. The platform supported barge-in. This
system rejected a turn whenever its acoustic confi-
dence score fell below thresholds predefined for each
dialogue state.
Subjects were 39 student interns; 20 native speak-
ers and 19 non-native, 16 female and 23 male. They
were asked to perform four tasks using one of several
versions of the system that differed in terms of locus
of initiative (system, user, or mixed), confirmation
strategy (explicit, implicit, or none), and whether
these conditions could be changed by the user during
the task (adaptive versus non-adaptive). Dialogues
were recorded and system and user behavior logged
automatically.
We examined 2328 user turns from 152 dialogues,
with a mean number of turns per dialogue of 15.3.
The turns were transcribed and automatically com-
pared to the recognized string to produce a word er-
ror rate (WER) for each turn; the mean WER over
all turns was 40%. Actual words per turn averaged
3.6 and recognized words per turn, 2.4. The Concept
Accuracy (CA) for each turn was manually labeled.
If the ASR correctly captured all task-related infor-
mation in the turn (e.g. time, departure and arrival
cities), the turn&apos;s CA score was 1 (semantically cor-
rect). Otherwise, the CA score reflected the percent-
age of correctly recognized task information in the
turn. CA errors and explicit system rejections were
the only errors users could identify as such, and so
were the only ones corrected.
In addition, two authors independently labeled
each turn as to whether or not it constituted a cor-
rection of a prior system failure. They also identi-
fied the turn being corrected, and the type of each
correction: REP (repetition, including differences in
pronunciation or fluency), PAR (paraphrase); ADD
(task-relevant content added); OMIT (content omit-
ted); and ADD/OMIT (content both added and
omitted). Labeler disagreement was subsequently
resolved by consensus. 675 of the 2328 user turns in
the corpus (29%) were judged to be corrections. A
sample interaction between TOOT and a user, con-
taining the different correction types and their labels
is given in Figure 1.
</bodyText>
<sectionHeader confidence="0.983045" genericHeader="method">
3 Characterizing Corrections
</sectionHeader>
<bodyText confidence="0.999784166666667">
In this section we describe the features used in the
machine learning experiments described in Section 4
and the motivation behind their selection. The en-
tire feature set is presented in Figure 2, and includes
only features that could be automatically available
to a dialogue system.
</bodyText>
<subsectionHeader confidence="0.998351">
3.1 Prosodic Features
</subsectionHeader>
<bodyText confidence="0.999899416666667">
A previous study (Swerts et al., 2000) showed that
corrections were significantly longer, louder, higher
in pitch excursion, and followed longer pauses than
other turns. Thus, we expected that these features
would be useful in identifying corrections automat-
ically. We examined maximum and mean funda-
mental frequency values (f0max, fOmn) as indica-
tors of pitch range; maximum and mean energy val-
ues (rmsmax, rmsmn) as indicators of loudness;
total duration of the speaker turn (dur); length
of pause preceding the turn (ppau); speaking rate
(tempo); and amount of silence within the turn
</bodyText>
<table confidence="0.677312666666667">
fOmax fOmn rmsmax rmsmn dur ppau tempo zeros
Mean 227 Hz 163 Hz 1612 rms 396 rms 1.92 sec .71 sec 2.48 sps 44%
S.D. 77 Hz 44 Hz 1020 rms 261 rms 2.44 sec .79 sec 1.37 sps 17%
</table>
<tableCaption confidence="0.998683">
Table 1: Means and Standard Deviations for Prosodic Features over All Turns
</tableCaption>
<figure confidence="0.963421230769231">
Turn Turn ID Corr of Corr Type
S: On which day of the week do you want
to leave?
S: Sorry, I didn&apos;t hear anything from you.
Please tell me your departure date.
U: Urn tomorrow 776 NA NA
S: You can say the departure date. For
example, you can say &apos;today&apos; or &apos;tomorrow&apos;.
Or you can say &apos;Monday&apos;, &apos;Tuesday&apos;,
&apos;Wednesday&apos;...
U: Tomorrow 777 776 REP
S: You can say the departure date. For
example, you can say &apos;today&apos; or &apos;tomorrow&apos;.
Or you can say &apos;Monday&apos;, &apos;Tuesday&apos;,
&apos;Wednesday&apos;, &apos;Thursday&apos;, &apos;Friday&apos;...
U: Saturday 778 777 PAR
S: Do you want to leave on Saturday?
U: Yes 779 NA NA
S: What time do you want to leave?
U: 8:00 a.m. 780 NA NA
S: Do you want to leave around 10:00 p.m.
U: 8:00 781 780 OMIT
S: Do you want to leave around 10:00 p.m.
U: No 782 NA NA
S: What time do you want to leave?
U: 8:00 a.m. 783 781 ADD
</figure>
<figureCaption confidence="0.999977">
Figure 1: Dialogue Fragment with Correction Labels
</figureCaption>
<bodyText confidence="0.999602315789474">
(zeros).&apos; We examined both raw and normalized
versions of these features, with normalization either
by first turn (norml) or by prior turn (norm2)
in task. FO and rms values were calculated from
the output of Entropic Research Laboratory&apos;s pitch
tracker, get_f0. Speaking rate (tempo) was approx-
imated from syllables per second in the recognized
string, a method we previously found to accurately
approximate actual rate. Turn-internal silence (ze-
ros) was defined as the percentage of zero frames
in the turn, i.e., roughly the percentage of time the
speaker was silent. Table 1 shows the overall means
and standard deviations for these features over the
corpus.
&apos;While most features were automatically computed, dur
and ppau were hand segmented, as the turn-level files created
by TOOT were not available; these would have been available
from the turn-level speech, but perhaps not with the same
accuracy.
</bodyText>
<subsectionHeader confidence="0.445205">
Prosodic (PROS) :
</subsectionHeader>
<bodyText confidence="0.786455">
Raw (raw values): fOmax, fOmn, rmsmax, rmsmn,
dur, ppau, tempo, zeros
Norml (values normalized by first turn in dia-
logue): fOmaxl, fOmnl, rmsmaxl, rmsmnl,
dun, ppaul, tempol, zerosl
Norm2 (values normalized by previous turn in di-
alogue): f0max2, f0mn2, rmsmax2, rmsmn2,
dur2, ppau2, tempo2, zeros2
ASR (ASR) : gram, str, conf, ynstr, nofeat, canc,
help, wordsstr, syls, rejbool
</bodyText>
<figure confidence="0.86440975">
System Experimental (SYS) : inittype, conftype,
adapt, realstrat
Dialogue Position (POS) : diadist
Dialogue History (DIA) :
</figure>
<figureCaption confidence="0.6202754">
PreTurn : value of PROS and ASR features for
preceding turn (e.g., prefOmax)
PrepreTurn : value of PROS and ASR fea-
tures for turn preceding preceding turn (e.g.,
pprefOmax)
</figureCaption>
<bodyText confidence="0.961772571428571">
Prior : for each boolean-valued feature (yn-
str, nofeat, canc, help, rejbool), the num-
ber/percentage of prior turns exhibiting the
feature (e.g., priorynstrnum/priorynstrpct)
PMean : for each continuous-valued PROS and
ASR feature, the mean of the feature&apos;s value
over all prior turns (e g, pmnfOmax)
</bodyText>
<figureCaption confidence="0.9922">
Figure 2: Feature Set for Predicting Corrections
</figureCaption>
<subsectionHeader confidence="0.997717">
3.2 ASR Features
</subsectionHeader>
<bodyText confidence="0.999982142857143">
Since corrections in our corpus were misrecognized
more frequently than non-corrections (Swerts et al.,
2000), we included a set of ASR features that were
derived from TooT&apos;s speech recognition component
and its outputs: the grammar used as the ASR lan-
guage model at each dialogue state (gram), the rec-
ognizer&apos;s best hypothesis (str), and the turn-level
acoustic confidence score it produced (conf).2 As
subcases of the str feature, we included boolean
features representing whether or not the recognized
string included the strings yes or no (ynstr), some
variant of no, such as nope (nofeat), cancel (cane),
or help (help), as these lexical items often occurred
during problem resolution. To estimate durational
</bodyText>
<subsectionHeader confidence="0.428901">
2Confidence scores ranged from -0.087662 to -9.884418.
</subsectionHeader>
<bodyText confidence="0.999961">
features, we approximated the length of the user
turn in words (wordsstr) and in syllables (syls)
from the str feature. And we added a boolean fea-
ture identifying whether or not the turn had been
rejected by the system (rejbool).
</bodyText>
<subsectionHeader confidence="0.997784">
3.3 System Experimental Features
</subsectionHeader>
<bodyText confidence="0.995198357142857">
In (Swerts et al., 2000) we found that differences
in dialogue strategy affect the type and success of
user corrections. For example, TOOT users more fre-
quently repeat their misrecognized turns and pro-
duce the fewest corrections per task when TOOT
has the initiative and explicitly confirms all user
input. So, we hypothesized that system condi-
tions might prove important in our learning exper-
iments. We thus include features representing the
system&apos;s current initiative and confirmation strate-
gies (inittype, conftype), whether users could
adapt the system&apos;s dialogue strategies (adapt), and
the combined initiative and confirmation setting
(realstrat).
</bodyText>
<subsectionHeader confidence="0.976433">
3.4 Dialogue Position and History Features
</subsectionHeader>
<bodyText confidence="0.980563421052632">
(Swerts et al., 2000) also showed that the further a
correction is from the original error, the less likely
it is to be recognized correctly, and the stronger the
correlation with prosodic deviation from the mean
values over a speaker&apos;s turns (e.g., more distant cor-
rections are higher in pitch than closer corrections).
As a first approximation of this distance feature, we
included the feature diadist distance of the cur-
rent turn from the beginning of the dialogue.
In addition, previous research (Litman et al.,
1999; Walker et al., 2000b) has shown that features
of the dialogue as a whole and features of more local
context can be helpful in predicting &apos;problematic&apos;
dialogues. So we looked at a set of features sum-
marizing aspects of the prior dialogue for both the
absolute number of times prior turns exhibited cer-
tain characteristics (e.g., contained a key word like
cancel priorcancnum) and the percentage of the
prior dialogue containing one of these features (e.g.
priorcancpct). We also examined means for all our
continuous-valued features over the entire dialogue
preceding the turn to be predicted (pmn), such
as pmnsyls, the mean length of prior turns calcu-
lated in number of syllables per turn. Finally, we
examined more local contexts, including all features
of the preceding turn (pre) and for the turn pre-
ceding that (ppre).
It seemed particularly likely that lexical features
of the local context such as whether a user had
asked for help recently, or tried to cancel out of
an exchange, or replied no to a system query
might prove useful in identifying corrections.3 Also,
3Recall that these are lexical features from the recognized
string, not from the actual user transcript.
whether a prior turn had been rejected was clearly
a useful cue to the identification of the current turn
as a correction, since users generally supplied a cor-
rection when explicitly asked.
</bodyText>
<sectionHeader confidence="0.981873" genericHeader="method">
4 Predicting Corrections
</sectionHeader>
<bodyText confidence="0.999930195652174">
In this section we investigate whether the features
described in Section 3 (or interesting subsets of
them) can in fact be used to accurately predict
whether a turn will be a correction or not. We de-
scribe experiments using the machine learning pro-
gram RIPPER (Cohen, 1996) to automatically induce
such prediction models. RIPPER takes as input the
classes to be learned, the names and possible val-
ues of a set of features, and training data specify-
ing the class and feature values for each training ex-
ample. For our experiments, the features presented
in Figure 2 comprise the independent variables for
our learning experiments. The dependent variable to
be learned, correction (T) versus non-correction
(F), corresponds to the hand-labeled observations
described in Section 2. Given a vector of values for
the independent and dependent variables for each
speaker turn, RIPPER outputs a classification model
for classifying future examples. The model is learned
using greedy search guided by an information gain
metric, and is expressed as an ordered set of if-then
rules. When multiple rules are applicable, RIPPER
uses the first rule it finds. When no rules are appli-
cable, RIPPER classifies the turn as a non-correction
(F) by default.
Table 2 shows the performance of the learned clas-
sification models for some of the feature sets we ex-
amined; all performance figures are estimated us-
ing 25-fold cross-validation on the 2328 turns in our
corpus. The &apos;Features&apos; column identifies the set of
features (as defined in Figure 2) used to learn the
model. The second column, &apos;DIA&apos;, indicates which
type of dialogue history features (PreTurn, Prepre-
Turn, Prior, and/or PMean) were also included in
the feature set; these features represent the same
types of information (e.g. fOmax) that the &apos;Fea-
tures&apos; column denotes, but for one or more previous
turns in the dialogue. The third column shows the
mean error and standard error (SE) predicted for the
model specified by the first two columns. When er-
ror estimates in different rows differ by plus or minus
twice the standard error, they are significantly dif-
ferent (Cohen, 1995). The remaining columns show
the mean recall, precision, and 1,3 = 1 for &apos;correc-
tions&apos; (focus class=T) and &apos;non-corrections&apos; (focus
class=F), respectively.4 For comparison purposes,
</bodyText>
<footnote confidence="0.930667">
4 Recall is the percentage of actual members of a class that
are identified, while precision is the percentage of predicted
class members that are in fact members. The definition of
+1)PrecisionRecall
Fo is ,2 _re cicison„ +Recall /3 = 1 equally weights precision
and recall. These values are computed using our own cross-
</footnote>
<table confidence="0.9998805">
class=T class=F
Features DIA ErrorISE Rec. Prec. 1,3 = 1 Rec. Prec. 1,3 = 1
Raw+ASR+SYS+POS PreTurn 15.7210.80 70.61 74.96 .72 89.95 88.28 .89
Raw+ASR+SYS+POS all 16.1610.58 69.80 74.65 .72 90.12 87.82 .89
PROS+ASR+SYS+POS all 16.3810.61 69.01 74.05 .71 89.60 87.61 .88
ASR all 16.4110.93 69.93 72.39 .70 88.76 87.7 .88
ASR+SYS+POS all 17.0110.78 73.73 73.38 .73 88.68 89.00 .89
ASR+SYS+POS none 18.6010.81 56.48 72.79 .63 91.33 83.76 .87
Raw+ASR+SYS+POS none 18.6810.67 58.45 71.64 .64 90.37 84.17 .87
ASR+PROS none 19.2910.78 54.54 69.97 .61 90.25 82.90 .86
POS+PROS none 19.5910.73 52.96 69.70 .60 90.38 82.47 .86
Raw all 19.6810.78 55.62 70.89 .62 90.64 83.33 .87
PROS all 20.3310.90 56.45 69.23 .61 89.43 83.42 .86
ASR+POS none 20.4010.79 52.20 71.99 .60 91.43 82.41 .87
PROS none 20.5310.81 54.86 71.72 .62 90.78 83.07 .87
conf+rejbool all 21.2310.93 59.70 65.97 .62 87.05 84.05 .85
ASR+SYS none 23.4610.72 51.55 63.40 .56 87.53 81.65 .84
ASR none 24.1910.84 45.93 60.99 .52 87.80 79.90 .84
Raw none 25.3510.93 42.26 59.46 .48 88.29 78.97 .83
POS none 29.0011.02 0.00 - - 99.94 70.99 .83
SYS none 29.0011.02 0.00 - - 100.00 71.00 .83
Prerejbool Baseline Error = 25.70; Majority Baseline Error = 28.99
</table>
<tableCaption confidence="0.999609">
Table 2: Estimated Error, Recall, Precision, and 173 = 1 for Predicting Corrections
</tableCaption>
<bodyText confidence="0.998803173410405">
we compare our predictions to two potential base-
lines. The &apos;Majority&apos; baseline predicts that all turns
are non-corrections (the majority class of F), and
has a classification error of 28.99%. The `Prerej-
boor baseline predicts that all turns following re-
jected turns (prerejbool = T) are corrections
since after rejections, TOOT asks users to repeat their
turn and all others are non-corrections; this base-
line gives a classification error of 25.70%.
The first question addressed in our experiments
is whether or not corrections can be predicted sig-
nificantly better than our baselines. Table 2 shows
that in fact they can. Our best performing fea-
ture set (Raw+ASR+SYS+POS, DIA = PreTurn)
cuts the majority baseline error almost in half, from
28.99% to 15.72%, and predicts significantly better
than the rejection-based baseline as well. This fea-
ture set includes raw versions of all our prosodic fea-
tures and all of the non-prosodic features, for both
the turn being classified and the immediately prior
turn. Note that even if all of the available features
are used for learning (i.e., the normalized versions of
prosodic features and all of the various history fea-
tures (PROS+ASR+SYS+POS, DIA = all, Error =
16.38%)), performance is statistically comparable to
this model.&apos; In addition, the recall, precision and
1,3 = 1 values in Table 2 show that corrections are
validation program, while error is computed using RIPPER&apos;s
cross-validation option.
5Note that removing features sometimes changes perfor-
generally predicted with better precision than recall
while the reverse holds for non-corrections, and that
non-corrections (the majority class) are easier to ac-
curately predict than corrections.
We next turn to an examination of the contribu-
tion of the different types of features we used for
prediction. First, we consider the utility of our non-
prosodic features. Table 2 shows that, using only
non-prosodic features (ASR, SYS, POS), corrections
can still be predicted with an accuracy statistically
equivalent to our best results. That is, using all
feature types (PROS+ASR+SYS+POS, DIA = all,
Error = 16.38%) is equivalent to using only non-
prosodic features (ASR+SYS+POS, DIA = all, Er-
ror = 17.01%). Similarly, restricting our feature set
to the ASR-derived subset of our non-prosodic fea-
tures (ASR, DIA = all, Error = 16.41%) or remov-
ing all dialogue history (ASR+SYS+POS, DIA =
none, Error = 18.60%) yields results equivalent to
our best-performing classifier. However, when only
those ASR features derived from the acoustic con-
fidence score (i.e. conf, preconf, ppreconf, pm-
nconf, rejbool, prerejbool, pprerejbool, prior-
rejboolnum, priorrejboolpct) are used for pre-
diction, then performance does significantly degrade
(conf+rejbool, DIA = all, Error = 21.23%). So, it
appears that there are numerous ways to classify
mance, which might indicate a weakness in R,IPPER&apos;s feature
selection process.
corrections successfully, using various combinations
of feature types. This finding is an important one,
since it suggests that systems which have access to
restricted kinds of information can still hope to iden-
tify user corrections with some confidence. In par-
ticular, simply using information available to current
ASR systems such as acoustic confidence score, rec-
ognized string, grammar, and features derived from
these, produces classification results equivalent to
our best-performing classifier. A caveat here is that
some of the features in this ASR feature set (e.g.,
grammar and recognized string) are less likely to
generalize from task to task.
Turning now to the role of prosodic features in
classifying corrections, Table 2 shows that use of
only non-prosodic features (ASR+SYS+POS, DIA
= all, Error = 17.01%) slightly (but not quite sig-
nificantly) outperforms use of only raw prosodic fea-
tures (Raw, DIA = all, Error = 19.68%). How-
ever, using raw prosodic features alone (Error =
19.68%) is comparable to using only ASR features
alone (ASR, DIA = all, Error = 16.41%). And
both significantly outperform the majority class and
rejection-based baselines. Note also that prediction
from raw prosodic features alone (19.68%) is not
improved by the inclusion of their normalized ver-
sions (PROS, DIA = all, Error = 20.33%). Thus,
ASR-derived features and prosodic features seem to
provide equally successful classifications of user cor-
rections. Since ASR-derived features, in particular,
acoustic confidence score, are currently used by spo-
ken dialogue systems to determine when to reject a
turn, our results suggest that such features can also
be useful for identifying corrections. While prosodic
features are rarely made use of in spoken dialogue
systems, they would in fact seem more likely to gen-
eralize across tasks and recognizers than the ASR
features.
Now we turn to the issue of how useful features
of the dialogue history are in classifying corrections.
Recall that our best performing ruleset used only a
limited dialogue history features from the pre-
ceding turn (Raw+ASR+SYS+POS, DIA = Pre-
Turn, Error = 15.72%). While adding features of the
turn two turns back (PrepreTurn) and of the dia-
logue as a whole (Prior__ and PMean) does not sig-
nificantly change the error (Raw+ASR+SYS+POS,
DIA = all, Error = 16.16%), removing the fea-
tures of the immediately previous turn from the di-
alogue history does in fact cause a significant in-
crease in error rate (Raw+ASR+SYS+POS, DIA
= none, Error = 18.68%). However, as discussed
above, when only non-prosodic features are consid-
ered (ASR+SYS+POS), there is no significant dif-
ference between DIA = all and DIA = none. So,
it seems that features of the immediate local con-
text can improve our ability to classify corrections
accurately when prosodic features are included, but
adding a larger local context window and a global
context does not improve over these results. Con-
textual features seem particularly important to per-
formance when only raw prosodic features are con-
sidered (Raw, DIA = all, Error = 19.68%). When
the raw prosodic features of the dialogue history
are removed, the error rate dramatically increases
(Raw, DIA = none, Error = 25.35%). However, if
the normalized prosodic features (which themselves
encode much of the historical information) are also
included, then removing the DIA versions of these
features does not significantly degrade performance
(PROS, DIA = all, Error = 20.33% versus PROS,
DIA = none, Error = 20.53%). We might explain the
larger role that prosodic context plays in classifica-
tion by returning to the differences we found between
prosodic features of corrections and non-corrections,
described in Section 3. In our descriptive analyses
we found that prosodic features such as pitch, du-
ration, and loudness reliably distinguish corrections
based on relative differences between the two types
of turns, not absolute differences. In prediction also,
it seems that some form of normalization by context
improves the performance of prosodic features.
When we examine which class of features per-
forms best in the absence of contextual informa-
tion, we see that the prosodic features (PROS, DIA
= none, Error = 20.53%) significantly outperform
the ASR-derived features (ASR, DIA = none, Er-
ror = 24.19%), which in turn significantly outper-
form either of the remaining feature types (POS
and SYS). Table 2 also shows the cases in which
the addition of new sources of knowledge improves
prediction performance. For DIA = none, the sta-
tistically significant improvements involve adding
the feature diadist (distance of the current turn
from the beginning of the dialogue): for exam-
ple, ASR+POS (Error = 20.4%) outperforms both
ASR (Error = 24.19%) and POS (Error = 29%),
and ASR+SYS+POS (Error = 18.6%) outperforms
ASR+SYS (Error = 23.46%) . Again, these are fea-
tures which are easily made available to current spo-
ken dialogue systems.
The classification model learned from the best per-
forming feature set in Table 2 is shown in Figure 3.
Rules are presented in order of importance in clas-
sifying data. The first rule RIPPER finds with this
feature set specifies that, if the duration of the cur-
rent turn is &gt; 3.89046 seconds, and if the acoustic
confidence score of the prior turn is &lt; -0.645234,
and if the percentage of silence in the current turn
is &lt; 53.9474%, then predict that the turn is a cor-
rection; this rule correctly predicts 153 corrections,
and incorrectly predicts that 10 non-corrections are
corrections. So, this rule applies when the previous
turn has a low confidence score and the current turn
</bodyText>
<construct confidence="0.796196857142857">
if (dur &gt; 3.89046) A (preconf &lt; -0.645234) A (zeros &lt;
0.539474) then T (153/10)
if (dur &gt; 0.851477) A (preconf &lt; -2.20989) A (zeros &lt;
0.442509) then T (114/47)
if (syls &gt; 3) A (preppau &gt; 0.393313) A (gram = univer-
sal) A (pretempo &lt; 2.30808) then T (52/16)
if (preconf &lt; -3.85311) A (predur &lt; 0.982059) A (prere-
jbool = T) then T (51/12)
if (dur &gt; 0.736544) A (diadist &gt; 9) A (syls &gt; 4) A (con-
ftype = Implicit) then T (32/10)
if (prestr contains help) A (preppau &lt; 1.35977) then T
(46/13)
if (syls &gt; 2) A (preppau &gt; 0.509916) A (prefOmn &lt;
118.773) then T (35/22)
if (dur &gt; 0.66384) A (predur &gt; 0.698772) A (conf &lt; -
3.16533) A (syls &gt; 4) then T (24/11)
if (pretempo &lt; 0.437603) A (preconf &gt; -0.393746) then
T (15/2)
if (pretempo &lt; 1.39342) A (preconf &lt; -4.06433) A (pre-
wordsstr &lt; 3) then T (22/15)
else F (1495/131)
</construct>
<figureCaption confidence="0.9752275">
Figure 3: Best Performing Ruleset
(Raw+ASR+SYS+POS, DIA=PreTurn)
</figureCaption>
<bodyText confidence="0.99995503030303">
exhibits some marked prosodic features. The fourth
rule predicts a correction after a previous rejection,
but only when the rejected turn was relatively short
with a low confidence score. The fifth rule predicts
a correction when TOOT uses a particular confirma-
tion strategy, for turns that are relatively long and
far from the beginning of the dialogue. The sixth
rule predicts a correction when the previous turn
is spoken soon after the prompt, and contains the
problem indicator help. Note that this use of the
domain-independent help is the only reference to a
lexical item in this ruleset. This ruleset includes
features from all of the feature subsets in our inven-
tory (PROS, ASR, SYS, POS, DIA). For the cur-
rent turn, the feature types that appear in the rules
are PROS (dur, zeros), ASR (conf, gram, syls),
SYS (conftype), and POS (diadist). Of the pre-
vious turn&apos;s features, only two feature sets emerge
as important: PROS (prefOmn, predur, preppau,
pretempo) and ASR (preconf, prestr, preword-
str, prerejbool). Furthermore, within a feature set
such as PROS, the useful features of the current and
previous turns differ somewhat (e.g., zeros is use-
ful for the current turn, while tempo is useful for
the prior turn), suggesting important differences in
the prosodic characteristics of corrections versus the
turns they follow.
When we look at a ruleset produced using only
features commonly available to current dialogue sys-
tems, such as ASR+SYS+POS (DIA = all), we see
that creative use of these features could in fact sup-
port correction classification (Figure 4). For exam-
ple, the fourth rule predicts that the current turn
</bodyText>
<construct confidence="0.873841571428571">
if (pmnconf &lt; -2.67657) A (syls &gt; 3) A (gram = univer-
sal) then T (287/70)
if (preconf &lt; -3.0156) A (prerejbool = T) A (nofeat =
T) then T (26/5)
if (preconf &lt; -4.0034) A (ppreynstr = F) A (prerejbool
= T) then T (42/16)
if (ppreconf &lt; -2.29048) A (syls &gt; 3) A (prenofeat = T)
then T (31/2)
if (prestr contains help) then T (55/27)
if (syls &gt; 3) A (pmnwordsstr &gt; 2.05714) A (conftype =
Implicit) A (priorrejnum &gt; 1) then T (38/11)
if (preconf &lt; -3.94692) A (syls &gt; 3) A (priorynstrpct &lt;
0.142857) A (pmnwordsstr &gt; 1.66667) then T (17/2)
else F (1520/179)
</construct>
<figureCaption confidence="0.93197">
Figure 4: Ruleset for Non-Prosodic Features
(ASR+SYS+POS, DIA=a11)
</figureCaption>
<bodyText confidence="0.995268225">
is a correction when it is not too short, and when
the pre__ turn indicates awareness (evidenced by
the presence of no) of a problem in the ppre__ turn
(which was recognized with low confidence). This
ruleset uses both ASR (gram, nofeat, syls) and
SYS (conftype) features of the current turn; al-
though only one rule in fact makes use of SYS fea-
tures. For the contextual DIA features, only the
ASR features occur in the rule-set: PreTurn (pre-
conf, prestr, prenofeat, prerejbool), PrepreTurn
(ppreconf, ppreynstr), and Prior and PMean
(pmnconf, priorynstrpct, pmnwordsstr, prior-
rejnum). Comparing this ruleset to the previous
one (Figure 3), we see that, where timing features
(dur, predur, zeros, pretempo, preppau) ap-
pear often when prosodic features are available, re-
lated features such as syls and wordsstr (from
which, e.g., tempo is estimated) may be compen-
sating in this ruleset. And of course the rejec-
tion feature (prerejbool) itself is a function of the
confidence score of the prior turn. Note also that
lexical features of the recognized string (nofeat,
prenofeat, ppreynstr, prestr, priorynstrpct)
emerge as quite useful in this ruleset especially
as contextual features. So, what the system has rec-
ognized in prior turns is a good predictor of whether
the current turn is a correction. Also note that
the overall verbosity of the previous dialogue (pm-
nwordsstr) appears in two of the rules.
An example of a ruleset learned from only prosodic
features (Raw, DIA = all, from Table 2) is shown
in Figure 5. This ruleset is notably terser than
those shown in Figures 3 and 4 and includes pri-
marily timing-based features (current turn features
dur, zeros, and tempo; local contextual feature
pretempo; and dialogue-level features pmndur
and pmnppau). However, all prosodic feature types
but f0 appear at least once in the ruleset, and fea-
tures specific to the current turn differ from those
relevant to different types of dialogue history. Like
</bodyText>
<construct confidence="0.79707">
if (dur &gt; 1.322) A (pmndur &gt; 2.10576) then T (290/91)
if (pmndur &gt; 1.121814) A (dur &gt; 1.21814) A (zeros
&lt; 0.569767) A (rmsmax &gt; 1350.81) A (pretempo &lt;
2.34637) then T (39/3)
if (dur &gt; 0.66384 ) A (pmndur &gt; 1.20889) A (tempo &gt;
2.90934) A (pmnppau &gt; 0.823703 ) then T (90/64)
else F (1495/256)
</construct>
<figureCaption confidence="0.9335175">
Figure 5: Ruleset for Raw Prosodic Features (Raw,
DIA=a11)
</figureCaption>
<bodyText confidence="0.9980734">
our previous descriptive findings discussed in Sec-
tion 3, this ruleset shows that corrections are longer,
louder, follow longer pauses, and contain less inter-
nal silence than non-corrections, and that these fea-
tures can be used successfully to identify them.
</bodyText>
<sectionHeader confidence="0.999232" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999902885714286">
In this paper we have presented results of machine
learning experiments designed to distinguish user
corrections from non-corrections in the TOOT spo-
ken dialogue corpus. Previous studies have shown
that user corrections represent a serious problem for
recognition in spoken dialogue system. They are
recognized much more poorly than non-corrections
but are not recognized by the system as likely to
have been misrecognized, in corresponding propor-
tion. Clearly, new techniques must be developed to
interpret such corrections, but such techniques can
only be effective if corrections can be reliably iden-
tified as such for special handling.
Using a large set of prosodic, ASR-derived, and
system-specific features, both for the current turn
and for contextual windows, and using summary fea-
tures of the prior dialogue, we have demonstrated
that it is possible to classify user corrections sig-
nificantly better than either of two baseline classi-
fiers (15.72% error versus 25.70-28.99%). More use-
fully perhaps for current spoken dialogue systems,
we have found that we can derive classifiers that
perform equivalently well using only features cur-
rently available to most speech recognizers, such as
acoustic confidence score, recognized string, gram-
mar, and features easily derived from this data. For
example, using only such features, we can classify
user corrections with an estimated success rate of
16.41%. So, it does in fact seem quite feasible for
current systems to identify user corrections using
data they currently do not make use of. The next
steps, developing techniques to interpret these turns
more accurately and to use correction prediction to
drive modifications in dialogue strategy, are both
subjects of our future research.
</bodyText>
<sectionHeader confidence="0.996558" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.920822">
Thanks to Walter Daelemans for his cross-validation
software.
</bodyText>
<sectionHeader confidence="0.954754" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998323036363636">
L. Bell and J. Gustafson. 1999. Repetition
and its phonetic realizations: Investigating
a Swedish database of spontaneous computer-
directed speech. In Proc. ICPhS-99, San Fran-
cisco.
Paul R. Cohen. 1995. Empirical Methods for Artifi-
cial Intelligence. MIT Press, Boston.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In AAAI-96.
J. B. Hirschberg, D. J. Litman, and M. Swerts. 2000.
Generalizing prosodic prediction of speech recog-
nition errors. In Proc. ICSLP-00, Beijing.
C. Kamm, S. Narayanan, D. Dutton, and
R. Ritenour. 1997. Evaluating spoken dia-
log systems for telecommunication services. In
Proc. EUROSPEECH-97, Rhodes.
E. Krahmer, M. Swerts, M. Theune, and
M. Weegels. 1999. Error spotting in human-
machine interactions. In Proc. EUROSPEECH-
99.
G. Levow. 1998. Characterizing and recognizing
spoken corrections in human-computer dialogue.
In Proc. COLING/ACL-98.
D. J. Litman and S. Pan. 1999. Empirically eval-
uating an adaptable spoken dialogue system. In
Proc. 7th Int&apos;l Conference on User Modeling.
D. J. Litman, M. A. Walker, and Michael J. Kearns.
1999. Automatic detection of poor speech recog-
nition at the dialogue level. In Proc. ACL-99.
D. J. Litman, J. B. Hirschberg, and M. Swerts. 2000.
Predicting automatic speech recognition perfor-
mance using prosodic cues. In Proc. NAACL-00,
Seattle.
S. L. Oviatt, G. Levow, M. MacEarchern, and
K. Kuhn. 1996. Modeling hyperarticulate speech
during human-computer error resolution. In
Proc. ICSLP-96, Philadelphia.
H. Soltau and A. Waibel. 2000. Specialized
acoustic models for hyperarticulated speech. In
Proc. ICASSP-00, Istanbul.
M. Swerts, D. Litman, and J. Hirschberg. 2000.
Corrections in spoken dialogue systems. In
Proc. ICSLP-00, Beijing.
E. Wade, E. E. Shriberg, and P. J. Price. 1992.
User behaviors affecting speech recognition. In
Proc. ICSLP-92, Banff.
M. Walker, C. Kamm, and D. Litman. 2000a. To-
wards developing general models of usability with
PARADISE. Natural Language Engineering, 6,
October.
M. A. Walker, I. Langkilde, J. Wright, A. Gorin,
and D. Litman. 2000b. Learning to predict prob-
lematic situations in a spoken dialogue system:
Experiments with how may i help you? In
Proc. NAACL-00, Seattle.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.149123">
<title confidence="0.9962695">Identifying User Corrections Automatically in Spoken Dialogue Systems</title>
<author confidence="0.688507">Hirschberg&apos;</author>
<author confidence="0.688507">Diane Litman&apos;</author>
<author confidence="0.688507">Marc</author>
<affiliation confidence="0.814777">apos;AT&amp;T Labs-Research, Florham Park, NJ, Eindhoven, The Netherlands, and CNTS, Antwerp,</affiliation>
<email confidence="0.658129">fju1ia/dianel@research.att.com,m.g.j.swertAtue.n1</email>
<abstract confidence="0.950344818181818">We present results of machine learning experiments designed to identify user corrections of speech recognition errors in a corpus collected from a train information spoken dialogue system. We investigate the predictive power of features automatically computable from the prosody of the turn, the speech recognition process, experimental conditions, and the dialogue history. Our best performing features reduce classification error from baselines of 25.70- 28.99% to 15.72%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bell</author>
<author>J Gustafson</author>
</authors>
<title>Repetition and its phonetic realizations: Investigating a Swedish database of spontaneous computerdirected speech.</title>
<date>1999</date>
<booktitle>In Proc. ICPhS-99,</booktitle>
<location>San Francisco.</location>
<contexts>
<context position="1364" citStr="Bell and Gustafson, 1999" startWordPosition="199" endWordPosition="202">70- 28.99% to 15.72%. 1 Introduction Users&apos; evaluations of spoken dialogue systems largely depend on the number of errors the system makes (Walker et al., 2000a) and how easy it is for the user and system to correct them. Poor automatic speech recognition (ASR) accuracy is the primary source of error in most systems, compounded by user behavior when confronted by such error. Studies have shown that speakers tend to switch to a prosodically &apos;marked&apos; speaking style hyperarticulated speech, e.g. I said BAL-TI-MORE, not Boston after ASR errors (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999). While possibly effective in human-human communicative settings, such correction behavior appears to lead to further errors in human-machine interactions (Levow, 1998; Soltau and Waibel, 2000), perhaps because it differs from the speech most recognizers are trained on. In addition, system responses indicating incorrect beliefs, such as an implicit verification containing mistaken information (e.g. Where do you want to go from Boston when the user has said she wants to depart from Baltimore), cause considerable difficulties for users faced with the need to correct a misconception and also answ</context>
</contexts>
<marker>Bell, Gustafson, 1999</marker>
<rawString>L. Bell and J. Gustafson. 1999. Repetition and its phonetic realizations: Investigating a Swedish database of spontaneous computerdirected speech. In Proc. ICPhS-99, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Boston.</location>
<contexts>
<context position="18702" citStr="Cohen, 1995" startWordPosition="2994" endWordPosition="2995">ed in Figure 2) used to learn the model. The second column, &apos;DIA&apos;, indicates which type of dialogue history features (PreTurn, PrepreTurn, Prior, and/or PMean) were also included in the feature set; these features represent the same types of information (e.g. fOmax) that the &apos;Features&apos; column denotes, but for one or more previous turns in the dialogue. The third column shows the mean error and standard error (SE) predicted for the model specified by the first two columns. When error estimates in different rows differ by plus or minus twice the standard error, they are significantly different (Cohen, 1995). The remaining columns show the mean recall, precision, and 1,3 = 1 for &apos;corrections&apos; (focus class=T) and &apos;non-corrections&apos; (focus class=F), respectively.4 For comparison purposes, 4 Recall is the percentage of actual members of a class that are identified, while precision is the percentage of predicted class members that are in fact members. The definition of +1)PrecisionRecall Fo is ,2 _re cicison„ +Recall /3 = 1 equally weights precision and recall. These values are computed using our own crossclass=T class=F Features DIA ErrorISE Rec. Prec. 1,3 = 1 Rec. Prec. 1,3 = 1 Raw+ASR+SYS+POS PreTu</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R. Cohen. 1995. Empirical Methods for Artificial Intelligence. MIT Press, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Learning trees and rules with setvalued features.</title>
<date>1996</date>
<booktitle>In AAAI-96.</booktitle>
<contexts>
<context position="16851" citStr="Cohen, 1996" startWordPosition="2693" endWordPosition="2694">orrections.3 Also, 3Recall that these are lexical features from the recognized string, not from the actual user transcript. whether a prior turn had been rejected was clearly a useful cue to the identification of the current turn as a correction, since users generally supplied a correction when explicitly asked. 4 Predicting Corrections In this section we investigate whether the features described in Section 3 (or interesting subsets of them) can in fact be used to accurately predict whether a turn will be a correction or not. We describe experiments using the machine learning program RIPPER (Cohen, 1996) to automatically induce such prediction models. RIPPER takes as input the classes to be learned, the names and possible values of a set of features, and training data specifying the class and feature values for each training example. For our experiments, the features presented in Figure 2 comprise the independent variables for our learning experiments. The dependent variable to be learned, correction (T) versus non-correction (F), corresponds to the hand-labeled observations described in Section 2. Given a vector of values for the independent and dependent variables for each speaker turn, RIP</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>W. Cohen. 1996. Learning trees and rules with setvalued features. In AAAI-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Hirschberg</author>
<author>D J Litman</author>
<author>M Swerts</author>
</authors>
<title>Generalizing prosodic prediction of speech recognition errors.</title>
<date>2000</date>
<booktitle>In Proc. ICSLP-00,</booktitle>
<location>Beijing.</location>
<contexts>
<context position="2804" citStr="Hirschberg et al., 2000" startWordPosition="424" endWordPosition="427">onality of the system or further restricting the vocabulary the system must recognize. However, as ASR accuracy improves, dialogue systems will be called upon to handle ever more complex tasks and ever less restricted vocabularies. So, it seems likely that spoken dialogue systems will, for the foreseeable future, always require effective error detection and repair strategies. Ideally, such repair strategies, would consist of steps to immediately detect an error when it occurs and to interact with the user to correct the error in subsequent exchanges. In previous research (Litman et al., 2000; Hirschberg et al., 2000), we identified new procedures to detect recognition errors, which perform well when tested on two different corpora, the TOOT and W99 corpora (train information and conference registration dialogues) collected using two different ASR systems). We found that prosodic features, in combination with information already available to the recognizer, such as acoustic confidence scores, grammar and recognized string, can distinguish speaker turns that are misrecognized far better than traditional methods for ASR rejection (the system decision that its hypothesis is so weak that it should reprompt for</context>
</contexts>
<marker>Hirschberg, Litman, Swerts, 2000</marker>
<rawString>J. B. Hirschberg, D. J. Litman, and M. Swerts. 2000. Generalizing prosodic prediction of speech recognition errors. In Proc. ICSLP-00, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kamm</author>
<author>S Narayanan</author>
<author>D Dutton</author>
<author>R Ritenour</author>
</authors>
<title>Evaluating spoken dialog systems for telecommunication services.</title>
<date>1997</date>
<booktitle>In Proc. EUROSPEECH-97,</booktitle>
<location>Rhodes.</location>
<contexts>
<context position="6593" citStr="Kamm et al., 1997" startWordPosition="1019" endWordPosition="1022"> the results of those experiments. In Section 5 we summarize our conclusions and describe future research directions. 2 The TOOT Corpus Our corpus consists of 152 dialogues between human subjects and TOOT, a spoken dialogue system that allowed users access to train information from the web via telephone. The TOOT corpus was collected in a laboratory setting, to study variations in dialogue strategy and in user-adapted interaction (Litman and Pan, 1999). TOOT was implemented using an interactive voice response platform developed at AT&amp;T, combining ASR and text-to-speech with a phone interface (Kamm et al., 1997). The system&apos;s speech recognizer was a speaker-independent hidden Markov model system, with context-dependent phone models for telephone speech and constrained grammars defining the vocabulary allowed at any dialogue state. The platform supported barge-in. This system rejected a turn whenever its acoustic confidence score fell below thresholds predefined for each dialogue state. Subjects were 39 student interns; 20 native speakers and 19 non-native, 16 female and 23 male. They were asked to perform four tasks using one of several versions of the system that differed in terms of locus of initia</context>
</contexts>
<marker>Kamm, Narayanan, Dutton, Ritenour, 1997</marker>
<rawString>C. Kamm, S. Narayanan, D. Dutton, and R. Ritenour. 1997. Evaluating spoken dialog systems for telecommunication services. In Proc. EUROSPEECH-97, Rhodes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Swerts</author>
<author>M Theune</author>
<author>M Weegels</author>
</authors>
<title>Error spotting in humanmachine interactions.</title>
<date>1999</date>
<booktitle>In Proc. EUROSPEECH99.</booktitle>
<contexts>
<context position="2004" citStr="Krahmer et al., 1999" startWordPosition="298" endWordPosition="301">fective in human-human communicative settings, such correction behavior appears to lead to further errors in human-machine interactions (Levow, 1998; Soltau and Waibel, 2000), perhaps because it differs from the speech most recognizers are trained on. In addition, system responses indicating incorrect beliefs, such as an implicit verification containing mistaken information (e.g. Where do you want to go from Boston when the user has said she wants to depart from Baltimore), cause considerable difficulties for users faced with the need to correct a misconception and also answer a new question (Krahmer et al., 1999). To date, attempts to improve system performance have largely focussed on improving ASR accuracy, or simplifying the task, either by further constraining the domain and functionality of the system or further restricting the vocabulary the system must recognize. However, as ASR accuracy improves, dialogue systems will be called upon to handle ever more complex tasks and ever less restricted vocabularies. So, it seems likely that spoken dialogue systems will, for the foreseeable future, always require effective error detection and repair strategies. Ideally, such repair strategies, would consis</context>
</contexts>
<marker>Krahmer, Swerts, Theune, Weegels, 1999</marker>
<rawString>E. Krahmer, M. Swerts, M. Theune, and M. Weegels. 1999. Error spotting in humanmachine interactions. In Proc. EUROSPEECH99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Levow</author>
</authors>
<title>Characterizing and recognizing spoken corrections in human-computer dialogue.</title>
<date>1998</date>
<booktitle>In Proc. COLING/ACL-98.</booktitle>
<contexts>
<context position="1337" citStr="Levow, 1998" startWordPosition="197" endWordPosition="198">elines of 25.70- 28.99% to 15.72%. 1 Introduction Users&apos; evaluations of spoken dialogue systems largely depend on the number of errors the system makes (Walker et al., 2000a) and how easy it is for the user and system to correct them. Poor automatic speech recognition (ASR) accuracy is the primary source of error in most systems, compounded by user behavior when confronted by such error. Studies have shown that speakers tend to switch to a prosodically &apos;marked&apos; speaking style hyperarticulated speech, e.g. I said BAL-TI-MORE, not Boston after ASR errors (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999). While possibly effective in human-human communicative settings, such correction behavior appears to lead to further errors in human-machine interactions (Levow, 1998; Soltau and Waibel, 2000), perhaps because it differs from the speech most recognizers are trained on. In addition, system responses indicating incorrect beliefs, such as an implicit verification containing mistaken information (e.g. Where do you want to go from Boston when the user has said she wants to depart from Baltimore), cause considerable difficulties for users faced with the need to correct a </context>
</contexts>
<marker>Levow, 1998</marker>
<rawString>G. Levow. 1998. Characterizing and recognizing spoken corrections in human-computer dialogue. In Proc. COLING/ACL-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>S Pan</author>
</authors>
<title>Empirically evaluating an adaptable spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proc. 7th Int&apos;l Conference on User Modeling.</booktitle>
<contexts>
<context position="6431" citStr="Litman and Pan, 1999" startWordPosition="994" endWordPosition="998">e TOOT spoken dialogue corpus, which we describe in Section 2. In Section 3, we describe the features we use for our machine learning experiments. Section 4 presents the results of those experiments. In Section 5 we summarize our conclusions and describe future research directions. 2 The TOOT Corpus Our corpus consists of 152 dialogues between human subjects and TOOT, a spoken dialogue system that allowed users access to train information from the web via telephone. The TOOT corpus was collected in a laboratory setting, to study variations in dialogue strategy and in user-adapted interaction (Litman and Pan, 1999). TOOT was implemented using an interactive voice response platform developed at AT&amp;T, combining ASR and text-to-speech with a phone interface (Kamm et al., 1997). The system&apos;s speech recognizer was a speaker-independent hidden Markov model system, with context-dependent phone models for telephone speech and constrained grammars defining the vocabulary allowed at any dialogue state. The platform supported barge-in. This system rejected a turn whenever its acoustic confidence score fell below thresholds predefined for each dialogue state. Subjects were 39 student interns; 20 native speakers and</context>
</contexts>
<marker>Litman, Pan, 1999</marker>
<rawString>D. J. Litman and S. Pan. 1999. Empirically evaluating an adaptable spoken dialogue system. In Proc. 7th Int&apos;l Conference on User Modeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>M A Walker</author>
<author>Michael J Kearns</author>
</authors>
<title>Automatic detection of poor speech recognition at the dialogue level. In</title>
<date>1999</date>
<booktitle>Proc. ACL-99.</booktitle>
<contexts>
<context position="15181" citStr="Litman et al., 1999" startWordPosition="2414" endWordPosition="2417">mbined initiative and confirmation setting (realstrat). 3.4 Dialogue Position and History Features (Swerts et al., 2000) also showed that the further a correction is from the original error, the less likely it is to be recognized correctly, and the stronger the correlation with prosodic deviation from the mean values over a speaker&apos;s turns (e.g., more distant corrections are higher in pitch than closer corrections). As a first approximation of this distance feature, we included the feature diadist distance of the current turn from the beginning of the dialogue. In addition, previous research (Litman et al., 1999; Walker et al., 2000b) has shown that features of the dialogue as a whole and features of more local context can be helpful in predicting &apos;problematic&apos; dialogues. So we looked at a set of features summarizing aspects of the prior dialogue for both the absolute number of times prior turns exhibited certain characteristics (e.g., contained a key word like cancel priorcancnum) and the percentage of the prior dialogue containing one of these features (e.g. priorcancpct). We also examined means for all our continuous-valued features over the entire dialogue preceding the turn to be predicted (pmn)</context>
</contexts>
<marker>Litman, Walker, Kearns, 1999</marker>
<rawString>D. J. Litman, M. A. Walker, and Michael J. Kearns. 1999. Automatic detection of poor speech recognition at the dialogue level. In Proc. ACL-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>J B Hirschberg</author>
<author>M Swerts</author>
</authors>
<title>Predicting automatic speech recognition performance using prosodic cues.</title>
<date>2000</date>
<booktitle>In Proc. NAACL-00,</booktitle>
<location>Seattle.</location>
<contexts>
<context position="2778" citStr="Litman et al., 2000" startWordPosition="419" endWordPosition="423">the domain and functionality of the system or further restricting the vocabulary the system must recognize. However, as ASR accuracy improves, dialogue systems will be called upon to handle ever more complex tasks and ever less restricted vocabularies. So, it seems likely that spoken dialogue systems will, for the foreseeable future, always require effective error detection and repair strategies. Ideally, such repair strategies, would consist of steps to immediately detect an error when it occurs and to interact with the user to correct the error in subsequent exchanges. In previous research (Litman et al., 2000; Hirschberg et al., 2000), we identified new procedures to detect recognition errors, which perform well when tested on two different corpora, the TOOT and W99 corpora (train information and conference registration dialogues) collected using two different ASR systems). We found that prosodic features, in combination with information already available to the recognizer, such as acoustic confidence scores, grammar and recognized string, can distinguish speaker turns that are misrecognized far better than traditional methods for ASR rejection (the system decision that its hypothesis is so weak t</context>
</contexts>
<marker>Litman, Hirschberg, Swerts, 2000</marker>
<rawString>D. J. Litman, J. B. Hirschberg, and M. Swerts. 2000. Predicting automatic speech recognition performance using prosodic cues. In Proc. NAACL-00, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>G Levow</author>
<author>M MacEarchern</author>
<author>K Kuhn</author>
</authors>
<title>Modeling hyperarticulate speech during human-computer error resolution.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP-96,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="1324" citStr="Oviatt et al., 1996" startWordPosition="192" endWordPosition="196">cation error from baselines of 25.70- 28.99% to 15.72%. 1 Introduction Users&apos; evaluations of spoken dialogue systems largely depend on the number of errors the system makes (Walker et al., 2000a) and how easy it is for the user and system to correct them. Poor automatic speech recognition (ASR) accuracy is the primary source of error in most systems, compounded by user behavior when confronted by such error. Studies have shown that speakers tend to switch to a prosodically &apos;marked&apos; speaking style hyperarticulated speech, e.g. I said BAL-TI-MORE, not Boston after ASR errors (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999). While possibly effective in human-human communicative settings, such correction behavior appears to lead to further errors in human-machine interactions (Levow, 1998; Soltau and Waibel, 2000), perhaps because it differs from the speech most recognizers are trained on. In addition, system responses indicating incorrect beliefs, such as an implicit verification containing mistaken information (e.g. Where do you want to go from Boston when the user has said she wants to depart from Baltimore), cause considerable difficulties for users faced with the need </context>
</contexts>
<marker>Oviatt, Levow, MacEarchern, Kuhn, 1996</marker>
<rawString>S. L. Oviatt, G. Levow, M. MacEarchern, and K. Kuhn. 1996. Modeling hyperarticulate speech during human-computer error resolution. In Proc. ICSLP-96, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Soltau</author>
<author>A Waibel</author>
</authors>
<title>Specialized acoustic models for hyperarticulated speech.</title>
<date>2000</date>
<booktitle>In Proc. ICASSP-00,</booktitle>
<location>Istanbul.</location>
<contexts>
<context position="1557" citStr="Soltau and Waibel, 2000" startWordPosition="224" endWordPosition="227">and system to correct them. Poor automatic speech recognition (ASR) accuracy is the primary source of error in most systems, compounded by user behavior when confronted by such error. Studies have shown that speakers tend to switch to a prosodically &apos;marked&apos; speaking style hyperarticulated speech, e.g. I said BAL-TI-MORE, not Boston after ASR errors (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999). While possibly effective in human-human communicative settings, such correction behavior appears to lead to further errors in human-machine interactions (Levow, 1998; Soltau and Waibel, 2000), perhaps because it differs from the speech most recognizers are trained on. In addition, system responses indicating incorrect beliefs, such as an implicit verification containing mistaken information (e.g. Where do you want to go from Boston when the user has said she wants to depart from Baltimore), cause considerable difficulties for users faced with the need to correct a misconception and also answer a new question (Krahmer et al., 1999). To date, attempts to improve system performance have largely focussed on improving ASR accuracy, or simplifying the task, either by further constrainin</context>
</contexts>
<marker>Soltau, Waibel, 2000</marker>
<rawString>H. Soltau and A. Waibel. 2000. Specialized acoustic models for hyperarticulated speech. In Proc. ICASSP-00, Istanbul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swerts</author>
<author>D Litman</author>
<author>J Hirschberg</author>
</authors>
<title>Corrections in spoken dialogue systems.</title>
<date>2000</date>
<booktitle>In Proc. ICSLP-00,</booktitle>
<location>Beijing.</location>
<contexts>
<context position="3618" citStr="Swerts et al., 2000" startWordPosition="550" endWordPosition="553">ogues) collected using two different ASR systems). We found that prosodic features, in combination with information already available to the recognizer, such as acoustic confidence scores, grammar and recognized string, can distinguish speaker turns that are misrecognized far better than traditional methods for ASR rejection (the system decision that its hypothesis is so weak that it should reprompt for fresh input), which use acoustic confidence scores alone. Now we have turned to the question of how users correct such errors. In a descriptive analysis of user corrections in the TOOT corpus (Swerts et al., 2000), we found that corrections differ significantly from non-corrections prosodically, being higher in pitch, louder, longer, with longer pauses preceding them and less internal silence. They are also misrecognized more frequently than non-corrections though they are no more likely to be rejected by the system. And corrections more distant from the error they correct tend to exhibit greater prosodic differences and are recognized more poorly, suggesting that users are not learning to modify their own behavior to improve system performance. So, dealing with corrections is a particularly difficult </context>
<context position="9296" citStr="Swerts et al., 2000" startWordPosition="1450" endWordPosition="1453">t was subsequently resolved by consensus. 675 of the 2328 user turns in the corpus (29%) were judged to be corrections. A sample interaction between TOOT and a user, containing the different correction types and their labels is given in Figure 1. 3 Characterizing Corrections In this section we describe the features used in the machine learning experiments described in Section 4 and the motivation behind their selection. The entire feature set is presented in Figure 2, and includes only features that could be automatically available to a dialogue system. 3.1 Prosodic Features A previous study (Swerts et al., 2000) showed that corrections were significantly longer, louder, higher in pitch excursion, and followed longer pauses than other turns. Thus, we expected that these features would be useful in identifying corrections automatically. We examined maximum and mean fundamental frequency values (f0max, fOmn) as indicators of pitch range; maximum and mean energy values (rmsmax, rmsmn) as indicators of loudness; total duration of the speaker turn (dur); length of pause preceding the turn (ppau); speaking rate (tempo); and amount of silence within the turn fOmax fOmn rmsmax rmsmn dur ppau tempo zeros Mean </context>
<context position="13040" citStr="Swerts et al., 2000" startWordPosition="2078" endWordPosition="2081">ASR features for preceding turn (e.g., prefOmax) PrepreTurn : value of PROS and ASR features for turn preceding preceding turn (e.g., pprefOmax) Prior : for each boolean-valued feature (ynstr, nofeat, canc, help, rejbool), the number/percentage of prior turns exhibiting the feature (e.g., priorynstrnum/priorynstrpct) PMean : for each continuous-valued PROS and ASR feature, the mean of the feature&apos;s value over all prior turns (e g, pmnfOmax) Figure 2: Feature Set for Predicting Corrections 3.2 ASR Features Since corrections in our corpus were misrecognized more frequently than non-corrections (Swerts et al., 2000), we included a set of ASR features that were derived from TooT&apos;s speech recognition component and its outputs: the grammar used as the ASR language model at each dialogue state (gram), the recognizer&apos;s best hypothesis (str), and the turn-level acoustic confidence score it produced (conf).2 As subcases of the str feature, we included boolean features representing whether or not the recognized string included the strings yes or no (ynstr), some variant of no, such as nope (nofeat), cancel (cane), or help (help), as these lexical items often occurred during problem resolution. To estimate durati</context>
<context position="14682" citStr="Swerts et al., 2000" startWordPosition="2332" endWordPosition="2335"> of user corrections. For example, TOOT users more frequently repeat their misrecognized turns and produce the fewest corrections per task when TOOT has the initiative and explicitly confirms all user input. So, we hypothesized that system conditions might prove important in our learning experiments. We thus include features representing the system&apos;s current initiative and confirmation strategies (inittype, conftype), whether users could adapt the system&apos;s dialogue strategies (adapt), and the combined initiative and confirmation setting (realstrat). 3.4 Dialogue Position and History Features (Swerts et al., 2000) also showed that the further a correction is from the original error, the less likely it is to be recognized correctly, and the stronger the correlation with prosodic deviation from the mean values over a speaker&apos;s turns (e.g., more distant corrections are higher in pitch than closer corrections). As a first approximation of this distance feature, we included the feature diadist distance of the current turn from the beginning of the dialogue. In addition, previous research (Litman et al., 1999; Walker et al., 2000b) has shown that features of the dialogue as a whole and features of more local</context>
</contexts>
<marker>Swerts, Litman, Hirschberg, 2000</marker>
<rawString>M. Swerts, D. Litman, and J. Hirschberg. 2000. Corrections in spoken dialogue systems. In Proc. ICSLP-00, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Wade</author>
<author>E E Shriberg</author>
<author>P J Price</author>
</authors>
<title>User behaviors affecting speech recognition.</title>
<date>1992</date>
<booktitle>In Proc. ICSLP-92,</booktitle>
<location>Banff.</location>
<contexts>
<context position="1303" citStr="Wade et al., 1992" startWordPosition="188" endWordPosition="191">res reduce classification error from baselines of 25.70- 28.99% to 15.72%. 1 Introduction Users&apos; evaluations of spoken dialogue systems largely depend on the number of errors the system makes (Walker et al., 2000a) and how easy it is for the user and system to correct them. Poor automatic speech recognition (ASR) accuracy is the primary source of error in most systems, compounded by user behavior when confronted by such error. Studies have shown that speakers tend to switch to a prosodically &apos;marked&apos; speaking style hyperarticulated speech, e.g. I said BAL-TI-MORE, not Boston after ASR errors (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999). While possibly effective in human-human communicative settings, such correction behavior appears to lead to further errors in human-machine interactions (Levow, 1998; Soltau and Waibel, 2000), perhaps because it differs from the speech most recognizers are trained on. In addition, system responses indicating incorrect beliefs, such as an implicit verification containing mistaken information (e.g. Where do you want to go from Boston when the user has said she wants to depart from Baltimore), cause considerable difficulties for users</context>
</contexts>
<marker>Wade, Shriberg, Price, 1992</marker>
<rawString>E. Wade, E. E. Shriberg, and P. J. Price. 1992. User behaviors affecting speech recognition. In Proc. ICSLP-92, Banff.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>C Kamm</author>
<author>D Litman</author>
</authors>
<title>Towards developing general models of usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<contexts>
<context position="898" citStr="Walker et al., 2000" startWordPosition="119" endWordPosition="122">stract We present results of machine learning experiments designed to identify user corrections of speech recognition errors in a corpus collected from a train information spoken dialogue system. We investigate the predictive power of features automatically computable from the prosody of the turn, the speech recognition process, experimental conditions, and the dialogue history. Our best performing features reduce classification error from baselines of 25.70- 28.99% to 15.72%. 1 Introduction Users&apos; evaluations of spoken dialogue systems largely depend on the number of errors the system makes (Walker et al., 2000a) and how easy it is for the user and system to correct them. Poor automatic speech recognition (ASR) accuracy is the primary source of error in most systems, compounded by user behavior when confronted by such error. Studies have shown that speakers tend to switch to a prosodically &apos;marked&apos; speaking style hyperarticulated speech, e.g. I said BAL-TI-MORE, not Boston after ASR errors (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999). While possibly effective in human-human communicative settings, such correction behavior appears to lead to further errors in human-</context>
<context position="15202" citStr="Walker et al., 2000" startWordPosition="2418" endWordPosition="2421"> confirmation setting (realstrat). 3.4 Dialogue Position and History Features (Swerts et al., 2000) also showed that the further a correction is from the original error, the less likely it is to be recognized correctly, and the stronger the correlation with prosodic deviation from the mean values over a speaker&apos;s turns (e.g., more distant corrections are higher in pitch than closer corrections). As a first approximation of this distance feature, we included the feature diadist distance of the current turn from the beginning of the dialogue. In addition, previous research (Litman et al., 1999; Walker et al., 2000b) has shown that features of the dialogue as a whole and features of more local context can be helpful in predicting &apos;problematic&apos; dialogues. So we looked at a set of features summarizing aspects of the prior dialogue for both the absolute number of times prior turns exhibited certain characteristics (e.g., contained a key word like cancel priorcancnum) and the percentage of the prior dialogue containing one of these features (e.g. priorcancpct). We also examined means for all our continuous-valued features over the entire dialogue preceding the turn to be predicted (pmn), such as pmnsyls, th</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>M. Walker, C. Kamm, and D. Litman. 2000a. Towards developing general models of usability with PARADISE. Natural Language Engineering, 6, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>I Langkilde</author>
<author>J Wright</author>
<author>A Gorin</author>
<author>D Litman</author>
</authors>
<title>Learning to predict problematic situations in a spoken dialogue system: Experiments with how may i help you?</title>
<date>2000</date>
<booktitle>In Proc. NAACL-00,</booktitle>
<location>Seattle.</location>
<contexts>
<context position="898" citStr="Walker et al., 2000" startWordPosition="119" endWordPosition="122">stract We present results of machine learning experiments designed to identify user corrections of speech recognition errors in a corpus collected from a train information spoken dialogue system. We investigate the predictive power of features automatically computable from the prosody of the turn, the speech recognition process, experimental conditions, and the dialogue history. Our best performing features reduce classification error from baselines of 25.70- 28.99% to 15.72%. 1 Introduction Users&apos; evaluations of spoken dialogue systems largely depend on the number of errors the system makes (Walker et al., 2000a) and how easy it is for the user and system to correct them. Poor automatic speech recognition (ASR) accuracy is the primary source of error in most systems, compounded by user behavior when confronted by such error. Studies have shown that speakers tend to switch to a prosodically &apos;marked&apos; speaking style hyperarticulated speech, e.g. I said BAL-TI-MORE, not Boston after ASR errors (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999). While possibly effective in human-human communicative settings, such correction behavior appears to lead to further errors in human-</context>
<context position="15202" citStr="Walker et al., 2000" startWordPosition="2418" endWordPosition="2421"> confirmation setting (realstrat). 3.4 Dialogue Position and History Features (Swerts et al., 2000) also showed that the further a correction is from the original error, the less likely it is to be recognized correctly, and the stronger the correlation with prosodic deviation from the mean values over a speaker&apos;s turns (e.g., more distant corrections are higher in pitch than closer corrections). As a first approximation of this distance feature, we included the feature diadist distance of the current turn from the beginning of the dialogue. In addition, previous research (Litman et al., 1999; Walker et al., 2000b) has shown that features of the dialogue as a whole and features of more local context can be helpful in predicting &apos;problematic&apos; dialogues. So we looked at a set of features summarizing aspects of the prior dialogue for both the absolute number of times prior turns exhibited certain characteristics (e.g., contained a key word like cancel priorcancnum) and the percentage of the prior dialogue containing one of these features (e.g. priorcancpct). We also examined means for all our continuous-valued features over the entire dialogue preceding the turn to be predicted (pmn), such as pmnsyls, th</context>
</contexts>
<marker>Walker, Langkilde, Wright, Gorin, Litman, 2000</marker>
<rawString>M. A. Walker, I. Langkilde, J. Wright, A. Gorin, and D. Litman. 2000b. Learning to predict problematic situations in a spoken dialogue system: Experiments with how may i help you? In Proc. NAACL-00, Seattle.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>