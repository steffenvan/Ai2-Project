<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000206">
<title confidence="0.880712">
Identifying fake Amazon reviews as learning from crowds
</title>
<author confidence="0.463177">
Tommaso Fornaciari
</author>
<note confidence="0.834724">
Ministero dell’Interno
Dipartimento della Pubblica Sicurezza
Segreteria del Dipartimento
ComISSIT
</note>
<email confidence="0.978512">
tommaso.fornaciari@interno.it
</email>
<sectionHeader confidence="0.993453" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999565">
Customers who buy products such as
books online often rely on other customers
reviews more than on reviews found on
specialist magazines. Unfortunately the
confidence in such reviews is often mis-
placed due to the explosion of so-called
sock puppetry–authors writing glowing
reviews of their own books. Identifying
such deceptive reviews is not easy. The
first contribution of our work is the cre-
ation of a collection including a number
of genuinely deceptive Amazon book re-
views in collaboration with crime writer
Jeremy Duns, who has devoted a great
deal of effort in unmasking sock puppet-
ing among his colleagues. But there can
be no certainty concerning the other re-
views in the collection: all we have is a
number of cues, also developed in collab-
oration with Duns, suggesting that a re-
view may be genuine or deceptive. Thus
this corpus is an example of a collection
where it is not possible to acquire the
actual label for all instances, and where
clues of deception were treated as anno-
tators who assign them heuristic labels. A
number of approaches have been proposed
for such cases; we adopt here the ‘learn-
ing from crowds’ approach proposed by
Raykar et al. (2010). Thanks to Duns’ cer-
tainly fake reviews, the second contribu-
tion of this work consists in the evaluation
of the effectiveness of different methods of
annotation, according to the performance
of models trained to detect deceptive re-
views.
</bodyText>
<sectionHeader confidence="0.998637" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9379155">
Customer reviews of books, hotels and other prod-
ucts are widely perceived as an important rea-
</bodyText>
<author confidence="0.617387">
Massimo Poesio
</author>
<affiliation confidence="0.63641975">
University of Essex
CSEE
University of Trento
CIMeC
</affiliation>
<email confidence="0.958777">
poesio@essex.ac.uk
</email>
<bodyText confidence="0.999971333333334">
son for the success of e-commerce sites such as
amazon.com or tripadvisor.com. How-
ever, customer confidence in such reviews is often
misplaced, due to the growth of the so-called sock
puppetry phenomenon: authors / hoteliers writing
glowing reviews of their own works / hotels (and
occasionally also negative reviews of the competi-
tors).1 The prevalence of this phenomenon has
been revealed by campaigners such as crime writer
Jeremy Duns, who exposed a number of fellow au-
thors involved in such practices.2 A number of
sites have also emerged offering Amazon reviews
to authors for a fee.3
Several automatic techniques for exposing such
deceptive reviews have been proposed in recent
years (Feng et al., 2012; Ott et al., 2001). But like
all work on deceptive language (computational or
otherwise) (Newman et al., 2003; Strapparava and
Mihalcea, 2009), such works suffer from a seri-
ous problem: the lack of a gold standard contain-
ing ‘real life’ examples of deceptive uses of lan-
guage. This is because it is very difficult to find
definite proof that an Amazon review is either de-
ceptive or genuine. Thus most researchers recre-
ate deceptive behavior in the lab, as done by New-
man et al. (2003). For instance, Ott et al. (2001),
Feng et al. (2012) and Strapparava and Mihalcea
(2009) used crowdsourcing, asking turkers to pro-
duce instances of deceptive behavior. Finally, Li
et al. (2011) classify reviews as deceptive or truth-
ful by hand on the basis of a series of heuristics:
they start by excluding anonymous reviews, then
use their helpfulness and other criteria to decide
</bodyText>
<footnote confidence="0.797805">
1The phenomenon predates Internet - see e.g., Amy Har-
mon, ‘Amazon Glitch Unmasks War Of Reviewers’, New
York Times, February 14, 2004.
2See Andrew Hough, ‘RJ Ellory: fake book reviews
are rife on internet, authors warn’, telegraph.co.uk,
September 3, 2012
3See Alison Flood, ‘Sock puppetry and fake reviews:
publish and be damned’, guardian.co.uk, September 4,
2012 and David Streitfeld, ‘Buy Reviews on Yelp, Get Black
Mark’, nytimes.com, October 18, 2012.
</footnote>
<page confidence="0.957673">
279
</page>
<note confidence="0.993163">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 279–287,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999808571428571">
whether they are deceptive or not. Clearly a more
rigorous approach to establishing the truth or oth-
erwise of reviews on the basis of such heuristic
criteria would be useful.
In this work we develop a system for identify-
ing deceptive reviews in Amazon. Our proposal
makes two main contributions:
</bodyText>
<listItem confidence="0.9654045">
1. we identified in collaboration with Jeremy
Duns a series of criteria used by Duns and
other ‘sock puppet hunters’ to find suspicious
reviews / reviewers, and collected a dataset of
reviews some of which are certainly false as
the authors admitted so, whereas others may
be genuine or deceptive.
2. we developed an approach to the truthful-
ness of reviews based on the notion that the
truthfulness of a review is a latent variable
whose value cannot be known, but can be es-
timated using some criteria as potential indi-
cators of such value–as annotators–and then
we used the learning from crowds algorithm
proposed by Raykar et al. (2010) to assign a
class to each review in the dataset.
</listItem>
<bodyText confidence="0.9974745">
The structure of the paper is as follows. In Sec-
tion 2 we describe how we collected our dataset;
in Section 3 we show the experiments we carried
out and in Section 4 we discuss the results.
</bodyText>
<sectionHeader confidence="0.912084" genericHeader="method">
2 Deception clues and dataset
</sectionHeader>
<subsectionHeader confidence="0.999783">
2.1 Examples of Unmasked Sock Puppetry
</subsectionHeader>
<bodyText confidence="0.999815868421053">
After reading an article by Alison Flood on The
Guardian of September 4th, 2012 4, discussing
how crime writer Jeremy Duns had unmasked a
number of ‘sock puppeteers,’ we contacted him.
Duns was extremely helpful; he pointed us to the
other articles on the topic, mostly on The New York
Times, and helped us create a set of deception
clues and the dataset used in this work.
On July 25th, 2011, an article appeared on
www.moneytalksnews.com, entitled ‘3 Tips
for Spotting Fake Product Reviews - From Some-
one Who Wrote Them’.5 Sandra Parker, author
of the text, in that page described her experience
as ‘professional review writer’. According to her
statements, advertising agencies were used to pay
her $10-20 for writing reviews on sites like Ama-
zon.com. She was not asked to lie, but ‘if the re-
view wasn’t five star, they didn’t pay’. In an arti-
cle of August 19th, written by David Streitfeld on
www.nytimes.com,6 she actually denied that
point: ‘We were not asked to provide a five-star
review, but would be asked to turn down an as-
signment if we could not give one’.
In any case, in her article Sandra Parker gave
the readers some common sense-based advices, in
order to help them to recognize possible fake re-
views. One of these suggestions were also useful
for this study, as discussed in Section 2.3. From
our point of view, however, the most interesting
aspect of the article relied in the fact that, letting
know the name of an author of fake reviews, it
made possible to identify them in Amazon.com,
with an high degree of confidence.
A further article written on August 25th by
David Streitfeld gave us another similar opportu-
nity.7 In fact, thanks to his survey, it was possible
to come to know the titles of four books, whose the
authors paid an agency in order to receive reviews.
</bodyText>
<subsectionHeader confidence="0.998777">
2.2 The corpus
</subsectionHeader>
<bodyText confidence="0.99812005">
Using the suggestions of Jeremy Duns and the in-
formation in these articles we built a corpus we
called DEREV (DEception in REViews), consist-
ing of clearly fake, possibly fake, and possibly
genuine book reviews posted on www.amazon.
com. The corpus, which will be freely available
on demand, consists of 6819 reviews downloaded
from www.amazon.com, concerning 68 books
and written by 4811 different reviewers. The 68
books were chosen trying to balance the number
of reviews (our units of analysis) related to sus-
pect books which probably or surely received fake
reviews, with the number of reviews hypothesized
to be genuine in that we expected the authors of
the books not to have bought reviews. In partic-
ular, we put into the group of the suspect books -
henceforth SB - the reviews of the four books in-
dicated by David Streitfeld. To this first nucleus,
we also added other four books, written by three
of the authors of the previous group. We also in-
</bodyText>
<footnote confidence="0.9887045">
4Sock puppetry and fake reviews: publish and be damned, 6http://www.nytimes.com/2011/08/20/
http://www.guardian.co.uk/books/2012/ technology/finding-fake-reviews-online.
sep/04/sock-puppetry-publish-be-damned html?_r=1&amp;
5http://www.moneytalksnews.com/2011/07/25 7http://www.nytimes.com/2012/08/26/busin
/3-tips-for-spotting-fake-product-reviews-- ess/book-reviewers-for-hire-meet-a-demand-
-from-someone-who-wrote-them/ for-online-raves.html?pagewanted=all
</footnote>
<page confidence="0.996725">
280
</page>
<bodyText confidence="0.999984043478261">
cluded in the SB group the 22 books for which
Sandra Parker wrote a review. Lastly, we noticed
that some reviewers of the books pointed out by
David Streitfeld tended to write reviews of the
same books: we identified 16 of them, and consid-
ered suspect as well. In total, on November 17th,
2011 we downloaded the reviews of 46 books con-
sidered as suspect, which received 2707 reviews.8
We also collected the reviews of 22 so called ‘in-
nocent books’, for a total of 4112 reviews. These
books were mainly chosen among classic authors,
such as Conan Doyle or Kipling, or among liv-
ing writers who are so renowned that any reviews’
purchase would be pointless: this is the case, for
example, of Ken Follett and Stephen King. As
shown by the number of the reviews, the books
of these authors are so famous that they receive a
great amount of readers’ opinions.
The size of DEREV is 1175410 tokens, con-
sidering punctuation blocks as single token. The
mean size of the reviews is 172.37 tokens. The ti-
tles of the reviews were neither included in these
statistics nor in the following analyses.
</bodyText>
<subsectionHeader confidence="0.998878">
2.3 Deception clues
</subsectionHeader>
<bodyText confidence="0.999221227272727">
Once created the corpus, we identified a set of
clues, whose presence suggested the deceptiveness
of the reviews. These clues are:
Suspect Book - SB The first clue of deceptive-
ness was the reference of the reviews to a sus-
pect book, identified as described above. This
is the only clue which is constant for all the
reviews of the same book.
Cluster - Cl The second clue comes from the
suggestions given by Sandra Parker in her
mentioned article. As she pointed out, the
agencies she worked for were used to give her
48 hours to write a review. Being likely that
the same deadline was given to other review-
ers, Sandra Parker warns to pay attention if
the books receive many reviews in a short pe-
riod of time. Following her advice, we con-
sidered as positive this clue of deceptiveness
if the review belonged to a group of at least
two reviews posted within 3 days.
Nickname - NN A service provided by Amazon
is the possibility for the reviewers to register
8We specify the date of the download because, obviously,
if the data collection would be repeated today, the overall
number of reviews would be greater.
in the website and to post comments using
their real name. Since the real identity of the
reviewers involves issues related to their rep-
utation, we supposed it is less probable that
the writers of fake reviews post their texts us-
ing their true name. Moreover, a similar as-
sumption was probably accepted by Li et al.
(2011), who considered the profile features of
the reviewers, and among them the use or not
of their real name.
Unknown Purchase - UP Lastly, the probably
most interesting information provided by
Amazon is whether the reviewer bought the
reviewed book through Amazon itself. It
is reasonable to think that, if the reviewer
bought the book, he also read it. Therefore,
the absence of information about the certified
purchase was considered a clue of deceptive-
ness.
</bodyText>
<subsectionHeader confidence="0.988436">
2.4 Gold and silver standard
</subsectionHeader>
<bodyText confidence="0.9461538">
The clues of deception discussed above give us
a heuristic estimate of the truthfulness of the re-
views. Such estimation represents a silver stan-
dard of our classes, as these are not determined
through certain knowledge of the ground truth, but
simply thanks to hints of deceptiveness. The meth-
ods we used in order to assign the heuristic classes
to the reviews are described in the next Section;
however for our purposes we needed a gold stan-
dard, that is at least a subset of reviews whose
ground truth was known with a high degree of con-
fidence. This subset was identified as follows.
First, we considered as false the 22 reviews
published by Sandra Parker, even though not all
her reviews are characterized by the presence of
all the deception clues. Even though we cannot
really say whether her reviews reflect her opin-
ion of the books in question or not, she explic-
itly claimed to have been paid for writing them;
and she only bought on Amazon three of these
22 books. This is the most accurate knowledge
about fake reviews not artificially produced we
have found in literature. Then we focused on the
four books whose authors admitted to have bought
the reviews.9 Three of them received many re-
views, which made it difficult to understand if
they were truthful or not. However, one of these
9http://www.nytimes.com/2012/08/26/busin
ess/book-reviewers-for-hire-meet-a-demand-
for-online-raves.html?pagewanted=all
</bodyText>
<page confidence="0.998313">
281
</page>
<tableCaption confidence="0.931371">
Table 1: The distribution of deception clues in the
reviews
</tableCaption>
<bodyText confidence="0.909368125">
Nr. clues Reviews Tot. %
False 4 903
rev. 3 1913 2816 41.30%
True 2 2528
rev. 1 1210
0 265 4003 58.70%
books (‘Write your first book’, by Peter Biadasz)
received only 20 reviews, which therefore could
be considered as fake with high degree of proba-
bility. Even though we have no clear evidence that
a small number of reviews correlates with a greater
likelihood of deception, since we know this book
received fake reviews, and there are only few re-
views for it, we felt it is pretty likely that those
are fake. Therefore we examined the reviews writ-
ten by these twenty authors, and considered as
false only those showing the presence of all the
deception clues described above. In this way, we
found 96 reviews published by 14 reviewers, and
we added them to the 22 of Sandra Parker, for a
total of 118 reviews written by 15 authors.
Once identified this subset of fake reviews, we
selected other 118 reviews which did not show
the presence of any deception clue, that is chosen
from books above any suspicion, written by au-
thors who published the review having made use
of their real name and having bought the book
through Amazon and so on.
In the end, we identified a subset of DEREV
constituted by 236 reviews, whose class was
known with high degree of confidence and con-
sidered them as our gold standard.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999949826086956">
We carried out two experiments, in which the
classes assigned to the reviews of DEREV were
found adopting two different strategies. In the first
experiment the classes of the reviews were de-
termined using majority voting of our deception
clues. This experiment is thus conceptually simi-
lar to those of Li et al. (2011), who trained models
using supervised methods with the aim of identi-
fying fake reviews. We discuss this experiment in
the next Section. In the second experiment, learn-
ing from crowds was used (Raykar et al., 2010).
This approach is discussed in Section 3.2.1.
In both experiments we carried out a 10-fold
cross-validation where in each iteration feature se-
lection and training were carried out using 90% of
the part of the corpus with only silver standard an-
notation and 90% of the subset with gold. The test
set used in each iteration consisted of the remain-
ing tenth of reviews with gold standard classes,
which were employed in order to evaluate the pre-
dictions of the models. This allowed to estimate
the efficiency of the strategies we used to deter-
mine our silver standard classes.
</bodyText>
<subsectionHeader confidence="0.992650333333333">
3.1 Majority Voting
3.1.1 Determining the class of reviews by
majority voting
</subsectionHeader>
<bodyText confidence="0.999978909090909">
The deception clues discussed in Section 2.3 were
used in our first experiment to identify the class of
each review using majority voting. In other words,
those clues were considered as independent pre-
dictors of the class; the class predicted by the ma-
jority of the annotators/clues was assigned to the
review. Specifically, if 0, 1 or 2 deception clues
were found, the review was classified as true; if
there were 3 or 4, the review was considered false.
Table 1 shows the distribution of the number of
deception clues in the reviews in DEREV.
</bodyText>
<subsectionHeader confidence="0.772786">
3.1.2 Feature selection
</subsectionHeader>
<bodyText confidence="0.9999814">
In both experiments each review was represented
as feature vector. The features were just of uni-
grams, bigrams and trigrams of lemmas and part-
of-speech (POS), as collected from the reviews
through TreeTagger10 (Schmid, 1994).
Since in each experiment we applied a 10-fold
cross-validation, in every fold the features were
extracted from the nine-tenths of DEREV em-
ployed as training set. Once identified the train-
ing set, we computed the frequency lists of the
n-grams of lemmas and POS. The lists were col-
lected separately from the reviews belonging to
the class ‘true’ and to the class ‘false’. Such sep-
aration was aimed to take into consideration the
most highly frequent n-grams of both genuine and
fake reviews. However, for the following steps of
the feature selection, only the n-grams which ap-
peared more than 300 times in every frequency list
were considered: a threshold empirically chosen
for ease of calculations. In fact, among the most
</bodyText>
<footnote confidence="0.815704666666667">
10 http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
</footnote>
<page confidence="0.991773">
282
</page>
<tableCaption confidence="0.999594">
Table 2: The most frequent n-grams collected
</tableCaption>
<table confidence="0.982191">
N-grams Lemmas POS Total
Unigrams 34 21
Bigrams 21 13
Trigrams 13 8
Total 68 42 110
</table>
<bodyText confidence="0.99985551724138">
frequents, in order to identify the features most
effective in discriminating the two classes of re-
views, the Information Gain (IG) of the selected n-
grams was computed (Kullback and Leibler, 1951;
Yang and Pedersen, 1997).
Then, after having found the Information Gain
of the n-grams of lemmas and part-of-speech, a
further reduction of the features was realized. In
fact, we selected a relatively small amount of fea-
tures, in order to facilitate the computation of the
Raykar et al.’s algorithm (discussed in Sub-section
3.2.1), and only the n-grams with the highest IG
values were selected to be taken as features of the
vectors which represented the reviews. In par-
ticular, the n-grams were collected according to
the scheme shown in Table 2. By the way, 8,
13, 21 and 34 are numbers belonging to the Fi-
bonacci series (Sigler, 2003). They were chosen
because they grow exponentially and are used, in
our case, to give wider representation to the short-
est n-grams.
Lastly, two more features were added to the fea-
ture set, that is the length of the review, considered
with and without punctuation. Therefore, in each
fold of the experiment, the vectors of the reviews
were constituted by 112 values: 2 corresponding
to the length of the review, and 110 representing
the (not normalized) frequency, into the review it-
self, of the selected n-grams of lemmas and POS.
</bodyText>
<subsectionHeader confidence="0.937314">
3.1.3 Baselines
</subsectionHeader>
<bodyText confidence="0.999863677419355">
The best way to assess the improvement coming
from the algorithm would have been with respect
to a supervised baseline. However this was not
possible as we could only be certain regarding the
classification of a fraction of the reviews (our gold
standard: 236 reviews, for a total of about 23,000
tokens). We felt such a small dataset could not be
used for training, but only for evaluation; therefore
we used instead two simple heuristic baselines.
Majority baseline. The simplest metric for per-
formance evaluation is the majority baseline: al-
ways assign to a review the class most represented
in the dataset. Since in the subset of DEREV with
gold standard we had 50% of true and false re-
views, simply 50% is our majority baseline.
Random baseline. Furthermore, we estimated a
random baseline through a Monte Carlo simula-
tion. This kind of simulation allows to estimate the
performance of a classifier which performs several
times a task over random outputs whose distribu-
tion reflects that of real data.
In particular, for this experiment, since we had
236 reviews whose 50% were labeled as false,
100000 times we produced 236 random binomial
predictions, having p = .5. In each simulation,
the random prediction was compared with our real
data. It turned out that in less than .01% of tri-
als the level of 62.29% of correct predictions was
exceeded. The thresholds for precision and recall
in detecting deceptive reviews were 62.26% and
66.95% respectively.
</bodyText>
<subsectionHeader confidence="0.855735">
3.1.4 Models
</subsectionHeader>
<bodyText confidence="0.999713428571429">
We tested a number of supervised learning meth-
ods to learn a classifier using the classes deter-
mined by majority voting, but the best results
were obtained using Support Vector Machines
(SVMs) (Cortes and Vapnik, 1995), already em-
ployed in many applications involving text classi-
fication (Yang and Liu, 1999).
</bodyText>
<sectionHeader confidence="0.942728" genericHeader="method">
3.1.5 Results
</sectionHeader>
<bodyText confidence="0.999962416666667">
The results obtained by training a supervised clas-
sifier over the dataset with classes identified with
majority voting are shown in the Table 3. The
highest results are in bold. The methodological
approach and performance achieved in this exper-
iment seems to be comparable to that of Strappar-
ava and Mihalcea (2009) and, more recently, of Li
et al. (2011). However Li et al. (2011) evaluate the
effectiveness of different kind of features with the
aim of annotating unlabeled data, while we try to
evaluate the reliability of heuristic classes in train-
ing.
</bodyText>
<subsectionHeader confidence="0.9698765">
3.2 Learning from Crowds
3.2.1 The Learning from Crowds algorithm
</subsectionHeader>
<bodyText confidence="0.998374">
As pointed out by Raykar et al. (2010), major-
ity voting is not necessarily the most effective
way to determine the real classes in problems like
</bodyText>
<page confidence="0.999555">
283
</page>
<tableCaption confidence="0.999948">
Table 3: The experiment with the majority voting classes
</tableCaption>
<table confidence="0.999644">
Correctly Incorrectly Precision Recall F-measure
classified reviews classified reviews
False reviews 75 43 83.33% 63.56% 72.12%
True reviews 103 15
Total 178 58
Total accuracy 75.42%
Random baseline 62.29% 62.26% 66.95%
</table>
<bodyText confidence="0.999293470588235">
those of reviews where there is no gold standard.
This is because annotators are not equally reli-
able, and the reviews are not equally challenging.
Hence the output of the majority voting may be af-
fected by unevaluated biases. To address this prob-
lem, Raykar et al. (2010) presented a maximum-
likelihood estimator that jointly learns the classi-
fier/regressor, the annotator accuracy, and the ac-
tual true label.
For ease of exposition, Raykar et al. (2010) use
as classifier the logistic regression, even though
they specify their algorithm would work with any
classifier. In case of logistic regression, the prob-
ability for an entity x ∈ X of belonging to a class
y ∈ Y with Y = {1, 0} is a sigmoid function
of the weight vector w of the features of each in-
stance xi, that is p[y = 1|x, w] = σ(wTx), where,
given a threshold γ, the class y = 1 if wTx ≥ γ.
Annotators’ performance, then, is evaluated ‘in
terms of the sensitivity and specificity with respect
to the unknown gold standard’: in particular, in a
binary classification problem, for the annotator j
the sensitivity αj is the rate of positive cases iden-
tified by the annotator –i.e., the recall of positive
cases– while the specificity βj is the annotator’s
recall of negative cases.
Given a dataset D constituted of indepen-
dently sampled entities, a number of annotators
R, and the relative parameters θ = {w, α, β},
the likelihood function which needs to be maxi-
mized, according to Raykar et al. (2010), would
be p[D|θ] = nNi=1 p[y1i , ...yRi |xi, θ], and the
maximum-likelihood estimator is obtained by
maximizing the log-likelihood, that is
</bodyText>
<equation confidence="0.9609095">
�θML = {a, �β, w} = arg max
B
</equation>
<bodyText confidence="0.9992136875">
Raykar et al. (2010) propose to solve this max-
imization problem (Bickel and Doksum, 2000)
through the technique of Expectation Maximiza-
tion (EM) (Dempster et al., 1977). The EM al-
gorithm can be used to recover the parameters of
the hidden distributions accounting for the distri-
bution of data. It consists of two steps, an Expecta-
tion step (E-step) followed by a Maximization step
(M-step), which are iterated until convergence.
During the E-step the expectation of the term yi is
computed starting from the current estimate of the
parameters. In the M-step the parameters θ are up-
dated by maximizing the conditional expectation.
Regarding the third parameter, w, Raykar et al.
(2010) admit there is not a closed form solution
and suggest to use the Newton-Raphson method.
</bodyText>
<subsectionHeader confidence="0.996073">
3.2.2 Determining the class of reviews using
Learning from Crowds
</subsectionHeader>
<bodyText confidence="0.999337375">
In order to apply Raykar’s algorithm, we pro-
ceeded as follows. First, we applied the procedure
for feature selection described in Subsection 3.1.2
to create a single dataset: that is, the corpus was
not divided in folds, but the feature selection in-
volved all of DEREV. This dataset was built using
the classes resulting from the majority voting ap-
proach and included these columns:
</bodyText>
<listItem confidence="0.9696698">
• The class assignments of the four clues dis-
cussed in Sub-section 2.3 – SB, Cl, NN, UP;
• The majority voting class;
• The 112 features identified according to the
procedure presented in Sub-section 3.1.2.
</listItem>
<bodyText confidence="0.999941428571429">
Then, we implemented the algorithm proposed
by Raykar et al. (2010) in R.11 We computed a Lo-
gistic Regression (Gelman and Hill, 2007) on the
dataset to compute the weight vector w, used to es-
timate for each instance the probability pi for the
review of belonging to the class ‘true’. For the lo-
gistic regression we used the 112 surface features
</bodyText>
<equation confidence="0.560634">
11 http://www.r-project.org/
{lnp[D|θ]}. (1)
</equation>
<page confidence="0.997564">
284
</page>
<tableCaption confidence="0.999901">
Table 4: The experiment with Raykar et al.’s algorithm classes
</tableCaption>
<table confidence="0.999698571428571">
Correctly Incorrectly Precision Recall F-measure
classified reviews classified reviews
False reviews 85 33 78.70% 72.03% 75.22%
True reviews 95 23
Total 180 56
Total accuracy 76.27%
Random baseline 62.29% 62.26% 66.95%
</table>
<bodyText confidence="0.998909466666667">
mentioned above, adopting as class the majority
voting, as suggested by Raykar et al. (2010).
The parameters α and Q were estimated regard-
ing the three clues Cl - Cluster, NN - Nickname
and UP - Unknown Purchase. The attribute SB -
Suspect Book was not used, in order to carry out
the EM algorithm exclusively on heuristic data, re-
moving the information obtained through sources
external to the dataset. The parameters α and Q
of the three clues were obtained not from ran-
dom classes, as the EM algorithm would allow, but
again comparing the clues’ labels with the major-
ity voting class. In fact, aware of the local maxi-
mum problem of EM, in this way we tried to en-
hance the reliability of the results posing a config-
uration which could be, at least theoretically, bet-
ter than a completely random one.
Knowing these values for each instance of the
dataset, we computed the E-step and we updated
our parameters in M-step.
The E-step and the M-step were iterated 100
times, in which the log-likelihood increases mono-
tonically, indicating a convergence to a local max-
imum.
The final value of pi determined the new class of
each instance: if pi &gt; .5 the review was labeled as
true, otherwise as false. In the end, the EM clus-
terization allowed to label 3267 reviews as false
and 3552 as true, that is 47.91% and 52.09% of
DEREV respectively.
</bodyText>
<subsectionHeader confidence="0.96499">
3.2.3 Feature selection
</subsectionHeader>
<bodyText confidence="0.999978714285714">
The feature selection for this experiment was ex-
actly the same presented for the previous one in
Sub-section 3.1.2; the only, fundamental differ-
ence was that in the first experiment the classes
derived from the majority voting rule, while in
the second experiment the classes were identified
through the Raykar et al.’s strategy.
</bodyText>
<subsectionHeader confidence="0.962844">
3.2.4 Baselines
</subsectionHeader>
<bodyText confidence="0.9995985">
As in the first experiment, we compared the per-
formance of the models with the same majority
and random baselines discussed in Sub-section
3.1.3.
</bodyText>
<subsectionHeader confidence="0.919201">
3.2.5 Models
</subsectionHeader>
<bodyText confidence="0.9998935">
We used the classes determined through the Learn-
ing by Crowds algorithm to train SVMs models,
with the same settings employed in the first exper-
iments.
</bodyText>
<sectionHeader confidence="0.966367" genericHeader="evaluation">
3.2.6 Results
</sectionHeader>
<bodyText confidence="0.999905">
Table 4 shows the results of the classifier trained
over the dataset whose the classes were identified
through the Raykar et al.’s algorithm.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999233">
4.1 Deceptive language in reviews
</subsectionHeader>
<bodyText confidence="0.99999545">
Of the 4811 reviewers who wrote reviews included
in our corpus, about 900 were anonymous, and
only 16 wrote 10 or more reviews. If, in one hand,
this prevented us from verifying the performance
of the models with respect to particular reviewers,
on the other hand we had the opportunity of evalu-
ating the style in writing reviews across many sub-
jects.
In our experiments, we extracted simple surface
features constituted by short n-grams of lemmas
and part-of-speech. In literature there is evidence
that also other kinds of features are effective in de-
tecting deception in reviews: for example, infor-
mation about the syntactic structures of the texts
(Feng et al., 2012). In our pilot studies we did not
obtain improvements using syntactic features. But
even the frequency of n-grams can provide some
insight regarding deceptive language in reviews;
and with this aim we focused on the unigrams ap-
pearing more than 50 times in the 236 reviews
</bodyText>
<page confidence="0.992676">
285
</page>
<bodyText confidence="0.999960259259259">
constituting the gold standard of DEREV, whose
un/truthfulness is known. The use of self-referred
pronouns and adjectives is remarkably different in
true and fake reviews: in the genuine ones, the pro-
nouns ‘I’, ‘my’ and ‘me’ are found 371, 74 and 51
times respectively, while in the fake ones the pro-
noun ‘I’ is present only 149 and ‘me’ and ‘my’
less than 50 times. This reduced number of self-
references is coherent with the findings of other
well-known studies regarding deception detection
(Newman et al., 2003); however, while in truthful
reviews the pronoun ‘you’ appears only 84 times,
in the fake ones the frequency of ‘you’ and ‘your’
is 151 and 75. It seems that while the truth-tellers
simple state their opinions, the deceivers address
directly the reader. Probably they tend to give ad-
vice: after all, this is what they are paid for. The
frequency of the word ‘read’ - that is the activ-
ity simulated in fake reviews - is also quite imbal-
anced: 137 in true reviews and 97 in the fake ones.
Lastly, it is maybe surprising that in the false re-
views terms related to positive feelings/judgments
do not have the highest frequency; instead in truth-
ful reviews we found 52 times the term ‘good’
(and 56 times the ambiguous term ‘like’): also this
outcome is similar to that of the mentioned study
of Newman et al. (2003).
</bodyText>
<subsectionHeader confidence="0.987526">
4.2 Estimating the gold standard
</subsectionHeader>
<bodyText confidence="0.999561111111111">
The estimation of the gold standard is a recur-
rent problem in many tasks of text classification
and in particular with deceptive review identifica-
tion, that is an application where the deceptiveness
of the reviews cannot be properly determined but
only heuristically assessed.
In this paper we introduced a new dataset for
studying deceptive reviews, constituted by 6819
instances whose 236 (that is about 3.5% of the cor-
pus) were labeled with the highest degree of confi-
dence ever seen before. We used this subset to test
the models that we trained on the other reviews of
DEREV, whose the class was heuristically deter-
mined.
With this purpose, we adopted two techniques.
First, we simply considered the value of our clues
of deception as outputs of just as many annotators,
and we assigned the classes to each review accord-
ing to majority voting. Then we clustered our in-
stances using the Learning from Crowd algorithm
proposed by Raykar et al. (2010). Lastly we car-
ried out the two experiments of text classification
described above.
The results suggest that both methods achieve
accuracy well above the baseline. However, the
models trained using Learning from Crowd classes
not only achieved the highest accuracy, but also
outperformed the thresholds for precision and re-
call in detecting deceptive reviews (Table 4), while
the models trained with the majority voting classes
showed a very high precision, but at the expense of
the recall, which was lower than the baseline (Ta-
ble 3).
Since the results even with simple majority vot-
ing classes were positive, we carried out two more
experiments, identical to those described above
except that we included in the feature set the three
deception clues Cluster - Cl, Nickname - NN and
Unknown Purchase - UP. Both with majority vot-
ing and with learning from Crowds classes, the ac-
curacy of the models exceeded 97%. This might
seem to suggest that those clues are very effective;
but given that the deception clues were used to de-
rive the silver standard, their use as features could
be considered to some extent circular (Subsection
2.4). Moreover, not all of our non-linguistic cues
may be found in all review scenarios, and therefore
the applicability of our methods to all review sce-
narios will have to be investigated. Specifically,
Cluster is likely to be applicable to most review
domains, Nickname and Unknown Purchase are
Amazon features that may or may not be adopted
by other services allowing users to provide re-
views. However, our main concern was not to
evaluate the effectiveness of these specific clues of
deception, but to investigate whether better strate-
gies for labeling instances than simple majority
voting could be found.
In this perspective, the performance of our
second experiment, in which the Learning from
Crowds algorithm was employed, stands out. In
fact in that case we tried to identify the classes of
the instances abstaining from making use of any
external information regarding the reviews: in par-
ticular, we ignored the Suspect Book - SB clue of
deception which, by contrast, took part in the cre-
ation of the majority voting classes.
This outcome suggests that, even in scenarios
where the gold standard is unknown, the Learning
from Crowds algorithm is a reliable tool for label-
ing the reviews, so that effective models can be
trained in order to classify them as truthful or not.
</bodyText>
<page confidence="0.99667">
286
</page>
<sectionHeader confidence="0.995847" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998131342465754">
Bickel, P. and Doksum, K. (2000). Mathemati-
cal statistics: basic ideas and selected topics.
Number v. 1 in Mathematical Statistics: Basic
Ideas and Selected Topics. Prentice Hall.
Cortes, C. and Vapnik, V. (1995). Support-vector
networks. Machine Learning, 20.
Dempster, A. P., Laird, N. M., and Rubin, D. B.
(1977). Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the
Royal Statistical Society. Series B (Methodolog-
ical), 39(1):1–38.
Feng, S., Banerjee, R., and Choi, Y. (2012). Syn-
tactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 171–175, Jeju
Island, Korea. Association for Computational
Linguistics.
Gelman, A. and Hill, J. (2007). Data Analysis
Using Regression and Multilevel/Hierarchical
Models. Analytical Methods for Social Re-
search. Cambridge University Press.
Kullback, S. and Leibler, R. A. (1951). On in-
formation and sufficiency. Ann. Math. Statist.,
22(1):79–86.
Li, F., Huang, M., Yang, Y., and Zhu, X. (2011).
Learning to identify review spam. In Proceed-
ings of the Twenty-Second international joint
conference on Artificial Intelligence-Volume
Volume Three, pages 2488–2493. AAAI Press.
Newman, M. L., Pennebaker, J. W., Berry, D. S.,
and Richards, J. M. (2003). Lying Words:
Predicting Deception From Linguistic Styles.
Personality and Social Psychology Bulletin,
29(5):665–675.
Ott, M., Choi, Y., Cardie, C., and Hancock, J.
(2001). Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 309–319, Portland, Ore-
gon, USA. Association for Computational Lin-
guistics.
Raykar, V. C., Yu, S., Zhao, L. H., Valadez, G. H.,
Florin, C., Bogoni, L., and Moy, L. (2010).
Learning from crowds. Journal of Machine
Learning Research, 11:1297–1322.
Schmid, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings
of International Conference on New Methods in
Language Processing.
Sigler, L., editor (2003). Fibonacci’s Liber Abaci:
A Translation Into Modern English of Leonardo
Pisano’s Book of Calculation. Sources and
Studies in the History of Mathematics and Phys-
ical Sciences. Springer Verlag.
Strapparava, C. and Mihalcea, R. (2009). The Lie
Detector: Explorations in the Automatic Recog-
nition of Deceptive Language. In Proceed-
ing ACLShort ’09 - Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers.
Yang, Y. and Liu, X. (1999). A re-examination of
text categorization methods. In Proceedings of
the 22nd annual international ACM SIGIR con-
ference on Research and development in infor-
mation retrieval, SIGIR ’99, pages 42–49, New
York, NY, USA. ACM.
Yang, Y. and Pedersen, J. O. (1997). A
comparative study on feature selection in
text categorization. CiteSeerX - Scientific
Literature Digital Library and Search En-
gine [http://citeseerx.ist.psu.edu/oai2] (United
States).
</reference>
<page confidence="0.997234">
287
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.161883">
<title confidence="0.806219">Identifying fake Amazon reviews as learning from crowds Tommaso Ministero Dipartimento della Pubblica</title>
<author confidence="0.822886">Segreteria del</author>
<affiliation confidence="0.514798">ComISSIT</affiliation>
<email confidence="0.972766">tommaso.fornaciari@interno.it</email>
<abstract confidence="0.993006486486486">Customers who buy products such as books online often rely on other customers reviews more than on reviews found on specialist magazines. Unfortunately the confidence in such reviews is often misplaced due to the explosion of so-called writing glowing reviews of their own books. Identifying such deceptive reviews is not easy. The first contribution of our work is the creation of a collection including a number of genuinely deceptive Amazon book reviews in collaboration with crime writer Jeremy Duns, who has devoted a great deal of effort in unmasking sock puppeting among his colleagues. But there can be no certainty concerning the other reviews in the collection: all we have is a number of cues, also developed in collaboration with Duns, suggesting that a review may be genuine or deceptive. Thus this corpus is an example of a collection where it is not possible to acquire the actual label for all instances, and where clues of deception were treated as annotators who assign them heuristic labels. A number of approaches have been proposed for such cases; we adopt here the ‘learning from crowds’ approach proposed by Raykar et al. (2010). Thanks to Duns’ certainly fake reviews, the second contribution of this work consists in the evaluation of the effectiveness of different methods of annotation, according to the performance of models trained to detect deceptive reviews.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Bickel</author>
<author>K Doksum</author>
</authors>
<title>Mathematical statistics: basic ideas and selected topics. Number v. 1 in Mathematical Statistics: Basic Ideas and Selected Topics.</title>
<date>2000</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="23239" citStr="Bickel and Doksum, 2000" startWordPosition="3877" endWordPosition="3880">ve cases identified by the annotator –i.e., the recall of positive cases– while the specificity βj is the annotator’s recall of negative cases. Given a dataset D constituted of independently sampled entities, a number of annotators R, and the relative parameters θ = {w, α, β}, the likelihood function which needs to be maximized, according to Raykar et al. (2010), would be p[D|θ] = nNi=1 p[y1i , ...yRi |xi, θ], and the maximum-likelihood estimator is obtained by maximizing the log-likelihood, that is �θML = {a, �β, w} = arg max B Raykar et al. (2010) propose to solve this maximization problem (Bickel and Doksum, 2000) through the technique of Expectation Maximization (EM) (Dempster et al., 1977). The EM algorithm can be used to recover the parameters of the hidden distributions accounting for the distribution of data. It consists of two steps, an Expectation step (E-step) followed by a Maximization step (M-step), which are iterated until convergence. During the E-step the expectation of the term yi is computed starting from the current estimate of the parameters. In the M-step the parameters θ are updated by maximizing the conditional expectation. Regarding the third parameter, w, Raykar et al. (2010) admi</context>
</contexts>
<marker>Bickel, Doksum, 2000</marker>
<rawString>Bickel, P. and Doksum, K. (2000). Mathematical statistics: basic ideas and selected topics. Number v. 1 in Mathematical Statistics: Basic Ideas and Selected Topics. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<contexts>
<context position="20361" citStr="Cortes and Vapnik, 1995" startWordPosition="3386" endWordPosition="3389">whose 50% were labeled as false, 100000 times we produced 236 random binomial predictions, having p = .5. In each simulation, the random prediction was compared with our real data. It turned out that in less than .01% of trials the level of 62.29% of correct predictions was exceeded. The thresholds for precision and recall in detecting deceptive reviews were 62.26% and 66.95% respectively. 3.1.4 Models We tested a number of supervised learning methods to learn a classifier using the classes determined by majority voting, but the best results were obtained using Support Vector Machines (SVMs) (Cortes and Vapnik, 1995), already employed in many applications involving text classification (Yang and Liu, 1999). 3.1.5 Results The results obtained by training a supervised classifier over the dataset with classes identified with majority voting are shown in the Table 3. The highest results are in bold. The methodological approach and performance achieved in this experiment seems to be comparable to that of Strapparava and Mihalcea (2009) and, more recently, of Li et al. (2011). However Li et al. (2011) evaluate the effectiveness of different kind of features with the aim of annotating unlabeled data, while we try</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Cortes, C. and Vapnik, V. (1995). Support-vector networks. Machine Learning, 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="23318" citStr="Dempster et al., 1977" startWordPosition="3889" endWordPosition="3892">he specificity βj is the annotator’s recall of negative cases. Given a dataset D constituted of independently sampled entities, a number of annotators R, and the relative parameters θ = {w, α, β}, the likelihood function which needs to be maximized, according to Raykar et al. (2010), would be p[D|θ] = nNi=1 p[y1i , ...yRi |xi, θ], and the maximum-likelihood estimator is obtained by maximizing the log-likelihood, that is �θML = {a, �β, w} = arg max B Raykar et al. (2010) propose to solve this maximization problem (Bickel and Doksum, 2000) through the technique of Expectation Maximization (EM) (Dempster et al., 1977). The EM algorithm can be used to recover the parameters of the hidden distributions accounting for the distribution of data. It consists of two steps, an Expectation step (E-step) followed by a Maximization step (M-step), which are iterated until convergence. During the E-step the expectation of the term yi is computed starting from the current estimate of the parameters. In the M-step the parameters θ are updated by maximizing the conditional expectation. Regarding the third parameter, w, Raykar et al. (2010) admit there is not a closed form solution and suggest to use the Newton-Raphson met</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feng</author>
<author>R Banerjee</author>
<author>Y Choi</author>
</authors>
<title>Syntactic stylometry for deception detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>171--175</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="2527" citStr="Feng et al., 2012" startWordPosition="400" endWordPosition="403">omer confidence in such reviews is often misplaced, due to the growth of the so-called sock puppetry phenomenon: authors / hoteliers writing glowing reviews of their own works / hotels (and occasionally also negative reviews of the competitors).1 The prevalence of this phenomenon has been revealed by campaigners such as crime writer Jeremy Duns, who exposed a number of fellow authors involved in such practices.2 A number of sites have also emerged offering Amazon reviews to authors for a fee.3 Several automatic techniques for exposing such deceptive reviews have been proposed in recent years (Feng et al., 2012; Ott et al., 2001). But like all work on deceptive language (computational or otherwise) (Newman et al., 2003; Strapparava and Mihalcea, 2009), such works suffer from a serious problem: the lack of a gold standard containing ‘real life’ examples of deceptive uses of language. This is because it is very difficult to find definite proof that an Amazon review is either deceptive or genuine. Thus most researchers recreate deceptive behavior in the lab, as done by Newman et al. (2003). For instance, Ott et al. (2001), Feng et al. (2012) and Strapparava and Mihalcea (2009) used crowdsourcing, askin</context>
<context position="28148" citStr="Feng et al., 2012" startWordPosition="4701" endWordPosition="4704">ur corpus, about 900 were anonymous, and only 16 wrote 10 or more reviews. If, in one hand, this prevented us from verifying the performance of the models with respect to particular reviewers, on the other hand we had the opportunity of evaluating the style in writing reviews across many subjects. In our experiments, we extracted simple surface features constituted by short n-grams of lemmas and part-of-speech. In literature there is evidence that also other kinds of features are effective in detecting deception in reviews: for example, information about the syntactic structures of the texts (Feng et al., 2012). In our pilot studies we did not obtain improvements using syntactic features. But even the frequency of n-grams can provide some insight regarding deceptive language in reviews; and with this aim we focused on the unigrams appearing more than 50 times in the 236 reviews 285 constituting the gold standard of DEREV, whose un/truthfulness is known. The use of self-referred pronouns and adjectives is remarkably different in true and fake reviews: in the genuine ones, the pronouns ‘I’, ‘my’ and ‘me’ are found 371, 74 and 51 times respectively, while in the fake ones the pronoun ‘I’ is present onl</context>
</contexts>
<marker>Feng, Banerjee, Choi, 2012</marker>
<rawString>Feng, S., Banerjee, R., and Choi, Y. (2012). Syntactic stylometry for deception detection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 171–175, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gelman</author>
<author>J Hill</author>
</authors>
<title>Data Analysis Using Regression and Multilevel/Hierarchical Models. Analytical Methods for Social Research.</title>
<date>2007</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="24712" citStr="Gelman and Hill, 2007" startWordPosition="4122" endWordPosition="4125">ature selection described in Subsection 3.1.2 to create a single dataset: that is, the corpus was not divided in folds, but the feature selection involved all of DEREV. This dataset was built using the classes resulting from the majority voting approach and included these columns: • The class assignments of the four clues discussed in Sub-section 2.3 – SB, Cl, NN, UP; • The majority voting class; • The 112 features identified according to the procedure presented in Sub-section 3.1.2. Then, we implemented the algorithm proposed by Raykar et al. (2010) in R.11 We computed a Logistic Regression (Gelman and Hill, 2007) on the dataset to compute the weight vector w, used to estimate for each instance the probability pi for the review of belonging to the class ‘true’. For the logistic regression we used the 112 surface features 11 http://www.r-project.org/ {lnp[D|θ]}. (1) 284 Table 4: The experiment with Raykar et al.’s algorithm classes Correctly Incorrectly Precision Recall F-measure classified reviews classified reviews False reviews 85 33 78.70% 72.03% 75.22% True reviews 95 23 Total 180 56 Total accuracy 76.27% Random baseline 62.29% 62.26% 66.95% mentioned above, adopting as class the majority voting, a</context>
</contexts>
<marker>Gelman, Hill, 2007</marker>
<rawString>Gelman, A. and Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. Analytical Methods for Social Research. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kullback</author>
<author>R A Leibler</author>
</authors>
<title>On information and sufficiency.</title>
<date>1951</date>
<journal>Ann. Math. Statist.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="17482" citStr="Kullback and Leibler, 1951" startWordPosition="2903" endWordPosition="2906">ng steps of the feature selection, only the n-grams which appeared more than 300 times in every frequency list were considered: a threshold empirically chosen for ease of calculations. In fact, among the most 10 http://www.ims.uni-stuttgart. de/projekte/corplex/TreeTagger/ DecisionTreeTagger.html 282 Table 2: The most frequent n-grams collected N-grams Lemmas POS Total Unigrams 34 21 Bigrams 21 13 Trigrams 13 8 Total 68 42 110 frequents, in order to identify the features most effective in discriminating the two classes of reviews, the Information Gain (IG) of the selected ngrams was computed (Kullback and Leibler, 1951; Yang and Pedersen, 1997). Then, after having found the Information Gain of the n-grams of lemmas and part-of-speech, a further reduction of the features was realized. In fact, we selected a relatively small amount of features, in order to facilitate the computation of the Raykar et al.’s algorithm (discussed in Sub-section 3.2.1), and only the n-grams with the highest IG values were selected to be taken as features of the vectors which represented the reviews. In particular, the n-grams were collected according to the scheme shown in Table 2. By the way, 8, 13, 21 and 34 are numbers belongin</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>Kullback, S. and Leibler, R. A. (1951). On information and sufficiency. Ann. Math. Statist., 22(1):79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Li</author>
<author>M Huang</author>
<author>Y Yang</author>
<author>X Zhu</author>
</authors>
<title>Learning to identify review spam.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three,</booktitle>
<pages>2488--2493</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="3206" citStr="Li et al. (2011)" startWordPosition="517" endWordPosition="520">omputational or otherwise) (Newman et al., 2003; Strapparava and Mihalcea, 2009), such works suffer from a serious problem: the lack of a gold standard containing ‘real life’ examples of deceptive uses of language. This is because it is very difficult to find definite proof that an Amazon review is either deceptive or genuine. Thus most researchers recreate deceptive behavior in the lab, as done by Newman et al. (2003). For instance, Ott et al. (2001), Feng et al. (2012) and Strapparava and Mihalcea (2009) used crowdsourcing, asking turkers to produce instances of deceptive behavior. Finally, Li et al. (2011) classify reviews as deceptive or truthful by hand on the basis of a series of heuristics: they start by excluding anonymous reviews, then use their helpfulness and other criteria to decide 1The phenomenon predates Internet - see e.g., Amy Harmon, ‘Amazon Glitch Unmasks War Of Reviewers’, New York Times, February 14, 2004. 2See Andrew Hough, ‘RJ Ellory: fake book reviews are rife on internet, authors warn’, telegraph.co.uk, September 3, 2012 3See Alison Flood, ‘Sock puppetry and fake reviews: publish and be damned’, guardian.co.uk, September 4, 2012 and David Streitfeld, ‘Buy Reviews on Yelp, </context>
<context position="11071" citStr="Li et al. (2011)" startWordPosition="1824" endWordPosition="1827">up of at least two reviews posted within 3 days. Nickname - NN A service provided by Amazon is the possibility for the reviewers to register 8We specify the date of the download because, obviously, if the data collection would be repeated today, the overall number of reviews would be greater. in the website and to post comments using their real name. Since the real identity of the reviewers involves issues related to their reputation, we supposed it is less probable that the writers of fake reviews post their texts using their true name. Moreover, a similar assumption was probably accepted by Li et al. (2011), who considered the profile features of the reviewers, and among them the use or not of their real name. Unknown Purchase - UP Lastly, the probably most interesting information provided by Amazon is whether the reviewer bought the reviewed book through Amazon itself. It is reasonable to think that, if the reviewer bought the book, he also read it. Therefore, the absence of information about the certified purchase was considered a clue of deceptiveness. 2.4 Gold and silver standard The clues of deception discussed above give us a heuristic estimate of the truthfulness of the reviews. Such esti</context>
<context position="14680" citStr="Li et al. (2011)" startWordPosition="2441" endWordPosition="2444">ors who published the review having made use of their real name and having bought the book through Amazon and so on. In the end, we identified a subset of DEREV constituted by 236 reviews, whose class was known with high degree of confidence and considered them as our gold standard. 3 Experiments We carried out two experiments, in which the classes assigned to the reviews of DEREV were found adopting two different strategies. In the first experiment the classes of the reviews were determined using majority voting of our deception clues. This experiment is thus conceptually similar to those of Li et al. (2011), who trained models using supervised methods with the aim of identifying fake reviews. We discuss this experiment in the next Section. In the second experiment, learning from crowds was used (Raykar et al., 2010). This approach is discussed in Section 3.2.1. In both experiments we carried out a 10-fold cross-validation where in each iteration feature selection and training were carried out using 90% of the part of the corpus with only silver standard annotation and 90% of the subset with gold. The test set used in each iteration consisted of the remaining tenth of reviews with gold standard c</context>
<context position="20822" citStr="Li et al. (2011)" startWordPosition="3462" endWordPosition="3465">assifier using the classes determined by majority voting, but the best results were obtained using Support Vector Machines (SVMs) (Cortes and Vapnik, 1995), already employed in many applications involving text classification (Yang and Liu, 1999). 3.1.5 Results The results obtained by training a supervised classifier over the dataset with classes identified with majority voting are shown in the Table 3. The highest results are in bold. The methodological approach and performance achieved in this experiment seems to be comparable to that of Strapparava and Mihalcea (2009) and, more recently, of Li et al. (2011). However Li et al. (2011) evaluate the effectiveness of different kind of features with the aim of annotating unlabeled data, while we try to evaluate the reliability of heuristic classes in training. 3.2 Learning from Crowds 3.2.1 The Learning from Crowds algorithm As pointed out by Raykar et al. (2010), majority voting is not necessarily the most effective way to determine the real classes in problems like 283 Table 3: The experiment with the majority voting classes Correctly Incorrectly Precision Recall F-measure classified reviews classified reviews False reviews 75 43 83.33% 63.56% 72.12</context>
</contexts>
<marker>Li, Huang, Yang, Zhu, 2011</marker>
<rawString>Li, F., Huang, M., Yang, Y., and Zhu, X. (2011). Learning to identify review spam. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three, pages 2488–2493. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Newman</author>
<author>J W Pennebaker</author>
<author>D S Berry</author>
<author>J M Richards</author>
</authors>
<title>Lying Words: Predicting Deception From Linguistic Styles. Personality and Social Psychology Bulletin,</title>
<date>2003</date>
<contexts>
<context position="2637" citStr="Newman et al., 2003" startWordPosition="418" endWordPosition="421">non: authors / hoteliers writing glowing reviews of their own works / hotels (and occasionally also negative reviews of the competitors).1 The prevalence of this phenomenon has been revealed by campaigners such as crime writer Jeremy Duns, who exposed a number of fellow authors involved in such practices.2 A number of sites have also emerged offering Amazon reviews to authors for a fee.3 Several automatic techniques for exposing such deceptive reviews have been proposed in recent years (Feng et al., 2012; Ott et al., 2001). But like all work on deceptive language (computational or otherwise) (Newman et al., 2003; Strapparava and Mihalcea, 2009), such works suffer from a serious problem: the lack of a gold standard containing ‘real life’ examples of deceptive uses of language. This is because it is very difficult to find definite proof that an Amazon review is either deceptive or genuine. Thus most researchers recreate deceptive behavior in the lab, as done by Newman et al. (2003). For instance, Ott et al. (2001), Feng et al. (2012) and Strapparava and Mihalcea (2009) used crowdsourcing, asking turkers to produce instances of deceptive behavior. Finally, Li et al. (2011) classify reviews as deceptive </context>
<context position="28939" citStr="Newman et al., 2003" startWordPosition="4835" endWordPosition="4838">reviews; and with this aim we focused on the unigrams appearing more than 50 times in the 236 reviews 285 constituting the gold standard of DEREV, whose un/truthfulness is known. The use of self-referred pronouns and adjectives is remarkably different in true and fake reviews: in the genuine ones, the pronouns ‘I’, ‘my’ and ‘me’ are found 371, 74 and 51 times respectively, while in the fake ones the pronoun ‘I’ is present only 149 and ‘me’ and ‘my’ less than 50 times. This reduced number of selfreferences is coherent with the findings of other well-known studies regarding deception detection (Newman et al., 2003); however, while in truthful reviews the pronoun ‘you’ appears only 84 times, in the fake ones the frequency of ‘you’ and ‘your’ is 151 and 75. It seems that while the truth-tellers simple state their opinions, the deceivers address directly the reader. Probably they tend to give advice: after all, this is what they are paid for. The frequency of the word ‘read’ - that is the activity simulated in fake reviews - is also quite imbalanced: 137 in true reviews and 97 in the fake ones. Lastly, it is maybe surprising that in the false reviews terms related to positive feelings/judgments do not have</context>
</contexts>
<marker>Newman, Pennebaker, Berry, Richards, 2003</marker>
<rawString>Newman, M. L., Pennebaker, J. W., Berry, D. S., and Richards, J. M. (2003). Lying Words: Predicting Deception From Linguistic Styles. Personality and Social Psychology Bulletin, 29(5):665–675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ott</author>
<author>Y Choi</author>
<author>C Cardie</author>
<author>J Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2001</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>309--319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2546" citStr="Ott et al., 2001" startWordPosition="404" endWordPosition="407">such reviews is often misplaced, due to the growth of the so-called sock puppetry phenomenon: authors / hoteliers writing glowing reviews of their own works / hotels (and occasionally also negative reviews of the competitors).1 The prevalence of this phenomenon has been revealed by campaigners such as crime writer Jeremy Duns, who exposed a number of fellow authors involved in such practices.2 A number of sites have also emerged offering Amazon reviews to authors for a fee.3 Several automatic techniques for exposing such deceptive reviews have been proposed in recent years (Feng et al., 2012; Ott et al., 2001). But like all work on deceptive language (computational or otherwise) (Newman et al., 2003; Strapparava and Mihalcea, 2009), such works suffer from a serious problem: the lack of a gold standard containing ‘real life’ examples of deceptive uses of language. This is because it is very difficult to find definite proof that an Amazon review is either deceptive or genuine. Thus most researchers recreate deceptive behavior in the lab, as done by Newman et al. (2003). For instance, Ott et al. (2001), Feng et al. (2012) and Strapparava and Mihalcea (2009) used crowdsourcing, asking turkers to produc</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2001</marker>
<rawString>Ott, M., Choi, Y., Cardie, C., and Hancock, J. (2001). Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 309–319, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V C Raykar</author>
<author>S Yu</author>
<author>L H Zhao</author>
<author>G H Valadez</author>
<author>C Florin</author>
<author>L Bogoni</author>
<author>L Moy</author>
</authors>
<title>Learning from crowds.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--1297</pages>
<contexts>
<context position="1385" citStr="Raykar et al. (2010)" startWordPosition="219" endWordPosition="222">al of effort in unmasking sock puppeting among his colleagues. But there can be no certainty concerning the other reviews in the collection: all we have is a number of cues, also developed in collaboration with Duns, suggesting that a review may be genuine or deceptive. Thus this corpus is an example of a collection where it is not possible to acquire the actual label for all instances, and where clues of deception were treated as annotators who assign them heuristic labels. A number of approaches have been proposed for such cases; we adopt here the ‘learning from crowds’ approach proposed by Raykar et al. (2010). Thanks to Duns’ certainly fake reviews, the second contribution of this work consists in the evaluation of the effectiveness of different methods of annotation, according to the performance of models trained to detect deceptive reviews. 1 Introduction Customer reviews of books, hotels and other products are widely perceived as an important reaMassimo Poesio University of Essex CSEE University of Trento CIMeC poesio@essex.ac.uk son for the success of e-commerce sites such as amazon.com or tripadvisor.com. However, customer confidence in such reviews is often misplaced, due to the growth of th</context>
<context position="5003" citStr="Raykar et al. (2010)" startWordPosition="807" endWordPosition="810">n collaboration with Jeremy Duns a series of criteria used by Duns and other ‘sock puppet hunters’ to find suspicious reviews / reviewers, and collected a dataset of reviews some of which are certainly false as the authors admitted so, whereas others may be genuine or deceptive. 2. we developed an approach to the truthfulness of reviews based on the notion that the truthfulness of a review is a latent variable whose value cannot be known, but can be estimated using some criteria as potential indicators of such value–as annotators–and then we used the learning from crowds algorithm proposed by Raykar et al. (2010) to assign a class to each review in the dataset. The structure of the paper is as follows. In Section 2 we describe how we collected our dataset; in Section 3 we show the experiments we carried out and in Section 4 we discuss the results. 2 Deception clues and dataset 2.1 Examples of Unmasked Sock Puppetry After reading an article by Alison Flood on The Guardian of September 4th, 2012 4, discussing how crime writer Jeremy Duns had unmasked a number of ‘sock puppeteers,’ we contacted him. Duns was extremely helpful; he pointed us to the other articles on the topic, mostly on The New York Times</context>
<context position="14893" citStr="Raykar et al., 2010" startWordPosition="2477" endWordPosition="2480">n with high degree of confidence and considered them as our gold standard. 3 Experiments We carried out two experiments, in which the classes assigned to the reviews of DEREV were found adopting two different strategies. In the first experiment the classes of the reviews were determined using majority voting of our deception clues. This experiment is thus conceptually similar to those of Li et al. (2011), who trained models using supervised methods with the aim of identifying fake reviews. We discuss this experiment in the next Section. In the second experiment, learning from crowds was used (Raykar et al., 2010). This approach is discussed in Section 3.2.1. In both experiments we carried out a 10-fold cross-validation where in each iteration feature selection and training were carried out using 90% of the part of the corpus with only silver standard annotation and 90% of the subset with gold. The test set used in each iteration consisted of the remaining tenth of reviews with gold standard classes, which were employed in order to evaluate the predictions of the models. This allowed to estimate the efficiency of the strategies we used to determine our silver standard classes. 3.1 Majority Voting 3.1.1</context>
<context position="21128" citStr="Raykar et al. (2010)" startWordPosition="3513" endWordPosition="3516">ised classifier over the dataset with classes identified with majority voting are shown in the Table 3. The highest results are in bold. The methodological approach and performance achieved in this experiment seems to be comparable to that of Strapparava and Mihalcea (2009) and, more recently, of Li et al. (2011). However Li et al. (2011) evaluate the effectiveness of different kind of features with the aim of annotating unlabeled data, while we try to evaluate the reliability of heuristic classes in training. 3.2 Learning from Crowds 3.2.1 The Learning from Crowds algorithm As pointed out by Raykar et al. (2010), majority voting is not necessarily the most effective way to determine the real classes in problems like 283 Table 3: The experiment with the majority voting classes Correctly Incorrectly Precision Recall F-measure classified reviews classified reviews False reviews 75 43 83.33% 63.56% 72.12% True reviews 103 15 Total 178 58 Total accuracy 75.42% Random baseline 62.29% 62.26% 66.95% those of reviews where there is no gold standard. This is because annotators are not equally reliable, and the reviews are not equally challenging. Hence the output of the majority voting may be affected by uneva</context>
<context position="22979" citStr="Raykar et al. (2010)" startWordPosition="3831" endWordPosition="3834">if wTx ≥ γ. Annotators’ performance, then, is evaluated ‘in terms of the sensitivity and specificity with respect to the unknown gold standard’: in particular, in a binary classification problem, for the annotator j the sensitivity αj is the rate of positive cases identified by the annotator –i.e., the recall of positive cases– while the specificity βj is the annotator’s recall of negative cases. Given a dataset D constituted of independently sampled entities, a number of annotators R, and the relative parameters θ = {w, α, β}, the likelihood function which needs to be maximized, according to Raykar et al. (2010), would be p[D|θ] = nNi=1 p[y1i , ...yRi |xi, θ], and the maximum-likelihood estimator is obtained by maximizing the log-likelihood, that is �θML = {a, �β, w} = arg max B Raykar et al. (2010) propose to solve this maximization problem (Bickel and Doksum, 2000) through the technique of Expectation Maximization (EM) (Dempster et al., 1977). The EM algorithm can be used to recover the parameters of the hidden distributions accounting for the distribution of data. It consists of two steps, an Expectation step (E-step) followed by a Maximization step (M-step), which are iterated until convergence. </context>
<context position="24646" citStr="Raykar et al. (2010)" startWordPosition="4110" endWordPosition="4113"> we proceeded as follows. First, we applied the procedure for feature selection described in Subsection 3.1.2 to create a single dataset: that is, the corpus was not divided in folds, but the feature selection involved all of DEREV. This dataset was built using the classes resulting from the majority voting approach and included these columns: • The class assignments of the four clues discussed in Sub-section 2.3 – SB, Cl, NN, UP; • The majority voting class; • The 112 features identified according to the procedure presented in Sub-section 3.1.2. Then, we implemented the algorithm proposed by Raykar et al. (2010) in R.11 We computed a Logistic Regression (Gelman and Hill, 2007) on the dataset to compute the weight vector w, used to estimate for each instance the probability pi for the review of belonging to the class ‘true’. For the logistic regression we used the 112 surface features 11 http://www.r-project.org/ {lnp[D|θ]}. (1) 284 Table 4: The experiment with Raykar et al.’s algorithm classes Correctly Incorrectly Precision Recall F-measure classified reviews classified reviews False reviews 85 33 78.70% 72.03% 75.22% True reviews 95 23 Total 180 56 Total accuracy 76.27% Random baseline 62.29% 62.26</context>
<context position="30738" citStr="Raykar et al. (2010)" startWordPosition="5148" endWordPosition="5151">eceptive reviews, constituted by 6819 instances whose 236 (that is about 3.5% of the corpus) were labeled with the highest degree of confidence ever seen before. We used this subset to test the models that we trained on the other reviews of DEREV, whose the class was heuristically determined. With this purpose, we adopted two techniques. First, we simply considered the value of our clues of deception as outputs of just as many annotators, and we assigned the classes to each review according to majority voting. Then we clustered our instances using the Learning from Crowd algorithm proposed by Raykar et al. (2010). Lastly we carried out the two experiments of text classification described above. The results suggest that both methods achieve accuracy well above the baseline. However, the models trained using Learning from Crowd classes not only achieved the highest accuracy, but also outperformed the thresholds for precision and recall in detecting deceptive reviews (Table 4), while the models trained with the majority voting classes showed a very high precision, but at the expense of the recall, which was lower than the baseline (Table 3). Since the results even with simple majority voting classes were</context>
</contexts>
<marker>Raykar, Yu, Zhao, Valadez, Florin, Bogoni, Moy, 2010</marker>
<rawString>Raykar, V. C., Yu, S., Zhao, L. H., Valadez, G. H., Florin, C., Bogoni, L., and Moy, L. (2010). Learning from crowds. Journal of Machine Learning Research, 11:1297–1322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="16339" citStr="Schmid, 1994" startWordPosition="2723" endWordPosition="2724">ed as independent predictors of the class; the class predicted by the majority of the annotators/clues was assigned to the review. Specifically, if 0, 1 or 2 deception clues were found, the review was classified as true; if there were 3 or 4, the review was considered false. Table 1 shows the distribution of the number of deception clues in the reviews in DEREV. 3.1.2 Feature selection In both experiments each review was represented as feature vector. The features were just of unigrams, bigrams and trigrams of lemmas and partof-speech (POS), as collected from the reviews through TreeTagger10 (Schmid, 1994). Since in each experiment we applied a 10-fold cross-validation, in every fold the features were extracted from the nine-tenths of DEREV employed as training set. Once identified the training set, we computed the frequency lists of the n-grams of lemmas and POS. The lists were collected separately from the reviews belonging to the class ‘true’ and to the class ‘false’. Such separation was aimed to take into consideration the most highly frequent n-grams of both genuine and fake reviews. However, for the following steps of the feature selection, only the n-grams which appeared more than 300 ti</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Sigler</author>
</authors>
<title>editor (2003). Fibonacci’s Liber Abaci: A Translation Into</title>
<booktitle>Modern English of Leonardo Pisano’s Book of Calculation. Sources and Studies in the History of Mathematics and Physical Sciences.</booktitle>
<publisher>Springer Verlag.</publisher>
<marker>Sigler, </marker>
<rawString>Sigler, L., editor (2003). Fibonacci’s Liber Abaci: A Translation Into Modern English of Leonardo Pisano’s Book of Calculation. Sources and Studies in the History of Mathematics and Physical Sciences. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<title>The Lie Detector: Explorations in the Automatic Recognition of Deceptive Language.</title>
<date>2009</date>
<booktitle>In Proceeding ACLShort ’09 - Proceedings of the ACLIJCNLP 2009 Conference Short Papers.</booktitle>
<contexts>
<context position="2670" citStr="Strapparava and Mihalcea, 2009" startWordPosition="422" endWordPosition="425">ers writing glowing reviews of their own works / hotels (and occasionally also negative reviews of the competitors).1 The prevalence of this phenomenon has been revealed by campaigners such as crime writer Jeremy Duns, who exposed a number of fellow authors involved in such practices.2 A number of sites have also emerged offering Amazon reviews to authors for a fee.3 Several automatic techniques for exposing such deceptive reviews have been proposed in recent years (Feng et al., 2012; Ott et al., 2001). But like all work on deceptive language (computational or otherwise) (Newman et al., 2003; Strapparava and Mihalcea, 2009), such works suffer from a serious problem: the lack of a gold standard containing ‘real life’ examples of deceptive uses of language. This is because it is very difficult to find definite proof that an Amazon review is either deceptive or genuine. Thus most researchers recreate deceptive behavior in the lab, as done by Newman et al. (2003). For instance, Ott et al. (2001), Feng et al. (2012) and Strapparava and Mihalcea (2009) used crowdsourcing, asking turkers to produce instances of deceptive behavior. Finally, Li et al. (2011) classify reviews as deceptive or truthful by hand on the basis </context>
<context position="20782" citStr="Strapparava and Mihalcea (2009)" startWordPosition="3453" endWordPosition="3457">d a number of supervised learning methods to learn a classifier using the classes determined by majority voting, but the best results were obtained using Support Vector Machines (SVMs) (Cortes and Vapnik, 1995), already employed in many applications involving text classification (Yang and Liu, 1999). 3.1.5 Results The results obtained by training a supervised classifier over the dataset with classes identified with majority voting are shown in the Table 3. The highest results are in bold. The methodological approach and performance achieved in this experiment seems to be comparable to that of Strapparava and Mihalcea (2009) and, more recently, of Li et al. (2011). However Li et al. (2011) evaluate the effectiveness of different kind of features with the aim of annotating unlabeled data, while we try to evaluate the reliability of heuristic classes in training. 3.2 Learning from Crowds 3.2.1 The Learning from Crowds algorithm As pointed out by Raykar et al. (2010), majority voting is not necessarily the most effective way to determine the real classes in problems like 283 Table 3: The experiment with the majority voting classes Correctly Incorrectly Precision Recall F-measure classified reviews classified reviews</context>
</contexts>
<marker>Strapparava, Mihalcea, 2009</marker>
<rawString>Strapparava, C. and Mihalcea, R. (2009). The Lie Detector: Explorations in the Automatic Recognition of Deceptive Language. In Proceeding ACLShort ’09 - Proceedings of the ACLIJCNLP 2009 Conference Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>X Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99,</booktitle>
<pages>42--49</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="20451" citStr="Yang and Liu, 1999" startWordPosition="3400" endWordPosition="3403">ng p = .5. In each simulation, the random prediction was compared with our real data. It turned out that in less than .01% of trials the level of 62.29% of correct predictions was exceeded. The thresholds for precision and recall in detecting deceptive reviews were 62.26% and 66.95% respectively. 3.1.4 Models We tested a number of supervised learning methods to learn a classifier using the classes determined by majority voting, but the best results were obtained using Support Vector Machines (SVMs) (Cortes and Vapnik, 1995), already employed in many applications involving text classification (Yang and Liu, 1999). 3.1.5 Results The results obtained by training a supervised classifier over the dataset with classes identified with majority voting are shown in the Table 3. The highest results are in bold. The methodological approach and performance achieved in this experiment seems to be comparable to that of Strapparava and Mihalcea (2009) and, more recently, of Li et al. (2011). However Li et al. (2011) evaluate the effectiveness of different kind of features with the aim of annotating unlabeled data, while we try to evaluate the reliability of heuristic classes in training. 3.2 Learning from Crowds 3.</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yang, Y. and Liu, X. (1999). A re-examination of text categorization methods. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99, pages 42–49, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>CiteSeerX - Scientific Literature Digital Library and Search Engine [http://citeseerx.ist.psu.edu/oai2]</booktitle>
<location>(United States).</location>
<contexts>
<context position="17508" citStr="Yang and Pedersen, 1997" startWordPosition="2907" endWordPosition="2910">ction, only the n-grams which appeared more than 300 times in every frequency list were considered: a threshold empirically chosen for ease of calculations. In fact, among the most 10 http://www.ims.uni-stuttgart. de/projekte/corplex/TreeTagger/ DecisionTreeTagger.html 282 Table 2: The most frequent n-grams collected N-grams Lemmas POS Total Unigrams 34 21 Bigrams 21 13 Trigrams 13 8 Total 68 42 110 frequents, in order to identify the features most effective in discriminating the two classes of reviews, the Information Gain (IG) of the selected ngrams was computed (Kullback and Leibler, 1951; Yang and Pedersen, 1997). Then, after having found the Information Gain of the n-grams of lemmas and part-of-speech, a further reduction of the features was realized. In fact, we selected a relatively small amount of features, in order to facilitate the computation of the Raykar et al.’s algorithm (discussed in Sub-section 3.2.1), and only the n-grams with the highest IG values were selected to be taken as features of the vectors which represented the reviews. In particular, the n-grams were collected according to the scheme shown in Table 2. By the way, 8, 13, 21 and 34 are numbers belonging to the Fibonacci series </context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yang, Y. and Pedersen, J. O. (1997). A comparative study on feature selection in text categorization. CiteSeerX - Scientific Literature Digital Library and Search Engine [http://citeseerx.ist.psu.edu/oai2] (United States).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>