<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.941127">
Using Answer Set Programming in an Inference-Based approach to Natural
Language Semantics
</title>
<table confidence="0.91366375">
Farid Nouioua Pascal Nicolas
LIPN UMR 7030 du C.N.R.S. LERIA
Institut Galilée – Univ. Paris-Nord University of Angers
93430 Villetaneuse – FRANCE 2, bd Lavoisier F-49045 Angers cedex
</table>
<email confidence="0.54907">
nouiouaf@lipn.univ-paris13.fr pascal.nicolas@univ-angers.fr
</email>
<sectionHeader confidence="0.977128" genericHeader="abstract">
1. Motivation
</sectionHeader>
<bodyText confidence="0.999959285714286">
The traditional tri-partition syntax/semantics/pragmatics is commonly used in most of the computer
systems that aim at the simulation of the human understanding of Natural Language (NL). This
conception does not reflect the flexible and creative manner that humans use in reality to interpret
texts. Generally speaking, formal NL semantics is referential i.e. it assumes that it is possible to
create a static discourse universe and to equate the objects of this universe to the (static) meanings
of words. The meaning of a sentence is then built from the meanings of the words in a
compositional process and the semantic interpretation of a sentence is reduced to its logical
interpretation based on the truth conditions. The very difficult task of adapting the meaning of a
sentence to its context is often left to the pragmatic level, and this task requires to use a huge
amount of common sense knowledge about the domain. This approach is seriously challenged (see
for example [4][14]). It has been showed that the above tri-partition is very artificial because
linguistic as well as extra-linguistic knowledge interact in the same global process to provide the
necessary elements for understanding. Linguistic phenomena such as polysemy, plurals, metaphors
and shifts in meaning create real difficulties to the referential approach of the NL semantics
discussed above. As an alternative solution to these problems, [4] proposes an inferential approach
to the NL semantics in which words trigger inferences depending on the context of their apparition.
In the same spirit we claim that understanding a NL text is a reasoning process based on our
knowledge about the norms1 of its domain i.e. what we generally expect to happen in normal
situations. But what kind of reasoning is needed for natural language semantics?
The answer to this question is based on the remark that texts seldom provide normal details that are
assumed to be known to the reader. Instead, they focus on abnormal situations or at least on events
that cannot be inferred by default from the text by an ordinary reader. A central issue in the human
understanding of NL is the ability to infer systematically and easily an amount of implicit
information necessary to answer indirect questions about the text. The consequences resulting from
truth-based entailments are logically valid but they are poor and quite limited. Those obtained by a
norm-based approach are defeasible: they are admitted as long as the text does not mention explicit
elements that contradict them. However they provide richer information and enable a deeper
understanding of the text. That is why the norm-based reasoning must be non-monotonic. In
addition to this central question, the representation language must take into account a number of
modalities (including the temporal aspect) that are very useful to answer different questions on NL
texts.
The next section gives a general logical framework to represent in a first order language the
necessary knowledge about a domain and allows non-monotonic reasoning. Section 3 shows how to
implement our representation language fragment in the formalism of Answer Set Programming by
transforming them into extended logic programs. In section 4, we discuss the use of our language in
</bodyText>
<footnote confidence="0.545991">
1 In A.I, the word norm is commonly used in the « normative » sens. Here, it is rather used in the « normal » sens.
</footnote>
<bodyText confidence="0.97002725">
the car crash domain to find automatically the cause of an accident from its textual description. The
kind of inference rules required in this application is showed through a detailed presentation of the
analysis of a text from the corpus we are using. Finally, we conclude and give some perspectives for
future work in section 5.
</bodyText>
<sectionHeader confidence="0.68492" genericHeader="method">
2. Knowledge representation language
</sectionHeader>
<bodyText confidence="0.999978625">
The explicit information evoked in a given text provides the starting point for the reasoning process
that aims to understand it. Thus, the first task to do is to extract from the text this explicit
information and to represent it in an adequate language. The richness and flexibility of NL
constrains the representation language to take into account a number of aspects whose necessity and
importance may vary from an application to another. In what follows, we describe a logical
language which enhances within the first order framework some aspects that we believe to be useful
in an inferential approach to NL semantics. Namely, the proposed language allows the
representation of time, modalities and non-monotonic inferences (see [7] for more details).
</bodyText>
<subsectionHeader confidence="0.935734">
2.1 Reification
</subsectionHeader>
<bodyText confidence="0.999982904761905">
The first idea that comes to mind when representing knowledge about NL statements is to use first
order predicates to express properties of objects, agents ...etc. However we need often to treat
further aspects. For example, we need to represent modalities on the considered properties and to
reason about them i.e. to use the predicate names themselves as variables over which one can
quantify in order to avoid the use of ad hoc inference rules, i.e. to factorise the rules at an adequate
level of abstraction. To solve this problem within the framework of first order logic, we use the
reification technique, commonly used in Artificial Intelligence (AI). Instead of writing P(X, Y) to
express the fact that property P applies to arguments X and Y, we write Holds(P, X, Y). The property
name P becomes then an argument in the new predicate Holds. i.e. P will be a variable over
properties and it can be quantified in inference rules.
The use of the reification technique yields to two main drawbacks: first, it forces a fixed arity for
the predicate Holds whereas properties in general may have a different number of arguments. The
second problem is the necessity to redefine ad hoc axioms about the properties (negation,
conjunction, disjunction... of properties). One possible solution to the first problem is to consider a
special binary function combine which constructs a new “complex” argument from two other
arguments. For example, as the predicate Holds has three arguments then, the predicate Q(X, Y, Z)
can be reified as : Holds(combine(Q, X), Y, Z)2. In general, this corresponds well to linguistic
practice: for example the application of a transitive verb to its complement can be considered as a
unique “complex” property comparable to an intransitive verb. Concerning the second problem, it
turns out that in practice we often do not need all the axioms but only some particular ones. So we
have to represent only those axioms that we really need in the application considered.
</bodyText>
<subsectionHeader confidence="0.999915">
2.2 Representing time
</subsectionHeader>
<bodyText confidence="0.970481392857143">
Generally, narrative texts describe events that take place in a time perceived as continuous. The
temporal aspect is crucial in their understanding. Two representation approaches are possible for
time: either we represent the continuous time which reflects the physical reality and use the elegant
mathematical tools developed for mechanics, or we represent the discrete time which reflects the
text structure and which corresponds rather to a naive physics. We chose the second approach,
because generally, texts are written by persons who ignore the mathematical details of motion, and
they can be understood without having such knowledge. Two approaches are still possible for a
discrete model of time. Either we use a linear model in which only the events that happened in
reality are represented, or we take into account the unrealized futures as part of the temporal
information. In this case, we use a branching time model [5][10]. This last model is richer than the
2 As a concrete example, the ternary predicate bump(A, B, T) (vehicle A bumps vehicle B at time T) is written after
reification and by using the combine function as : Holds(combine(bump, B), A, T). The term combine(bump, B)
expresses then the complex property of « bumping the vehicle B ».
former and can be very useful in some cases. In this paper we are interested only on the linear
model. What is important for us in time modelling is to establish an order between the events
evoked in the text. Of course, this choice limits the use of our language to applications which do not
need deeper structure of time but it remains useful in practice (see section 4 for a possible
application). Indeed, the unrealized futures are not completely excluded in our model, as they can
be represented implicitly by modalities (see the modality able in section 4.2.2 ).
The semantics used for time in our model is situated somehow between an interval-based and a
point-based semantics: the scene of the accident described in the text is decomposed as a succession
of ordered time elements. Each time element is denoted by an integer representing its order number.
This integer is used as an argument in the predicates. The meaning of the element depends on the
nature of the property. If it is a persistent property, the time parameter denotes the entire time
interval during which this property remains true (interval based semantics). If the property is not
persistent (corresponds to an action or a punctual event) then the temporal argument denotes the
starting point of the interval on which the property occurs and causes at least one persistent property
to change its truth value.
</bodyText>
<subsectionHeader confidence="0.989178">
2.3 Modalities
</subsectionHeader>
<bodyText confidence="0.99981075">
Modalities express properties of the predicates other than their truth value, which can be considered
as a null modality. Different types of modal logics have been developed to formalize the reasoning
about modalities. Even though the reasoning we want to apply on texts makes use of modalities, it
can be carried out without developing new modal logics with ‘complete’ axiomatizations. What we
really need is to represent the modalities as first order predicates using the reification technique
discussed in section 2.1., and to define only useful axioms as inference rules. For example, to
represent the fact that the modality Mod is applied to the predicate P having X1, ..., Xn as arguments
we write : Mod(P, X1, ..., Xn) instead of the classical notation : Mod P(X1, ..., Xn).
</bodyText>
<subsectionHeader confidence="0.994869">
2.4 Non-monotonicity
</subsectionHeader>
<bodyText confidence="0.9897406">
Non-monotonicity is an essential characteristic of the nature of the reasoning used by humans to
understand texts. Among the different approaches proposed in the literature to formalise this variant
of commonsense reasoning, we have used Reiter’s default logic [11] to represent our inference
rules. The fixed point semantics used to compute the default theories extensions seems to be
adequate to the nature of the NL understanding process. Indeed, as discussed in section 1, the NL
understanding process cannot be decomposed in a sequence of separate steps but it consists in the
simultaneous satisfaction of several linguistic as well extra-linguistic constraints in a manner that
can be approached by the search of some fixed point of the meaning of the given text.
Two kinds of inference rules are considered: the strict inferences represented by material
implications and the defeasible ones represented by Reiter’s defaults. To facilitate the
implementation of our rules on the answer set programming paradigm (see section 3) we limit their
forms as follows:
Let A1,..., An, B, C1, ..., Ck be first order literals.
The Expression (1) is a material implication. It means that B is inferred whenever A1, ..., An are
verified. Two kinds of default rules are considered. The first form (2) corresponds to a “normal”
default. It means that if we have A1, ..., An then, we can infer B as long as this is consistent. The
second one (3) corresponds to a semi-normal default and its meaning is that in general, when we
have A1, ..., An then, we can infer B as long as this is consistent and none of ¬Ci (i=1..k)
belongs to the extension3. Semi normal defaults are particularly useful to establish a priority order
between inference rules which can not be done using only normal defaults[12].
</bodyText>
<footnote confidence="0.851211">
A1 A ...A An → B (1)
A1 A ...A An : B (2)
A1 A ...A An : B[C1, ..., Ck] (3)
3 We use a notation in which A : B stands for A : B and A : B[C] stands for A : B, C
B B
</footnote>
<sectionHeader confidence="0.368038" genericHeader="method">
3. Implementation by Answer Set Programming
</sectionHeader>
<subsectionHeader confidence="0.983743">
3.1. Theoretical backgrounds
</subsectionHeader>
<bodyText confidence="0.989600428571429">
Answer Set Programming (ASP) is a recent paradigm covering different kinds of logic programs,
and associated semantics. It allows representing and solving various problems in Artificial
Intelligence. On one hand, we can cite combinatorial problems as k-coloring graph, path finding,
timetabling, ... On another hand, ASP is also concerned by problems arising when available
information is incomplete as non-monotonic reasoning, planning, diagnosis, ... The non familiar
reader will find additional information about ASP on the web site of the working group WASP
(http://wasp.unime.it/).
In the present work we are particularly interested in using ASP as a framework for default
reasoning. For this we use Extended Logic Programs (ELP) to represent knowledge by means of
rules containing positive information and strong or default negative information and we interpret
them by answer set semantics [3]. Formally, an ELP is a set of rules of the form
c &lt;— a1, ..., an, not b1, ..., not bm. n &gt; 0 and m &gt; 0
where c, ai and bj are literals.
For a given rule r, we denote
</bodyText>
<equation confidence="0.817012">
head(r) = c body+(r)={a1, ..., an} body-(r)={b1, ..., bm} r+=c &lt;— a1, ..., an
Definition
Let R be a set of rules without default negation (V r E R, body-(r) = 0), R is called a Definite Logic
Program. A literal set X is closed wrt R when V r E R, body+(r) c X =;&gt; head(r) E X.
</equation>
<bodyText confidence="0.991493333333333">
The set of consequences of R is Cn(R) the minimal literal set that is closed wrt R consistent or equal
to the whole set of literals of the language
For a given literal set A and an ELP P, the reduct of P by A is the definite Logic Program
</bodyText>
<equation confidence="0.751200333333333">
PA={r+  |r E P and body-(r) n A = 01
Definition
Let P be an ELP and A a literal set. A is an answer set of P if and only if A=Cn(PA)
Examples
P1={a &lt;— not b., b &lt;— not a., -c &lt;— b.} has two answer sets {a} and {b, -c}
P2={a &lt;— not a.} has no answer set at all.
</equation>
<bodyText confidence="0.801257142857143">
We have recalled the basic notions of answer set semantics only in the case of propositional rules.
But, obviously, for a more flexible knowledge representation, rules may contain variables. In this
case, a rule is considered as a global schema for the set of fully instanciated rules that can be
obtained by replacing every variable by every constant in the language.
Example
P={bird(1)., bird(2)., penguin(2)., fly(X) &lt;— bird(X), not penguin(X)., -fly(X) &lt;— penguin(X).} is
equivalent to the program
</bodyText>
<equation confidence="0.83299">
P&apos;={bird(1)., bird(2)., penguin(2)., fly(1) &lt;— bird(1), not penguin(1)., -fly(1) &lt;— penguin(1)., fly(2)
&lt;— bird(2), not penguin(2)., -fly(2) :&lt;— penguin(2).}
Then, P (formally P&apos;) has one answer set {bird(1), bird(2), penguin(2), fly(1), -fly(2)}.
</equation>
<bodyText confidence="0.9908368">
Let us mention an important point for our work that is answer set semantics for ELP can be viewed
as a subcase of default logic [2][3]. By translating every rule r = c &lt;— a1, ..., an, not b1, ..., not bm.
into the default rule : T(r) = a1 A ... A an : c [-b1, ..., -bm ]
By this way :
If S is an answer set of an ELP P, then Th(S) is an extension of the default theory (0,T(P))
every extension of (0,T(P)) is the deductive closure of one answer set of P.
Obviously, in whole generality every default theory cannot be translated into an ELP. But as we
explain it later, it is possible to encode some restricted default theories in an ELP. By this way it is
possible to envisage realistic applications of default reasoning since several software packages for
ASP are available today, e.g. the following ones:
</bodyText>
<footnote confidence="0.98449625">
DLV[8] http://www.dbai.tuwien.ac.at/proj/dlv
Smodels [13] http://www.tcs.hut.fi/Software/smodels
Cmodels [9] http://www.cs.utexas.edu/users/tag/cmodels.html
Nomore++[1] http://www.cs.uni­potsdam.de/wv/nomore++
</footnote>
<subsectionHeader confidence="0.915331">
3.2. From Default Logic to ASP
</subsectionHeader>
<bodyText confidence="0.654964944444444">
Here, we explain how we have encoded our knowledge base that is originally a default theory, into
an extended logic program. A very important point to note is that our original knowledge base does
not contain disjunctions. Since a default theory is a pair consisting in a set of classical formulas and
a set of default rules, we distinguish two major translations.
classical formulas ELP
one fact : a one rule with an empty body : a.
a conjunction of n facts : a1 A ... A an n rules with empty bodies: a1. ... an.
a material implication a1 A ... A an —&gt; b one direct rule b &lt;- a1, ... , an.
and n contrapositive rules :
--a1 &lt;---b, a2, ... , an.
...
--an &lt;---b, a1, ... , an-1.
default rules ELP
A1, ..., An : B 7 b ← a1, ..., an, not ¬b.
A1, ..., An : B[C1, ..., CkJ b ← a1, ..., an, not ¬b, not ¬c1,..., not ¬cm.
We have preferred to encode firstly our rules in default logic instead using directly ASP because
default logic is more compact than ASP, which needs more rules, especially for contrapositives. The
translation of default logic into ASP can be easily auomated.
</bodyText>
<sectionHeader confidence="0.907211" genericHeader="method">
4. From the description of an accident to its cause
</sectionHeader>
<subsectionHeader confidence="0.992978">
4.1. The corpus
</subsectionHeader>
<bodyText confidence="0.998901666666667">
We are working on a sample of 60 representative texts of a larger corpus. These texts are short
descriptions of car accident circumstances. They are written (in French) by persons implied in the
accidents to be sent to their insurance company4. The length of our texts varies between 9 and 167
words. They contain 129 sentences whose length varies between 4 and 55 words; the longest report
has 7 sentences and there are 24 reports that contain only one sentence. The total number of word
occurrences is 2256. But there are only 500 distinct words corresponding to 391 dictionary entries.
</bodyText>
<sectionHeader confidence="0.3899" genericHeader="method">
4 We are grateful to the MAIF insurance company for having given us access to the reports that constitute our corpus.
</sectionHeader>
<subsectionHeader confidence="0.7477195">
4.2. Our task
4.2.1. Finding the cause of the accident
</subsectionHeader>
<bodyText confidence="0.999992565217391">
The objective of the system we are developing is to find automatically the cause of an accident from
its textual description. Because of the very controversial nature of causality we must define more
precisely our objective. We are interested in our study by the interventionist conception of causality
in which voluntary actions are privileged as potential causes of events. This is in correspondence
with the practical use of causality in AI. Moreover, we claim that the most plausible causes for
abnormal situations like accidents are those that reflect violation of norms (anomalies)[6]. We
consider that the system has understood a text if it finds the same cause as the one given by an
ordinary human reader. We have then determined manually the cause of each text and we have used
this information to validate the results of the system.
Two essential steps are considered in the overall architecture of the system. The first one “the
linguistic step” applies a tagger and syntactical analyser to extract a set of surface relations between
words. These relations are then progressively transformed by an adequate “ linguistic reasoning”
into the so-called “semantic predicates” which express the explicit information provided by the text.
The semantic predicates are represented in a “semantic language” as the one discussed in section 2.
This part of the system, which is under construction, tries to adapt existing methods to deal with the
problems of anaphora resolution and time ordering of the events described in a text. We will not
discuss the details of the linguistic step in this paper. The second step: “the semantic step” applies a
set of strict and default inference rules based on norms of the road domain to enrich the semantic
predicates initially extracted from the text by further semantic predicates enhancing implicit
information. The inference rules are designed manually and reflect rudimentary reasoning that any
reader of the text makes systematically. This semantic reasoning process stops as soon as the system
infers the necessary information that characterizes an anomaly. Section 5 gives further details about
the semantic reasoning through an example taken from the corpus.
</bodyText>
<subsectionHeader confidence="0.76437">
4.2.2. Some specificities
</subsectionHeader>
<bodyText confidence="0.999447">
The majority of the semantic predicates used in our system have the form: Holds(P, A, T) where P
is a simple or a complex property (expressed by the binary function combine), A is an agent
(generally a vehicle involved in the accident) and T is the order number of a time interval during
which (or at the beginning of which) the property P holds (to simplify, we will say henceforth that
property P holds at time T). For example Holds(stop, ag, 3) means that the agent ‘ag’ is stopped at
time 3 and Holds(combine(follows, ag1), ag2, 2) means that at time 2, agent ‘ag2’ follows agent ‘ag1’
(in a file of vehicles). When needed a function neg is applied to a property to have its negation. We
introduce the rule (4)
</bodyText>
<equation confidence="0.941912">
Holds(neg(P), A, T) ↔ ¬Holds(P, A, T) (4)
</equation>
<bodyText confidence="0.916815533333333">
The main modalities that we use in our system cope respectively with duties and capacities :
must(P, A, T) means that at time T, agent A has the duty to achieve the property P.
able(P, A, T) means that at time T, agent A is able to achieve the property P. In terms of branching
time, this means that there is some possible future in which P holds.
The semantic reasoning is designed so that it converges to a “kernel” containing a limited number
of semantic predicates5 in terms of which all possible anomalies can be expressed. In a given text, it
is possible that several anomalies coexist. In this case, the system distinguishes between the primary
anomaly which can be considered as the most plausible cause of the accident and the other
anomalies called “derived anomalies”. A primary anomaly has two forms: either an agent A has the
duty and the capacity to achieve a property P at a time T and at time T+1 a property P&apos; incompatible
5 The predicates of the kernel are : Holds(control, A, T) [A has the control of his/her vehicle], Holds(moves_back, A, T) [A moves
back], Holds(starts, A, T) [A moves off], Holds(drives_slowly, A, T) [ A drives fairly slowly], Holds(stops, A, T) [A is stopped],
Holds(comb(disruptive_factor, X), A, T) [X is a disruptive factor for A]
with P holds (5) or some disruptive and inevitable factor occurs and causes the accident (6). The
form of a derived anomaly (7) differs from that of a primary one only on the agent&apos;s capacity.
</bodyText>
<equation confidence="0.691414333333333">
primary_an(P, A, T) &lt;— property(P), vehicle(A), time(T), must(P, A, T), able(P, A, T),
holds(P&apos;, A, T+1), incompatible(P, P&apos;) (5)
primary_an(combine(disruptive_factor, X), A, T) &lt;— object(X), vehicle(A), time(T),
holds(combine(disruptive_factor, X), A, T) (6)�
derived_an(P, A, T) &lt;— property(P), vehicle(A), time(T), must(P, A, T), ¬ able(P, A, T), holds(P&apos;,
A, T+1), incompatible(P, P&apos;) (7)
</equation>
<subsectionHeader confidence="0.994652">
4.3. An example
</subsectionHeader>
<bodyText confidence="0.991866833333333">
To illustrate our methodology, let us consider the following text of the corpus (translated into
english) and explain the inference rules involved in its analysis :
« Whereas vehicle B was overtaking me, the driver lost the control of its vehicle. It bumped on the
central guardrail , and crossed the ways. It then cut my way. My vehicle A initially bumped on
vehicle B on its right side, before being crushed on the guardrail. »
The set of the semantic predicates extracted from the text are :
</bodyText>
<construct confidence="0.921105">
holds(overtake, veh_b, 1), ¬ holds(control, veh_b, 2),
holds(combine(bump, guardrail), veh_b, 3), ¬ holds(stop, veh_b, 4),
holds(combine(bump, veh_b),veh_a, 5), holds(combine(bump, guardrail), veh_a, 6)
vehicle(veh_a), vehicle(veh_b), object(veh_a), object(veh_b), object(guardrail).
</construct>
<bodyText confidence="0.999463">
In what follows, we show how the application of inference rules leads to the determination of the
primary and the derived anomalies:
</bodyText>
<equation confidence="0.837283">
Rule(8) states that “at the starting state 0, each vehicle has the control”.
holds(control, A, 0) &lt;— agent(A), vehicle(A) (8)
</equation>
<bodyText confidence="0.925392">
It allows to infer : holds(control, veh_a, 0), holds(control, veh_b, 0)
</bodyText>
<equation confidence="0.474354">
Rule(9) states that “if B is a vehicle that bumps on A at time T, then B is not stopped at this time”.
¬ holds(stop, A, T) &lt;— vehicle(A), object(B), time(T), holds(combine(bump, B), A, T) (9)
</equation>
<bodyText confidence="0.564139">
It allows to infer: ¬ holds(stop, veh_b, 3), ¬ holds(stop, veh_a, 5), ¬ holds(stop, veh_a, 6)
Rules(10) and (11) state that “if A is a vehicle that bumps on B at time T, then there is at this time a
shock (symmetric) between A and B”.
</bodyText>
<construct confidence="0.814529333333333">
holds(combine(shock, B), A, T) &lt;— vehicle(A), object(B), time(T), holds(combine(bump, B), A, T) (10)
holds(combine(shock, A), B, T) &lt;— object(A), object(B), time(T), holds(combine(shock, B), A, T) (11)
The set of predicates inferred by these rules are :
holds(combine(shock, guardrail), veh_b, 3), holds(combine(shock, veh_b), guardrail, 3),
holds(combine(shock, veh_b), veh_a, T), holds(combine(shock, veh_a), veh_b, T),
holds(combine(shock, veh_a), guardrail, T), holds(combine(shock, guardrail), veh_a, T)
</construct>
<footnote confidence="0.5769575">
Rule(12) states that “if A is implied in two successive shocks at times T and T+1, then we deduce
that it lost the control after the first shock (during the time interval T)”.
¬ holds(control, A, T) &lt;— agent(A), object(B), object(C), time(T), holds(combine(shock, A), B, T),
holds(combine(shock, A), C, T+1) (12)
</footnote>
<bodyText confidence="0.98606725">
It allows to infer: ¬ holds(control, veh_a, 5)
The remainder of information about the control of vehicles A and B during the other time intervals are
deduced using appropriate rules that handle the persistence of some particular properties. The complete set of
conclusions concerning control is as follows :
</bodyText>
<construct confidence="0.716723333333333">
holds(control, veh_b, T) (for 05 T 5 1), ¬ holds(control, veh_b, T) (for 25 T 5 6),
holds(control, veh_a, T) (for 05 T 5 4), ¬ holds(control, veh_a, T) (for 55 T 5 6)
Rule(13) states that “in general if there is a collision between a vehicle A and an object B at time T,
then B represents an obstacle for A at time T-1”.
holds(combine(obstacle, A), B, T-1) &lt;— object(A), vehicle(B), time(T),
holds(combine(shock, A), B, T), not ¬ holds(combine(obstacle, A), B, T-1) (13)
</construct>
<bodyText confidence="0.883913">
We obtain from this rule :
</bodyText>
<construct confidence="0.9278298">
holds(combine(obstacle, guardrail), veh_b, 1), holds(combine(obstacle, veh_a), veh_b, 4),
holds(combine(obstacle, veh_b), veh_a, 4), holds(combine(obstacle, guardrail), veh_a, 5)
Rules (14) and (15) allows to infer that some obstacles are not predictable. The rule (14) states that
“if a vehicle B not controlled represents at time T an obstacle to vehicle A, then this obstacle is not
predictable for A at this time T”. Whereas rule (15) states that “in general, if a vehicle B bumps a
vehicle A at time T, then B is considered as an umpredictable obstacle for A at time T”.
¬ predictable(combine(obstacle, B), A, T) &lt;— vehicle(B), vehicle(A), time(T),
holds(combine(obstacle, B), A, T), ¬ holds(control, B, T) (14)
¬ predictable(combine(obstacle, B), A, T) &lt;— vehicle(A), vehicle(B), instant(T),
vrai(combine(bump, A), B, T), not predictable(combine(obstacle, B), A, T) (15)
</construct>
<bodyText confidence="0.974365">
By these two rules we can infer : ¬ predictable(combine(obstacle, veh_a),veh_b, 4),
</bodyText>
<equation confidence="0.7982635">
¬ predictable(combine(obstacle, veh_b), veh_a, 4)
Rule(16) states that “in general, one must keep the control of one&apos;s vehicle ”
must(control,A,T) &lt;— vehicle(A), time(T), not ¬ must(control,A,T),
not ¬ holds(control,A,T) (16)
</equation>
<bodyText confidence="0.990407">
This rule infers : must(control, veh_b, 1), must(control, veh_a, 4)
The meaning of rule(17) is that “one must avoid any obstacle”.
</bodyText>
<reference confidence="0.30526525">
must(combine(avoid, X), A, T) &lt;— vehicle(A), object(X), time(T),
holds(combine(obstacle, X), A, T) (17)
This rule infers : must(combine(avoid, guardrail), veh_b, 1), must(combine(avoid, veh_a), veh_b, 4)
must(combine(avoid, veh_b), veh_a, 4), must(combine(avoid, guardrail), veh_a, 5)
Rule(18) states that “in general the duty to avoid an obstacle turns out to the duty to stop (this
default is inhibited by a number of situations illustrated in the rule)”
must(stop, A, T) &lt;— vehicle(A), object(B), time(T), must(combine(avoid, B), A, T),
holds(combine(shock, B), A, T+1), not ¬ must(stop, A, T), not must(drive_slowly, A, T),
not holds(stop, A, T), not holds(combine(follow, A), B, T), not must(not(backwards), A, T-1),
not must(not(move_off), A, T-1), not ¬ predictable(combine(obstacle, B), A, T) (18)
We can infer from this rule : must(stop, veh_b, 1), must(stop, veh_a, 5)
Rules (19) and (20) are the main rules that allow to infer agent&apos;s capacities :
able(P, A, T) &lt;— vehicle(A), object(B), time(T), action(Act), property(P), pcb(Act, P),
available(Act, P, A, T) (19)
¬able(P, A, T) &lt;— vehicle(A), object(B), time(T), action(Act), property(P), pcb(Act, P),
¬available(Act, P, A, T) (20)
</reference>
<bodyText confidence="0.9543061875">
they mean that “vehicle A is able to reach property P at time Tn if and only if there is some action
Act which is a “potential cause” for P and which is available for A to reach P at time T (the
contrapositives are omitted)”.
The occurrences of the relation pcb (which abreviates: potentially caused by) are statically
determined and stored in a static database. In our case we have : pcb(brake, stop),
pcb(combine(keep_state, control)6, control).
By default, actions are available for agents to reach the corresponding properties. This default
inference is inhibited by a number of strict rules. In our case, we obtain :
available(combine(keep_state, control), control, veh_b, 1) (the default is applied)
¬available(combine(keep_state, control), control, veh_a, 4)7
¬available(brake, stop, veh_a, 5)8
From these results it follows :
able(control, veh_b, 1), ¬ able(stop, veh_a, 4), ¬ able(stop, veh_a, 5).
The application of rules (5) and (7) we can detect the primary and the derived anomalies :
primary_an(control, veh_b, 1), derived_an(control, veh_a, 4), derived_an(stop, veh_a, 5)
Finally, the cause of the accident is expressed by: &amp;quot;the loss of control of vehicle B at time 1&amp;quot;
</bodyText>
<sectionHeader confidence="0.646605" genericHeader="conclusions">
5. Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.999952785714286">
This paper defends the idea that inferences are at the heart of the problematic of NL semantics. We
have showed that the inferences we need to understand natural language are based on our
knowledge about the norms of the domain and are non-monotonic since the conclusions of this kind
of reasoning are in general defeasible. We proposed a general representation language which takes
into account within a first order framework modalities, time and non-monotonicity that are essential
aspects in an inferential approach of NL understanding. We presented also how to transform our
inference rules into extended logic programs. To illustrate our approach in a practical domain we
have used a corpus of 60 short texts describing the circumstances of road accidents. We have used
Smodels to implement our reasoning system. With about 200 inference rules, the system succeeds to
find for each text only one stable model containing the necessary literals which express the primary
and the derived anomalies. We have determined manually for each text the answer that we hope to
obtain. Thus, the validation criterion is that the system gives for each text the same answer as the
predetermined one. The running time varies from a text to another but it does not exceed 30 seconds
which is rather encouraging. Many other perspectives of future work are open, among them:
</bodyText>
<listItem confidence="0.842845">
• Analyzing more texts of the same domain in order to verify :
</listItem>
<bodyText confidence="0.9484574">
- The validity of our hypotheses, especially those concerning the relationship
between norms and causes and the sufficiency of a linear model of time;
- that the inference rules have a sufficient degree of generality to be adapted
easily to new situations by giving the expected answers for new reports.
- the adequacy of the proposed representation language to deal with new texts.
</bodyText>
<listItem confidence="0.810446">
• Generalizing the approach to other domains
</listItem>
<footnote confidence="0.914362333333333">
6 we consider as action the fact of keeping holded some persistent property.
7 the lost of control because of a shock at time T makes unavailable the action of keeping the control at time T-1.
8 if a vehicle is not under control, then, any action is unavailable for its driver.
</footnote>
<bodyText confidence="0.7846855">
Acknowledgment. The authors are indebted to Daniel Kayser for very helpful remarks on previous
versions of this text.
</bodyText>
<sectionHeader confidence="0.930674" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898875">
[1] C. Anger, M. Gebser, T. Linke, A. Neumann and T. Schaub. The nomore++ system. In C. Baral,
G. Greco, N. Leone, and G. Terracina, editors, 8th International Conference on Logic Programming
and Nonmonotonic Reasoning (LPNMR&apos;05), 3662 : 422-426. LNAI, Springer-Verlag, Diamante,
Cosenza, Italy. 2005.
[2] N. Bidoit and C. Froidevaux. General logical databases and programs: Default logic, semantics
and stratification. Information and Computation, 91(1):1554. 1991.
[3] M. Gelfond and V. Lifschitz. Classical negation in logic programs and disjunctive databases.
New Generation Computing, 9(3-4):363385. 1991.
[4] D. Kayser. Abstraction and natural language semantics. Philosophical Transactions. R. Soc.
Lond. B 358 : 1261-1268. 2003.
[5] D. Kayser, A. Mokhtari. Time in a Causal Theory. Annals of Mathematics and Artificial
Intelligence. 22(1-2): 117-138. 1998.
[6] D. Kayser, F. Nouioua. About Norms and Causes. International Journal on Artificial Intelligence
Tools. Special Issue on FLAIRS 2004, 14(1-2): 7-23. 2005.
[7] D. Kayser, F. Nouioua. Representing Knowledge about Norms. Proc of the 16th European
Conference on Artificial Intelligence (ECAI&apos;04), pp. 363-367, Valencia, Spain. 2004.
[8] N.Leone, G. Pfeifer , W. Faber, T. Eiter, G. Gottlob, S. Perri, and F. Scarcello. The dlv system
for knowledge representation and reasoning. ACM Transactions on Computational Logic, (to
appear). 2006.
[9] Y. Lierler and M. Maratea. Cmodels-2: Sat-based answer set solver enhanced to non-tight
programs. 7th International Conference on Logic Programming and NonMonotonic Reasoning
(LPNMR&apos;04), 2923: 346-350. LNCS, Springer-Verlag, Florida, USA. 2004.
[10] D.V. McDermott. A Temporal Logic for Reasoning about Processes and Plans. Cognitive
Science 6: 101-155. 1982.
[11] R. Reiter. A Logic for Default Reasoning, Artificial Intelligence, Special Issue on
Nonmonotonic Logic, 13(1-2): 81-132. 1980.
[12] R. Reiter, G. Criscuolo : On Interacting Defaults. Proc. of the 7th International Joint
Conference on Artificial Intelligence. pp. 270-276, Vancouver, Canada. 1981
[13] T. Syrjänen and I.Niemelä. The Smodels systems. Proc. of the 6th International Conference on
Logic Programming and NonMonotonic Reasoning (LPNMR&apos;01), pp 434-438, Springer-Verlag,
Vienna, Austria. 2001.
[14] t.a.l . Special issue “Compositionnalité”. Traiteent automatique des langues 39(1). 1998.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000464">
<title confidence="0.9993905">Using Answer Set Programming in an Inference-Based approach to Language Semantics</title>
<author confidence="0.984375">Farid Nouioua Pascal Nicolas</author>
<affiliation confidence="0.871757">LIPN UMR 7030 du University of Institut Galilée – Univ. 2, bd Lavoisier F-49045 Angers</affiliation>
<address confidence="0.823812">93430 Villetaneuse – pascal.nicolas@univ-angers.fr</address>
<abstract confidence="0.981439202657807">nouiouaf@lipn.univ-paris13.fr 1. Motivation The traditional tri-partition syntax/semantics/pragmatics is commonly used in most of the computer that aim at the simulation of the human understanding of Natural Language This conception does not reflect the flexible and creative manner that humans use in reality to interpret Generally speaking, formal is referential i.e. it assumes that it is possible to create a static discourse universe and to equate the objects of this universe to the (static) meanings of words. The meaning of a sentence is then built from the meanings of the words in a compositional process and the semantic interpretation of a sentence is reduced to its logical interpretation based on the truth conditions. The very difficult task of adapting the meaning of a sentence to its context is often left to the pragmatic level, and this task requires to use a huge amount of common sense knowledge about the domain. This approach is seriously challenged (see for example [4][14]). It has been showed that the above tri-partition is very artificial because linguistic as well as extra-linguistic knowledge interact in the same global process to provide the necessary elements for understanding. Linguistic phenomena such as polysemy, plurals, metaphors shifts in meaning create real difficulties to the referential approach of the discussed above. As an alternative solution to these problems, [4] proposes an inferential approach the in which words trigger inferences depending on the context of their apparition. the same spirit we claim that understanding a is a reasoning process based on our about the of its domain i.e. what we generally expect to happen in normal situations. But what kind of reasoning is needed for natural language semantics? The answer to this question is based on the remark that texts seldom provide normal details that are assumed to be known to the reader. Instead, they focus on abnormal situations or at least on events that cannot be inferred by default from the text by an ordinary reader. A central issue in the human of the ability to infer systematically and easily an amount of implicit information necessary to answer indirect questions about the text. The consequences resulting from truth-based entailments are logically valid but they are poor and quite limited. Those obtained by a norm-based approach are defeasible: they are admitted as long as the text does not mention explicit elements that contradict them. However they provide richer information and enable a deeper understanding of the text. That is why the norm-based reasoning must be non-monotonic. In addition to this central question, the representation language must take into account a number of (including the temporal aspect) that are very useful to answer different questions on texts. The next section gives a general logical framework to represent in a first order language the necessary knowledge about a domain and allows non-monotonic reasoning. Section 3 shows how to implement our representation language fragment in the formalism of Answer Set Programming by transforming them into extended logic programs. In section 4, we discuss the use of our language in A.I, the word norm is commonly used in the « normative » sens. it is rather used in the « normal » sens. the car crash domain to find automatically the cause of an accident from its textual description. The kind of inference rules required in this application is showed through a detailed presentation of the analysis of a text from the corpus we are using. Finally, we conclude and give some perspectives for future work in section 5. 2. Knowledge representation language The explicit information evoked in a given text provides the starting point for the reasoning process that aims to understand it. Thus, the first task to do is to extract from the text this explicit and to represent it in an adequate language. The richness and flexibility of constrains the representation language to take into account a number of aspects whose necessity and importance may vary from an application to another. In what follows, we describe a logical language which enhances within the first order framework some aspects that we believe to be useful an inferential approach to Namely, the proposed language allows the representation of time, modalities and non-monotonic inferences (see [7] for more details). 2.1 Reification first idea that comes to mind when representing knowledge about is to use first order predicates to express properties of objects, agents ...etc. However we need often to treat further aspects. For example, we need to represent modalities on the considered properties and to reason about them i.e. to use the predicate names themselves as variables over which one can quantify in order to avoid the use of ad hoc inference rules, i.e. to factorise the rules at an adequate level of abstraction. To solve this problem within the framework of first order logic, we use the technique, commonly used in Artificial Intelligence Instead of writing Y) the fact that property to arguments we write X, The property then an argument in the new predicate i.e. P will be a variable over properties and it can be quantified in inference rules. The use of the reification technique yields to two main drawbacks: first, it forces a fixed arity for the predicate Holds whereas properties in general may have a different number of arguments. The second problem is the necessity to redefine ad hoc axioms about the properties (negation, conjunction, disjunction... of properties). One possible solution to the first problem is to consider a binary function constructs a new “complex” argument from two other For example, as the predicate three arguments then, the predicate Y, Z) be reified as : X), Y, In general, this corresponds well to linguistic practice: for example the application of a transitive verb to its complement can be considered as a unique “complex” property comparable to an intransitive verb. Concerning the second problem, it turns out that in practice we often do not need all the axioms but only some particular ones. So we have to represent only those axioms that we really need in the application considered. 2.2 Representing time Generally, narrative texts describe events that take place in a time perceived as continuous. The temporal aspect is crucial in their understanding. Two representation approaches are possible for time: either we represent the continuous time which reflects the physical reality and use the elegant mathematical tools developed for mechanics, or we represent the discrete time which reflects the text structure and which corresponds rather to a naive physics. We chose the second approach, because generally, texts are written by persons who ignore the mathematical details of motion, and they can be understood without having such knowledge. Two approaches are still possible for a discrete model of time. Either we use a linear model in which only the events that happened in reality are represented, or we take into account the unrealized futures as part of the temporal information. In this case, we use a branching time model [5][10]. This last model is richer than the 2 As a concrete example, the ternary predicate bump(A, B, T) (vehicle A bumps vehicle B at time T) is written after reification and by using the combine function as : Holds(combine(bump, B), A, T). The term combine(bump, B) expresses then the complex property of « bumping the vehicle B ». former and can be very useful in some cases. In this paper we are interested only on the linear model. What is important for us in time modelling is to establish an order between the events evoked in the text. Of course, this choice limits the use of our language to applications which do not need deeper structure of time but it remains useful in practice (see section 4 for a possible application). Indeed, the unrealized futures are not completely excluded in our model, as they can represented implicitly by modalities (see the modality section 4.2.2 ). The semantics used for time in our model is situated somehow between an interval-based and a point-based semantics: the scene of the accident described in the text is decomposed as a succession of ordered time elements. Each time element is denoted by an integer representing its order number. This integer is used as an argument in the predicates. The meaning of the element depends on the nature of the property. If it is a persistent property, the time parameter denotes the entire time interval during which this property remains true (interval based semantics). If the property is not persistent (corresponds to an action or a punctual event) then the temporal argument denotes the starting point of the interval on which the property occurs and causes at least one persistent property to change its truth value. 2.3 Modalities Modalities express properties of the predicates other than their truth value, which can be considered as a null modality. Different types of modal logics have been developed to formalize the reasoning about modalities. Even though the reasoning we want to apply on texts makes use of modalities, it can be carried out without developing new modal logics with ‘complete’ axiomatizations. What we really need is to represent the modalities as first order predicates using the reification technique discussed in section 2.1., and to define only useful axioms as inference rules. For example, to the fact that the modality applied to the predicate ..., as arguments write : ..., of the classical notation : ..., 2.4 Non-monotonicity Non-monotonicity is an essential characteristic of the nature of the reasoning used by humans to understand texts. Among the different approaches proposed in the literature to formalise this variant of commonsense reasoning, we have used Reiter’s default logic [11] to represent our inference rules. The fixed point semantics used to compute the default theories extensions seems to be to the nature of the process. Indeed, as discussed in section 1, the understanding process cannot be decomposed in a sequence of separate steps but it consists in the simultaneous satisfaction of several linguistic as well extra-linguistic constraints in a manner that can be approached by the search of some fixed point of the meaning of the given text. Two kinds of inference rules are considered: the strict inferences represented by material implications and the defeasible ones represented by Reiter’s defaults. To facilitate the implementation of our rules on the answer set programming paradigm (see section 3) we limit their forms as follows: B, ..., first order literals. Expression (1) is a material implication. It means that inferred whenever ..., are verified. Two kinds of default rules are considered. The first form (2) corresponds to a “normal” It means that if we have ..., then, we can infer long as this is consistent. The second one (3) corresponds to a semi-normal default and its meaning is that in general, when we ..., then, we can infer long as this is consistent and none of to the Semi normal defaults are particularly useful to establish a priority order between inference rules which can not be done using only normal defaults[12]. → (1) : B (2) : ..., (3) use a notation in which A : B stands for : and A : B[C] stands for : B, C B B 3. Implementation by Answer Set Programming 3.1. Theoretical backgrounds Set Programming a recent paradigm covering different kinds of logic programs, and associated semantics. It allows representing and solving various problems in Artificial Intelligence. On one hand, we can cite combinatorial problems as k-coloring graph, path finding, ... On another hand, also concerned by problems arising when available information is incomplete as non-monotonic reasoning, planning, diagnosis, ... The non familiar will find additional information about the web site of the working group (http://wasp.unime.it/). the present work we are particularly interested in using a framework for default For this we use Extended Logic Programs represent knowledge by means of rules containing positive information and strong or default negative information and we interpret by answer set semantics [3]. Formally, an a set of rules of the form ..., not ..., not n and m ai and literals. a given rule we denote = c ..., ..., ..., Definition a set of rules without default negation = called a Definite Logic A literal set closed wrt set of consequences of minimal literal set that is closed wrt or equal to the whole set of literals of the language a given literal set an the reduct of the definite Logic Program  |r and = Definition an literal set. an answer set of and only if Examples b., b a., two answer sets a.} no answer set at all. We have recalled the basic notions of answer set semantics only in the case of propositional rules. But, obviously, for a more flexible knowledge representation, rules may contain variables. In this case, a rule is considered as a global schema for the set of fully instanciated rules that can be obtained by replacing every variable by every constant in the language. Example bird(2)., penguin(2)., fly(X) not penguin(X)., equivalent to the program bird(2)., penguin(2)., fly(1) not penguin(1)., fly(2) not penguin(2)., has one answer set bird(2), penguin(2), fly(1), us mention an important point for our work that is answer set semantics for be viewed a subcase of default logic [2][3]. By translating every rule = c ..., not ..., not the default rule : = : c ..., ] By this way : an answer set of an then an extension of the default theory extension of the deductive closure of one answer set of in whole generality every default theory cannot be translated into an But as we it later, it is possible to encode restricted theories in an By this way it is possible to envisage realistic applications of default reasoning since several software packages for available today, e.g. the following ones: http://www.dbai.tuwien.ac.at/proj/dlv [13]http://www.tcs.hut.fi/Software/smodels [9]http://www.cs.utexas.edu/users/tag/cmodels.html http://www.cs.uni­potsdam.de/wv/nomore++ 3.2. From Default Logic to ASP Here, we explain how we have encoded our knowledge base that is originally a default theory, into an extended logic program. A very important point to note is that our original knowledge base does not contain disjunctions. Since a default theory is a pair consisting in a set of classical formulas and a set of default rules, we distinguish two major translations. classical formulas ELP fact : rule with an empty body : conjunction of n facts : rules with empty bodies: ... material implication —&gt; direct rule , and n contrapositive rules : ... , ... , default rules ELP ..., : B 7 ..., not ..., : ..., ..., not not not have preferred to encode firstly our rules in default logic instead using directly logic is more compact than needs more rules, especially for contrapositives. The of default logic into be easily auomated. 4. From the description of an accident to its cause 4.1. The corpus We are working on a sample of 60 representative texts of a larger corpus. These texts are short descriptions of car accident circumstances. They are written (in French) by persons implied in the to be sent to their insurance The length of our texts varies between 9 and 167 words. They contain 129 sentences whose length varies between 4 and 55 words; the longest report has 7 sentences and there are 24 reports that contain only one sentence. The total number of word occurrences is 2256. But there are only 500 distinct words corresponding to 391 dictionary entries. 4 We are grateful to the MAIF insurance company for having given us access to the reports that constitute our corpus. 4.2. Our task 4.2.1. Finding the cause of the accident The objective of the system we are developing is to find automatically the cause of an accident from its textual description. Because of the very controversial nature of causality we must define more precisely our objective. We are interested in our study by the interventionist conception of causality in which voluntary actions are privileged as potential causes of events. This is in correspondence the practical use of causality in Moreover, we claim that the most plausible causes for abnormal situations like accidents are those that reflect violation of norms (anomalies)[6]. We consider that the system has understood a text if it finds the same cause as the one given by an ordinary human reader. We have then determined manually the cause of each text and we have used this information to validate the results of the system. Two essential steps are considered in the overall architecture of the system. The first one “the linguistic step” applies a tagger and syntactical analyser to extract a set of surface relations between words. These relations are then progressively transformed by an adequate “ linguistic reasoning” into the so-called “semantic predicates” which express the explicit information provided by the text. The semantic predicates are represented in a “semantic language” as the one discussed in section 2. This part of the system, which is under construction, tries to adapt existing methods to deal with the problems of anaphora resolution and time ordering of the events described in a text. We will not discuss the details of the linguistic step in this paper. The second step: “the semantic step” applies a set of strict and default inference rules based on norms of the road domain to enrich the semantic predicates initially extracted from the text by further semantic predicates enhancing implicit information. The inference rules are designed manually and reflect rudimentary reasoning that any reader of the text makes systematically. This semantic reasoning process stops as soon as the system infers the necessary information that characterizes an anomaly. Section 5 gives further details about the semantic reasoning through an example taken from the corpus. 4.2.2. Some specificities majority of the semantic predicates used in our system have the form: A, T) a simple or a complex property (expressed by the binary function combine), an agent a vehicle involved in the accident) and the order number of a time interval during (or at the beginning of which) the property (to simplify, we will say henceforth that at time For example ag, 3) that the agent is stopped at 3 and 2) that at time 2, agent follows agent a file of vehicles). When needed a function applied to a property to have its negation. We introduce the rule (4) A, T) A, T) (4) The main modalities that we use in our system cope respectively with duties and capacities : A, T) that at time agent the duty to achieve the property A, T) that at time agent able to achieve the property In terms of branching this means that there is some possible future in which The semantic reasoning is designed so that it converges to a “kernel” containing a limited number semantic in terms of which all possible anomalies can be expressed. In a given text, it is possible that several anomalies coexist. In this case, the system distinguishes between the primary anomaly which can be considered as the most plausible cause of the accident and the other called “derived anomalies”. A primary anomaly has two forms: either an agent the and the capacity to achieve a property a time at time property predicates of the kernel are : A, T) the control of his/her vehicle], A, T) A, T) off], A, T) fairly slowly], A, T) stopped], X), A, T) a disruptive factor for (5) or some disruptive and inevitable factor occurs and causes the accident (6). The form of a derived anomaly (7) differs from that of a primary one only on the agent&apos;s capacity. A, T) vehicle(A), time(T), must(P, A, T), able(P, A, T), holds(P&apos;, A, T+1), incompatible(P, P&apos;) (5) X), A, T) vehicle(A), time(T), holds(combine(disruptive_factor, X), A, T) (6)� A, T) vehicle(A), time(T), must(P, A, T), A, T), holds(P&apos;, A, T+1), incompatible(P, P&apos;) (7) 4.3. An example To illustrate our methodology, let us consider the following text of the corpus (translated into english) and explain the inference rules involved in its analysis : « Whereas vehicle B was overtaking me, the driver lost the control of its vehicle. It bumped on the central guardrail , and crossed the ways. It then cut my way. My vehicle A initially bumped on vehicle B on its right side, before being crushed on the guardrail. » The set of the semantic predicates extracted from the text are : veh_b, 1), veh_b, 2), guardrail), veh_b, 3), veh_b, 4), holds(combine(bump, veh_b),veh_a, 5), holds(combine(bump, guardrail), veh_a, 6) vehicle(veh_a), vehicle(veh_b), object(veh_a), object(veh_b), object(guardrail). In what follows, we show how the application of inference rules leads to the determination of the primary and the derived anomalies: states that the starting state 0, each vehicle has the control”. A, 0) vehicle(A) (8) allows to infer : veh_a, 0), holds(control, veh_b, 0) states that B is a vehicle that bumps on A at time T, then B is not stopped at this time”. A, T) object(B), time(T), holds(combine(bump, B), A, T) (9) allows to infer: veh_b, 3), veh_a, 5), veh_a, 6) and (11) state that A is a vehicle that bumps on B at time T, then there is at this time a shock (symmetric) between A and B”. B), A, T) object(B), time(T), holds(combine(bump, B), A, T) A), B, T) object(B), time(T), holds(combine(shock, B), A, T) (11) The set of predicates inferred by these rules are : holds(combine(shock, guardrail), veh_b, 3), holds(combine(shock, veh_b), guardrail, 3), holds(combine(shock, veh_b), veh_a, T), holds(combine(shock, veh_a), veh_b, T), holds(combine(shock, veh_a), guardrail, T), holds(combine(shock, guardrail), veh_a, T) states that A is implied in two successive shocks at times T and T+1, then we deduce that it lost the control after the first shock (during the time interval T)”.</abstract>
<note confidence="0.554815">A, T) object(B), object(C), time(T), holds(combine(shock, A), B, T), A), C, T+1) allows to infer: veh_a, 5</note>
<abstract confidence="0.976720757575758">remainder of information about the control of vehicles the other time intervals are deduced using appropriate rules that handle the persistence of some particular properties. The complete set of conclusions concerning control is as follows : veh_b, T) (for veh_b, T) (for veh_a, T) (for veh_a, T) (for states that general if there is a collision between a vehicle A and an object B at time T, then B represents an obstacle for A at time T-1”. A), B, T-1) vehicle(B), time(T), A), B, T), not A), B, T-1) We obtain from this rule : holds(combine(obstacle, guardrail), veh_b, 1), holds(combine(obstacle, veh_a), veh_b, holds(combine(obstacle, veh_b), veh_a, 4), holds(combine(obstacle, guardrail), veh_a, 5) Rules (14) and (15) allows to infer that some obstacles are not predictable. The rule (14) states that “if a vehicle B not controlled represents at time T an obstacle to vehicle A, then this obstacle is not for A at this time T”. rule (15) states that general, if a vehicle B bumps a vehicle A at time T, then B is considered as an umpredictable obstacle for A at time T”. B), A, T) vehicle(A), time(T), B), A, T), B, T) B), A, T) vehicle(B), instant(T), vrai(combine(bump, A), B, T), not predictable(combine(obstacle, B), A, T) these two rules we can infer : veh_a),veh_b, 4), veh_b), veh_a, 4) states that general, one must keep the control of one&apos;s vehicle ” time(T), not not rule infers : veh_b, 1), must(control, veh_a, 4) meaning of rule(17) is that must avoid any obstacle”. X), A, T) object(X), time(T), X), A, T) rule infers : guardrail), veh_b, 1), must(combine(avoid, veh_a), veh_b, 4) must(combine(avoid, veh_b), veh_a, 4), must(combine(avoid, guardrail), veh_a, 5) states that general the duty to avoid an obstacle turns out to the duty to stop (this default is inhibited by a number of situations illustrated in the rule)”</abstract>
<note confidence="0.7633784">A, T) object(B), time(T), must(combine(avoid, B), A, T), B), A, T+1), not A, T), not must(drive_slowly, A, T), not holds(stop, A, T), not holds(combine(follow, A), B, T), not must(not(backwards), A, T-1), must(not(move_off), A, T-1), not B), A, T) can infer from this rule : veh_b, 1), must(stop, veh_a, 5) Rules (19) and (20) are the main rules that allow to infer agent&apos;s capacities : A, T) object(B), time(T), action(Act), property(P), pcb(Act, P), available(Act, P, A, T) (19) A, T) object(B), time(T), action(Act), property(P), pcb(Act, P), P, A, T) (20)</note>
<abstract confidence="0.979507860465117">they mean that “vehicle A is able to reach property P at time Tn if and only if there is some action Act which is a “potential cause” for P and which is available for A to reach P at time T (the omitted)”. occurrences of the relation abreviates: potentially caused by) are statically and stored in a static database. In our case we have : stop), control). By default, actions are available for agents to reach the corresponding properties. This default is inhibited by a number of strict rules. In our case, we obtain available(combine(keep_state, control), control, veh_b, 1) (the default is control), control, veh_a, stop, veh_a, From these results it follows : veh_b, 1), veh_a, 4), veh_a, 5). The application of rules (5) and (7) we can detect the primary and the derived anomalies : primary_an(control, veh_b, 1), derived_an(control, veh_a, 4), derived_an(stop, veh_a, 5) Finally, the cause of the accident is expressed by: &amp;quot;the loss of control of vehicle B at time 1&amp;quot; 5. Conclusion and perspectives This paper defends the idea that inferences are at the heart of the problematic of NL semantics. We have showed that the inferences we need to understand natural language are based on our knowledge about the norms of the domain and are non-monotonic since the conclusions of this kind of reasoning are in general defeasible. We proposed a general representation language which takes into account within a first order framework modalities, time and non-monotonicity that are essential aspects in an inferential approach of NL understanding. We presented also how to transform our inference rules into extended logic programs. To illustrate our approach in a practical domain we have used a corpus of 60 short texts describing the circumstances of road accidents. We have used implement our reasoning system. With about 200 inference rules, the system succeeds to find for each text only one stable model containing the necessary literals which express the primary and the derived anomalies. We have determined manually for each text the answer that we hope to obtain. Thus, the validation criterion is that the system gives for each text the same answer as the predetermined one. The running time varies from a text to another but it does not exceed 30 seconds which is rather encouraging. Many other perspectives of future work are open, among them: • Analyzing more texts of the same domain in order to verify : - The validity of our hypotheses, especially those concerning the relationship between norms and causes and the sufficiency of a linear model of time; that the inference rules have a sufficient degree of generality to be adapted easily to new situations by giving the expected answers for new reports. the adequacy of the proposed representation language to deal with new texts. • Generalizing the approach to other domains consider as action the fact of keeping holded some persistent property. lost of control because of a shock at time T makes unavailable the action of keeping the control at time T-1. a vehicle is not under control, then, any action is unavailable for its driver. authors are indebted to Daniel Kayser for very helpful remarks on previous versions of this text.</abstract>
<title confidence="0.925794">References</title>
<author confidence="0.964547">nomore In C Baral</author>
<affiliation confidence="0.448189">Greco, N. Leone, and G. Terracina, editors, International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR&apos;05), 3662 : 422-426. LNAI, Springer-Verlag, Diamante,</affiliation>
<address confidence="0.800476">Cosenza, Italy. 2005.</address>
<note confidence="0.905698392857143">N. Bidoit and C. Froidevaux. logical databases and programs: Default logic, semantics Information and Computation, 91(1):1554. 1991. M. Gelfond and V. Lifschitz. negation in logic programs and disjunctive databases. New Generation Computing, 9(3-4):363385. 1991. D. Kayser. and natural language semantics. Transactions. R. Soc. Lond. B 358 : 1261-1268. 2003. D. Kayser, A. Mokhtari. in a Causal Annals of Mathematics and Artificial Intelligence. 22(1-2): 117-138. 1998. D. Kayser, F. Nouioua. Norms and International Journal on Artificial Intelligence Tools. Special Issue on FLAIRS 2004, 14(1-2): 7-23. 2005. D. Kayser, F. Nouioua. Knowledge about Proc of the European Conference on Artificial Intelligence (ECAI&apos;04), pp. 363-367, Valencia, Spain. 2004. N.Leone, G. Pfeifer , W. Faber, T. Eiter, G. Gottlob, S. Perri, and F. Scarcello. dlv system knowledge representation and ACM Transactions on Computational Logic, (to appear). 2006. Y. Lierler and M. Maratea. Sat-based answer set solver enhanced to non-tight International Conference on Logic Programming and NonMonotonic Reasoning (LPNMR&apos;04), 2923: 346-350. LNCS, Springer-Verlag, Florida, USA. 2004. D.V. McDermott. Temporal Logic for Reasoning about Processes and Plans. Science 6: 101-155. 1982. R. Reiter. Logic for Default Reasoning, Intelligence, Special Issue on Nonmonotonic Logic, 13(1-2): 81-132. 1980. R. Reiter, G. Criscuolo : Interacting Proc. of the 7th International Joint Conference on Artificial Intelligence. pp. 270-276, Vancouver, Canada. 1981 T. and Smodels Proc. of the International Conference on Logic Programming and NonMonotonic Reasoning (LPNMR&apos;01), pp 434-438, Springer-Verlag, Vienna, Austria. 2001. t.a.l . Special issue “Compositionnalité”. automatique des langues 1998.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>object vehicle</author>
<author>holds time</author>
<author>T A</author>
</authors>
<title>This rule infers : must(combine(avoid, guardrail), veh_b, 1), must(combine(avoid, veh_a), veh_b, 4) must(combine(avoid, veh_b), veh_a, 4), must(combine(avoid, guardrail), veh_a, 5) Rule(18) states that “in general the duty to avoid an obstacle turns out to the duty to stop (this default is inhibited by a number of situations illustrated in the rule)” must(stop,</title>
<date></date>
<journal>A, T) &lt;— vehicle(A), object(B), time(T), must(combine(avoid, B), A, T), holds(combine(shock, B), A, T+1), not ¬ must(stop, A, T), not must(drive_slowly, A, T), not holds(stop, A, T), not holds(combine(follow, A), B, T), not must(not(backwards), A, T-1), not must(not(move_off), A, T-1), not ¬ predictable(combine(obstacle, B), A, T</journal>
<volume>20</volume>
<marker>vehicle, time, A, </marker>
<rawString> must(combine(avoid, X), A, T) &lt;— vehicle(A), object(X), time(T), holds(combine(obstacle, X), A, T) (17) This rule infers : must(combine(avoid, guardrail), veh_b, 1), must(combine(avoid, veh_a), veh_b, 4) must(combine(avoid, veh_b), veh_a, 4), must(combine(avoid, guardrail), veh_a, 5) Rule(18) states that “in general the duty to avoid an obstacle turns out to the duty to stop (this default is inhibited by a number of situations illustrated in the rule)” must(stop, A, T) &lt;— vehicle(A), object(B), time(T), must(combine(avoid, B), A, T), holds(combine(shock, B), A, T+1), not ¬ must(stop, A, T), not must(drive_slowly, A, T), not holds(stop, A, T), not holds(combine(follow, A), B, T), not must(not(backwards), A, T-1), not must(not(move_off), A, T-1), not ¬ predictable(combine(obstacle, B), A, T) (18) We can infer from this rule : must(stop, veh_b, 1), must(stop, veh_a, 5) Rules (19) and (20) are the main rules that allow to infer agent&apos;s capacities : able(P, A, T) &lt;— vehicle(A), object(B), time(T), action(Act), property(P), pcb(Act, P), available(Act, P, A, T) (19) ¬able(P, A, T) &lt;— vehicle(A), object(B), time(T), action(Act), property(P), pcb(Act, P), ¬available(Act, P, A, T) (20)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Anger</author>
<author>M Gebser</author>
<author>T Linke</author>
<author>A Neumann</author>
<author>T Schaub</author>
</authors>
<title>The nomore++ system.</title>
<date>2005</date>
<booktitle>8th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR&apos;05), 3662 :</booktitle>
<pages>422--426</pages>
<editor>In C. Baral, G. Greco, N. Leone, and G. Terracina, editors,</editor>
<publisher>LNAI, Springer-Verlag,</publisher>
<location>Diamante, Cosenza, Italy.</location>
<contexts>
<context position="15971" citStr="[1]" startWordPosition="2658" endWordPosition="2658">T(P)) every extension of (0,T(P)) is the deductive closure of one answer set of P. Obviously, in whole generality every default theory cannot be translated into an ELP. But as we explain it later, it is possible to encode some restricted default theories in an ELP. By this way it is possible to envisage realistic applications of default reasoning since several software packages for ASP are available today, e.g. the following ones: DLV[8] http://www.dbai.tuwien.ac.at/proj/dlv Smodels [13] http://www.tcs.hut.fi/Software/smodels Cmodels [9] http://www.cs.utexas.edu/users/tag/cmodels.html Nomore++[1] http://www.cs.unipotsdam.de/wv/nomore++ 3.2. From Default Logic to ASP Here, we explain how we have encoded our knowledge base that is originally a default theory, into an extended logic program. A very important point to note is that our original knowledge base does not contain disjunctions. Since a default theory is a pair consisting in a set of classical formulas and a set of default rules, we distinguish two major translations. classical formulas ELP one fact : a one rule with an empty body : a. a conjunction of n facts : a1 A ... A an n rules with empty bodies: a1. ... an. a material im</context>
</contexts>
<marker>[1]</marker>
<rawString>C. Anger, M. Gebser, T. Linke, A. Neumann and T. Schaub. The nomore++ system. In C. Baral, G. Greco, N. Leone, and G. Terracina, editors, 8th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR&apos;05), 3662 : 422-426. LNAI, Springer-Verlag, Diamante, Cosenza, Italy. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bidoit</author>
<author>C Froidevaux</author>
</authors>
<title>General logical databases and programs: Default logic, semantics and stratification.</title>
<date>1991</date>
<journal>Information and Computation,</journal>
<volume>91</volume>
<issue>1</issue>
<contexts>
<context position="15127" citStr="[2]" startWordPosition="2520" endWordPosition="2520"> rules that can be obtained by replacing every variable by every constant in the language. Example P={bird(1)., bird(2)., penguin(2)., fly(X) &lt;— bird(X), not penguin(X)., -fly(X) &lt;— penguin(X).} is equivalent to the program P&apos;={bird(1)., bird(2)., penguin(2)., fly(1) &lt;— bird(1), not penguin(1)., -fly(1) &lt;— penguin(1)., fly(2) &lt;— bird(2), not penguin(2)., -fly(2) :&lt;— penguin(2).} Then, P (formally P&apos;) has one answer set {bird(1), bird(2), penguin(2), fly(1), -fly(2)}. Let us mention an important point for our work that is answer set semantics for ELP can be viewed as a subcase of default logic [2][3]. By translating every rule r = c &lt;— a1, ..., an, not b1, ..., not bm. into the default rule : T(r) = a1 A ... A an : c [-b1, ..., -bm ] By this way : If S is an answer set of an ELP P, then Th(S) is an extension of the default theory (0,T(P)) every extension of (0,T(P)) is the deductive closure of one answer set of P. Obviously, in whole generality every default theory cannot be translated into an ELP. But as we explain it later, it is possible to encode some restricted default theories in an ELP. By this way it is possible to envisage realistic applications of default reasoning since seve</context>
</contexts>
<marker>[2]</marker>
<rawString>N. Bidoit and C. Froidevaux. General logical databases and programs: Default logic, semantics and stratification. Information and Computation, 91(1):1554. 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gelfond</author>
<author>V Lifschitz</author>
</authors>
<title>Classical negation in logic programs and disjunctive databases.</title>
<date>1991</date>
<journal>New Generation Computing,</journal>
<pages>9--3</pages>
<contexts>
<context position="13303" citStr="[3]" startWordPosition="2170" endWordPosition="2170">.. On another hand, ASP is also concerned by problems arising when available information is incomplete as non-monotonic reasoning, planning, diagnosis, ... The non familiar reader will find additional information about ASP on the web site of the working group WASP (http://wasp.unime.it/). In the present work we are particularly interested in using ASP as a framework for default reasoning. For this we use Extended Logic Programs (ELP) to represent knowledge by means of rules containing positive information and strong or default negative information and we interpret them by answer set semantics [3]. Formally, an ELP is a set of rules of the form c &lt;— a1, ..., an, not b1, ..., not bm. n &gt; 0 and m &gt; 0 where c, ai and bj are literals. For a given rule r, we denote head(r) = c body+(r)={a1, ..., an} body-(r)={b1, ..., bm} r+=c &lt;— a1, ..., an Definition Let R be a set of rules without default negation (V r E R, body-(r) = 0), R is called a Definite Logic Program. A literal set X is closed wrt R when V r E R, body+(r) c X =;&gt; head(r) E X. The set of consequences of R is Cn(R) the minimal literal set that is closed wrt R consistent or equal to the whole set of literals of the language For a gi</context>
<context position="15130" citStr="[3]" startWordPosition="2520" endWordPosition="2520">les that can be obtained by replacing every variable by every constant in the language. Example P={bird(1)., bird(2)., penguin(2)., fly(X) &lt;— bird(X), not penguin(X)., -fly(X) &lt;— penguin(X).} is equivalent to the program P&apos;={bird(1)., bird(2)., penguin(2)., fly(1) &lt;— bird(1), not penguin(1)., -fly(1) &lt;— penguin(1)., fly(2) &lt;— bird(2), not penguin(2)., -fly(2) :&lt;— penguin(2).} Then, P (formally P&apos;) has one answer set {bird(1), bird(2), penguin(2), fly(1), -fly(2)}. Let us mention an important point for our work that is answer set semantics for ELP can be viewed as a subcase of default logic [2][3]. By translating every rule r = c &lt;— a1, ..., an, not b1, ..., not bm. into the default rule : T(r) = a1 A ... A an : c [-b1, ..., -bm ] By this way : If S is an answer set of an ELP P, then Th(S) is an extension of the default theory (0,T(P)) every extension of (0,T(P)) is the deductive closure of one answer set of P. Obviously, in whole generality every default theory cannot be translated into an ELP. But as we explain it later, it is possible to encode some restricted default theories in an ELP. By this way it is possible to envisage realistic applications of default reasoning since several</context>
</contexts>
<marker>[3]</marker>
<rawString>M. Gelfond and V. Lifschitz. Classical negation in logic programs and disjunctive databases. New Generation Computing, 9(3-4):363385. 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kayser</author>
</authors>
<title>Abstraction and natural language semantics.</title>
<date>2003</date>
<journal>Philosophical Transactions. R. Soc. Lond. B</journal>
<volume>358</volume>
<pages>1261--1268</pages>
<contexts>
<context position="1331" citStr="[4]" startWordPosition="203" endWordPosition="203"> possible to create a static discourse universe and to equate the objects of this universe to the (static) meanings of words. The meaning of a sentence is then built from the meanings of the words in a compositional process and the semantic interpretation of a sentence is reduced to its logical interpretation based on the truth conditions. The very difficult task of adapting the meaning of a sentence to its context is often left to the pragmatic level, and this task requires to use a huge amount of common sense knowledge about the domain. This approach is seriously challenged (see for example [4][14]). It has been showed that the above tri-partition is very artificial because linguistic as well as extra-linguistic knowledge interact in the same global process to provide the necessary elements for understanding. Linguistic phenomena such as polysemy, plurals, metaphors and shifts in meaning create real difficulties to the referential approach of the NL semantics discussed above. As an alternative solution to these problems, [4] proposes an inferential approach to the NL semantics in which words trigger inferences depending on the context of their apparition. In the same spirit we claim</context>
</contexts>
<marker>[4]</marker>
<rawString>D. Kayser. Abstraction and natural language semantics. Philosophical Transactions. R. Soc. Lond. B 358 : 1261-1268. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kayser</author>
<author>A Mokhtari</author>
</authors>
<title>Time in a Causal Theory.</title>
<date>1998</date>
<journal>Annals of Mathematics and Artificial Intelligence.</journal>
<volume>22</volume>
<issue>1</issue>
<pages>117--138</pages>
<contexts>
<context position="7839" citStr="[5]" startWordPosition="1256" endWordPosition="1256"> mechanics, or we represent the discrete time which reflects the text structure and which corresponds rather to a naive physics. We chose the second approach, because generally, texts are written by persons who ignore the mathematical details of motion, and they can be understood without having such knowledge. Two approaches are still possible for a discrete model of time. Either we use a linear model in which only the events that happened in reality are represented, or we take into account the unrealized futures as part of the temporal information. In this case, we use a branching time model [5][10]. This last model is richer than the 2 As a concrete example, the ternary predicate bump(A, B, T) (vehicle A bumps vehicle B at time T) is written after reification and by using the combine function as : Holds(combine(bump, B), A, T). The term combine(bump, B) expresses then the complex property of « bumping the vehicle B ». former and can be very useful in some cases. In this paper we are interested only on the linear model. What is important for us in time modelling is to establish an order between the events evoked in the text. Of course, this choice limits the use of our language to ap</context>
</contexts>
<marker>[5]</marker>
<rawString>D. Kayser, A. Mokhtari. Time in a Causal Theory. Annals of Mathematics and Artificial Intelligence. 22(1-2): 117-138. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kayser</author>
<author>F Nouioua</author>
</authors>
<title>About Norms and Causes.</title>
<date>2004</date>
<journal>International Journal on Artificial Intelligence Tools. Special Issue on FLAIRS</journal>
<pages>14--1</pages>
<contexts>
<context position="18539" citStr="[6]" startWordPosition="3109" endWordPosition="3109">the accident The objective of the system we are developing is to find automatically the cause of an accident from its textual description. Because of the very controversial nature of causality we must define more precisely our objective. We are interested in our study by the interventionist conception of causality in which voluntary actions are privileged as potential causes of events. This is in correspondence with the practical use of causality in AI. Moreover, we claim that the most plausible causes for abnormal situations like accidents are those that reflect violation of norms (anomalies)[6]. We consider that the system has understood a text if it finds the same cause as the one given by an ordinary human reader. We have then determined manually the cause of each text and we have used this information to validate the results of the system. Two essential steps are considered in the overall architecture of the system. The first one “the linguistic step” applies a tagger and syntactical analyser to extract a set of surface relations between words. These relations are then progressively transformed by an adequate “ linguistic reasoning” into the so-called “semantic predicates” which </context>
</contexts>
<marker>[6]</marker>
<rawString>D. Kayser, F. Nouioua. About Norms and Causes. International Journal on Artificial Intelligence Tools. Special Issue on FLAIRS 2004, 14(1-2): 7-23. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kayser</author>
<author>F Nouioua</author>
</authors>
<title>Representing Knowledge about Norms.</title>
<date>2004</date>
<booktitle>Proc of the 16th European Conference on Artificial Intelligence (ECAI&apos;04),</booktitle>
<pages>363--367</pages>
<location>Valencia,</location>
<contexts>
<context position="4852" citStr="[7]" startWordPosition="769" endWordPosition="769"> the first task to do is to extract from the text this explicit information and to represent it in an adequate language. The richness and flexibility of NL constrains the representation language to take into account a number of aspects whose necessity and importance may vary from an application to another. In what follows, we describe a logical language which enhances within the first order framework some aspects that we believe to be useful in an inferential approach to NL semantics. Namely, the proposed language allows the representation of time, modalities and non-monotonic inferences (see [7] for more details). 2.1 Reification The first idea that comes to mind when representing knowledge about NL statements is to use first order predicates to express properties of objects, agents ...etc. However we need often to treat further aspects. For example, we need to represent modalities on the considered properties and to reason about them i.e. to use the predicate names themselves as variables over which one can quantify in order to avoid the use of ad hoc inference rules, i.e. to factorise the rules at an adequate level of abstraction. To solve this problem within the framework of first</context>
</contexts>
<marker>[7]</marker>
<rawString>D. Kayser, F. Nouioua. Representing Knowledge about Norms. Proc of the 16th European Conference on Artificial Intelligence (ECAI&apos;04), pp. 363-367, Valencia, Spain. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Faber</author>
<author>T Eiter</author>
<author>G Gottlob</author>
<author>S Perri</author>
<author>F Scarcello</author>
</authors>
<title>The dlv system for knowledge representation and reasoning.</title>
<date>2006</date>
<journal>ACM Transactions on Computational Logic,</journal>
<note>(to appear).</note>
<contexts>
<context position="15809" citStr="[8]" startWordPosition="2650" endWordPosition="2650">the default rule : T(r) = a1 A ... A an : c [-b1, ..., -bm ] By this way : If S is an answer set of an ELP P, then Th(S) is an extension of the default theory (0,T(P)) every extension of (0,T(P)) is the deductive closure of one answer set of P. Obviously, in whole generality every default theory cannot be translated into an ELP. But as we explain it later, it is possible to encode some restricted default theories in an ELP. By this way it is possible to envisage realistic applications of default reasoning since several software packages for ASP are available today, e.g. the following ones: DLV[8] http://www.dbai.tuwien.ac.at/proj/dlv Smodels [13] http://www.tcs.hut.fi/Software/smodels Cmodels [9] http://www.cs.utexas.edu/users/tag/cmodels.html Nomore++[1] http://www.cs.unipotsdam.de/wv/nomore++ 3.2. From Default Logic to ASP Here, we explain how we have encoded our knowledge base that is originally a default theory, into an extended logic program. A very important point to note is that our original knowledge base does not contain disjunctions. Since a default theory is a pair consisting in a set of classical formulas and a set of default rules, we distinguish two major translations. </context>
</contexts>
<marker>[8]</marker>
<rawString>N.Leone, G. Pfeifer , W. Faber, T. Eiter, G. Gottlob, S. Perri, and F. Scarcello. The dlv system for knowledge representation and reasoning. ACM Transactions on Computational Logic, (to appear). 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lierler</author>
<author>M Maratea</author>
</authors>
<title>Cmodels-2: Sat-based answer set solver enhanced to non-tight programs.</title>
<date>2004</date>
<booktitle>7th International Conference on Logic Programming and NonMonotonic Reasoning (LPNMR&apos;04), 2923: 346-350. LNCS,</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>Florida, USA.</location>
<contexts>
<context position="15911" citStr="[9]" startWordPosition="2656" endWordPosition="2656"> ELP P, then Th(S) is an extension of the default theory (0,T(P)) every extension of (0,T(P)) is the deductive closure of one answer set of P. Obviously, in whole generality every default theory cannot be translated into an ELP. But as we explain it later, it is possible to encode some restricted default theories in an ELP. By this way it is possible to envisage realistic applications of default reasoning since several software packages for ASP are available today, e.g. the following ones: DLV[8] http://www.dbai.tuwien.ac.at/proj/dlv Smodels [13] http://www.tcs.hut.fi/Software/smodels Cmodels [9] http://www.cs.utexas.edu/users/tag/cmodels.html Nomore++[1] http://www.cs.unipotsdam.de/wv/nomore++ 3.2. From Default Logic to ASP Here, we explain how we have encoded our knowledge base that is originally a default theory, into an extended logic program. A very important point to note is that our original knowledge base does not contain disjunctions. Since a default theory is a pair consisting in a set of classical formulas and a set of default rules, we distinguish two major translations. classical formulas ELP one fact : a one rule with an empty body : a. a conjunction of n facts : a1 A .</context>
</contexts>
<marker>[9]</marker>
<rawString>Y. Lierler and M. Maratea. Cmodels-2: Sat-based answer set solver enhanced to non-tight programs. 7th International Conference on Logic Programming and NonMonotonic Reasoning (LPNMR&apos;04), 2923: 346-350. LNCS, Springer-Verlag, Florida, USA. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D V McDermott</author>
</authors>
<title>A Temporal Logic for Reasoning about Processes and Plans.</title>
<date>1982</date>
<journal>Cognitive Science</journal>
<volume>6</volume>
<pages>101--155</pages>
<contexts>
<context position="7843" citStr="[10]" startWordPosition="1256" endWordPosition="1256">chanics, or we represent the discrete time which reflects the text structure and which corresponds rather to a naive physics. We chose the second approach, because generally, texts are written by persons who ignore the mathematical details of motion, and they can be understood without having such knowledge. Two approaches are still possible for a discrete model of time. Either we use a linear model in which only the events that happened in reality are represented, or we take into account the unrealized futures as part of the temporal information. In this case, we use a branching time model [5][10]. This last model is richer than the 2 As a concrete example, the ternary predicate bump(A, B, T) (vehicle A bumps vehicle B at time T) is written after reification and by using the combine function as : Holds(combine(bump, B), A, T). The term combine(bump, B) expresses then the complex property of « bumping the vehicle B ». former and can be very useful in some cases. In this paper we are interested only on the linear model. What is important for us in time modelling is to establish an order between the events evoked in the text. Of course, this choice limits the use of our language to applic</context>
</contexts>
<marker>[10]</marker>
<rawString>D.V. McDermott. A Temporal Logic for Reasoning about Processes and Plans. Cognitive Science 6: 101-155. 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reiter</author>
</authors>
<title>A Logic for Default Reasoning,</title>
<date>1980</date>
<journal>Artificial Intelligence, Special Issue on Nonmonotonic Logic,</journal>
<pages>13--1</pages>
<contexts>
<context position="10640" citStr="[11]" startWordPosition="1721" endWordPosition="1721">using the reification technique discussed in section 2.1., and to define only useful axioms as inference rules. For example, to represent the fact that the modality Mod is applied to the predicate P having X1, ..., Xn as arguments we write : Mod(P, X1, ..., Xn) instead of the classical notation : Mod P(X1, ..., Xn). 2.4 Non-monotonicity Non-monotonicity is an essential characteristic of the nature of the reasoning used by humans to understand texts. Among the different approaches proposed in the literature to formalise this variant of commonsense reasoning, we have used Reiter’s default logic [11] to represent our inference rules. The fixed point semantics used to compute the default theories extensions seems to be adequate to the nature of the NL understanding process. Indeed, as discussed in section 1, the NL understanding process cannot be decomposed in a sequence of separate steps but it consists in the simultaneous satisfaction of several linguistic as well extra-linguistic constraints in a manner that can be approached by the search of some fixed point of the meaning of the given text. Two kinds of inference rules are considered: the strict inferences represented by material impl</context>
</contexts>
<marker>[11]</marker>
<rawString>R. Reiter. A Logic for Default Reasoning, Artificial Intelligence, Special Issue on Nonmonotonic Logic, 13(1-2): 81-132. 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reiter</author>
<author>G</author>
</authors>
<title>Criscuolo : On Interacting Defaults.</title>
<date>1981</date>
<booktitle>Proc. of the 7th International Joint Conference on Artificial Intelligence.</booktitle>
<pages>270--276</pages>
<location>Vancouver,</location>
<contexts>
<context position="12163" citStr="[12]" startWordPosition="1978" endWordPosition="1978">is inferred whenever A1, ..., An are verified. Two kinds of default rules are considered. The first form (2) corresponds to a “normal” default. It means that if we have A1, ..., An then, we can infer B as long as this is consistent. The second one (3) corresponds to a semi-normal default and its meaning is that in general, when we have A1, ..., An then, we can infer B as long as this is consistent and none of ¬Ci (i=1..k) belongs to the extension3. Semi normal defaults are particularly useful to establish a priority order between inference rules which can not be done using only normal defaults[12]. A1 A ...A An → B (1) A1 A ...A An : B (2) A1 A ...A An : B[C1, ..., Ck] (3) 3 We use a notation in which A : B stands for A : B and A : B[C] stands for A : B, C B B 3. Implementation by Answer Set Programming 3.1. Theoretical backgrounds Answer Set Programming (ASP) is a recent paradigm covering different kinds of logic programs, and associated semantics. It allows representing and solving various problems in Artificial Intelligence. On one hand, we can cite combinatorial problems as k-coloring graph, path finding, timetabling, ... On another hand, ASP is also concerned by problems arising w</context>
</contexts>
<marker>[12]</marker>
<rawString>R. Reiter, G. Criscuolo : On Interacting Defaults. Proc. of the 7th International Joint Conference on Artificial Intelligence. pp. 270-276, Vancouver, Canada. 1981</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Syrjänen</author>
<author>I Niemelä</author>
</authors>
<title>The Smodels systems.</title>
<date>2001</date>
<booktitle>Proc. of the 6th International Conference on Logic Programming and NonMonotonic Reasoning (LPNMR&apos;01),</booktitle>
<pages>434--438</pages>
<publisher>Springer-Verlag,</publisher>
<location>Vienna,</location>
<contexts>
<context position="15860" citStr="[13]" startWordPosition="2653" endWordPosition="2653">..., -bm ] By this way : If S is an answer set of an ELP P, then Th(S) is an extension of the default theory (0,T(P)) every extension of (0,T(P)) is the deductive closure of one answer set of P. Obviously, in whole generality every default theory cannot be translated into an ELP. But as we explain it later, it is possible to encode some restricted default theories in an ELP. By this way it is possible to envisage realistic applications of default reasoning since several software packages for ASP are available today, e.g. the following ones: DLV[8] http://www.dbai.tuwien.ac.at/proj/dlv Smodels [13] http://www.tcs.hut.fi/Software/smodels Cmodels [9] http://www.cs.utexas.edu/users/tag/cmodels.html Nomore++[1] http://www.cs.unipotsdam.de/wv/nomore++ 3.2. From Default Logic to ASP Here, we explain how we have encoded our knowledge base that is originally a default theory, into an extended logic program. A very important point to note is that our original knowledge base does not contain disjunctions. Since a default theory is a pair consisting in a set of classical formulas and a set of default rules, we distinguish two major translations. classical formulas ELP one fact : a one rule with a</context>
</contexts>
<marker>[13]</marker>
<rawString>T. Syrjänen and I.Niemelä. The Smodels systems. Proc. of the 6th International Conference on Logic Programming and NonMonotonic Reasoning (LPNMR&apos;01), pp 434-438, Springer-Verlag, Vienna, Austria. 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>t a l</author>
</authors>
<title>Special issue “Compositionnalité”. Traiteent automatique des langues 39(1).</title>
<date>1998</date>
<contexts>
<context position="1335" citStr="[14]" startWordPosition="203" endWordPosition="203">ssible to create a static discourse universe and to equate the objects of this universe to the (static) meanings of words. The meaning of a sentence is then built from the meanings of the words in a compositional process and the semantic interpretation of a sentence is reduced to its logical interpretation based on the truth conditions. The very difficult task of adapting the meaning of a sentence to its context is often left to the pragmatic level, and this task requires to use a huge amount of common sense knowledge about the domain. This approach is seriously challenged (see for example [4][14]). It has been showed that the above tri-partition is very artificial because linguistic as well as extra-linguistic knowledge interact in the same global process to provide the necessary elements for understanding. Linguistic phenomena such as polysemy, plurals, metaphors and shifts in meaning create real difficulties to the referential approach of the NL semantics discussed above. As an alternative solution to these problems, [4] proposes an inferential approach to the NL semantics in which words trigger inferences depending on the context of their apparition. In the same spirit we claim tha</context>
</contexts>
<marker>[14]</marker>
<rawString>t.a.l . Special issue “Compositionnalité”. Traiteent automatique des langues 39(1). 1998.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>