<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002293">
<title confidence="0.83173">
Baselines and Bigrams: Simple, Good Sentiment and Topic Classification
</title>
<author confidence="0.996723">
Sida Wang and Christopher D. Manning
</author>
<affiliation confidence="0.988879">
Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.931353">
Stanford, CA 94305
</address>
<email confidence="0.999438">
{sidaw,manning}@stanford.edu
</email>
<sectionHeader confidence="0.99568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996867944444444">
Variants of Naive Bayes (NB) and Support
Vector Machines (SVM) are often used as
baseline methods for text classification, but
their performance varies greatly depending on
the model variant, features used and task/
dataset. We show that: (i) the inclusion of
word bigram features gives consistent gains on
sentiment analysis tasks; (ii) for short snippet
sentiment tasks, NB actually does better than
SVMs (while for longer documents the oppo-
site result holds); (iii) a simple but novel SVM
variant using NB log-count ratios as feature
values consistently performs well across tasks
and datasets. Based on these observations, we
identify simple NB and SVM variants which
outperform most published results on senti-
ment analysis datasets, sometimes providing
a new state-of-the-art performance level.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999803214285714">
Naive Bayes (NB) and Support Vector Machine
(SVM) models are often used as baselines for other
methods in text categorization and sentiment analy-
sis research. However, their performance varies sig-
nificantly depending on which variant, features and
datasets are used. We show that researchers have
not paid sufficient attention to these model selec-
tion issues. Indeed, we show that the better variants
often outperform recently published state-of-the-art
methods on many datasets. We attempt to catego-
rize which method, which variants and which fea-
tures perform better under which circumstances.
First, we make an important distinction between
sentiment classification and topical text classifica-
</bodyText>
<page confidence="0.989918">
90
</page>
<bodyText confidence="0.999981173913043">
tion. We show that the usefulness of bigram features
in bag of features sentiment classification has been
underappreciated, perhaps because their usefulness
is more of a mixed bag for topical text classifica-
tion tasks. We then distinguish between short snip-
pet sentiment tasks and longer reviews, showing that
for the former, NB outperforms SVMs. Contrary to
claims in the literature, we show that bag of features
models are still strong performers on snippet senti-
ment classification tasks, with NB models generally
outperforming the sophisticated, structure-sensitive
models explored in recent work. Furthermore, by
combining generative and discriminative classifiers,
we present a simple model variant where an SVM is
built over NB log-count ratios as feature values, and
show that it is a strong and robust performer over all
the presented tasks. Finally, we confirm the well-
known result that MNB is normally better and more
stable than multivariate Bernoulli NB, and the in-
creasingly known result that binarized MNB is bet-
ter than standard MNB. The code and datasets to
reproduce the results in this paper are publicly avail-
able. 1
</bodyText>
<sectionHeader confidence="0.999149" genericHeader="method">
2 The Methods
</sectionHeader>
<bodyText confidence="0.999996">
We formulate our main model variants as linear clas-
sifiers, where the prediction for test case k is
</bodyText>
<equation confidence="0.998846">
y(k) = sign(wTx(k) + b) (1)
</equation>
<bodyText confidence="0.97506125">
Details of the equivalent probabilistic formulations
are presented in (McCallum and Nigam, 1998).
Let f(i) E R|V  |be the feature count vector for
training case i with label y(i) E {−1,1}. V is the
</bodyText>
<footnote confidence="0.993504">
1http://www.stanford.edu/—sidaw
</footnote>
<note confidence="0.797483">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 90–94,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<listItem confidence="0.778281833333333">
set of features, and f(i)
j represents the number of oc-
currences of feature Vj in training case i. Define
the count vectors as p = α + Ei:y(i)=1 f(i) and
q = α + Ei:y(i)=_1 f(i) for smoothing parameter
α. The log-count ratio is:
</listItem>
<equation confidence="0.782599">
r = logl P/||p||11 (2)
\q
</equation>
<subsectionHeader confidence="0.995852">
2.1 Multinomial Naive Bayes (MNB)
</subsectionHeader>
<bodyText confidence="0.999023714285714">
In MNB, x(k) = f(k), w = r and b = log(N+/N_).
N+, N_ are the number of positive and negative
training cases. However, as in (Metsis et al., 2006),
we find that binarizing f(k) is better. We take x(k) =
ˆf(k) = 1If(k) &gt; 0}, where 1 is the indicator func-
tion. ˆp, ˆq,ˆr are calculated using ˆf(i) instead of f(i)
in (2).
</bodyText>
<subsectionHeader confidence="0.999541">
2.2 Support Vector Machine (SVM)
</subsectionHeader>
<bodyText confidence="0.9962635">
For the SVM, x(k) = ˆf(k), and w, b are obtained by
minimizing
</bodyText>
<equation confidence="0.915734">
wT w + C i max(0,1 − y(i)(wTˆf(i) + b))2 (3)
</equation>
<bodyText confidence="0.998466333333333">
We find this L2-regularized L2-loss SVM to work
the best and L1-loss SVM to be less stable. The LI-
BLINEAR library (Fan et al., 2008) is used here.
</bodyText>
<subsectionHeader confidence="0.985797">
2.3 SVM with NB features (NBSVM)
</subsectionHeader>
<bodyText confidence="0.999980833333333">
Otherwise identical to the SVM, except we use
x(k) = ˜f(k), where ˜f(k) = rˆ o ˆf(k) is the elemen-
twise product. While this does very well for long
documents, we find that an interpolation between
MNB and SVM performs excellently for all docu-
ments and we report results using this model:
</bodyText>
<equation confidence="0.742626">
w, = (1 − Q) 17V + Qw (4)
</equation>
<bodyText confidence="0.99974125">
where w = ||w||1/|V  |is the mean magnitude of w,
and Q E [0, 1] is the interpolation parameter. This
interpolation can be seen as a form of regularization:
trust NB unless the SVM is very confident.
</bodyText>
<sectionHeader confidence="0.989968" genericHeader="method">
3 Datasets and Task
</sectionHeader>
<bodyText confidence="0.99813825">
We compare with published results on the following
datasets. Detailed statistics are shown in table 1.
RT-s: Short movie reviews dataset containing one
sentence per review (Pang and Lee, 2005).
</bodyText>
<table confidence="0.9995263">
Dataset (N+, N_) l CV |V  |A
RT-s (5331,5331) 21 10 21K 0.8
CR (2406,1366) 20 10 5713 1.3
MPQA (3316,7308) 3 10 6299 0.8
Subj. (5000,5000) 24 10 24K 0.8
RT-2k (1000,1000) 787 10 51K 1.5
IMDB (25k,25k) 231 N 392K 0.4
AthR (799,628) 345 N 22K 2.9
XGraph (980,973) 261 N 32K 1.8
BbCrypt (992,995) 269 N 25K 0.5
</table>
<tableCaption confidence="0.99959">
Table 1: Dataset statistics. (N+, N_): number of
</tableCaption>
<bodyText confidence="0.996510869565217">
positive and negative examples. l: average num-
ber of words per example. CV: number of cross-
validation splits, or N for train/test split. |V |: the
vocabulary size. A: upper-bounds of the differences
required to be statistically significant at the p &lt; 0.05
level.
CR: Customer review dataset (Hu and Liu, 2004)
processed like in (Nakagawa et al., 2010).2
MPQA: Opinion polarity subtask of the MPQA
dataset (Wiebe et al., 2005).3
Subj: The subjectivity dataset with subjective re-
views and objective plot summaries (Pang and
Lee, 2004).
RT-2k: The standard 2000 full-length movie re-
view dataset (Pang and Lee, 2004).
IMDB: A large movie review dataset with 50k full-
length reviews (Maas et al., 2011).4
AthR, XGraph, BbCrypt: Classify pairs of
newsgroups in the 20-newsgroups dataset with
all headers stripped off (the third (18828) ver-
sion5), namely: alt.atheism vs. religion.misc,
comp.windows.x vs. comp.graphics, and
rec.sport.baseball vs. sci.crypt, respectively.
</bodyText>
<sectionHeader confidence="0.995244" genericHeader="conclusions">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.973613">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999901333333333">
We use the provided tokenizations when they exist.
If not, we split at spaces for unigrams, and we filter
out anything that is not [A-Za-z] for bigrams. We do
</bodyText>
<footnote confidence="0.9999365">
2http://www.cs.uic.edu/—liub/FBS/sentiment-analysis.html
3http://www.cs.pitt.edu/mpqa/
4http://ai.stanford.edu/—amaas/data/sentiment
5http://people.csail.mit.edu/jrennie/20Newsgroups
</footnote>
<page confidence="0.997824">
91
</page>
<bodyText confidence="0.999789090909091">
not use stopwords, lexicons or other resources. All
results reported use α = 1, C = 1, Q = 0.25 for
NBSVM, and C = 0.1 for SVM.
For comparison with other published results, we
use either 10-fold cross-validation or train/test split
depending on what is standard for the dataset. The
CV column of table 1 specifies what is used. The
standard splits are used when they are available.
The approximate upper-bounds on the difference re-
quired to be statistically significant at the p &lt; 0.05
level are listed in table 1, column A.
</bodyText>
<subsectionHeader confidence="0.96388">
4.2 MNB is better at snippets
</subsectionHeader>
<bodyText confidence="0.999974935483871">
(Moilanen and Pulman, 2007) suggests that while
“statistical methods” work well for datasets with
hundreds of words in each example, they cannot
handle snippets datasets and some rule-based sys-
tem is necessary. Supporting this claim are examples
such as not an inhumane monster6, or killing cancer
that express an overall positive sentiment with nega-
tive words.
Some previous work on classifying snippets in-
clude using pre-defined polarity reversing rules
(Moilanen and Pulman, 2007), and learning com-
plex models on parse trees such as in (Nakagawa et
al., 2010) and (Socher et al., 2011). These works
seem promising as they perform better than many
sophisticated, rule-based methods used as baselines
in (Nakagawa et al., 2010). However, we find that
several NB/SVM variants in fact do better than these
state-of-the-art methods, even compared to meth-
ods that use lexicons, reversal rules, or unsupervised
pretraining. The results are in table 2.
Our SVM-uni results are consistent with BoF-
noDic and BoF-w/Rev used in (Nakagawa et al.,
2010) and BoWSVM in (Pang and Lee, 2004).
(Nakagawa et al., 2010) used a SVM with second-
order polynomial kernel and additional features.
With the only exception being MPQA, MNB per-
formed better than SVM in all cases.7
Table 2 show that a linear SVM is a weak baseline
for snippets. MNB (and NBSVM) are much better
on sentiment snippet tasks, and usually better than
other published results. Thus, we find the hypothe-
</bodyText>
<footnote confidence="0.99311325">
6A positive example from the RT-s dataset.
7We are unsure, but feel that MPQA may be less discrimi-
native, since the documents are extremely short and all methods
perform similarly.
</footnote>
<table confidence="0.999812066666667">
Method RT-s MPQA CR Subj.
MNB-uni 77.9 85.3 79.8 92.6
MNB-bi 79.0 86.3 80.0 93.6
SVM-uni 76.2 86.1 79.0 90.8
SVM-bi 77.7 86.7 80.8 91.7
NBSVM-uni 78.1 85.3 80.5 92.4
NBSVM-bi 79.4 86.3 81.8 93.2
RAE 76.8 85.7 – –
RAE-pretrain 77.7 86.4 – –
Voting-w/Rev. 63.1 81.7 74.2 –
Rule 62.9 81.8 74.3 –
BoF-noDic. 75.7 81.8 79.3 –
BoF-w/Rev. 76.4 84.1 81.4 –
Tree-CRF 77.3 86.1 81.4 –
BoWSVM – – – 90.0
</table>
<tableCaption confidence="0.976078">
Table 2: Results for snippets datasets. Tree-CRF:
</tableCaption>
<bodyText confidence="0.944178888888889">
(Nakagawa et al., 2010) RAE: Recursive Autoen-
coders (Socher et al., 2011). RAE-pretrain: train on
Wikipedia (Collobert and Weston, 2008). “Voting”
and “Rule”: use a sentiment lexicon and hard-coded
reversal rules. “w/Rev”: “the polarities of phrases
which have odd numbers of reversal phrases in their
ancestors”. The top 3 methods are in bold and the
best is also underlined.
sis that rule-based systems have an edge for snippet
datasets to be false. MNB is stronger for snippets
than for longer documents. While (Ng and Jordan,
2002) showed that NB is better than SVM/logistic
regression (LR) with few training cases, we show
that MNB is also better with short documents. In
contrast to their result that an SVM usually beats
NB when it has more than 30–50 training cases, we
show that MNB is still better on snippets even with
relatively large training sets (9k cases).
</bodyText>
<subsectionHeader confidence="0.976999">
4.3 SVM is better at full-length reviews
</subsectionHeader>
<bodyText confidence="0.9999555">
As seen in table 1, the RT-2k and IMDB datasets
contain much longer reviews. Compared to the ex-
cellent performance of MNB on snippet datasets,
the many poor assumptions of MNB pointed out
in (Rennie et al., 2003) become more crippling for
these longer documents. SVM is much stronger
than MNB for the 2 full-length sentiment analy-
sis tasks, but still worse than some other published
results. However, NBSVM either exceeds or ap-
proaches previous state-of-the art methods, even the
</bodyText>
<page confidence="0.987102">
92
</page>
<table confidence="0.999939111111111">
Our results RT-2k IMDB Subj.
MNB-uni 83.45 83.55 92.58
MNB-bi 85.85 86.59 93.56
SVM-uni 86.25 86.95 90.84
SVM-bi 87.40 89.16 91.74
NBSVM-uni 87.80 88.29 92.40
NBSVM-bi 89.45 91.22 93.18
BoW (bnc) 85.45 87.8 87.77
BoW (bAt&apos;c) 85.8 88.23 85.65
LDA 66.7 67.42 66.65
Full+BoW 87.85 88.33 88.45
Full+Unlab’d+BoW 88.9 88.89 88.13
BoWSVM 87.15 – 90.00
Valence Shifter 86.2 – –
tf.Aidf 88.1 – –
Appr. Taxonomy 90.20 – –
WRRBM – 87.42 –
WRRBM + BoW(bnc) – 89.23 –
</table>
<tableCaption confidence="0.986601">
Table 3: Results for long reviews (RT-2k and
</tableCaption>
<figureCaption confidence="0.943706916666667">
IMDB). The snippet dataset Subj. is also included
for comparison. Results in rows 7-11 are from
(Maas et al., 2011). BoW: linear SVM on bag of
words features. bnc: binary, no idf, cosine nor-
malization. At&apos;: smoothed delta idf. Full: the
full model. Unlab’d: additional unlabeled data.
BoWSVM: bag of words SVM used in (Pang and
Lee, 2004). Valence Shifter: (Kennedy and Inkpen,
2006). tf.Aidf: (Martineau and Finin, 2009). Ap-
praisal Taxonomy: (Whitelaw et al., 2005). WR-
RBM: Word Representation Restricted Boltzmann
Machine (Dahl et al., 2012).
</figureCaption>
<bodyText confidence="0.991388">
ones that use additional data. These sentiment anal-
ysis results are shown in table 3.
</bodyText>
<subsectionHeader confidence="0.999878">
4.4 Benefits of bigrams depends on the task
</subsectionHeader>
<bodyText confidence="0.954654727272727">
Word bigram features are not that commonly used
in text classification tasks (hence, the usual term,
“bag of words”), probably due to their having mixed
and overall limited utility in topical text classifica-
tion tasks, as seen in table 4. This likely reflects that
certain topic keywords are indicative alone. How-
ever, in both tables 2 and 3, adding bigrams always
improved the performance, and often gives better
results than previously published.8 This presum-
ably reflects that in sentiment classification there are
8However, adding trigrams hurts slightly.
</bodyText>
<table confidence="0.999796111111111">
Method AthR XGraph BbCrypt
MNB-uni 85.0 90.0 99.3
MNB-bi 85.1 +0.1 91.2 +1.2 99.4 +0.1
SVM-uni 82.6 85.1 98.3
SVM-bi 83.7 +1.1 86.2 +0.9 97.7 −0.5
NBSVM-uni 87.9 91.2 99.7
NBSVM-bi 87.7 −0.2 90.7 −0.5 99.5 −0.2
ActiveSVM 90 99
DiscLDA 83 – –
</table>
<tableCaption confidence="0.732050666666667">
Table 4: On 3 20-newsgroup subtasks, we compare
to DiscLDA (Lacoste-Julien et al., 2008) and Ac-
tiveSVM (Schohn and Cohn, 2000).
</tableCaption>
<bodyText confidence="0.985843">
much bigger gains from bigrams, because they can
capture modified verbs and nouns.
</bodyText>
<subsectionHeader confidence="0.640045">
4.5 NBSVM is a robust performer
</subsectionHeader>
<bodyText confidence="0.99996625">
NBSVM performs well on snippets and longer doc-
uments, for sentiment, topic and subjectivity clas-
sification, and is often better than previously pub-
lished results. Therefore, NBSVM seems to be an
appropriate and very strong baseline for sophisti-
cated methods aiming to beat a bag of features.
One disadvantage of NBSVM is having the inter-
polation parameter β. The performance on longer
documents is virtually identical (within 0.1%) for
β E [1/a, 1], while β = 1/a is on average 0.5% better
for snippets than β = 1. Using β E [1/a, 1/2] makes
the NBSVM more robust than more extreme values.
</bodyText>
<subsectionHeader confidence="0.971435">
4.6 Other results
</subsectionHeader>
<bodyText confidence="0.999518428571428">
Multivariate Bernoulli NB (BNB) usually performs
worse than MNB. The only place where BNB is
comparable to MNB is for snippet tasks using only
unigrams. In general, BNB is less stable than MNB
and performs up to 10% worse. Therefore, bench-
marking against BNB is untrustworthy, cf. (McCal-
lum and Nigam, 1998).
For MNB and NBSVM, using the binarized MNB
f� is slightly better (by 1%) than using the raw count
feature f. The difference is negligible for snippets.
Using logistic regression in place of SVM gives
similar results, and some of our results can be
viewed more generally in terms of generative vs.
discriminative learning.
</bodyText>
<page confidence="0.998084">
93
</page>
<sectionHeader confidence="0.989674" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99784079710145">
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In Proceedings of ICML.
George E. Dahl, Ryan P. Adams, and Hugo Larochelle.
2012. Training restricted boltzmann machines on
word observations. arXiv:1202.5695v1 [cs.LG].
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874, June.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings ACM SIGKDD,
pages 168–177.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative learning for dimen-
sionality reduction and classification. In Proceedings
of NIPS, pages 897–904.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of ACL.
Justin Martineau and Tim Finin. 2009. Delta tfidf: An
improved feature space for sentiment analysis. In Pro-
ceedings of ICWSM.
Andrew McCallum and Kamal Nigam. 1998. A compar-
ison of event models for naive bayes text classification.
In AAAI-98 Workshop, pages 41–48.
Vangelis Metsis, Ion Androutsopoulos, and Georgios
Paliouras. 2006. Spam filtering with naive bayes -
which naive bayes? In Proceedings of CEAS.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of RANLP, pages 378–
382, September 27-29.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using CRFs with hidden variables. In Proceedings of
ACL:HLT.
Andrew Y Ng and Michael I Jordan. 2002. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive bayes. In Proceedings of
NIPS, volume 2, pages 841–848.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of ACL.
Jason D. Rennie, Lawrence Shih, Jaime Teevan, and
David R. Karger. 2003. Tackling the poor assump-
tions of naive bayes text classifiers. In Proceedings of
ICML, pages 616–623.
Greg Schohn and David Cohn. 2000. Less is more: Ac-
tive learning with support vector machines. In Pro-
ceedings of ICML, pages 839–846.
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
EMNLP.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal taxonomies for sentiment anal-
ysis. In Proceedings of CIKM-05.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165–210.
</reference>
<page confidence="0.999551">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.494247">
<title confidence="0.99995">Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</title>
<author confidence="0.999647">D Wang</author>
<affiliation confidence="0.761381">Department of Computer Stanford</affiliation>
<address confidence="0.891296">Stanford, CA</address>
<abstract confidence="0.999692526315789">Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/ dataset. We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets. Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="9569" citStr="Collobert and Weston, 2008" startWordPosition="1557" endWordPosition="1560">remely short and all methods perform similarly. Method RT-s MPQA CR Subj. MNB-uni 77.9 85.3 79.8 92.6 MNB-bi 79.0 86.3 80.0 93.6 SVM-uni 76.2 86.1 79.0 90.8 SVM-bi 77.7 86.7 80.8 91.7 NBSVM-uni 78.1 85.3 80.5 92.4 NBSVM-bi 79.4 86.3 81.8 93.2 RAE 76.8 85.7 – – RAE-pretrain 77.7 86.4 – – Voting-w/Rev. 63.1 81.7 74.2 – Rule 62.9 81.8 74.3 – BoF-noDic. 75.7 81.8 79.3 – BoF-w/Rev. 76.4 84.1 81.4 – Tree-CRF 77.3 86.1 81.4 – BoWSVM – – – 90.0 Table 2: Results for snippets datasets. Tree-CRF: (Nakagawa et al., 2010) RAE: Recursive Autoencoders (Socher et al., 2011). RAE-pretrain: train on Wikipedia (Collobert and Weston, 2008). “Voting” and “Rule”: use a sentiment lexicon and hard-coded reversal rules. “w/Rev”: “the polarities of phrases which have odd numbers of reversal phrases in their ancestors”. The top 3 methods are in bold and the best is also underlined. sis that rule-based systems have an edge for snippet datasets to be false. MNB is stronger for snippets than for longer documents. While (Ng and Jordan, 2002) showed that NB is better than SVM/logistic regression (LR) with few training cases, we show that MNB is also better with short documents. In contrast to their result that an SVM usually beats NB when </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Ryan P Adams</author>
<author>Hugo Larochelle</author>
</authors>
<title>Training restricted boltzmann machines on word observations.</title>
<date>2012</date>
<note>arXiv:1202.5695v1 [cs.LG].</note>
<contexts>
<context position="11873" citStr="Dahl et al., 2012" startWordPosition="1944" endWordPosition="1947"> – WRRBM + BoW(bnc) – 89.23 – Table 3: Results for long reviews (RT-2k and IMDB). The snippet dataset Subj. is also included for comparison. Results in rows 7-11 are from (Maas et al., 2011). BoW: linear SVM on bag of words features. bnc: binary, no idf, cosine normalization. At&apos;: smoothed delta idf. Full: the full model. Unlab’d: additional unlabeled data. BoWSVM: bag of words SVM used in (Pang and Lee, 2004). Valence Shifter: (Kennedy and Inkpen, 2006). tf.Aidf: (Martineau and Finin, 2009). Appraisal Taxonomy: (Whitelaw et al., 2005). WRRBM: Word Representation Restricted Boltzmann Machine (Dahl et al., 2012). ones that use additional data. These sentiment analysis results are shown in table 3. 4.4 Benefits of bigrams depends on the task Word bigram features are not that commonly used in text classification tasks (hence, the usual term, “bag of words”), probably due to their having mixed and overall limited utility in topical text classification tasks, as seen in table 4. This likely reflects that certain topic keywords are indicative alone. However, in both tables 2 and 3, adding bigrams always improved the performance, and often gives better results than previously published.8 This presumably re</context>
</contexts>
<marker>Dahl, Adams, Larochelle, 2012</marker>
<rawString>George E. Dahl, Ryan P. Adams, and Hugo Larochelle. 2012. Training restricted boltzmann machines on word observations. arXiv:1202.5695v1 [cs.LG].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="4311" citStr="Fan et al., 2008" startWordPosition="690" endWordPosition="693">omial Naive Bayes (MNB) In MNB, x(k) = f(k), w = r and b = log(N+/N_). N+, N_ are the number of positive and negative training cases. However, as in (Metsis et al., 2006), we find that binarizing f(k) is better. We take x(k) = ˆf(k) = 1If(k) &gt; 0}, where 1 is the indicator function. ˆp, ˆq,ˆr are calculated using ˆf(i) instead of f(i) in (2). 2.2 Support Vector Machine (SVM) For the SVM, x(k) = ˆf(k), and w, b are obtained by minimizing wT w + C i max(0,1 − y(i)(wTˆf(i) + b))2 (3) We find this L2-regularized L2-loss SVM to work the best and L1-loss SVM to be less stable. The LIBLINEAR library (Fan et al., 2008) is used here. 2.3 SVM with NB features (NBSVM) Otherwise identical to the SVM, except we use x(k) = ˜f(k), where ˜f(k) = rˆ o ˆf(k) is the elementwise product. While this does very well for long documents, we find that an interpolation between MNB and SVM performs excellently for all documents and we report results using this model: w, = (1 − Q) 17V + Qw (4) where w = ||w||1/|V |is the mean magnitude of w, and Q E [0, 1] is the interpolation parameter. This interpolation can be seen as a form of regularization: trust NB unless the SVM is very confident. 3 Datasets and Task We compare with pub</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings ACM SIGKDD,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="5751" citStr="Hu and Liu, 2004" startWordPosition="952" endWordPosition="955">10 21K 0.8 CR (2406,1366) 20 10 5713 1.3 MPQA (3316,7308) 3 10 6299 0.8 Subj. (5000,5000) 24 10 24K 0.8 RT-2k (1000,1000) 787 10 51K 1.5 IMDB (25k,25k) 231 N 392K 0.4 AthR (799,628) 345 N 22K 2.9 XGraph (980,973) 261 N 32K 1.8 BbCrypt (992,995) 269 N 25K 0.5 Table 1: Dataset statistics. (N+, N_): number of positive and negative examples. l: average number of words per example. CV: number of crossvalidation splits, or N for train/test split. |V |: the vocabulary size. A: upper-bounds of the differences required to be statistically significant at the p &lt; 0.05 level. CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2 MPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005).3 Subj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004). RT-2k: The standard 2000 full-length movie review dataset (Pang and Lee, 2004). IMDB: A large movie review dataset with 50k fulllength reviews (Maas et al., 2011).4 AthR, XGraph, BbCrypt: Classify pairs of newsgroups in the 20-newsgroups dataset with all headers stripped off (the third (18828) version5), namely: alt.atheism vs. religion.misc, comp.windows.x vs. comp.graphi</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings ACM SIGKDD, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment classification of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<contexts>
<context position="11713" citStr="Kennedy and Inkpen, 2006" startWordPosition="1921" endWordPosition="1924">6.65 Full+BoW 87.85 88.33 88.45 Full+Unlab’d+BoW 88.9 88.89 88.13 BoWSVM 87.15 – 90.00 Valence Shifter 86.2 – – tf.Aidf 88.1 – – Appr. Taxonomy 90.20 – – WRRBM – 87.42 – WRRBM + BoW(bnc) – 89.23 – Table 3: Results for long reviews (RT-2k and IMDB). The snippet dataset Subj. is also included for comparison. Results in rows 7-11 are from (Maas et al., 2011). BoW: linear SVM on bag of words features. bnc: binary, no idf, cosine normalization. At&apos;: smoothed delta idf. Full: the full model. Unlab’d: additional unlabeled data. BoWSVM: bag of words SVM used in (Pang and Lee, 2004). Valence Shifter: (Kennedy and Inkpen, 2006). tf.Aidf: (Martineau and Finin, 2009). Appraisal Taxonomy: (Whitelaw et al., 2005). WRRBM: Word Representation Restricted Boltzmann Machine (Dahl et al., 2012). ones that use additional data. These sentiment analysis results are shown in table 3. 4.4 Benefits of bigrams depends on the task Word bigram features are not that commonly used in text classification tasks (hence, the usual term, “bag of words”), probably due to their having mixed and overall limited utility in topical text classification tasks, as seen in table 4. This likely reflects that certain topic keywords are indicative alone</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Alistair Kennedy and Diana Inkpen. 2006. Sentiment classification of movie reviews using contextual valence shifters. Computational Intelligence, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Fei Sha</author>
<author>Michael I Jordan</author>
</authors>
<title>DiscLDA: Discriminative learning for dimensionality reduction and classification.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>897--904</pages>
<contexts>
<context position="12895" citStr="Lacoste-Julien et al., 2008" startWordPosition="2112" endWordPosition="2115">at certain topic keywords are indicative alone. However, in both tables 2 and 3, adding bigrams always improved the performance, and often gives better results than previously published.8 This presumably reflects that in sentiment classification there are 8However, adding trigrams hurts slightly. Method AthR XGraph BbCrypt MNB-uni 85.0 90.0 99.3 MNB-bi 85.1 +0.1 91.2 +1.2 99.4 +0.1 SVM-uni 82.6 85.1 98.3 SVM-bi 83.7 +1.1 86.2 +0.9 97.7 −0.5 NBSVM-uni 87.9 91.2 99.7 NBSVM-bi 87.7 −0.2 90.7 −0.5 99.5 −0.2 ActiveSVM 90 99 DiscLDA 83 – – Table 4: On 3 20-newsgroup subtasks, we compare to DiscLDA (Lacoste-Julien et al., 2008) and ActiveSVM (Schohn and Cohn, 2000). much bigger gains from bigrams, because they can capture modified verbs and nouns. 4.5 NBSVM is a robust performer NBSVM performs well on snippets and longer documents, for sentiment, topic and subjectivity classification, and is often better than previously published results. Therefore, NBSVM seems to be an appropriate and very strong baseline for sophisticated methods aiming to beat a bag of features. One disadvantage of NBSVM is having the interpolation parameter β. The performance on longer documents is virtually identical (within 0.1%) for β E [1/a,</context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008. DiscLDA: Discriminative learning for dimensionality reduction and classification. In Proceedings of NIPS, pages 897–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6138" citStr="Maas et al., 2011" startWordPosition="1015" endWordPosition="1018">mber of crossvalidation splits, or N for train/test split. |V |: the vocabulary size. A: upper-bounds of the differences required to be statistically significant at the p &lt; 0.05 level. CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2 MPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005).3 Subj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004). RT-2k: The standard 2000 full-length movie review dataset (Pang and Lee, 2004). IMDB: A large movie review dataset with 50k fulllength reviews (Maas et al., 2011).4 AthR, XGraph, BbCrypt: Classify pairs of newsgroups in the 20-newsgroups dataset with all headers stripped off (the third (18828) version5), namely: alt.atheism vs. religion.misc, comp.windows.x vs. comp.graphics, and rec.sport.baseball vs. sci.crypt, respectively. 4 Experiments and Results 4.1 Experimental setup We use the provided tokenizations when they exist. If not, we split at spaces for unigrams, and we filter out anything that is not [A-Za-z] for bigrams. We do 2http://www.cs.uic.edu/—liub/FBS/sentiment-analysis.html 3http://www.cs.pitt.edu/mpqa/ 4http://ai.stanford.edu/—amaas/data/</context>
<context position="11445" citStr="Maas et al., 2011" startWordPosition="1877" endWordPosition="1880">92 Our results RT-2k IMDB Subj. MNB-uni 83.45 83.55 92.58 MNB-bi 85.85 86.59 93.56 SVM-uni 86.25 86.95 90.84 SVM-bi 87.40 89.16 91.74 NBSVM-uni 87.80 88.29 92.40 NBSVM-bi 89.45 91.22 93.18 BoW (bnc) 85.45 87.8 87.77 BoW (bAt&apos;c) 85.8 88.23 85.65 LDA 66.7 67.42 66.65 Full+BoW 87.85 88.33 88.45 Full+Unlab’d+BoW 88.9 88.89 88.13 BoWSVM 87.15 – 90.00 Valence Shifter 86.2 – – tf.Aidf 88.1 – – Appr. Taxonomy 90.20 – – WRRBM – 87.42 – WRRBM + BoW(bnc) – 89.23 – Table 3: Results for long reviews (RT-2k and IMDB). The snippet dataset Subj. is also included for comparison. Results in rows 7-11 are from (Maas et al., 2011). BoW: linear SVM on bag of words features. bnc: binary, no idf, cosine normalization. At&apos;: smoothed delta idf. Full: the full model. Unlab’d: additional unlabeled data. BoWSVM: bag of words SVM used in (Pang and Lee, 2004). Valence Shifter: (Kennedy and Inkpen, 2006). tf.Aidf: (Martineau and Finin, 2009). Appraisal Taxonomy: (Whitelaw et al., 2005). WRRBM: Word Representation Restricted Boltzmann Machine (Dahl et al., 2012). ones that use additional data. These sentiment analysis results are shown in table 3. 4.4 Benefits of bigrams depends on the task Word bigram features are not that common</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Martineau</author>
<author>Tim Finin</author>
</authors>
<title>Delta tfidf: An improved feature space for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="11751" citStr="Martineau and Finin, 2009" startWordPosition="1926" endWordPosition="1929">Unlab’d+BoW 88.9 88.89 88.13 BoWSVM 87.15 – 90.00 Valence Shifter 86.2 – – tf.Aidf 88.1 – – Appr. Taxonomy 90.20 – – WRRBM – 87.42 – WRRBM + BoW(bnc) – 89.23 – Table 3: Results for long reviews (RT-2k and IMDB). The snippet dataset Subj. is also included for comparison. Results in rows 7-11 are from (Maas et al., 2011). BoW: linear SVM on bag of words features. bnc: binary, no idf, cosine normalization. At&apos;: smoothed delta idf. Full: the full model. Unlab’d: additional unlabeled data. BoWSVM: bag of words SVM used in (Pang and Lee, 2004). Valence Shifter: (Kennedy and Inkpen, 2006). tf.Aidf: (Martineau and Finin, 2009). Appraisal Taxonomy: (Whitelaw et al., 2005). WRRBM: Word Representation Restricted Boltzmann Machine (Dahl et al., 2012). ones that use additional data. These sentiment analysis results are shown in table 3. 4.4 Benefits of bigrams depends on the task Word bigram features are not that commonly used in text classification tasks (hence, the usual term, “bag of words”), probably due to their having mixed and overall limited utility in topical text classification tasks, as seen in table 4. This likely reflects that certain topic keywords are indicative alone. However, in both tables 2 and 3, add</context>
</contexts>
<marker>Martineau, Finin, 2009</marker>
<rawString>Justin Martineau and Tim Finin. 2009. Delta tfidf: An improved feature space for sentiment analysis. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classification.</title>
<date>1998</date>
<booktitle>In AAAI-98 Workshop,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="3104" citStr="McCallum and Nigam, 1998" startWordPosition="468" endWordPosition="471">s as feature values, and show that it is a strong and robust performer over all the presented tasks. Finally, we confirm the wellknown result that MNB is normally better and more stable than multivariate Bernoulli NB, and the increasingly known result that binarized MNB is better than standard MNB. The code and datasets to reproduce the results in this paper are publicly available. 1 2 The Methods We formulate our main model variants as linear classifiers, where the prediction for test case k is y(k) = sign(wTx(k) + b) (1) Details of the equivalent probabilistic formulations are presented in (McCallum and Nigam, 1998). Let f(i) E R|V |be the feature count vector for training case i with label y(i) E {−1,1}. V is the 1http://www.stanford.edu/—sidaw Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 90–94, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics set of features, and f(i) j represents the number of occurrences of feature Vj in training case i. Define the count vectors as p = α + Ei:y(i)=1 f(i) and q = α + Ei:y(i)=_1 f(i) for smoothing parameter α. The log-count ratio is: r = logl P/||p||11 (2) \q 2.1 Multinomial Naiv</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for naive bayes text classification. In AAAI-98 Workshop, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vangelis Metsis</author>
</authors>
<title>Ion Androutsopoulos, and Georgios Paliouras.</title>
<date>2006</date>
<booktitle>In Proceedings of CEAS.</booktitle>
<marker>Metsis, 2006</marker>
<rawString>Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras. 2006. Spam filtering with naive bayes -which naive bayes? In Proceedings of CEAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment composition.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>378--382</pages>
<contexts>
<context position="7383" citStr="Moilanen and Pulman, 2007" startWordPosition="1193" endWordPosition="1196">eople.csail.mit.edu/jrennie/20Newsgroups 91 not use stopwords, lexicons or other resources. All results reported use α = 1, C = 1, Q = 0.25 for NBSVM, and C = 0.1 for SVM. For comparison with other published results, we use either 10-fold cross-validation or train/test split depending on what is standard for the dataset. The CV column of table 1 specifies what is used. The standard splits are used when they are available. The approximate upper-bounds on the difference required to be statistically significant at the p &lt; 0.05 level are listed in table 1, column A. 4.2 MNB is better at snippets (Moilanen and Pulman, 2007) suggests that while “statistical methods” work well for datasets with hundreds of words in each example, they cannot handle snippets datasets and some rule-based system is necessary. Supporting this claim are examples such as not an inhumane monster6, or killing cancer that express an overall positive sentiment with negative words. Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al., 2011). These works seem promising as they pe</context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Karo Moilanen and Stephen Pulman. 2007. Sentiment composition. In Proceedings of RANLP, pages 378– 382, September 27-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFs with hidden variables.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL:HLT.</booktitle>
<contexts>
<context position="5793" citStr="Nakagawa et al., 2010" startWordPosition="959" endWordPosition="962">1.3 MPQA (3316,7308) 3 10 6299 0.8 Subj. (5000,5000) 24 10 24K 0.8 RT-2k (1000,1000) 787 10 51K 1.5 IMDB (25k,25k) 231 N 392K 0.4 AthR (799,628) 345 N 22K 2.9 XGraph (980,973) 261 N 32K 1.8 BbCrypt (992,995) 269 N 25K 0.5 Table 1: Dataset statistics. (N+, N_): number of positive and negative examples. l: average number of words per example. CV: number of crossvalidation splits, or N for train/test split. |V |: the vocabulary size. A: upper-bounds of the differences required to be statistically significant at the p &lt; 0.05 level. CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2 MPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005).3 Subj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004). RT-2k: The standard 2000 full-length movie review dataset (Pang and Lee, 2004). IMDB: A large movie review dataset with 50k fulllength reviews (Maas et al., 2011).4 AthR, XGraph, BbCrypt: Classify pairs of newsgroups in the 20-newsgroups dataset with all headers stripped off (the third (18828) version5), namely: alt.atheism vs. religion.misc, comp.windows.x vs. comp.graphics, and rec.sport.baseball vs. sci.crypt, </context>
<context position="7918" citStr="Nakagawa et al., 2010" startWordPosition="1277" endWordPosition="1280"> listed in table 1, column A. 4.2 MNB is better at snippets (Moilanen and Pulman, 2007) suggests that while “statistical methods” work well for datasets with hundreds of words in each example, they cannot handle snippets datasets and some rule-based system is necessary. Supporting this claim are examples such as not an inhumane monster6, or killing cancer that express an overall positive sentiment with negative words. Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al., 2011). These works seem promising as they perform better than many sophisticated, rule-based methods used as baselines in (Nakagawa et al., 2010). However, we find that several NB/SVM variants in fact do better than these state-of-the-art methods, even compared to methods that use lexicons, reversal rules, or unsupervised pretraining. The results are in table 2. Our SVM-uni results are consistent with BoFnoDic and BoF-w/Rev used in (Nakagawa et al., 2010) and BoWSVM in (Pang and Lee, 2004). (Nakagawa et al., 2010) used a SVM with secondorder polynomial kernel and additiona</context>
<context position="9456" citStr="Nakagawa et al., 2010" startWordPosition="1541" endWordPosition="1544">the RT-s dataset. 7We are unsure, but feel that MPQA may be less discriminative, since the documents are extremely short and all methods perform similarly. Method RT-s MPQA CR Subj. MNB-uni 77.9 85.3 79.8 92.6 MNB-bi 79.0 86.3 80.0 93.6 SVM-uni 76.2 86.1 79.0 90.8 SVM-bi 77.7 86.7 80.8 91.7 NBSVM-uni 78.1 85.3 80.5 92.4 NBSVM-bi 79.4 86.3 81.8 93.2 RAE 76.8 85.7 – – RAE-pretrain 77.7 86.4 – – Voting-w/Rev. 63.1 81.7 74.2 – Rule 62.9 81.8 74.3 – BoF-noDic. 75.7 81.8 79.3 – BoF-w/Rev. 76.4 84.1 81.4 – Tree-CRF 77.3 86.1 81.4 – BoWSVM – – – 90.0 Table 2: Results for snippets datasets. Tree-CRF: (Nakagawa et al., 2010) RAE: Recursive Autoencoders (Socher et al., 2011). RAE-pretrain: train on Wikipedia (Collobert and Weston, 2008). “Voting” and “Rule”: use a sentiment lexicon and hard-coded reversal rules. “w/Rev”: “the polarities of phrases which have odd numbers of reversal phrases in their ancestors”. The top 3 methods are in bold and the best is also underlined. sis that rule-based systems have an edge for snippet datasets to be false. MNB is stronger for snippets than for longer documents. While (Ng and Jordan, 2002) showed that NB is better than SVM/logistic regression (LR) with few training cases, we </context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In Proceedings of ACL:HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes.</title>
<date>2002</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<volume>2</volume>
<pages>841--848</pages>
<contexts>
<context position="9968" citStr="Ng and Jordan, 2002" startWordPosition="1623" endWordPosition="1626">77.3 86.1 81.4 – BoWSVM – – – 90.0 Table 2: Results for snippets datasets. Tree-CRF: (Nakagawa et al., 2010) RAE: Recursive Autoencoders (Socher et al., 2011). RAE-pretrain: train on Wikipedia (Collobert and Weston, 2008). “Voting” and “Rule”: use a sentiment lexicon and hard-coded reversal rules. “w/Rev”: “the polarities of phrases which have odd numbers of reversal phrases in their ancestors”. The top 3 methods are in bold and the best is also underlined. sis that rule-based systems have an edge for snippet datasets to be false. MNB is stronger for snippets than for longer documents. While (Ng and Jordan, 2002) showed that NB is better than SVM/logistic regression (LR) with few training cases, we show that MNB is also better with short documents. In contrast to their result that an SVM usually beats NB when it has more than 30–50 training cases, we show that MNB is still better on snippets even with relatively large training sets (9k cases). 4.3 SVM is better at full-length reviews As seen in table 1, the RT-2k and IMDB datasets contain much longer reviews. Compared to the excellent performance of MNB on snippet datasets, the many poor assumptions of MNB pointed out in (Rennie et al., 2003) become m</context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>Andrew Y Ng and Michael I Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Proceedings of NIPS, volume 2, pages 841–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5974" citStr="Pang and Lee, 2004" startWordPosition="987" endWordPosition="990">K 1.8 BbCrypt (992,995) 269 N 25K 0.5 Table 1: Dataset statistics. (N+, N_): number of positive and negative examples. l: average number of words per example. CV: number of crossvalidation splits, or N for train/test split. |V |: the vocabulary size. A: upper-bounds of the differences required to be statistically significant at the p &lt; 0.05 level. CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2 MPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005).3 Subj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004). RT-2k: The standard 2000 full-length movie review dataset (Pang and Lee, 2004). IMDB: A large movie review dataset with 50k fulllength reviews (Maas et al., 2011).4 AthR, XGraph, BbCrypt: Classify pairs of newsgroups in the 20-newsgroups dataset with all headers stripped off (the third (18828) version5), namely: alt.atheism vs. religion.misc, comp.windows.x vs. comp.graphics, and rec.sport.baseball vs. sci.crypt, respectively. 4 Experiments and Results 4.1 Experimental setup We use the provided tokenizations when they exist. If not, we split at spaces for unigrams, and we filter out anything</context>
<context position="8433" citStr="Pang and Lee, 2004" startWordPosition="1360" endWordPosition="1363">oilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al., 2011). These works seem promising as they perform better than many sophisticated, rule-based methods used as baselines in (Nakagawa et al., 2010). However, we find that several NB/SVM variants in fact do better than these state-of-the-art methods, even compared to methods that use lexicons, reversal rules, or unsupervised pretraining. The results are in table 2. Our SVM-uni results are consistent with BoFnoDic and BoF-w/Rev used in (Nakagawa et al., 2010) and BoWSVM in (Pang and Lee, 2004). (Nakagawa et al., 2010) used a SVM with secondorder polynomial kernel and additional features. With the only exception being MPQA, MNB performed better than SVM in all cases.7 Table 2 show that a linear SVM is a weak baseline for snippets. MNB (and NBSVM) are much better on sentiment snippet tasks, and usually better than other published results. Thus, we find the hypothe6A positive example from the RT-s dataset. 7We are unsure, but feel that MPQA may be less discriminative, since the documents are extremely short and all methods perform similarly. Method RT-s MPQA CR Subj. MNB-uni 77.9 85.3</context>
<context position="11668" citStr="Pang and Lee, 2004" startWordPosition="1915" endWordPosition="1918">At&apos;c) 85.8 88.23 85.65 LDA 66.7 67.42 66.65 Full+BoW 87.85 88.33 88.45 Full+Unlab’d+BoW 88.9 88.89 88.13 BoWSVM 87.15 – 90.00 Valence Shifter 86.2 – – tf.Aidf 88.1 – – Appr. Taxonomy 90.20 – – WRRBM – 87.42 – WRRBM + BoW(bnc) – 89.23 – Table 3: Results for long reviews (RT-2k and IMDB). The snippet dataset Subj. is also included for comparison. Results in rows 7-11 are from (Maas et al., 2011). BoW: linear SVM on bag of words features. bnc: binary, no idf, cosine normalization. At&apos;: smoothed delta idf. Full: the full model. Unlab’d: additional unlabeled data. BoWSVM: bag of words SVM used in (Pang and Lee, 2004). Valence Shifter: (Kennedy and Inkpen, 2006). tf.Aidf: (Martineau and Finin, 2009). Appraisal Taxonomy: (Whitelaw et al., 2005). WRRBM: Word Representation Restricted Boltzmann Machine (Dahl et al., 2012). ones that use additional data. These sentiment analysis results are shown in table 3. 4.4 Benefits of bigrams depends on the task Word bigram features are not that commonly used in text classification tasks (hence, the usual term, “bag of words”), probably due to their having mixed and overall limited utility in topical text classification tasks, as seen in table 4. This likely reflects tha</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5084" citStr="Pang and Lee, 2005" startWordPosition="831" endWordPosition="834">oduct. While this does very well for long documents, we find that an interpolation between MNB and SVM performs excellently for all documents and we report results using this model: w, = (1 − Q) 17V + Qw (4) where w = ||w||1/|V |is the mean magnitude of w, and Q E [0, 1] is the interpolation parameter. This interpolation can be seen as a form of regularization: trust NB unless the SVM is very confident. 3 Datasets and Task We compare with published results on the following datasets. Detailed statistics are shown in table 1. RT-s: Short movie reviews dataset containing one sentence per review (Pang and Lee, 2005). Dataset (N+, N_) l CV |V |A RT-s (5331,5331) 21 10 21K 0.8 CR (2406,1366) 20 10 5713 1.3 MPQA (3316,7308) 3 10 6299 0.8 Subj. (5000,5000) 24 10 24K 0.8 RT-2k (1000,1000) 787 10 51K 1.5 IMDB (25k,25k) 231 N 392K 0.4 AthR (799,628) 345 N 22K 2.9 XGraph (980,973) 261 N 32K 1.8 BbCrypt (992,995) 269 N 25K 0.5 Table 1: Dataset statistics. (N+, N_): number of positive and negative examples. l: average number of words per example. CV: number of crossvalidation splits, or N for train/test split. |V |: the vocabulary size. A: upper-bounds of the differences required to be statistically significant at</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Rennie</author>
<author>Lawrence Shih</author>
<author>Jaime Teevan</author>
<author>David R Karger</author>
</authors>
<title>Tackling the poor assumptions of naive bayes text classifiers.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>616--623</pages>
<contexts>
<context position="10559" citStr="Rennie et al., 2003" startWordPosition="1727" endWordPosition="1730">. While (Ng and Jordan, 2002) showed that NB is better than SVM/logistic regression (LR) with few training cases, we show that MNB is also better with short documents. In contrast to their result that an SVM usually beats NB when it has more than 30–50 training cases, we show that MNB is still better on snippets even with relatively large training sets (9k cases). 4.3 SVM is better at full-length reviews As seen in table 1, the RT-2k and IMDB datasets contain much longer reviews. Compared to the excellent performance of MNB on snippet datasets, the many poor assumptions of MNB pointed out in (Rennie et al., 2003) become more crippling for these longer documents. SVM is much stronger than MNB for the 2 full-length sentiment analysis tasks, but still worse than some other published results. However, NBSVM either exceeds or approaches previous state-of-the art methods, even the 92 Our results RT-2k IMDB Subj. MNB-uni 83.45 83.55 92.58 MNB-bi 85.85 86.59 93.56 SVM-uni 86.25 86.95 90.84 SVM-bi 87.40 89.16 91.74 NBSVM-uni 87.80 88.29 92.40 NBSVM-bi 89.45 91.22 93.18 BoW (bnc) 85.45 87.8 87.77 BoW (bAt&apos;c) 85.8 88.23 85.65 LDA 66.7 67.42 66.65 Full+BoW 87.85 88.33 88.45 Full+Unlab’d+BoW 88.9 88.89 88.13 BoWSV</context>
</contexts>
<marker>Rennie, Shih, Teevan, Karger, 2003</marker>
<rawString>Jason D. Rennie, Lawrence Shih, Jaime Teevan, and David R. Karger. 2003. Tackling the poor assumptions of naive bayes text classifiers. In Proceedings of ICML, pages 616–623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Schohn</author>
<author>David Cohn</author>
</authors>
<title>Less is more: Active learning with support vector machines.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>839--846</pages>
<contexts>
<context position="12933" citStr="Schohn and Cohn, 2000" startWordPosition="2119" endWordPosition="2122">ne. However, in both tables 2 and 3, adding bigrams always improved the performance, and often gives better results than previously published.8 This presumably reflects that in sentiment classification there are 8However, adding trigrams hurts slightly. Method AthR XGraph BbCrypt MNB-uni 85.0 90.0 99.3 MNB-bi 85.1 +0.1 91.2 +1.2 99.4 +0.1 SVM-uni 82.6 85.1 98.3 SVM-bi 83.7 +1.1 86.2 +0.9 97.7 −0.5 NBSVM-uni 87.9 91.2 99.7 NBSVM-bi 87.7 −0.2 90.7 −0.5 99.5 −0.2 ActiveSVM 90 99 DiscLDA 83 – – Table 4: On 3 20-newsgroup subtasks, we compare to DiscLDA (Lacoste-Julien et al., 2008) and ActiveSVM (Schohn and Cohn, 2000). much bigger gains from bigrams, because they can capture modified verbs and nouns. 4.5 NBSVM is a robust performer NBSVM performs well on snippets and longer documents, for sentiment, topic and subjectivity classification, and is often better than previously published results. Therefore, NBSVM seems to be an appropriate and very strong baseline for sophisticated methods aiming to beat a bag of features. One disadvantage of NBSVM is having the interpolation parameter β. The performance on longer documents is virtually identical (within 0.1%) for β E [1/a, 1], while β = 1/a is on average 0.5% </context>
</contexts>
<marker>Schohn, Cohn, 2000</marker>
<rawString>Greg Schohn and David Cohn. 2000. Less is more: Active learning with support vector machines. In Proceedings of ICML, pages 839–846.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7944" citStr="Socher et al., 2011" startWordPosition="1282" endWordPosition="1285">. 4.2 MNB is better at snippets (Moilanen and Pulman, 2007) suggests that while “statistical methods” work well for datasets with hundreds of words in each example, they cannot handle snippets datasets and some rule-based system is necessary. Supporting this claim are examples such as not an inhumane monster6, or killing cancer that express an overall positive sentiment with negative words. Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al., 2011). These works seem promising as they perform better than many sophisticated, rule-based methods used as baselines in (Nakagawa et al., 2010). However, we find that several NB/SVM variants in fact do better than these state-of-the-art methods, even compared to methods that use lexicons, reversal rules, or unsupervised pretraining. The results are in table 2. Our SVM-uni results are consistent with BoFnoDic and BoF-w/Rev used in (Nakagawa et al., 2010) and BoWSVM in (Pang and Lee, 2004). (Nakagawa et al., 2010) used a SVM with secondorder polynomial kernel and additional features. With the only </context>
<context position="9506" citStr="Socher et al., 2011" startWordPosition="1549" endWordPosition="1552"> may be less discriminative, since the documents are extremely short and all methods perform similarly. Method RT-s MPQA CR Subj. MNB-uni 77.9 85.3 79.8 92.6 MNB-bi 79.0 86.3 80.0 93.6 SVM-uni 76.2 86.1 79.0 90.8 SVM-bi 77.7 86.7 80.8 91.7 NBSVM-uni 78.1 85.3 80.5 92.4 NBSVM-bi 79.4 86.3 81.8 93.2 RAE 76.8 85.7 – – RAE-pretrain 77.7 86.4 – – Voting-w/Rev. 63.1 81.7 74.2 – Rule 62.9 81.8 74.3 – BoF-noDic. 75.7 81.8 79.3 – BoF-w/Rev. 76.4 84.1 81.4 – Tree-CRF 77.3 86.1 81.4 – BoWSVM – – – 90.0 Table 2: Results for snippets datasets. Tree-CRF: (Nakagawa et al., 2010) RAE: Recursive Autoencoders (Socher et al., 2011). RAE-pretrain: train on Wikipedia (Collobert and Weston, 2008). “Voting” and “Rule”: use a sentiment lexicon and hard-coded reversal rules. “w/Rev”: “the polarities of phrases which have odd numbers of reversal phrases in their ancestors”. The top 3 methods are in bold and the best is also underlined. sis that rule-based systems have an edge for snippet datasets to be false. MNB is stronger for snippets than for longer documents. While (Ng and Jordan, 2002) showed that NB is better than SVM/logistic regression (LR) with few training cases, we show that MNB is also better with short documents.</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal taxonomies for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM-05.</booktitle>
<contexts>
<context position="11796" citStr="Whitelaw et al., 2005" startWordPosition="1933" endWordPosition="1936"> Valence Shifter 86.2 – – tf.Aidf 88.1 – – Appr. Taxonomy 90.20 – – WRRBM – 87.42 – WRRBM + BoW(bnc) – 89.23 – Table 3: Results for long reviews (RT-2k and IMDB). The snippet dataset Subj. is also included for comparison. Results in rows 7-11 are from (Maas et al., 2011). BoW: linear SVM on bag of words features. bnc: binary, no idf, cosine normalization. At&apos;: smoothed delta idf. Full: the full model. Unlab’d: additional unlabeled data. BoWSVM: bag of words SVM used in (Pang and Lee, 2004). Valence Shifter: (Kennedy and Inkpen, 2006). tf.Aidf: (Martineau and Finin, 2009). Appraisal Taxonomy: (Whitelaw et al., 2005). WRRBM: Word Representation Restricted Boltzmann Machine (Dahl et al., 2012). ones that use additional data. These sentiment analysis results are shown in table 3. 4.4 Benefits of bigrams depends on the task Word bigram features are not that commonly used in text classification tasks (hence, the usual term, “bag of words”), probably due to their having mixed and overall limited utility in topical text classification tasks, as seen in table 4. This likely reflects that certain topic keywords are indicative alone. However, in both tables 2 and 3, adding bigrams always improved the performance, </context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal taxonomies for sentiment analysis. In Proceedings of CIKM-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="5867" citStr="Wiebe et al., 2005" startWordPosition="971" endWordPosition="974">0,1000) 787 10 51K 1.5 IMDB (25k,25k) 231 N 392K 0.4 AthR (799,628) 345 N 22K 2.9 XGraph (980,973) 261 N 32K 1.8 BbCrypt (992,995) 269 N 25K 0.5 Table 1: Dataset statistics. (N+, N_): number of positive and negative examples. l: average number of words per example. CV: number of crossvalidation splits, or N for train/test split. |V |: the vocabulary size. A: upper-bounds of the differences required to be statistically significant at the p &lt; 0.05 level. CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2 MPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005).3 Subj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004). RT-2k: The standard 2000 full-length movie review dataset (Pang and Lee, 2004). IMDB: A large movie review dataset with 50k fulllength reviews (Maas et al., 2011).4 AthR, XGraph, BbCrypt: Classify pairs of newsgroups in the 20-newsgroups dataset with all headers stripped off (the third (18828) version5), namely: alt.atheism vs. religion.misc, comp.windows.x vs. comp.graphics, and rec.sport.baseball vs. sci.crypt, respectively. 4 Experiments and Results 4.1 Experimental setup We use the </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>