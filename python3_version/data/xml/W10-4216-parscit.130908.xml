<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006426">
<title confidence="0.996552">
Feature Selection for Fluency Ranking
</title>
<author confidence="0.993686">
Dani¨el de Kok
</author>
<affiliation confidence="0.996847">
University of Groningen
</affiliation>
<email confidence="0.994707">
d.j.a.de.kok@rug.nl
</email>
<sectionHeader confidence="0.995606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999844538461539">
Fluency rankers are used in modern sentence
generation systems to pick sentences that are
not just grammatical, but also fluent. It has
been shown that feature-based models, such as
maximum entropy models, work well for this
task.
Since maximum entropy models allow for in-
corporation of arbitrary real-valued features,
it is often attractive to create very general
feature templates, that create a huge num-
ber of features. To select the most discrim-
inative features, feature selection can be ap-
plied. In this paper we compare three fea-
ture selection methods: frequency-based se-
lection, a generalization of maximum entropy
feature selection for ranking tasks with real-
valued features, and a new selection method
based on feature value correlation. We show
that the often-used frequency-based selection
performs badly compared to maximum en-
tropy feature selection, and that models with a
few hundred well-picked features are compet-
itive to models with no feature selection ap-
plied. In the experiments described in this pa-
per, we compressed a model of approximately
490.000 features to 1.000 features.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995445">
As shown previously, maximum entropy models
have proven to be viable for fluency ranking (Nakan-
ishi et al., 2005; Velldal and Oepen, 2006; Velldal,
2008). The basic principle of maximum entropy
models is to minimize assumptions, while impos-
ing constraints such that the expected feature value
is equal to the observed feature value in the train-
ing data. In its canonical form, the probability of a
certain event (y) occurring in the context (x) is a log-
linear combination of features and feature weights,
where Z(x) is a normalization over all events in con-
text x (Berger et al., 1996):
</bodyText>
<equation confidence="0.997112666666667">
1
p(y|x) = Z(x)exp λifi (1)
i=1
</equation>
<bodyText confidence="0.999877636363636">
The training process estimates optimal feature
weights, given the constraints and the principle of
maximum entropy. In fluency ranking the input (e.g.
a dependency structure) is a context, and a realiza-
tion of that input is an event within that context.
Features can be hand-crafted or generated auto-
matically using very general feature templates. For
example, if we apply a template rule that enumer-
ates the rules used to construct a derivation tree to
the partial tree in figure 1 the rule(max xp(np)) and
rule(np det n) features will be created.
</bodyText>
<figureCaption confidence="0.9969175">
Figure 1: Partial derivation tree for the noun phrase de
adviezen (the advices).
</figureCaption>
<bodyText confidence="0.994613214285714">
To achieve high accuracy in fluency ranking
quickly, it is attractive to capture as much of the lan-
n
guage generation process as possible. For instance,
in sentence realization, one could extract nearly ev-
ery aspect of a derivation tree as a feature using very
general templates. This path is followed in recent
work, such as Velldal (2008). The advantage of this
approach is that it requires little human labor, and
generally gives good ranking performance. How-
ever, the generality of templates leads to huge mod-
els in terms of number of features. For instance, the
model that we will discuss contains about 490,000
features when no feature selection is applied. Such
models are very opaque, giving very little under-
standing of good discriminators for fluency ranking,
and the size of the models may also be inconvenient.
To make such models more compact and transpar-
ent, feature selection can be applied.
In this paper we make the following contribu-
tions: we modify a maximum entropy feature selec-
tion method for ranking tasks; we introduce a new
feature selection method based on statistical corre-
lation of features; we compare the performance of
the preceding feature selection methods, plus a com-
monly used frequency-based method; and we give
an analysis of the most effective features for fluency
ranking.
</bodyText>
<sectionHeader confidence="0.860724" genericHeader="method">
2 Feature Selection
2.1 Introduction
</sectionHeader>
<bodyText confidence="0.999913296296297">
Feature selection is a process that tries to extract
5 ⊂ F from a set of features F, such that the model
using 5 performs comparably to the model using
F. Such a compression of a feature set can be ob-
tained if there are features: that occur sporadically;
that correlate strongly with other features (features
that show the same behavior within events and con-
texts); or have values with little or no correlation to
the classification or ranking.
Features that do have no correlation to the classi-
fication can be removed from the model. For a set
of highly-correlating features, one feature can be se-
lected to represent the whole group.
Initially it may seem attractive to perform fluency
selection by training a model on all features, select-
ing features with relatively high weights. However,
if features overlap, weight mass will usually be di-
vided over these features. For instance, suppose that
f1 alone has a weight of 0.5 in a given model. If
we retrain the model, after adding the features f2..f5
that behave identically to f1, the weight may be dis-
tributed evenly between f1..f5, giving each feature
the weight 0.1.
In the following sections, we will give a short
overview of previous research in feature selection,
and will then proceed to give a more detailed de-
scription of three feature selection methods.
</bodyText>
<subsectionHeader confidence="0.99854">
2.2 Background
</subsectionHeader>
<bodyText confidence="0.999988689655172">
Feature selection can be seen as model selection,
where the best model of all models that can be
formed using a set of features should be selected.
Madigan and Raftery (1994) propose an method for
model selection aptly named Occam’s window. This
method excludes models that do not perform com-
petitively to other models or that do not perform bet-
ter than one of its submodels. Although this method
is conceptually firm, it is nearly infeasable to apply
it with the number of features used in fluency rank-
ing. Berger et al. (1996) propose a selection method
that iteratively builds a maximum entropy model,
adding features that improve the model. We modify
this method for ranking tasks in section 2.5. Ratna-
parkhi (1999) uses a simple frequency-based cutoff,
where features that occur infrequently are excluded.
We discuss a variant of this selection criterium in
section 2.3. Perkins et al. (2003) describe an ap-
proach where feature selection is applied as a part
of model parameter estimation. They rely on the
fact that i1 regularizers have a tendency to force a
subset of weights to zero. However, such integrated
approaches rely on parameter tuning to get the re-
quested number of features.
In the fluency ranking literature, the use of a fre-
quency cut-off (Velldal and Oepen, 2006) and i1
regularization (Cahill et al., 2007) is prevalent. We
are not aware of any detailed studies that compare
feature selection methods for fluency ranking.
</bodyText>
<subsectionHeader confidence="0.994449">
2.3 Frequency-based Selection
</subsectionHeader>
<bodyText confidence="0.999828153846154">
In frequency-based selection we follow Malouf and
Van Noord (2004), and count for each feature f the
number of inputs where there are at least two realiza-
tions y1, y2, such that f(y1) =� f(y2). We then use
the first N features with the most frequent changes
from the resulting feature frequency list.
Veldall (2008) also experiments with this selec-
tion method, and suggests to apply frequency-based
selection to fluency ranking models that will be dis-
tributed to the public (for compactness’ sake). In
the variant he and Malouf and Van Noord (2004)
discuss, all features that change within more than n
contexts are included in the model.
</bodyText>
<subsectionHeader confidence="0.990926">
2.4 Correlation-based Selection
</subsectionHeader>
<bodyText confidence="0.9999623">
While frequency-based selection helps selecting fea-
tures that are discriminative, it cannot account for
feature overlap. Discriminative features that have a
strong correlation to features that were selected pre-
viously may still be added.
To detect overlap, we calculate the correlation of a
candidate feature and exclude the feature if it shows
a high correlation with features selected previously.
To estimate Pearson’s correlation of two features, we
calculate the sample correlation coefficient,
</bodyText>
<equation confidence="0.9906125">
E x∈X,y∈Y (f1(x, y) − f1)(f2(x,y) − f2)
(n − 1)sf1sf2
</equation>
<bodyText confidence="0.981481666666667">
(2)
where ¯fx is the average feature value of fx, and
sfx is the sample standard deviation of fx.
Of course, correlation can only indicate overlap,
and is in itself not enough to find effective features.
In our experiments with correlation-based selection
we used frequency-based selection as described in
2.3, to make an initial ranking of feature effective-
ness.
</bodyText>
<subsectionHeader confidence="0.994423">
2.5 Maximum Entropy Feature Selection
</subsectionHeader>
<bodyText confidence="0.999959596153846">
Correlation-based selection can detect overlap, how-
ever, there is yet another spurious type of feature
that may reduce its effectiveness. Features with rel-
atively noisy values may contribute less than their
frequency of change may seem to indicate. For in-
stance, consider a feature that returns a completely
random value for every context. Not only does this
feature change very often, its correlation with other
features will also be weak. Such a feature may seem
attractive from the point of view of a frequency or
correlation-based method, but is useless in practice.
To account for both problems, we have to measure
the effectiveness of features in terms of how much
their addition to the model can improve prediction
of the training sample. Or in other words: does the
log-likelihood of the training data increase?
We have modified the Selective Gain Com-
putation (SGC) algorithm described by Zhou et
al. (2003) for ranking tasks rather than classification
tasks. This method builds upon the maximum en-
tropy feature selection method described by Berger
et al. (1996). In this method features are added iter-
atively to a model that is initially uniform. During
each step, the feature that provides the highest gain
as a result of being added to the model, is selected
and added to the model.
In maximum entropy modeling, the weights of the
features in a model are optimized simultaneously.
However, optimizing the weights of the features in
model pS,f for every candidate feature f is compu-
tationally intractable. As a simplification, it is as-
sumed that the weights of features that are already
in the model are not affected by the addition of a
feature f. As a result, the optimal weight α of f can
be found using a simple line search method.
However, as Zhou et al. (2003) note, there is still
an inefficiency in that the weight of every candidate
feature is recalculated during every selection step.
They observe that gains of remaining candidate fea-
tures rarely increase as the result of adding a fea-
ture. If it is assumed that this never happens, a list
of candidate features ordered by gain can be kept.
To account for the fact that the topmost feature in
that list may have lost its effectiveness as the result
of a previous addition of a feature to the model, the
gain of the topmost feature is recalculated and rein-
serted into the list according to its new gain. When
the topmost feature retains its position, it is selected
and added to the model.
Since we use feature selection with features that
are not binary, and for a ranking task, we modified
the recursive forms of the model to:
</bodyText>
<equation confidence="0.9698815">
sumαSuf(y|x) = sumS(y|x) &apos; eαf(y) (3)
�Zα Suf(x) = ZS(x) − sumS(y|x)
y
+
� sumSuf(y|x) (4)
y
</equation>
<bodyText confidence="0.9949245">
Another issue that needs to be dealt with is the
calculation of context and event probabilities. In the
</bodyText>
<equation confidence="0.961531">
rf1,f2 =
</equation>
<bodyText confidence="0.9990554">
literature two approaches are prevalent. The first ap-
proach divides the probability mass uniformly over
contexts, and the probability of events within a con-
text is proportional to the event score (Osborne,
2000):
</bodyText>
<equation confidence="0.994175666666667">
p(x) =|X |1 (5)
p(y|x) = ( score(),y) ) (6)
Py score(x,y)
</equation>
<bodyText confidence="0.988778833333333">
where |X |is the number of contexts. The sec-
ond approach puts more emphasis on the contexts
that contain relatively many events with high scores,
by making the context probability dependent on the
scores of events within that context (Malouf and van
Noord, 2004):
</bodyText>
<equation confidence="0.99765125">
p(x)
Ey score(x, y) =
7
Ey∈X score(x, y)
</equation>
<bodyText confidence="0.99996175">
In our experiments, the second definition of con-
text probability outperformed the first by such a
wide margin, that we only used the second defini-
tion in the experiments described in this paper.
</bodyText>
<subsectionHeader confidence="0.992175">
2.6 A Note on Overlap Detection
</subsectionHeader>
<bodyText confidence="0.996279368421053">
Although maximum-entropy based feature-selection
may be worthwhile in itself, the technique can also
be used during feature engineering to find overlap-
ping features. In the selection method of Berger et
al. (1996), the weight and gain of each candidate fea-
ture is re-estimated during each selection step. We
can exploit the changes in gains to detect overlap be-
tween a selected feature fn, and the candidates for
fn+1. If the gain of a feature changed drastically in
the selection of fn+1 compared to that of fn, this
feature has overlap with fn.
To determine which features had a drastic change
in gain, we determine whether the change has a sig-
nificance with a confidence interval of 99% after
normalization. The normalized gain change is cal-
culated in the following manner as described in al-
gorithm 1.
Algorithm 1 Calculation of the normalized gain
delta
</bodyText>
<figure confidence="0.824986125">
ΔGf ← Gf,n − Gf,n−1
if ΔGf ≥ 0.0 then
OG f
ΔGf,norm ←Gf ,n
else
OG f
ΔGf,norm ←Gf,n−1
end if
</figure>
<sectionHeader confidence="0.960248" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.945623">
3.1 Task
</subsectionHeader>
<bodyText confidence="0.999953588235294">
We evaluated the feature selection methods in con-
junction with a sentence realizer for Dutch. Sen-
tences are realized with a chart generator for the
Alpino wide-coverage grammar and lexicon (Bouma
et al., 2001). As the input of the chart genera-
tor, we use abstract dependency structures, which
are dependency structures leaving out information
such as word order. During generation, we store the
compressed derivation trees and associated (HPSG-
inspired) attribute-value structures for every real-
ization of an abstract dependency structure. We
then use feature templates to extract features from
the derivation trees. Two classes of features (and
templates) can be distinguished output features that
model the output of a process and construction fea-
tures that model the process that constructs the out-
put.
</bodyText>
<subsectionHeader confidence="0.469892">
3.1.1 Output Features
</subsectionHeader>
<bodyText confidence="0.995676833333333">
Currently, there are two output features, both rep-
resenting auxiliary distributions (Johnson and Rie-
zler, 2000): a word trigram model and a part-of-
speech trigram model. The part-of-speech tag set
consists of the Alpino part of speech tags. Both
models are trained on newspaper articles, consist-
ing of 110 million words, from the Twente Nieuws
Corpus1.
The probability of unknown trigrams is estimated
using linear interpolation smoothing (Brants, 2000).
Unknown word probabilities are determined with
Laplacian smoothing.
</bodyText>
<footnote confidence="0.955538">
1http://wwwhome.cs.utwente.nl/druid/
TwNC/TwNC-main.html
</footnote>
<subsectionHeader confidence="0.684955">
3.1.2 Construction Features
</subsectionHeader>
<bodyText confidence="0.9999745">
The construction feature templates consist of tem-
plates that are used for parse disambiguation, and
templates that are specifically targeted at generation.
The parse disambiguation features are used in the
Alpino parser for Dutch, and model various linguis-
tic phenomena that can indicate preferred readings.
The following aspects of a realization are described
by parse disambiguation features:
</bodyText>
<listItem confidence="0.999926">
• Topicalization of (non-)NPs and subjects.
• Use of long-distance/local dependencies.
• Orderings in the middle field.
• Identifiers of grammar rules used to build the
derivation tree.
• Parent-daughter combinations.
</listItem>
<bodyText confidence="0.5353455">
Output features for parse disambiguation, such
as features describing dependency triples, were not
used. Additionally, we use most of the templates de-
scribed by Velldal (2008):
</bodyText>
<listItem confidence="0.999513555555555">
• Local derivation subtrees with optional grand-
parenting, with a maximum of three parents.
• Local derivation subtrees with back-off and
optional grand-parenting, with a maximum of
three parents.
• Binned word domination frequencies of the
daughters of a node.
• Binned standard deviation of word domination
of node daughters.
</listItem>
<subsectionHeader confidence="0.995302">
3.2 Data
</subsectionHeader>
<bodyText confidence="0.999964111111111">
The training and evaluation data was constructed by
parsing 11764 sentences of 5-25 tokens, that were
randomly selected from the (unannotated) Dutch
Wikipedia of August 20082, with the wide-coverage
Alpino parser. For every sentence, the best parse ac-
cording to the disambiguation component was ex-
tracted and considered to be correct. The Alpino
system achieves a concept accuracy of around 90%
on common Dutch corpora (Van Noord, 2007). The
</bodyText>
<footnote confidence="0.84418">
2http://ilps.science.uva.nl/WikiXML/
</footnote>
<bodyText confidence="0.999562722222222">
original sentence is considered to be the best realiza-
tion of the abstract dependency structure of the best
parse.
We then used the Alpino chart generator to con-
struct derivation trees that realize the abstract de-
pendency structure of the best parse. The result-
ing derivation trees, including attribute-value struc-
tures associated with each node, are compressed and
stored in a derivation treebank. Training and testing
data was then obtained by extracting features from
derivation trees stored in the derivation treebank.
At this time, the realizations are also scored using
the General Text Matcher method (GTM) (Melamed
et al., 2003), by comparing them to the original
sentence. We have previously experimented with
ROUGE-N scores, which gave rise to similar results.
However, it is shown that GTM shows the highest
correlation with human judgments (Cahill, 2009).
</bodyText>
<subsectionHeader confidence="0.992091">
3.3 Methodology
</subsectionHeader>
<bodyText confidence="0.99561276">
To evaluate the feature selection methods, we first
train models for each selection method in three
steps: 1. For each abstract dependency structure in
the training data 100 realizations (and corresponding
features) are randomly selected. 2. Feature selection
is applied, and the N-best features according to the
selection method are extracted. 3. A maximum en-
tropy model is trained using the TADM3 software,
with a E2 prior of 0.001, and using the N-best fea-
tures.
We used 5884 training instances (abstract depen-
dency trees, and scored realizations) to train the
model. The maximum entropy selection method was
used with a weight convergence threshold of 1e−6.
Correlation is considered to be strong enough for
overlap in the correlation-based method when two
features have a correlation coefficient of rf1,f2 ≥
0.9
Each model is then evaluated using 5880 held-
out evaluation instances, where we select only in-
stances with 5 or more realizations (4184 instances),
to avoid trivial ranking cases. For every instance,
we select the realization that is the closest to the
original sentence to be the correct realization4. We
then calculate the fraction of instances for which the
</bodyText>
<footnote confidence="0.992315">
3http://tadm.sourceforge.net/
4We follow this approach, because the original sentence is
not always exactly reproduced by the generator.
</footnote>
<bodyText confidence="0.996556666666667">
model picked the correct sentence. Of course, this is
a fairly strict evaluation, since there may be multiple
equally fluent sentences.
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.99993">
4.1 Comparing the Candidates
</subsectionHeader>
<bodyText confidence="0.9969952">
Since each feature selection method that we evalu-
ated gives us a ranked list of features, we can train
models for an increasing number of features. We
have followed this approach, and created models for
each method, using 100 to 5000 features with a step
size of 100 features. Figure 2 shows the accuracy
for all selection methods after N features. We have
also added the line that indicates the accuracy that
is obtained when a model is trained with all features
(490667 features).
</bodyText>
<figure confidence="0.992439">
0.45
all
maxent
cutoff
corr
0 1000 2000 3000 4000 5000
Features
</figure>
<figureCaption confidence="0.9379">
Figure 2: Accuracy of maximum entropy, correlation-
based, and frequency-based selection methods after se-
lecting N features (N ≤ 5000), with increments of 100
features.
</figureCaption>
<bodyText confidence="0.999949706896552">
In this graph we can see two interesting phenom-
ena. First of all, only a very small number of fea-
tures is required to perform this task almost as well
as a model with all extracted features. Secondly, the
maximum entropy feature selection model is able
to select the most effective features quickly - fewer
than 1000 features are necessary to achieve a rela-
tively high accuracy.
As expected, the frequency-based method fared
worse than maximum entropy selection. Initially
some very useful features, such as the n-gram
models are selected, but improvement of accuracy
quickly stagnates. We expect this to be caused by
overlap of newly selected features with features that
were initially selected. Even after selecting 5000
features, this method does not reach the same accu-
racy as the maximum entropy selection method had
after selecting only a few hundred features.
The correlation-based selection method fares bet-
ter than the frequency-based method without over-
lap detection. This clearly shows that feature over-
lap is a problem. However, the correlation-based
method does not achieve good accuracy as quickly
as the maximum entropy selection method. There
are three possible explanations for this. First, there
may be noisy features that are frequent, and since
they show no overlap with selected features they are
good candidates according to the correlation-based
method. Second, less frequent features that overlap
with a frequent feature in a subset of contexts may
show a low correlation. Third, some less frequent
features may still be very discriminative for the con-
texts where they appear, while more frequent fea-
tures may just be a small indicator for a sentence
to be fluent or non-fluent. It is possible to refine
the correlation-based method to deal with the second
class of problems. However, the lack of performance
of the correlation-based method makes this unattrac-
tive - during every selection step a candidate feature
needs to be compared with all previously selected
features, rather than some abstraction of them.
Table 1 shows the peak accuracies when select-
ing up to 5000 features with the feature selection
methods described. Accuracy scores of the random
selection baseline, the n-gram models, and a model
trained on all features are included for comparison.
The random selection baseline picks a realizations
randomly. The n-gram models are the very same
n-gram models that were used as auxiliary distribu-
tions in the feature-based models. The combined
word/tag n-gram model was created by training a
model with both n-gram models as the only fea-
tures. We also list a variation of the frequency-based
method often used in other work (such as Velldal
(2008) and Malouf and Van Noord (2004)), where
there is a fixed frequency threshold (here 4), rather
than using the first N most frequently changing fea-
tures.
</bodyText>
<figure confidence="0.9970125">
Best match accuracy 0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
</figure>
<bodyText confidence="0.992807">
Besides confirming the observation that feature
selection can compress models very well, this table
shows that the popular method of using a frequency
cutoff, still gives a lot of opportunity for compress-
ing the model further. In practice, it seems best to
plot a graph as shown in figure 2, choose an accept-
able accuracy, and to use the (number of) features
that can provide that accuracy.
</bodyText>
<table confidence="0.9989906">
Method Features Accuracy
Random 0 0.0778
Tag n-gram 1 0.2039
Word n-gram 1 0.2799
Word/tag n-gram 2 0.2908
All 490667 0.4220
Fixed cutoff (4) 90103 0.4181
Frequency 4600 0.4029
Correlation 4700 0.4172
Maxent 4300 0.4201
</table>
<tableCaption confidence="0.8929606">
Table 1: Peak accuracies for the maximum entropy,
correlation-based, and frequency-based selection meth-
ods when selecting up to 5000 features. Accuracies for
random, n-gram and full models are included for com-
parison.
</tableCaption>
<subsectionHeader confidence="0.978304">
4.2 Overlap in Frequency-based Selection
</subsectionHeader>
<bodyText confidence="0.999877542857143">
As we argued in section 2.5, the primary disadvan-
tage of the frequency-based selection is that it cannot
account for correlation between features. In the ex-
treme case, we could have two very distinctive fea-
tures f1 and f2 that behave exactly the same in any
event. While adding f2 after adding f1 does not im-
prove the model, frequency-based selection cannot
detect this. To support this argumentation empiri-
cally, we analyzed the first 100 selected features to
find good examples of this overlap.
Initially, the frequency-based selection chooses
three distinctive features that are also selected by
the maximum entropy selection method: the two n-
gram language models, and a preference for topical-
ized NP subjects. After that, features that indicate
whether the vp arg v(np) rule was used change very
frequently within a context. However, this aspect
of the parse tree is embodied in 13 successively se-
lected features. Due to the generality of the feature
templates, there are multiple templates to capture the
use of this grammar rule: through local derivation
trees (with optional grandparenting), back-off for lo-
cal derivation trees, and the features that calculate
lexical node dominance.
Another example of such overlap in the first
100 features is in features modeling the use of the
non wh topicalization(np) rule. Features containing
this rule identifier are used 30 times in sequence,
where it occurs in local derivation subtrees (with
varying amounts of context), back-off local deriva-
tion subtrees, lexical node domination, or as a grand-
parent of another local derivation subtree.
In the first 100 features, there were many overlap-
ping features, and we expect that this also is the case
for more infrequent features.
</bodyText>
<subsectionHeader confidence="0.998215">
4.3 Effective Features
</subsectionHeader>
<bodyText confidence="0.999458257142857">
The maximum entropy selection method shows that
only a small number of features is necessary to per-
form fluency ranking (section 4.1). The first fea-
tures that were selected in maximum entropy selec-
tion can give us good insight of what features are
important for fluency ranking. Table 2 shows the 10
topmost features as returned by the maximum en-
tropy selection. The weights shown in this table, are
those given by the selection method, and their sign
indicates whether the feature was characteristic of a
fluent sentence (+) or a non-fluent sentence (−).
As expected (see table 1) the n-gram models are
a very important predictor for fluency. The only
surprise here may be that the overlap between both
n-gram models is small enough to have both mod-
els as a prominent feature. While the tag n-gram
model is a worse predictor than the word n-gram
model, we expect that the tag n-gram model is espe-
cially useful for estimating fluency of phrases with
word sequences that are unknown to the word n-
gram model.
The next feature that was selected,
r2(vp arg v(pred),2,vproj vc), indicates that
the rule vp arg v(pred) was used with a vproj vc
node as its second daughter. This combination
occurs when the predicative complement is placed
after the copula, for instance as in Amsterdam is de
hoofdstad van Nederland (Amsterdam is the capital
of The Netherlands), rather than De hoofdstad
van Nederland is Amsterdam (The capital of The
Netherlands is Amsterdam).
The feature s1(non subj np topic) and its neg-
ative weight indicates that realizations with non-
topicalized NP subjects are dispreferred. In Dutch,
non-topicalized NP subjects arise in the OVS word-
order, such as in de soep eet Jan (the soup eats Jan).
While this is legal, SVO word-order is clearly pre-
ferred (Jan eet de soep).
The next selected feature (ldsb(vc vb,vb v,
[vproj vc,vp arg v(pp)])) is also related to top-
icalization: it usually indicates a preference for
prepositional complements that are not topicalized.
For instance, dit zorgde voor veel verdeeldheid
(this caused lots of discord) is preferred over the
PP-topicalized voor veel verdeeldheid zorgde dit
(lots of discord caused this).
ldsb(n n pps,pp p arg(np),[]) gives preference
PP-ordering in conjuncts where the PP modifier fol-
lows the head. For instance, the conjunct groepen
van bestaan of khandas (planes of existance or khan-
das) is preferred by this feature over van bestaan
groepen of khandas (of existence planes or khan-
das).
The next feature (lds dl(mod2,[pp p arg(np)],
[1],[non wh topicalization(modifier)])) forms an
exception to the dispreference of topicalization of
PPs. If we have a PP that modifies a copula in a
subject-predicate structure, topicalization of the PP
can make the realization more fluent. For instance,
volgens Williamson is dit de synthese (according to
Williamson is this the synthesis) is considered more
fluent than dit is de synthese volgens Williamson
(this is the synthesis according to Williamson).
The final three features deal with punctuation.
Since punctuation is very prevalent in Wikipedia
texts due to the amount of definitions and clarifi-
cations, punctuation-related features are common.
Note that the last two lds dl features may seem to
be overlapping, they are not: they use different fre-
quency bins for word domination.
</bodyText>
<sectionHeader confidence="0.998743" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.997455">
Our conclusion after performing experiments with
feature selection is twofold. First, fluency models
can be compressed enormously by applying feature
selection, without losing much in terms of accuracy.
Second, we only need a small number of targeted
features to perform fluency ranking.
The maximum entropy feature selection method
</bodyText>
<table confidence="0.999743117647059">
Weight Name
0.012 ngram lm
0.009 ngram tag
0.087 r2(vp arg v(pred),2,vproj vc)
-0.094 s1(non subj np topic)
0.090 ldsb(vc vb,vb v,
[vproj vc,vp arg v(pp)])
0.083 ldsb(n n pps,pp p arg(np),[])
0.067 lds dl(mod2,[pp p arg(np)],[1],
[non wh topicalization(modifier)])
0.251 lds dl(start start ligg streep,
[top start xp,punct(ligg streep),
top start xp],[0,0,1],[top start])
0.186 lds dl(start start ligg streep,
[top start xp,punct(ligg streep),
top start xp],[0,0,2],[top start])
0.132 r2(n n modroot(haak),5,l)
</table>
<tableCaption confidence="0.986487">
Table 2: The first 10 features returned by maximum en-
tropy feature selection, including the weights estimated
by this feature selection method.
</tableCaption>
<bodyText confidence="0.999973375">
shows a high accuracy after selecting just a few fea-
tures. The commonly used frequency-based selec-
tion method fares far worse, and requires addition
of many more features to achieve the same perfor-
mance as the maximum entropy method. By exper-
imenting with a correlation-based selection method
that uses the frequency method to make an initial
ordering of features, but skips features that show a
high correlation with previously selected features,
we have shown that the ineffectiveness of frequency-
based selection can be attributed partly to feature
overlap. However, the maximum entropy method
was still more effective in our experiments.
In the future, we hope to evaluate the same tech-
niques to parse disambiguation. We also plan to
compare the feature selection methods described in
this paper to selection by imposing a Ei prior.
The feature selection methods described in this
paper are usable for feature sets devised for ranking
and classification tasks, especially when huge sets of
automatically extracted features are used. An open
source implementation of the methods described in
this paper is available5, and is optimized to work on
large data and feature sets.
</bodyText>
<footnote confidence="0.916739">
5http://danieldk.eu/Code/FeatureSqueeze/
</footnote>
<sectionHeader confidence="0.966081" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808971830986">
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational linguistics, 22(1):71.
G. Bouma, G. Van Noord, and R. Malouf. 2001. Alpino:
Wide-coverage computational analysis of Dutch. In
Computational Linguistics in the Netherlands 2000.
Selected Papers from the 11th CLIN Meeting.
T. Brants. 2000. TnT – a statistical part-of-speech tagger.
In Proceedings of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
A. Cahill, M. Forst, and C. Rohrer. 2007. Stochas-
tic realisation ranking for a free word order language.
In ENLG ’07: Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
17–24, Morristown, NJ, USA. Association for Com-
putational Linguistics.
A. Cahill. 2009. Correlating Human and Automatic
Evaluation of a German Surface Realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97–100.
M. Johnson and S. Riezler. 2000. Exploiting auxiliary
distributions in stochastic unification-based grammars.
In Proceedings of the 1st North American chapter of
the Association for Computational Linguistics confer-
ence, pages 154–161, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
D. Madigan and A.E. Raftery. 1994. Model selection and
accounting for model uncertainty in graphical models
using Occam’s window. Journal of the American Sta-
tistical Association, 89(428):1535–1546.
R. Malouf and G. van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars.
In IJCNLP-04 Workshop: Beyond shallow analyses -
Formalisms and statistical modeling for deep analy-
ses. JST CREST, March.
I. D. Melamed, R. Green, and J. P. Turian. 2003. Pre-
cision and recall of machine translation. In HLT-
NAACL.
H. Nakanishi, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic models for disambiguation of an hpsg-based chart
generator. In Parsing ’05: Proceedings of the Ninth
International Workshop on Parsing Technology, pages
93–102, Morristown, NJ, USA. Association for Com-
putational Linguistics.
M. Osborne. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Pro-
ceedings of the 18th conference on Computational lin-
guistics, pages 586–592, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
S. Perkins, K. Lacker, and J. Theiler. 2003. Grafting:
Fast, incremental feature selection by gradient descent
in function space. The Journal of Machine Learning
Research, 3:1333–1356.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1):151–175.
G. Van Noord. 2007. Using self-trained bilexical pref-
erences to improve disambiguation accuracy. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, pages 1–10. Association for Compu-
tational Linguistics.
E. Velldal and S. Oepen. 2006. Statistical ranking in
tactical generation. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 517–525. Association for Compu-
tational Linguistics.
E. Velldal. 2008. Empirical Realization Ranking. Ph.D.
thesis, University of Oslo, Department of Informatics.
Y. Zhou, F. Weng, L. Wu, and H. Schmidt. 2003. A
fast algorithm for feature selection in conditional max-
imum entropy modeling.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.842381">
<title confidence="0.999977">Feature Selection for Fluency Ranking</title>
<author confidence="0.998317">Dani¨el_de_Kok</author>
<affiliation confidence="0.999011">University of</affiliation>
<email confidence="0.968165">d.j.a.de.kok@rug.nl</email>
<abstract confidence="0.995157185185185">Fluency rankers are used in modern sentence generation systems to pick sentences that are not just grammatical, but also fluent. It has been shown that feature-based models, such as maximum entropy models, work well for this task. Since maximum entropy models allow for incorporation of arbitrary real-valued features, it is often attractive to create very general feature templates, that create a huge number of features. To select the most discriminative features, feature selection can be applied. In this paper we compare three feature selection methods: frequency-based selection, a generalization of maximum entropy feature selection for ranking tasks with realvalued features, and a new selection method based on feature value correlation. We show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection, and that models with a few hundred well-picked features are competitive to models with no feature selection applied. In the experiments described in this paper, we compressed a model of approximately 490.000 features to 1.000 features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="1805" citStr="Berger et al., 1996" startWordPosition="283" endWordPosition="286"> to 1.000 features. 1 Introduction As shown previously, maximum entropy models have proven to be viable for fluency ranking (Nakanishi et al., 2005; Velldal and Oepen, 2006; Velldal, 2008). The basic principle of maximum entropy models is to minimize assumptions, while imposing constraints such that the expected feature value is equal to the observed feature value in the training data. In its canonical form, the probability of a certain event (y) occurring in the context (x) is a loglinear combination of features and feature weights, where Z(x) is a normalization over all events in context x (Berger et al., 1996): 1 p(y|x) = Z(x)exp λifi (1) i=1 The training process estimates optimal feature weights, given the constraints and the principle of maximum entropy. In fluency ranking the input (e.g. a dependency structure) is a context, and a realization of that input is an event within that context. Features can be hand-crafted or generated automatically using very general feature templates. For example, if we apply a template rule that enumerates the rules used to construct a derivation tree to the partial tree in figure 1 the rule(max xp(np)) and rule(np det n) features will be created. Figure 1: Partial</context>
<context position="5677" citStr="Berger et al. (1996)" startWordPosition="936" endWordPosition="939">ceed to give a more detailed description of three feature selection methods. 2.2 Background Feature selection can be seen as model selection, where the best model of all models that can be formed using a set of features should be selected. Madigan and Raftery (1994) propose an method for model selection aptly named Occam’s window. This method excludes models that do not perform competitively to other models or that do not perform better than one of its submodels. Although this method is conceptually firm, it is nearly infeasable to apply it with the number of features used in fluency ranking. Berger et al. (1996) propose a selection method that iteratively builds a maximum entropy model, adding features that improve the model. We modify this method for ranking tasks in section 2.5. Ratnaparkhi (1999) uses a simple frequency-based cutoff, where features that occur infrequently are excluded. We discuss a variant of this selection criterium in section 2.3. Perkins et al. (2003) describe an approach where feature selection is applied as a part of model parameter estimation. They rely on the fact that i1 regularizers have a tendency to force a subset of weights to zero. However, such integrated approaches </context>
<context position="9319" citStr="Berger et al. (1996)" startWordPosition="1520" endWordPosition="1523">ature may seem attractive from the point of view of a frequency or correlation-based method, but is useless in practice. To account for both problems, we have to measure the effectiveness of features in terms of how much their addition to the model can improve prediction of the training sample. Or in other words: does the log-likelihood of the training data increase? We have modified the Selective Gain Computation (SGC) algorithm described by Zhou et al. (2003) for ranking tasks rather than classification tasks. This method builds upon the maximum entropy feature selection method described by Berger et al. (1996). In this method features are added iteratively to a model that is initially uniform. During each step, the feature that provides the highest gain as a result of being added to the model, is selected and added to the model. In maximum entropy modeling, the weights of the features in a model are optimized simultaneously. However, optimizing the weights of the features in model pS,f for every candidate feature f is computationally intractable. As a simplification, it is assumed that the weights of features that are already in the model are not affected by the addition of a feature f. As a result</context>
<context position="12070" citStr="Berger et al. (1996)" startWordPosition="2001" endWordPosition="2004">ts with high scores, by making the context probability dependent on the scores of events within that context (Malouf and van Noord, 2004): p(x) Ey score(x, y) = 7 Ey∈X score(x, y) In our experiments, the second definition of context probability outperformed the first by such a wide margin, that we only used the second definition in the experiments described in this paper. 2.6 A Note on Overlap Detection Although maximum-entropy based feature-selection may be worthwhile in itself, the technique can also be used during feature engineering to find overlapping features. In the selection method of Berger et al. (1996), the weight and gain of each candidate feature is re-estimated during each selection step. We can exploit the changes in gains to detect overlap between a selected feature fn, and the candidates for fn+1. If the gain of a feature changed drastically in the selection of fn+1 compared to that of fn, this feature has overlap with fn. To determine which features had a drastic change in gain, we determine whether the change has a significance with a confidence interval of 99% after normalization. The normalized gain change is calculated in the following manner as described in algorithm 1. Algorith</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
<author>G Van Noord</author>
<author>R Malouf</author>
</authors>
<title>Alpino: Wide-coverage computational analysis of Dutch.</title>
<date>2001</date>
<booktitle>In Computational Linguistics in the Netherlands 2000. Selected Papers from the 11th CLIN Meeting.</booktitle>
<marker>Bouma, Van Noord, Malouf, 2001</marker>
<rawString>G. Bouma, G. Van Noord, and R. Malouf. 2001. Alpino: Wide-coverage computational analysis of Dutch. In Computational Linguistics in the Netherlands 2000. Selected Papers from the 11th CLIN Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Applied Natural Language Processing (ANLP-2000),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="14117" citStr="Brants, 2000" startWordPosition="2338" endWordPosition="2339">tinguished output features that model the output of a process and construction features that model the process that constructs the output. 3.1.1 Output Features Currently, there are two output features, both representing auxiliary distributions (Johnson and Riezler, 2000): a word trigram model and a part-ofspeech trigram model. The part-of-speech tag set consists of the Alpino part of speech tags. Both models are trained on newspaper articles, consisting of 110 million words, from the Twente Nieuws Corpus1. The probability of unknown trigrams is estimated using linear interpolation smoothing (Brants, 2000). Unknown word probabilities are determined with Laplacian smoothing. 1http://wwwhome.cs.utwente.nl/druid/ TwNC/TwNC-main.html 3.1.2 Construction Features The construction feature templates consist of templates that are used for parse disambiguation, and templates that are specifically targeted at generation. The parse disambiguation features are used in the Alpino parser for Dutch, and model various linguistic phenomena that can indicate preferred readings. The following aspects of a realization are described by parse disambiguation features: • Topicalization of (non-)NPs and subjects. • Use </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT – a statistical part-of-speech tagger. In Proceedings of the Sixth Applied Natural Language Processing (ANLP-2000), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Forst</author>
<author>C Rohrer</author>
</authors>
<title>Stochastic realisation ranking for a free word order language. In</title>
<date>2007</date>
<booktitle>ENLG ’07: Proceedings of the Eleventh European Workshop on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6478" citStr="Cahill et al., 2007" startWordPosition="1067" endWordPosition="1070">khi (1999) uses a simple frequency-based cutoff, where features that occur infrequently are excluded. We discuss a variant of this selection criterium in section 2.3. Perkins et al. (2003) describe an approach where feature selection is applied as a part of model parameter estimation. They rely on the fact that i1 regularizers have a tendency to force a subset of weights to zero. However, such integrated approaches rely on parameter tuning to get the requested number of features. In the fluency ranking literature, the use of a frequency cut-off (Velldal and Oepen, 2006) and i1 regularization (Cahill et al., 2007) is prevalent. We are not aware of any detailed studies that compare feature selection methods for fluency ranking. 2.3 Frequency-based Selection In frequency-based selection we follow Malouf and Van Noord (2004), and count for each feature f the number of inputs where there are at least two realizations y1, y2, such that f(y1) =� f(y2). We then use the first N features with the most frequent changes from the resulting feature frequency list. Veldall (2008) also experiments with this selection method, and suggests to apply frequency-based selection to fluency ranking models that will be distri</context>
</contexts>
<marker>Cahill, Forst, Rohrer, 2007</marker>
<rawString>A. Cahill, M. Forst, and C. Rohrer. 2007. Stochastic realisation ranking for a free word order language. In ENLG ’07: Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 17–24, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
</authors>
<title>Correlating Human and Automatic Evaluation of a German Surface Realiser.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="16743" citStr="Cahill, 2009" startWordPosition="2722" endWordPosition="2723">e resulting derivation trees, including attribute-value structures associated with each node, are compressed and stored in a derivation treebank. Training and testing data was then obtained by extracting features from derivation trees stored in the derivation treebank. At this time, the realizations are also scored using the General Text Matcher method (GTM) (Melamed et al., 2003), by comparing them to the original sentence. We have previously experimented with ROUGE-N scores, which gave rise to similar results. However, it is shown that GTM shows the highest correlation with human judgments (Cahill, 2009). 3.3 Methodology To evaluate the feature selection methods, we first train models for each selection method in three steps: 1. For each abstract dependency structure in the training data 100 realizations (and corresponding features) are randomly selected. 2. Feature selection is applied, and the N-best features according to the selection method are extracted. 3. A maximum entropy model is trained using the TADM3 software, with a E2 prior of 0.001, and using the N-best features. We used 5884 training instances (abstract dependency trees, and scored realizations) to train the model. The maximum</context>
</contexts>
<marker>Cahill, 2009</marker>
<rawString>A. Cahill. 2009. Correlating Human and Automatic Evaluation of a German Surface Realiser. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Riezler</author>
</authors>
<title>Exploiting auxiliary distributions in stochastic unification-based grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>154--161</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="13776" citStr="Johnson and Riezler, 2000" startWordPosition="2282" endWordPosition="2286">ctures leaving out information such as word order. During generation, we store the compressed derivation trees and associated (HPSGinspired) attribute-value structures for every realization of an abstract dependency structure. We then use feature templates to extract features from the derivation trees. Two classes of features (and templates) can be distinguished output features that model the output of a process and construction features that model the process that constructs the output. 3.1.1 Output Features Currently, there are two output features, both representing auxiliary distributions (Johnson and Riezler, 2000): a word trigram model and a part-ofspeech trigram model. The part-of-speech tag set consists of the Alpino part of speech tags. Both models are trained on newspaper articles, consisting of 110 million words, from the Twente Nieuws Corpus1. The probability of unknown trigrams is estimated using linear interpolation smoothing (Brants, 2000). Unknown word probabilities are determined with Laplacian smoothing. 1http://wwwhome.cs.utwente.nl/druid/ TwNC/TwNC-main.html 3.1.2 Construction Features The construction feature templates consist of templates that are used for parse disambiguation, and temp</context>
</contexts>
<marker>Johnson, Riezler, 2000</marker>
<rawString>M. Johnson and S. Riezler. 2000. Exploiting auxiliary distributions in stochastic unification-based grammars. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 154–161, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Madigan</author>
<author>A E Raftery</author>
</authors>
<title>Model selection and accounting for model uncertainty in graphical models using Occam’s window.</title>
<date>1994</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>89</volume>
<issue>428</issue>
<contexts>
<context position="5323" citStr="Madigan and Raftery (1994)" startWordPosition="874" endWordPosition="877">nce, suppose that f1 alone has a weight of 0.5 in a given model. If we retrain the model, after adding the features f2..f5 that behave identically to f1, the weight may be distributed evenly between f1..f5, giving each feature the weight 0.1. In the following sections, we will give a short overview of previous research in feature selection, and will then proceed to give a more detailed description of three feature selection methods. 2.2 Background Feature selection can be seen as model selection, where the best model of all models that can be formed using a set of features should be selected. Madigan and Raftery (1994) propose an method for model selection aptly named Occam’s window. This method excludes models that do not perform competitively to other models or that do not perform better than one of its submodels. Although this method is conceptually firm, it is nearly infeasable to apply it with the number of features used in fluency ranking. Berger et al. (1996) propose a selection method that iteratively builds a maximum entropy model, adding features that improve the model. We modify this method for ranking tasks in section 2.5. Ratnaparkhi (1999) uses a simple frequency-based cutoff, where features t</context>
</contexts>
<marker>Madigan, Raftery, 1994</marker>
<rawString>D. Madigan and A.E. Raftery. 1994. Model selection and accounting for model uncertainty in graphical models using Occam’s window. Journal of the American Statistical Association, 89(428):1535–1546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
<author>G van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In IJCNLP-04 Workshop: Beyond shallow</booktitle>
<location>CREST,</location>
<marker>Malouf, van Noord, 2004</marker>
<rawString>R. Malouf and G. van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In IJCNLP-04 Workshop: Beyond shallow analyses -Formalisms and statistical modeling for deep analyses. JST CREST, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
<author>R Green</author>
<author>J P Turian</author>
</authors>
<title>Precision and recall of machine translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="16513" citStr="Melamed et al., 2003" startWordPosition="2685" endWordPosition="2688">ce is considered to be the best realization of the abstract dependency structure of the best parse. We then used the Alpino chart generator to construct derivation trees that realize the abstract dependency structure of the best parse. The resulting derivation trees, including attribute-value structures associated with each node, are compressed and stored in a derivation treebank. Training and testing data was then obtained by extracting features from derivation trees stored in the derivation treebank. At this time, the realizations are also scored using the General Text Matcher method (GTM) (Melamed et al., 2003), by comparing them to the original sentence. We have previously experimented with ROUGE-N scores, which gave rise to similar results. However, it is shown that GTM shows the highest correlation with human judgments (Cahill, 2009). 3.3 Methodology To evaluate the feature selection methods, we first train models for each selection method in three steps: 1. For each abstract dependency structure in the training data 100 realizations (and corresponding features) are randomly selected. 2. Feature selection is applied, and the N-best features according to the selection method are extracted. 3. A ma</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. D. Melamed, R. Green, and J. P. Turian. 2003. Precision and recall of machine translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nakanishi</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an hpsg-based chart generator.</title>
<date>2005</date>
<booktitle>In Parsing ’05: Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>93--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1332" citStr="Nakanishi et al., 2005" startWordPosition="201" endWordPosition="205">f maximum entropy feature selection for ranking tasks with realvalued features, and a new selection method based on feature value correlation. We show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection, and that models with a few hundred well-picked features are competitive to models with no feature selection applied. In the experiments described in this paper, we compressed a model of approximately 490.000 features to 1.000 features. 1 Introduction As shown previously, maximum entropy models have proven to be viable for fluency ranking (Nakanishi et al., 2005; Velldal and Oepen, 2006; Velldal, 2008). The basic principle of maximum entropy models is to minimize assumptions, while imposing constraints such that the expected feature value is equal to the observed feature value in the training data. In its canonical form, the probability of a certain event (y) occurring in the context (x) is a loglinear combination of features and feature weights, where Z(x) is a normalization over all events in context x (Berger et al., 1996): 1 p(y|x) = Z(x)exp λifi (1) i=1 The training process estimates optimal feature weights, given the constraints and the princip</context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>H. Nakanishi, Y. Miyao, and J. Tsujii. 2005. Probabilistic models for disambiguation of an hpsg-based chart generator. In Parsing ’05: Proceedings of the Ninth International Workshop on Parsing Technology, pages 93–102, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Osborne</author>
</authors>
<title>Estimation of stochastic attributevalue grammars using an informative sample.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>586--592</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11265" citStr="Osborne, 2000" startWordPosition="1867" endWordPosition="1868">ains its position, it is selected and added to the model. Since we use feature selection with features that are not binary, and for a ranking task, we modified the recursive forms of the model to: sumαSuf(y|x) = sumS(y|x) &apos; eαf(y) (3) �Zα Suf(x) = ZS(x) − sumS(y|x) y + � sumSuf(y|x) (4) y Another issue that needs to be dealt with is the calculation of context and event probabilities. In the rf1,f2 = literature two approaches are prevalent. The first approach divides the probability mass uniformly over contexts, and the probability of events within a context is proportional to the event score (Osborne, 2000): p(x) =|X |1 (5) p(y|x) = ( score(),y) ) (6) Py score(x,y) where |X |is the number of contexts. The second approach puts more emphasis on the contexts that contain relatively many events with high scores, by making the context probability dependent on the scores of events within that context (Malouf and van Noord, 2004): p(x) Ey score(x, y) = 7 Ey∈X score(x, y) In our experiments, the second definition of context probability outperformed the first by such a wide margin, that we only used the second definition in the experiments described in this paper. 2.6 A Note on Overlap Detection Although</context>
</contexts>
<marker>Osborne, 2000</marker>
<rawString>M. Osborne. 2000. Estimation of stochastic attributevalue grammars using an informative sample. In Proceedings of the 18th conference on Computational linguistics, pages 586–592, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Perkins</author>
<author>K Lacker</author>
<author>J Theiler</author>
</authors>
<title>Grafting: Fast, incremental feature selection by gradient descent in function space.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1333</pages>
<contexts>
<context position="6046" citStr="Perkins et al. (2003)" startWordPosition="993" endWordPosition="996"> not perform competitively to other models or that do not perform better than one of its submodels. Although this method is conceptually firm, it is nearly infeasable to apply it with the number of features used in fluency ranking. Berger et al. (1996) propose a selection method that iteratively builds a maximum entropy model, adding features that improve the model. We modify this method for ranking tasks in section 2.5. Ratnaparkhi (1999) uses a simple frequency-based cutoff, where features that occur infrequently are excluded. We discuss a variant of this selection criterium in section 2.3. Perkins et al. (2003) describe an approach where feature selection is applied as a part of model parameter estimation. They rely on the fact that i1 regularizers have a tendency to force a subset of weights to zero. However, such integrated approaches rely on parameter tuning to get the requested number of features. In the fluency ranking literature, the use of a frequency cut-off (Velldal and Oepen, 2006) and i1 regularization (Cahill et al., 2007) is prevalent. We are not aware of any detailed studies that compare feature selection methods for fluency ranking. 2.3 Frequency-based Selection In frequency-based sel</context>
</contexts>
<marker>Perkins, Lacker, Theiler, 2003</marker>
<rawString>S. Perkins, K. Lacker, and J. Theiler. 2003. Grafting: Fast, incremental feature selection by gradient descent in function space. The Journal of Machine Learning Research, 3:1333–1356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="5868" citStr="Ratnaparkhi (1999)" startWordPosition="967" endWordPosition="969">ed using a set of features should be selected. Madigan and Raftery (1994) propose an method for model selection aptly named Occam’s window. This method excludes models that do not perform competitively to other models or that do not perform better than one of its submodels. Although this method is conceptually firm, it is nearly infeasable to apply it with the number of features used in fluency ranking. Berger et al. (1996) propose a selection method that iteratively builds a maximum entropy model, adding features that improve the model. We modify this method for ranking tasks in section 2.5. Ratnaparkhi (1999) uses a simple frequency-based cutoff, where features that occur infrequently are excluded. We discuss a variant of this selection criterium in section 2.3. Perkins et al. (2003) describe an approach where feature selection is applied as a part of model parameter estimation. They rely on the fact that i1 regularizers have a tendency to force a subset of weights to zero. However, such integrated approaches rely on parameter tuning to get the requested number of features. In the fluency ranking literature, the use of a frequency cut-off (Velldal and Oepen, 2006) and i1 regularization (Cahill et </context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>A. Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34(1):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Van Noord</author>
</authors>
<title>Using self-trained bilexical preferences to improve disambiguation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technologies,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Van Noord, 2007</marker>
<rawString>G. Van Noord. 2007. Using self-trained bilexical preferences to improve disambiguation accuracy. In Proceedings of the 10th International Conference on Parsing Technologies, pages 1–10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Velldal</author>
<author>S Oepen</author>
</authors>
<title>Statistical ranking in tactical generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>517--525</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1357" citStr="Velldal and Oepen, 2006" startWordPosition="206" endWordPosition="209">e selection for ranking tasks with realvalued features, and a new selection method based on feature value correlation. We show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection, and that models with a few hundred well-picked features are competitive to models with no feature selection applied. In the experiments described in this paper, we compressed a model of approximately 490.000 features to 1.000 features. 1 Introduction As shown previously, maximum entropy models have proven to be viable for fluency ranking (Nakanishi et al., 2005; Velldal and Oepen, 2006; Velldal, 2008). The basic principle of maximum entropy models is to minimize assumptions, while imposing constraints such that the expected feature value is equal to the observed feature value in the training data. In its canonical form, the probability of a certain event (y) occurring in the context (x) is a loglinear combination of features and feature weights, where Z(x) is a normalization over all events in context x (Berger et al., 1996): 1 p(y|x) = Z(x)exp λifi (1) i=1 The training process estimates optimal feature weights, given the constraints and the principle of maximum entropy. In</context>
<context position="6434" citStr="Velldal and Oepen, 2006" startWordPosition="1060" endWordPosition="1063">ethod for ranking tasks in section 2.5. Ratnaparkhi (1999) uses a simple frequency-based cutoff, where features that occur infrequently are excluded. We discuss a variant of this selection criterium in section 2.3. Perkins et al. (2003) describe an approach where feature selection is applied as a part of model parameter estimation. They rely on the fact that i1 regularizers have a tendency to force a subset of weights to zero. However, such integrated approaches rely on parameter tuning to get the requested number of features. In the fluency ranking literature, the use of a frequency cut-off (Velldal and Oepen, 2006) and i1 regularization (Cahill et al., 2007) is prevalent. We are not aware of any detailed studies that compare feature selection methods for fluency ranking. 2.3 Frequency-based Selection In frequency-based selection we follow Malouf and Van Noord (2004), and count for each feature f the number of inputs where there are at least two realizations y1, y2, such that f(y1) =� f(y2). We then use the first N features with the most frequent changes from the resulting feature frequency list. Veldall (2008) also experiments with this selection method, and suggests to apply frequency-based selection t</context>
</contexts>
<marker>Velldal, Oepen, 2006</marker>
<rawString>E. Velldal and S. Oepen. 2006. Statistical ranking in tactical generation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 517–525. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Velldal</author>
</authors>
<title>Empirical Realization Ranking.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Oslo, Department of Informatics.</institution>
<contexts>
<context position="1373" citStr="Velldal, 2008" startWordPosition="210" endWordPosition="211">asks with realvalued features, and a new selection method based on feature value correlation. We show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection, and that models with a few hundred well-picked features are competitive to models with no feature selection applied. In the experiments described in this paper, we compressed a model of approximately 490.000 features to 1.000 features. 1 Introduction As shown previously, maximum entropy models have proven to be viable for fluency ranking (Nakanishi et al., 2005; Velldal and Oepen, 2006; Velldal, 2008). The basic principle of maximum entropy models is to minimize assumptions, while imposing constraints such that the expected feature value is equal to the observed feature value in the training data. In its canonical form, the probability of a certain event (y) occurring in the context (x) is a loglinear combination of features and feature weights, where Z(x) is a normalization over all events in context x (Berger et al., 1996): 1 p(y|x) = Z(x)exp λifi (1) i=1 The training process estimates optimal feature weights, given the constraints and the principle of maximum entropy. In fluency ranking</context>
<context position="2809" citStr="Velldal (2008)" startWordPosition="454" endWordPosition="455">ple, if we apply a template rule that enumerates the rules used to construct a derivation tree to the partial tree in figure 1 the rule(max xp(np)) and rule(np det n) features will be created. Figure 1: Partial derivation tree for the noun phrase de adviezen (the advices). To achieve high accuracy in fluency ranking quickly, it is attractive to capture as much of the lann guage generation process as possible. For instance, in sentence realization, one could extract nearly every aspect of a derivation tree as a feature using very general templates. This path is followed in recent work, such as Velldal (2008). The advantage of this approach is that it requires little human labor, and generally gives good ranking performance. However, the generality of templates leads to huge models in terms of number of features. For instance, the model that we will discuss contains about 490,000 features when no feature selection is applied. Such models are very opaque, giving very little understanding of good discriminators for fluency ranking, and the size of the models may also be inconvenient. To make such models more compact and transparent, feature selection can be applied. In this paper we make the followi</context>
<context position="15060" citStr="Velldal (2008)" startWordPosition="2465" endWordPosition="2466">uation features are used in the Alpino parser for Dutch, and model various linguistic phenomena that can indicate preferred readings. The following aspects of a realization are described by parse disambiguation features: • Topicalization of (non-)NPs and subjects. • Use of long-distance/local dependencies. • Orderings in the middle field. • Identifiers of grammar rules used to build the derivation tree. • Parent-daughter combinations. Output features for parse disambiguation, such as features describing dependency triples, were not used. Additionally, we use most of the templates described by Velldal (2008): • Local derivation subtrees with optional grandparenting, with a maximum of three parents. • Local derivation subtrees with back-off and optional grand-parenting, with a maximum of three parents. • Binned word domination frequencies of the daughters of a node. • Binned standard deviation of word domination of node daughters. 3.2 Data The training and evaluation data was constructed by parsing 11764 sentences of 5-25 tokens, that were randomly selected from the (unannotated) Dutch Wikipedia of August 20082, with the wide-coverage Alpino parser. For every sentence, the best parse according to </context>
<context position="21648" citStr="Velldal (2008)" startWordPosition="3509" endWordPosition="3510"> when selecting up to 5000 features with the feature selection methods described. Accuracy scores of the random selection baseline, the n-gram models, and a model trained on all features are included for comparison. The random selection baseline picks a realizations randomly. The n-gram models are the very same n-gram models that were used as auxiliary distributions in the feature-based models. The combined word/tag n-gram model was created by training a model with both n-gram models as the only features. We also list a variation of the frequency-based method often used in other work (such as Velldal (2008) and Malouf and Van Noord (2004)), where there is a fixed frequency threshold (here 4), rather than using the first N most frequently changing features. Best match accuracy 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 Besides confirming the observation that feature selection can compress models very well, this table shows that the popular method of using a frequency cutoff, still gives a lot of opportunity for compressing the model further. In practice, it seems best to plot a graph as shown in figure 2, choose an acceptable accuracy, and to use the (number of) features that can provide that accuracy. </context>
</contexts>
<marker>Velldal, 2008</marker>
<rawString>E. Velldal. 2008. Empirical Realization Ranking. Ph.D. thesis, University of Oslo, Department of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhou</author>
<author>F Weng</author>
<author>L Wu</author>
<author>H Schmidt</author>
</authors>
<title>A fast algorithm for feature selection in conditional maximum entropy modeling.</title>
<date>2003</date>
<contexts>
<context position="9164" citStr="Zhou et al. (2003)" startWordPosition="1496" endWordPosition="1499">completely random value for every context. Not only does this feature change very often, its correlation with other features will also be weak. Such a feature may seem attractive from the point of view of a frequency or correlation-based method, but is useless in practice. To account for both problems, we have to measure the effectiveness of features in terms of how much their addition to the model can improve prediction of the training sample. Or in other words: does the log-likelihood of the training data increase? We have modified the Selective Gain Computation (SGC) algorithm described by Zhou et al. (2003) for ranking tasks rather than classification tasks. This method builds upon the maximum entropy feature selection method described by Berger et al. (1996). In this method features are added iteratively to a model that is initially uniform. During each step, the feature that provides the highest gain as a result of being added to the model, is selected and added to the model. In maximum entropy modeling, the weights of the features in a model are optimized simultaneously. However, optimizing the weights of the features in model pS,f for every candidate feature f is computationally intractable.</context>
</contexts>
<marker>Zhou, Weng, Wu, Schmidt, 2003</marker>
<rawString>Y. Zhou, F. Weng, L. Wu, and H. Schmidt. 2003. A fast algorithm for feature selection in conditional maximum entropy modeling.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>