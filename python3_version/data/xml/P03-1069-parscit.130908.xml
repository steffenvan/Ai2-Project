<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005319">
<title confidence="0.977523">
Probabilistic Text Structuring: Experiments with Sentence Ordering
</title>
<author confidence="0.993877">
Mirella Lapata
</author>
<affiliation confidence="0.997717">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.914566">
Regent Court, 211 Portobello Street
Sheffield S1 4DP, UK
</address>
<email confidence="0.998498">
mlap@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.99389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997905625">
Ordering information is a critical task for
natural language generation applications.
In this paper we propose an approach to
information ordering that is particularly
suited for text-to-text generation. We de-
scribe a model that learns constraints on
sentence order from a corpus of domain-
specific texts and an algorithm that yields
the most likely order among several al-
ternatives. We evaluate the automatically
generated orderings against authored texts
from our corpus and against human sub-
jects that are asked to mimic the model’s
task. We also assess the appropriateness of
such a model for multidocument summa-
rization.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985653061225">
Structuring a set of facts into a coherent text is a
non-trivial task which has received much attention
in the area of concept-to-text generation (see Reiter
and Dale 2000 for an overview). The structured text
is typically assumed to be a tree (i.e., to have a hier-
archical structure) whose leaves express the content
being communicated and whose nodes specify how
this content is grouped via rhetorical or discourse re-
lations (e.g., contrast, sequence, elaboration).
For domains with large numbers of facts and
rhetorical relations, there can be more than one pos-
sible tree representing the intended content. These
different trees will be realized as texts with different
sentence orders or even paragraph orders and differ-
ent levels of coherence. Finding the tree that yields
the best possible text is effectively a search prob-
lem. One way to address it is by narrowing down
the search space either exhaustively or heuristically.
Marcu (1997) argues that global coherence can be
achieved if constraints on local coherence are sat-
isfied. The latter are operationalized as weights on
the ordering and adjacency of facts and are derived
from a corpus of naturally occurring texts. A con-
straint satisfaction algorithm is used to find the tree
with maximal weights from the space of all possi-
ble trees. Mellish et al. (1998) advocate stochastic
search as an alternative to exhaustively examining
the search space. Rather than requiring a global op-
timum to be found, they use a genetic algorithm to
select a tree that is coherent enough for people to
understand (local optimum).
The problem of finding an acceptable order-
ing does not arise solely in concept-to-text gener-
ation but also in the emerging field of text-to-text
generation (Barzilay, 2003). Examples of applica-
tions that require some form of text structuring, are
single- and multidocument summarization as well as
question answering. Note that these applications do
not typically assume rich semantic knowledge orga-
nized in tree-like structures or communicative goals
as is often the case in concept-to-text generation. Al-
though in single document summarization the posi-
tion of a sentence in a document can provide cues
with respect to its ordering in the summary, this is
not the case in multidocument summarization where
sentences are selected from different documents and
must be somehow ordered so as to produce a coher-
ent summary (Barzilay et al., 2002). Answering a
question may also involve the extraction, potentially
summarization, and ordering of information across
multiple information sources.
Barzilay et al. (2002) address the problem of
information ordering in multidocument summariza-
tion and show that naive ordering algorithms such
as majority ordering (selects most frequent orders
across input documents) and chronological ordering
(orders facts according to publication date) do not
always yield coherent summaries although the latter
produces good results when the information is event-
based. Barzilay et al. further conduct a study where
subjects are asked to produce a coherent text from
the output of a multidocument summarizer. Their re-
sults reveal that although the generated orders differ
from subject to subject, topically related sentences
always appear together. Based on the human study
they propose an algorithm that first identifies top-
ically related groups of sentences and then orders
them according to chronological information.
In this paper we introduce an unsupervised
probabilistic model for text structuring that learns
ordering constraints from a large corpus. The model
operates on sentences rather than facts in a knowl-
edge base and is potentially useful for text-to-text
generation applications. For example, it can be used
to order the sentences obtained from a multidocu-
ment summarizer or a question answering system.
Sentences are represented by a set of informative
features (e.g., a verb and its subject, a noun and its
modifier) that can be automatically extracted from
the corpus without recourse to manual annotation.
The model learns which sequences of features
are likely to co-occur and makes predictions con-
cerning preferred orderings. Local coherence is thus
operationalized by sentence proximity in the train-
ing corpus. Global coherence is obtained by greedily
searching through the space of possible orders. As in
the case of Mellish et al. (1998) we construct an ac-
ceptable ordering rather than the best possible one.
We propose an automatic method of evaluating the
orders generated by our model by measuring close-
ness or distance from the gold standard, a collection
of orders produced by humans.
The remainder of this paper is organized as fol-
lows. Section 2 introduces our model and an algo-
rithm for producing a possible order. Section 3 de-
scribes our corpus and the estimation of the model
parameters. Our experiments are detailed in Sec-
tion 4. We conclude with a discussion in Section 5.
</bodyText>
<sectionHeader confidence="0.712616" genericHeader="method">
2 Learning to Order
</sectionHeader>
<bodyText confidence="0.999906888888889">
Given a collection of texts from a particular domain,
our task is to learn constraints on the ordering of
their sentences. In the training phase our model will
learn these constraints from adjacent sentences rep-
resented by a set of informative features. In the test-
ing phase, given a set of unseen sentences, we will
rely on our prior experience of how sentences are
usually ordered for choosing the most likely order-
ing.
</bodyText>
<subsectionHeader confidence="0.76889">
2.1 The Model
</subsectionHeader>
<bodyText confidence="0.9978005">
We express the probability of a text made up of sen-
tences S1...Sn as shown in (1). According to (1), the
task of predicting the next sentence is dependent on
its n − i previous sentences.
</bodyText>
<equation confidence="0.9937478">
(1) P(T) = P(S1 ...Sn)
= P(S1)P(S2|S1)P(S3|S1,S2)...P(Sn|S1 ... Sn−1)
= n P(Sn|S1 ...Sn−i)
∏
i=1
</equation>
<bodyText confidence="0.999947863636363">
We will simplify (1) by assuming that the prob-
ability of any given sentence is determined only by
its previous sentence:
This is a somewhat simplistic attempt at cap-
turing Marcu’s (1997) local coherence constraints as
well as Barzilay et al.’s (2002) observations about
topical relatedness. While this is clearly a naive view
of text coherence, our model has some notion of the
types of sentences that typically go together, even
though it is agnostic about the specific rhetorical re-
lations that glue sentences into a coherent text. Also
note that the simplification in (2) will make the es-
timation of the probabilities P(Si|Si−1) more reli-
able in the face of sparse data. Of course estimat-
ing P(Si|Si−1) would be impossible if Si and Si−1
were actual sentences. It is unlikely to find the ex-
act same sentence repeated several times in a corpus.
What we can find and count is the number of times
a given structure or word appears in the corpus. We
will therefore estimate P(Si|Si−1) from features that
express its structure and content (these features are
described in detail in Section 3):
</bodyText>
<equation confidence="0.9980795">
(3) P(Si|Si−1) =
P(ai,1,ai,2 ...ai,n|ai−1,1,ai−1,2 ...ai−1,m)
</equation>
<bodyText confidence="0.992739666666667">
where ai,1,ai,2 ...ai,n are features relevant for
sentence Si and ai−1,1,ai−1,2 ...ai−1,m for sen-
tence Si−1. We will assume that these features are
independent and that P(Si|Si−1) can be estimated
from the pairs in the Cartesian product defined
over the features expressing sentences Si and Si−1:
</bodyText>
<equation confidence="0.9978">
(ai,j,ai−1,k)  Si × Si−1. Under these assumptions
P(Si|Si−1) can be written as follows:
(4) P(Si|Si−1) = P(ai,1|ai−1,1) ... P(ai,n Iai−1,m)
(ai,j,ai−1,k)Si×Si−1
∏ P(ai,jJai−1,k)
</equation>
<bodyText confidence="0.99996175">
Assuming that the features are independent
again makes parameter estimation easier. The Carte-
sian product over the features in Si and Si−1 is an at-
tempt to capture inter-sentential dependencies. Since
</bodyText>
<equation confidence="0.995580142857143">
(2) P(T) = P(S1)P(S2|S1)P(S3|S2)...P(Sn|Sn−1)
= n P(Si|Si−1)
∏
i=1
S1 : a b c d
S2 : e f g
S3 : h i
</equation>
<figureCaption confidence="0.9982">
Figure 1: Example of probability estimation
</figureCaption>
<bodyText confidence="0.999906">
we don’t know a priori what the important feature
combinations are, we are considering all possible
combinations over two sentences. This will admit-
tedly introduce some noise, given that some depen-
dencies will be spurious, but the model can be easily
retrained for different domains for which different
feature combinations will be important. The proba-
</bodyText>
<equation confidence="0.914544">
bility P(ai,j|ai−1,k) is estimated as:
(5) P(ai, j |ai−1_ f(ai,j,ai−1,k)
k) ∑ f(ai,j,ai−1,k)
ai,j
</equation>
<bodyText confidence="0.999651285714286">
where f(ai,j,ai−1,k) is the number of times fea-
ture ai,j is preceded by feature ai−1,k in the
corpus. The denominator expresses the number of
times ai−1,k is attested in the corpus (preceded
by any feature). The probabilities P(ai,j|ai−1,k)
will be unreliable when the frequency estimates for
f (ai, j,ai−1,k) are small, and undefined in cases
where the feature combinations are unattested in the
corpus. We therefore smooth the observed frequen-
cies using back-off smoothing (Katz, 1987).
To illustrate with an example consider the text
in Figure 1 which has three sentences S1, S2, S3,
each represented by their respective features denoted
by letters. The probability P(S3|S2) will be calcu-
lated by taking the product of P(h|e), P(h |f), P(h|g),
P(i|e), P(i |f), and P(i|g). To obtain P(h|e), we need
f(h,e) and f(e) which can be estimated in Figure 1
by counting the number of edges connecting e and
h and the number of edges starting from e, respec-
tively. So, P(h|e) will be 0.16 given that f(h,e) is
one and f(e) is six (see the normalization in (5)).
</bodyText>
<subsectionHeader confidence="0.999571">
2.2 Determining an Order
</subsectionHeader>
<bodyText confidence="0.999996555555556">
Once we have collected the counts for our features
we can determine the order for a new text that
we haven’t encountered before, since some of the
features representing its sentences will be familiar.
Given a text with N sentences there are N! possi-
ble orders. The set of orders can be represented as a
complete graph, where the set of vertices V is equal
to the set of sentences S and each edge u  v has
a weight, the probability P(u|v). Cohen et al. (1999)
</bodyText>
<equation confidence="0.986250111111111">
START
�
������ � � � � �
S1 S2 S3
(0.3) (0.05)
�� � �
3 S1 S3
(0.006) (0.02)
S3 S1
</equation>
<figureCaption confidence="0.990455">
Figure 2: Finding an order for a three sentence text
</figureCaption>
<bodyText confidence="0.999876375">
show that the problem of finding an optimal ordering
through a directed weighted graph is NP-complete.
Fortunately, they propose a simple greedy algorithm
that provides an approximate solution which can be
easily modified for our task (see also Barzilay et al.
2002).
The algorithm starts by assigning each vertex
v  V a probability. Recall that in our case vertices
are sentences and their probabilities can be calcu-
lated by taking the product of the probabilities of
their features. The greedy algorithm then picks the
node with the highest probability and orders it ahead
of the other nodes. The selected node and its incident
edges are deleted from the graph. Each remaining
node is now assigned the conditional probability of
seeing this node given the previously selected node
(see (4)). The node which yields the highest condi-
tional probability is selected and ordered ahead. The
process is repeated until the graph is empty.
As an example consider again a three sentence
text. We illustrate the search for a path through the
graph in Figure 2. First we calculate which of the
three sentences S1, S2, and S3 is most likely to start
the text (during training we record which sentences
appear in the beginning of each text). Assuming that
P(S2|START) is the highest, we will order S2 first,
and ignore the nodes headed by S1 and S3. We next
compare the probabilities P(S1|S2) and P(S3|S2).
Since P(S3|S2) is more likely than P(S1|S2), we or-
der S3 after S2 and stop, returning the order S2, S3,
and S1. As can be seen in Figure 2 for each vertex
we keep track of the most probable edge that ends in
that vertex, thus setting th beam search width to one.
Note, that equation (4) would assign lower and
lower probabilities to sentences with large numbers
of features. Since we need to compare sentence pairs
with varied numbers of features, we will normalize
the conditional probabilities P(Si|Si−1) by the num-
ber feature of pairs that form the Cartesian product
over Si and Si−1.
</bodyText>
<figure confidence="0.957194142857143">
(0.2)
�
S2
S3
S2
2 1
S1 S2
</figure>
<listItem confidence="0.68794475">
1. Laidlaw Transportation Ltd. said shareholders will be asked at its Dec. 7 annual meeting to approve a change of name to
Laidlaw Inc.
2. The company said its existing name hasn’t represented its businesses since the 1984 sale of its trucking operations.
3. Laidlaw is a waste management and school-bus operator, in which Canadian Pacific Ltd. has a 47% voting interest.
</listItem>
<figureCaption confidence="0.959278">
Figure 3: A text from the BLLIP corpus
</figureCaption>
<sectionHeader confidence="0.973318" genericHeader="method">
3 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999956905797102">
The model in Section 2.1 was trained on the BLLIP
corpus (30 M words), a collection of texts from the
Wall Street Journal (years 1987-89). The corpus con-
tains 98,732 stories. The average story length is 19.2
sentences. 71.30% of the texts in the corpus are less
than 50 sentences long. An example of the texts in
this newswire corpus is shown in Figure 3.
The corpus is distributed in a Treebank-
style machine-parsed version which was produced
with Charniak’s (2000) parser. The parser is a
“maximum-entropy inspired” probabilistic gener-
ative model. It achieves 90.1% average preci-
sion/recall for sentences with maximum length 40
and 89.5% for sentences with maximum length 100
when trained and tested on the standard sections
of the Wall Street Journal Treebank (Marcus et al.,
1993).
We also obtained a dependency-style version
of the corpus using MINIPAR (Lin, 1998) a broad
coverage parser for English which employs a manu-
ally constructed grammar and a lexicon derived from
WordNet with an additional dictionary of proper
names (130,000 entries in total). The grammar is
represented as a network of 35 nodes (i.e., grammat-
ical categories) and 59 edges (i.e., types of syntactic
(dependency) relations). The output of MINIPAR is a
dependency graph which represents the dependency
relations between words in a sentence (see Table 1
for an example). Lin (1998) evaluated the parser on
the SUSANNE corpus (Sampson, 1996), a domain in-
dependent corpus of British English, and achieved a
recall of 79% and precision of 89% on the depen-
dency relations.
From the two different parsed versions of the
BLLIP corpus the following features were extracted:
Verbs. Investigations into the interpretation of nar-
rative discourse (Asher and Lascarides, 2003) have
shown that specific lexical information (e.g., verbs,
adjectives) plays an important role in determining
the discourse relations between propositions. Al-
though we don’t have an explicit model of rhetorical
relations and their effects on sentence ordering, we
capture the lexical inter-dependencies between sen-
tences by focusing on verbs and their precedence re-
lationships in the corpus.
From the Treebank parses we extracted the
verbs contained in each sentence. We obtained
two versions of this feature: (a) a lemmatized ver-
sion where verbs were reduced to their base forms
and (b) a non-lemmatized version which preserved
tense-related information; more specifically, verbal
complexes (e.g., I will have been going) were iden-
tified from the parse trees heuristically by devis-
ing a set of 30 patterns that search for sequences
of modals, auxiliaries and verbs. This is an attempt
at capturing temporal coherence by encoding se-
quences of events and their morphology which in-
directly indicates their tense.
To give an example consider the text in Fig-
ure 3. For the lemmatized version, sentence (1) will
be represented by say, will, be, ask, and approve; for
the tensed version, the relevant features will be said,
will be asked, and to approve.
Nouns. Centering Theory (CT, Grosz et al. 1995)
is an entity-based theory of local coherence, which
claims that certain entities mentioned in an utterance
are more central than others and that this property
constrains a speaker’s use of certain referring ex-
pressions. The principles underlying CT (e.g., conti-
nuity, salience) are of interest to concept-to-text gen-
eration as they offer an entity-based model of text
and sentence planning which is particularly suited
for descriptional genres (Kibble and Power, 2000).
We operationalize entity-based coherence for
text-to-text generation by simply keeping track of
the nouns attested in a sentence without however
taking personal pronouns into account. This simpli-
fication is reasonable if one has text-to-text genera-
tion mind. In multidocument summarization for ex-
ample, sentences are extracted from different docu-
ments; the referents of the pronouns attested in these
sentences are typically not known and in some cases
identical pronouns may refer to different entities. So
making use of noun-pronoun or pronoun-pronoun
co-occurrences will be uninformative or in fact mis-
leading.
We extracted nouns from a lemmatized version
of the Treebank-style parsed corpus. In cases of noun
compounds, only the compound head (i.e., rightmost
noun) was taken into account. A small set of rules
was used to identify organizations (e.g., United Lab-
oratories Inc.), person names (e.g., Jose Y. Cam-
pos), and locations (e.g., New England) spanning
more than one word. These were grouped together
and were also given the general categories person,
organization, and location. The model backs off
to these categories when unknown person names, lo-
cations, and organizations are encountered. Dates,
years, months and numbers were substituted by the
categories date, year, month, and number.
In sentence (1) (see Figure 3) we identify
the nouns Laidlaw Transportation Ltd., shareholder,
Dec 7, meeting, change, name and Laidlaw Inc. In
sentence (2) the relevant nouns are company, name,
business, 1984, sale, and operation.
Dependencies. Note that the noun and verb fea-
tures do not capture the structure of the sentences
to be ordered. This is important for our domain, as
texts seem to be rather formulaic and similar syn-
tactic structures are often used (e.g., direct and in-
direct speech, restrictive relative clauses, predicative
structures). In this domain companies typically say
things, and texts often begin with a statement of what
a company or an individual has said (see sentence (1)
in Figure 3). Furthermore, companies and individu-
als are described with certain attributes (persons can
be presidents or governors, companies are bankrupt
or manufacturers, etc.) that can give clues for infer-
ring coherence.
The dependencies were obtained from the out-
put of MINIPAR. Some of the dependencies for sen-
tence (2) from Figure 3 are shown in Table 1. The
dependencies capture structural as well lexical infor-
mation. They are represented as triples, consisting of
a head (leftmost element, e.g., say, name), a modi-
fier (rightmost element, e.g., company, its) and a re-
lation (e.g., subject (V:subj:N), object (V:obj:N),
modifier (N:mod:A)).
For efficiency reasons we focused on triples
whose dependency relations (e.g., V:subj:N) were
attested in the corpus with frequency larger than
one per million. We further looked at how individ-
ual types of relations contribute to the ordering task.
More specifically we experimented with dependen-
cies relating to verbs (49 types), nouns (52 types),
verbs and nouns (101 types) (see Table 1 for exam-
ples). We also ran a version of our model with all
types of relations, including adjectives, adverbs and
</bodyText>
<tableCaption confidence="0.971988">
Table 1: Dependencies for sentence (2) in Figure 3
</tableCaption>
<table confidence="0.988959">
A B C D E F G H I J
Model 1 1 2 3 4 5 6 7 8 9 10
Model 2 2 1 5 3 4 6 7 9 8 10
Model 3 10 2 3 4 5 6 7 8 9 1
</table>
<tableCaption confidence="0.896739">
Table 2: Example of rankings for a 10 sentence text
prepositions (147 types in total).
</tableCaption>
<sectionHeader confidence="0.995217" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999981692307692">
In this section we describe our experiments with the
model and the features introduced in the previous
sections. We first evaluate the model by attempting
to reproduce the structure of unseen texts from the
BLLIP corpus, i.e., the corpus on which the model
is trained on. We next obtain an upper bound for the
task by conducting a sentence ordering experiment
with humans and comparing the model against the
human data. Finally, we assess whether this model
can be used for multi-document summarization us-
ing data from Barzilay et al. (2002). But before we
outline the details of our experiments we discuss our
choice of metric for comparing different orders.
</bodyText>
<subsectionHeader confidence="0.98359">
4.1 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.99987075">
Our task is to produce an ordering for the sentences
of a given text. We can think of the sentences as
objects for which a ranking must be produced. Ta-
ble 2 gives an example of a text containing 10 sen-
tences (A–J) and the orders (i.e., rankings) produced
by three hypothetical models.
A number of metrics can be used to measure
the distance between two rankings such as Spear-
man’s correlation coefficient for ranked data, Cayley
distance, or Kendall’s τ (see Lebanon and Lafferty
2002 for details). Kendall’s τ is based on the number
of inversions in the rankings and is defined in (6):
</bodyText>
<equation confidence="0.670870333333333">
_
2(number of inversions)
(6) τ 1−N(N −1)/2
</equation>
<bodyText confidence="0.999646">
where N is the number of objects (i.e., sentences)
being ranked and inversions are the number of in-
terchanges of consecutive elements necessary to ar-
range them in their natural order. If we think in terms
</bodyText>
<figure confidence="0.994225545454546">
Verb
say V:subj:N company
represent V:subj:N name
represent V:have:have have
represent V:obj:N business
Noun
name N:gen:N its
name N:mod:A existing
business N:gen:N its
business N:mod:Prep since
company N:det:Det the
</figure>
<bodyText confidence="0.999942111111111">
of permutations, then i can be interpreted as the min-
imum number of adjacent transpositions needed to
bring one order to the other. In Table 2 the number
of inversions can be calculated by counting the num-
ber of intersections of the lines. The metric ranges
from −1 (inverse ranks) to 1 (identical ranks). The i
for Model 1 and Model 2 in Table 2 is .822.
Kendall’s i seems particularly appropriate for
the tasks considered in this paper. The metric is sen-
sitive to the fact that some sentences may be always
ordered next to each other even though their absolute
orders might differ. It also penalizes inverse rank-
ings. Comparison between Model 1 and Model 3
would give a i of 0.244 even though the orders be-
tween the two models are identical modulo the be-
ginning and the end. This seems appropriate given
that flipping the introduction in a document with the
conclusions seriously disrupts coherence.
</bodyText>
<subsectionHeader confidence="0.962674">
4.2 Experiment 1: Ordering Newswire Texts
</subsectionHeader>
<bodyText confidence="0.985035575757576">
The model from Section 2.1 was trained on the
BLLIP corpus and tested on 20 held-out randomly
selected unseen texts (average length 15.3). We also
used 20 randomly chosen texts (disjoint from the
test data) for development purposes (average length
16.2). All our results are reported on the test set.
The input to the the greedy algorithm (see Sec-
tion 2.2) was a text with a randomized sentence or-
dering. The ordered output was compared against
the original authored text using i. Table 3 gives the
average i (T) for all 20 test texts when the fol-
lowing features are used: lemmatized verbs (VL),
tensed verbs (VT), lemmatized nouns (NL), lem-
matized verbs and nouns (VLNL), tensed verbs and
lemmatized nouns (VTNL), verb-related dependen-
cies (VD), noun-related dependencies (ND), verb and
noun dependencies (VDND), and all available de-
pendencies (AD). For comparison we also report the
naive baseline of generating a random oder (BR). As
can be seen from Table 3 the best performing fea-
tures are NL and VDND. This is not surprising given
that NL encapsulates notions of entity-based coher-
ence, which is relatively important for our domain. A
lot of texts are about a particular entity (company or
individual) and their properties. The feature VDND
subsumes several other features and does expectedly
better: it captures entity-based coherence, the inter-
relations among verbs, the structure of sentences and
also preserves information about argument structure
(who is doing what to whom). The distance between
the orders produced by the model and the original
texts increases when all types of dependencies are
Feature T StdDev Min Max
</bodyText>
<tableCaption confidence="0.9579395">
Table 3: Comparison between original BLLIP texts
and model generated variants
</tableCaption>
<bodyText confidence="0.99976625">
taken into account. The feature space becomes too
big, there are too many spurious feature pairs, and
the model can’t distinguish informative from non-
informative features.
We carried out a one-way Analysis of Vari-
ance (ANOVA) to examine the effect of different fea-
ture types. The ANOVA revealed a reliable effect
of feature type (F(9,171) = 3.31; p &lt; 0.01). We
performed Post-hoc Tukey tests to further examine
whether there are any significant differences among
the different features and between our model and
the baseline. We found out that NL, VTNL, VD, and
VDND are significantly better than BR (a = 0.01),
whereas NL and VDND are not significantly differ-
ent from each other. However, they are significantly
better than all other features (a = 0.05).
</bodyText>
<subsectionHeader confidence="0.998448">
4.3 Experiment 2: Human Evaluation
</subsectionHeader>
<bodyText confidence="0.999900142857143">
In this experiment we compare our model’s perfor-
mance against human judges. Twelve texts were ran-
domly selected from the 20 texts in our test data. The
texts were presented to subjects with the order of
their sentences scrambled. Participants were asked
to reorder the sentences so as to produce a coherent
text. Each participant saw three texts randomly cho-
sen from the pool of 12 texts. A random order of sen-
tences was generated for every text the participants
saw. Sentences were presented verbatim, pronouns
and connectives were retained in order to make or-
dering feasible. Notice that this information is absent
from the features the model takes into account.
The study was conducted remotely over the In-
ternet using a variant of Barzilay et al.’s (2002) soft-
ware. Subjects first saw a set of instructions that ex-
plained the task, and had to fill in a short question-
naire including basic demographic information. The
experiment was completed by 137 volunteers (ap-
proximately 33 per text), all native speakers of En-
glish. Subjects were recruited via postings to local
</bodyText>
<table confidence="0.994447625">
BR .35 .09 .17 .47
VL .44 .24 .17 .93
VT .46 .21 .17 .80
NL .54 .16 .18 .76
VLNL .46 .12 .18 .61
.49 .17 .21 .86
.51 .17 .10 .83
.45 .17 .10 .67
.57 .12 .62 .83
.48 .17 .10 .83
VTNL
VD
ND
VDND
AD
Feature T StdDev Min Max
</table>
<tableCaption confidence="0.999433">
Table 4: Comparison between orderings produced by
humans and the model on BLLIP texts
</tableCaption>
<table confidence="0.9993846">
Features T StdDev Min Max
BR .43 .13 .19 .97
NL .48 .16 .21 .86
VDND .56 .13 .32 .86
HH .60 .17 −1 .98
</table>
<tableCaption confidence="0.7040422">
Table 5: Comparison between orderings produced by
humans and the model on multidocument summaries
Usenet newsgroups.
Table 4 reports pairwise i averaged over
12 texts for all participants (HH) and the average i
</tableCaption>
<bodyText confidence="0.990561384615385">
between the model and each of the subjects for all
features used in Experiment 1. The average distance
in the orderings produced by our subjects is .58. The
distance between the humans and the best features
is .51 for NL and .55 for VDND. An ANOVA yielded
a significant effect of feature type (F(9,99) = 5.213;
p &lt; 0.01). Post-hoc Tukey tests revealed that VL,
VT, VD, ND, AD, VLNL, and VTNL perform sig-
nificantly worse than HH (a = 0.01), whereas NL
and VDND are not significantly different from HH
(a = 0.01). This is in agreement with Experiment 1
and points to the importance of lexical and structural
information for the ordering task.
</bodyText>
<subsectionHeader confidence="0.997329">
4.4 Experiment 3: Summarization
</subsectionHeader>
<bodyText confidence="0.999982280701754">
Barzilay et al. (2002) collected a corpus of multiple
orderings in order to study what makes an order co-
hesive. Their goal was to improve the ordering strat-
egy of MULTIGEN (McKeown et al., 1999) a mul-
tidocument summarization system that operates on
news articles describing the same event. MULTIGEN
identifies text units that convey similar information
across documents and clusters them into themes.
Each theme is next syntactically analysed into pred-
icate argument structures; the structures that are re-
peated often enough are chosen to be included into
the summary. A language generation system outputs
a sentence (per theme) from the selected predicate
argument structures.
Barzilay et al. (2002) collected ten sets of arti-
cles each consisting of two to three articles reporting
the same event and simulated MULTIGEN by man-
ually selecting the sentences to be included in the
final summary. This way they ensured that order-
ings were not influenced by mistakes their system
could have made. Explicit references and connec-
tives were removed from the sentences so as not to
reveal clues about the sentence ordering. Ten sub-
jects provided orders for each summary which had
an average length of 8.8.
We simulated the participants’ task by using the
model from Section 2.1 to produce an order for each
candidate summary1. We then compared the differ-
ences in the orderings generated by the model and
participants using the best performing features from
Experiment 2 (i.e., NL and VDND). Note that the
model was trained on the BLLIP corpus, whereas the
sentences to be ordered were taken from news arti-
cles describing the same event. Not only were the
news articles unseen but also their syntactic struc-
ture was unfamiliar to the model. The results are
shown in table 5, again average pairwise i is re-
ported. We also give the naive baseline of choosing
a random order (BR). The average distance in the
orderings produced by Barzilay et al.’s (2002) par-
ticipants is .60. The distance between the humans
and NL is .48 whereas the average distance between
VDND and the humans is .56. An ANOVA yielded a
significant effect of feature type (F(3,27) = 15.25;
p &lt; 0.01). Post-hoc Tukey tests showed that VDND
was significantly better than BR, but NL wasn’t. The
difference between VDND and HH was not signifi-
cant.
Although NL performed adequately in Experi-
ments 1 and 2, it failed to outperform the baseline in
the summarization task. This may be due to the fact
that entity-based coherence is not as important as
temporal coherence for the news articles summaries.
Recall that the summaries describe events across
documents. This information is captured more ad-
equately by VDND and not by NL that only keeps a
record of the entities in the sentence.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.992127">
In this paper we proposed a data intensive approach
to text coherence where constraints on sentence or-
dering are learned from a corpus of domain-specific
</bodyText>
<footnote confidence="0.9935255">
1The summaries as well as the human data are available from
http://www.cs.columbia.edu/˜noemie/ordering/.
</footnote>
<table confidence="0.8634516">
VL .45 .16 .10 .90
VT .46 .18 .10 .90
NL .51 .14 .10 .90
VLNL .44 .14 .18 .61
VTNL .49 .18 .21 .86
.47 .14 .10 .93
.46 .15 .10 .86
.55 .15 .10 .90
.48 .16 .10 .83
.58 .08 .26 .75
VD
ND
VDND
AD
HH
</table>
<bodyText confidence="0.999967244444444">
texts. We experimented with different feature encod-
ings and showed that lexical and syntactic informa-
tion is important for the ordering task. Our results
indicate that the model can successfully generate or-
ders for texts taken from the corpus on which it is
trained. The model also compares favorably with hu-
man performance on a single- and multiple docu-
ment ordering task.
Our model operates on the surface level rather
than the logical form and is therefore suitable for
text-to-text generation systems; it acquires ordering
constraints automatically, and can be easily ported to
different domains and text genres. The model is par-
ticularly relevant for multidocument summarization
since it could provide an alternative to chronolog-
ical ordering especially for documents where pub-
lication date information is unavailable or uninfor-
mative (e.g., all documents have the same date). We
proposed Kendall’s τ as an automated method for
evaluating the generated orders.
There are a number of issues that must be ad-
dressed in future work. So far our evaluation metric
measures order similarities or dissimilarities. This
enables us to assess the importance of particular
feature combinations automatically and to evaluate
whether the model and the search algorithm gener-
ate potentially acceptable orders without having to
run comprehension experiments each time. Such ex-
periments however are crucial for determining how
coherent the generated texts are and whether they
convey the same semantic content as the originally
authored texts. For multidocument summarization
comparisons between our model and alternative or-
dering strategies are important if we want to pursue
this approach further.
Several improvements can take place with re-
spect to the model. An obvious question is whether
a trigram model performs better than the model
presented here. The greedy algorithm implements
a search procedure with a beam of width one. In
the future we plan to experiment with larger widths
(e.g., two or three) and also take into account fea-
tures that express semantic similarities across docu-
ments either by relying on WordNet or on automatic
clustering methods.
</bodyText>
<sectionHeader confidence="0.998949" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996063166666667">
The author was supported by EPSRC grant number R40036. We
are grateful to Regina Barzilay and Noemie Elhadad for making
available their software and for providing valuable comments
on this work. Thanks also to Stephen Clark, Nikiforos Kara-
manis, Frank Keller, Alex Lascarides, Katja Markert, and Miles
Osborne for helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.995726" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999850655172414">
Asher, Nicholas and Alex Lascarides. 2003. Logics of Conver-
sation. Cambridge University Press.
Barzilay, Regina. 2003. Information Fusion for Multi-
Document Summarization: Praphrasing and Generation.
Ph.D. thesis, Columbia University.
Barzilay, Regina, Noemie Elhadad, and Kathleen R. McKeown.
2002. Inferring strategies for sentence ordering in multidoc-
ument news summarization. Journal ofArtificial Intelligence
Research 17:35–55.
Charniak, Eugene. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Conference of the North American
Chapter of the Association for Computational Linguistics.
Seattle, WA, pages 132–139.
Cohen, William W., Robert E. Schapire, and Yoram Singer.
1999. Learning to order things. Journal of Artificial Intelli-
gence Research 10:243–270.
Grosz, Barbara, Aravind Joshi, , and Scott Weinstein. 1995.
Centering: A framework for modeling the local coherence
of discourse. Computational Linguistics 21(2):203–225.
Katz, Slava M. 1987. Estimation of probabilities from sparse
data for the language model component of a speech recog-
nizer. IEEE Transactions on Acoustics Speech and Signal
Processing 33(3):400–401.
Kibble, Rodger and Richard Power. 2000. An integrated frame-
work for text planning and pronominalisation. In In Pro-
ceedings of the 1st International Conference on Natural Lan-
guage Generation. Mitzpe Ramon, Israel, pages 77–84.
Lebanon, Guy and John Lafferty. 2002. Combining rankings
using conditional probability models on permutations. In
C. Sammut and A. Hoffmann, editors, In Proceedings of the
19th International Conference on Machine Learning. Mor-
gan Kaufmann Publishers, San Francisco, CA.
Lin, Dekang. 1998. Dependency-based evaluation of MINIPAR.
In In Proceedings on of the LREC Workshop on the Evalua-
tion of Parsing Systems. Granada, pages 48–56.
Marcu, Daniel. 1997. From local to global coherence: A
bottom-up approach to text planning. In In Proceedings of
the 14th National Conference on Artificial Intelligence. Prov-
idence, Rhode Island, pages 629–635.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of english: The penn treebank. Computational Linguistics
19(2):313–330.
McKeown, Kathleen R., Judith L. Klavans, Vasileios Hatzivas-
siloglou, Regina Barzilay, and Eleazar Eskin. 1999. Towards
multidocument summarization by reformulation: Progress
and prospects. In Proceedings of the 16th National Confer-
ence on Artificial Intelligence. Orlando, FL, pages 453–459.
Mellish, Chris, Alistair Knott, Jon Oberlander, and Mick O’
Donnell. 1998. Experiments using stochastic search for text
planning. In In Proceedings of the 9th International Work-
shop on Natural Language Generation. Ontario, Canada,
pages 98–107.
Reiter, Ehud and Robert Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University Press,
Cambridge.
Sampson, Geoffrey. 1996. English for the Computer. Oxford
University Press.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.754919">
<title confidence="0.999636">Probabilistic Text Structuring: Experiments with Sentence Ordering</title>
<author confidence="0.999702">Mirella Lapata</author>
<affiliation confidence="0.9998875">Department of Computer Science University of Sheffield</affiliation>
<address confidence="0.975584">Regent Court, 211 Portobello Street Sheffield S1 4DP, UK</address>
<email confidence="0.997892">mlap@dcs.shef.ac.uk</email>
<abstract confidence="0.987969705882353">Ordering information is a critical task for natural language generation applications. In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation. We describe a model that learns constraints on sentence order from a corpus of domainspecific texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model’s task. We also assess the appropriateness of such a model for multidocument summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="14895" citStr="Asher and Lascarides, 2003" startWordPosition="2430" endWordPosition="2433">, grammatical categories) and 59 edges (i.e., types of syntactic (dependency) relations). The output of MINIPAR is a dependency graph which represents the dependency relations between words in a sentence (see Table 1 for an example). Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson, 1996), a domain independent corpus of British English, and achieved a recall of 79% and precision of 89% on the dependency relations. From the two different parsed versions of the BLLIP corpus the following features were extracted: Verbs. Investigations into the interpretation of narrative discourse (Asher and Lascarides, 2003) have shown that specific lexical information (e.g., verbs, adjectives) plays an important role in determining the discourse relations between propositions. Although we don’t have an explicit model of rhetorical relations and their effects on sentence ordering, we capture the lexical inter-dependencies between sentences by focusing on verbs and their precedence relationships in the corpus. From the Treebank parses we extracted the verbs contained in each sentence. We obtained two versions of this feature: (a) a lemmatized version where verbs were reduced to their base forms and (b) a non-lemma</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Asher, Nicholas and Alex Lascarides. 2003. Logics of Conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
</authors>
<title>Information Fusion for MultiDocument Summarization: Praphrasing and Generation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="2610" citStr="Barzilay, 2003" startWordPosition="409" endWordPosition="410">ved from a corpus of naturally occurring texts. A constraint satisfaction algorithm is used to find the tree with maximal weights from the space of all possible trees. Mellish et al. (1998) advocate stochastic search as an alternative to exhaustively examining the search space. Rather than requiring a global optimum to be found, they use a genetic algorithm to select a tree that is coherent enough for people to understand (local optimum). The problem of finding an acceptable ordering does not arise solely in concept-to-text generation but also in the emerging field of text-to-text generation (Barzilay, 2003). Examples of applications that require some form of text structuring, are single- and multidocument summarization as well as question answering. Note that these applications do not typically assume rich semantic knowledge organized in tree-like structures or communicative goals as is often the case in concept-to-text generation. Although in single document summarization the position of a sentence in a document can provide cues with respect to its ordering in the summary, this is not the case in multidocument summarization where sentences are selected from different documents and must be someh</context>
</contexts>
<marker>Barzilay, 2003</marker>
<rawString>Barzilay, Regina. 2003. Information Fusion for MultiDocument Summarization: Praphrasing and Generation. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization.</title>
<date>2002</date>
<journal>Journal ofArtificial Intelligence Research</journal>
<pages>17--35</pages>
<contexts>
<context position="3280" citStr="Barzilay et al., 2002" startWordPosition="513" endWordPosition="516">m of text structuring, are single- and multidocument summarization as well as question answering. Note that these applications do not typically assume rich semantic knowledge organized in tree-like structures or communicative goals as is often the case in concept-to-text generation. Although in single document summarization the position of a sentence in a document can provide cues with respect to its ordering in the summary, this is not the case in multidocument summarization where sentences are selected from different documents and must be somehow ordered so as to produce a coherent summary (Barzilay et al., 2002). Answering a question may also involve the extraction, potentially summarization, and ordering of information across multiple information sources. Barzilay et al. (2002) address the problem of information ordering in multidocument summarization and show that naive ordering algorithms such as majority ordering (selects most frequent orders across input documents) and chronological ordering (orders facts according to publication date) do not always yield coherent summaries although the latter produces good results when the information is eventbased. Barzilay et al. further conduct a study where</context>
<context position="10973" citStr="Barzilay et al. 2002" startWordPosition="1768" endWordPosition="1771">N! possible orders. The set of orders can be represented as a complete graph, where the set of vertices V is equal to the set of sentences S and each edge u  v has a weight, the probability P(u|v). Cohen et al. (1999) START � ������ � � � � � S1 S2 S3 (0.3) (0.05) �� � � 3 S1 S3 (0.006) (0.02) S3 S1 Figure 2: Finding an order for a three sentence text show that the problem of finding an optimal ordering through a directed weighted graph is NP-complete. Fortunately, they propose a simple greedy algorithm that provides an approximate solution which can be easily modified for our task (see also Barzilay et al. 2002). The algorithm starts by assigning each vertex v  V a probability. Recall that in our case vertices are sentences and their probabilities can be calculated by taking the product of the probabilities of their features. The greedy algorithm then picks the node with the highest probability and orders it ahead of the other nodes. The selected node and its incident edges are deleted from the graph. Each remaining node is now assigned the conditional probability of seeing this node given the previously selected node (see (4)). The node which yields the highest conditional probability is selected a</context>
<context position="20619" citStr="Barzilay et al. (2002)" startWordPosition="3365" endWordPosition="3368">rankings for a 10 sentence text prepositions (147 types in total). 4 Experiments In this section we describe our experiments with the model and the features introduced in the previous sections. We first evaluate the model by attempting to reproduce the structure of unseen texts from the BLLIP corpus, i.e., the corpus on which the model is trained on. We next obtain an upper bound for the task by conducting a sentence ordering experiment with humans and comparing the model against the human data. Finally, we assess whether this model can be used for multi-document summarization using data from Barzilay et al. (2002). But before we outline the details of our experiments we discuss our choice of metric for comparing different orders. 4.1 Evaluation Metric Our task is to produce an ordering for the sentences of a given text. We can think of the sentences as objects for which a ranking must be produced. Table 2 gives an example of a text containing 10 sentences (A–J) and the orders (i.e., rankings) produced by three hypothetical models. A number of metrics can be used to measure the distance between two rankings such as Spearman’s correlation coefficient for ranked data, Cayley distance, or Kendall’s τ (see </context>
<context position="27648" citStr="Barzilay et al. (2002)" startWordPosition="4569" endWordPosition="4572">iment 1. The average distance in the orderings produced by our subjects is .58. The distance between the humans and the best features is .51 for NL and .55 for VDND. An ANOVA yielded a significant effect of feature type (F(9,99) = 5.213; p &lt; 0.01). Post-hoc Tukey tests revealed that VL, VT, VD, ND, AD, VLNL, and VTNL perform significantly worse than HH (a = 0.01), whereas NL and VDND are not significantly different from HH (a = 0.01). This is in agreement with Experiment 1 and points to the importance of lexical and structural information for the ordering task. 4.4 Experiment 3: Summarization Barzilay et al. (2002) collected a corpus of multiple orderings in order to study what makes an order cohesive. Their goal was to improve the ordering strategy of MULTIGEN (McKeown et al., 1999) a multidocument summarization system that operates on news articles describing the same event. MULTIGEN identifies text units that convey similar information across documents and clusters them into themes. Each theme is next syntactically analysed into predicate argument structures; the structures that are repeated often enough are chosen to be included into the summary. A language generation system outputs a sentence (per </context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Barzilay, Regina, Noemie Elhadad, and Kathleen R. McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Journal ofArtificial Intelligence Research 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<pages>132--139</pages>
<location>Seattle, WA,</location>
<marker>Charniak, 2000</marker>
<rawString>Charniak, Eugene. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics. Seattle, WA, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Learning to order things.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research</journal>
<pages>10--243</pages>
<contexts>
<context position="10570" citStr="Cohen et al. (1999)" startWordPosition="1693" endWordPosition="1696"> edges starting from e, respectively. So, P(h|e) will be 0.16 given that f(h,e) is one and f(e) is six (see the normalization in (5)). 2.2 Determining an Order Once we have collected the counts for our features we can determine the order for a new text that we haven’t encountered before, since some of the features representing its sentences will be familiar. Given a text with N sentences there are N! possible orders. The set of orders can be represented as a complete graph, where the set of vertices V is equal to the set of sentences S and each edge u  v has a weight, the probability P(u|v). Cohen et al. (1999) START � ������ � � � � � S1 S2 S3 (0.3) (0.05) �� � � 3 S1 S3 (0.006) (0.02) S3 S1 Figure 2: Finding an order for a three sentence text show that the problem of finding an optimal ordering through a directed weighted graph is NP-complete. Fortunately, they propose a simple greedy algorithm that provides an approximate solution which can be easily modified for our task (see also Barzilay et al. 2002). The algorithm starts by assigning each vertex v  V a probability. Recall that in our case vertices are sentences and their probabilities can be calculated by taking the product of the probabilit</context>
</contexts>
<marker>Cohen, Schapire, Singer, 1999</marker>
<rawString>Cohen, William W., Robert E. Schapire, and Yoram Singer. 1999. Learning to order things. Journal of Artificial Intelligence Research 10:243–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Aravind Joshi</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Grosz, Joshi, 1995</marker>
<rawString>Grosz, Barbara, Aravind Joshi, , and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics Speech and Signal Processing</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="9517" citStr="Katz, 1987" startWordPosition="1502" endWordPosition="1503">i,j|ai−1,k) is estimated as: (5) P(ai, j |ai−1_ f(ai,j,ai−1,k) k) ∑ f(ai,j,ai−1,k) ai,j where f(ai,j,ai−1,k) is the number of times feature ai,j is preceded by feature ai−1,k in the corpus. The denominator expresses the number of times ai−1,k is attested in the corpus (preceded by any feature). The probabilities P(ai,j|ai−1,k) will be unreliable when the frequency estimates for f (ai, j,ai−1,k) are small, and undefined in cases where the feature combinations are unattested in the corpus. We therefore smooth the observed frequencies using back-off smoothing (Katz, 1987). To illustrate with an example consider the text in Figure 1 which has three sentences S1, S2, S3, each represented by their respective features denoted by letters. The probability P(S3|S2) will be calculated by taking the product of P(h|e), P(h |f), P(h|g), P(i|e), P(i |f), and P(i|g). To obtain P(h|e), we need f(h,e) and f(e) which can be estimated in Figure 1 by counting the number of edges connecting e and h and the number of edges starting from e, respectively. So, P(h|e) will be 0.16 given that f(h,e) is one and f(e) is six (see the normalization in (5)). 2.2 Determining an Order Once w</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics Speech and Signal Processing 33(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodger Kibble</author>
<author>Richard Power</author>
</authors>
<title>An integrated framework for text planning and pronominalisation. In</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Conference on Natural Language Generation. Mitzpe</booktitle>
<pages>77--84</pages>
<location>Ramon,</location>
<contexts>
<context position="16661" citStr="Kibble and Power, 2000" startWordPosition="2708" endWordPosition="2711">k, and approve; for the tensed version, the relevant features will be said, will be asked, and to approve. Nouns. Centering Theory (CT, Grosz et al. 1995) is an entity-based theory of local coherence, which claims that certain entities mentioned in an utterance are more central than others and that this property constrains a speaker’s use of certain referring expressions. The principles underlying CT (e.g., continuity, salience) are of interest to concept-to-text generation as they offer an entity-based model of text and sentence planning which is particularly suited for descriptional genres (Kibble and Power, 2000). We operationalize entity-based coherence for text-to-text generation by simply keeping track of the nouns attested in a sentence without however taking personal pronouns into account. This simplification is reasonable if one has text-to-text generation mind. In multidocument summarization for example, sentences are extracted from different documents; the referents of the pronouns attested in these sentences are typically not known and in some cases identical pronouns may refer to different entities. So making use of noun-pronoun or pronoun-pronoun co-occurrences will be uninformative or in f</context>
</contexts>
<marker>Kibble, Power, 2000</marker>
<rawString>Kibble, Rodger and Richard Power. 2000. An integrated framework for text planning and pronominalisation. In In Proceedings of the 1st International Conference on Natural Language Generation. Mitzpe Ramon, Israel, pages 77–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Lebanon</author>
<author>John Lafferty</author>
</authors>
<title>Combining rankings using conditional probability models on permutations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Machine Learning.</booktitle>
<editor>In C. Sammut and A. Hoffmann, editors,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="21244" citStr="Lebanon and Lafferty 2002" startWordPosition="3473" endWordPosition="3476">. But before we outline the details of our experiments we discuss our choice of metric for comparing different orders. 4.1 Evaluation Metric Our task is to produce an ordering for the sentences of a given text. We can think of the sentences as objects for which a ranking must be produced. Table 2 gives an example of a text containing 10 sentences (A–J) and the orders (i.e., rankings) produced by three hypothetical models. A number of metrics can be used to measure the distance between two rankings such as Spearman’s correlation coefficient for ranked data, Cayley distance, or Kendall’s τ (see Lebanon and Lafferty 2002 for details). Kendall’s τ is based on the number of inversions in the rankings and is defined in (6): _ 2(number of inversions) (6) τ 1−N(N −1)/2 where N is the number of objects (i.e., sentences) being ranked and inversions are the number of interchanges of consecutive elements necessary to arrange them in their natural order. If we think in terms Verb say V:subj:N company represent V:subj:N name represent V:have:have have represent V:obj:N business Noun name N:gen:N its name N:mod:A existing business N:gen:N its business N:mod:Prep since company N:det:Det the of permutations, then i can be </context>
</contexts>
<marker>Lebanon, Lafferty, 2002</marker>
<rawString>Lebanon, Guy and John Lafferty. 2002. Combining rankings using conditional probability models on permutations. In C. Sammut and A. Hoffmann, editors, In Proceedings of the 19th International Conference on Machine Learning. Morgan Kaufmann Publishers, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR. In</title>
<date>1998</date>
<booktitle>In Proceedings on of the LREC Workshop on the Evaluation of Parsing Systems. Granada,</booktitle>
<pages>48--56</pages>
<contexts>
<context position="14020" citStr="Lin, 1998" startWordPosition="2293" endWordPosition="2294">ess than 50 sentences long. An example of the texts in this newswire corpus is shown in Figure 3. The corpus is distributed in a Treebankstyle machine-parsed version which was produced with Charniak’s (2000) parser. The parser is a “maximum-entropy inspired” probabilistic generative model. It achieves 90.1% average precision/recall for sentences with maximum length 40 and 89.5% for sentences with maximum length 100 when trained and tested on the standard sections of the Wall Street Journal Treebank (Marcus et al., 1993). We also obtained a dependency-style version of the corpus using MINIPAR (Lin, 1998) a broad coverage parser for English which employs a manually constructed grammar and a lexicon derived from WordNet with an additional dictionary of proper names (130,000 entries in total). The grammar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges (i.e., types of syntactic (dependency) relations). The output of MINIPAR is a dependency graph which represents the dependency relations between words in a sentence (see Table 1 for an example). Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson, 1996), a domain independent corpus of British English,</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, Dekang. 1998. Dependency-based evaluation of MINIPAR. In In Proceedings on of the LREC Workshop on the Evaluation of Parsing Systems. Granada, pages 48–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From local to global coherence: A bottom-up approach to text planning. In</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th National Conference on Artificial Intelligence.</booktitle>
<pages>629--635</pages>
<location>Providence, Rhode Island,</location>
<contexts>
<context position="1807" citStr="Marcu (1997)" startWordPosition="277" endWordPosition="278">ated and whose nodes specify how this content is grouped via rhetorical or discourse relations (e.g., contrast, sequence, elaboration). For domains with large numbers of facts and rhetorical relations, there can be more than one possible tree representing the intended content. These different trees will be realized as texts with different sentence orders or even paragraph orders and different levels of coherence. Finding the tree that yields the best possible text is effectively a search problem. One way to address it is by narrowing down the search space either exhaustively or heuristically. Marcu (1997) argues that global coherence can be achieved if constraints on local coherence are satisfied. The latter are operationalized as weights on the ordering and adjacency of facts and are derived from a corpus of naturally occurring texts. A constraint satisfaction algorithm is used to find the tree with maximal weights from the space of all possible trees. Mellish et al. (1998) advocate stochastic search as an alternative to exhaustively examining the search space. Rather than requiring a global optimum to be found, they use a genetic algorithm to select a tree that is coherent enough for people </context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Marcu, Daniel. 1997. From local to global coherence: A bottom-up approach to text planning. In In Proceedings of the 14th National Conference on Artificial Intelligence. Providence, Rhode Island, pages 629–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13935" citStr="Marcus et al., 1993" startWordPosition="2278" endWordPosition="2281">32 stories. The average story length is 19.2 sentences. 71.30% of the texts in the corpus are less than 50 sentences long. An example of the texts in this newswire corpus is shown in Figure 3. The corpus is distributed in a Treebankstyle machine-parsed version which was produced with Charniak’s (2000) parser. The parser is a “maximum-entropy inspired” probabilistic generative model. It achieves 90.1% average precision/recall for sentences with maximum length 40 and 89.5% for sentences with maximum length 100 when trained and tested on the standard sections of the Wall Street Journal Treebank (Marcus et al., 1993). We also obtained a dependency-style version of the corpus using MINIPAR (Lin, 1998) a broad coverage parser for English which employs a manually constructed grammar and a lexicon derived from WordNet with an additional dictionary of proper names (130,000 entries in total). The grammar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges (i.e., types of syntactic (dependency) relations). The output of MINIPAR is a dependency graph which represents the dependency relations between words in a sentence (see Table 1 for an example). Lin (1998) evaluated the parser o</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Judith L Klavans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Regina Barzilay</author>
<author>Eleazar Eskin</author>
</authors>
<title>Towards multidocument summarization by reformulation: Progress and prospects.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence.</booktitle>
<pages>453--459</pages>
<location>Orlando, FL,</location>
<contexts>
<context position="27820" citStr="McKeown et al., 1999" startWordPosition="4600" endWordPosition="4603">A yielded a significant effect of feature type (F(9,99) = 5.213; p &lt; 0.01). Post-hoc Tukey tests revealed that VL, VT, VD, ND, AD, VLNL, and VTNL perform significantly worse than HH (a = 0.01), whereas NL and VDND are not significantly different from HH (a = 0.01). This is in agreement with Experiment 1 and points to the importance of lexical and structural information for the ordering task. 4.4 Experiment 3: Summarization Barzilay et al. (2002) collected a corpus of multiple orderings in order to study what makes an order cohesive. Their goal was to improve the ordering strategy of MULTIGEN (McKeown et al., 1999) a multidocument summarization system that operates on news articles describing the same event. MULTIGEN identifies text units that convey similar information across documents and clusters them into themes. Each theme is next syntactically analysed into predicate argument structures; the structures that are repeated often enough are chosen to be included into the summary. A language generation system outputs a sentence (per theme) from the selected predicate argument structures. Barzilay et al. (2002) collected ten sets of articles each consisting of two to three articles reporting the same ev</context>
</contexts>
<marker>McKeown, Klavans, Hatzivassiloglou, Barzilay, Eskin, 1999</marker>
<rawString>McKeown, Kathleen R., Judith L. Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin. 1999. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of the 16th National Conference on Artificial Intelligence. Orlando, FL, pages 453–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Alistair Knott</author>
<author>Jon Oberlander</author>
<author>Mick O’ Donnell</author>
</authors>
<title>Experiments using stochastic search for text planning. In</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th International Workshop on Natural Language Generation.</booktitle>
<pages>98--107</pages>
<location>Ontario, Canada,</location>
<contexts>
<context position="2184" citStr="Mellish et al. (1998)" startWordPosition="339" endWordPosition="342">agraph orders and different levels of coherence. Finding the tree that yields the best possible text is effectively a search problem. One way to address it is by narrowing down the search space either exhaustively or heuristically. Marcu (1997) argues that global coherence can be achieved if constraints on local coherence are satisfied. The latter are operationalized as weights on the ordering and adjacency of facts and are derived from a corpus of naturally occurring texts. A constraint satisfaction algorithm is used to find the tree with maximal weights from the space of all possible trees. Mellish et al. (1998) advocate stochastic search as an alternative to exhaustively examining the search space. Rather than requiring a global optimum to be found, they use a genetic algorithm to select a tree that is coherent enough for people to understand (local optimum). The problem of finding an acceptable ordering does not arise solely in concept-to-text generation but also in the emerging field of text-to-text generation (Barzilay, 2003). Examples of applications that require some form of text structuring, are single- and multidocument summarization as well as question answering. Note that these applications</context>
<context position="5228" citStr="Mellish et al. (1998)" startWordPosition="806" endWordPosition="809">ences obtained from a multidocument summarizer or a question answering system. Sentences are represented by a set of informative features (e.g., a verb and its subject, a noun and its modifier) that can be automatically extracted from the corpus without recourse to manual annotation. The model learns which sequences of features are likely to co-occur and makes predictions concerning preferred orderings. Local coherence is thus operationalized by sentence proximity in the training corpus. Global coherence is obtained by greedily searching through the space of possible orders. As in the case of Mellish et al. (1998) we construct an acceptable ordering rather than the best possible one. We propose an automatic method of evaluating the orders generated by our model by measuring closeness or distance from the gold standard, a collection of orders produced by humans. The remainder of this paper is organized as follows. Section 2 introduces our model and an algorithm for producing a possible order. Section 3 describes our corpus and the estimation of the model parameters. Our experiments are detailed in Section 4. We conclude with a discussion in Section 5. 2 Learning to Order Given a collection of texts from</context>
</contexts>
<marker>Mellish, Knott, Oberlander, Donnell, 1998</marker>
<rawString>Mellish, Chris, Alistair Knott, Jon Oberlander, and Mick O’ Donnell. 1998. Experiments using stochastic search for text planning. In In Proceedings of the 9th International Workshop on Natural Language Generation. Ontario, Canada, pages 98–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1034" citStr="Reiter and Dale 2000" startWordPosition="151" endWordPosition="154"> generation. We describe a model that learns constraints on sentence order from a corpus of domainspecific texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model’s task. We also assess the appropriateness of such a model for multidocument summarization. 1 Introduction Structuring a set of facts into a coherent text is a non-trivial task which has received much attention in the area of concept-to-text generation (see Reiter and Dale 2000 for an overview). The structured text is typically assumed to be a tree (i.e., to have a hierarchical structure) whose leaves express the content being communicated and whose nodes specify how this content is grouped via rhetorical or discourse relations (e.g., contrast, sequence, elaboration). For domains with large numbers of facts and rhetorical relations, there can be more than one possible tree representing the intended content. These different trees will be realized as texts with different sentence orders or even paragraph orders and different levels of coherence. Finding the tree that </context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Reiter, Ehud and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>English for the Computer.</title>
<date>1996</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="14571" citStr="Sampson, 1996" startWordPosition="2381" endWordPosition="2382">ependency-style version of the corpus using MINIPAR (Lin, 1998) a broad coverage parser for English which employs a manually constructed grammar and a lexicon derived from WordNet with an additional dictionary of proper names (130,000 entries in total). The grammar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges (i.e., types of syntactic (dependency) relations). The output of MINIPAR is a dependency graph which represents the dependency relations between words in a sentence (see Table 1 for an example). Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson, 1996), a domain independent corpus of British English, and achieved a recall of 79% and precision of 89% on the dependency relations. From the two different parsed versions of the BLLIP corpus the following features were extracted: Verbs. Investigations into the interpretation of narrative discourse (Asher and Lascarides, 2003) have shown that specific lexical information (e.g., verbs, adjectives) plays an important role in determining the discourse relations between propositions. Although we don’t have an explicit model of rhetorical relations and their effects on sentence ordering, we capture the</context>
</contexts>
<marker>Sampson, 1996</marker>
<rawString>Sampson, Geoffrey. 1996. English for the Computer. Oxford University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>