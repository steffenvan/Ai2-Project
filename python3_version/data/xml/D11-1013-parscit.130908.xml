<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000240">
<title confidence="0.9233305">
Domain-Assisted Product Aspect Hierarchy Generation: Towards
Hierarchical Organization of Unstructured Consumer Reviews
</title>
<author confidence="0.998258">
Jianxing Yu1, Zheng-Jun Zha1, Meng Wang1, Kai Wang2, Tat-Seng Chua1
</author>
<affiliation confidence="0.985628">
1School of Computing, National University of Singapore
2Institute for Infocomm Research, Singapore
</affiliation>
<email confidence="0.980345">
{jianxing, zhazj, wangm, chuats}@comp.nus.edu.sg kwang@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.996496" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999771888888889">
This paper presents a domain-assisted ap-
proach to organize various aspects of a prod-
uct into a hierarchy by integrating domain
knowledge (e.g., the product specifications),
as well as consumer reviews. Based on the
derived hierarchy, we generate a hierarchical
organization of consumer reviews on various
product aspects and aggregate consumer opin-
ions on these aspects. With such organiza-
tion, user can easily grasp the overview of
consumer reviews. Furthermore, we apply the
hierarchy to the task of implicit aspect identi-
fication which aims to infer implicit aspects of
the reviews that do not explicitly express those
aspects but actually comment on them. The
experimental results on 11 popular products in
four domains demonstrate the effectiveness of
our approach.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983">
With the rapidly expanding e-commerce, most retail
Web sites encourage consumers to write reviews to
express their opinions on various aspects of prod-
ucts. Huge collections of consumer reviews are
now available on the Web. These reviews have be-
come an important resource for both consumers and
firms. Consumers commonly seek quality informa-
tion from online consumer reviews prior to purchas-
ing a product, while many firms use online reviews
as an important resource in their product develop-
ment, marketing, and consumer relationship man-
agement. However, the reviews are disorganized,
leading to the difficulty in information navigation
and knowledge acquisition. It is impractical for user
to grasp the overview of consumer reviews and opin-
ions on various aspects of a product from such enor-
mous reviews. Among hundreds of product aspects,
it is also inefficient for user to browse consumer re-
views and opinions on a specific aspect. Thus, there
is a compelling need to organize consumer reviews,
so as to transform the reviews into a useful knowl-
edge structure. Since the hierarchy can improve in-
formation representation and accessibility (Cimiano,
2006), we propose to organize the aspects of a prod-
uct into a hierarchy and generate a hierarchical or-
ganization of consumer reviews accordingly.
Towards automatically deriving an aspect hierar-
chy from the reviews, we could refer to traditional
hierarchy generation methods in ontology learning,
which first identify concepts from the text, then
determine the parent-child relations between these
concepts using either pattern-based or clustering-
based methods (Murthy et al., 2010). However,
pattern-based methods usually suffer from inconsis-
tency of parent-child relationships among the con-
cepts, while clustering-based methods often result
in low accuracy. Thus, by directly utilizing these
methods to generate an aspect hierarchy from con-
sumer reviews, the resulting hierarchy is usually in-
accurate, leading to unsatisfactory review organiza-
tion. On the other hand, domain knowledge of prod-
ucts is now available on the Web. For example,
there are more than 248,474 product specifications
in the product selling Web site CNet.com (Beckham,
2005). These product specifications cover some
product aspects and provide coarse-grained parent-
child relations among these aspects. Such domain
knowledge is useful to help organize the product as-
</bodyText>
<page confidence="0.964664">
140
</page>
<note confidence="0.9578265">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 140–150,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.973319835443038">
initial hierarchy, we develop a multi-criteria opti-
mization approach to construct an aspect hierarchy
to contain all the identified aspects. Our approach
incrementally inserts the aspects into the initial hi-
erarchy based on inter-aspect semantic distance, a
metric used to measure the semantic relation among
aspects. In order to derive reliable semantic dis-
tance, we propose to leverage external hierarchies,
sampled from WordNet and Open Directory Project,
to assist semantic distance learning. With resultant
aspect hierarchy, the consumer reviews are then or-
ganized to their corresponding aspect nodes in the
hierarchy. We then perform sentiment classification
to determine consumer opinions on these aspects.
Furthermore, we apply the hierarchy to the task of
implicit aspect identification. This task aims to infer
implicit aspects of the reviews that do not explic-
itly express those aspects but actually comment on
them. For example, the implicit aspect of the review
“It is so expensive” is “price.” Most existing aspect
identification approaches rely on the appearance of
aspect terms, and thus are not able to handle implicit
aspect problem. Based on our aspect hierarchy, we
can infer the implicit aspects by clustering the re-
views into their corresponding aspect nodes in the
hierarchy. We conduct experiments on 11 popular
products in four domains. More details of the corpus
are discussed in Section 4. The experimental results
demonstrate the effectiveness of our approach.
The main contributions of this work can be sum-
marized as follows:
1) We propose to hierarchically organize con-
sumer reviews according to an aspect hierarchy, so
as to transfer the reviews into a useful knowledge
structure.
2) We develop a domain-assisted approach to
generate an aspect hierarchy by integrating domain
knowledge and consumer reviews. In order to de-
rive reliable semantic distance between aspects, we
propose to leverage external hierarchies to assist se-
mantic distance learning.
3) We apply the aspect hierarchy to the task of im-
plicit aspect identification, and achieve satisfactory
performance.
The rest of this paper is organized as follows. Our
approach is elaborated in Section 2 and applied to
implicit aspect identification in Section 3. Section
4 presents the evaluations, while Section 5 reviews
Figure 1: Sample hierarchical organization for iPhone 3G
pects into a hierarchy. However, the initial hierarchy
obtained from domain knowledge usually cannot fit
the review data well. For example, the initial hierar-
chy is usually too coarse and may not cover the spe-
cific aspects commented in the reviews, while some
aspects in the hierarchy may not be of interests to
users in the reviews.
Motivated by the above observations, we propose
in this paper to organize the product aspects into a
hierarchy by simultaneously exploiting the domain
knowledge (e.g., the product specification) and con-
sumer reviews. With derived aspect hierarchy, we
generate a hierarchical organization of consumer re-
views on various aspects and aggregate consumer
opinions on these aspects. Figure 1 illustrates a sam-
ple of hierarchical review organization for the prod-
uct “iPhone 3G”. With such organization, users can
easily grasp the overview of product aspects as well
as conveniently navigate the consumer reviews and
opinions on any aspect. For example, users can find
that 623 reviews, out of 9,245 reviews, are about the
aspect “price”, with 241 positive and 382 negative
reviews.
Given a collection of consumer reviews on a spe-
cific product, we first automatically acquire an ini-
tial aspect hierarchy from domain knowledge and
identify the aspects from the reviews. Based on the
141
related work. Finally, Section 6 concludes this paper
with future works.
</bodyText>
<sectionHeader confidence="0.969949" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.9995965">
Our approach consists of four components, includ-
ing initial hierarchy acquisition, aspect identifica-
tion, semantic distance learning, and aspect hierar-
chy generation. Next, we first define some prelimi-
nary and notations and then elaborate these compo-
nents.
</bodyText>
<subsectionHeader confidence="0.979248">
2.1 Preliminary and Notations
</subsectionHeader>
<bodyText confidence="0.934518">
Preliminary 1. An aspect hierarchy is defined as a
tree that consists of a set of unique aspects A and
a set of parent-child relations R between these as-
pects.
Given the consumer reviews of a product, let
A = ja1, · · · , akI denotes the product aspects com-
mented in the reviews. W0(A0, R0) denotes the ini-
tial hierarchy derived from domain knowledge. It
contains a set of aspects A0 and relations R0. Our
task is to construct an aspect hierarchy W(A, R), to
cover all the aspects in A and their parent-child re-
lations R, so that the consumer reviews are hierar-
chically organized. Note that W0 can be empty.
</bodyText>
<subsectionHeader confidence="0.996853">
2.2 Initial Hierarchy Acquisition
</subsectionHeader>
<bodyText confidence="0.999995428571429">
As aforementioned, product specifications on prod-
uct selling websites cover some product aspects and
coarse-grained parent-child relations among these
aspects. Such domain knowledge is useful to help
organize aspects into a hierarchy. We here employ
the approach proposed by Ye and Chua (2006) to au-
tomatically acquire an initial aspect hierarchy from
the product specifications. The method first identi-
fies the Web page region covering product descrip-
tions and removes the irrelevant contents from the
Web page. It then parses the region containing the
product information to identify the aspects as well as
their structure. Based on the aspects and their struc-
ture, it generates an aspect hierarchy.
</bodyText>
<subsectionHeader confidence="0.998833">
2.3 Aspect Identification
</subsectionHeader>
<bodyText confidence="0.986578666666667">
To identify aspects in consumer reviews, we first
parse each review using the Stanford parser 1. Since
the aspects in consumer reviews are usually noun
</bodyText>
<page confidence="0.438387">
lhttp://nlp.stanford.edu/software/lex-parser.shtml
</page>
<figureCaption confidence="0.993754">
Figure 2: Sample Pros and Cons reviews
</figureCaption>
<bodyText confidence="0.99987815">
or noun phrases (Liu, 2009), we extract the noun
phrases (NP) from the parse tree as aspect candi-
dates. While these candidates may contain much
noise, we leverage Pros and Cons reviews (see Fig-
ure 2), which are prevalent in forum Web sites,
to assist identify aspects from the candidates. It
has been shown that simply extracting the frequent
noun terms from the Pros and Cons reviews can get
high accurate aspect terms (Liu el al., 2005). Thus,
we extract the frequent noun terms from Pros and
Cons reviews as features, then train a one-class SVM
(Manevitz et al., 2002) to identify aspects from the
candidates. While the obtained aspects may con-
tain some synonym terms, such as “earphone” and
“headphone”, we further perform synonym cluster-
ing to get unique aspects. Specifically, we first ex-
pand each aspect term with its synonym terms ob-
tained from the synonym terms Web site 2, then clus-
ter them to obtain unique aspects based on unigram
feature.
</bodyText>
<subsectionHeader confidence="0.996066">
2.4 Semantic Distance Learning
</subsectionHeader>
<bodyText confidence="0.999791428571428">
Our aspect hierarchy generation approach is essen-
tially based on the semantic relations among as-
pects. We here define a metric, Semantic Distance,
d(ax, ay), to quantitatively measure the semantic re-
lation between aspects ax and ay. We formulate
d(ax, ay) as the weighted sum of some underlying
features,
</bodyText>
<equation confidence="0.786891">
�d(ax, ay) = j wjfj(ax, ay), (1)
</equation>
<bodyText confidence="0.998681166666667">
where wj is the weight for j-th feature function
fj(·).
Next, we first introduce the linguistic features
used in our work and then present the semantic dis-
tance learning algorithm that aims to find the opti-
mal weights in Eq.(1).
</bodyText>
<footnote confidence="0.988701">
2http://thesaurus.com
</footnote>
<page confidence="0.989825">
142
</page>
<subsectionHeader confidence="0.577852">
2.4.1 Linguistic Features
</subsectionHeader>
<bodyText confidence="0.9998982">
Given two aspects a, and ay, a feature is defined
as a function generating a numeric score f(ate, ay)
or a vector of scores. The features include Contex-
tual, Co-occurrence, Syntactic, Pattern and Lexical
features (Yang and Callan, 2009). These features are
generated based on auxiliary documents collected
from Web.
Specifically, we issue each aspect term and aspect
term pair as queries to Google and Wikipedia, re-
spectively, and collect the top 100 returned docu-
ments of each query. We then split the documents
into sentences. Based on these documents and sen-
tences, the features are generated as follows.
Contextual features. For each aspect, we collect
the documents containing the aspect as context to
build a unigram language model without smoothing.
Given two aspects, the KL-divergence between their
language models is computed as the Global-Context
feature between them. Similarly, we collect the left
two and right two words surrounding each aspect as
context and build a unigram language model. The
KL-divergence between the language models of two
aspects is defined as the Local-Context feature.
Co-occurrence features. We measure the co-
occurrence of two aspects by Pointwise Mutual
Information (PMI): PMI(a�,ay)=log(Count(a�,ay)/
Count(ax) Count(ay)), where Count(·) stands for the
number of documents or sentences containing the
aspect(s), or the number of Google document hits
for the aspect(s). Based on different definitions of
Count(·), we define the features of Document PMI,
Sentence PMI, and Google PMI, respectively.
Syntactic features. We parse the sentences that
contain each aspect pair into syntactic trees via the
Stanford Parser. The Syntactic-path feature is de-
fined as the average length of the shortest syntactic
path between the aspect pair in the tree. In addi-
tion, for each aspect, we collect a set of sentences
containing it, and label the semantic role of the sen-
tences via ASSERT parser 3. Given two aspects,
the number of the Subject terms overlaps between
their sentence sets is computed as the Subject Over-
lap feature. Similarly, for other semantic roles, such
as objects, modifiers, and verbs, we define the fea-
tures of Object Overlap, Modifier Overlap, and Verb
</bodyText>
<footnote confidence="0.489117">
3http://cemantix.org/assert.html
</footnote>
<bodyText confidence="0.998507263157895">
Overlap, respectively.
Pattern features. 46 patterns are used in our
work, including 6 patterns indicating the hypernym
relations of two aspects (Hearst, 1992), and 40 pat-
terns measuring the part-of relations of two aspects
(Girju et al., 2006). These pattern features are
asymmetric, and they take the parent-child relations
among the aspects into consideration. All the pat-
terns are listed in Appendix A (submitted as supple-
mentary material). Based on these patterns, a 46-
dimensional score vector is obtained for each aspect
pair. A score is 1 if two aspects match a pattern, and
0 otherwise.
Lexical features. We take the word length differ-
ence between two aspects, as Length Difference fea-
ture. In addition, we issue the query “define:aspect”
to Google, and collect the definition of each aspect.
We then count the word overlaps between the defini-
tions of two aspects, as Definition Overlap feature.
</bodyText>
<subsectionHeader confidence="0.892154">
2.4.2 Semantic Distance Learning
</subsectionHeader>
<bodyText confidence="0.9998752">
This section elaborates the learning algorithm
that optimizes the semantic distance metric, i.e.,
the weighting parameters in Eq.(1). Typically, we
can utilize the initial hierarchy as training data.
The ground-truth distance between two aspects
dO(ate, ay) is generated by summing up all the edge
distances along the shortest path between a,, and ay,
where every edge weight is assumed as 1. The dis-
tance metric is then obtained by solving the follow-
ing optimization problem,
</bodyText>
<equation confidence="0.9752234">
∑arg min
T
_j j=1 a.,&apos;EA0
x&lt;y
(2)
</equation>
<bodyText confidence="0.999227666666667">
where m is the dimension of linguistic feature, q is
a tradeoff parameter. Eq.(2) can be rewrote to its
matrix form as,
</bodyText>
<equation confidence="0.814478">
� �
�d − fTw �2 + q · ∥w∥2 , (3)
</equation>
<bodyText confidence="0.9995606">
where vector d contains the ground-truth distance of
all the aspect pairs. Each element corresponds to
the distance of certain aspect pair, and f is the corre-
sponding feature vector. The optimal solution of w
is given as
</bodyText>
<equation confidence="0.872444">
w- = (fTf + q · I)-1(fTd) (4)
arg min
W
(do(ax,ay) − M wjfj(ax, ay ))2+77· M 2
∑ ∑ wj,
j=1 j=1
</equation>
<page confidence="0.99099">
143
</page>
<bodyText confidence="0.999957111111111">
where I is the identity metric.
The above learning algorithm can perform well
when sufficient training data (i.e., aspect (term)
pairs) is available. However, the initial hierarchy is
usually too coarse and thus cannot provide sufficient
information. On the other hand, abundant hand-
crafted hierarchies are available on the Web, such
as WordNet and Open Directory Project (ODP). We
here propose to leverage these external hierarchies
to assist semantic distance learning. A distance met-
ric w0 is learned from the external hierarchies us-
ing the above algorithm. Since w0 might be biased
to the characteristics of the external hierarchies, di-
rectly using w0 in our task may not perform well.
Alternatively, we use w0 as prior knowledge to as-
sist learning the optimal distance metric w from the
initial hierarchy. The learning problem is formulated
as follows,
</bodyText>
<equation confidence="0.849927">
arg min
W � �
</equation>
<bodyText confidence="0.78239825">
d − fTw 2 + q · ∥w∥2 + &apos;Y · ∥w − w0∥2 ,
(5)
where q and &apos;Y are tradeoff parameters.
The optimal solution of w can be obtained as
</bodyText>
<equation confidence="0.997552">
w* = (fTf + (q + &apos;Y) · I)−1(fTd + &apos;Y · w0). (6)
</equation>
<bodyText confidence="0.9982955">
As a result, we can compute the semantic distance
between each two aspects according to Eq.(1).
</bodyText>
<subsectionHeader confidence="0.993479">
2.5 Aspect Hierarchy Generation
</subsectionHeader>
<bodyText confidence="0.999949">
Given the aspects A = {a1, · · · , ak} identified from
reviews and the initial hierarchy H0(A0,R0) ob-
tained from domain knowledge, our task is to con-
struct an aspect hierarchy to contain all the aspects
in A. Inspired by Yang and Callan (2009), we adopt
a multi-criteria optimization approach to incremen-
tally insert the aspects into appropriate positions in
the hierarchy based on multiple criteria.
Before going to the details, we first introduce an
information function to measure the amount of in-
formation carried in a hierarchy. An information
function Info(H) is defined as the sum of the se-
mantic distances of all the aspect pairs in the hierar-
chy (Yang and Callan, 2009).
</bodyText>
<equation confidence="0.948103">
Info(H(A,R)) = ∑ d(ax, ay). (7)
x&lt;y;ax,ayEA
</equation>
<bodyText confidence="0.994393538461539">
Based on this information function, we then intro-
duce the following three criteria for aspect insertion:
minimum Hierarchy Evolution, minimum Hierarchy
Discrepancy and minimum Semantic Inconsistency.
Hierarchy Evolution is designed to monitor the
structure evolution of a hierarchy. The hierarchy is
incrementally hosting more aspects until all the as-
pects are allocated. The insertion of a new aspect a
into different positions in the current hierarchy H(i)
leads to different new hierarchies. Among these new
hierarchies, we here assume that the optimal one
H(i+1) should introduce the least changes of infor-
mation to H(i).
</bodyText>
<equation confidence="0.998641">
ˆH(i+1) =argmin ∆Info(H(i+1) − H(i)). (8)
9{(i+1)
</equation>
<bodyText confidence="0.935143">
By plugging in Eq.(7) and using least square to
measure the information changes, we have,
</bodyText>
<equation confidence="0.99479175">
obj1 = arg min
9{(i+1) (∑x&lt;y;ax,ayEAiU{a} d(ax, ay)
− ∑x&lt;y;ax,ayEAi d(ax, ay))2,
(9)
</equation>
<bodyText confidence="0.987678">
Hierarchy Discrepancy is used to measure the
global changes of the structure. We assume a good
hierarchy should bring the least changes to the initial
hierarchy,
</bodyText>
<equation confidence="0.95604">
ˆH(i+1) = arg min ∆Info(H(i+1) − H(0)) . (10)
9{(i+1)
i + 1
We then get, i+1(∑
obj2 = arg min 1 x&lt;y;ax,ayEAiU{a} d(ax, ay)
9{(i+1)
− ∑x&lt;y;ax,ayEA0 d(ax, ay))2.
</equation>
<bodyText confidence="0.992525846153846">
Semantic Inconsistency is introduced to quantify
the inconsistency between the semantic distance es-
timated via the hierarchy and that computed from
the feature functions. We assume that a good hier-
archy should precisely reflect the semantic distance
between aspects. For two aspects, their semantic
distance reflected by the hierarchy is computed as
the sum of adjacent distances along the shortest path
between them,
∑d9{(ax, ay) = p&lt;q;(ap,a�)ESP(ax,ay) d(ap, aq),
where SP(ax, ay) is the shortest path between the
aspects (ax, ay), (ap, aq) are the adjacent nodes
along the path.
</bodyText>
<page confidence="0.994538">
144
</page>
<bodyText confidence="0.999879333333333">
We then define the following criteria to find the
hierarchy with minimum semantic inconsistency,
where d(ax, ay) is the distance computed based on
the feature functions in Section 2.4.
Through integrating the above criteria, the multi-
criteria optimization framework is formulated as,
</bodyText>
<equation confidence="0.960788666666667">
obj = arg min (A1 · obj1 + A2 · obj2 + A3 · obj3)
9{(z+1)
A1 + A2 + A3 = 1; 0 G A1, A2, A3 G 1.
</equation>
<bodyText confidence="0.994950090909091">
where A1, A2, A3 are the tradeoff parameters.
To summarize, our aspect hierarchy generation
process starts from an initial hierarchy and inserts
the aspects into it one-by-one until all the aspects
are allocated. Each aspect is inserted to the op-
timal position found by Eq.(14). It is worth not-
ing that the insertion order may influence the result.
To avoid such influence, we select the aspect with
the least objective function value in Eq.(14) to in-
sert. Based on resultant hierarchy, the consumer re-
views are then organized to their corresponding as-
pect nodes in the hierarchy. We further prune out the
nodes without reviews from the hierarchy.
Moreover, we perform sentiment classification to
determine consumer opinions on various aspects. In
particular, we train a SVM sentiment classifier based
on the Pros and Cons reviews described in Section
2.3. We collect sentiment terms in the reviews as
features and represent reviews as feature vectors us-
ing Boolean weighting. Note that we define senti-
ment terms as those appear in the sentiment lexicon
provided by MPQA project (Wilson et al., 2005).
</bodyText>
<sectionHeader confidence="0.995324" genericHeader="method">
3 Implicit Aspect Identification
</sectionHeader>
<bodyText confidence="0.99995052173913">
In this section, we apply the aspect hierarchy to the
task of implicit aspect identification. This task aims
to infer the aspects of reviews that do not explic-
itly express those aspects but actually comment on
them (Liu et al. 2005). Take the review “The phone
is too large” as an example, the task is to infer its
implicit aspect “size.” It has been observed that the
reviews commenting on a same aspect usually use
some same sentiment terms (Su et al., 2008). There-
fore, sentiment term is an effective feature for identi-
fying implicit aspects. We here collect the sentiment
terms as features to represent each review into a fea-
ture vector. For each aspect node in the hierarchy,
we define its centroid as the average of its feature
vectors, i.e., the feature vectors of all the reviews
that are allocated at this node. We then calculate
the cosine similarity of each implicit-aspect review
to the centriods of all the aspect nodes, and allo-
cate the review into the node with maximum sim-
ilarity. As a result, the implicit aspect reviews are
grouped to their related aspect nodes. In other word,
their aspects are identified as the corresponding as-
pect nodes.
</bodyText>
<sectionHeader confidence="0.99908" genericHeader="method">
4 Evaluations
</sectionHeader>
<bodyText confidence="0.999741666666667">
In this section, we evaluate the effectiveness of our
approach on aspect identification, aspect hierarchy
generation, and implicit aspect identification.
</bodyText>
<subsectionHeader confidence="0.979421">
4.1 Data and Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999950222222222">
The details of our product review corpus are given
in Table 1. This corpus contains consumer reviews
on 11 popular products in four domains. These
reviews were crawled from several prevalent fo-
rum Web sites, including cnet.com, viewpoints.com,
reevoo.com and gsmarena.com. All of the reviews
were posted between June, 2009 and Sep 2010. The
aspects of the reviews, as well as the opinions on
the aspects were manually annotated. We also in-
vited five annotators to construct the gold-standard
hierarchies for the products by providing them the
initial hierarchies and the aspects in reviews. The
conflicts between annotators were resolved through
their discussions. For semantic distance learning, we
collected 50 hierarchies from WordNet and ODP, re-
spectively. The details are shown in Table 2. We
listed the topics of the hierarchies in Appendix B
(submitted as supplementary material).
</bodyText>
<table confidence="0.999803583333333">
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S63916GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
</table>
<tableCaption confidence="0.887739">
Table 1: Statistics of the reviews corpus, # denotes the
size of the reviews/sentences
</tableCaption>
<equation confidence="0.97908425">
obj3 = arg min
W(%+1) x&lt;y;ay,ayEA{U{a};
(d7&apos;(ax, ay)−d(ax, ay))2,
∑
</equation>
<page confidence="0.994998">
145
</page>
<table confidence="0.8483514">
Statistic WordNet ODP
Total # hierarchies 50 50
Total # terms 1,964 2,210
Average # depth 5.5 5.9
Total # related topics 12 16
</table>
<tableCaption confidence="0.78852">
Table 2: Statistics of the External Hierarchies
</tableCaption>
<figureCaption confidence="0.993356">
Figure 4: Evaluations on Aspect Hierarchy Generation. t-
test, p-values&lt;0.05. w/ H denotes the methods with ini-
tial hierarchy, accordingly, w/o H refers to the methods
without initial hierarchy.
Figure 3: Evaluations on Aspect Identification. t-test, p-
values&lt;0.05
</figureCaption>
<bodyText confidence="0.999082">
We employed F1-measure, which is the combina-
tion of precision and recall, as the evaluation metric
for all the evaluations. For the evaluation on aspect
hierarchy, we defined precision as the percentage of
correctly returned parent-child pairs out of the to-
tal returned pairs, and recall as the percentage of
correctly returned parent-child pairs out of the to-
tal pairs in the gold standard. Throughout the ex-
periments, we empirically set A1 = 0.4, A2 = 0.3,
A3 = 0.3, 7 = 0.4 and -y=0.6.
</bodyText>
<subsectionHeader confidence="0.999258">
4.2 Evaluations on Aspect Identification
</subsectionHeader>
<bodyText confidence="0.999355666666667">
We compared our approach against two state-of-the-
art methods: a) the method proposed by Hu and Liu
(2004), which is based on the association rule min-
ing, and b) the method proposed by Wu et al. (2009),
which is based on the dependency parser. The re-
sults are presented in Figure 3. On average, our
approach significantly outperforms Hu’s and Wu’s
method in terms of F1-measure by over 5.87% and
3.27%, respectively.
</bodyText>
<subsectionHeader confidence="0.961553">
4.3 Evaluations on Aspect Hierarchy
4.3.1 Comparisons with the State-of-the-Arts
</subsectionHeader>
<bodyText confidence="0.999811815789473">
We compared our approach against four tra-
ditional hierarchy generation methods in the re-
searches on ontology learning, including a) pattern-
based method (Hearst, 1992) and b) clustering-based
method by Shi et al. (2008), c) the method proposed
by Snow et al. (2006) which was based on a proba-
bilistic model, and d) the method proposed by Yang
and Callan (2009). Since our approach and Yang’s
method can utilize initial hierarchy to assist hier-
archy generation, we evaluated their performance
with or without initial hierarchy, respectively. For
the sake of fair comparison, Snow’s, Yang’s and our
methods used the same linguistic features in Section
2.4.1.
Figure 4 shows the performance comparisons
of these five methods. We can see that our ap-
proach without using initial hierarchy outperforms
the pattern-based, clustering-based, Snow’s, and
Yang’s methods by over 17.9%, 19.8%, 2.9% and
6.1% respectively in terms of average F1-measure.
By exploiting initial hierarchy, our approach im-
proves the performance significantly. As compared
to the pattern-based, clustering-based and Snow’s
methods, it improves the average performance by
over 49.4%, 51.2% and 34.3% respectively. Com-
pared to Yang’s method with initial hierarchy, it
achieves 4.7% improvements on the average perfor-
mance.
The results show that pattern-based and
clustering-based methods perform poor. Pattern-
based method may suffer from the problem of low
coverage of patterns, especially when the patterns
are manually pre-defined, while the clustering-
based method (Shi et al., 2008) may sustain to the
bisection clustering mechanism which can only
generate a binary-tree. The results also illustrate
that our approach outperforms Snow’s and Yang’s
methods. By exploiting external hierarchies, our
</bodyText>
<page confidence="0.998267">
146
</page>
<figureCaption confidence="0.9986655">
Figure 5: Evaluations on the Impact of Initial Hierarchy.
t-test, p-values&lt;0.05.
</figureCaption>
<bodyText confidence="0.979196">
approach is able to derive reliable semantic distance
between aspects and thus improve the performance.
</bodyText>
<subsectionHeader confidence="0.7736105">
4.3.2 Evaluations on Effectiveness of Initial
Hierarchy
</subsectionHeader>
<bodyText confidence="0.9999775">
In this section, we show that even based on a small
part of the initial hierarchy, our approach can still
generate a satisfactory hierarchy. We explored dif-
ferent proportion of initial hierarchy, including 0%,
20%, 40%, 60% and 80% of the aspect pairs which
are collected top-down from the initial hierarchy. As
shown in Figure 5, the performance increases when
larger proportion of the initial hierarchy is used.
Thus, we can speculate that domain knowledge is
valuable in aspect hierarchy generation.
</bodyText>
<subsectionHeader confidence="0.965507">
4.3.3 Evaluations on Effectiveness of
Optimization Criteria
</subsectionHeader>
<bodyText confidence="0.999823428571429">
We conducted a leave-one-out study to evaluate
the effectiveness of each optimization criterion. In
particular, we set one of the tradeoff parameters (A1,
A2, A3) in Eq.(14) to zero, and distributed its weight
to the rest parameters averagely. From Figure 6, we
find that removing any optimization criterion would
degrade the performance on most products. It is in-
teresting to note that removing the third optimiza-
tion criterion, i.e., minimum semantic inconsistency,
slightly increases the performance on two products
(ipad touch and sony MP3). The reason might be
that the values of the three tradeoff parameters (em-
pirically set in Section 4.1) are not suitable for these
two products.
</bodyText>
<figureCaption confidence="0.999278">
Figure 6: Evaluations of the Optimization Criteria. % of
change in Fl-measure when a single criterion is removed.
t-test, p-values&lt;0.05.
Figure 7: Evaluations on the Impact of Linguistic Fea-
tures. t-test, p-values&lt;0.05.
</figureCaption>
<subsectionHeader confidence="0.943189">
4.3.4 Evaluations on Semantic Distance
Learning
</subsectionHeader>
<bodyText confidence="0.999852">
In this section, we evaluated the impact of the fea-
tures and external hierarchies in semantic distance
learning. We investigated five sets of features as de-
scribed in Section 2.4.1, including contextual, co-
occurrence, syntactic, pattern and lexical features.
From Figure 7, we observe that the co-occurrence
and pattern features perform much better than con-
textual and syntactic features. A possible reason
is that co-occurrence and pattern features are more
likely to indicate parent-child aspect relationships,
while contextual and syntactic features are proba-
ble to measure sibling aspect relationships. Among
these features, the lexical features perform the worst.
The combination of all the features achieves the best
performance.
Next, we evaluated the effectiveness of external
hierarchies in semantic distance learning. We com-
pared the performance of our approach with or with-
out the external hierarchies. From Figure 8, we find
that by exploiting the external hierarchies, our ap-
</bodyText>
<page confidence="0.99543">
147
</page>
<figureCaption confidence="0.9991515">
Figure 8: Evaluations on the Impact of External Hierar-
chy. t-test, p-values&lt;0.05.
</figureCaption>
<bodyText confidence="0.999937888888889">
proach improves the performance significantly. The
improvement is over 2.81% in terms of average Fi-
measure. This implies that by using external hier-
archies, our approach can obtain effective semantic
distance, and thus improve the performance of as-
pect hierarchy generation.
Additionally, for sentiment classification, our
SVM classifier achieves an average Fl-measure of
0.787 in the 11 products.
</bodyText>
<subsectionHeader confidence="0.6781085">
4.4 Evaluations on Implicit Aspect
Identification
</subsectionHeader>
<bodyText confidence="0.999976705882353">
To evaluate the performance of our approach on im-
plicit aspect identification, we collected 29,657 im-
plicit aspect review sentences on the 11 products
from the four forum Web sites introduced in Section
4.1. While most existing approaches for implicit as-
pect identification rely on hand-crafted rules (Liu,
2009), the method proposed in Su et al. (2008) can
identify implicit aspects without hand-crafted rules
based on mutual clustering. Therefore, we adopt
Su’s method as the baseline here. Figure 9 illustrates
the performance comparison between Su’s and our
approach. We can see that our approach outperforms
Su’s method by over 9.18% in terms of average Fi-
measure. This shows that our approach can iden-
tify the implicit aspects accurately by exploiting the
underlying associations among the sentiment terms
and each aspect in the hierarchy.
</bodyText>
<sectionHeader confidence="0.999969" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.998159">
Some researches treated review organization as a
multi-document summarization problem, and gen-
erated a summary by selecting and ordering sen-
tences taken from multiple reviews (Nishikawa et
</bodyText>
<figureCaption confidence="0.995214">
Figure 9: Evaluations on Implicit Aspects Identification.
t-test, p-values&lt;0.05
</figureCaption>
<bodyText confidence="0.999735823529412">
al., 2010). These works did not drill down to the
fine-grained level to explore the opinions on the
product aspects. Other researchers proposed to pro-
duce a summary covering consumer opinions on
each aspect. For example, Hu and Liu (2004) fo-
cused on extracting the aspects and determining
opinions on the aspects. However, their gener-
ated summary was unstructured, where the possible
relationships between aspects were not recognized
(Cadilhac et al., 2010). Subsequently, Carenini et
al. (2006) proposed to map the aspect to a user-
defined taxonomy, but the taxonomy was hand-
crafted which was not scalable.
Different from the previous works, we focus on
automatically generating an aspect hierarchy to hi-
erarchically organize consumer reviews. There are
some related works on ontology learning, which
first identify concepts from text, and then determine
parent-child relations between these concepts us-
ing either pattern-based or clustering-based methods
(Murthy et al., 2010). Pattern-based methods usu-
ally defined some lexical syntactic patterns to extract
the relations, while clustering-based methods mostly
utilized the hierarchical clustering methods to build
a hierarchy (Roy et al., 2006). Some works proposed
to integrate the pattern-based and clustering-based
methods in a general model, such as the probabilistic
model (Snow et al., 2006) and metric-based model
(Yang and Callan, 2009).
The researches on aspect identification are also
related to our work. Various aspect identification
methods have been proposed (Popescu et al., 2005),
including supervised methods (Liu el al., 2005), and
unsupervised methods (Mei et al., 2007). Different
</bodyText>
<page confidence="0.957517">
148
</page>
<bodyText confidence="0.988737375">
features have been investigated for this task. For 2003.
example, Wu et al. (2009) identified aspects based J. Beckham. The Cnet E-commerce Data set. Technical
on the features explored by dependency parser. Reports, 2005.
For implicit aspect identification, some works pro- G. Carenini, R. Ng, and E. Zwart. Multi-document Sum-
posed to define rules for identification (Liu el al., marization of Evaluative Text. ACL, 2006.
2005), while others suggested to automatically gen- A. Cadilhac, F. Benamara, and N. Aussenac-Gilles. On-
erate rules via mutual clustering (Su et al., 2008). tolexical Resources for Feature based Opinion Mining:
On the other hand, there are some related works a Case-study. Ontolex, 2010.
on sentiment classification (Pang and Lee, 2008). P. Cimiano, A. Madche, S. Staab, and J. Volker. Ontology
These works can be categorized into four granu- Learning. Handbook on Ontologies, Springer, 2004.
larities: document-level, sentence-level, aspect-level P. Cimiano, A. Hotho, and S. Staab. Learning Concept
and word-level sentiment classification (Liu, 2009). Hierarchies from Text Corpora using Formal Concept
Existing researches have been studied unsupervised Analysis. Artificial Intelligence, 2005.
(Kim et al., 2004), supervised (Pang et al., 2002; P. Cimiano. Ontology Learning and Population from
Pang et al., 2005) and semi-supervised classification Text: Algorithms, Evaluation and Applications.
approaches (Goldberg et al., 2006; Li et al., 2009) Springer-Verlag New York, Inc. Secaucus, NJ, USA,
on these four levels. 2006.
6 Conclusions and Future Works S. Dasgupta and V. Ng. Mine the Easy, Classify the Hard:
In this paper, we have developed a domain-assisted A Semi-supervised Approach to Automatic Sentiment
approach to generate product aspect hierarchy by in- Classification. ACL, 2009.
tegrating domain knowledge and consumer reviews. O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
Based on the derived hierarchy, we can generate T. Shaked, S. Soderland, D. Weld, and A. Yates. Un-
a hierarchical organization of consumer reviews as supervised Named-entity Extraction from the Web: An
well as consumer opinions on the aspects. With such Experimental Study. Artificial Intelligence, 2005.
organization, user can easily grasp the overview of A. Esuli and F. Sebastiani. A Publicly Available Lexical
consumer reviews, as well as seek consumer reviews Resource for Opinion Mining. LREC, 2006.
and opinions on any specific aspect by navigating M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
through the hierarchy. We have further applied the Pulse: Mining Customer Opinions from Free Text.
hierarchy to the task of implicit aspect identification. IDA, 2005.
We have conducted evaluations on 11 different prod- R. Girju and A. Badulescu. Automatic Discovery of Part-
ucts in four domains. The experimental results have whole Relations Computational Linguistics, 2006.
demonstrated the effectiveness of our approach. In A. Goldberg and X. Zhu. Seeing Stars When There
the future, we will explore other linguistic features Aren’t Many Stars: Graph-based Semi-supervised
to learn the semantic distance between aspects, as Learning for Sentiment Categorization. ACL, 2006.
well as apply our approach to other applications. M.A. Hearst. Automatic Acquisition of Hyponyms from
Acknowledgments Large Text Corpora. Coling, 1992.
This work is supported by NUS-Tsinghua Extreme M. Hu and B. Liu. Mining and Summarizing Customer
Search (NExT) project under the grant number: R- Reviews. SIGKDD, 2004.
252-300-001-490. We give warm thanks to the X. Hu, N. Sun, C. Zhang, and T.-S. Chua Exploiting
project and anonymous reviewers for their valuable Internal and External Semantics for the Clustering of
</bodyText>
<reference confidence="0.997296554216868">
comments. Short Texts Using World Knowledge. CIKM, 2009.
References S. Kim and E. Hovy. Determining the Sentiment of Opin-
P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan. ions. COLING, 2004.
An Exploration of Sentiment Summarization. AAAI, A. C. Konig and E. Brill. Reducing the Human Overhead
149 in Text Categorization. KDD, 2006.
Z. Kozareva, E. Riloff, and E. Hovy. Semantic Class
Learning from the Web with Hyponym Pattern Link-
age Graphs. ACL, 2008.
T. Li, Y. Zhang, and V. Sindhwani. A Non-negative Ma-
trix Tri-factorization Approach to Sentiment Classifi-
cation with Lexical Prior Knowledge. ACL, 2009.
B. Liu, M. Hu, and J. Cheng. Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web. WWW,
2005.
B. Liu. Handbook Chapter: Sentiment Analysis and Sub-
jectivity. Handbook of Natural Language Processing.
Marcel Dekker, Inc. New York, NY, USA, 2009.
L.M. Manevitz and M. Yousef. One-class SVMs for Doc-
ument Classification. Machine Learning, 2002.
Q. Mei, X. Ling, M. Wondra, H. Su, and C.X. Zhai. Topic
Sentiment Mixture: Modeling Facets and Opinions in
Weblogs. WWW, 2007.
X. Meng and H. Wang. Mining User Reviews: from
Specification to Summarization. ACL-IJCNLP, 2009.
K. Murthy, T.A. Faruquie, L.V. Subramaniam,
K.H. Prasad, and M. Mohania. Automatically
Generating Term-frequency-induced Taxonomies.
ACL, 2010.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Optimizing Informativeness and Readability for Senti-
ment Summarization. ACL, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. EMNLP, 2002.
B. Pang and L. Lee. Seeing Stars: Exploiting Class Rela-
tionships for Sentiment Categorization with respect to
Rating Scales. ACL, 2005.
B. Pang and L. Lee. Opinion mining and sentiment anal-
ysis. Foundations and Trends in Information Retrieval,
2008.
HH. Pang, J. Shen, and R. Krishnan Privacy-Preserving,
Similarity-Based Text Retrieval. ACM Transactions
on Internet Technology, 2010.
A.M. Popescu and O. Etzioni. Extracting Product Fea-
tures and Opinions from Reviews. HLT/EMNLP,
2005.
H. Poon and P. Domingos. Unsupervised Ontology In-
duction from Text. ACL, 2010.
G. Qiu, B. Liu, J. Bu, and C. Chen. Expanding Domain
Sentiment Lexicon through Double Propagation. IJ-
CAI, 2009.
S. Roy and L.V. Subramaniam. Automatic Generation
of Domain Models for Call Centers from Noisy Tran-
scriptions. ACL, 2009.
B. Shi and K. Chang. Generating a Concept Hierarchy
for Sentiment Analysis. SMC, 2008.
R. Snow and D. Jurafsky. Semantic Taxonomy Induction
from Heterogenous Evidence. ACL, 2006.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization. ACL,
2008.
P. Turney. Thumbs up or thumbs down? Semantic Orien-
tation Applied to Unsupervised Classification of Re-
views. ACL, 2002.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. Phrase Depen-
dency Parsing for Opinion Mining. ACL, 2009.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
H. Yang and J. Callan A Metric-based Framework for
Automatic Taxonomy Induction. ACL, 2009.
S. Ye and T.-S. Chua. Learning Object Models from
Semi-structured Web Documents. IEEE Transactions
on Knowledge and Data Engineering, 2006.
J. Yi, T. Nasukawa, W. Niblack, and R. Bunescu. Senti-
ment Analyzer: Extracting Sentiments about a Given
Topic using Natural Language Processing Techniques.
ICDM, 2003.
L. Zhuang,F. Jing, and X.Y. Zhu Movie Review Mining
and Summarization CIKM, 2006.
</reference>
<page confidence="0.998318">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.690816">
<title confidence="0.9970435">Domain-Assisted Product Aspect Hierarchy Generation: Hierarchical Organization of Unstructured Consumer Reviews</title>
<author confidence="0.997734">Zheng-Jun Meng Kai Tat-Seng</author>
<affiliation confidence="0.8838835">of Computing, National University of for Infocomm Research,</affiliation>
<email confidence="0.819378">zhazj,wangm,kwang@i2r.a-star.edu.sg</email>
<abstract confidence="0.99959347368421">This paper presents a domain-assisted approach to organize various aspects of a product into a hierarchy by integrating domain knowledge (e.g., the product specifications), as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of consumer reviews on various product aspects and aggregate consumer opinions on these aspects. With such organization, user can easily grasp the overview of consumer reviews. Furthermore, we apply the hierarchy to the task of implicit aspect identification which aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>