<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.853939">
Automatic Keyphrase Extraction via Topic Decomposition
</title>
<author confidence="0.99738">
Zhiyuan Liu, Wenyi Huang, Yabin Zheng and Maosong Sun
</author>
<affiliation confidence="0.7749735">
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.95997">
{lzy.thu, harrywy, yabin.zheng}@gmail.com,
sms@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.997286" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999620722222222">
Existing graph-based ranking methods for
keyphrase extraction compute a single impor-
tance score for each word via a single ran-
dom walk. Motivated by the fact that both
documents and words can be represented by
a mixture of semantic topics, we propose to
decompose traditional random walk into mul-
tiple random walks specific to various topics.
We thus build a Topical PageRank (TPR) on
word graph to measure word importance with
respect to different topics. After that, given
the topic distribution of the document, we fur-
ther calculate the ranking scores of words and
extract the top ranked ones as keyphrases. Ex-
perimental results show that TPR outperforms
state-of-the-art keyphrase extraction methods
on two datasets under various evaluation met-
rics.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994897826087">
Keyphrases are defined as a set of terms in a doc-
ument that give a brief summary of its content for
readers. Automatic keyphrase extraction is widely
used in information retrieval and digital library (Tur-
ney, 2000; Nguyen and Kan, 2007). Keyphrase ex-
traction is also an essential step in various tasks of
natural language processing such as document cate-
gorization, clustering and summarization (Manning
and Schutze, 2000).
There are two principled approaches to extracting
keyphrases: supervised and unsupervised. The su-
pervised approach (Turney, 1999) regards keyphrase
extraction as a classification task, in which a model
is trained to determine whether a candidate phrase
is a keyphrase. Supervised methods require a doc-
ument set with human-assigned keyphrases as train-
ing set. In Web era, articles increase exponentially
and change dynamically, which demands keyphrase
extraction to be efficient and adaptable. However,
since human labeling is time consuming, it is im-
practical to label training set from time to time.
We thus focus on the unsupervised approach in this
study.
In the unsupervised approach, graph-based rank-
ing methods are state-of-the-art (Mihalcea and Ta-
rau, 2004). These methods first build a word graph
according to word co-occurrences within the docu-
ment, and then use random walk techniques (e.g.,
PageRank) to measure word importance. After that,
top ranked words are selected as keyphrases.
Existing graph-based methods maintain a single
importance score for each word. However, a docu-
ment (e.g., news article or research article) is usu-
ally composed of multiple semantic topics. Taking
this paper for example, it refers to two major top-
ics, “keyphrase extraction” and “random walk”. As
words are used to express various meanings corre-
sponding to different semantic topics, a word will
play different importance roles in different topics
of the document. For example, the words “phrase”
and “extraction” will be ranked to be more impor-
tant in topic “keyphrase extraction”, while the words
“graph” and “PageRank” will be more important in
topic “random walk”. Since they do not take topics
into account, graph-based methods may suffer from
the following two problems:
</bodyText>
<footnote confidence="0.59194725">
1. Good keyphrases should be relevant to the ma-
jor topics of the given document. In graph-
based methods, the words that are strongly con-
nected with other words tend to be ranked high,
</footnote>
<page confidence="0.940301">
366
</page>
<note confidence="0.8193015">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 366–376,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.992098129032258">
which do not necessarily guarantee they are rel-
evant to major topics of the document.
2. An appropriate set of keyphrases should also
have a good coverage of the document’s ma-
jor topics. In graph-based methods, the ex-
tracted keyphrases may fall into a single topic
of the document and fail to cover other substan-
tial topics of the document.
To address the problem, it is intuitive to consider
the topics of words and document in random walk
for keyphrase extraction. In this paper, we pro-
pose to decompose traditional PageRank into multi-
ple PageRanks specific to various topics and obtain
the importance scores of words under different top-
ics. After that, with the help of the document topics,
we can further extract keyphrases that are relevant
to the document and at the same time have a good
coverage of the document’s major topics. We call
the topic-decomposed PageRank as Topical PageR-
ank (TPR).
In experiments we find that TPR can extract
keyphrases with high relevance and good cover-
age, which outperforms other baseline methods un-
der various evaluation metrics on two datasets. We
also investigate the performance of TPR with dif-
ferent parameter values and demonstrate its robust-
ness. Moreover, TPR is unsupervised and language-
independent, which is applicable in Web era with
enormous information.
TPR for keyphrase extraction is a two-stage pro-
cess:
</bodyText>
<listItem confidence="0.99433725">
1. Build a topic interpreter to acquire the topics of
words and documents.
2. Perform TPR to extract keyphrases for docu-
ments.
</listItem>
<bodyText confidence="0.991569">
We will introduce the two stages in Section 2 and
Section 3.
</bodyText>
<sectionHeader confidence="0.920716" genericHeader="introduction">
2 Building Topic Interpreters
</sectionHeader>
<bodyText confidence="0.9999492">
To run TPR on a word graph, we have to acquire
topic distributions of words. There are roughly two
approaches that can provide topics of words: (1) Use
manually annotated knowledge bases, e.g., Word-
Net (Miller et al., 1990); (2) Use unsupervised ma-
chine learning techniques to obtain word topics from
a large-scale document collection. Since the vocab-
ulary in WordNet cannot cover many words in mod-
ern news and research articles, we employ the sec-
ond approach to build topic interpreters for TPR.
In machine learning, various methods have been
proposed to infer latent topics of words and docu-
ments. These methods, known as latent topic mod-
els, derive latent topics from a large-scale document
collection according to word occurrence informa-
tion. Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is a representative of topic models. Com-
pared to Latent Semantic Analysis (LSA) (Landauer
et al., 1998) and probabilistic LSA (pLSA) (Hof-
mann, 1999), LDA has more feasibility for inference
and can reduce the risk of over-fitting.
In LDA, each word w of a document d is regarded
to be generated by first sampling a topic z from d’s
topic distribution θ(d), and then sampling a word
from the distribution over words φ(z) that charac-
terizes topic z. In LDA, θ(d) and φ(z) are drawn
from conjugate Dirichlet priors α and β, separately.
Therefore, θ and φ are integrated out and the prob-
ability of word w given document d and priors is
represented as follows:
</bodyText>
<equation confidence="0.997494333333333">
K
pr(w|d, α, β) = pr(w|z,β)pr(z|d, α), (1)
z=1
</equation>
<bodyText confidence="0.999077888888889">
where K is the number of topics.
Using LDA, we can obtain the topic distribution
of each word w, namely pr(z|w) for topic z ∈ K.
The word topic distributions will be used in TPR.
Moreover, using the obtained word topic distribu-
tions, we can infer the topic distribution of a new
document (Blei et al., 2003), namely pr(z|d) for
each topic z ∈ K, which will be used for ranking
keyphrases.
</bodyText>
<sectionHeader confidence="0.974453" genericHeader="method">
3 Topical PageRank for Keyphrase
Extraction
</sectionHeader>
<bodyText confidence="0.989396">
After building a topic interpreter to acquire the
topics of words and documents, we can perform
keyphrase extraction for documents via TPR. Given
a document d, the process of keyphrase extraction
using TPR consists of the following four steps which
is also illustrated in Fig. 1:
</bodyText>
<footnote confidence="0.562825">
1. Construct a word graph for d according to word
co-occurrences within d.
</footnote>
<page confidence="0.996899">
367
</page>
<figureCaption confidence="0.997273">
Figure 1: Topical PageRank for Keyphrase Extraction.
</figureCaption>
<listItem confidence="0.9963402">
2. Perform TPR to calculate the importance
scores for each word with respect to different
topics.
3. Using the topic-specific importance scores of
words, rank candidate keyphrases respect to
each topic separately.
4. Given the topics of document d, integrate the
topic-specific rankings of candidate keyphrases
into a final ranking, and the top ranked ones are
selected as keyphrases.
</listItem>
<subsectionHeader confidence="0.999626">
3.1 Constructing Word Graph
</subsectionHeader>
<bodyText confidence="0.999985111111111">
We construct a word graph according to word co-
occurrences within the given document, which ex-
presses the cohesion relationship between words
in the context of document. The document is re-
garded as a word sequence, and the link weights be-
tween words is simply set to the co-occurrence count
within a sliding window with maximum W words in
the word sequence.
It was reported in (Mihalcea and Tarau, 2004)
the graph direction does not influence the perfor-
mance of keyphrase extraction very much. In this
paper we simply construct word graphs with direc-
tions. The link directions are determined as follows.
When sliding a W-width window, at each position,
we add links from the first word pointing to other
words within the window. Since keyphrases are usu-
ally noun phrases, we only add adjectives and nouns
in word graph.
</bodyText>
<subsectionHeader confidence="0.998607">
3.2 Topical PageRank
</subsectionHeader>
<bodyText confidence="0.997673555555556">
Before introducing TPR, we first give some formal
notations. We denote G = (V, E) as the graph of a
document, with vertex set V = {w1, w2,··· , wN}
and link set (wi, wj) E E if there is a link from
wi to wj. In a word graph, each vertex represents
a word, and each link indicates the relatedness be-
tween words. We denote the weight of link (wi, wj)
as e(wi, wj), and the out-degree of vertex wi as
O(wi)=L�j:wi→wj e(wi,wj).
Topical PageRank is based on PageRank (Page et
al., 1998). PageRank is a well known ranking al-
gorithm that uses link information to assign global
importance scores to web pages. The basic idea of
PageRank is that a vertex is important if there are
other important vertices pointing to it. This can be
regarded as voting or recommendation among ver-
tices. In PageRank, the score R(wi) of word wi is
defined as
</bodyText>
<equation confidence="0.987335333333333">
e(wj, wi)
O(wj) R(wj) + (1 − λ) 1
|V |,
</equation>
<bodyText confidence="0.976942727272727">
(2)
where λ is a damping factor range from 0 to 1, and
|V  |is the number of vertices. The damping fac-
tor indicates that each vertex has a probability of
(1 − λ) to perform random jump to another vertex
within this graph. PageRank scores are obtained by
running Eq. (2) iteratively until convergence. The
second term in Eq. (2) can be regarded as a smooth-
ing factor to make the graph fulfill the property of
being aperiodic and irreducible, so as to guarantee
that PageRank converges to a unique stationary dis-
</bodyText>
<equation confidence="0.9426422">
R(wi) = λ �
j:wj→wi
368
e(wj, wi)
O(wj) Rz(wj)+(1−λ)pz(wi).
</equation>
<bodyText confidence="0.999762083333333">
tribution. In PageRank, the second term is set to be
the same value 1
�V for all vertices within the graph,
which indicates there are equal probabilities of ran-
dom jump to all vertices.
In fact, the second term of PageRank in Eq. (2)
can be set to be non-uniformed. Suppose we as-
sign larger probabilities to some vertices, the final
PageRank scores will prefer these vertices. We call
this Biased PageRank.
The idea of Topical PageRank (TPR) is to run
Biased PageRank for each topic separately. Each
topic-specific PageRank prefers those words with
high relevance to the corresponding topic. And
the preferences are represented using random jump
probabilities of words.
Formally, in the PageRank of a specific topic
z, we will assign a topic-specific preference value
pz(w) to each word w as its random jump proba-
bility with EwEV pz(w) = 1. The words that are
more relevant to topic z will be assigned larger prob-
abilities when performing the PageRank. For topic
z, the topic-specific PageRank scores are defined as
follows:
</bodyText>
<equation confidence="0.9243875">
�
Rz(wi) = λ
j:wj→wi
(3)
</equation>
<bodyText confidence="0.9998512">
In Fig. 1, we show an example with two topics. In
this figure, we use the size of circles to indicate how
relevant the word is to the topic. In the PageRanks
of the two topics, high preference values will be as-
signed to different words with respect to the topic.
Finally, the words will get different PageRank val-
ues in the two PageRanks.
The setting of preference values pz(w) will have
a great influence to TPR. In this paper we use three
measures to set preference values for TPR:
</bodyText>
<listItem confidence="0.951795111111111">
• pz(w) = pr(w|z), is the probability that word
w occurs given topic z. This indicates how
much that topic z focuses on word w.
• pz(w) = pr(z|w), is the probability of topic z
given word w. This indicates how much that
word w focuses on topic z.
• pz(w) = pr(w|z) x pr(z|w), is the product of
hub and authority values. This measure is in-
spired by the work in (Cohn and Chang, 2000).
</listItem>
<bodyText confidence="0.9930338">
Both PageRank and TPR are all iterative algo-
rithms. We terminate the algorithms when the num-
ber of iterations reaches 100 or the difference of each
vertex between two neighbor iterations is less than
0.001.
</bodyText>
<subsectionHeader confidence="0.997792">
3.3 Extract Keyphrases Using Ranking Scores
</subsectionHeader>
<bodyText confidence="0.999842785714286">
After obtaining word ranking scores using TPR, we
begin to rank candidate keyphrases. As reported in
(Hulth, 2003), most manually assigned keyphrases
turn out to be noun phrases. We thus select noun
phrases from a document as candidate keyphrases
for ranking.
The candidate keyphrases of a document is ob-
tained as follows. The document is first tokenized.
After that, we annotate the document with part-
of-speech (POS) tags 1. Third, we extract noun
phrases with pattern (adjective)*(noun)+,
which represents zero or more adjectives followed
by one or more nouns. We regard these noun phrases
as candidate keyphrases.
After identifying candidate keyphrases, we rank
them using the ranking scores obtained by TPR.
In PageRank for keyphrase extraction, the ranking
score of a candidate keyphrase p is computed by
summing up the ranking scores of all words within
the phrase: R(p)=EwiEp R(wi) (Mihalcea and Ta-
rau, 2004; Wan and Xiao, 2008a; Wan and Xiao,
2008b). Then candidate keyphrases are ranked in
descending order of ranking scores. The top M can-
didates are selected as keyphrases.
In TPR for keyphrase extraction, we first com-
pute the ranking scores of candidate keyphrases sep-
arately for each topic. That is for each topic z we
compute
</bodyText>
<equation confidence="0.843766">
Rz(p) = � Rz(wi). (4)
wiEp
</equation>
<bodyText confidence="0.999981571428571">
By considering the topic distribution of document,
We further integrate topic-specific rankings of can-
didate keyphrases into a final ranking and extract
top-ranked ones as the keyphrases of the document.
Denote the topic distribution of the document d
as pr(z|d) for each topic z. For each candidate
keyphrase p, we compute its final ranking score as
</bodyText>
<footnote confidence="0.906099666666667">
1In experiments we use Stanford POS Tagger from http:
//nlp.stanford.edu/software/tagger.shtml
with English tagging model left3words-distsim-wsj.
</footnote>
<page confidence="0.99159">
369
</page>
<bodyText confidence="0.999817142857143">
follows: R(p) = K Rz(p) x pr(z d). (5) Porter Stemmer 5 for comparison. In experiments
z=1 we select three evaluation metrics.
The first metric is precision/recall/F-measure rep-
resented as follows,
After ranking candidate phrases in descending order
of their integrated ranking scores, we select the top
M as the keyphrases of document d.
</bodyText>
<sectionHeader confidence="0.999095" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.945479">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999944448275862">
To evaluate the performance of TPR for keyphrase
extraction, we carry out experiments on two
datasets.
One dataset was built by Wan and Xiao 2 which
was used in (Wan and Xiao, 2008b). This dataset
contains 308 news articles in DUC2001 (Over et al.,
2001) with 2,488 manually annotated keyphrases.
There are at most 10 keyphrases for each document.
In experiments we refer to this dataset as NEWS.
The other dataset was built by Hulth 3 which was
used in (Hulth, 2003). This dataset contains 2, 000
abstracts of research articles and 19, 254 manually
annotated keyphrases. In experiments we refer to
this dataset as RESEARCH.
Since neither NEWS nor RESEARCH itself is
large enough to learn efficient topics, we use the
Wikipedia snapshot at March 2008 4 to build topic
interpreters with LDA. After removing non-article
pages and the articles shorter than 100 words, we
collected 2,122, 618 articles. After tokenization,
stop word removal and word stemming, we build the
vocabulary by selecting 20, 000 words according to
their document frequency. We learn LDA models by
taking each Wikipedia article as a document. In ex-
periments we learned several models with different
numbers of topics, from 50 to 1, 500 respectively.
For the words absent in topic models, we simply set
the topic distribution of the word as uniform distri-
bution.
</bodyText>
<subsectionHeader confidence="0.939777">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9759955">
For evaluation, the words in both standard and ex-
tracted keyphrases are reduced to base forms using
</bodyText>
<footnote confidence="0.9845645">
2http://wanxiaojun1979.googlepages.com.
3It was obtained from the author.
4http://en.wikipedia.org/wiki/Wikipedia_
database.
</footnote>
<equation confidence="0.804419">
f = 2prp+r (6)
,
</equation>
<bodyText confidence="0.999906571428571">
where ccorrect is the total number of correct
keyphrases extracted by a method, cextract the to-
tal number of automatic extracted keyphrases, and
cstandard the total number of human-labeled stan-
dard keyphrases.
We note that the ranking order of extracted
keyphrases also indicates the method performance.
An extraction method will be better than another one
if it can rank correct keyphrases higher. However,
precision/recall/F-measure does not take the order
of extracted keyphrases into account. To address the
problem, we select the following two additional met-
rics.
One metric is binary preference measure
(Bpref) (Buckley and Voorhees, 2004). Bpref is
desirable to evaluate the performance considering
the order in which the extracted keyphrases are
ranked. For a document, if there are R correct
keyphrases within M extracted keyphrases by a
method, in which r is a correct keyphrase and n is
an incorrect keyphrase, Bpref is defined as follows,
</bodyText>
<table confidence="0.94622925">
1 � 1 − M
Bpref = n ranked higher than r . (7)
R
rER
</table>
<bodyText confidence="0.997499166666667">
The other metric is mean reciprocal rank
(MRR) (Voorhees, 2000) which is used to evaluate
how the first correct keyphrase for each document is
ranked. For a document d, rankd is denoted as the
rank of the first correct keyphrase with all extracted
keyphrases, MRR is defined as follows,
</bodyText>
<equation confidence="0.9957925">
MRR = D d rankd, (8)
1 1
</equation>
<bodyText confidence="0.99949975">
where D is the document set for keyphrase extrac-
tion.
Note that although the evaluation scores of most
keyphrase extractors are still lower compared to
</bodyText>
<footnote confidence="0.856027">
5http://tartarus.org/˜martin/
PorterStemmer.
</footnote>
<figure confidence="0.9319555">
ccorrect
ccorrect
p=
, r =
cextract
cstandard
</figure>
<page confidence="0.98125">
370
</page>
<bodyText confidence="0.999332666666667">
other NLP-tasks, it does not indicate the perfor-
mance is poor because even different annotators may
assign different keyphrases to the same document.
</bodyText>
<subsectionHeader confidence="0.996874">
4.3 Influences of Parameters to TPR
</subsectionHeader>
<bodyText confidence="0.993772066666667">
There are four parameters in TPR that may influence
the performance of keyphrase extraction including:
(1) window size W for constructing word graph, (2)
the number of topics K learned by LDA, (3) dif-
ferent settings of preference values pz(w), and (4)
damping factor A of TPR.
In this section, we look into the influences of these
parameters to TPR for keyphrase extraction. Except
the parameter under investigation, we set parameters
to the following values: W =10, K =1, 000, A=0.3
and pz(w) = pr(zIw), which are the settings when
TPR achieves the best (or near best) performance on
both NEWS and RESEARCH. In the following tables,
we use “Pre.”, “Rec.” and “F.” as the abbreviations
of precision, recall and F-measure.
</bodyText>
<subsectionHeader confidence="0.513105">
4.3.1 Window Size W
</subsectionHeader>
<bodyText confidence="0.999482">
In experiments on NEWS, we find that the perfor-
mance of TPR is stable when W ranges from 5 to 20
as shown in Table 1. This observation is consistent
with the findings reported in (Wan and Xiao, 2008b).
</bodyText>
<table confidence="0.998405">
Size Pre. Rec. F. Bpref MRR
5 0.280 0.345 0.309 0.213 0.636
10 0.282 0.348 0.312 0.214 0.638
15 0.282 0.347 0.311 0.214 0.646
20 0.284 0.350 0.313 0.215 0.644
</table>
<tableCaption confidence="0.998756">
Table 1: Influence of window size W when the num-
</tableCaption>
<bodyText confidence="0.965153076923077">
ber of keyphrases M =10 on NEWS.
Similarly, when W ranges from 2 to 10, the per-
formance on RESEARCH does not change much.
However, the performance on NEWS will become
poor when W = 20. This is because the abstracts
in RESEARCH (there are 121 words per abstract on
average) are much shorter than the news articles
in NEWS (there are 704 words per article on av-
erage). If the window size W is set too large on
RESEARCH, the graph will become full-connected
and the weights of links will tend to be equal, which
cannot capture the local structure information of ab-
stracts for keyphrase extraction.
</bodyText>
<subsectionHeader confidence="0.792392">
4.3.2 The Number of Topics K
</subsectionHeader>
<bodyText confidence="0.999651">
We demonstrate the influence of the number of
topics K of LDA models in Table 2. Table 2 shows
the results when K ranges from 50 to 1, 500 and
M =10 on NEWS. We observe that the performance
does not change much as the number of topics
varies until the number is much smaller (K = 50).
The influence is similar on RESEARCH which indi-
cates that LDA is appropriate for obtaining topics of
words and documents for TPR to extract keyphrases.
</bodyText>
<table confidence="0.998787833333333">
K Pre. Rec. F. Bpref MRR
50 0.268 0.330 0.296 0.204 0.632
100 0.276 0.340 0.304 0.208 0.632
500 0.284 0.350 0.313 0.215 0.648
1000 0.282 0.348 0.312 0.214 0.638
1500 0.282 0.348 0.311 0.214 0.631
</table>
<tableCaption confidence="0.9703645">
Table 2: Influence of the number of topics K when
the number of keyphrases M =10 on NEWS.
</tableCaption>
<subsectionHeader confidence="0.815241">
4.3.3 Damping Factor A
</subsectionHeader>
<bodyText confidence="0.871084545454545">
Damping factor A of TPR reconciles the influ-
ences of graph walks (the first term in Eq.(3)) and
preference values (the second term in Eq.(3)) to the
topic-specific PageRank scores. We demonstrate
the influence of A on NEWS in Fig. 2. This fig-
ure shows the precision/recall/F-measure when A =
0.1, 0.3, 0.5, 0.7, 0.9 and M ranges from 1 to 20.
From this figure we find that, when A is set from 0.2
to 0.7, the performance is consistently good. The
values of Bpref and MRR also keep stable with the
variations of A.
</bodyText>
<subsectionHeader confidence="0.628073">
4.3.4 Preference Values
</subsectionHeader>
<bodyText confidence="0.999977461538462">
Finally, we explore the influences of different set-
tings of preference values for TPR in Eq.(3). In Ta-
ble 3 we show the influence when the number of
keyphrases M = 10 on NEWS. From the table, we
observe that pr(zIw) performs the best. The similar
observation is also got on RESEARCH.
In keyphrase extraction task, it is required to find
the keyphrases that can appropriately represent the
topics of the document. It thus does not want to ex-
tract those phrases that may appear in multiple top-
ics like common words. The measure pr(wlz) as-
signs preference values according to how frequently
that words appear in the given topic. Therefore, the
</bodyText>
<page confidence="0.989226">
371
</page>
<figure confidence="0.999764418181818">
Recall
(a) Precision
Keyphrase Number
(b) Recall
(c) F-measure
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Keyphrase Number
Precision
0.45
0.35
0.25
0.5
0.4
0.3
0.2
λ=0.1
λ=0.3
λ=0.5
λ=0.7
λ=0.9
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
λ=0.1
λ=0.3
λ=0.5
λ=0.7
λ=0.9
Keyphrase Number
F−measure
0.28
0.26
0.24
0.22
0.18
0.16
0.14
0.12
0.3
0.2
0.1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
λ=0.1
λ=0.3
λ=0.5
λ=0.7
λ=0.9
</figure>
<figureCaption confidence="0.99922">
Figure 2: Precision, recall and F-measure of TPR with A=0.1, 0.3, 0.5, 0.7 and 0.9 when M ranges from 1
to 20 on NEWS.
</figureCaption>
<bodyText confidence="0.998915125">
common words will always be assigned to a rela-
tively large value in each topic-specific PageRank
and finally obtain a high rank. pr(w|z) is thus not a
good setting of preference values in TPR. In the con-
trast, pr(z|w) prefers those words that are focused
on the given topic. Using pr(z|w) to set preference
values for TPR, we will tend to extract topic-focused
phrases as keyphrases.
</bodyText>
<table confidence="0.95163675">
Pref Pre. Rec. F. Bpref MRR
pr(w|z) 0.256 0.316 0.283 0.192 0.584
pr(z|w) 0.282 0.348 0.312 0.214 0.638
prod 0.259 0.320 0.286 0.193 0.587
</table>
<tableCaption confidence="0.8857935">
Table 3: Influence of three preference value settings
when the number of keyphrases M =10 on NEWS.
</tableCaption>
<subsectionHeader confidence="0.999803">
4.4 Comparing with Baseline Methods
</subsectionHeader>
<bodyText confidence="0.996506642857143">
After we explore the influences of parameters to
TPR, we obtain the best results on both NEWS and
RESEARCH. We further select three baseline meth-
ods, i.e., TFIDF, PageRank and LDA, to compare
with TPR.
The TFIDF computes the ranking scores of words
based on words’ tfidf values in the document,
namely R(w) = tf,,, × log(idf,,,). While in PageR-
ank (i.e., TextRank), the ranking scores of words are
obtained using Eq.(2). The two baselines do not use
topic information of either words or documents. The
LDA computes the ranking score for each word us-
ing the topical similarity between the word and the
document. Given the topics of the document d and
a word w, We have used various methods to com-
pute similarity including cosine similarity, predic-
tive likelihood and KL-divergence (Heinrich, 2005),
among which cosine similarity performs the best on
both datasets. Therefore, we only show the results of
the LDA baseline calculated using cosine similarity.
In Tables 4 and 5 we show the compar-
ing results of the four methods on both NEWS
and RESEARCH. Since the average number of
manual-labeled keyphrases on NEWS is larger than
RESEARCH, we set M = 10 for NEWS and M =
5 for RESEARCH. The parameter settings on both
NEWS and RESEARCH have been stated in Section
4.3.
</bodyText>
<table confidence="0.9978682">
Method Pre. Rec. F. Bpref MRR
TFIDF 0.239 0.295 0.264 0.179 0.576
PageRank 0.242 0.299 0.267 0.184 0.564
LDA 0.259 0.320 0.286 0.194 0.518
TPR 0.282 0.348 0.312 0.214 0.638
</table>
<tableCaption confidence="0.84433">
Table 4: Comparing results on NEWS when the num-
ber of keyphrases M =10.
</tableCaption>
<table confidence="0.9994914">
Method Pre. Rec. F. Bpref MRR
TFIDF 0.333 0.173 0.227 0.255 0.565
PageRank 0.330 0.171 0.225 0.263 0.575
LDA 0.332 0.172 0.227 0.254 0.548
TPR 0.354 0.183 0.242 0.274 0.583
</table>
<tableCaption confidence="0.984323">
Table 5: Comparing results on RESEARCH when the
</tableCaption>
<bodyText confidence="0.880208">
number of keyphrases M =5.
From the two tables, we have the following obser-
vations.
</bodyText>
<page confidence="0.995735">
372
</page>
<figureCaption confidence="0.99256975">
Figure 3: Precision-recall results on NEWS when M
ranges from 1 to 20.
Figure 4: Precision-recall results on RESEARCH
when M ranges from 1 to 10.
</figureCaption>
<figure confidence="0.998292545454545">
Precision
Recall
0.45
0.35
0.25
0.15
0.05
0.5
0.4
0.3
0.2
0.1
0
0.2 0.25 0.3 0.35 0.4 0.45
TFIDF
PageRank
LDA
TPR
0.5
Precision
Recall
0.25
0.15
0.05
0.3
0.2
0.1
0
0.3 0.32 0.34 0.36 0.38 0.4 0.42
TFIDF
PageRank
LDA
TPR
</figure>
<bodyText confidence="0.999557807692308">
First, TPR outperform all baselines on both
datasets. The improvements are all statistically sig-
nificant tested with bootstrap re-sampling with 95%
confidence. This indicates the robustness and effec-
tiveness of TPR.
Second, LDA performs equal or better than
TFIDF and PageRank under precision/recall/F-
measure. However, the performance of LDA un-
der MRR is much worse than TFIDF and PageR-
ank, which indicates LDA fails to correctly extract
the first keyphrase earlier than other methods. The
reason is: (1) LDA does not consider the local struc-
ture information of document as PageRank, and (2)
LDA also does not consider the frequency infor-
mation of words within the document. In the con-
trast, TPR enjoys the advantages of both LDA and
TFIDF/PageRank, by using the external topic infor-
mation like LDA and internal document structure
like TFIDF/PageRank.
Moreover, in Figures 3 and 4 we show the
precision-recall relations of four methods on NEWS
and RESEARCH. Each point on the precision-recall
curve is evaluated on different numbers of extracted
keyphrases M. The closer the curve to the upper
right, the better the overall performance. The results
again illustrate the superiority of TPR.
</bodyText>
<subsectionHeader confidence="0.999228">
4.5 Extracting Example
</subsectionHeader>
<bodyText confidence="0.994631774193548">
At the end, in Table 6 we show an example of
extracted keyphrases using TPR from a news arti-
cle with title “Arafat Says U.S. Threatening to Kill
PLO Officials” (The article number in DUC2001
is AP880510-0178). Here we only show the top
10 keyphrases, and the correctly extracted ones
are marked with “(+)”. We also mark the num-
ber of correctly extracted keyphrases after method
name like “(+7)” after TPR. We also illustrate the
top 3 topics of the document with their topic-
specific keyphrases. It is obvious that the top topics,
on “Palestine”, “Israel” and “terrorism” separately,
have a good coverage on the discussion objects of
this article, which also demonstrate a good diversity
with each other. By integrating these topic-specific
keyphrases considering the proportions of these top-
ics, we obtain the best performance of keyphrase ex-
traction using TPR.
In Table 7 we also show the extracted keyphrases
of baselines from the same news article. For TFIDF,
it only considered the frequency properties of words,
and thus highly ranked the phrases with “PLO”
which appeared about 16 times in this article, and
failed to extract the keyphrases on topic “Israel”.
LDA only measured the importance of words using
document topics without considering the frequency
information of words and thus missed keyphrases
with high-frequency words. For example, LDA
failed to extract keyphrase “political assassination”,
in which the word “assassination” occurred 8 times
in this article.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.998089142857143">
In this paper we proposed TPR for keyphrase ex-
traction. A pioneering achievement in keyphrase ex-
traction was carried out in (Turney, 1999) which re-
garded keyphrase extraction as a classification task.
Generally, the supervised methods need manually
annotated training set which is time-consuming and
in this paper we focus on unsupervised method.
</bodyText>
<page confidence="0.997892">
373
</page>
<table confidence="0.998253783783784">
TFIDF (+5)
PLO leader Yasser Arafat(+), PLO attacks, PLO
offices, PLO officials(+), PLO leaders, Abu Ji-
had, terrorist attacks(+), Khalil Wazir(+), slaying
wazir, political assassination(+)
PageRank (+3)
PLO leader Yasser Arafat(+), PLO officials(+),
PLO attacks, United States(+), PLO offices, PLO
leaders, State Department spokesman Charles
Redman, U.S. government document, alleged
document, Abu Jihad
TPR (+7)
PLO leader Yasser Arafat(+), Abu Jihad, Khalil
Wazir(+), slaying Wazir, political assassina-
tion(+), Palestinian guerrillas(+), particulary
Palestinian circles, Israeli officials(+), Israeli
squad(+), terrorist attacks(+)
TPR, Rank 1 Topic on “Palestine”
PLO leader Yasser Arafat(+), United States(+),
State Department spokesman Charles Redman,
Abu Jihad, U.S. government document, Palestine
Liberation Organization leader, political assassi-
nation(+), Israeli officials(+), alleged document
TPR, Rank 2 Topic on “Israel”
PLO leader Yasser Arafat(+), United States(+),
Palestine Liberation Organization leader, Israeli
officials(+), U.S. government document, alleged
document, Arab government, slaying Wazir, State
Department spokesman Charles Redman, Khalil
Wazir(+)
TPR, Rank 3 Topic on “terrorism”
terrorist attacks(+), PLO leader Yasser Arafat(+),
Abu Jihad, United States(+), alleged docu-
ment, U.S. government document, Palestine Lib-
eration Organization leader, State Department
spokesman Charles Redman, political assassina-
tion(+), full cooperation
</table>
<tableCaption confidence="0.998826">
Table 6: Extracted keyphrases by TPR.
</tableCaption>
<bodyText confidence="0.999916388888889">
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becoming
the most widely used unsupervised approach for
keyphrase extraction. Litvak and Last (2008) ap-
plied HITS algorithm on the word graph of a docu-
ment for keyphrase extraction. Although HITS itself
worked the similar performance to PageRank, we
plan to explore the integration of topics and HITS in
future work. Wan (2008b; 2008a) used a small num-
ber of nearest neighbor documents to provide more
knowledge for keyphrase extraction. Some meth-
ods used clustering techniques on word graphs for
keyphrase extraction (Grineva et al., 2009; Liu et
al., 2009). The clustering-based method performed
well on short abstracts (with F-measure 0.382 on
RESEARCH) but poorly on long articles (NEWS with
F-measure score 0.216) due to two non-trivial is-
sues: (1) how to determine the number of clus-
</bodyText>
<sectionHeader confidence="0.920171" genericHeader="method">
LDA (+5)
</sectionHeader>
<reference confidence="0.903937333333333">
PLO leader Yasser Arafat(+), Palestine Liberation
Organization leader, Khalil Wazir(+), Palestinian
guerrillas(+), Abu Jihad, Israeli officials(+), par-
ticulary Palestinian circles, Arab government,
State Department spokesman Charles Redman,
Israeli squad(+)
</reference>
<tableCaption confidence="0.989791">
Table 7: Extracted keyphrases by baselines.
</tableCaption>
<bodyText confidence="0.999930961538462">
ters, and (2) how to weight each cluster and select
keyphrases from the clusters. In this paper we fo-
cus on improving graph-based methods via topic de-
composition, we thus only compare with PageRank
as well as TFIDF and LDA and do not compare with
clustering-based methods in details.
In recent years, two algorithms were proposed to
rank web pages by incorporating topic information
of web pages within PageRank (Haveliwala, 2002;
Nie et al., 2006). The method in (Haveliwala, 2002),
is similar to TPR which also decompose PageRank
into various topics. However, the method in (Haveli-
wala, 2002) only considered to set the preference
values using pr(wIz) (In the context of (Haveliwala,
2002), w indicates Web pages). In Section 4.3.4 we
have shown that the setting of using pr(z|w) is much
better than pr(wIz).
Nie et al. (2006) proposed a more complicated
ranking method. In this method, topical PageRanks
are performed together. The basic idea of (Nie et al.,
2006) is, when surfing following a graph link from
vertex wz to wj, the ranking score on topic z of wz
will have a higher probability to pass to the same
topic of wj and have a lower probability to pass to
a different topic of wj. When the inter-topic jump
probability is 0, this method is identical to (Haveli-
</bodyText>
<page confidence="0.996388">
374
</page>
<bodyText confidence="0.99893075">
wala, 2002). We implemented the method and found
that the random jumps between topics did not help
improve the performance for keyphrase extraction,
and did not demonstrate the results of this method.
</bodyText>
<sectionHeader confidence="0.997519" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9996168">
In this paper we propose a new graph-based frame-
work, Topical PageRank, which incorporates topic
information within random walk for keyphrase ex-
traction. Experiments on two datasets show that
TPR achieves better performance than other base-
line methods. We also investigate the influence of
various parameters on TPR, which indicates the ef-
fectiveness and robustness of the new method.
We consider the following research directions as
future work.
</bodyText>
<listItem confidence="0.77329375">
1. In this paper we obtained latent topics us-
ing LDA learned from Wikipedia. We de-
sign to obtain topics using other machine learn-
ing methods and from other knowledge bases,
and investigate the influence to performance of
keyphrase extraction.
2. In this paper we integrated topic information
in PageRank. We plan to consider topic infor-
mation in other graph-based ranking algorithms
such as HITS (Kleinberg, 1999).
3. In this paper we used Wikipedia to train
LDA by assuming Wikipedia is an exten-
</listItem>
<bodyText confidence="0.965979285714286">
sive snapshot of human knowledge which can
cover most topics talked about in NEWS and
RESEARCH. In fact, the learned topics are
highly dependent on the learning corpus. We
will investigate the influence of corpus selec-
tion in training LDA for keyphrase extraction
using TPR.
</bodyText>
<sectionHeader confidence="0.998964" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994228571428571">
This work is supported by the National Natu-
ral Science Foundation of China under Grant No.
60873174. The authors would like to thank Anette
Hulth and Xiaojun Wan for kindly sharing their
datasets. The authors would also thank Xiance Si,
Tom Chao Zhou, Peng Li for their insightful sug-
gestions and comments.
</bodyText>
<sectionHeader confidence="0.997724" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999154">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022, January.
C. Buckley and E.M. Voorhees. 2004. Retrieval evalu-
ation with incomplete information. In Proceedings of
SIGIR, pages 25–32.
David Cohn and Huan Chang. 2000. Learning to prob-
abilistically identify authoritative documents. In Pro-
ceedings ofICML, pages 167–174.
M. Grineva, M. Grinev, and D. Lizorkin. 2009. Extract-
ing key terms from noisy and multi-theme documents.
In Proceedings of WWW, pages 661–670.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
Proceedings of WWW, pages 517–526.
G. Heinrich. 2005. Parameter estimation for text anal-
ysis. Web: http://www. arbylon. net/publications/text-
est.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of SIGIR, pages 50–57.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Proceed-
ings of EMNLP, pages 216–223.
J.M. Kleinberg. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5):604–
632.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259–284.
Marina Litvak and Mark Last. 2008. Graph-based key-
word extraction for single-document summarization.
In Proceedings of the workshop Multi-source Mul-
tilingual Information Extraction and Summarization,
pages 17–24.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun.
2009. Clustering to find exemplar terms for keyphrase
extraction. In Proceedings of EMNLP, pages 257–
266.
C.D. Manning and H. Schutze. 2000. Foundations of
statistical natural language processing. MIT Press.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, pages
404–411.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
WordNet: An on-line lexical database. International
Journal ofLexicography, 3:235–244.
Thuy Nguyen and Min-Yen Kan. 2007. Keyphrase ex-
traction in scientific publications. In Proceedings of
the 10th International Conference on Asian Digital Li-
braries, pages 317–326.
</reference>
<page confidence="0.987204">
375
</page>
<reference confidence="0.999422814814815">
Lan Nie, Brian D. Davison, and Xiaoguang Qi. 2006.
Topical link analysis for web search. In Proceedings
of SIGIR, pages 91–98.
P. Over, W. Liggett, H. Gilbert, A. Sakharov, and
M. Thatcher. 2001. Introduction to duc-2001: An in-
trinsic evaluation of generic news text summarization
systems. In Proceedings ofDUC2001.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford Digital Library Tech-
nologies Project, 1998.
Peter D. Turney. 1999. Learning to extract keyphrases
from text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303–336.
E.M. Voorhees. 2000. The trec-8 question answering
track report. In Proceedings of TREC, pages 77–82.
Xiaojun Wan and Jianguo Xiao. 2008a. Collabrank:
Towards a collaborative approach to single-document
keyphrase extraction. In Proceedings of COLING,
pages 969–976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single document
keyphrase extraction using neighborhood knowledge.
In Proceedings ofAAAI, pages 855–860.
</reference>
<page confidence="0.999103">
376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.401074">
<title confidence="0.999786">Automatic Keyphrase Extraction via Topic Decomposition</title>
<author confidence="0.996233">Wenyi Huang Liu</author>
<author confidence="0.996233">Yabin Zheng</author>
<affiliation confidence="0.962672">Department of Computer Science and State Key Lab on Intelligent Technology and National Lab for Information Science and Tsinghua University, Beijing 100084,</affiliation>
<email confidence="0.78194">harrywy,sms@tsinghua.edu.cn</email>
<abstract confidence="0.988124210526316">Existing graph-based ranking methods for extraction compute a imporscore for each word via a random walk. Motivated by the fact that both documents and words can be represented by a mixture of semantic topics, we propose to decompose traditional random walk into multiple random walks specific to various topics. We thus build a Topical PageRank (TPR) on word graph to measure word importance with respect to different topics. After that, given the topic distribution of the document, we further calculate the ranking scores of words and extract the top ranked ones as keyphrases. Experimental results show that TPR outperforms state-of-the-art keyphrase extraction methods on two datasets under various evaluation metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>PLO leader Yasser</author>
</authors>
<title>Arafat(+), Palestine Liberation Organization leader, Khalil Wazir(+), Palestinian guerrillas(+), Abu Jihad, Israeli officials(+), particulary Palestinian circles, Arab government, State Department spokesman Charles Redman, Israeli squad(+)</title>
<marker>Yasser, </marker>
<rawString>PLO leader Yasser Arafat(+), Palestine Liberation Organization leader, Khalil Wazir(+), Palestinian guerrillas(+), Abu Jihad, Israeli officials(+), particulary Palestinian circles, Arab government, State Department spokesman Charles Redman, Israeli squad(+)</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="6100" citStr="Blei et al., 2003" startWordPosition="960" endWordPosition="963">ases, e.g., WordNet (Miller et al., 1990); (2) Use unsupervised machine learning techniques to obtain word topics from a large-scale document collection. Since the vocabulary in WordNet cannot cover many words in modern news and research articles, we employ the second approach to build topic interpreters for TPR. In machine learning, various methods have been proposed to infer latent topics of words and documents. These methods, known as latent topic models, derive latent topics from a large-scale document collection according to word occurrence information. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a representative of topic models. Compared to Latent Semantic Analysis (LSA) (Landauer et al., 1998) and probabilistic LSA (pLSA) (Hofmann, 1999), LDA has more feasibility for inference and can reduce the risk of over-fitting. In LDA, each word w of a document d is regarded to be generated by first sampling a topic z from d’s topic distribution θ(d), and then sampling a word from the distribution over words φ(z) that characterizes topic z. In LDA, θ(d) and φ(z) are drawn from conjugate Dirichlet priors α and β, separately. Therefore, θ and φ are integrated out and the probability of word w</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
<author>E M Voorhees</author>
</authors>
<title>Retrieval evaluation with incomplete information.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="16888" citStr="Buckley and Voorhees, 2004" startWordPosition="2782" endWordPosition="2785">the total number of correct keyphrases extracted by a method, cextract the total number of automatic extracted keyphrases, and cstandard the total number of human-labeled standard keyphrases. We note that the ranking order of extracted keyphrases also indicates the method performance. An extraction method will be better than another one if it can rank correct keyphrases higher. However, precision/recall/F-measure does not take the order of extracted keyphrases into account. To address the problem, we select the following two additional metrics. One metric is binary preference measure (Bpref) (Buckley and Voorhees, 2004). Bpref is desirable to evaluate the performance considering the order in which the extracted keyphrases are ranked. For a document, if there are R correct keyphrases within M extracted keyphrases by a method, in which r is a correct keyphrase and n is an incorrect keyphrase, Bpref is defined as follows, 1 � 1 − M Bpref = n ranked higher than r . (7) R rER The other metric is mean reciprocal rank (MRR) (Voorhees, 2000) which is used to evaluate how the first correct keyphrase for each document is ranked. For a document d, rankd is denoted as the rank of the first correct keyphrase with all ext</context>
</contexts>
<marker>Buckley, Voorhees, 2004</marker>
<rawString>C. Buckley and E.M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proceedings of SIGIR, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Huan Chang</author>
</authors>
<title>Learning to probabilistically identify authoritative documents.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>167--174</pages>
<contexts>
<context position="12258" citStr="Cohn and Chang, 2000" startWordPosition="2051" endWordPosition="2054">ect to the topic. Finally, the words will get different PageRank values in the two PageRanks. The setting of preference values pz(w) will have a great influence to TPR. In this paper we use three measures to set preference values for TPR: • pz(w) = pr(w|z), is the probability that word w occurs given topic z. This indicates how much that topic z focuses on word w. • pz(w) = pr(z|w), is the probability of topic z given word w. This indicates how much that word w focuses on topic z. • pz(w) = pr(w|z) x pr(z|w), is the product of hub and authority values. This measure is inspired by the work in (Cohn and Chang, 2000). Both PageRank and TPR are all iterative algorithms. We terminate the algorithms when the number of iterations reaches 100 or the difference of each vertex between two neighbor iterations is less than 0.001. 3.3 Extract Keyphrases Using Ranking Scores After obtaining word ranking scores using TPR, we begin to rank candidate keyphrases. As reported in (Hulth, 2003), most manually assigned keyphrases turn out to be noun phrases. We thus select noun phrases from a document as candidate keyphrases for ranking. The candidate keyphrases of a document is obtained as follows. The document is first to</context>
</contexts>
<marker>Cohn, Chang, 2000</marker>
<rawString>David Cohn and Huan Chang. 2000. Learning to probabilistically identify authoritative documents. In Proceedings ofICML, pages 167–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Grineva</author>
<author>M Grinev</author>
<author>D Lizorkin</author>
</authors>
<title>Extracting key terms from noisy and multi-theme documents.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>661--670</pages>
<marker>Grineva, Grinev, Lizorkin, 2009</marker>
<rawString>M. Grineva, M. Grinev, and D. Lizorkin. 2009. Extracting key terms from noisy and multi-theme documents. In Proceedings of WWW, pages 661–670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>517--526</pages>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In Proceedings of WWW, pages 517–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heinrich</author>
</authors>
<title>Parameter estimation for text analysis.</title>
<date>2005</date>
<note>Web: http://www. arbylon. net/publications/textest.</note>
<contexts>
<context position="23757" citStr="Heinrich, 2005" startWordPosition="4032" endWordPosition="4033">to compare with TPR. The TFIDF computes the ranking scores of words based on words’ tfidf values in the document, namely R(w) = tf,,, × log(idf,,,). While in PageRank (i.e., TextRank), the ranking scores of words are obtained using Eq.(2). The two baselines do not use topic information of either words or documents. The LDA computes the ranking score for each word using the topical similarity between the word and the document. Given the topics of the document d and a word w, We have used various methods to compute similarity including cosine similarity, predictive likelihood and KL-divergence (Heinrich, 2005), among which cosine similarity performs the best on both datasets. Therefore, we only show the results of the LDA baseline calculated using cosine similarity. In Tables 4 and 5 we show the comparing results of the four methods on both NEWS and RESEARCH. Since the average number of manual-labeled keyphrases on NEWS is larger than RESEARCH, we set M = 10 for NEWS and M = 5 for RESEARCH. The parameter settings on both NEWS and RESEARCH have been stated in Section 4.3. Method Pre. Rec. F. Bpref MRR TFIDF 0.239 0.295 0.264 0.179 0.576 PageRank 0.242 0.299 0.267 0.184 0.564 LDA 0.259 0.320 0.286 0.</context>
</contexts>
<marker>Heinrich, 2005</marker>
<rawString>G. Heinrich. 2005. Parameter estimation for text analysis. Web: http://www. arbylon. net/publications/textest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="6249" citStr="Hofmann, 1999" startWordPosition="985" endWordPosition="987"> Since the vocabulary in WordNet cannot cover many words in modern news and research articles, we employ the second approach to build topic interpreters for TPR. In machine learning, various methods have been proposed to infer latent topics of words and documents. These methods, known as latent topic models, derive latent topics from a large-scale document collection according to word occurrence information. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a representative of topic models. Compared to Latent Semantic Analysis (LSA) (Landauer et al., 1998) and probabilistic LSA (pLSA) (Hofmann, 1999), LDA has more feasibility for inference and can reduce the risk of over-fitting. In LDA, each word w of a document d is regarded to be generated by first sampling a topic z from d’s topic distribution θ(d), and then sampling a word from the distribution over words φ(z) that characterizes topic z. In LDA, θ(d) and φ(z) are drawn from conjugate Dirichlet priors α and β, separately. Therefore, θ and φ are integrated out and the probability of word w given document d and priors is represented as follows: K pr(w|d, α, β) = pr(w|z,β)pr(z|d, α), (1) z=1 where K is the number of topics. Using LDA, we</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of SIGIR, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="12625" citStr="Hulth, 2003" startWordPosition="2112" endWordPosition="2113"> pr(z|w), is the probability of topic z given word w. This indicates how much that word w focuses on topic z. • pz(w) = pr(w|z) x pr(z|w), is the product of hub and authority values. This measure is inspired by the work in (Cohn and Chang, 2000). Both PageRank and TPR are all iterative algorithms. We terminate the algorithms when the number of iterations reaches 100 or the difference of each vertex between two neighbor iterations is less than 0.001. 3.3 Extract Keyphrases Using Ranking Scores After obtaining word ranking scores using TPR, we begin to rank candidate keyphrases. As reported in (Hulth, 2003), most manually assigned keyphrases turn out to be noun phrases. We thus select noun phrases from a document as candidate keyphrases for ranking. The candidate keyphrases of a document is obtained as follows. The document is first tokenized. After that, we annotate the document with partof-speech (POS) tags 1. Third, we extract noun phrases with pattern (adjective)*(noun)+, which represents zero or more adjectives followed by one or more nouns. We regard these noun phrases as candidate keyphrases. After identifying candidate keyphrases, we rank them using the ranking scores obtained by TPR. In</context>
<context position="15112" citStr="Hulth, 2003" startWordPosition="2513" endWordPosition="2514"> phrases in descending order of their integrated ranking scores, we select the top M as the keyphrases of document d. 4 Experiments 4.1 Datasets To evaluate the performance of TPR for keyphrase extraction, we carry out experiments on two datasets. One dataset was built by Wan and Xiao 2 which was used in (Wan and Xiao, 2008b). This dataset contains 308 news articles in DUC2001 (Over et al., 2001) with 2,488 manually annotated keyphrases. There are at most 10 keyphrases for each document. In experiments we refer to this dataset as NEWS. The other dataset was built by Hulth 3 which was used in (Hulth, 2003). This dataset contains 2, 000 abstracts of research articles and 19, 254 manually annotated keyphrases. In experiments we refer to this dataset as RESEARCH. Since neither NEWS nor RESEARCH itself is large enough to learn efficient topics, we use the Wikipedia snapshot at March 2008 4 to build topic interpreters with LDA. After removing non-article pages and the articles shorter than 100 words, we collected 2,122, 618 articles. After tokenization, stop word removal and word stemming, we build the vocabulary by selecting 20, 000 words according to their document frequency. We learn LDA models b</context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of EMNLP, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>Journal of the ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<pages>632</pages>
<marker>Kleinberg, 1999</marker>
<rawString>J.M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="6204" citStr="Landauer et al., 1998" startWordPosition="977" endWordPosition="980">n word topics from a large-scale document collection. Since the vocabulary in WordNet cannot cover many words in modern news and research articles, we employ the second approach to build topic interpreters for TPR. In machine learning, various methods have been proposed to infer latent topics of words and documents. These methods, known as latent topic models, derive latent topics from a large-scale document collection according to word occurrence information. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a representative of topic models. Compared to Latent Semantic Analysis (LSA) (Landauer et al., 1998) and probabilistic LSA (pLSA) (Hofmann, 1999), LDA has more feasibility for inference and can reduce the risk of over-fitting. In LDA, each word w of a document d is regarded to be generated by first sampling a topic z from d’s topic distribution θ(d), and then sampling a word from the distribution over words φ(z) that characterizes topic z. In LDA, θ(d) and φ(z) are drawn from conjugate Dirichlet priors α and β, separately. Therefore, θ and φ are integrated out and the probability of word w given document d and priors is represented as follows: K pr(w|d, α, β) = pr(w|z,β)pr(z|d, α), (1) z=1 w</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Litvak</author>
<author>Mark Last</author>
</authors>
<title>Graph-based keyword extraction for single-document summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the workshop Multi-source Multilingual Information Extraction and Summarization,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="29893" citStr="Litvak and Last (2008)" startWordPosition="4982" endWordPosition="4985">ent, alleged document, Arab government, slaying Wazir, State Department spokesman Charles Redman, Khalil Wazir(+) TPR, Rank 3 Topic on “terrorism” terrorist attacks(+), PLO leader Yasser Arafat(+), Abu Jihad, United States(+), alleged document, U.S. government document, Palestine Liberation Organization leader, State Department spokesman Charles Redman, political assassination(+), full cooperation Table 6: Extracted keyphrases by TPR. Starting with TextRank (Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction. Litvak and Last (2008) applied HITS algorithm on the word graph of a document for keyphrase extraction. Although HITS itself worked the similar performance to PageRank, we plan to explore the integration of topics and HITS in future work. Wan (2008b; 2008a) used a small number of nearest neighbor documents to provide more knowledge for keyphrase extraction. Some methods used clustering techniques on word graphs for keyphrase extraction (Grineva et al., 2009; Liu et al., 2009). The clustering-based method performed well on short abstracts (with F-measure 0.382 on RESEARCH) but poorly on long articles (NEWS with F-me</context>
</contexts>
<marker>Litvak, Last, 2008</marker>
<rawString>Marina Litvak and Mark Last. 2008. Graph-based keyword extraction for single-document summarization. In Proceedings of the workshop Multi-source Multilingual Information Extraction and Summarization, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Peng Li</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Clustering to find exemplar terms for keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>257--266</pages>
<marker>Liu, Li, Zheng, Sun, 2009</marker>
<rawString>Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun. 2009. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of EMNLP, pages 257– 266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1565" citStr="Manning and Schutze, 2000" startWordPosition="232" endWordPosition="235">nd extract the top ranked ones as keyphrases. Experimental results show that TPR outperforms state-of-the-art keyphrase extraction methods on two datasets under various evaluation metrics. 1 Introduction Keyphrases are defined as a set of terms in a document that give a brief summary of its content for readers. Automatic keyphrase extraction is widely used in information retrieval and digital library (Turney, 2000; Nguyen and Kan, 2007). Keyphrase extraction is also an essential step in various tasks of natural language processing such as document categorization, clustering and summarization (Manning and Schutze, 2000). There are two principled approaches to extracting keyphrases: supervised and unsupervised. The supervised approach (Turney, 1999) regards keyphrase extraction as a classification task, in which a model is trained to determine whether a candidate phrase is a keyphrase. Supervised methods require a document set with human-assigned keyphrases as training set. In Web era, articles increase exponentially and change dynamically, which demands keyphrase extraction to be efficient and adaptable. However, since human labeling is time consuming, it is impractical to label training set from time to tim</context>
</contexts>
<marker>Manning, Schutze, 2000</marker>
<rawString>C.D. Manning and H. Schutze. 2000. Foundations of statistical natural language processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="2331" citStr="Mihalcea and Tarau, 2004" startWordPosition="346" endWordPosition="350">eyphrase extraction as a classification task, in which a model is trained to determine whether a candidate phrase is a keyphrase. Supervised methods require a document set with human-assigned keyphrases as training set. In Web era, articles increase exponentially and change dynamically, which demands keyphrase extraction to be efficient and adaptable. However, since human labeling is time consuming, it is impractical to label training set from time to time. We thus focus on the unsupervised approach in this study. In the unsupervised approach, graph-based ranking methods are state-of-the-art (Mihalcea and Tarau, 2004). These methods first build a word graph according to word co-occurrences within the document, and then use random walk techniques (e.g., PageRank) to measure word importance. After that, top ranked words are selected as keyphrases. Existing graph-based methods maintain a single importance score for each word. However, a document (e.g., news article or research article) is usually composed of multiple semantic topics. Taking this paper for example, it refers to two major topics, “keyphrase extraction” and “random walk”. As words are used to express various meanings corresponding to different s</context>
<context position="8463" citStr="Mihalcea and Tarau, 2004" startWordPosition="1363" endWordPosition="1366"> each topic separately. 4. Given the topics of document d, integrate the topic-specific rankings of candidate keyphrases into a final ranking, and the top ranked ones are selected as keyphrases. 3.1 Constructing Word Graph We construct a word graph according to word cooccurrences within the given document, which expresses the cohesion relationship between words in the context of document. The document is regarded as a word sequence, and the link weights between words is simply set to the co-occurrence count within a sliding window with maximum W words in the word sequence. It was reported in (Mihalcea and Tarau, 2004) the graph direction does not influence the performance of keyphrase extraction very much. In this paper we simply construct word graphs with directions. The link directions are determined as follows. When sliding a W-width window, at each position, we add links from the first word pointing to other words within the window. Since keyphrases are usually noun phrases, we only add adjectives and nouns in word graph. 3.2 Topical PageRank Before introducing TPR, we first give some formal notations. We denote G = (V, E) as the graph of a document, with vertex set V = {w1, w2,··· , wN} and link set (</context>
<context position="13425" citStr="Mihalcea and Tarau, 2004" startWordPosition="2236" endWordPosition="2240">document is obtained as follows. The document is first tokenized. After that, we annotate the document with partof-speech (POS) tags 1. Third, we extract noun phrases with pattern (adjective)*(noun)+, which represents zero or more adjectives followed by one or more nouns. We regard these noun phrases as candidate keyphrases. After identifying candidate keyphrases, we rank them using the ranking scores obtained by TPR. In PageRank for keyphrase extraction, the ranking score of a candidate keyphrase p is computed by summing up the ranking scores of all words within the phrase: R(p)=EwiEp R(wi) (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b). Then candidate keyphrases are ranked in descending order of ranking scores. The top M candidates are selected as keyphrases. In TPR for keyphrase extraction, we first compute the ranking scores of candidate keyphrases separately for each topic. That is for each topic z we compute Rz(p) = � Rz(wi). (4) wiEp By considering the topic distribution of document, We further integrate topic-specific rankings of candidate keyphrases into a final ranking and extract top-ranked ones as the keyphrases of the document. Denote the topic distribution of the docume</context>
<context position="29759" citStr="Mihalcea and Tarau, 2004" startWordPosition="4964" endWordPosition="4967">ael” PLO leader Yasser Arafat(+), United States(+), Palestine Liberation Organization leader, Israeli officials(+), U.S. government document, alleged document, Arab government, slaying Wazir, State Department spokesman Charles Redman, Khalil Wazir(+) TPR, Rank 3 Topic on “terrorism” terrorist attacks(+), PLO leader Yasser Arafat(+), Abu Jihad, United States(+), alleged document, U.S. government document, Palestine Liberation Organization leader, State Department spokesman Charles Redman, political assassination(+), full cooperation Table 6: Extracted keyphrases by TPR. Starting with TextRank (Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction. Litvak and Last (2008) applied HITS algorithm on the word graph of a document for keyphrase extraction. Although HITS itself worked the similar performance to PageRank, we plan to explore the integration of topics and HITS in future work. Wan (2008b; 2008a) used a small number of nearest neighbor documents to provide more knowledge for keyphrase extraction. Some methods used clustering techniques on word graphs for keyphrase extraction (Grineva et al., 2009; Liu et al., 2009). The cl</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Proceedings of EMNLP, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal ofLexicography,</journal>
<pages>3--235</pages>
<contexts>
<context position="5523" citStr="Miller et al., 1990" startWordPosition="868" endWordPosition="871"> robustness. Moreover, TPR is unsupervised and languageindependent, which is applicable in Web era with enormous information. TPR for keyphrase extraction is a two-stage process: 1. Build a topic interpreter to acquire the topics of words and documents. 2. Perform TPR to extract keyphrases for documents. We will introduce the two stages in Section 2 and Section 3. 2 Building Topic Interpreters To run TPR on a word graph, we have to acquire topic distributions of words. There are roughly two approaches that can provide topics of words: (1) Use manually annotated knowledge bases, e.g., WordNet (Miller et al., 1990); (2) Use unsupervised machine learning techniques to obtain word topics from a large-scale document collection. Since the vocabulary in WordNet cannot cover many words in modern news and research articles, we employ the second approach to build topic interpreters for TPR. In machine learning, various methods have been proposed to infer latent topics of words and documents. These methods, known as latent topic models, derive latent topics from a large-scale document collection according to word occurrence information. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a representative of</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. WordNet: An on-line lexical database. International Journal ofLexicography, 3:235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Nguyen</author>
<author>Min-Yen Kan</author>
</authors>
<title>Keyphrase extraction in scientific publications.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Asian Digital Libraries,</booktitle>
<pages>317--326</pages>
<contexts>
<context position="1379" citStr="Nguyen and Kan, 2007" startWordPosition="205" endWordPosition="208"> word graph to measure word importance with respect to different topics. After that, given the topic distribution of the document, we further calculate the ranking scores of words and extract the top ranked ones as keyphrases. Experimental results show that TPR outperforms state-of-the-art keyphrase extraction methods on two datasets under various evaluation metrics. 1 Introduction Keyphrases are defined as a set of terms in a document that give a brief summary of its content for readers. Automatic keyphrase extraction is widely used in information retrieval and digital library (Turney, 2000; Nguyen and Kan, 2007). Keyphrase extraction is also an essential step in various tasks of natural language processing such as document categorization, clustering and summarization (Manning and Schutze, 2000). There are two principled approaches to extracting keyphrases: supervised and unsupervised. The supervised approach (Turney, 1999) regards keyphrase extraction as a classification task, in which a model is trained to determine whether a candidate phrase is a keyphrase. Supervised methods require a document set with human-assigned keyphrases as training set. In Web era, articles increase exponentially and chang</context>
</contexts>
<marker>Nguyen, Kan, 2007</marker>
<rawString>Thuy Nguyen and Min-Yen Kan. 2007. Keyphrase extraction in scientific publications. In Proceedings of the 10th International Conference on Asian Digital Libraries, pages 317–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lan Nie</author>
<author>Brian D Davison</author>
<author>Xiaoguang Qi</author>
</authors>
<title>Topical link analysis for web search.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>91--98</pages>
<marker>Nie, Davison, Qi, 2006</marker>
<rawString>Lan Nie, Brian D. Davison, and Xiaoguang Qi. 2006. Topical link analysis for web search. In Proceedings of SIGIR, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Over</author>
<author>W Liggett</author>
<author>H Gilbert</author>
<author>A Sakharov</author>
<author>M Thatcher</author>
</authors>
<title>Introduction to duc-2001: An intrinsic evaluation of generic news text summarization systems.</title>
<date>2001</date>
<booktitle>In Proceedings ofDUC2001.</booktitle>
<contexts>
<context position="14899" citStr="Over et al., 2001" startWordPosition="2474" endWordPosition="2477">llows: R(p) = K Rz(p) x pr(z d). (5) Porter Stemmer 5 for comparison. In experiments z=1 we select three evaluation metrics. The first metric is precision/recall/F-measure represented as follows, After ranking candidate phrases in descending order of their integrated ranking scores, we select the top M as the keyphrases of document d. 4 Experiments 4.1 Datasets To evaluate the performance of TPR for keyphrase extraction, we carry out experiments on two datasets. One dataset was built by Wan and Xiao 2 which was used in (Wan and Xiao, 2008b). This dataset contains 308 news articles in DUC2001 (Over et al., 2001) with 2,488 manually annotated keyphrases. There are at most 10 keyphrases for each document. In experiments we refer to this dataset as NEWS. The other dataset was built by Hulth 3 which was used in (Hulth, 2003). This dataset contains 2, 000 abstracts of research articles and 19, 254 manually annotated keyphrases. In experiments we refer to this dataset as RESEARCH. Since neither NEWS nor RESEARCH itself is large enough to learn efficient topics, we use the Wikipedia snapshot at March 2008 4 to build topic interpreters with LDA. After removing non-article pages and the articles shorter than </context>
</contexts>
<marker>Over, Liggett, Gilbert, Sakharov, Thatcher, 2001</marker>
<rawString>P. Over, W. Liggett, H. Gilbert, A. Sakharov, and M. Thatcher. 2001. Introduction to duc-2001: An intrinsic evaluation of generic news text summarization systems. In Proceedings ofDUC2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Page</author>
<author>S Brin</author>
<author>R Motwani</author>
<author>T Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web. Technical report,</title>
<date>1998</date>
<booktitle>Stanford Digital Library Technologies Project,</booktitle>
<contexts>
<context position="9382" citStr="Page et al., 1998" startWordPosition="1530" endWordPosition="1533">words within the window. Since keyphrases are usually noun phrases, we only add adjectives and nouns in word graph. 3.2 Topical PageRank Before introducing TPR, we first give some formal notations. We denote G = (V, E) as the graph of a document, with vertex set V = {w1, w2,··· , wN} and link set (wi, wj) E E if there is a link from wi to wj. In a word graph, each vertex represents a word, and each link indicates the relatedness between words. We denote the weight of link (wi, wj) as e(wi, wj), and the out-degree of vertex wi as O(wi)=L�j:wi→wj e(wi,wj). Topical PageRank is based on PageRank (Page et al., 1998). PageRank is a well known ranking algorithm that uses link information to assign global importance scores to web pages. The basic idea of PageRank is that a vertex is important if there are other important vertices pointing to it. This can be regarded as voting or recommendation among vertices. In PageRank, the score R(wi) of word wi is defined as e(wj, wi) O(wj) R(wj) + (1 − λ) 1 |V |, (2) where λ is a damping factor range from 0 to 1, and |V |is the number of vertices. The damping factor indicates that each vertex has a probability of (1 − λ) to perform random jump to another vertex within </context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>L. Page, S. Brin, R. Motwani, and T. Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford Digital Library Technologies Project, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Learning to extract keyphrases from text.</title>
<date>1999</date>
<tech>Technical Report ERB-1057.</tech>
<institution>National Research Council Canada, Institute for Information Technology,</institution>
<contexts>
<context position="1696" citStr="Turney, 1999" startWordPosition="251" endWordPosition="252">o datasets under various evaluation metrics. 1 Introduction Keyphrases are defined as a set of terms in a document that give a brief summary of its content for readers. Automatic keyphrase extraction is widely used in information retrieval and digital library (Turney, 2000; Nguyen and Kan, 2007). Keyphrase extraction is also an essential step in various tasks of natural language processing such as document categorization, clustering and summarization (Manning and Schutze, 2000). There are two principled approaches to extracting keyphrases: supervised and unsupervised. The supervised approach (Turney, 1999) regards keyphrase extraction as a classification task, in which a model is trained to determine whether a candidate phrase is a keyphrase. Supervised methods require a document set with human-assigned keyphrases as training set. In Web era, articles increase exponentially and change dynamically, which demands keyphrase extraction to be efficient and adaptable. However, since human labeling is time consuming, it is impractical to label training set from time to time. We thus focus on the unsupervised approach in this study. In the unsupervised approach, graph-based ranking methods are state-of</context>
<context position="27993" citStr="Turney, 1999" startWordPosition="4739" endWordPosition="4740">d thus highly ranked the phrases with “PLO” which appeared about 16 times in this article, and failed to extract the keyphrases on topic “Israel”. LDA only measured the importance of words using document topics without considering the frequency information of words and thus missed keyphrases with high-frequency words. For example, LDA failed to extract keyphrase “political assassination”, in which the word “assassination” occurred 8 times in this article. 5 Related Work In this paper we proposed TPR for keyphrase extraction. A pioneering achievement in keyphrase extraction was carried out in (Turney, 1999) which regarded keyphrase extraction as a classification task. Generally, the supervised methods need manually annotated training set which is time-consuming and in this paper we focus on unsupervised method. 373 TFIDF (+5) PLO leader Yasser Arafat(+), PLO attacks, PLO offices, PLO officials(+), PLO leaders, Abu Jihad, terrorist attacks(+), Khalil Wazir(+), slaying wazir, political assassination(+) PageRank (+3) PLO leader Yasser Arafat(+), PLO officials(+), PLO attacks, United States(+), PLO offices, PLO leaders, State Department spokesman Charles Redman, U.S. government document, alleged doc</context>
</contexts>
<marker>Turney, 1999</marker>
<rawString>Peter D. Turney. 1999. Learning to extract keyphrases from text. National Research Council Canada, Institute for Information Technology, Technical Report ERB-1057.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction.</title>
<date>2000</date>
<journal>Information Retrieval,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="1356" citStr="Turney, 2000" startWordPosition="202" endWordPosition="204">eRank (TPR) on word graph to measure word importance with respect to different topics. After that, given the topic distribution of the document, we further calculate the ranking scores of words and extract the top ranked ones as keyphrases. Experimental results show that TPR outperforms state-of-the-art keyphrase extraction methods on two datasets under various evaluation metrics. 1 Introduction Keyphrases are defined as a set of terms in a document that give a brief summary of its content for readers. Automatic keyphrase extraction is widely used in information retrieval and digital library (Turney, 2000; Nguyen and Kan, 2007). Keyphrase extraction is also an essential step in various tasks of natural language processing such as document categorization, clustering and summarization (Manning and Schutze, 2000). There are two principled approaches to extracting keyphrases: supervised and unsupervised. The supervised approach (Turney, 1999) regards keyphrase extraction as a classification task, in which a model is trained to determine whether a candidate phrase is a keyphrase. Supervised methods require a document set with human-assigned keyphrases as training set. In Web era, articles increase </context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>Peter D. Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2(4):303–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>The trec-8 question answering track report.</title>
<date>2000</date>
<booktitle>In Proceedings of TREC,</booktitle>
<pages>77--82</pages>
<contexts>
<context position="17310" citStr="Voorhees, 2000" startWordPosition="2861" endWordPosition="2862"> order of extracted keyphrases into account. To address the problem, we select the following two additional metrics. One metric is binary preference measure (Bpref) (Buckley and Voorhees, 2004). Bpref is desirable to evaluate the performance considering the order in which the extracted keyphrases are ranked. For a document, if there are R correct keyphrases within M extracted keyphrases by a method, in which r is a correct keyphrase and n is an incorrect keyphrase, Bpref is defined as follows, 1 � 1 − M Bpref = n ranked higher than r . (7) R rER The other metric is mean reciprocal rank (MRR) (Voorhees, 2000) which is used to evaluate how the first correct keyphrase for each document is ranked. For a document d, rankd is denoted as the rank of the first correct keyphrase with all extracted keyphrases, MRR is defined as follows, MRR = D d rankd, (8) 1 1 where D is the document set for keyphrase extraction. Note that although the evaluation scores of most keyphrase extractors are still lower compared to 5http://tartarus.org/˜martin/ PorterStemmer. ccorrect ccorrect p= , r = cextract cstandard 370 other NLP-tasks, it does not indicate the performance is poor because even different annotators may assi</context>
</contexts>
<marker>Voorhees, 2000</marker>
<rawString>E.M. Voorhees. 2000. The trec-8 question answering track report. In Proceedings of TREC, pages 77–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Collabrank: Towards a collaborative approach to single-document keyphrase extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>969--976</pages>
<contexts>
<context position="13445" citStr="Wan and Xiao, 2008" startWordPosition="2241" endWordPosition="2244">llows. The document is first tokenized. After that, we annotate the document with partof-speech (POS) tags 1. Third, we extract noun phrases with pattern (adjective)*(noun)+, which represents zero or more adjectives followed by one or more nouns. We regard these noun phrases as candidate keyphrases. After identifying candidate keyphrases, we rank them using the ranking scores obtained by TPR. In PageRank for keyphrase extraction, the ranking score of a candidate keyphrase p is computed by summing up the ranking scores of all words within the phrase: R(p)=EwiEp R(wi) (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b). Then candidate keyphrases are ranked in descending order of ranking scores. The top M candidates are selected as keyphrases. In TPR for keyphrase extraction, we first compute the ranking scores of candidate keyphrases separately for each topic. That is for each topic z we compute Rz(p) = � Rz(wi). (4) wiEp By considering the topic distribution of document, We further integrate topic-specific rankings of candidate keyphrases into a final ranking and extract top-ranked ones as the keyphrases of the document. Denote the topic distribution of the document d as pr(z|d) for </context>
<context position="14825" citStr="Wan and Xiao, 2008" startWordPosition="2462" endWordPosition="2465">are/tagger.shtml with English tagging model left3words-distsim-wsj. 369 follows: R(p) = K Rz(p) x pr(z d). (5) Porter Stemmer 5 for comparison. In experiments z=1 we select three evaluation metrics. The first metric is precision/recall/F-measure represented as follows, After ranking candidate phrases in descending order of their integrated ranking scores, we select the top M as the keyphrases of document d. 4 Experiments 4.1 Datasets To evaluate the performance of TPR for keyphrase extraction, we carry out experiments on two datasets. One dataset was built by Wan and Xiao 2 which was used in (Wan and Xiao, 2008b). This dataset contains 308 news articles in DUC2001 (Over et al., 2001) with 2,488 manually annotated keyphrases. There are at most 10 keyphrases for each document. In experiments we refer to this dataset as NEWS. The other dataset was built by Hulth 3 which was used in (Hulth, 2003). This dataset contains 2, 000 abstracts of research articles and 19, 254 manually annotated keyphrases. In experiments we refer to this dataset as RESEARCH. Since neither NEWS nor RESEARCH itself is large enough to learn efficient topics, we use the Wikipedia snapshot at March 2008 4 to build topic interpreters</context>
<context position="18932" citStr="Wan and Xiao, 2008" startWordPosition="3135" endWordPosition="3138">of these parameters to TPR for keyphrase extraction. Except the parameter under investigation, we set parameters to the following values: W =10, K =1, 000, A=0.3 and pz(w) = pr(zIw), which are the settings when TPR achieves the best (or near best) performance on both NEWS and RESEARCH. In the following tables, we use “Pre.”, “Rec.” and “F.” as the abbreviations of precision, recall and F-measure. 4.3.1 Window Size W In experiments on NEWS, we find that the performance of TPR is stable when W ranges from 5 to 20 as shown in Table 1. This observation is consistent with the findings reported in (Wan and Xiao, 2008b). Size Pre. Rec. F. Bpref MRR 5 0.280 0.345 0.309 0.213 0.636 10 0.282 0.348 0.312 0.214 0.638 15 0.282 0.347 0.311 0.214 0.646 20 0.284 0.350 0.313 0.215 0.644 Table 1: Influence of window size W when the number of keyphrases M =10 on NEWS. Similarly, when W ranges from 2 to 10, the performance on RESEARCH does not change much. However, the performance on NEWS will become poor when W = 20. This is because the abstracts in RESEARCH (there are 121 words per abstract on average) are much shorter than the news articles in NEWS (there are 704 words per article on average). If the window size W i</context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. 2008a. Collabrank: Towards a collaborative approach to single-document keyphrase extraction. In Proceedings of COLING, pages 969–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Single document keyphrase extraction using neighborhood knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings ofAAAI,</booktitle>
<pages>855--860</pages>
<contexts>
<context position="13445" citStr="Wan and Xiao, 2008" startWordPosition="2241" endWordPosition="2244">llows. The document is first tokenized. After that, we annotate the document with partof-speech (POS) tags 1. Third, we extract noun phrases with pattern (adjective)*(noun)+, which represents zero or more adjectives followed by one or more nouns. We regard these noun phrases as candidate keyphrases. After identifying candidate keyphrases, we rank them using the ranking scores obtained by TPR. In PageRank for keyphrase extraction, the ranking score of a candidate keyphrase p is computed by summing up the ranking scores of all words within the phrase: R(p)=EwiEp R(wi) (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b). Then candidate keyphrases are ranked in descending order of ranking scores. The top M candidates are selected as keyphrases. In TPR for keyphrase extraction, we first compute the ranking scores of candidate keyphrases separately for each topic. That is for each topic z we compute Rz(p) = � Rz(wi). (4) wiEp By considering the topic distribution of document, We further integrate topic-specific rankings of candidate keyphrases into a final ranking and extract top-ranked ones as the keyphrases of the document. Denote the topic distribution of the document d as pr(z|d) for </context>
<context position="14825" citStr="Wan and Xiao, 2008" startWordPosition="2462" endWordPosition="2465">are/tagger.shtml with English tagging model left3words-distsim-wsj. 369 follows: R(p) = K Rz(p) x pr(z d). (5) Porter Stemmer 5 for comparison. In experiments z=1 we select three evaluation metrics. The first metric is precision/recall/F-measure represented as follows, After ranking candidate phrases in descending order of their integrated ranking scores, we select the top M as the keyphrases of document d. 4 Experiments 4.1 Datasets To evaluate the performance of TPR for keyphrase extraction, we carry out experiments on two datasets. One dataset was built by Wan and Xiao 2 which was used in (Wan and Xiao, 2008b). This dataset contains 308 news articles in DUC2001 (Over et al., 2001) with 2,488 manually annotated keyphrases. There are at most 10 keyphrases for each document. In experiments we refer to this dataset as NEWS. The other dataset was built by Hulth 3 which was used in (Hulth, 2003). This dataset contains 2, 000 abstracts of research articles and 19, 254 manually annotated keyphrases. In experiments we refer to this dataset as RESEARCH. Since neither NEWS nor RESEARCH itself is large enough to learn efficient topics, we use the Wikipedia snapshot at March 2008 4 to build topic interpreters</context>
<context position="18932" citStr="Wan and Xiao, 2008" startWordPosition="3135" endWordPosition="3138">of these parameters to TPR for keyphrase extraction. Except the parameter under investigation, we set parameters to the following values: W =10, K =1, 000, A=0.3 and pz(w) = pr(zIw), which are the settings when TPR achieves the best (or near best) performance on both NEWS and RESEARCH. In the following tables, we use “Pre.”, “Rec.” and “F.” as the abbreviations of precision, recall and F-measure. 4.3.1 Window Size W In experiments on NEWS, we find that the performance of TPR is stable when W ranges from 5 to 20 as shown in Table 1. This observation is consistent with the findings reported in (Wan and Xiao, 2008b). Size Pre. Rec. F. Bpref MRR 5 0.280 0.345 0.309 0.213 0.636 10 0.282 0.348 0.312 0.214 0.638 15 0.282 0.347 0.311 0.214 0.646 20 0.284 0.350 0.313 0.215 0.644 Table 1: Influence of window size W when the number of keyphrases M =10 on NEWS. Similarly, when W ranges from 2 to 10, the performance on RESEARCH does not change much. However, the performance on NEWS will become poor when W = 20. This is because the abstracts in RESEARCH (there are 121 words per abstract on average) are much shorter than the news articles in NEWS (there are 704 words per article on average). If the window size W i</context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. 2008b. Single document keyphrase extraction using neighborhood knowledge. In Proceedings ofAAAI, pages 855–860.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>