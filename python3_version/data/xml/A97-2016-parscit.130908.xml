<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.100622">
<title confidence="0.982609">
Software for Annotating Argument Structure
</title>
<author confidence="0.973473">
Wojciech Skut, Brigitte Krenn, Thorsten Brants, Hans Uszkoreit
</author>
<affiliation confidence="0.87059">
Universitat des Saarlandes
</affiliation>
<address confidence="0.585129">
66041 Saarbriicken, Germany
</address>
<email confidence="0.713759">
Iskut,krenn,brants,uszkoreitlOcoli.uni-sb.de
</email>
<sectionHeader confidence="0.951686" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999122">
We present a tool developed for annota-
ting corpora with argument structure re-
presentations. The presentation focuses on
the architecture of the annotation scheme
and a number of techniques for increa-
sing the efficiency and accuracy of annota-
tion. Among others, we show how the as-
signment of grammatical functions can be
automatised using standard part-of-speech
tagging methods.
</bodyText>
<sectionHeader confidence="0.832648" genericHeader="method">
1 The Annotation Scheme
</sectionHeader>
<bodyText confidence="0.99609325">
Several features of the tool have been introduced to
suite the requirements imposed by the architecture
of the annotation scheme (cf. (Skut et al., 1997)),
which can itself be characterised as follows:
</bodyText>
<listItem confidence="0.994594166666667">
• Direct representation of the underlying argu-
ment structure in terms of unordered trees;
• Rudimentary, flat representations; uniform
treatment of local and non-local dependencies;
• Extensive encoding of linguistic information in
grammatical function labels.
</listItem>
<bodyText confidence="0.999865777777778">
Thus the format of the annotations is somewhat
different from treebanks relying on a context-free
backbone augmented with trace-filler annotations of
non-local dependencies. (cf. (Marcus et al., 1994),
(Sampson, 1995), (Black et al., 1996)) Nevertheless,
such treebanks can also be developed using our tool.
To back this claim, the representation of structures
from the SUZANNE corpus (cf. (Sampson, 1995))
will be shown in the presentation.
</bodyText>
<sectionHeader confidence="0.935926" genericHeader="method">
2 User Interface
</sectionHeader>
<bodyText confidence="0.999911">
A screen dump of the tool is shown in fig. 1. The lar-
gest part of the window contains the graphical repre-
sentation of the structure being annotated. The no-
des and edges are assigned category and grammati-
cal function labels, respectively. The words are num-
bered and labelled with part-of-speech tags. Any
change into the structure of the sentence being an-
notated is immediately displayed.
Extra effort has been put into the development
of a convenient keyboard interface. Menus are sup-
ported as a useful way of getting help on commands
and labels. Automatic completion and error check
on user input are supported.
Three tagsets have to be defined by the user: part-
of-speech tags, phrasal categories and grammatical
functions. They are stored together with the corpus,
which permits easy modification when needed.
The user interface is implemented in Tcl/Tk Ver-
sion 4.1. The corpus is stored in an SQL database.
</bodyText>
<sectionHeader confidence="0.990965" genericHeader="method">
3 Automation
</sectionHeader>
<bodyText confidence="0.989484607142857">
To increase the efficiency of annotation and avoid
certain types of errors made by the human annota-
tor, manual and automatic annotation are combined
in an interactive way. The automatic component of
the tool employs a stochastic tagging model indu-
ced from previously annotated sentences. Thus the
degree of automation increases with the amount of
data available.
At the current stage of automation, the annotator
determines the substructures to be grouped into a
new phrase and assigns it a syntactic category. The
assignment of grammatical functions is performed
automatically. To do this, we adapted a standard
part-of-speech tagging algorithm (the best sequence
of grammatical functions is to be determined for a
sequence of syntactic categories, cf. (Skut, et al.,
1997))
The annotator supervises the automatic assi-
gnment of function tags. In order to keep him from
missing tagging errors, the grammatical function
tagger is equipped with a function measuring the
reliability of its output. On the basis of the diffe-
rence between the best and second-best assignment,
the prediction is classified as belonging to one of the
following certainty intervals:
Reliable: the most probable tag is assigned,
Less reliable: the tagger suggests a function tag;
the annotator is asked to confirm the choice,
</bodyText>
<page confidence="0.995684">
27
</page>
<table confidence="0.99903128">
—General: Sentence•
Corpus:
Editor:
• Parser
El No.: 4 / 1269 Last edited: Thorsten, 07/02/97, 17:39:29
El Comment:
Origin:
RefCorpus Testkopie
Thorsten
Ok Reload Exit refcorp.tt
rZ
511
SO9El U
(l)
11171 507 rim
EU
4) 504 Go
4) 505
ra
4)
Es PPER spelt eben keine Role EI 501E1 Mus aligist Mil d IN3
0 ADV 2 500:: , ob die NN gef&apos; s - nur etwas Neues &amp;quot; mu&amp;quot;s
VVFI■ PIAT 3 4 5 6 8 VAFIN ■o I 12 13 4 15 16
NN $, KOUS ART ADJD $? AL/ HAT ${ NN $( vmFir
Ig 1 IA Kt
</table>
<tableCaption confidence="0.36121">
Switching to sentence no. 4... Done.
</tableCaption>
<figureCaption confidence="0.998459">
Figure 1: Screen dump of the annotation tool
</figureCaption>
<figure confidence="0.9967798">
Prey
Next
-10
-100
+10
+100
Parentlabel:
.—Dependency:
Selection:
Command:
Node no.:
Parentlabel.
Execute
Prey
Next
End
Go to:
Filter
Matches: 0
Mask„,
</figure>
<bodyText confidence="0.999094947368421">
Unreliable: the annotator has to determine the
function himself.
The annotator always has the option of altering
already assigned tags.
The tagger rates 90% of all assignments as relia-
ble. Accuracy for these cases is 97%. Most errors
are due to wrong identification of the subject and
different kinds of objects in S&apos;s and VP&apos;s. Accuracy
of the unreliable 10% of assignments is 75%, i.e., the
annotator has to alter the choice in 1 of 4 cases when
asked for confirmation. Overall accuracy of the tag-
ger is 95%.
In several cases, the tagger has been able to ab-
stract from annotation errors in training material,
which has proved very helpful in detecting inconsi-
stencies and wrong structures.
This first automation step has considerably in-
creased the efficiency of annotation. The average
annotation time per sentence improved by 25%.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999503125">
Ezra Black et al. 1996. Beyond Skeleton Par-
sing: Producing a Comprehensive Large-Scale
General-English Treebank With Full Grammati-
cal Analysis. In The 16th International Confe-
rence on Computational Linguistics, pages 107 —
113, Copenhagen, Denmark.
Mitchell Marcus et al. 1994. The Penn Treebank:
Annotating Predicate Argument Structure. In
Proceedings of the Human Language Technology
Workshop, San Francisco. Morgan Kaufmann.
Geoffrey Sampson. 1995. English for the Computer.
The SUSANNE Corpus and Analytic Scheme.
Wojciech Skut et al. 1997. An Annotation Scheme
For Free Word Order Languages. In The 7th Con-
ference on Applied Natural Language Processing,
Washington, DC.
</reference>
<page confidence="0.999068">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.996808">Software for Annotating Argument Structure</title>
<author confidence="0.975865">Wojciech Skut</author>
<author confidence="0.975865">Brigitte Krenn</author>
<author confidence="0.975865">Thorsten Brants</author>
<author confidence="0.975865">Hans Uszkoreit</author>
<affiliation confidence="0.995328">Universitat des Saarlandes</affiliation>
<address confidence="0.997042">66041 Saarbriicken, Germany</address>
<abstract confidence="0.986692875">We present a tool developed for annotating corpora with argument structure representations. The presentation focuses on the architecture of the annotation scheme and a number of techniques for increasing the efficiency and accuracy of annotation. Among others, we show how the assignment of grammatical functions can be automatised using standard part-of-speech tagging methods. 1 The Annotation Scheme Several features of the tool have been introduced to suite the requirements imposed by the architecture of the annotation scheme (cf. (Skut et al., 1997)), which can itself be characterised as follows: • Direct representation of the underlying argument structure in terms of unordered trees; • Rudimentary, flat representations; uniform treatment of local and non-local dependencies; • Extensive encoding of linguistic information in grammatical function labels. Thus the format of the annotations is somewhat different from treebanks relying on a context-free backbone augmented with trace-filler annotations of non-local dependencies. (cf. (Marcus et al., 1994), (Sampson, 1995), (Black et al., 1996)) Nevertheless, such treebanks can also be developed using our tool. To back this claim, the representation of structures from the SUZANNE corpus (cf. (Sampson, 1995)) will be shown in the presentation. 2 User Interface A screen dump of the tool is shown in fig. 1. The largest part of the window contains the graphical representation of the structure being annotated. The nodes and edges are assigned category and grammatical function labels, respectively. The words are numbered and labelled with part-of-speech tags. Any change into the structure of the sentence being annotated is immediately displayed. Extra effort has been put into the development of a convenient keyboard interface. Menus are supported as a useful way of getting help on commands and labels. Automatic completion and error check on user input are supported. Three tagsets have to be defined by the user: partof-speech tags, phrasal categories and grammatical functions. They are stored together with the corpus, which permits easy modification when needed. The user interface is implemented in Tcl/Tk Version 4.1. The corpus is stored in an SQL database. 3 Automation To increase the efficiency of annotation and avoid certain types of errors made by the human annotator, manual and automatic annotation are combined in an interactive way. The automatic component of a stochastic tagging model induced from previously annotated sentences. Thus the degree of automation increases with the amount of data available. At the current stage of automation, the annotator determines the substructures to be grouped into a new phrase and assigns it a syntactic category. The assignment of grammatical functions is performed automatically. To do this, we adapted a standard part-of-speech tagging algorithm (the best sequence of grammatical functions is to be determined for a sequence of syntactic categories, cf. (Skut, et al., 1997)) The annotator supervises the automatic assignment of function tags. In order to keep him from missing tagging errors, the grammatical function tagger is equipped with a function measuring the reliability of its output. On the basis of the difference between the best and second-best assignment, the prediction is classified as belonging to one of the following certainty intervals: most probable tag is assigned, reliable: tagger suggests a function tag; the annotator is asked to confirm the choice,</abstract>
<note confidence="0.24473525">27 —General: Corpus: Editor: Sentence• • Parser El El No.: 4 / 1269 edited: 07/02/97, 17:39:29</note>
<title confidence="0.9284625">Comment: Origin: RefCorpus Testkopie</title>
<author confidence="0.796107">Thorsten Ok Reload Exit refcorp tt</author>
<abstract confidence="0.668102333333333">rZ 511 U 11171 507 rim EU 4) 504 Go 4) 505 ra 4</abstract>
<note confidence="0.83953532">Es PPER spelt eben keine Role EI NN Mus s Mil 0 ADV 2 3 , ob die gef&apos; VAFIN nur etwas Neues VVFI■ PIAT NN 4 5 6 8 o I 134 $, KOUS ART ADJD AL/ HAT ${ NN $( 1 IA Switching to sentence no. 4... Done. Figure 1: Screen dump of the annotation tool Prey Next -10 -100 +10 +100 Parentlabel: .—Dependency: Selection: Command: Node no.: Parentlabel. Execute Prey Next End Filter Matches: 0</note>
<abstract confidence="0.9650645">Mask„, Unreliable: the annotator has to determine the function himself. The annotator always has the option of altering already assigned tags. The tagger rates 90% of all assignments as reliable. Accuracy for these cases is 97%. Most errors are due to wrong identification of the subject and different kinds of objects in S&apos;s and VP&apos;s. Accuracy of the unreliable 10% of assignments is 75%, i.e., the annotator has to alter the choice in 1 of 4 cases when asked for confirmation. Overall accuracy of the tagger is 95%. In several cases, the tagger has been able to abstract from annotation errors in training material, which has proved very helpful in detecting inconsistencies and wrong structures. This first automation step has considerably increased the efficiency of annotation. The average annotation time per sentence improved by 25%.</abstract>
<title confidence="0.896616">References</title>
<author confidence="0.890621">Ezra Black</author>
<title confidence="0.514069">sing: Producing a Comprehensive Large-Scale General-English Treebank With Full Grammati-</title>
<note confidence="0.895935666666667">Analysis. 16th International Confeon Computational Linguistics, 107 — 113, Copenhagen, Denmark. Mitchell Marcus et al. 1994. The Penn Treebank: Annotating Predicate Argument Structure. In Proceedings of the Human Language Technology Francisco. Morgan Kaufmann. Sampson. 1995. for the Computer. The SUSANNE Corpus and Analytic Scheme. Skut et al. 1997. Annotation Scheme Free Word Order Languages. 7th Conference on Applied Natural Language Processing,</note>
<address confidence="0.623195">Washington, DC.</address>
<intro confidence="0.49568">28</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ezra Black</author>
</authors>
<title>Beyond Skeleton Parsing: Producing a Comprehensive Large-Scale General-English Treebank With Full Grammatical Analysis.</title>
<date>1996</date>
<booktitle>In The 16th International Conference on Computational Linguistics,</booktitle>
<pages>107--113</pages>
<location>Copenhagen, Denmark.</location>
<marker>Black, 1996</marker>
<rawString>Ezra Black et al. 1996. Beyond Skeleton Parsing: Producing a Comprehensive Large-Scale General-English Treebank With Full Grammatical Analysis. In The 16th International Conference on Computational Linguistics, pages 107 — 113, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
</authors>
<title>The Penn Treebank: Annotating Predicate Argument Structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<marker>Marcus, 1994</marker>
<rawString>Mitchell Marcus et al. 1994. The Penn Treebank: Annotating Predicate Argument Structure. In Proceedings of the Human Language Technology Workshop, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>English for the Computer. The SUSANNE Corpus and Analytic Scheme.</title>
<date>1995</date>
<contexts>
<context position="1298" citStr="Sampson, 1995" startWordPosition="178" endWordPosition="179">uirements imposed by the architecture of the annotation scheme (cf. (Skut et al., 1997)), which can itself be characterised as follows: • Direct representation of the underlying argument structure in terms of unordered trees; • Rudimentary, flat representations; uniform treatment of local and non-local dependencies; • Extensive encoding of linguistic information in grammatical function labels. Thus the format of the annotations is somewhat different from treebanks relying on a context-free backbone augmented with trace-filler annotations of non-local dependencies. (cf. (Marcus et al., 1994), (Sampson, 1995), (Black et al., 1996)) Nevertheless, such treebanks can also be developed using our tool. To back this claim, the representation of structures from the SUZANNE corpus (cf. (Sampson, 1995)) will be shown in the presentation. 2 User Interface A screen dump of the tool is shown in fig. 1. The largest part of the window contains the graphical representation of the structure being annotated. The nodes and edges are assigned category and grammatical function labels, respectively. The words are numbered and labelled with part-of-speech tags. Any change into the structure of the sentence being annota</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>Geoffrey Sampson. 1995. English for the Computer. The SUSANNE Corpus and Analytic Scheme.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
</authors>
<title>An Annotation Scheme For Free Word Order Languages.</title>
<date>1997</date>
<booktitle>In The 7th Conference on Applied Natural Language Processing,</booktitle>
<location>Washington, DC.</location>
<marker>Skut, 1997</marker>
<rawString>Wojciech Skut et al. 1997. An Annotation Scheme For Free Word Order Languages. In The 7th Conference on Applied Natural Language Processing, Washington, DC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>