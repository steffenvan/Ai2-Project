<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000307">
<title confidence="0.987782">
Parser lexicalisation through self-learning
</title>
<author confidence="0.978223">
Marek Rei
</author>
<affiliation confidence="0.9712105">
Computer Labratory
University of Cambridge
</affiliation>
<address confidence="0.665849">
United Kingdom
</address>
<email confidence="0.992802">
Marek.Rei@cl.cam.ac.uk
</email>
<author confidence="0.994949">
Ted Briscoe
</author>
<affiliation confidence="0.9921725">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.667924">
United Kingdom
</address>
<email confidence="0.995987">
Ted.Briscoe@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.995598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899157894737">
We describe a new self-learning framework
for parser lexicalisation that requires only a
plain-text corpus of in-domain text. The
method first creates augmented versions of de-
pendency graphs by applying a series of mod-
ifications designed to directly capture higher-
order lexical path dependencies. Scores are
assigned to each edge in the graph using statis-
tics from an automatically parsed background
corpus. As bilexical dependencies are sparse,
a novel directed distributional word similar-
ity measure is used to smooth edge score es-
timates. Edge scores are then combined into
graph scores and used for reranking the top-
n analyses found by the unlexicalised parser.
The approach achieves significant improve-
ments on WSJ and biomedical text over the
unlexicalised baseline parser, which is origi-
nally trained on a subset of the Brown corpus.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963956521739">
Most parsers exploit supervised machine learning
methods and a syntactically annotated dataset (i.e.
treebank), incorporating a wide range of features in
the training process to deliver competitive perfor-
mance. The use of lexically-conditioned features,
such as relations between lemmas or word forms,
is often critical when choosing the correct syntac-
tic analysis in ambiguous contexts. However, util-
ising such features leads the parser to learn infor-
mation that is often specific to the domain and/or
genre of the training data. Several experiments have
demonstrated that many lexical features learnt in
one domain provide little if any benefit when pars-
ing text from different domains and genres (Sekine,
1997; Gildea, 2001). Furthermore, manual creation
of in-domain treebanks is an expensive and time-
consuming process, which can only be performed by
experts with sufficient linguistic and domain knowl-
edge.
In contrast, unlexicalised parsers avoid using lex-
ical information and select a syntactic analysis us-
ing only more general features, such as POS tags.
While they cannot be expected to achieve optimal
performance when trained and tested in a single do-
main, unlexicalised parsers can be surprisingly com-
petitive with their lexicalised counterparts (Klein
and Manning, 2003; Petrov et al., 2006). In this
work, instead of trying to adapt a lexicalised parser
to new domains, we explore how bilexical features
can be integrated effectively with any unlexicalised
parser. As our novel self-learning framework re-
quires only a large unannotated corpus, lexical fea-
tures can be easily tuned to a specific domain or
genre by selecting a suitable dataset. In addition,
we describe a graph expansion process that captures
selected bilexical relations which improve perfor-
mance but would otherwise require sparse higher-
order dependency path feature types in most ap-
proaches to dependency parsing. As many bilex-
ical features will still be sparse, we also develop
an approach to estimating confidence scores for de-
pendency relations using a directional distributional
word similarity measure. The final framework in-
tegrates easily with any unlexicalised (and therefore
potentially less domain/genre-biased) parser capable
of returning ranked dependency analyses.
</bodyText>
<page confidence="0.981441">
391
</page>
<note confidence="0.512192">
Proceedings of NAACL-HLT 2013, pages 391–400,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.971708" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999427142857143">
We hypothesise that a large corpus will often contain
examples of dependency relations in non-ambiguous
contexts, and these will mostly be correctly parsed
by an unlexicalised parser. Lexical statistics derived
from the corpus can then be used to select the cor-
rect parse in a more difficult context. For example,
consider the following sentences:
</bodyText>
<listItem confidence="0.98002075">
(1) a. Government projects interest researchers
b. Government raises interest rates
c. Government projects receive funding
d. Interest rates are increasing
</listItem>
<bodyText confidence="0.999987583333334">
Noun-verb ambiguities over projects and interest
might erroneously result in the unlexicalised parser
returning similar dependency graphs for both a and
b. However, sentences c and d contain less ambigu-
ous instances of the same phrases and can provide
clues to correctly parsing the first two examples. In
a large in-domain corpus we are likely to find more
cases of researchers being the object for interest and
fewer cases where it is the object of project. In con-
trast, rates is more likely to have interest as a mod-
ifier than as a head in an object relation. Exploiting
this lexical information, we can assign the correct
derivation to each of the more ambiguous sentences.
Similar intuitions have been used to motivate the
acquisition of bilexical features from background
corpora for improving parser accuracy. However,
previous work has focused on including these statis-
tics as auxiliary features during supervised training.
For example, van Noord (2007) incorporated bilex-
ical preferences as features via self-training to im-
prove the Alpino parser for Dutch. Plank and van
Noord (2008) investigated the application of aux-
iliary distributions for domain adaptation. They
incorporated information from both in-domain and
out-of-domain sources into their maximum entropy
model and found that the out-of-domain auxiliary
distributions did not contribute to parsing accuracy
in the target domain. Zhou et al. (2011) extracted n-
gram counts from Google queries and a large corpus
to improve the MSTParser. In contrast to previous
work, we refer to our approach as self-learning be-
cause it differs from self-training by utilising statis-
tics found using an initial parse ranking model to
create a separate unsupervised reranking compo-
nent, without retraining the baseline unlexicalised
model.
We formulate our self-learning framework as a
reranking process that assigns new scores to the top-
n ranked analyses found by the original parser. Parse
reranking has been successfully used in previous
work as a method of including a wider range of fea-
tures to rescore a smaller selection of highly-ranked
candidate parses. Collins (2000) was one of the first
to propose supervised reranking as an additional step
to increase parser accuracy and achieved 1.55% ac-
curacy improvement for his parser. Charniak and
Johnson (2005) utilise a discriminative reranker and
show a 1.3% improvement for the Charniak parser.
McClosky et al. (2006) extend their work by adding
new features and further increase the performance
by 0.3%. Ng et al. (2010) implemented a dis-
criminative maximum entropy reranker for the C&amp;C
parser and showed a 0.23% improvement over the
baseline. Bansal and Klein (2011) discriminatively
rerank derivations from the Berkeley unlexicalised
parser (Petrov et al., 2006) demonstrating that lex-
ical features derived from the Google n-gram cor-
pus improve accuracy even when used in conjunc-
tion with other reranking features. They have all
treated reranking as a supervised task and trained a
discriminative classifier using parse tree features and
annotated in-domain data. In contrast, our reranker
only uses statistics from an unlabelled source and
requires no manual annotation or training of the
reranking component. As we utilise an unlexicalised
parser, our baseline performance on WSJ text is
lower compared to some fully-lexicalised parsers.
However, an unlexicalised parser is also likely to be
less biased to domains or genres manifested in the
text used to train its original ranking model. This
may allow the reranker to adapt it to a new domain
and/or genre more effectively.
</bodyText>
<sectionHeader confidence="0.928575" genericHeader="method">
3 Reordering dependency graphs
</sectionHeader>
<bodyText confidence="0.999892833333333">
For our experiments, we make use of the unlexi-
calised RASP parser (Briscoe et al., 2006) as the
baseline system. For every sentence s the parser
returns a list of dependency graphs G3, ranked by
the log probability of the associated derivation in the
structural ranking model. Our goal is to reorder this
</bodyText>
<page confidence="0.997384">
392
</page>
<bodyText confidence="0.999963095238095">
list to improve ranking accuracy and, most impor-
tantly, to improve the quality of the highest-ranked
dependency graph. This is done by assigning a con-
fidence score to every graph gs,r E Gs where r is the
rank of gs for sentence s. The method treats each
sentence independently, therefore we can omit the
sentence identifiers and refer to gs,r as gr.
We first calculate confidence scores for all the in-
dividual edges and then combine them into an over-
all score for the dependency graph. In the following
sections, we describe a series of graph modifications
that incorporates selected higher-order dependency
path relations, without introducing unwanted noise
or complexity into the reranker. Next, we outline
different approaches for calculating and smoothing
the confidence scores for bilexical relations. Finally,
we describe methods for combining together these
scores and calculating an overall score for a depen-
dency graph. We make publically available all the
code developed for performing these steps in the
parse reranking system.1
</bodyText>
<subsectionHeader confidence="0.998757">
3.1 Graph modifications
</subsectionHeader>
<bodyText confidence="0.936517863636364">
For every dependency graph gr the graph expan-
sion procedure creates a modified representation g&apos;r
which contains a wider range of bilexical relations.
The motivation for this graph expansion step is sim-
ilar to that motivating the move from first-order to
higher-order dependency path feature types (e.g.,
Carreras (2007)). However, compared to using all
nth-order paths, these rules are chosen to maximise
the utility and minimise the sparsity of the result-
ing bilexical features. In addition, the cascading na-
ture of the expansion steps means in some cases the
expansion captures useful 3rd and 4th order depen-
dencies. Similar approaches to graph modifications
have been successfully used for several NLP tasks
(van Noord, 2007; Arora et al., 2010).
For any edge e we also use notation (rel, w1, w2),
referring to an edge from w1 to w2 with the label
rel. We perform the following modifications on ev-
ery dependency graph:
1. Normalising lemmas. All lemmas are converted
to lowercase. Numerical lemmas are replaced
with more generic tags to reduce sparsity.
</bodyText>
<footnote confidence="0.718513">
1www.marekrei.com/projects/lexicalisation
</footnote>
<listItem confidence="0.880055416666667">
2. Bypassing conjunctions. For every edge pair
(rel1, w1, w2) and (rel2, w2, w3) where w2 is
tagged as a conjunction, we create an additional
edge (rel1, w1, w3). This bypasses the conjunc-
tion node and creates direct edges between the
head and dependents of the conjunctive lemma.
3. Bypassing prepositions. For every edge pair
(rel1, w1, w2) and (rel2, w2, w3) where w2 is
tagged as a preposition, we create an additional
edge (rel3, w1, w3). rel3 = rel1 +‘ prep’, where
‘ prep’ is added as a marker to indicate that the
relation originally contained a preposition.
4. Bypassing verbs. For every edge pair
(rel1, w1, w2) and (rel2, w1, w3) where w1 is
tagged as a verb, w2 and w3 are both tagged
as open-class lemmas, rel1 starts with a subject
relation, and rel2 starts with an object relation,
we create an additional edge (rel3, w2, w3) where
rel3 = rel1 + ‘-’ + rel2. This creates an additional
edge between the subject and the object, with the
new edge label containing both of the original la-
bels.
5. Duplicating nodes. For every existing node in
the graph, containing the lemma and POS for
</listItem>
<bodyText confidence="0.966156117647059">
each token (lemma pos), we create a parallel node
without the POS information (lemma). Then, for
each existing edge, we create three correspond-
ing edges, interconnecting the parallel nodes to
each other and the original graph. This allows the
reranker to exploit both specific and more generic
instantiations of each lemma.
Figure 1 illustrates the graph modification pro-
cess. It is important to note that each of these mod-
ifications gets applied in the order that they are de-
scribed above. For example, when creating edges for
bypassing verbs, the new edges for prepositions and
conjunctions have already been created and also par-
ticipate in this step. We performed ablation tests on
the development data and verified that each of these
modifications contributes positively to the final per-
formance.
</bodyText>
<subsectionHeader confidence="0.998865">
3.2 Edge scoring methods
</subsectionHeader>
<bodyText confidence="0.9996085">
We start the scoring process by assigning individual
confidence scores to every bilexical relation in the
</bodyText>
<page confidence="0.993147">
393
</page>
<figure confidence="0.9965166">
dobj conj
ncmod
ncmod ncsubj iobj ncmod conj
italian pm meet with cabinet member and senior official
JJ NP1 VVZ IW NN1 NN2 CC JJ NN2
</figure>
<figureCaption confidence="0.999355666666667">
Figure 1: Modified graph for the sentence ‘Italian PM meets with Cabinet members and senior officials’ after steps
1-4. Edges above the text are created by the parser, edges below the text are automatically created using the operations
described in Section 3.1. The 5th step will create 9 new nodes and 45 additional edges (not shown).
</figureCaption>
<figure confidence="0.994292714285714">
dobj
dobj
iobj prep
iobj prep
iobj prep
ncsubj-iobj prep
ncsubj-iobj prep
</figure>
<bodyText confidence="0.98575">
modified graph. In this section we give an overview
of some possible strategies for performing this task.
The parser returns a ranked list of graphs and this
can be used to derive an edge score without requir-
ing any additional information. We estimate that the
likelihood of a parse being the best possible parse for
a given sentence is roughly inversely proportional
to the rank that it is assigned by the parser. These
values can be summed for all graphs that contain a
specific edge, normalised to approximate a proba-
bility. We then calculate the score for edge e as the
Reciprocal Edge Score (RES) – the probability of e
belonging to the best possible parse:
</bodyText>
<equation confidence="0.905791333333333">
�Rr=1[1r x contains(g&apos;r, e)]
ER 1
r=1 r
</equation>
<bodyText confidence="0.9998452">
where R is the total number of parses for a sentence,
and contains(g&apos;r, e) returns 1 if graphg&apos;r contains
edge e, and 0 otherwise. The value is normalised,
so that an edge which is found in all parses will have
a score of 1.0, but occurrences at higher ranks will
have a considerably larger contribution.
The score of an edge can also be assigned by es-
timating the probability of that edge using a parsed
reference corpus. van Noord (2007) improved over-
all parsing performance in a supervised self-training
framework using feature weights based on pointwise
mutual information:
where P(rel, w1, w2) is the probability of seeing an
edge from w1 to w2 with label rel, P(rel, w1, *) is
the probability of seeing an edge from w1 to any
node with label rel, and P(*, *, w2) is the prob-
ability of seeing any type of edge linking to w2.
Plank and van Noord (2008) used the same approach
for semi-supervised domain adaptation but were not
able to achieve similar performance benefits. In our
implementation we omit the logarithm in the equa-
tion, as this improves performance and avoids prob-
lems with log(0) for unseen edges.
I(e) compares the probability of the complete
edge to the probabilities of partially specified edges,
but it assumes that w2 will have an incoming rela-
tion, and that w1 will have an outgoing relation of
type rel to some unknown node. These assumptions
may or may not be true – given the input sentence,
we have observed w1 and w2 but do not know what
relations they are involved in. Therefore, we create
a more general version of the measure that compares
the probability of the complete edge to the individual
probabilities of the two lemmas – the Conditional
Edge Score (CES1):
</bodyText>
<equation confidence="0.999647">
CES1(e) = P(w1) x P(w2)
</equation>
<bodyText confidence="0.999939555555555">
where P(w1) is the probability of seeing w1 in text,
estimated from a background corpus using maxi-
mum likelihood.
Finally, we know that w1 and w2 are in a sen-
tence together but cannot assume that there is a de-
pendency relation between them. However, we can
choose to think of each sentence as a fully connected
graph, with an edge going from every lemma to ev-
ery other lemma in the same sentence. If there exists
</bodyText>
<equation confidence="0.977003769230769">
RES(e) =
I(e) = log P(rel, w1, *) x P(*, *, w2)
P(rel, w1, w2)
P(rel, w1, w2)
394
P1sim(c w ) X P(rel,c1,w2) j` sim(c w ) X P(rel,w1,c2)
ECES1(rel,w1,w2) = × ( c1EC1 1� 1 P(c1)XP(w2) + czEC2 2� 2 P(w1)XP(c2) )
2 Pc1 EC1 sim (c1,w1) Pc2EC2 sim(c2,w2)
sim(/C w X P(rel,c1,w2) sim(/C w X P(rel,w1,c2)
1 PCJ
c EC l 1, 1) P(* ) �c EC l 2, 2) P(* )
2
c1EC1 sim(c1, w1) c2EC2 sim(c2, w2)
</equation>
<figureCaption confidence="0.835792">
Figure 2: Expanded edge score calculation methods using the list of distributionally similar lemmas
</figureCaption>
<equation confidence="0.995879">
ECES2(rel, w1, w2) = × ( i i cl,wz + z z wl,cz )
P P
</equation>
<bodyText confidence="0.999689727272727">
no genuine relation between the lemmas, the edge is
simply considered a null edge. We can then find the
conditional probability of the relation type given the
two lemmas:
where P(rel, w1, w2) is the probability of the fully-
specified relation, and P(∗, w1, w2) is the probability
of there being an edge of any type from w1 to w2, in-
cluding a null edge. Using fully connected graphs,
the latter is equivalent to the probability of w1 and
w2 appearing in a sentence together, which again can
be calculated from the background corpus.
</bodyText>
<subsectionHeader confidence="0.999491">
3.3 Smoothing edge scores
</subsectionHeader>
<bodyText confidence="0.996512818181818">
Apart from RES, all the scoring methods from
the previous section rely on correctly estimat-
ing the probability of the fully-specified edge,
P(rel, w1, w2). Even in a large background corpus
these triples will be very sparse, and it can be useful
to find approximate methods for estimating the edge
scores.
Using smoothing techniques derived from work
on language modelling, we could back-off to a more
general version of the relation. For example, if
(dobj, read, publication) is not frequent enough, the
value could be approximated using the probabilities
of (dobj, read, *) and (dobj, *, publication). How-
ever, this can lead to unexpected results due to com-
positionality – while (dobj, read, *) and (dobj, *,
rugby) can be fairly common, (dobj, read, rugby) is
an unlikely relation.
Instead, we can consider looking at other lemmas
which are similar to the rare lemmas in the relation.
If (dobj, read, publication) is infrequent in the data,
the system might predict that book is a reasonable
substitute for publication and use (dobj, read, book)
to estimate the original probability.
Given that we have a reliable way of finding likely
substitutes for a given lemma, we can create ex-
panded versions of CES1 and CES2, as shown in
Figure 2. C1 is the list of substitute lemmas for w1,
and sim(c1, w1) is a measure showing how similar
c1 is to w1. The methods iterate over the list of sub-
stitutes and calculate the CES score for each of the
modified relations. The values are then combined by
using the similarity score as a weight – more similar
lemmas will have a higher contribution to the final
result. This is done for both the head and the depen-
dent in the original relation, and the scores are then
normalised and averaged.
Experiments with a wide variety of distributional
word similarity measures revealed that WeightedCo-
sine (Rei, 2013), a directional similarity measure
designed to better capture hyponymy relations, per-
formed best. Hyponyms are more specific versions
of a word and normally include the general proper-
ties of the hypernym, making them well-suited for
lexical substitution. The WeightedCosine measure
incorporates an additional directional weight into
the standard cosine similarity, assigning different
importance to individual features for the hyponymy
relation. We retain the 10 most distributionally simi-
lar putative hyponyms for each lemma and substitute
them in the relation. The original lemma is also in-
cluded with similarity 1.0, thereby assigning it the
highest weight. The lemma vectors are built from
the same vector space model that is used for cal-
culating edge probabilities, which includes all the
graph modifications described in Section 3.1.
</bodyText>
<subsectionHeader confidence="0.993593">
3.4 Combining edge scores
</subsectionHeader>
<bodyText confidence="0.99983625">
While the CES and ECES measures calculate con-
fidence scores for bilexical relations using statistics
from a large background corpus, they do not include
any knowledge about grammar, syntax, or the con-
</bodyText>
<equation confidence="0.8621782">
CES2(e) = P(∗, w1, w2)
P(rel, w1, w2)
395
� �
CMB1(e) = 3 RES(e) * CES1(e) * CES2(e) CMB2(e) = 3 RES(e) * ECES1(e) * ECES2(e)
</equation>
<figureCaption confidence="0.991036">
Figure 3: Edge score combination methods
</figureCaption>
<bodyText confidence="0.999970833333333">
text in a specific sentence. In contrast, the RES score
implicitly includes some of this information, as it is
calculated based on the original parser ranking. In
order to take advantage of both information sources,
we combine these scores into CMB1 and CMB2, as
shown in Figure 3.
</bodyText>
<subsectionHeader confidence="0.987639">
3.5 Graph scoring
</subsectionHeader>
<bodyText confidence="0.999892466666667">
Every edge in graph g&apos;r is assigned a score indicat-
ing the reranker’s confidence in that edge belonging
to the best parse. We investigated different strate-
gies for combining these values together into a con-
fidence score for the whole graph. The simplest so-
lution is to sum together individual edge scores, but
this would lead to always preferring graphs that have
a larger number of edges. Interestingly, averaging
the edge scores does not produce good results either
because it is biased towards smaller graph fragments
containing only highly-confident edges.
We created a new scoring method which prefers
graphs that cover all the nodes, but does not create
bias for a higher number of edges. For every node
in the graph, it finds the average score of all edges
which have that node as a dependent. These scores
are then averaged again over all nodes:
where g is the graph being scored, n E Ng is a
node in graph g, e E Eg is an edge in graph g,
isDep(e, n) is a function returning 1.0 if n is the de-
pendent in edge e, and 0.0 otherwise. NScore(n) is
set to 0 if the node does not appear as a dependent in
any edges. We found this metric performs well, as
it prefers graphs that connect together many nodes
without simply rewarding a larger number of edges.
While the score calculation is done using the
modified graph g&apos;r, the resulting score is directly as-
signed to the corresponding original graph gr, and
the reordering of the original dependency graphs is
used for evaluation.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.927184">
4.1 Evaluation methods
</subsectionHeader>
<bodyText confidence="0.999645958333333">
In order to evaluate how much the reranker improves
the highest-ranked dependency graph, we calculate
the microaveraged precision, recall and F-score over
all dependencies from the top-ranking parses for
the test set. Following the official RASP evalua-
tion (Briscoe et al., 2006) we employ the hierarchi-
cal edge matching scheme which aggregates counts
up the dependency relation subsumption hierarchy
and thus rewards the parser for making more fine-
grained distinctions.2 Statistical significance of the
change in F-score is calculated by using the Approx-
imate Randomisation Test (Noreen, 1989; Cohen,
1995) with 106 iterations.
We also wish to measure how well the reranker
does at the overall task of ordering dependency
graphs. For this we make use of an oracle that cre-
ates the perfect ranking for a set of graphs by calcu-
lating their individual F-scores; this ideal ranking is
then compared to the output of our system. Spear-
man’s rank correlation coefficient between the two
rankings is calculated for each sentence and then av-
eraged over all sentences. If the scores for all of the
returned analyses are equal, this coefficient cannot
be calculated and is set to 0.
</bodyText>
<subsectionHeader confidence="0.955236">
4.2 DepBank
</subsectionHeader>
<bodyText confidence="0.999132714285714">
We evaluated our self-learning framework using
the DepBank/GR reannotation (Briscoe and Carroll,
2006) of the PARC 700 Dependency Bank (King
et al., 2003). The dataset is provided with the
open-source RASP distribution3 and has been used
for evaluating different parsers, including RASP
(Briscoe and Carroll, 2006; Watson et al., 2007) and
</bodyText>
<footnote confidence="0.69151925">
2Slight changes in the performance of the baseline parser
compared to previous publications are due to using a more re-
cent version of the parser and minor corrections to the gold stan-
dard annotation.
</footnote>
<equation confidence="0.898933166666667">
3ilexir.co.uk/2012/open-source-rasp-3-1/
NScore(n) = EeEEg isDep(e, n)
GraphScore(g) = |N9|
E
eEEg EdgeScore(e) x isDep(e, n)
EnENg NScore(n)
</equation>
<page confidence="0.995122">
396
</page>
<bodyText confidence="0.999899974358975">
C&amp;C (Clark and Curran, 2007). It contains 700 sen-
tences, randomly chosen from section 23 of the WSJ
Penn Treebank (Marcus et al., 1993), divided into
development (140 sentences) and test data (560 sen-
tences). We made use of the development data to
experiment with a wider selection of edge and graph
scoring methods, and report the final results on the
test data.
For reranking we collect up to 1000 top-ranked
analyses for each sentence. The actual number of
analyses that the RASP parser outputs depends on
the sentence and can be smaller. As the parser first
constructs parse trees and converts them to depen-
dency graphs, several parse trees may result in iden-
tical graphs; we remove any duplicates to obtain a
ranking of unique dependency graphs.
Our approach relies on a large unannotated corpus
of in-domain text, and for this we used the BLLIP
corpus containing 50M words of in-domain WSJ ar-
ticles. Our version of this corpus excludes texts that
are found in the Penn Treebank, thereby also exclud-
ing the section that we use for evaluation.
The baseline system is the unlexicalised RASP
parser with default settings. In order to construct
the upper bound, we use an oracle to calculate the F-
score for each dependency graph individually, and
then create the best possible ranking using these
scores.
Table 1 contains evaluation results on the Dep-
Bank/GR test set. The baseline system achieves
76.41% F-score on the test data, with 32.70% av-
erage correlation. I and RES scoring methods give
comparable results, with RES improving correlation
by 9.56%. The CES and ECES scores all make use
of corpus-based statistics and all significantly im-
prove over the baseline system, with absolute in-
creases in F-score of more than 2% for the fully-
connected edge score variants.
Finally, we combine the RES score with the
corpus-based methods and the fully-connected
CMB2 variant again delivers the best overall results.
The final F-score is 79.21%, an absolute improve-
ment of 2.8%, corresponding to 33.65% relative er-
ror reduction with respect to the upper bound. Cor-
relation is also increased by 16.32%; this means the
methods not only improve the chances of finding the
best dependency graph, but also manage to create
a better overall ranking. The F-scores for all the
corpus-based scoring methods are statistically sig-
nificant when compared to the baseline (p &lt; 0.05).
By using our self-learning framework, we were
able to significantly improve the original unlexi-
calised parser. To put the overall result in a wider
perspective, Clark and Curran (2007) achieve an
F-score of 81.86% on the DepBank/GR test sen-
tences using the C&amp;C lexicalised parser, trained
on 40,000 manually-treebanked sentences from the
WSJ. The unlexicalised RASP parser, using a
manually-developed grammar and a parse ranking
component trained on 4,000 partially-bracketed un-
labelled sentences from a domain/genre balanced
subset of Brown (Watson et al., 2007), achieves an
F-score of 76.41% on the same test set. The method
introduced here improves this to 79.21% F-score
without using any further manually-annotated data,
closing more than half of the gap between the perfor-
mance of a fully-supervised in-domain parser and a
more weakly-supervised more domain-neutral one.
We also performed an additional detailed analysis
of the results and found that, with the exception of
the auxiliary dependency relation, the reranking pro-
cess was able to improve the F-score of all other in-
dividual dependency types. Complements and mod-
ifiers are attached with much higher accuracy, result-
ing in 3.34% and 3.15% increase in the correspond-
ing F-scores. The non-clausal modifier relation (nc-
mod), which is the most frequent label in the dataset,
increases by 3.16%.
</bodyText>
<subsectionHeader confidence="0.996991">
4.3 Genia
</subsectionHeader>
<bodyText confidence="0.99965725">
One advantage of our reranking framework is that
it does not rely on any domain-dependent manually
annotated resources. Therefore, we are interested in
seeing how it performs on text from a completely
different domain and genre.
The GENIA-GR dataset (Tateisi et al., 2008) is
a collection of 492 sentences taken from biomedi-
cal research papers in the GENIA corpus (Kim et
al., 2003). The sentences have been manually anno-
tated with dependency-based grammatical relations
identical to those output by the RASP parser. How-
ever, it does not contain dependencies for all tokens
and many multi-word phrases are treated as single
units. For example, the tokens ‘intracellular redox
status’ are annotated as one node with label intra-
cellular redox status. We retain this annotation and
</bodyText>
<page confidence="0.993728">
397
</page>
<table confidence="0.999869333333333">
Prec DepBank/GR p Prec GENIA-GR p
Rec F Rec F
Baseline 77.91 74.97 76.41 32.70 79.91 78.86 79.38 36.54
Upper Bound 86.74 82.82 84.73 75.36 86.33 84.71 85.51 78.66
I 77.77 75.00 76.36 33.32 77.18 76.21 76.69 30.23
RES 78.13 74.94 76.50 42.26 80.06 78.89 79.47 47.52
CES1 79.68 76.40 78.01 41.95 78.64 77.50 78.07 36.06
CES2 80.48 77.28 78.85 48.43 79.92 78.92 79.42 43.09
ECES1 79.96 76.68 78.29 42.41 79.09 78.11 78.60 38.02
ECES2 80.71 77.52 79.08 49.05 79.84 78.95 79.39 43.64
CMB1 80.64 77.31 78.94 48.25 80.60 79.51 80.05 44.96
CMB2 80.88 77.60 79.21 49.02 80.69 79.64 80.16 46.24
</table>
<tableCaption confidence="0.933100666666667">
Table 1: Performance of different edge scoring methods on the test data. For each measure we report precision,
recall, F-score, and average Spearman’s correlation (p). The highest results for each measure are marked in bold. The
underlined F-scores are significantly better compared to the baseline.
</tableCaption>
<bodyText confidence="0.999862041666667">
allow the unlexicalised parser to treat these nodes as
atomic unseen words during POS tagging and pars-
ing. However, we use the last lemma in each multi-
word phrase for calculating the edge score statistics.
In order to initialise our parse reranking frame-
work, we also need a background corpus that closely
matches the evaluation domain. The annotated sen-
tences in GENIA-GR were chosen from abstracts
that are labelled with the MeSH term ‘NF-kappa B’.
Following this method, we created our background
corpus by extracting 7,100 full-text articles (1.6M
sentences) from the PubMed Central Open Access
collection, containing any of the following terms
with any capitalisation: ‘nf-kappa b’, ‘nf-kappab’,
‘nf kappa b’, ‘nf-kappa b’, ‘nf-kb’, ‘nf-nb’. Since
we retain all texts from matching documents, this
keyword search acts as a broad indicator that the sen-
tences contain topics which correspond to the evalu-
ation dataset. This focussed corpus was then parsed
with the unlexicalised parser and used to create a
statistical model for the reranking system, following
the same methods as described in Sections 3 and 4.2.
Table 1 also contains the results for experiments
in the biomedical domain. The first thing to notice
is that while the upper bound for the unlexicalised
parser is similar to that for the DepBank experiments
in Section 4.2, the baseline results are considerably
higher. This is largely due to the nature of the dataset
– since many complicated multi-word phrases are
treated as single nodes, the parser is not evaluated on
edges within these nodes. In addition, treating these
nodes as unseen words eliminates many incorrect
derivations that would otherwise split the phrases.
This results in a naturally higher baseline of 79.38%,
and also makes it more difficult to further improve
the performance.
The edge scoring methods I, CES1 and ECES1
deliver F-scores lower than the baseline in this ex-
periment. RES, CES2 and ECES2 yield a modest
improvement in both F-score and Spearman’s cor-
relation. Finally, the combination methods again
give the best performance, with CMB2 delivering an
F-score of 80.16%, an absolute increase of 0.78%,
which is statistically significant (p &lt; 0.05). The
experiment shows that our self-learning framework
works on very different domains, and it can be used
to significantly increase the accuracy of an unlexi-
calised parser without requiring any annotated data.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999959636363636">
We developed a new self-learning framework for de-
pendency graph reranking that requires only a plain-
text corpus from a suitable domain. We automati-
cally parse this corpus and use the highest ranked
analyses to estimate maximum likelihood probabili-
ties for bilexical relations. Every dependency graph
is first modified to incorporate additional edges that
model selected higher-order dependency path rela-
tionships. Each edge in the graph is then assigned a
confidence score based on statistics from the back-
ground corpus and ranking preferences from the un-
</bodyText>
<page confidence="0.996">
398
</page>
<bodyText confidence="0.9999319">
lexicalised parser. We also described a novel method
for smoothing these scores using directional dis-
tributional similarity measures. Finally, the edge
scores are combined into an overall graph score by
first averaging them over individual nodes.
As the method requires no annotated data, it can
be easily adapted to different domains and genres.
Our experiments showed that the reranking process
significantly improved performance on both WSJ
and biomedical data.
</bodyText>
<sectionHeader confidence="0.99852" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999851677777778">
Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Ros´e,
and Eric Nyberg. 2010. Sentiment Classification us-
ing Automatically Extracted Subgraph Features. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text.
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, pages 693–702.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the COLING/ACL
on Main conference poster sessions, number July,
pages 41–48, Morristown, NJ, USA. Association for
Computational Linguistics.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, number July, pages 77–80, Sydney, Aus-
tralia. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, vol-
ume 7, pages 957–961.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics - ACL ’05,
1(June):173–180.
Stephen Clark and James R. Curran. 2007. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, vol-
ume 45, pages 248–255.
Paul R Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press, Cambridge, MA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In The 17th International Con-
ference on Machine Learning (ICML).
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing,
pages 167–202.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi
Tsujii. 2003. GENIA corpus - a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(1):180–182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the EACL03:
4th International Workshop on Linguistically Inter-
preted Corpora (LINC-03), pages 1–8.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, number July, pages 423–430. Association for
Computational Linguistics Morristown, NJ, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, pages 1–22.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, number June, pages 152–
159, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Dominick Ng, Matthew Honnibal, and James R. Curran.
2010. Reranking a wide-coverage CCG parser. In
Australasian Language Technology Association Work-
shop 2010, page 90.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley, New
York.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of theACL (ACL ’06),
pages 433–440, Morristown, NJ, USA. Association for
Computational Linguistics.
Barbara Plank and Gertjan van Noord. 2008. Explor-
ing an auxiliary distribution based approach to domain
adaptation of a syntactic disambiguation model. In
Coling 2008: Proceedings of the Workshop on Cross-
Framework and Cross- Domain Parser Evaluation,
pages 9–16, Manchester, UK. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.988524">
399
</page>
<reference confidence="0.999550785714286">
Marek Rei. 2013. Minimally supervised dependency-
based methods for natural language processing. Ph.D.
thesis, University of Cambridge.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proceedings of the fifth conference on Applied
natural language processing, volume 1, pages 96–102,
Morristown, NJ, USA. Association for Computational
Linguistics.
Yuka Tateisi, Yusuke Miyao, Kenji Sagae, and Jun’ichi
Tsujii. 2008. GENIA-GR: a Grammatical Relation
Corpus for Parser Evaluation in the Biomedical Do-
main. In Proceedings of LREC, pages 1942–1948.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the 10th International Conference on
Parsing Technologies, number June, pages 1–10, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Rebecca Watson, Ted Briscoe, and John Carroll. 2007.
Semi-supervised training of a statistical parser from
unlabeled partially-bracketed data. Proceedings of the
10th International Conference on Parsing Technolo-
gies - IWPT ’07, (June):23–32.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting Web-Derived Selectional Preference to Im-
prove Statistical Dependency Parsing. In 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1556–1565.
</reference>
<page confidence="0.997275">
400
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.198841">
<title confidence="0.999315">Parser lexicalisation through self-learning</title>
<author confidence="0.995326">Marek</author>
<affiliation confidence="0.872352333333333">Computer University of United</affiliation>
<email confidence="0.449562">Marek.Rei@cl.cam.ac.uk</email>
<author confidence="0.884378">Ted</author>
<affiliation confidence="0.882589333333333">Computer University of United</affiliation>
<email confidence="0.781048">Ted.Briscoe@cl.cam.ac.uk</email>
<abstract confidence="0.99654925">We describe a new self-learning framework for parser lexicalisation that requires only a plain-text corpus of in-domain text. The method first creates augmented versions of dependency graphs by applying a series of modifications designed to directly capture higherorder lexical path dependencies. Scores are assigned to each edge in the graph using statistics from an automatically parsed background corpus. As bilexical dependencies are sparse, a novel directed distributional word similarity measure is used to smooth edge score estimates. Edge scores are then combined into graph scores and used for reranking the topfound by the unlexicalised parser. The approach achieves significant improvements on WSJ and biomedical text over the unlexicalised baseline parser, which is originally trained on a subset of the Brown corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shilpa Arora</author>
<author>Elijah Mayfield</author>
<author>Carolyn Penstein-Ros´e</author>
<author>Eric Nyberg</author>
</authors>
<title>Sentiment Classification using Automatically Extracted Subgraph Features.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text.</booktitle>
<marker>Arora, Mayfield, Penstein-Ros´e, Nyberg, 2010</marker>
<rawString>Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Ros´e, and Eric Nyberg. 2010. Sentiment Classification using Automatically Extracted Subgraph Features. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>693--702</pages>
<contexts>
<context position="6684" citStr="Bansal and Klein (2011)" startWordPosition="1011" endWordPosition="1014">score a smaller selection of highly-ranked candidate parses. Collins (2000) was one of the first to propose supervised reranking as an additional step to increase parser accuracy and achieved 1.55% accuracy improvement for his parser. Charniak and Johnson (2005) utilise a discriminative reranker and show a 1.3% improvement for the Charniak parser. McClosky et al. (2006) extend their work by adding new features and further increase the performance by 0.3%. Ng et al. (2010) implemented a discriminative maximum entropy reranker for the C&amp;C parser and showed a 0.23% improvement over the baseline. Bansal and Klein (2011) discriminatively rerank derivations from the Berkeley unlexicalised parser (Petrov et al., 2006) demonstrating that lexical features derived from the Google n-gram corpus improve accuracy even when used in conjunction with other reranking features. They have all treated reranking as a supervised task and trained a discriminative classifier using parse tree features and annotated in-domain data. In contrast, our reranker only uses statistics from an unlabelled source and requires no manual annotation or training of the reranking component. As we utilise an unlexicalised parser, our baseline pe</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions, number July,</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="22794" citStr="Briscoe and Carroll, 2006" startWordPosition="3739" endWordPosition="3742"> the reranker does at the overall task of ordering dependency graphs. For this we make use of an oracle that creates the perfect ranking for a set of graphs by calculating their individual F-scores; this ideal ranking is then compared to the output of our system. Spearman’s rank correlation coefficient between the two rankings is calculated for each sentence and then averaged over all sentences. If the scores for all of the returned analyses are equal, this coefficient cannot be calculated and is set to 0. 4.2 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation (Briscoe and Carroll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3ilexir.co.uk/2012/open-source-rasp-3-1/ NScore(n) = EeEEg isDep(e, n) GraphScore(g) = |N9| E eEEg EdgeScore(e) x isDep(e, n) EnENg NScore(n) 396 C&amp;C (Clark and C</context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank. In Proceedings of the COLING/ACL on Main conference poster sessions, number July, pages 41–48, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, number July,</booktitle>
<pages>77--80</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="7717" citStr="Briscoe et al., 2006" startWordPosition="1172" endWordPosition="1175">our reranker only uses statistics from an unlabelled source and requires no manual annotation or training of the reranking component. As we utilise an unlexicalised parser, our baseline performance on WSJ text is lower compared to some fully-lexicalised parsers. However, an unlexicalised parser is also likely to be less biased to domains or genres manifested in the text used to train its original ranking model. This may allow the reranker to adapt it to a new domain and/or genre more effectively. 3 Reordering dependency graphs For our experiments, we make use of the unlexicalised RASP parser (Briscoe et al., 2006) as the baseline system. For every sentence s the parser returns a list of dependency graphs G3, ranked by the log probability of the associated derivation in the structural ranking model. Our goal is to reorder this 392 list to improve ranking accuracy and, most importantly, to improve the quality of the highest-ranked dependency graph. This is done by assigning a confidence score to every graph gs,r E Gs where r is the rank of gs for sentence s. The method treats each sentence independently, therefore we can omit the sentence identifiers and refer to gs,r as gr. We first calculate confidence</context>
<context position="21786" citStr="Briscoe et al., 2006" startWordPosition="3576" endWordPosition="3579">t connect together many nodes without simply rewarding a larger number of edges. While the score calculation is done using the modified graph g&apos;r, the resulting score is directly assigned to the corresponding original graph gr, and the reordering of the original dependency graphs is used for evaluation. 4 Experiments 4.1 Evaluation methods In order to evaluate how much the reranker improves the highest-ranked dependency graph, we calculate the microaveraged precision, recall and F-score over all dependencies from the top-ranking parses for the test set. Following the official RASP evaluation (Briscoe et al., 2006) we employ the hierarchical edge matching scheme which aggregates counts up the dependency relation subsumption hierarchy and thus rewards the parser for making more finegrained distinctions.2 Statistical significance of the change in F-score is calculated by using the Approximate Randomisation Test (Noreen, 1989; Cohen, 1995) with 106 iterations. We also wish to measure how well the reranker does at the overall task of ordering dependency graphs. For this we make use of an oracle that creates the perfect ranking for a set of graphs by calculating their individual F-scores; this ideal ranking </context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, number July, pages 77–80, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL,</booktitle>
<volume>7</volume>
<pages>957--961</pages>
<contexts>
<context position="9322" citStr="Carreras (2007)" startWordPosition="1428" endWordPosition="1429">dence scores for bilexical relations. Finally, we describe methods for combining together these scores and calculating an overall score for a dependency graph. We make publically available all the code developed for performing these steps in the parse reranking system.1 3.1 Graph modifications For every dependency graph gr the graph expansion procedure creates a modified representation g&apos;r which contains a wider range of bilexical relations. The motivation for this graph expansion step is similar to that motivating the move from first-order to higher-order dependency path feature types (e.g., Carreras (2007)). However, compared to using all nth-order paths, these rules are chosen to maximise the utility and minimise the sparsity of the resulting bilexical features. In addition, the cascading nature of the expansion steps means in some cases the expansion captures useful 3rd and 4th order dependencies. Similar approaches to graph modifications have been successfully used for several NLP tasks (van Noord, 2007; Arora et al., 2010). For any edge e we also use notation (rel, w1, w2), referring to an edge from w1 to w2 with the label rel. We perform the following modifications on every dependency grap</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, volume 7, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics - ACL ’05,</booktitle>
<pages>1--173</pages>
<contexts>
<context position="6323" citStr="Charniak and Johnson (2005)" startWordPosition="953" endWordPosition="956">eate a separate unsupervised reranking component, without retraining the baseline unlexicalised model. We formulate our self-learning framework as a reranking process that assigns new scores to the topn ranked analyses found by the original parser. Parse reranking has been successfully used in previous work as a method of including a wider range of features to rescore a smaller selection of highly-ranked candidate parses. Collins (2000) was one of the first to propose supervised reranking as an additional step to increase parser accuracy and achieved 1.55% accuracy improvement for his parser. Charniak and Johnson (2005) utilise a discriminative reranker and show a 1.3% improvement for the Charniak parser. McClosky et al. (2006) extend their work by adding new features and further increase the performance by 0.3%. Ng et al. (2010) implemented a discriminative maximum entropy reranker for the C&amp;C parser and showed a 0.23% improvement over the baseline. Bansal and Klein (2011) discriminatively rerank derivations from the Berkeley unlexicalised parser (Petrov et al., 2006) demonstrating that lexical features derived from the Google n-gram corpus improve accuracy even when used in conjunction with other reranking</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics - ACL ’05, 1(June):173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Formalismindependent parser evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>248--255</pages>
<contexts>
<context position="23406" citStr="Clark and Curran, 2007" startWordPosition="3834" endWordPosition="3837">roll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3ilexir.co.uk/2012/open-source-rasp-3-1/ NScore(n) = EeEEg isDep(e, n) GraphScore(g) = |N9| E eEEg EdgeScore(e) x isDep(e, n) EnENg NScore(n) 396 C&amp;C (Clark and Curran, 2007). It contains 700 sentences, randomly chosen from section 23 of the WSJ Penn Treebank (Marcus et al., 1993), divided into development (140 sentences) and test data (560 sentences). We made use of the development data to experiment with a wider selection of edge and graph scoring methods, and report the final results on the test data. For reranking we collect up to 1000 top-ranked analyses for each sentence. The actual number of analyses that the RASP parser outputs depends on the sentence and can be smaller. As the parser first constructs parse trees and converts them to dependency graphs, sev</context>
<context position="25929" citStr="Clark and Curran (2007)" startWordPosition="4253" endWordPosition="4256">ore is 79.21%, an absolute improvement of 2.8%, corresponding to 33.65% relative error reduction with respect to the upper bound. Correlation is also increased by 16.32%; this means the methods not only improve the chances of finding the best dependency graph, but also manage to create a better overall ranking. The F-scores for all the corpus-based scoring methods are statistically significant when compared to the baseline (p &lt; 0.05). By using our self-learning framework, we were able to significantly improve the original unlexicalised parser. To put the overall result in a wider perspective, Clark and Curran (2007) achieve an F-score of 81.86% on the DepBank/GR test sentences using the C&amp;C lexicalised parser, trained on 40,000 manually-treebanked sentences from the WSJ. The unlexicalised RASP parser, using a manually-developed grammar and a parse ranking component trained on 4,000 partially-bracketed unlabelled sentences from a domain/genre balanced subset of Brown (Watson et al., 2007), achieves an F-score of 76.41% on the same test set. The method introduced here improves this to 79.21% F-score without using any further manually-annotated data, closing more than half of the gap between the performance</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Formalismindependent parser evaluation with CCG and DepBank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, volume 45, pages 248–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="22114" citStr="Cohen, 1995" startWordPosition="3626" endWordPosition="3627">hods In order to evaluate how much the reranker improves the highest-ranked dependency graph, we calculate the microaveraged precision, recall and F-score over all dependencies from the top-ranking parses for the test set. Following the official RASP evaluation (Briscoe et al., 2006) we employ the hierarchical edge matching scheme which aggregates counts up the dependency relation subsumption hierarchy and thus rewards the parser for making more finegrained distinctions.2 Statistical significance of the change in F-score is calculated by using the Approximate Randomisation Test (Noreen, 1989; Cohen, 1995) with 106 iterations. We also wish to measure how well the reranker does at the overall task of ordering dependency graphs. For this we make use of an oracle that creates the perfect ranking for a set of graphs by calculating their individual F-scores; this ideal ranking is then compared to the output of our system. Spearman’s rank correlation coefficient between the two rankings is calculated for each sentence and then averaged over all sentences. If the scores for all of the returned analyses are equal, this coefficient cannot be calculated and is set to 0. 4.2 DepBank We evaluated our self-</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R Cohen. 1995. Empirical Methods for Artificial Intelligence. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In The 17th International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="6136" citStr="Collins (2000)" startWordPosition="925" endWordPosition="926">ast to previous work, we refer to our approach as self-learning because it differs from self-training by utilising statistics found using an initial parse ranking model to create a separate unsupervised reranking component, without retraining the baseline unlexicalised model. We formulate our self-learning framework as a reranking process that assigns new scores to the topn ranked analyses found by the original parser. Parse reranking has been successfully used in previous work as a method of including a wider range of features to rescore a smaller selection of highly-ranked candidate parses. Collins (2000) was one of the first to propose supervised reranking as an additional step to increase parser accuracy and achieved 1.55% accuracy improvement for his parser. Charniak and Johnson (2005) utilise a discriminative reranker and show a 1.3% improvement for the Charniak parser. McClosky et al. (2006) extend their work by adding new features and further increase the performance by 0.3%. Ng et al. (2010) implemented a discriminative maximum entropy reranker for the C&amp;C parser and showed a 0.23% improvement over the baseline. Bansal and Klein (2011) discriminatively rerank derivations from the Berkel</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In The 17th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>167--202</pages>
<contexts>
<context position="1823" citStr="Gildea, 2001" startWordPosition="267" endWordPosition="268">corporating a wide range of features in the training process to deliver competitive performance. The use of lexically-conditioned features, such as relations between lemmas or word forms, is often critical when choosing the correct syntactic analysis in ambiguous contexts. However, utilising such features leads the parser to learn information that is often specific to the domain and/or genre of the training data. Several experiments have demonstrated that many lexical features learnt in one domain provide little if any benefit when parsing text from different domains and genres (Sekine, 1997; Gildea, 2001). Furthermore, manual creation of in-domain treebanks is an expensive and timeconsuming process, which can only be performed by experts with sufficient linguistic and domain knowledge. In contrast, unlexicalised parsers avoid using lexical information and select a syntactic analysis using only more general features, such as POS tags. While they cannot be expected to achieve optimal performance when trained and tested in a single domain, unlexicalised parsers can be surprisingly competitive with their lexicalised counterparts (Klein and Manning, 2003; Petrov et al., 2006). In this work, instead</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 167–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>GENIA corpus - a semantically annotated corpus for bio-textmining.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="27496" citStr="Kim et al., 2003" startWordPosition="4499" endWordPosition="4502">ers are attached with much higher accuracy, resulting in 3.34% and 3.15% increase in the corresponding F-scores. The non-clausal modifier relation (ncmod), which is the most frequent label in the dataset, increases by 3.16%. 4.3 Genia One advantage of our reranking framework is that it does not rely on any domain-dependent manually annotated resources. Therefore, we are interested in seeing how it performs on text from a completely different domain and genre. The GENIA-GR dataset (Tateisi et al., 2008) is a collection of 492 sentences taken from biomedical research papers in the GENIA corpus (Kim et al., 2003). The sentences have been manually annotated with dependency-based grammatical relations identical to those output by the RASP parser. However, it does not contain dependencies for all tokens and many multi-word phrases are treated as single units. For example, the tokens ‘intracellular redox status’ are annotated as one node with label intracellular redox status. We retain this annotation and 397 Prec DepBank/GR p Prec GENIA-GR p Rec F Rec F Baseline 77.91 74.97 76.41 32.70 79.91 78.86 79.38 36.54 Upper Bound 86.74 82.82 84.73 75.36 86.33 84.71 85.51 78.66 I 77.77 75.00 76.36 33.32 77.18 76.2</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsujii. 2003. GENIA corpus - a semantically annotated corpus for bio-textmining. Bioinformatics, 19(1):180–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Stefan Riezler</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
</authors>
<title>The PARC 700 dependency bank.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL03: 4th International Workshop on Linguistically Interpreted Corpora (LINC-03),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="22846" citStr="King et al., 2003" startWordPosition="3749" endWordPosition="3752">y graphs. For this we make use of an oracle that creates the perfect ranking for a set of graphs by calculating their individual F-scores; this ideal ranking is then compared to the output of our system. Spearman’s rank correlation coefficient between the two rankings is calculated for each sentence and then averaged over all sentences. If the scores for all of the returned analyses are equal, this coefficient cannot be calculated and is set to 0. 4.2 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation (Briscoe and Carroll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3ilexir.co.uk/2012/open-source-rasp-3-1/ NScore(n) = EeEEg isDep(e, n) GraphScore(g) = |N9| E eEEg EdgeScore(e) x isDep(e, n) EnENg NScore(n) 396 C&amp;C (Clark and Curran, 2007). It contains 700 sentences, randomly ch</context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700 dependency bank. In Proceedings of the EACL03: 4th International Workshop on Linguistically Interpreted Corpora (LINC-03), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, number July,</booktitle>
<pages>423--430</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2378" citStr="Klein and Manning, 2003" startWordPosition="349" endWordPosition="352"> text from different domains and genres (Sekine, 1997; Gildea, 2001). Furthermore, manual creation of in-domain treebanks is an expensive and timeconsuming process, which can only be performed by experts with sufficient linguistic and domain knowledge. In contrast, unlexicalised parsers avoid using lexical information and select a syntactic analysis using only more general features, such as POS tags. While they cannot be expected to achieve optimal performance when trained and tested in a single domain, unlexicalised parsers can be surprisingly competitive with their lexicalised counterparts (Klein and Manning, 2003; Petrov et al., 2006). In this work, instead of trying to adapt a lexicalised parser to new domains, we explore how bilexical features can be integrated effectively with any unlexicalised parser. As our novel self-learning framework requires only a large unannotated corpus, lexical features can be easily tuned to a specific domain or genre by selecting a suitable dataset. In addition, we describe a graph expansion process that captures selected bilexical relations which improve performance but would otherwise require sparse higherorder dependency path feature types in most approaches to depen</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, number July, pages 423–430. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<pages>1--22</pages>
<contexts>
<context position="23513" citStr="Marcus et al., 1993" startWordPosition="3853" endWordPosition="3856">RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3ilexir.co.uk/2012/open-source-rasp-3-1/ NScore(n) = EeEEg isDep(e, n) GraphScore(g) = |N9| E eEEg EdgeScore(e) x isDep(e, n) EnENg NScore(n) 396 C&amp;C (Clark and Curran, 2007). It contains 700 sentences, randomly chosen from section 23 of the WSJ Penn Treebank (Marcus et al., 1993), divided into development (140 sentences) and test data (560 sentences). We made use of the development data to experiment with a wider selection of edge and graph scoring methods, and report the final results on the test data. For reranking we collect up to 1000 top-ranked analyses for each sentence. The actual number of analyses that the RASP parser outputs depends on the sentence and can be smaller. As the parser first constructs parse trees and converts them to dependency graphs, several parse trees may result in identical graphs; we remove any duplicates to obtain a ranking of unique dep</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, pages 1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, number June,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6433" citStr="McClosky et al. (2006)" startWordPosition="970" endWordPosition="973"> our self-learning framework as a reranking process that assigns new scores to the topn ranked analyses found by the original parser. Parse reranking has been successfully used in previous work as a method of including a wider range of features to rescore a smaller selection of highly-ranked candidate parses. Collins (2000) was one of the first to propose supervised reranking as an additional step to increase parser accuracy and achieved 1.55% accuracy improvement for his parser. Charniak and Johnson (2005) utilise a discriminative reranker and show a 1.3% improvement for the Charniak parser. McClosky et al. (2006) extend their work by adding new features and further increase the performance by 0.3%. Ng et al. (2010) implemented a discriminative maximum entropy reranker for the C&amp;C parser and showed a 0.23% improvement over the baseline. Bansal and Klein (2011) discriminatively rerank derivations from the Berkeley unlexicalised parser (Petrov et al., 2006) demonstrating that lexical features derived from the Google n-gram corpus improve accuracy even when used in conjunction with other reranking features. They have all treated reranking as a supervised task and trained a discriminative classifier using </context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, number June, pages 152– 159, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominick Ng</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Reranking a wide-coverage CCG parser.</title>
<date>2010</date>
<booktitle>In Australasian Language Technology Association Workshop 2010,</booktitle>
<pages>90</pages>
<contexts>
<context position="6537" citStr="Ng et al. (2010)" startWordPosition="988" endWordPosition="991">by the original parser. Parse reranking has been successfully used in previous work as a method of including a wider range of features to rescore a smaller selection of highly-ranked candidate parses. Collins (2000) was one of the first to propose supervised reranking as an additional step to increase parser accuracy and achieved 1.55% accuracy improvement for his parser. Charniak and Johnson (2005) utilise a discriminative reranker and show a 1.3% improvement for the Charniak parser. McClosky et al. (2006) extend their work by adding new features and further increase the performance by 0.3%. Ng et al. (2010) implemented a discriminative maximum entropy reranker for the C&amp;C parser and showed a 0.23% improvement over the baseline. Bansal and Klein (2011) discriminatively rerank derivations from the Berkeley unlexicalised parser (Petrov et al., 2006) demonstrating that lexical features derived from the Google n-gram corpus improve accuracy even when used in conjunction with other reranking features. They have all treated reranking as a supervised task and trained a discriminative classifier using parse tree features and annotated in-domain data. In contrast, our reranker only uses statistics from an</context>
</contexts>
<marker>Ng, Honnibal, Curran, 2010</marker>
<rawString>Dominick Ng, Matthew Honnibal, and James R. Curran. 2010. Reranking a wide-coverage CCG parser. In Australasian Language Technology Association Workshop 2010, page 90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="22100" citStr="Noreen, 1989" startWordPosition="3624" endWordPosition="3625">Evaluation methods In order to evaluate how much the reranker improves the highest-ranked dependency graph, we calculate the microaveraged precision, recall and F-score over all dependencies from the top-ranking parses for the test set. Following the official RASP evaluation (Briscoe et al., 2006) we employ the hierarchical edge matching scheme which aggregates counts up the dependency relation subsumption hierarchy and thus rewards the parser for making more finegrained distinctions.2 Statistical significance of the change in F-score is calculated by using the Approximate Randomisation Test (Noreen, 1989; Cohen, 1995) with 106 iterations. We also wish to measure how well the reranker does at the overall task of ordering dependency graphs. For this we make use of an oracle that creates the perfect ranking for a set of graphs by calculating their individual F-scores; this ideal ranking is then compared to the output of our system. Spearman’s rank correlation coefficient between the two rankings is calculated for each sentence and then averaged over all sentences. If the scores for all of the returned analyses are equal, this coefficient cannot be calculated and is set to 0. 4.2 DepBank We evalu</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses: An Introduction. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of theACL (ACL ’06),</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2400" citStr="Petrov et al., 2006" startWordPosition="353" endWordPosition="356">ins and genres (Sekine, 1997; Gildea, 2001). Furthermore, manual creation of in-domain treebanks is an expensive and timeconsuming process, which can only be performed by experts with sufficient linguistic and domain knowledge. In contrast, unlexicalised parsers avoid using lexical information and select a syntactic analysis using only more general features, such as POS tags. While they cannot be expected to achieve optimal performance when trained and tested in a single domain, unlexicalised parsers can be surprisingly competitive with their lexicalised counterparts (Klein and Manning, 2003; Petrov et al., 2006). In this work, instead of trying to adapt a lexicalised parser to new domains, we explore how bilexical features can be integrated effectively with any unlexicalised parser. As our novel self-learning framework requires only a large unannotated corpus, lexical features can be easily tuned to a specific domain or genre by selecting a suitable dataset. In addition, we describe a graph expansion process that captures selected bilexical relations which improve performance but would otherwise require sparse higherorder dependency path feature types in most approaches to dependency parsing. As many</context>
<context position="6781" citStr="Petrov et al., 2006" startWordPosition="1023" endWordPosition="1026">propose supervised reranking as an additional step to increase parser accuracy and achieved 1.55% accuracy improvement for his parser. Charniak and Johnson (2005) utilise a discriminative reranker and show a 1.3% improvement for the Charniak parser. McClosky et al. (2006) extend their work by adding new features and further increase the performance by 0.3%. Ng et al. (2010) implemented a discriminative maximum entropy reranker for the C&amp;C parser and showed a 0.23% improvement over the baseline. Bansal and Klein (2011) discriminatively rerank derivations from the Berkeley unlexicalised parser (Petrov et al., 2006) demonstrating that lexical features derived from the Google n-gram corpus improve accuracy even when used in conjunction with other reranking features. They have all treated reranking as a supervised task and trained a discriminative classifier using parse tree features and annotated in-domain data. In contrast, our reranker only uses statistics from an unlabelled source and requires no manual annotation or training of the reranking component. As we utilise an unlexicalised parser, our baseline performance on WSJ text is lower compared to some fully-lexicalised parsers. However, an unlexicali</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of theACL (ACL ’06), pages 433–440, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Gertjan van Noord</author>
</authors>
<title>Exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the Workshop on CrossFramework and Cross- Domain Parser Evaluation,</booktitle>
<pages>9--16</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Manchester, UK.</location>
<marker>Plank, van Noord, 2008</marker>
<rawString>Barbara Plank and Gertjan van Noord. 2008. Exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model. In Coling 2008: Proceedings of the Workshop on CrossFramework and Cross- Domain Parser Evaluation, pages 9–16, Manchester, UK. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marek Rei</author>
</authors>
<title>Minimally supervised dependencybased methods for natural language processing.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="18453" citStr="Rei, 2013" startWordPosition="3022" endWordPosition="3023">re 2. C1 is the list of substitute lemmas for w1, and sim(c1, w1) is a measure showing how similar c1 is to w1. The methods iterate over the list of substitutes and calculate the CES score for each of the modified relations. The values are then combined by using the similarity score as a weight – more similar lemmas will have a higher contribution to the final result. This is done for both the head and the dependent in the original relation, and the scores are then normalised and averaged. Experiments with a wide variety of distributional word similarity measures revealed that WeightedCosine (Rei, 2013), a directional similarity measure designed to better capture hyponymy relations, performed best. Hyponyms are more specific versions of a word and normally include the general properties of the hypernym, making them well-suited for lexical substitution. The WeightedCosine measure incorporates an additional directional weight into the standard cosine similarity, assigning different importance to individual features for the hyponymy relation. We retain the 10 most distributionally similar putative hyponyms for each lemma and substitute them in the relation. The original lemma is also included w</context>
</contexts>
<marker>Rei, 2013</marker>
<rawString>Marek Rei. 2013. Minimally supervised dependencybased methods for natural language processing. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>The domain dependence of parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied natural language processing,</booktitle>
<volume>1</volume>
<pages>96--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1808" citStr="Sekine, 1997" startWordPosition="265" endWordPosition="266"> treebank), incorporating a wide range of features in the training process to deliver competitive performance. The use of lexically-conditioned features, such as relations between lemmas or word forms, is often critical when choosing the correct syntactic analysis in ambiguous contexts. However, utilising such features leads the parser to learn information that is often specific to the domain and/or genre of the training data. Several experiments have demonstrated that many lexical features learnt in one domain provide little if any benefit when parsing text from different domains and genres (Sekine, 1997; Gildea, 2001). Furthermore, manual creation of in-domain treebanks is an expensive and timeconsuming process, which can only be performed by experts with sufficient linguistic and domain knowledge. In contrast, unlexicalised parsers avoid using lexical information and select a syntactic analysis using only more general features, such as POS tags. While they cannot be expected to achieve optimal performance when trained and tested in a single domain, unlexicalised parsers can be surprisingly competitive with their lexicalised counterparts (Klein and Manning, 2003; Petrov et al., 2006). In thi</context>
</contexts>
<marker>Sekine, 1997</marker>
<rawString>Satoshi Sekine. 1997. The domain dependence of parsing. In Proceedings of the fifth conference on Applied natural language processing, volume 1, pages 96–102, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuka Tateisi</author>
<author>Yusuke Miyao</author>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>GENIA-GR: a Grammatical Relation Corpus for Parser Evaluation in the Biomedical Domain.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1942--1948</pages>
<contexts>
<context position="27386" citStr="Tateisi et al., 2008" startWordPosition="4479" endWordPosition="4482">reranking process was able to improve the F-score of all other individual dependency types. Complements and modifiers are attached with much higher accuracy, resulting in 3.34% and 3.15% increase in the corresponding F-scores. The non-clausal modifier relation (ncmod), which is the most frequent label in the dataset, increases by 3.16%. 4.3 Genia One advantage of our reranking framework is that it does not rely on any domain-dependent manually annotated resources. Therefore, we are interested in seeing how it performs on text from a completely different domain and genre. The GENIA-GR dataset (Tateisi et al., 2008) is a collection of 492 sentences taken from biomedical research papers in the GENIA corpus (Kim et al., 2003). The sentences have been manually annotated with dependency-based grammatical relations identical to those output by the RASP parser. However, it does not contain dependencies for all tokens and many multi-word phrases are treated as single units. For example, the tokens ‘intracellular redox status’ are annotated as one node with label intracellular redox status. We retain this annotation and 397 Prec DepBank/GR p Prec GENIA-GR p Rec F Rec F Baseline 77.91 74.97 76.41 32.70 79.91 78.8</context>
</contexts>
<marker>Tateisi, Miyao, Sagae, Tsujii, 2008</marker>
<rawString>Yuka Tateisi, Yusuke Miyao, Kenji Sagae, and Jun’ichi Tsujii. 2008. GENIA-GR: a Grammatical Relation Corpus for Parser Evaluation in the Biomedical Domain. In Proceedings of LREC, pages 1942–1948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Using self-trained bilexical preferences to improve disambiguation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technologies, number June,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>van Noord, 2007</marker>
<rawString>Gertjan van Noord. 2007. Using self-trained bilexical preferences to improve disambiguation accuracy. In Proceedings of the 10th International Conference on Parsing Technologies, number June, pages 1–10, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Watson</author>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Semi-supervised training of a statistical parser from unlabeled partially-bracketed data.</title>
<date>2007</date>
<booktitle>Proceedings of the 10th International Conference on Parsing Technologies - IWPT ’07,</booktitle>
<location>(June):23–32.</location>
<contexts>
<context position="23027" citStr="Watson et al., 2007" startWordPosition="3776" endWordPosition="3779">he output of our system. Spearman’s rank correlation coefficient between the two rankings is calculated for each sentence and then averaged over all sentences. If the scores for all of the returned analyses are equal, this coefficient cannot be calculated and is set to 0. 4.2 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation (Briscoe and Carroll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3ilexir.co.uk/2012/open-source-rasp-3-1/ NScore(n) = EeEEg isDep(e, n) GraphScore(g) = |N9| E eEEg EdgeScore(e) x isDep(e, n) EnENg NScore(n) 396 C&amp;C (Clark and Curran, 2007). It contains 700 sentences, randomly chosen from section 23 of the WSJ Penn Treebank (Marcus et al., 1993), divided into development (140 sentences) and test data (560 sentences). We made use of the development data to e</context>
<context position="26308" citStr="Watson et al., 2007" startWordPosition="4308" endWordPosition="4311">ly significant when compared to the baseline (p &lt; 0.05). By using our self-learning framework, we were able to significantly improve the original unlexicalised parser. To put the overall result in a wider perspective, Clark and Curran (2007) achieve an F-score of 81.86% on the DepBank/GR test sentences using the C&amp;C lexicalised parser, trained on 40,000 manually-treebanked sentences from the WSJ. The unlexicalised RASP parser, using a manually-developed grammar and a parse ranking component trained on 4,000 partially-bracketed unlabelled sentences from a domain/genre balanced subset of Brown (Watson et al., 2007), achieves an F-score of 76.41% on the same test set. The method introduced here improves this to 79.21% F-score without using any further manually-annotated data, closing more than half of the gap between the performance of a fully-supervised in-domain parser and a more weakly-supervised more domain-neutral one. We also performed an additional detailed analysis of the results and found that, with the exception of the auxiliary dependency relation, the reranking process was able to improve the F-score of all other individual dependency types. Complements and modifiers are attached with much hi</context>
</contexts>
<marker>Watson, Briscoe, Carroll, 2007</marker>
<rawString>Rebecca Watson, Ted Briscoe, and John Carroll. 2007. Semi-supervised training of a statistical parser from unlabeled partially-bracketed data. Proceedings of the 10th International Conference on Parsing Technologies - IWPT ’07, (June):23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Li Cai</author>
</authors>
<title>Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1556--1565</pages>
<contexts>
<context position="5425" citStr="Zhou et al. (2011)" startWordPosition="810" endWordPosition="813">parser accuracy. However, previous work has focused on including these statistics as auxiliary features during supervised training. For example, van Noord (2007) incorporated bilexical preferences as features via self-training to improve the Alpino parser for Dutch. Plank and van Noord (2008) investigated the application of auxiliary distributions for domain adaptation. They incorporated information from both in-domain and out-of-domain sources into their maximum entropy model and found that the out-of-domain auxiliary distributions did not contribute to parsing accuracy in the target domain. Zhou et al. (2011) extracted ngram counts from Google queries and a large corpus to improve the MSTParser. In contrast to previous work, we refer to our approach as self-learning because it differs from self-training by utilising statistics found using an initial parse ranking model to create a separate unsupervised reranking component, without retraining the baseline unlexicalised model. We formulate our self-learning framework as a reranking process that assigns new scores to the topn ranked analyses found by the original parser. Parse reranking has been successfully used in previous work as a method of inclu</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing. In 49th Annual Meeting of the Association for Computational Linguistics, pages 1556–1565.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>