<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000146">
<title confidence="0.962943">
Minimum Bayes-risk System Combination
</title>
<author confidence="0.650673">
Jes´us Gonz´alez-Rubio
</author>
<note confidence="0.552466666666667">
Instituto Tecnol´ogico de Inform´atica
U. Polit`ecnica de Val`encia
46022 Valencia, Spain
</note>
<email confidence="0.982445">
jegonzalez@iti.upv.es
</email>
<note confidence="0.3704445">
Alfons Juan Francisco Casacuberta
D. de Sistemas Inform´aticos y Computaci´on
U. Polit`ecnica de Val`encia
46022 Valencia, Spain
</note>
<email confidence="0.994215">
{ajuan,fcn}@dsic.upv.es
</email>
<sectionHeader confidence="0.993746" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998211565217391">
We present minimum Bayes-risk system com-
bination, a method that integrates consen-
sus decoding and system combination into
a unified multi-system minimum Bayes-risk
(MBR) technique. Unlike other MBR meth-
ods that re-rank translations of a single SMT
system, MBR system combination uses the
MBR decision rule and a linear combina-
tion of the component systems’ probability
distributions to search for the minimum risk
translation among all the finite-length strings
over the output vocabulary. We introduce ex-
pected BLEU, an approximation to the BLEU
score that allows to efficiently apply MBR in
these conditions. MBR system combination is
a general method that is independent of spe-
cific SMT models, enabling us to combine
systems with heterogeneous structure. Exper-
iments show that our approach bring sig-
nificant improvements to single-system-based
MBR decoding and achieves comparable re-
sults to different state-of-the-art system com-
bination methods.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999853510204082">
Once statistical models are trained, a decoding ap-
proach determines what translations are finally se-
lected. Two parallel lines of research have shown
consistent improvements over the max–derivation
decoding objective, which selects the highest prob-
ability derivation. Consensus decoding procedures
select translations for a single system with a mini-
mum Bayes risk (MBR) (Kumar and Byrne, 2004).
System combination procedures, on the other hand,
generate translations from the output of multiple
component systems by combining the best frag-
ments of these outputs (Frederking and Nirenburg,
1994). In this paper, we present minimum Bayes
risk system combination, a technique that unifies
these two approaches by learning a consensus trans-
lation over multiple underlying component systems.
MBR system combination operates directly on the
outputs of the component models. We perform an
MBR decoding using a linear combination of the
component models’ probability distributions. In-
stead of re-ranking the translations provided by the
component systems, we search for the hypothesis
with the minimum expected translation error among
all the possible finite-length strings in the target lan-
guage. By using a loss function based on BLEU (Pa-
pineni et al., 2002), we avoid the hypothesis align-
ment problem that is central to standard system com-
bination approaches (Rosti et al., 2007). MBR sys-
tem combination assumes only that each translation
model can produce expectations of n-gram counts;
the latent derivation structures of the component sys-
tems can differ arbitrary. This flexibility allows us to
combine a great variety of SMT systems.
The key contributions of this paper are three: the
usage of a linear combination of distributions within
the MBR decoding, which allows multiple SMT
models to be involved in, and makes the computa-
tion of n-grams statistics to be more accurate; the
decoding in an extended search space, which allows
to find better hypotheses than the evidences pro-
vided by the component models; and the use of an
expected BLEU score instead of the sentence-wise
BLEU, which allows to efficiently apply MBR de-
coding in the huge search space under consideration.
We evaluate in a multi-source translation task ob-
taining improvements of up to +2.0 BLEU abs. over
the best single system max-derivation, and state-of-
the-art performance in the system combination task
of the ACL 2010 workshop on SMT.
</bodyText>
<page confidence="0.93333">
1268
</page>
<note confidence="0.9843275">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1268–1277,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997173" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9992678">
MBR system combination is a multi-system gener-
alization of MBR decoding where the space of hy-
potheses is not constrained to the space of evidences.
We expand the space of hypotheses following some
underlying ideas of system combination techniques.
</bodyText>
<subsectionHeader confidence="0.965057">
2.1 Minimum Bayes risk
</subsectionHeader>
<bodyText confidence="0.999977930232558">
In SMT, MBR decoding allows to minimize the
loss of the output for a single translation system.
MBR is generally implemented by re-ranking an N-
best list of translations produced by a first pass de-
coder (Kumar and Byrne, 2004). Different tech-
niques to widen the search space have been de-
scribed (Tromble et al., 2008; DeNero et al., 2009;
Kumar et al., 2009; Li et al., 2009). These works
extend the traditional MBR algorithms based on N-
best lists to work with lattices.
The use of MBR to combine the outputs of vari-
ous MT systems has also been explored previously.
Duan et al. (2010) present an MBR decoding that
makes use of a mixture of different SMT systems to
improve translation accuracy. Our technique differs
in that we use a linear combination instead of a mix-
ture, which avoids the problem of component sys-
tems not sharing the same search space; perform the
decoding in a search space larger than the outputs
of the component models; and optimize an expected
BLEU score instead of the linear approximation to
it described in (Tromble et al., 2008).
DeNero et al. (2010) present model combination,
a multi-system lattice MBR decoding on the con-
joined evidences spaces of the component systems.
Our technique differs in that we perform the search
in an extended search space not restricted to the pro-
vided evidences, have fewer parameters to learn, and
optimizes an expected BLEU score instead of the
linear BLEU approximation.
Another MBR-related technique to combine the
outputs of various MT systems was presented by
Gonz´alez-Rubio and Casacuberta (2010). They use
different median string (Fu, 1982) algorithms to
combine various machine translation systems. Our
approach differs in that we take into account the pos-
terior distribution over translations instead of con-
sidering each translation equally likely, optimize the
expected BLEU score instead of a sentence-wise
measure such as the edit distance or the sentence-
level BLEU, and take into account the quality dif-
ferences by associating a tunable scaling factor to
each system.
</bodyText>
<subsectionHeader confidence="0.998747">
2.2 System Combination
</subsectionHeader>
<bodyText confidence="0.999935518518519">
System combination techniques in MT take as in-
put the outputs lei, · · · , eN} of N translation sys-
tems, where e, is a structured translation object
(or N-best lists thereof), typically viewed as a se-
quence of words. The dominant approach in the
field chooses a primary translation ep as a backbone,
then finds an alignment a,,, to the backbone for each
e,,,. A new search space is constructed from these
backbone-aligned outputs and then a voting proce-
dure of feature-based model predicts a final consen-
sus translation (Rosti et al., 2007). MBR system
combination entirely avoids this alignment prob-
lem by considering hypotheses as n-gram occur-
rence vectors rather than word sequences. MBR sys-
tem combination performs the decoding in a larger
search space and includes statistics from the compo-
nents’ posteriors, whereas system combination tech-
niques typically do not.
Despite these advantages, system combination
may be more appropriate in some settings. In par-
ticular, MBR system combination is designed pri-
marily for statistical systems that generate N-best
or lattice outputs. MBR system combination can in-
tegrate non-statistical systems that generate either a
single or an unweighted output. However, we would
not expect the same strong performance from MBR
system combination in these constrained settings.
</bodyText>
<sectionHeader confidence="0.986917" genericHeader="method">
3 Minimum Bayes risk Decoding
</sectionHeader>
<bodyText confidence="0.99815225">
MBR decoding aims to find the candidate hypothesis
that has the least expected loss under a probability
model (Bickel and Doksum, 1977). We begin with a
review of MBR for SMT.
SMT can be described as a mapping of a word se-
quence f in a source language to a word sequence
e in a target language; this mapping is produced by
the MT decoder D(f). If the reference translation
e is known, the decoder performance can be mea-
sured by the loss function L(e, D(f)). Given such a
loss function L(e, e&apos;) between an automatic transla-
tion e&apos; and a reference e, and an underlying proba-
</bodyText>
<page confidence="0.991304">
1269
</page>
<bodyText confidence="0.983864333333333">
bility model P (e|f), MBR decoding has the follow-
ing form (Goel and Byrne, 2000; Kumar and Byrne,
2004):
</bodyText>
<equation confidence="0.996337">
e� = arg min R(e&apos;) (1)
e&apos;EE
</equation>
<bodyText confidence="0.9882816">
combination of the Bayes gains of the components.
Each system provides its own space of evidences
Dn(f) and its posterior distribution over translations
Pn(e|f). Given a sentence f in the source language,
MBR system combination is written as follows:
</bodyText>
<equation confidence="0.997122">
X= arg min P(e|f) · L(e, e&apos;) , (2) e� = arg max G(e&apos;) (6)
e&apos;EE eEE e&apos;EEh
≈ arg max
e&apos;EEh
= arg max
e&apos;EEh
XN
n=1
XN
n=1
</equation>
<bodyText confidence="0.999584153846154">
where R(e&apos;) denotes the Bayes risk of candidate
translation e&apos; under loss function L, and E repre-
sents the space of translations.
If the loss function between any two hypotheses
can be bounded: L(e, e&apos;) ≤ Lmax, the MBR de-
coder can be rewritten in term of a similarity func-
tion S(e, e&apos;) = Lmax − L(e, e&apos;). In this case, in-
stead of minimizing the Bayes risk, we maximize
the Bayes gain G(e&apos;):
MBR decoding can use different spaces for hy-
pothesis selection and gain computation (arg max
and summatory in Eq. (4)). Therefore, the MBR de-
coder can be more generally written as follows:
</bodyText>
<equation confidence="0.976873333333333">
X
e� = arg max P(e|f) · S(e, e&apos;) , (5)
e&apos;EEh eEE,
</equation>
<bodyText confidence="0.999994166666667">
where Eh refers to the hypotheses space form where
the translations are chosen and Ee refers to the evi-
dences space that is used to compute the Bayes gain.
We will investigate the expansion of the hypotheses
space while keeping the evidences space as provided
by the decoder.
</bodyText>
<sectionHeader confidence="0.986485" genericHeader="method">
4 MBR System Combination
</sectionHeader>
<bodyText confidence="0.9996826">
MBR system combination is a multi-system gener-
alization of MBR decoding. It uses the MBR de-
cision rule on a linear combination of the probabil-
ity distributions of the component systems. Unlike
existing MBR decoding methods that re-rank trans-
lation outputs, MBR system combination search for
the minimum risk hypotheses on the complete set of
finite-length hypotheses over the output vocabulary.
We assume the component systems to be statistically
independent and define the Bayes gain as a linear
</bodyText>
<equation confidence="0.995338333333333">
an · Gn(e&apos;) (7)
X�n · Pn(e|f) · S(e, e&apos;) , (8)
eEDn(f)
</equation>
<bodyText confidence="0.996993">
where N is the total number of component systems,
Eh represents the hypotheses space where the search
is performed, Gn(e&apos;) is the Bayes gain of hypothe-
sis e&apos; given by the nth component system and an is
a scaling factor introduced to take into account the
differences in quality of the component models. It is
worth mentioning that by using a linear combination
instead of a mixture model, we avoid the problem
of component systems not sharing the same search
space (Duan et al., 2010).
MBR system combination parameters training
and decoding in the extended hypotheses space are
described below.
</bodyText>
<subsectionHeader confidence="0.999243">
4.1 Model Training
</subsectionHeader>
<bodyText confidence="0.9999669">
We learn the scaling factors in Eq. (8) using min-
imum error rate training (MERT) (Och, 2003).
MERT maximizes the translation quality of e� on a
held-out set, according to an evaluation metric that
compares to a reference set. We used BLEU, choos-
ing the scaling factors to maximize BLEU score
of the set of translations predicted by MBR sys-
tem combination. We perform the maximization by
means of the down-hill simplex algorithm (Nelder
and Mead, 1965).
</bodyText>
<subsectionHeader confidence="0.99807">
4.2 Model Decoding
</subsectionHeader>
<bodyText confidence="0.999988142857143">
In most MBR algorithms, the hypotheses space is
equal to the evidences space. Following the underly-
ing idea of system combination, we are interested in
extend the hypotheses space by including new sen-
tences created using fragments of the hypotheses in
the evidences spaces of the component models. We
perform the search (argmax operation in Eq. (8))
</bodyText>
<equation confidence="0.99826375">
e� = arg max G(e&apos;) (3)
e&apos;EE
X= arg max P(e|f) · S(e, e&apos;) . (4)
e&apos;EE eEE
</equation>
<page confidence="0.778515">
1270
</page>
<listItem confidence="0.926018181818182">
Algorithm 1 MBR system combination decoding.
Require: Initial hypothesis e
Require: Vocabulary the evidences E
1: e�← e
2: repeat
3: ecur ← e�
4: for j = 1 to |ecur |do
5: es ← ecur
6: for a ∈ Edo
7: e&apos;s ← 5ubstitute(ecur, a, j)
8: if G(e&apos;s) &gt; G(6s) then
9: es ← e&apos; s
10: ed ← Delete(ecur, j)
11: 6i ← ecur
12: for a ∈ Edo
13: e&apos;i ← Insert(ecur, a, j)
14: if G(e&apos;i) &gt; G(6i) then
15: 6i ← e&apos;i
16: e� ← arg maxe&apos;E{ec�r,ˆe3,ˆed,ˆe�� G(e&apos;)
17: until G(6) &gt;6 G(ecur)
18: return ecur
Ensure: G(ecur) ≥ G(e)
</listItem>
<bodyText confidence="0.9998462">
The complexity of the main loop (lines 2-17) is
O(|ecur |· |E |· Cg), where Cg is the cost of com-
puting the gain of a hypothesis, and usually only a
moderate number of iterations (&lt; 10) is needed to
converge (Mart´ınez et al., 2000).
</bodyText>
<sectionHeader confidence="0.632393" genericHeader="method">
5 Computing BLEU-based Gain
</sectionHeader>
<bodyText confidence="0.999979857142857">
We are interested in performing MBR system com-
bination under BLEU. BLEU behaves as a score
function: its value ranges between 0 and 1 and a
larger value reflects a higher similarity. Therefore,
we rewrite the gain function G(·) using single evi-
dence (or reference) BLEU (Papineni et al., 2002)
as the similarity function:
</bodyText>
<equation confidence="0.9985918">
Gn(e&apos;) = � Pn(e|f) · BLEU(e, e&apos;) (9)
eEDn(f)
BLEU = ri4 \mk / 1 · min (e1− c ,1.0) , (10)
ck 4
k=1
</equation>
<bodyText confidence="0.999795063829787">
using the approximate median string (AMS) algo-
rithm (Mart´ınez et al., 2000). AMS algorithm per-
form a search on a hypotheses space equal to the
free monoid E* of the vocabulary of the evidences
E = Voc(Ee).
The AMS algorithm is shown in Algorithm 1.
AMS starts with an initial hypothesis e that is mod-
ified using edit operations until there is no improve-
ment in the Bayes gain (Lines 3–16). On each posi-
tion j of the current solution ecur, we apply all the
possible single edit operations: substitution of the
jth word of ecur by each word a in the vocabulary
(Lines 5–9), deletion of the jth word of ecur (Line
10) and insertion of each word a in the vocabulary in
the jth position of ecur (Lines 11–15). If the Bayes
gain of any of the new edited hypotheses is higher
than the Bayes gain of the current hypothesis (Line
17), we repeat the loop with this new hypotheses 6,
in other case, we return the current hypothesis.
AMS algorithm takes as input an initial hypothe-
sis e and the combined vocabulary of the evidences
spaces E. Its output is a possibly new hypothesis
whose Bayes gain is assured to be higher or equal
than the Bayes gain of the initial hypothesis.
where r is the length of the evidence, c the length of
the hypothesis, mk the number of n-gram matches
of size k, and ck the count of n-grams of size k in
the hypothesis.
The evidences space Dn(f) may contain a huge
number of hypotheses1 which often make impracti-
cal to compute Eq. (9) directly. To avoid this prob-
lem, Tromble et al. (2008) propose linear BLEU, an
approximation to the BLEU score to efficiently per-
form MBR decoding when the search space is repre-
sented with lattices. However, our hypotheses space
is the full set of finite-length strings in the target vo-
cabulary and can not be represented in a lattice.
In Eq. (9), we have one hypothesis e&apos; that is to be
compared to a set of evidences e ∈ Dn(f) which
follow a probability distribution Pn(e|f). Instead
of computing the expected BLEU score by calcu-
lating the BLEU score with respect to each of the
evidences, our approach will be to use the expected
n-gram counts and sentence length of the evidences
to compute a single-reference BLEU score. We re-
place the reference statistics (r and mn in Eq. (10))
by the expected statistics (r&apos; and m&apos;n) given the pos-
</bodyText>
<footnote confidence="0.995627">
1For example, in a lattice the number of hypotheses may be
exponential in the size of its state set.
</footnote>
<page confidence="0.785446">
1271
</page>
<table confidence="0.994856214285714">
terior distribution Pn(e|f) over the evidences:
9n(e&apos;) = �� (m 1
kk / 4 min (e1−rc ,1.0) (11)
k=1
System dev TER test TER
BLEU BLEU
MAX 25.3 60.5 25.6* 60.3
de�en 25.1 60.7 25.4* 60.5
MBR
MAX 30.9* 53.3* 30.4* 53.9*
es-*en MBR 31.0* 53.4* 30.4* 54.0*
MAX 30.7* 53.9* 30.8* 53.4*
fr�en 30.7* 53.8* 30.9* 53.4*
MBR
</table>
<equation confidence="0.997708833333333">
Er&apos; = |e |· Pn(e|f) (12)
eEDn(f)
Em&apos;k = min(Cep(ng), C&apos;(ng)) (13) Table 1: Performance of base systems.
ngEArk(ep)
C&apos;(ng) = E Ce(ng) · Pn(e|f) , (14)
eEDn(f)
</equation>
<bodyText confidence="0.999225">
where Nk(e&apos;) is the set of n-grams of size k in the
hypothesis, Cep(ng) is the count of the n-gram ng in
the hypothesis and C&apos;(ng) is the expected count of
ng in the evidences. To compute the n-gram match-
ings m&apos;k, the count of each n-gram is truncated, if
necessary, to not exceed the expected count for that
n-gram in the evidences.
We have replaced a summation over a possibly ex-
ponential number of items (e&apos; E Dn(f) in Eq. (9))
with a summation over a polynomial number of n-
grams that occur in the evidences2. Both, the ex-
pected length of the evidences r&apos; and their expected
n-gram counts m&apos;k can be pre-computed efficiently
from N-best lists and translation lattices (Kumar et
al., 2009; DeNero et al., 2010).
</bodyText>
<sectionHeader confidence="0.999716" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999309">
We report results on a multi-source translation
task. From the Europarl corpus released for the
ACL 2006 workshop on MT (WMT2006), we se-
lect those sentence pairs from the German–English
(de–en), Spanish–English (es–en) and French–
English (fr–en) sub-corpora that share the same En-
glish translation. We obtain a multi-source corpus
with German, Spanish and French as source lan-
guages and English as target language. All the ex-
periments were carried out with the lowercased and
tokenized version of this corpus.
We report results using BLEU (Papineni et al.,
2002) and translation edit rate (Snover et al., 2006)
(TER). We measure statistical significance using
</bodyText>
<footnote confidence="0.998648">
2If Dn(f) is represented by a lattice, the number of n-grams is
polynomial in the number of edges in the lattice.
</footnote>
<table confidence="0.9996922">
Approach dev TER test TER
BLEU BLEU
Best MAX 30.9* 53.3* 30.8* 53.4*
Best MBR 31.0* 53.4* 30.9* 53.4*
MBR-SC 32.3 52.5 32.8 52.3
</table>
<tableCaption confidence="0.9832286">
Table 2: Performance from best single system max-
derivation decoding (Best MAX), the best single system
minimum Bayes risk decoding (Best MBR) and mini-
mum Bayes risk system combination (MBR-SC) combin-
ing three systems.
</tableCaption>
<bodyText confidence="0.9980376">
95% confidence intervals computed using paired
bootstrap re-sampling (Zhang and Vogel, 2004). In
all table cells (except for Table 3) systems without
statistically significant differences are marked with
the same superscript.
</bodyText>
<subsectionHeader confidence="0.999819">
6.1 Base Systems
</subsectionHeader>
<bodyText confidence="0.999978666666667">
We combine outputs from three systems, each one
translating from one source language (German,
Spanish or French) into English. Each individual
system is a phrase-based system trained using the
Moses toolkit (Koehn et al., 2007). The parame-
ters of the systems were tuned using MERT (Och,
2003) to optimize BLEU on the development set.
Each base system yields state-of-the-art perfor-
mance, summarized in Table 1. For each system,
we report the performance of max-derivation decod-
ing (MAX) and 1000-best3 MBR decoding (Kumar
and Byrne, 2004).
</bodyText>
<subsectionHeader confidence="0.997675">
6.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.98291">
Table 2 compares MBR system combination (MBR-
SC) to the best MAX and MBR systems. Both Best
</bodyText>
<footnote confidence="0.7177915">
3Ehling et al. (2007) studied up to 10000-best and show that the
use of 1000-best candidates is sufficient for MBR decoding.
</footnote>
<page confidence="0.929344">
1272
</page>
<table confidence="0.999841125">
Setup BLEU TER
Best MBR 30.9 53.4
MBR-SC Expected 30.9 53.5
MBR-SC E/Conjoin 32.4 52.1
MBR-SC E/C/evidences-best 30.9 53.5
MBR-SC E/C/hypotheses-best 31.8 52.5
MBR-SC E/C/Extended 32.7 52.3
MBR-SC E/C/Ex/MERT 32.8 52.3
</table>
<tableCaption confidence="0.9946315">
Table 3: Results on the test set for different setups of
minimum Bayes risk system combination.
</tableCaption>
<bodyText confidence="0.999904807228916">
MBR and MBR-SC were computed on 1000-best
lists. MBR-SC uses expected BLEU as gain func-
tion using the conjoined evidences spaces of the
three systems to compute expected BLEU statistics.
It performs the search in the free monoid of the out-
put vocabulary, and its model parameters were tuned
using MERT on the development set. This is the
standard setup for MBR system combination, and
we refer to it as MBR-SC-E/C/Ex/MERT in Table 3.
MBR system combination improves single Best
MAX system by +2.0 BLEU points in test, and al-
ways improves over MBR. This improvement could
arise due to multiple reasons: the expected BLEU
gain, the larger evidences space, the extended hy-
potheses space, or the MERT tuned scaling factor
values. Table 3 teases apart these contributions.
We first apply MBR-SC to the best system (MBR-
SC-Expected). Best MBR and MBR-SC-Expected
differ only in the gain function: MBR uses sentence
level BLEU while MBR-SC-Expected uses the ex-
pected BLEU gain described in Section 5. MBR-
SC-Expected performance is comparable to MBR
decoding on the 1000-best list from the single best
system. The expected BLEU approximation per-
forms as well as sentence-level BLEU and addition-
ally requires less total computation.
We now extend the evidences space to the con-
joined 1000-best lists (MBR-SC-E/Conjoin). MBR-
SC-E/Conjoin is much better than the best MBR on
a single system. This implies that either the ex-
pected BLEU statistics computed in the conjoined
evidences space are stronger or the larger conjoined
evidences spaces introduce better hypotheses.
When we restrict the BLEU statistics to be com-
puted from only the best system’s evidences space
(MBR-SC-E/C/evidences-best), BLEU scores dra-
matically decrease relative to MBR-SC-E/Conjoin.
This implies that the expected BLEU statistics com-
puted over the conjoined 1000-best lists are stronger
than the corresponding statistics from the single best
system. On the other hand, if we restrict the search
space to only the 1000-best list of the best sys-
tem (MBR-SC-E/C/hypotheses-best), BLEU scores
also decrease relative to MBR-SC-E/Conjoin. This
implies that the conjoined search space also con-
tains better hypotheses than the single best system’s
search space.
These results validate our approach. The linear
combination of the probability distributions in the
conjoined evidences spaces allows to compute much
stronger statistics for the expected BLEU gain and
also contains some better hypotheses than the single
best system’s search space does.
We next expand the conjoined evidences spaces
using the decoding algorithm described in Sec-
tion 4.2 (MBR-SC-E/C/Extended). In this case, the
expected BLEU statistics are computed from the
conjoined 1000-best lists of the three systems, but
the hypotheses space where we perform the decod-
ing is expanded to the set of all possible finite-
length hypotheses over the vocabulary of the evi-
dences. We take the output of MBR-SC-E/Conjoin
as the initial hypotheses of the decoding (see Algo-
rithm 1). MBR-SC-E/C/Extended improves BLEU
score of MBR-SC-E/Conjoin but obtains a slightly
worse TER score. Since these two systems are iden-
tical in their expected BLEU statistics, the improve-
ments in BLEU imply that the extended search space
has introduced better hypotheses. The degradation
in TER performance can be explained by the use of a
BLEU-based gain function in the decoding process.
We finally compute the optimum values for
the scaling factors of the different system us-
ing MERT (MBR-SC-E/C/Ex/MERT). MBR-SC-
E/C/Ex/MERT slightly improves BLEU score of
MBR-SC-E/C/Extended. This implies that the op-
timal values of the scaling factors do not deviate
much from 1.0; a similar result was reported in (Och
and Ney, 2001). We hypothesize that this is because
the three component systems share the same SMT
model, pre-process and decoding. We expect to ob-
tain larger improvements when combining systems
implementing different MT paradigms.
</bodyText>
<page confidence="0.98966">
1273
</page>
<figureCaption confidence="0.997564666666667">
Figure 1: Performance of minimum Bayes risk system
combination (MBR-SC) for different sizes of the evi-
dences space in comparison to other MBR-SC setups.
</figureCaption>
<bodyText confidence="0.976718419354839">
MBR-SC-E/C/Ex/MERT is the standard setup for
MBR system combination and, from now, on we will
refer to it as MBR-SC.
We next evaluate performance of MBR system
combination on N-best lists of increasing sizes, and
compare it to MBR-SC-E/C/Extended and MBR-
SC-E/Conjoin in the same N-best lists. We list the
results of the Best MAX system for comparison.
Results in Figure 1 confirm the conclusions ex-
tracted from results displayed in Table 3. MBR-SC-
Conjoin is consistently better than the Best MAX
system, and differences in BLEU increase with
the size of the evidences space. This implies that
the linear combination of posterior probabilities al-
low to compute stronger statistics for the expected
BLEU gain, and, in addition, the larger the evi-
dences space is, the stronger the computed statistics
are. MBR-SC-C/Extended is also consistently better
than MBR-SC-Conjoin with an almost constant im-
provement of +0.4 BLEU points. This result show
that the extended search space always contains bet-
ter hypotheses than the conjoined evidences spaces;
also confirms the soundness of Algorithm 1 that al-
lows to reach them. Finally, MBR-SC also slightly
improves MBR-SC-C/Extended. The optimization
of the scaling factors allows only small improve-
ments in BLEU.
Figure 2 display the MBR system combination
translation and compare it to the max-derivation
translations of the three component systems. Refer-
ence translation is also listed for comparison. MBR-
</bodyText>
<figure confidence="0.6723322">
MAX de—*en i will return later.
MAX es—*en i shall comeback to that later.
MAX fr—*en i will return to this later.
MBR-SC i will return to this point later.
Reference i will return to this point later.
</figure>
<figureCaption confidence="0.999232">
Figure 2: MBR system combination example.
</figureCaption>
<bodyText confidence="0.99980125">
SC adds word “point” to create a new translation
equal to the reference. MBR-SC is able to detect
that this is valuable word even though it does not
appear in the max-derivation hypotheses.
</bodyText>
<subsectionHeader confidence="0.999778">
6.3 Comparison to System Combination
</subsectionHeader>
<bodyText confidence="0.999496434782609">
Figure 3 compares MBR system combination
(MBR-SC) with state-of-the-art system combination
techniques presented to the system combination task
of the ACL 2010 workshop on MT (WMT2010).
All system combination techniques build a “word
sausage” from the outputs of the different compo-
nent systems and choose a path trough the sausage
with the highest score under different models. A de-
scription of these systems can be found in (Callison-
Burch et al., 2010).
In this task, the output of the component systems
are single hypotheses or unweighted lists thereof.
Therefore, we lack of the statistics of the com-
ponents’ posteriors which is one of the main ad-
vantages of MBR system combination over sys-
tem combination techniques. However, we find that,
even in these constrained setting, MBR system com-
bination performance is similar to the best sys-
tem combination techniques for all translation di-
rections. These experiments validate our approach.
MBR system combination yields state-of-the-art
performance while avoiding the challenge of align-
ing translation hypotheses.
</bodyText>
<sectionHeader confidence="0.993752" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999466571428571">
MBR system combination integrates consensus de-
coding and system combination into a unified multi-
system MBR technique. MBR system combination
uses the MBR decision rule on a linear combina-
tion of the component systems’ probability distri-
butions to search for the sentence with the mini-
mum Bayes risk on the complete set of finite-length
</bodyText>
<figure confidence="0.968978575757576">
33
32.5
32
31.5
31
30.5
100 101 102 103
Number of hypotheses in the N-best lists
BLEU
MBR-SC
MBR-SC C/Extended
MBR-SC Conjoin
Best MAX
1274
BLEU
32
30
28
26
24
22
20
18
16
MBR-SC
BBN
CMU
DCU
JHU
KOC
LIUM
RWTH
cz-en en-cz de-en en-de es-en en-es fr-en en-fr
</figure>
<figureCaption confidence="0.9947875">
Figure 3: Performance of minimum Bayes risk system combination (MBR-SC) for different language directions in
comparison to the rest of system combination techniques presented in the WMT2010 system combination task.
</figureCaption>
<bodyText confidence="0.999923351351352">
strings in the output vocabulary. Component sys-
tems can have varied decoding strategies; we only
require that each system produce an N-best list (or
a lattice) of translations. This flexibility allows the
technique to be applied quite broadly. For instance,
Leusch et al. (2010) generate intermediate transla-
tions in several pivot languages, translate them sep-
arately into the target language, and generate a con-
sensus translation out of these using a system combi-
nation technique. Likewise, these pivot translations
could be combined via MBR system combination.
MBR system combination has two significant ad-
vantages over current approaches to system combi-
nation. First, it does not rely on hypothesis align-
ment between outputs of individual systems. Align-
ing translation hypotheses can be challenging and
has a substantial effect on combination perfor-
mance (He et al., 2008). Instead of aligning the sen-
tences, we view the sentences as vectors of n-gram
counts and compute the expected statistics of the
BLEU score to compute the Bayes gain. Second, we
do not need to pick a backbone system for combina-
tion. Choosing a backbone system can also be chal-
lenging and also affects system combination per-
formance (He and Toutanova, 2009). MBR system
combination sidesteps this issue by working directly
on the conjoined evidences space produced by the
outputs of the component systems, and allows the
consensus model to express system preferences via
scaling factors.
Despite its simplicity, MBR system combination
provides strong performance by leveraging different
consensus, decoding and training techniques. It out-
performs best MAX or MBR derivation on each of
the component systems. In addition, it obtains state-
of-the-art performance in a constrained setting better
suited for dominant system combination techniques.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99984975">
Work supported by the EC (FEDER/FSE) and the
Spanish MEC/MICINN under the MIPRCV “Con-
solider Ingenio 2010” program (CSD2007-00018),
the iTrans2 (TIN2009-14511) project, the UPV
</bodyText>
<page confidence="0.960538">
1275
</page>
<bodyText confidence="0.9990174">
under grant 20091027 and the FPU scholarship
AP2006-00691. Also supported by the Spanish
MITyC under the erudito.com (TSI-020110-2009-
439) project and by the Generalitat Valenciana under
grant Prometeo/2009/014.
</bodyText>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999390112244898">
Peter J. Bickel and Kjell A Doksum. 1977. Mathe-
matical statistics : basic ideas and selected topics.
Holden-Day, San Francisco.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17–53, Morristown, NJ, USA. Association for
Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, pages 567–575,
Morristown, NJ, USA. Association for Computational
Linguistics.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine trans-
lation. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
975–983, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou.
2010. Mixture model-based minimum bayes risk de-
coding using multiple machine translation systems. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 313–
321, Beijing, China, August. Coling 2010 Organizing
Committee.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007.
Minimum bayes risk decoding for bleu. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 101–
104, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the fourth
conference on Applied natural language processing,
pages 95–100, Morristown, NJ, USA. Association for
Computational Linguistics.
K.S. Fu. 1982. Syntactic Pattern Recognition and Appli-
cations. Prentice Hall.
Vaibhava Goel and William J. Byrne. 2000. Minimum
bayes-risk automatic speech recognition. Computer
Speech &amp; Language, 14(2):115–135.
Jes´us Gonz´alez-Rubio and Francisco Casacuberta. 2010.
On the use of median string for multi-source transla-
tion. In In Proceedings of the International Confer-
ence on Pattern Recognition (ICPR2010), pages 4328–
4331.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3
- Volume 3, pages 1202–1211, Morristown, NJ, USA.
Association for Computational Linguistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 98–107, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177–
180, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Shankar Kumar and William J. Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In HLT-NAACL, pages 169–176.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1 - Volume 1,
pages 163–171, Morristown, NJ, USA. Association for
Computational Linguistics.
Gregor Leusch, Aur´elien Max, Josep Maria Crego, and
Hermann Ney. 2010. Multi-pivot translation by sys-
tem combination. In International Workshop on Spo-
ken Language Translation, Paris, France, December.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
</reference>
<page confidence="0.795958">
1276
</page>
<reference confidence="0.999329038461538">
ing of the AFNLP: Volume 2 - Volume 2, pages 593–
601, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
C. D. Martinez, A. Juan, and F. Casacuberta. 2000. Use
of Median String for Classification. In Proceedings of
the 15th International Conference on Pattern Recog-
nition, volume 2, pages 907–910, Barcelona (Spain),
September.
John A. Nelder and Roger Mead. 1965. A Simplex
Method for Function Minimization. The Computer
Journal, 7(4):308–313, January.
Franz Josef Och and Hermann Ney. 2001. Statistical
multi-source translation. In In Machine Translation
Summit, pages 253–258.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 160–167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311–318, Morristown, NJ,
USA. Association for Computational Linguistics.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining outputs from multiple machine
translation systems. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
228–235, Rochester, New York, April. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and Ralph Weischedel. 2006. A study
of translation error rate with targeted human annota-
tion. In In Proceedings of the Association for Machine
Transaltion in the Americas.
Roy W. Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 620–629, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In In Proceedings of the 10th International
Conference on Theoretical and Methodological Issues
in Machine Translation (TMI-2004, pages 4–6.
</reference>
<page confidence="0.992331">
1277
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418759">
<title confidence="0.999561">Minimum Bayes-risk System Combination</title>
<author confidence="0.838503">Jes´us</author>
<affiliation confidence="0.665241">Instituto Tecnol´ogico de U. Polit`ecnica de</affiliation>
<address confidence="0.950309">46022 Valencia,</address>
<email confidence="0.968688">jegonzalez@iti.upv.es</email>
<author confidence="0.999198">Alfons Juan Francisco</author>
<affiliation confidence="0.888521">D. de Sistemas Inform´aticos y U. Polit`ecnica de</affiliation>
<address confidence="0.947137">46022 Valencia,</address>
<abstract confidence="0.99843125">present Bayes-risk system coma method that integrates consensus decoding and system combination into a unified multi-system minimum Bayes-risk (MBR) technique. Unlike other MBR methods that re-rank translations of a single SMT system, MBR system combination uses the MBR decision rule and a linear combination of the component systems’ probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary. We introduce expected BLEU, an approximation to the BLEU score that allows to efficiently apply MBR in these conditions. MBR system combination is a general method that is independent of specific SMT models, enabling us to combine systems with heterogeneous structure. Experiments show that our approach bring significant improvements to single-system-based MBR decoding and achieves comparable results to different state-of-the-art system combination methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter J Bickel</author>
<author>Kjell A Doksum</author>
</authors>
<title>Mathematical statistics : basic ideas and selected topics. Holden-Day,</title>
<date>1977</date>
<location>San Francisco.</location>
<contexts>
<context position="7740" citStr="Bickel and Doksum, 1977" startWordPosition="1202" endWordPosition="1205">ically do not. Despite these advantages, system combination may be more appropriate in some settings. In particular, MBR system combination is designed primarily for statistical systems that generate N-best or lattice outputs. MBR system combination can integrate non-statistical systems that generate either a single or an unweighted output. However, we would not expect the same strong performance from MBR system combination in these constrained settings. 3 Minimum Bayes risk Decoding MBR decoding aims to find the candidate hypothesis that has the least expected loss under a probability model (Bickel and Doksum, 1977). We begin with a review of MBR for SMT. SMT can be described as a mapping of a word sequence f in a source language to a word sequence e in a target language; this mapping is produced by the MT decoder D(f). If the reference translation e is known, the decoder performance can be measured by the loss function L(e, D(f)). Given such a loss function L(e, e&apos;) between an automatic translation e&apos; and a reference e, and an underlying proba1269 bility model P (e|f), MBR decoding has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004): e� = arg min R(e&apos;) (1) e&apos;EE combination of the Bayes </context>
</contexts>
<marker>Bickel, Doksum, 1977</marker>
<rawString>Peter J. Bickel and Kjell A Doksum. 1977. Mathematical statistics : basic ideas and selected topics. Holden-Day, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar F Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>17--53</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar F. Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Fast consensus decoding over translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>567--575</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4530" citStr="DeNero et al., 2009" startWordPosition="684" endWordPosition="687">elated Work MBR system combination is a multi-system generalization of MBR decoding where the space of hypotheses is not constrained to the space of evidences. We expand the space of hypotheses following some underlying ideas of system combination techniques. 2.1 Minimum Bayes risk In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an Nbest list of translations produced by a first pass decoder (Kumar and Byrne, 2004). Different techniques to widen the search space have been described (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). These works extend the traditional MBR algorithms based on Nbest lists to work with lattices. The use of MBR to combine the outputs of various MT systems has also been explored previously. Duan et al. (2010) present an MBR decoding that makes use of a mixture of different SMT systems to improve translation accuracy. Our technique differs in that we use a linear combination instead of a mixture, which avoids the problem of component systems not sharing the same search space; perform the decoding in a search space larger than the outputs of the component m</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>John DeNero, David Chiang, and Kevin Knight. 2009. Fast consensus decoding over translation forests. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, pages 567–575, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Shankar Kumar</author>
<author>Ciprian Chelba</author>
<author>Franz Och</author>
</authors>
<title>Model combination for machine translation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>975--983</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5272" citStr="DeNero et al. (2010)" startWordPosition="815" endWordPosition="818">h lattices. The use of MBR to combine the outputs of various MT systems has also been explored previously. Duan et al. (2010) present an MBR decoding that makes use of a mixture of different SMT systems to improve translation accuracy. Our technique differs in that we use a linear combination instead of a mixture, which avoids the problem of component systems not sharing the same search space; perform the decoding in a search space larger than the outputs of the component models; and optimize an expected BLEU score instead of the linear approximation to it described in (Tromble et al., 2008). DeNero et al. (2010) present model combination, a multi-system lattice MBR decoding on the conjoined evidences spaces of the component systems. Our technique differs in that we perform the search in an extended search space not restricted to the provided evidences, have fewer parameters to learn, and optimizes an expected BLEU score instead of the linear BLEU approximation. Another MBR-related technique to combine the outputs of various MT systems was presented by Gonz´alez-Rubio and Casacuberta (2010). They use different median string (Fu, 1982) algorithms to combine various machine translation systems. Our appr</context>
<context position="16447" citStr="DeNero et al., 2010" startWordPosition="2794" endWordPosition="2797">n-gram ng in the hypothesis and C&apos;(ng) is the expected count of ng in the evidences. To compute the n-gram matchings m&apos;k, the count of each n-gram is truncated, if necessary, to not exceed the expected count for that n-gram in the evidences. We have replaced a summation over a possibly exponential number of items (e&apos; E Dn(f) in Eq. (9)) with a summation over a polynomial number of ngrams that occur in the evidences2. Both, the expected length of the evidences r&apos; and their expected n-gram counts m&apos;k can be pre-computed efficiently from N-best lists and translation lattices (Kumar et al., 2009; DeNero et al., 2010). 6 Experiments We report results on a multi-source translation task. From the Europarl corpus released for the ACL 2006 workshop on MT (WMT2006), we select those sentence pairs from the German–English (de–en), Spanish–English (es–en) and French– English (fr–en) sub-corpora that share the same English translation. We obtain a multi-source corpus with German, Spanish and French as source languages and English as target language. All the experiments were carried out with the lowercased and tokenized version of this corpus. We report results using BLEU (Papineni et al., 2002) and translation edit</context>
</contexts>
<marker>DeNero, Kumar, Chelba, Och, 2010</marker>
<rawString>John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och. 2010. Model combination for machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 975–983, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Duan</author>
<author>Mu Li</author>
<author>Dongdong Zhang</author>
<author>Ming Zhou</author>
</authors>
<title>Mixture model-based minimum bayes risk decoding using multiple machine translation systems.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>313--321</pages>
<location>Beijing, China,</location>
<contexts>
<context position="4777" citStr="Duan et al. (2010)" startWordPosition="730" endWordPosition="733">techniques. 2.1 Minimum Bayes risk In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an Nbest list of translations produced by a first pass decoder (Kumar and Byrne, 2004). Different techniques to widen the search space have been described (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). These works extend the traditional MBR algorithms based on Nbest lists to work with lattices. The use of MBR to combine the outputs of various MT systems has also been explored previously. Duan et al. (2010) present an MBR decoding that makes use of a mixture of different SMT systems to improve translation accuracy. Our technique differs in that we use a linear combination instead of a mixture, which avoids the problem of component systems not sharing the same search space; perform the decoding in a search space larger than the outputs of the component models; and optimize an expected BLEU score instead of the linear approximation to it described in (Tromble et al., 2008). DeNero et al. (2010) present model combination, a multi-system lattice MBR decoding on the conjoined evidences spaces of the </context>
<context position="10652" citStr="Duan et al., 2010" startWordPosition="1730" endWordPosition="1733"> systems to be statistically independent and define the Bayes gain as a linear an · Gn(e&apos;) (7) X�n · Pn(e|f) · S(e, e&apos;) , (8) eEDn(f) where N is the total number of component systems, Eh represents the hypotheses space where the search is performed, Gn(e&apos;) is the Bayes gain of hypothesis e&apos; given by the nth component system and an is a scaling factor introduced to take into account the differences in quality of the component models. It is worth mentioning that by using a linear combination instead of a mixture model, we avoid the problem of component systems not sharing the same search space (Duan et al., 2010). MBR system combination parameters training and decoding in the extended hypotheses space are described below. 4.1 Model Training We learn the scaling factors in Eq. (8) using minimum error rate training (MERT) (Och, 2003). MERT maximizes the translation quality of e� on a held-out set, according to an evaluation metric that compares to a reference set. We used BLEU, choosing the scaling factors to maximize BLEU score of the set of translations predicted by MBR system combination. We perform the maximization by means of the down-hill simplex algorithm (Nelder and Mead, 1965). 4.2 Model Decodi</context>
</contexts>
<marker>Duan, Li, Zhang, Zhou, 2010</marker>
<rawString>Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou. 2010. Mixture model-based minimum bayes risk decoding using multiple machine translation systems. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 313– 321, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ehling</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Minimum bayes risk decoding for bleu.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>101--104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18505" citStr="Ehling et al. (2007)" startWordPosition="3121" endWordPosition="3124">ng from one source language (German, Spanish or French) into English. Each individual system is a phrase-based system trained using the Moses toolkit (Koehn et al., 2007). The parameters of the systems were tuned using MERT (Och, 2003) to optimize BLEU on the development set. Each base system yields state-of-the-art performance, summarized in Table 1. For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). 6.2 Experimental Results Table 2 compares MBR system combination (MBRSC) to the best MAX and MBR systems. Both Best 3Ehling et al. (2007) studied up to 10000-best and show that the use of 1000-best candidates is sufficient for MBR decoding. 1272 Setup BLEU TER Best MBR 30.9 53.4 MBR-SC Expected 30.9 53.5 MBR-SC E/Conjoin 32.4 52.1 MBR-SC E/C/evidences-best 30.9 53.5 MBR-SC E/C/hypotheses-best 31.8 52.5 MBR-SC E/C/Extended 32.7 52.3 MBR-SC E/C/Ex/MERT 32.8 52.3 Table 3: Results on the test set for different setups of minimum Bayes risk system combination. MBR and MBR-SC were computed on 1000-best lists. MBR-SC uses expected BLEU as gain function using the conjoined evidences spaces of the three systems to compute expected BLEU s</context>
</contexts>
<marker>Ehling, Zens, Ney, 2007</marker>
<rawString>Nicola Ehling, Richard Zens, and Hermann Ney. 2007. Minimum bayes risk decoding for bleu. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 101– 104, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frederking</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Three heads are better than one.</title>
<date>1994</date>
<booktitle>In Proceedings of the fourth conference on Applied natural language processing,</booktitle>
<pages>95--100</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1895" citStr="Frederking and Nirenburg, 1994" startWordPosition="263" endWordPosition="266"> combination methods. 1 Introduction Once statistical models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the max–derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). System combination procedures, on the other hand, generate translations from the output of multiple component systems by combining the best fragments of these outputs (Frederking and Nirenburg, 1994). In this paper, we present minimum Bayes risk system combination, a technique that unifies these two approaches by learning a consensus translation over multiple underlying component systems. MBR system combination operates directly on the outputs of the component models. We perform an MBR decoding using a linear combination of the component models’ probability distributions. Instead of re-ranking the translations provided by the component systems, we search for the hypothesis with the minimum expected translation error among all the possible finite-length strings in the target language. By u</context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Robert Frederking and Sergei Nirenburg. 1994. Three heads are better than one. In Proceedings of the fourth conference on Applied natural language processing, pages 95–100, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Fu</author>
</authors>
<title>Syntactic Pattern Recognition and Applications.</title>
<date>1982</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="5804" citStr="Fu, 1982" startWordPosition="898" endWordPosition="899">approximation to it described in (Tromble et al., 2008). DeNero et al. (2010) present model combination, a multi-system lattice MBR decoding on the conjoined evidences spaces of the component systems. Our technique differs in that we perform the search in an extended search space not restricted to the provided evidences, have fewer parameters to learn, and optimizes an expected BLEU score instead of the linear BLEU approximation. Another MBR-related technique to combine the outputs of various MT systems was presented by Gonz´alez-Rubio and Casacuberta (2010). They use different median string (Fu, 1982) algorithms to combine various machine translation systems. Our approach differs in that we take into account the posterior distribution over translations instead of considering each translation equally likely, optimize the expected BLEU score instead of a sentence-wise measure such as the edit distance or the sentencelevel BLEU, and take into account the quality differences by associating a tunable scaling factor to each system. 2.2 System Combination System combination techniques in MT take as input the outputs lei, · · · , eN} of N translation systems, where e, is a structured translation o</context>
</contexts>
<marker>Fu, 1982</marker>
<rawString>K.S. Fu. 1982. Syntactic Pattern Recognition and Applications. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vaibhava Goel</author>
<author>William J Byrne</author>
</authors>
<title>Minimum bayes-risk automatic speech recognition.</title>
<date>2000</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="8261" citStr="Goel and Byrne, 2000" startWordPosition="1303" endWordPosition="1306">ate hypothesis that has the least expected loss under a probability model (Bickel and Doksum, 1977). We begin with a review of MBR for SMT. SMT can be described as a mapping of a word sequence f in a source language to a word sequence e in a target language; this mapping is produced by the MT decoder D(f). If the reference translation e is known, the decoder performance can be measured by the loss function L(e, D(f)). Given such a loss function L(e, e&apos;) between an automatic translation e&apos; and a reference e, and an underlying proba1269 bility model P (e|f), MBR decoding has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004): e� = arg min R(e&apos;) (1) e&apos;EE combination of the Bayes gains of the components. Each system provides its own space of evidences Dn(f) and its posterior distribution over translations Pn(e|f). Given a sentence f in the source language, MBR system combination is written as follows: X= arg min P(e|f) · L(e, e&apos;) , (2) e� = arg max G(e&apos;) (6) e&apos;EE eEE e&apos;EEh ≈ arg max e&apos;EEh = arg max e&apos;EEh XN n=1 XN n=1 where R(e&apos;) denotes the Bayes risk of candidate translation e&apos; under loss function L, and E represents the space of translations. If the loss function between any two hypothese</context>
</contexts>
<marker>Goel, Byrne, 2000</marker>
<rawString>Vaibhava Goel and William J. Byrne. 2000. Minimum bayes-risk automatic speech recognition. Computer Speech &amp; Language, 14(2):115–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gonz´alez-Rubio</author>
<author>Francisco Casacuberta</author>
</authors>
<title>On the use of median string for multi-source translation. In</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Pattern Recognition (ICPR2010),</booktitle>
<pages>4328--4331</pages>
<marker>Gonz´alez-Rubio, Casacuberta, 2010</marker>
<rawString>Jes´us Gonz´alez-Rubio and Francisco Casacuberta. 2010. On the use of median string for multi-source translation. In In Proceedings of the International Conference on Pattern Recognition (ICPR2010), pages 4328– 4331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Kristina Toutanova</author>
</authors>
<title>Joint optimization for machine translation system combination.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1202--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="28056" citStr="He and Toutanova, 2009" startWordPosition="4636" endWordPosition="4639">antages over current approaches to system combination. First, it does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al., 2008). Instead of aligning the sentences, we view the sentences as vectors of n-gram counts and compute the expected statistics of the BLEU score to compute the Bayes gain. Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging and also affects system combination performance (He and Toutanova, 2009). MBR system combination sidesteps this issue by working directly on the conjoined evidences space produced by the outputs of the component systems, and allows the consensus model to express system preferences via scaling factors. Despite its simplicity, MBR system combination provides strong performance by leveraging different consensus, decoding and training techniques. It outperforms best MAX or MBR derivation on each of the component systems. In addition, it obtains stateof-the-art performance in a constrained setting better suited for dominant system combination techniques. Acknowledgemen</context>
</contexts>
<marker>He, Toutanova, 2009</marker>
<rawString>Xiaodong He and Kristina Toutanova. 2009. Joint optimization for machine translation system combination. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, pages 1202–1211, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>98--107</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27699" citStr="He et al., 2008" startWordPosition="4574" endWordPosition="4577">eusch et al. (2010) generate intermediate translations in several pivot languages, translate them separately into the target language, and generate a consensus translation out of these using a system combination technique. Likewise, these pivot translations could be combined via MBR system combination. MBR system combination has two significant advantages over current approaches to system combination. First, it does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al., 2008). Instead of aligning the sentences, we view the sentences as vectors of n-gram counts and compute the expected statistics of the BLEU score to compute the Bayes gain. Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging and also affects system combination performance (He and Toutanova, 2009). MBR system combination sidesteps this issue by working directly on the conjoined evidences space produced by the outputs of the component systems, and allows the consensus model to express system preferences via scaling factors. Despite its </context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 98–107, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18055" citStr="Koehn et al., 2007" startWordPosition="3047" endWordPosition="3050">Best MAX), the best single system minimum Bayes risk decoding (Best MBR) and minimum Bayes risk system combination (MBR-SC) combining three systems. 95% confidence intervals computed using paired bootstrap re-sampling (Zhang and Vogel, 2004). In all table cells (except for Table 3) systems without statistically significant differences are marked with the same superscript. 6.1 Base Systems We combine outputs from three systems, each one translating from one source language (German, Spanish or French) into English. Each individual system is a phrase-based system trained using the Moses toolkit (Koehn et al., 2007). The parameters of the systems were tuned using MERT (Och, 2003) to optimize BLEU on the development set. Each base system yields state-of-the-art performance, summarized in Table 1. For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). 6.2 Experimental Results Table 2 compares MBR system combination (MBRSC) to the best MAX and MBR systems. Both Best 3Ehling et al. (2007) studied up to 10000-best and show that the use of 1000-best candidates is sufficient for MBR decoding. 1272 Setup BLEU TER Best MBR 30.9 53.4 MBR-SC </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177– 180, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William J Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="1694" citStr="Kumar and Byrne, 2004" startWordPosition="234" endWordPosition="237">erogeneous structure. Experiments show that our approach bring significant improvements to single-system-based MBR decoding and achieves comparable results to different state-of-the-art system combination methods. 1 Introduction Once statistical models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the max–derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). System combination procedures, on the other hand, generate translations from the output of multiple component systems by combining the best fragments of these outputs (Frederking and Nirenburg, 1994). In this paper, we present minimum Bayes risk system combination, a technique that unifies these two approaches by learning a consensus translation over multiple underlying component systems. MBR system combination operates directly on the outputs of the component models. We perform an MBR decoding using a linear combination of the component models’ probability distributions. Instead of re-ranki</context>
<context position="4419" citStr="Kumar and Byrne, 2004" startWordPosition="664" endWordPosition="667">uistics, pages 1268–1277, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work MBR system combination is a multi-system generalization of MBR decoding where the space of hypotheses is not constrained to the space of evidences. We expand the space of hypotheses following some underlying ideas of system combination techniques. 2.1 Minimum Bayes risk In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an Nbest list of translations produced by a first pass decoder (Kumar and Byrne, 2004). Different techniques to widen the search space have been described (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). These works extend the traditional MBR algorithms based on Nbest lists to work with lattices. The use of MBR to combine the outputs of various MT systems has also been explored previously. Duan et al. (2010) present an MBR decoding that makes use of a mixture of different SMT systems to improve translation accuracy. Our technique differs in that we use a linear combination instead of a mixture, which avoids the problem of component systems not s</context>
<context position="8285" citStr="Kumar and Byrne, 2004" startWordPosition="1307" endWordPosition="1310">s the least expected loss under a probability model (Bickel and Doksum, 1977). We begin with a review of MBR for SMT. SMT can be described as a mapping of a word sequence f in a source language to a word sequence e in a target language; this mapping is produced by the MT decoder D(f). If the reference translation e is known, the decoder performance can be measured by the loss function L(e, D(f)). Given such a loss function L(e, e&apos;) between an automatic translation e&apos; and a reference e, and an underlying proba1269 bility model P (e|f), MBR decoding has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004): e� = arg min R(e&apos;) (1) e&apos;EE combination of the Bayes gains of the components. Each system provides its own space of evidences Dn(f) and its posterior distribution over translations Pn(e|f). Given a sentence f in the source language, MBR system combination is written as follows: X= arg min P(e|f) · L(e, e&apos;) , (2) e� = arg max G(e&apos;) (6) e&apos;EE eEE e&apos;EEh ≈ arg max e&apos;EEh = arg max e&apos;EEh XN n=1 XN n=1 where R(e&apos;) denotes the Bayes risk of candidate translation e&apos; under loss function L, and E represents the space of translations. If the loss function between any two hypotheses can be bounded: L(e, e</context>
<context position="18366" citStr="Kumar and Byrne, 2004" startWordPosition="3097" endWordPosition="3100">ally significant differences are marked with the same superscript. 6.1 Base Systems We combine outputs from three systems, each one translating from one source language (German, Spanish or French) into English. Each individual system is a phrase-based system trained using the Moses toolkit (Koehn et al., 2007). The parameters of the systems were tuned using MERT (Och, 2003) to optimize BLEU on the development set. Each base system yields state-of-the-art performance, summarized in Table 1. For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). 6.2 Experimental Results Table 2 compares MBR system combination (MBRSC) to the best MAX and MBR systems. Both Best 3Ehling et al. (2007) studied up to 10000-best and show that the use of 1000-best candidates is sufficient for MBR decoding. 1272 Setup BLEU TER Best MBR 30.9 53.4 MBR-SC Expected 30.9 53.5 MBR-SC E/Conjoin 32.4 52.1 MBR-SC E/C/evidences-best 30.9 53.5 MBR-SC E/C/hypotheses-best 31.8 52.5 MBR-SC E/C/Extended 32.7 52.3 MBR-SC E/C/Ex/MERT 32.8 52.3 Table 3: Results on the test set for different setups of minimum Bayes risk system combination. MBR and MBR-SC were computed on 1000-</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William J. Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In HLT-NAACL, pages 169–176.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>163--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4550" citStr="Kumar et al., 2009" startWordPosition="688" endWordPosition="691">m combination is a multi-system generalization of MBR decoding where the space of hypotheses is not constrained to the space of evidences. We expand the space of hypotheses following some underlying ideas of system combination techniques. 2.1 Minimum Bayes risk In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an Nbest list of translations produced by a first pass decoder (Kumar and Byrne, 2004). Different techniques to widen the search space have been described (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). These works extend the traditional MBR algorithms based on Nbest lists to work with lattices. The use of MBR to combine the outputs of various MT systems has also been explored previously. Duan et al. (2010) present an MBR decoding that makes use of a mixture of different SMT systems to improve translation accuracy. Our technique differs in that we use a linear combination instead of a mixture, which avoids the problem of component systems not sharing the same search space; perform the decoding in a search space larger than the outputs of the component models; and optimize </context>
<context position="16425" citStr="Kumar et al., 2009" startWordPosition="2790" endWordPosition="2793">is the count of the n-gram ng in the hypothesis and C&apos;(ng) is the expected count of ng in the evidences. To compute the n-gram matchings m&apos;k, the count of each n-gram is truncated, if necessary, to not exceed the expected count for that n-gram in the evidences. We have replaced a summation over a possibly exponential number of items (e&apos; E Dn(f) in Eq. (9)) with a summation over a polynomial number of ngrams that occur in the evidences2. Both, the expected length of the evidences r&apos; and their expected n-gram counts m&apos;k can be pre-computed efficiently from N-best lists and translation lattices (Kumar et al., 2009; DeNero et al., 2010). 6 Experiments We report results on a multi-source translation task. From the Europarl corpus released for the ACL 2006 workshop on MT (WMT2006), we select those sentence pairs from the German–English (de–en), Spanish–English (es–en) and French– English (fr–en) sub-corpora that share the same English translation. We obtain a multi-source corpus with German, Spanish and French as source languages and English as target language. All the experiments were carried out with the lowercased and tokenized version of this corpus. We report results using BLEU (Papineni et al., 2002</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, pages 163–171, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Aur´elien Max</author>
<author>Josep Maria Crego</author>
<author>Hermann Ney</author>
</authors>
<title>Multi-pivot translation by system combination.</title>
<date>2010</date>
<booktitle>In International Workshop on Spoken Language Translation,</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="27102" citStr="Leusch et al. (2010)" startWordPosition="4483" endWordPosition="4486"> MAX 1274 BLEU 32 30 28 26 24 22 20 18 16 MBR-SC BBN CMU DCU JHU KOC LIUM RWTH cz-en en-cz de-en en-de es-en en-es fr-en en-fr Figure 3: Performance of minimum Bayes risk system combination (MBR-SC) for different language directions in comparison to the rest of system combination techniques presented in the WMT2010 system combination task. strings in the output vocabulary. Component systems can have varied decoding strategies; we only require that each system produce an N-best list (or a lattice) of translations. This flexibility allows the technique to be applied quite broadly. For instance, Leusch et al. (2010) generate intermediate translations in several pivot languages, translate them separately into the target language, and generate a consensus translation out of these using a system combination technique. Likewise, these pivot translations could be combined via MBR system combination. MBR system combination has two significant advantages over current approaches to system combination. First, it does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al., 2008). I</context>
</contexts>
<marker>Leusch, Max, Crego, Ney, 2010</marker>
<rawString>Gregor Leusch, Aur´elien Max, Josep Maria Crego, and Hermann Ney. 2010. Multi-pivot translation by system combination. In International Workshop on Spoken Language Translation, Paris, France, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>593--601</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4568" citStr="Li et al., 2009" startWordPosition="692" endWordPosition="695">ulti-system generalization of MBR decoding where the space of hypotheses is not constrained to the space of evidences. We expand the space of hypotheses following some underlying ideas of system combination techniques. 2.1 Minimum Bayes risk In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an Nbest list of translations produced by a first pass decoder (Kumar and Byrne, 2004). Different techniques to widen the search space have been described (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). These works extend the traditional MBR algorithms based on Nbest lists to work with lattices. The use of MBR to combine the outputs of various MT systems has also been explored previously. Duan et al. (2010) present an MBR decoding that makes use of a mixture of different SMT systems to improve translation accuracy. Our technique differs in that we use a linear combination instead of a mixture, which avoids the problem of component systems not sharing the same search space; perform the decoding in a search space larger than the outputs of the component models; and optimize an expected BLEU s</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Variational decoding for statistical machine translation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, pages 593– 601, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Martinez</author>
<author>A Juan</author>
<author>F Casacuberta</author>
</authors>
<title>Use of Median String for Classification.</title>
<date>2000</date>
<booktitle>In Proceedings of the 15th International Conference on Pattern Recognition,</booktitle>
<volume>2</volume>
<pages>907--910</pages>
<location>Barcelona</location>
<marker>Martinez, Juan, Casacuberta, 2000</marker>
<rawString>C. D. Martinez, A. Juan, and F. Casacuberta. 2000. Use of Median String for Classification. In Proceedings of the 15th International Conference on Pattern Recognition, volume 2, pages 907–910, Barcelona (Spain), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Nelder</author>
<author>Roger Mead</author>
</authors>
<title>A Simplex Method for Function Minimization.</title>
<date>1965</date>
<journal>The Computer Journal,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="11234" citStr="Nelder and Mead, 1965" startWordPosition="1825" endWordPosition="1828">he same search space (Duan et al., 2010). MBR system combination parameters training and decoding in the extended hypotheses space are described below. 4.1 Model Training We learn the scaling factors in Eq. (8) using minimum error rate training (MERT) (Och, 2003). MERT maximizes the translation quality of e� on a held-out set, according to an evaluation metric that compares to a reference set. We used BLEU, choosing the scaling factors to maximize BLEU score of the set of translations predicted by MBR system combination. We perform the maximization by means of the down-hill simplex algorithm (Nelder and Mead, 1965). 4.2 Model Decoding In most MBR algorithms, the hypotheses space is equal to the evidences space. Following the underlying idea of system combination, we are interested in extend the hypotheses space by including new sentences created using fragments of the hypotheses in the evidences spaces of the component models. We perform the search (argmax operation in Eq. (8)) e� = arg max G(e&apos;) (3) e&apos;EE X= arg max P(e|f) · S(e, e&apos;) . (4) e&apos;EE eEE 1270 Algorithm 1 MBR system combination decoding. Require: Initial hypothesis e Require: Vocabulary the evidences E 1: e�← e 2: repeat 3: ecur ← e� 4: for j </context>
</contexts>
<marker>Nelder, Mead, 1965</marker>
<rawString>John A. Nelder and Roger Mead. 1965. A Simplex Method for Function Minimization. The Computer Journal, 7(4):308–313, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical multi-source translation. In</title>
<date>2001</date>
<booktitle>In Machine Translation Summit,</booktitle>
<pages>253--258</pages>
<contexts>
<context position="22645" citStr="Och and Ney, 2001" startWordPosition="3771" endWordPosition="3774">ince these two systems are identical in their expected BLEU statistics, the improvements in BLEU imply that the extended search space has introduced better hypotheses. The degradation in TER performance can be explained by the use of a BLEU-based gain function in the decoding process. We finally compute the optimum values for the scaling factors of the different system using MERT (MBR-SC-E/C/Ex/MERT). MBR-SCE/C/Ex/MERT slightly improves BLEU score of MBR-SC-E/C/Extended. This implies that the optimal values of the scaling factors do not deviate much from 1.0; a similar result was reported in (Och and Ney, 2001). We hypothesize that this is because the three component systems share the same SMT model, pre-process and decoding. We expect to obtain larger improvements when combining systems implementing different MT paradigms. 1273 Figure 1: Performance of minimum Bayes risk system combination (MBR-SC) for different sizes of the evidences space in comparison to other MBR-SC setups. MBR-SC-E/C/Ex/MERT is the standard setup for MBR system combination and, from now, on we will refer to it as MBR-SC. We next evaluate performance of MBR system combination on N-best lists of increasing sizes, and compare it </context>
</contexts>
<marker>Och, Ney, 2001</marker>
<rawString>Franz Josef Och and Hermann Ney. 2001. Statistical multi-source translation. In In Machine Translation Summit, pages 253–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10875" citStr="Och, 2003" startWordPosition="1767" endWordPosition="1768">arch is performed, Gn(e&apos;) is the Bayes gain of hypothesis e&apos; given by the nth component system and an is a scaling factor introduced to take into account the differences in quality of the component models. It is worth mentioning that by using a linear combination instead of a mixture model, we avoid the problem of component systems not sharing the same search space (Duan et al., 2010). MBR system combination parameters training and decoding in the extended hypotheses space are described below. 4.1 Model Training We learn the scaling factors in Eq. (8) using minimum error rate training (MERT) (Och, 2003). MERT maximizes the translation quality of e� on a held-out set, according to an evaluation metric that compares to a reference set. We used BLEU, choosing the scaling factors to maximize BLEU score of the set of translations predicted by MBR system combination. We perform the maximization by means of the down-hill simplex algorithm (Nelder and Mead, 1965). 4.2 Model Decoding In most MBR algorithms, the hypotheses space is equal to the evidences space. Following the underlying idea of system combination, we are interested in extend the hypotheses space by including new sentences created using</context>
<context position="18120" citStr="Och, 2003" startWordPosition="3061" endWordPosition="3062">nd minimum Bayes risk system combination (MBR-SC) combining three systems. 95% confidence intervals computed using paired bootstrap re-sampling (Zhang and Vogel, 2004). In all table cells (except for Table 3) systems without statistically significant differences are marked with the same superscript. 6.1 Base Systems We combine outputs from three systems, each one translating from one source language (German, Spanish or French) into English. Each individual system is a phrase-based system trained using the Moses toolkit (Koehn et al., 2007). The parameters of the systems were tuned using MERT (Och, 2003) to optimize BLEU on the development set. Each base system yields state-of-the-art performance, summarized in Table 1. For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). 6.2 Experimental Results Table 2 compares MBR system combination (MBRSC) to the best MAX and MBR systems. Both Best 3Ehling et al. (2007) studied up to 10000-best and show that the use of 1000-best candidates is sufficient for MBR decoding. 1272 Setup BLEU TER Best MBR 30.9 53.4 MBR-SC Expected 30.9 53.5 MBR-SC E/Conjoin 32.4 52.1 MBR-SC E/C/evidence</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, pages 160–167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2553" citStr="Papineni et al., 2002" startWordPosition="363" endWordPosition="367">Bayes risk system combination, a technique that unifies these two approaches by learning a consensus translation over multiple underlying component systems. MBR system combination operates directly on the outputs of the component models. We perform an MBR decoding using a linear combination of the component models’ probability distributions. Instead of re-ranking the translations provided by the component systems, we search for the hypothesis with the minimum expected translation error among all the possible finite-length strings in the target language. By using a loss function based on BLEU (Papineni et al., 2002), we avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007). MBR system combination assumes only that each translation model can produce expectations of n-gram counts; the latent derivation structures of the component systems can differ arbitrary. This flexibility allows us to combine a great variety of SMT systems. The key contributions of this paper are three: the usage of a linear combination of distributions within the MBR decoding, which allows multiple SMT models to be involved in, and makes the computation of n-grams statist</context>
<context position="12738" citStr="Papineni et al., 2002" startWordPosition="2109" endWordPosition="2112">) 17: until G(6) &gt;6 G(ecur) 18: return ecur Ensure: G(ecur) ≥ G(e) The complexity of the main loop (lines 2-17) is O(|ecur |· |E |· Cg), where Cg is the cost of computing the gain of a hypothesis, and usually only a moderate number of iterations (&lt; 10) is needed to converge (Mart´ınez et al., 2000). 5 Computing BLEU-based Gain We are interested in performing MBR system combination under BLEU. BLEU behaves as a score function: its value ranges between 0 and 1 and a larger value reflects a higher similarity. Therefore, we rewrite the gain function G(·) using single evidence (or reference) BLEU (Papineni et al., 2002) as the similarity function: Gn(e&apos;) = � Pn(e|f) · BLEU(e, e&apos;) (9) eEDn(f) BLEU = ri4 \mk / 1 · min (e1− c ,1.0) , (10) ck 4 k=1 using the approximate median string (AMS) algorithm (Mart´ınez et al., 2000). AMS algorithm perform a search on a hypotheses space equal to the free monoid E* of the vocabulary of the evidences E = Voc(Ee). The AMS algorithm is shown in Algorithm 1. AMS starts with an initial hypothesis e that is modified using edit operations until there is no improvement in the Bayes gain (Lines 3–16). On each position j of the current solution ecur, we apply all the possible single</context>
<context position="17026" citStr="Papineni et al., 2002" startWordPosition="2885" endWordPosition="2888">s (Kumar et al., 2009; DeNero et al., 2010). 6 Experiments We report results on a multi-source translation task. From the Europarl corpus released for the ACL 2006 workshop on MT (WMT2006), we select those sentence pairs from the German–English (de–en), Spanish–English (es–en) and French– English (fr–en) sub-corpora that share the same English translation. We obtain a multi-source corpus with German, Spanish and French as source languages and English as target language. All the experiments were carried out with the lowercased and tokenized version of this corpus. We report results using BLEU (Papineni et al., 2002) and translation edit rate (Snover et al., 2006) (TER). We measure statistical significance using 2If Dn(f) is represented by a lattice, the number of n-grams is polynomial in the number of edges in the lattice. Approach dev TER test TER BLEU BLEU Best MAX 30.9* 53.3* 30.8* 53.4* Best MBR 31.0* 53.4* 30.9* 53.4* MBR-SC 32.3 52.5 32.8 52.3 Table 2: Performance from best single system maxderivation decoding (Best MAX), the best single system minimum Bayes risk decoding (Best MBR) and minimum Bayes risk system combination (MBR-SC) combining three systems. 95% confidence intervals computed using p</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Necip Fazil Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Bonnie Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>228--235</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="2675" citStr="Rosti et al., 2007" startWordPosition="384" endWordPosition="387">e underlying component systems. MBR system combination operates directly on the outputs of the component models. We perform an MBR decoding using a linear combination of the component models’ probability distributions. Instead of re-ranking the translations provided by the component systems, we search for the hypothesis with the minimum expected translation error among all the possible finite-length strings in the target language. By using a loss function based on BLEU (Papineni et al., 2002), we avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007). MBR system combination assumes only that each translation model can produce expectations of n-gram counts; the latent derivation structures of the component systems can differ arbitrary. This flexibility allows us to combine a great variety of SMT systems. The key contributions of this paper are three: the usage of a linear combination of distributions within the MBR decoding, which allows multiple SMT models to be involved in, and makes the computation of n-grams statistics to be more accurate; the decoding in an extended search space, which allows to find better hypotheses than the evidenc</context>
<context position="6801" citStr="Rosti et al., 2007" startWordPosition="1061" endWordPosition="1064">by associating a tunable scaling factor to each system. 2.2 System Combination System combination techniques in MT take as input the outputs lei, · · · , eN} of N translation systems, where e, is a structured translation object (or N-best lists thereof), typically viewed as a sequence of words. The dominant approach in the field chooses a primary translation ep as a backbone, then finds an alignment a,,, to the backbone for each e,,,. A new search space is constructed from these backbone-aligned outputs and then a voting procedure of feature-based model predicts a final consensus translation (Rosti et al., 2007). MBR system combination entirely avoids this alignment problem by considering hypotheses as n-gram occurrence vectors rather than word sequences. MBR system combination performs the decoding in a larger search space and includes statistics from the components’ posteriors, whereas system combination techniques typically do not. Despite these advantages, system combination may be more appropriate in some settings. In particular, MBR system combination is designed primarily for statistical systems that generate N-best or lattice outputs. MBR system combination can integrate non-statistical syste</context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spyros Matsoukas, Richard Schwartz, and Bonnie Dorr. 2007. Combining outputs from multiple machine translation systems. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 228–235, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>Ralph Weischedel</author>
</authors>
<title>A study of translation error rate with targeted human annotation. In</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Machine Transaltion in the Americas.</booktitle>
<contexts>
<context position="17074" citStr="Snover et al., 2006" startWordPosition="2893" endWordPosition="2896">eriments We report results on a multi-source translation task. From the Europarl corpus released for the ACL 2006 workshop on MT (WMT2006), we select those sentence pairs from the German–English (de–en), Spanish–English (es–en) and French– English (fr–en) sub-corpora that share the same English translation. We obtain a multi-source corpus with German, Spanish and French as source languages and English as target language. All the experiments were carried out with the lowercased and tokenized version of this corpus. We report results using BLEU (Papineni et al., 2002) and translation edit rate (Snover et al., 2006) (TER). We measure statistical significance using 2If Dn(f) is represented by a lattice, the number of n-grams is polynomial in the number of edges in the lattice. Approach dev TER test TER BLEU BLEU Best MAX 30.9* 53.3* 30.8* 53.4* Best MBR 31.0* 53.4* 30.9* 53.4* MBR-SC 32.3 52.5 32.8 52.3 Table 2: Performance from best single system maxderivation decoding (Best MAX), the best single system minimum Bayes risk decoding (Best MBR) and minimum Bayes risk system combination (MBR-SC) combining three systems. 95% confidence intervals computed using paired bootstrap re-sampling (Zhang and Vogel, 20</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Weischedel, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and Ralph Weischedel. 2006. A study of translation error rate with targeted human annotation. In In Proceedings of the Association for Machine Transaltion in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy W Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice minimum bayes-risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>620--629</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4509" citStr="Tromble et al., 2008" startWordPosition="680" endWordPosition="683">tional Linguistics 2 Related Work MBR system combination is a multi-system generalization of MBR decoding where the space of hypotheses is not constrained to the space of evidences. We expand the space of hypotheses following some underlying ideas of system combination techniques. 2.1 Minimum Bayes risk In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an Nbest list of translations produced by a first pass decoder (Kumar and Byrne, 2004). Different techniques to widen the search space have been described (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). These works extend the traditional MBR algorithms based on Nbest lists to work with lattices. The use of MBR to combine the outputs of various MT systems has also been explored previously. Duan et al. (2010) present an MBR decoding that makes use of a mixture of different SMT systems to improve translation accuracy. Our technique differs in that we use a linear combination instead of a mixture, which avoids the problem of component systems not sharing the same search space; perform the decoding in a search space larger than the outpu</context>
<context position="14374" citStr="Tromble et al. (2008)" startWordPosition="2418" endWordPosition="2421">, we return the current hypothesis. AMS algorithm takes as input an initial hypothesis e and the combined vocabulary of the evidences spaces E. Its output is a possibly new hypothesis whose Bayes gain is assured to be higher or equal than the Bayes gain of the initial hypothesis. where r is the length of the evidence, c the length of the hypothesis, mk the number of n-gram matches of size k, and ck the count of n-grams of size k in the hypothesis. The evidences space Dn(f) may contain a huge number of hypotheses1 which often make impractical to compute Eq. (9) directly. To avoid this problem, Tromble et al. (2008) propose linear BLEU, an approximation to the BLEU score to efficiently perform MBR decoding when the search space is represented with lattices. However, our hypotheses space is the full set of finite-length strings in the target vocabulary and can not be represented in a lattice. In Eq. (9), we have one hypothesis e&apos; that is to be compared to a set of evidences e ∈ Dn(f) which follow a probability distribution Pn(e|f). Instead of computing the expected BLEU score by calculating the BLEU score with respect to each of the evidences, our approach will be to use the expected n-gram counts and sen</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy W. Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice minimum bayes-risk decoding for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 620–629, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>Measuring confidence intervals for the machine translation evaluation metrics. In</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-2004,</booktitle>
<pages>4--6</pages>
<contexts>
<context position="17677" citStr="Zhang and Vogel, 2004" startWordPosition="2990" endWordPosition="2993">nover et al., 2006) (TER). We measure statistical significance using 2If Dn(f) is represented by a lattice, the number of n-grams is polynomial in the number of edges in the lattice. Approach dev TER test TER BLEU BLEU Best MAX 30.9* 53.3* 30.8* 53.4* Best MBR 31.0* 53.4* 30.9* 53.4* MBR-SC 32.3 52.5 32.8 52.3 Table 2: Performance from best single system maxderivation decoding (Best MAX), the best single system minimum Bayes risk decoding (Best MBR) and minimum Bayes risk system combination (MBR-SC) combining three systems. 95% confidence intervals computed using paired bootstrap re-sampling (Zhang and Vogel, 2004). In all table cells (except for Table 3) systems without statistically significant differences are marked with the same superscript. 6.1 Base Systems We combine outputs from three systems, each one translating from one source language (German, Spanish or French) into English. Each individual system is a phrase-based system trained using the Moses toolkit (Koehn et al., 2007). The parameters of the systems were tuned using MERT (Och, 2003) to optimize BLEU on the development set. Each base system yields state-of-the-art performance, summarized in Table 1. For each system, we report the perform</context>
</contexts>
<marker>Zhang, Vogel, 2004</marker>
<rawString>Ying Zhang and Stephan Vogel. 2004. Measuring confidence intervals for the machine translation evaluation metrics. In In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-2004, pages 4–6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>