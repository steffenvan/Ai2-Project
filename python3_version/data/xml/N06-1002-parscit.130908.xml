<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.7545875">
Do we need phrases? Challenging the conventional wisdom in Statistical
Machine Translation
</title>
<author confidence="0.914352">
Chris Quirk and Arul Menezes
</author>
<affiliation confidence="0.902291">
Microsoft Research
</affiliation>
<address confidence="0.921496">
One Microsoft Way
Redmond, WA 98052 USA
</address>
<email confidence="0.996323">
{chrisq,arulm}@microsoft.com
</email>
<sectionHeader confidence="0.997346" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961416666667">
We begin by exploring theoretical and
practical issues with phrasal SMT, several
of which are addressed by syntax-based
SMT. Next, to address problems not
handled by syntax, we propose the
concept of a Minimal Translation Unit
(MTU) and develop MTU sequence
models. Finally we incorporate these
models into a syntax-based SMT system
and demonstrate that it improves on the
state of the art translation quality within a
theoretically more desirable framework.
</bodyText>
<sectionHeader confidence="0.998774" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99991924137931">
The last several years have seen phrasal statistical
machine translation (SMT) systems outperform
word-based approaches by a wide margin (Koehn
2003). Unfortunately the use of phrases in SMT is
beset by a number of difficult theoretical and
practical problems, which we attempt to
characterize below. Recent research into syntax-
based SMT (Quirk and Menezes 2005; Chiang
2005) has produced promising results in
addressing some of the problems; research
motivated by other statistical models has helped
to address others (Banchs et al. 2005). We refine
and unify two threads of research in an attempt to
address all of these problems simultaneously.
Such an approach proves both theoretically more
desirable and empirically superior.
In brief, Phrasal SMT systems employ phrase
pairs automatically extracted from parallel
corpora. To translate, a source sentence is first
partitioned into a sequence of phrases I = s1...sI.
Each source phrase si is then translated into a
target phrase ti. Finally the target phrases are
permuted, and the translation is read off in order.
Beam search is used to approximate the optimal
translation. We refer the reader to Keohn et al.
(2003) for a detailed description. Unless
otherwise noted, the following discussion is
generally applicable to Alignment Template
systems (Och and Ney 2004) as well.
</bodyText>
<subsectionHeader confidence="0.987155">
1.1. Advantages of phrasal SMT
</subsectionHeader>
<bodyText confidence="0.987517454545454">
Non-compositionality
Phrases capture the translations of idiomatic and
other non-compositional fixed phrases as a unit,
side-stepping the need to awkwardly reconstruct
them word by word. While many words can be
translated into a single target word, common
everyday phrases such as the English password
translating as the French mot de passe cannot be
easily subdivided. Allowing such translations to
be first class entities simplifies translation
implementation and improves translation quality.
</bodyText>
<subsectionHeader confidence="0.692615">
Local re-ordering
</subsectionHeader>
<bodyText confidence="0.999784">
Phrases provide memorized re-ordering decisions.
As previously noted, translation can be
conceptually divided into two steps: first, finding
a set of phrase pairs that simultaneously covers
the source side and provides a bag of translated
target phrases; and second, picking an order for
those target phrases. Since phrase pairs consist of
memorized substrings of the training data, they
are very likely to produce correct local re-
orderings.
</bodyText>
<subsectionHeader confidence="0.618643">
Contextual information
</subsectionHeader>
<bodyText confidence="0.999960111111111">
Many phrasal translations may be easily
subdivided into word-for-word translation, for
instance the English phrase the cabbage may be
translated word-for-word as le chou. However we
note that la is also a perfectly reasonable word-
for-word translation of the, yet la chou is not a
grammatical French string. Even when a phrase
appears compositional, the incorporation of
contextual information often improves translation
</bodyText>
<page confidence="0.970643">
9
</page>
<note confidence="0.9950545">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 9–16,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.903281">
quality. Phrases are a straightforward means of
capturing local context.
</bodyText>
<subsectionHeader confidence="0.479007">
1.2. Theoretical problems with phrasal SMT
</subsectionHeader>
<bodyText confidence="0.978832666666667">
Exact substring match; no discontiguity
Large fixed phrase pairs are effective when an
exact match can be found, but are useless
otherwise. The alignment template approach
(where phrases are modeled in terms of word
classes instead of specific words) provides a
solution at the expense of truly fixed phrases.
Neither phrasal SMT nor alignment templates
allow discontiguous translation pairs.
</bodyText>
<subsectionHeader confidence="0.579789">
Global re-ordering
</subsectionHeader>
<bodyText confidence="0.999807277777778">
Phrases do capture local reordering, but provide
no global re-ordering strategy, and the number of
possible orderings to be considered is not
lessened significantly. Given a sentence of n
words, if the average target phrase length is 4
words (which is unusually high), then the re-
ordering space is reduced from n! to only (n/4)!:
still impractical for exact search in most
sentences. Systems must therefore impose some
limits on phrasal reordering, often hard limits
based on distance as in Koehn et al. (2003) or
some linguistically motivated constraint, such as
ITG (Zens and Ney, 2004). Since these phrases
are not bound by or even related to syntactic
constituents, linguistic generalizations (such as
SVO becoming SOV, or prepositions becoming
postpositions) are not easily incorporated into the
movement models.
</bodyText>
<subsectionHeader confidence="0.880305">
Probability estimation
</subsectionHeader>
<bodyText confidence="0.999818">
To estimate the translation probability of a phrase
pair, several approaches are used, often
concurrently as features in a log-linear model.
Conditional probabilities can be estimated by
maximum likelihood estimation. Yet the phrases
most likely to contribute important translational
and ordering information—the longest ones—are
the ones most subject to sparse data issues.
Alternately, conditional phrasal models can be
constructed from word translation probabilities;
this approach is often called lexical weighting
(Vogel et al. 2003). This avoids sparse data
issues, but tends to prefer literal translations
where the word-for-word probabilities are high
Furthermore most approaches model phrases as
bags of words, and fail to distinguish between
local re-ordering possibilities.
</bodyText>
<subsectionHeader confidence="0.898223">
Partitioning limitation
</subsectionHeader>
<bodyText confidence="0.99988864">
A phrasal approach partitions the sentence into
strings of words, making several questionable
assumptions along the way. First, the probability
of the partitioning is never considered. Long
phrases tend to be rare and therefore have sharp
probability distributions. This adds an inherent
bias toward long phrases with questionable MLE
probabilities (e.g. 1/1 or 2/2). 1
Second, the translation probability of each
phrase pair is modeled independently. Such an
approach fails to model any phenomena that reach
across boundaries; only the target language model
and perhaps whole-sentence bag of words models
cross phrase boundaries. This is especially
important when translating into languages with
agreement phenomena. Often a single phrase does
not cover all agreeing modifiers of a headword;
the uncovered modifiers are biased toward the
most common variant rather than the one agreeing
with its head. Ideally a system would consider
overlapping phrases rather than a single
partitioning, but this poses a problem for
generative models: when words are generated
multiple times by different phrases, they are
effectively penalized.
</bodyText>
<subsectionHeader confidence="0.896314">
1.3. Practical problem with phrases: size
</subsectionHeader>
<bodyText confidence="0.994582421052632">
In addition to the theoretical problems with
phrases, there are also practical issues. While
phrasal systems achieve diminishing returns due
1 The Alignment Template approach differs slightly here.
Phrasal SMT estimates the probability of a phrase pair as:
The Alignment Template method incorporates a loose
partitioning probability by instead estimating the probability
as (in the special case where each word has a unique class):
Note that these counts could differ significantly. Picture a
source phrase that almost always translates into a
discontiguous phrase (e.g. English not becoming French ne
... pas), except for the rare occasion where, due to an
alignment error or odd training data, it translates into a
contiguous phrase (e.g. French ne parle pas). Then the first
probability formulation of ne parle pas given not would be
unreasonably high. However, this is a partial fix since it
again suffers from data sparsity problems, especially on
longer templates where systems hope to achieve the best
benefits from phrases.
</bodyText>
<equation confidence="0.998101538461538">
count s t
( , )
=
φ (  |)
t s
&apos; ( , &apos; )
count s t
Et
p t s =
(  |)
count s t
( , )
count(s)
</equation>
<page confidence="0.950974">
10
</page>
<bodyText confidence="0.999953625">
to sparse data, one does see a small incremental
benefit with increasing phrase lengths. Given that
storing all of these phrases leads to very large
phrase tables, many research systems simply limit
the phrases gathered to those that could possibly
influence some test set. However, this is not
feasible for true production MT systems, since the
data to be translated is unknown.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="method">
2. Previous work
</sectionHeader>
<subsectionHeader confidence="0.999059">
2.1. Delayed phrase construction
</subsectionHeader>
<bodyText confidence="0.999982625">
To avoid the major practical problem of phrasal
SMT—namely large phrase tables, most of which
are not useful to any one sentence—one can
instead construct phrase tables on the fly using an
indexed form of the training data (Zhang and
Vogel 2005; Callison-Burch et al. 2005).
However, this does not relieve any of the
theoretical problems with phrase-based SMT.
</bodyText>
<subsectionHeader confidence="0.932939">
2.2. Syntax-based SMT
</subsectionHeader>
<bodyText confidence="0.999731666666667">
Two recent systems have attempted to address the
contiguity limitation and global re-ordering
problem using syntax-based approaches.
</bodyText>
<subsectionHeader confidence="0.710346">
Hierarchical phrases
</subsectionHeader>
<bodyText confidence="0.999963107142857">
Recent work in the use of hierarchical phrases
(Chiang 2005) improves the ability to capture
linguistic generalizations, and also removes the
limitation to contiguous phrases. Hierarchical
phrases differ from standard phrases in one
important way: in addition to lexical items, a
phrase pair may contain indexed placeholders,
where each index must occur exactly once on
each side. Such a formulation leads to a formally
syntax-based translation approach, where
translation is viewed as a parallel parsing problem
over a grammar with one non-terminal symbol.
This approach significantly outperforms a phrasal
SMT baseline in controlled experimentation.
Hierarchical phrases do address the need for
non-contiguous phrases and suggest a powerful
ordering story in the absence of linguistic
information, although this reordering information
is bound in a deeply lexicalized form. Yet they do
not address the phrase probability estimation
problem; nor do they provide a means of
modeling phenomena across phrase boundaries.
The practical problems with phrase-based
translation systems are further exacerbated, since
the number of translation rules with up to two
non-adjacent non-terminals in a 1-1 monotone
sentence pair of n source and target words is
O(n6), as compared to O(n2) phrases.
</bodyText>
<subsectionHeader confidence="0.658641">
Treelet Translation
</subsectionHeader>
<bodyText confidence="0.999977037037037">
Another means of extending phrase-based
translation is to incorporate source language
syntactic information. In Quirk and Menezes
(2005) we presented an approach to phrasal SMT
based on a parsed dependency tree representation
of the source language. We use a source
dependency parser and project a target
dependency tree using a word-based alignment,
after which we extract tree-based phrases
(‘treelets’) and train a tree-based ordering model.
We showed that using treelets and a tree-based
ordering model results in significantly better
translations than a leading phrase-based system
(Pharaoh, Koehn 2004), keeping all other models
identical.
Like the hierarchical phrase approach, treelet
translation succeeds in improving the global re-
ordering search and allowing discontiguous
phrases, but does not solve the partitioning or
estimation problems. While we found our treelet
system more resistant to degradation at smaller
phrase sizes than the phrase-based system, it
nevertheless suffered significantly at very small
phrase sizes. Thus it is also subject to practical
problems of size, and again these problems are
exacerbated since there are potentially an
exponential number of treelets.
</bodyText>
<subsectionHeader confidence="0.987096">
2.3. Bilingual n-gram channel models
</subsectionHeader>
<bodyText confidence="0.999940214285714">
To address on the problems of estimation and
partitioning, one recent approach transforms
channel modeling into a standard sequence
modeling problem (Banchs et al. 2005). Consider
the following aligned sentence pair in Figure 1a.
In such a well-behaved example, it is natural to
consider the problem in terms of sequence
models. Picture a generative process that
produces a sentence pair in left to right, emitting a
pair of words in lock step. Let M = &lt; m1, ..., mn &gt;
be a sequence of word pairs mi = &lt; s, t &gt;. Then one
can generatively model the probability of an
aligned sentence pair using techniques from n-
gram language modeling:
</bodyText>
<page confidence="0.991808">
11
</page>
<figure confidence="0.652965">
(b) More common non-monotone aligned sentence pair
</figure>
<figureCaption confidence="0.999772">
Figure 1. Example aligned sentence pairs.
</figureCaption>
<equation confidence="0.993647">
P(S, T, A) = P(M)
k
P(mi  |mi−1) 1
1
k
≈ m i−n )
P(mi
1
</equation>
<bodyText confidence="0.999345375">
When an alignment is one-to-one and
monotone, this definition is sufficient. However
alignments are seldom purely one-to-one and
monotone in practice; Figure 1b displays common
behavior such as one-to-many alignments,
inserted words, and non-monotone translation. To
address these problems, Banchs et al. (2005)
suggest defining tuples such that:
</bodyText>
<listItem confidence="0.998154666666667">
(1) the tuple sequence is monotone,
(2) there are no word alignment links between
two distinct tuples,
(3) each tuple has a non-NULL source side,
which may require that target words
aligned to NULL are joined with their
following word, and
(4) no smaller tuples can be extracted without
violating these constraints.
</listItem>
<bodyText confidence="0.99929775">
Note that M is now a sequence of phrase pairs
instead of word pairs. With this adjusted
definition, even Figure 1b can be generated using
the same process using the following tuples:
</bodyText>
<equation confidence="0.9998568">
m1 = &lt; the, l’ &gt;
m2 = &lt; following example, exemple suivant &gt;
m3 = &lt; renames, change le nom &gt;
m4 = &lt; the, de la &gt;
m5 = &lt; table, table &gt;
</equation>
<bodyText confidence="0.999979571428572">
There are several advantages to such an
approach. First, it largely avoids the partitioning
problem; instead of segmenting into potentially
large phrases, the sentence is segmented into
much smaller tuples, most often pairs of single
words. Furthermore the failure to model a
partitioning probability is much more defensible
when the partitions are much smaller. Secondly,
n-gram language model probabilities provide a
robust means of estimating phrasal translation
probabilities in context that models interactions
between all adjacent tuples, obviating the need for
overlapping mappings.
These tuple channel models still must address
practical issues such as model size, though much
work has been done to shrink language models
with minimal impact to perplexity (e.g. Stolcke
1998), which these models could immediately
leverage. Furthermore, these models do not
address the contiguity problem or the global
reordering problem.
</bodyText>
<sectionHeader confidence="0.920303" genericHeader="method">
3. Translation by MTUs
</sectionHeader>
<bodyText confidence="0.999965214285714">
In this paper, we address all four theoretical
problems using a novel combination of our
syntactically-informed treelet approach (Quirk
and Menezes 2005) and a modified version of
bilingual n-gram channel models (Banchs et al.
2005). As in our previous work, we first parse the
sentence into a dependency tree. After this initial
parse, we use a global search to find a candidate
that maximizes a log-linear model, where these
candidates consist of a target word sequence
annotated with a dependency structure, a word
alignment, and a treelet decomposition.
We begin by exploring minimal translation
units and the models that concern them.
</bodyText>
<subsectionHeader confidence="0.98059">
3.1. Minimal Translation Units
</subsectionHeader>
<bodyText confidence="0.999377333333334">
Minimal Translation Units (MTUs) are related to
the tuples of Banchs et al. (2005), but differ in
several important respects. First, we relieve the
restriction that the MTU sequence be monotone.
This prevents spurious expansion of MTUs to
incorporate adjacent context only to satisfy
monotonicity. In the example, note that the
previous algorithm would extract the tuple
&lt;following example, exemple suivant&gt; even though
the translations are mostly independent. Their
partitioning is also context dependent: if the
sentence did not contain the words following or
suivant, then &lt; example, exemple &gt; would be a
single MTU. Secondly we drop the requirement
that no MTU have a NULL source side. While
some insertions can be modeled in terms of
adjacent words, we believe more robust models
can be obtained if we consider insertions as
</bodyText>
<figure confidence="0.411340333333333">
(a) Monotone aligned sentence pair
=
i
=
i
=
</figure>
<page confidence="0.963441">
12
</page>
<table confidence="0.9997808">
English French English Japanese
Training Sentences 300,000 500,000
Words 4,441,465 5,198,932 7,909,198 9,379,240
Vocabulary 63,343 59,290 79,029 95,813
Singletons 35,328 29,448 44,111 52,911
Development test Sentences 200 200
Words 3,045 3,456 3,436 4,095
Test Sentences 2,000 2,000
Words 30,010 34,725 35,556 3,855
OOV rate 5.5% 4.6% 6.9% 6.8%
</table>
<tableCaption confidence="0.996025">
Table 4.1 Data characteristics
</tableCaption>
<bodyText confidence="0.965785">
independent units. In the end our MTUs are
defined quite simply as pairs of source and target
word sets that follow the given constraints:
</bodyText>
<listItem confidence="0.98793775">
(1) there are no word alignment links between
distinct MTUs, and
(2) no smaller MTUs can be extracted without
violating the previous constraint.
</listItem>
<bodyText confidence="0.999901833333333">
Since our word alignment algorithm is able to
produce one-to-one, one-to-many, many-to-one,
one-to-zero, and zero-to-one translations, these
act as our basic units. As an example, let us
consider example (1) once again. Using this new
algorithm, the MTUs would be:
</bodyText>
<equation confidence="0.999921142857143">
m1 = &lt; the, l’ &gt;
m2 = &lt; following, suivant &gt;
m3 = &lt; example, exemple &gt;
m4 = &lt; renames, change le nom &gt;
m5 = &lt; NULL, de &gt;
m6 = &lt; the, la &gt;
m7 = &lt; table, table &gt;
</equation>
<bodyText confidence="0.9999821">
A finer grained partitioning into MTUs further
reduces the data sparsity and partitioning issues
associated with phrases. Yet it poses issues in
modeling translation: given a sequence of MTUs
that does not have a monotone segmentation, how
do we model the probability of an aligned
translation pair? We propose several solutions,
and use each in a log-linear combination of
models.
First, one may walk the MTUs in source order,
ignoring insertion MTUs altogether. Such a
model is completely agnostic of the target word
order; instead of generating an aligned source
target pair, it generates a source sentence along
with a bag of target phrases. This approach
expends a great deal of modeling effort in
regenerating the source sentence, which may not
be altogether desirable, though it does condition
on surrounding translations. Also, it can be
evaluated on candidates before orderings are
considered. This latter property may be useful in
two-stage decoding strategies where translations
are considered before orderings.
Secondly, one may walk the MTUs in target
order, ignoring deletion MTUs. Where the source-
order MTU channel model expends probability
mass generating the source sentence, this model
expends a probability mass generating the target
sentence and therefore may be somewhat
redundant with the target language model.
Finally, one may walk the MTUs in
dependency tree order. Let us assume that in
addition to an aligned source-target candidate
pair, we have a dependency parse of the source
side. Where the past models conditioned on
surface adjacent MTUs, this model conditions on
tree adjacent MTUs. Currently we condition only
on the ancestor chain, where parent1(m) is the
parent MTU of m, parent2(m) is the grandparent
of m, and so on:
</bodyText>
<equation confidence="0.7438775">
P(S, T,A) = P(M) z ∏ P(m  |parent1&amp;quot; (m
M
</equation>
<bodyText confidence="0.999946538461538">
This model hopes to capture information
completely distinct from the other two models,
such as translational preferences contingent on the
head, even in the presence of long distance
dependencies. Note that it generates unordered
dependency tree pairs.
All of these models can be trained from a
parallel corpus that has been word aligned and the
source side dependency parsed. We walk through
each sentence extracting MTUs in source, target,
and tree order. Standard n-gram language
modeling tools can be used to train MTU
language models.
</bodyText>
<subsectionHeader confidence="0.970804">
3.2. Decoding
</subsectionHeader>
<bodyText confidence="0.999671">
We employ a dependency tree-based beam search
decoder to search the space of translations. First
the input is parsed into a dependency tree
</bodyText>
<equation confidence="0.661895">
))
mE
</equation>
<page confidence="0.990135">
13
</page>
<bodyText confidence="0.999946529411765">
structure. For each input node in the dependency
tree, an n-best list of candidates is produced.
Candidates consist of a target dependency tree
along with a treelet and word alignment. The
decoder generally assumes phrasal cohesion:
candidates covering a substring (not subsequence)
of the input sentence produce a potential substring
(not subsequence) of the final translation. In
addition to allowing a DP / beam decoder, this
allows us to evaluate string-based models (such as
the target language model and the source and
target order MTU n-gram models) on partial
candidates. This decoder is unchanged from our
previous work: the MTU n-gram models are
simply incorporated as feature functions in the
log-linear combination. In the experiments section
the MTU models are referred to as model set (1).
</bodyText>
<subsectionHeader confidence="0.740544">
3.3. Other translation models
Phrasal channel models
</subsectionHeader>
<bodyText confidence="0.9982955">
We can estimate traditional channel models using
maximum likelihood or lexical weighting:
</bodyText>
<equation confidence="0.981652833333333">
fDirectMLE
fInverseMLE
)= ∏ ∏ p t s
(  |)
(σ,τ)∈ treelets(A) t∈τ s∈ σ
(σ,τ)∈ treelets(A)s∈σ t∈τ
</equation>
<bodyText confidence="0.9999534">
We use word probability tables p(t  |s) and p(s  |t)
estimated by IBM Model 1 (Brown et al. 1993).
Such models can be built over phrases if used in a
phrasal decoder or over treelets if used in a treelet
decoder. These models are referred to as set (2).
</bodyText>
<subsectionHeader confidence="0.634791">
Word-based models
</subsectionHeader>
<bodyText confidence="0.999992333333333">
A target language model using modified Kneser-
Ney smoothing captures fluency; a word count
feature offsets the target LM preference for
shorter selections; and a treelet/phrase count helps
bias toward translations using fewer phrases.
These models are referred to as set (3).
</bodyText>
<equation confidence="0.99555">
 ||
T
) =∏ P(ti  |ti−n )
</equation>
<subsectionHeader confidence="0.691773">
Syntactic models
</subsectionHeader>
<bodyText confidence="0.999241666666667">
As in Quirk and Menezes (2005), we include a
linguistically-informed order model that predicts
the head-relative position of each node
independently, and a tree-based bigram target
language model; these models are referred to as
set (4).
</bodyText>
<equation confidence="0.9905595">
∏ (
P position
t∈
P t parent t
(  |( ))
t∈
</equation>
<sectionHeader confidence="0.984193" genericHeader="method">
4. Experimental setup
</sectionHeader>
<bodyText confidence="0.9999234">
We evaluate the translation quality of the system
using the BLEU metric (Papineni et al., 02) under
a variety of configurations. As an additional
baseline, we compare against a phrasal SMT
decoder, Pharaoh (Koehn et al. 2003).
</bodyText>
<subsectionHeader confidence="0.954616">
4.1. Data
</subsectionHeader>
<bodyText confidence="0.999991166666667">
Two language pairs were used for this
comparison: English to French, and English to
Japanese. The data was selected from technical
software documentation including software
manuals and product support articles; Table 4.1
presents the major characteristics of this data.
</bodyText>
<subsectionHeader confidence="0.974606">
4.2. Training
</subsectionHeader>
<bodyText confidence="0.999668571428571">
We parsed the source (English) side of the
corpora using NLPWIN, a broad-coverage rule-
based parser able to produce syntactic analyses at
varying levels of depth (Heidorn 2002). For the
purposes of these experiments we used a
dependency tree output with part-of-speech tags
and unstemmed surface words. Word alignments
were produced by GIZA++ (Och and Ney 2003)
with a standard training regimen of five iterations
of Model 1, five iterations of the HMM Model,
and five iterations of Model 4, in both directions.
These alignments were combined heuristically as
described in our previous work.
We then projected the dependency trees and
used the aligned dependency tree pairs to extract
treelet translation pairs, train the order model, and
train MTU models. The target language models
were trained using only the target side of the
corpus. Finally we trained model weights by
maximizing BLEU (Och 2003) and set decoder
optimization parameters (n-best list size, timeouts
</bodyText>
<figure confidence="0.84273193877551">
fInverseM1
= ∏ ∏
S T A p s t
( , , )
(  |)
=
∏
( , , )
S T A
( , )
σ τ ∈ treelets(A)
=
( , , )
S T A
∏
c
(*, )
τ
( , )
σ τ ∈ treelets(A)
( , )
σ τ
c
( , )
σ τ
c
(,*)
σ
c
( S T A
, ,
fDirectM1
( S T A
, ,
ftargetLM
fwordcount (S, T, A) = |T
fphrasecount (S,T,A) = |treelets(A) |
i
=
1
forder ( , , ) =
ftreeLM S T A =
( , , )
S T A
∏
T
T
( )  |, , )
t S T A
</figure>
<page confidence="0.988495">
14
</page>
<table confidence="0.999646090909091">
EF EJ
Phrasal decoder (Pharaoh) 45.8±2.0 32.9±0.9
Model sets (2),(3)
Treelet decoder, without discontiguous mappings
Model sets (2),(3) 45.1±2.1 33.2±0.9
Model sets (2),(3),(4) 48.4±2.0 34.8±0.9
Treelet decoder, with discontiguous mappings
Model sets (2),(3) 46.4±2.1 34.3 ±0.9
Model sets (2),(3),(4) 48.7±2.1 34.9 ±0.9
Model sets (1),(3),(4) 49.6±2.1 33.9 ±0.8
Model sets (1)-(4) 50.5±2.1 36.2 ±0.9
</table>
<tableCaption confidence="0.998959">
Table 5.1. Broad system comparison.
</tableCaption>
<bodyText confidence="0.9689818125">
etc) on a development test set of 200 held-out
sentences each with a single reference translation.
Parameters were individually estimated for each
distinct configuration.
Pharaoh
The same GIZA++ alignments as above were
used in the Pharaoh decoder (Koehn 2004). We
used the heuristic combination described in (Och
and Ney 2003) and extracted phrasal translation
pairs from this combined alignment as described
in (Koehn et al., 2003). Aside from MTU models
and syntactic models (Pharaoh uses its own
ordering approach), the same models were used:
MLE and lexical weighting channel models,
target LM, and phrase and word count. Model
weights were also trained following Och (2003).
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="evaluation">
5. Results
</sectionHeader>
<bodyText confidence="0.999125166666667">
We begin with a broad brush comparison of
systems in Table 5.1. Throughout this section,
treelet and phrase sizes are measured in terms of
MTUs, not words. By default, all systems
(including Pharaoh) use treelets or phrases of up
to four MTUs, and MTU bigram models. The first
results reiterate that the introduction of
discontiguous mappings and especially a
linguistically motivated order model (model set
(4)) can improve translation quality. Replacing
the standard channel models (model set (2)) with
MTU bigram models (model set (1)) does not
</bodyText>
<tableCaption confidence="0.918757">
Table 5.2. Varying MTU n-gram model order.
</tableCaption>
<bodyText confidence="0.999845444444445">
appear to degrade quality; it even seems to boost
quality on EF. Furthermore, the information in the
MTU models appears somewhat orthogonal to the
phrasal models; a combination results in
improvements for both language pairs.
The experiments in Table 5.2 compare quality
using different orders of MTU n-gram models.
(Treelets containing up to four MTUs were still
used as the basis for decoding; only the order of
the MTU n-gram models was adjusted.) A
unigram model performs surprisingly well. This
supports our intuition that atomic handling of
non-compositional multi-word translations is a
major contribution of phrasal SMT. Furthermore
bigram models increase translation quality
supporting the claim that local context is another
contribution. Models beyond bigrams had little
impact presumably due to sparsity and smoothing.
Table 5.3 explores the impact of using different
phrase/treelet sizes in decoding. We see that
adding MTU models makes translation more
resilient given smaller phrases. The poor
performance at size 1 is not particularly
surprising: both systems require insertions to be
lexically anchored: the only decoding operation
allowed is translation of some visible source
phrase, and insertions have no visible trace.
</bodyText>
<sectionHeader confidence="0.997921" genericHeader="conclusions">
6. Conclusions
</sectionHeader>
<subsectionHeader confidence="0.514856">
In this paper we have teased apart the role of
</subsectionHeader>
<figure confidence="0.995372586206896">
Treelet decoder, model sets (1),(3),(4)
Treelet decoder,
MTU unigram
MTU bigram
MTU trigram
MTU 4-gram
MTU unigram
MTU bigram
MTU trigram
MTU 4-gram
model sets (1)-(4)
47.8±2.1
49.6±2.1
49.9±2.0
49.6±2.1
48.6±2.1
48.9±2.0
50.5±2.1
50.4±2.0
EF
33.2±0.9
33.9±0.8
34.0±0.9
34.1±0.9
34.3±1.0
36.2±0.9
36.1±0.9
36.2±1.0
EJ
</figure>
<tableCaption confidence="0.998768">
Table 5.3. Varying phrase / treelet size.
</tableCaption>
<table confidence="0.95172">
Phrasal decoder Treelet decoder: MTU bigram Treelet decoder: MTU bigram
model sets (2),(3) model sets (1),(3),(4) model sets (1)-(4)
Size EF EJ EF EJ EF EJ
1 32.6±1.8 20.5±0.7 26.3±1.3 15.4±0.7 29.8±1.4 16.7±0.7
2 40.4±1.9 29.7±0.7 48.7±2.1 32.4±0.9 47.7±2.1 33.8±0.8
3 44.3±2.1 30.7±0.9 48.5±2.0 34.6±0.9 48.5±2.0 35.1±0.9
4 45.8±2.0 32.9±0.9 49.6±2.1 33.9±0.8 50.5±2.1 36.2±0.9
</table>
<page confidence="0.9945">
15
</page>
<bodyText confidence="0.999977703703704">
phrases and handled each contribution via a
distinct model best suited to the task. Non-
compositional translations stay as MTU phrases.
Context and robust estimation is provided by
MTU-based n-gram models. Local and global
ordering is handled by a tree-based model.
The first interesting result is that at normal
phrase sizes, augmenting an SMT system with
MTU n-gram models improves quality; whereas
replacing the standard phrasal channel models by
the more theoretically sound MTU n-gram
channel models leads to very similar
performance.
Even more interesting are the results on smaller
phrases. A system using very small phrases (size
2) and MTU bigram models matches (English-
French) or at least approaches (English-Japanese)
the performance of the baseline system using
large phrases (size 4). While this work does not
yet obviate the need for phrases, we consider it a
promising step in that direction.
An immediate practical benefit is that it allows
systems to use much smaller phrases (and hence
smaller phrase tables) with little or no loss in
quality. This result is particularly important for
syntax-based systems, or any system that allows
discontiguous phrases. Given a fixed length limit,
the number of surface phrases extracted from any
sentence pair of length n where all words are
uniquely aligned is O(n), but the number of
treelets is potentially exponential in the number of
children; and the number of rules with two gaps
extracted by Chiang (2005) is potentially O(n3).
Our results using MTUs suggest that such
systems can avoid unwieldy, poorly estimated
long phrases and instead anchor decoding on
shorter, more tractable knowledge units such as
MTUs, incorporating channel model information
and contextual knowledge with an MTU n-gram
model.
Much future work does remain. From
inspecting the model weights of the best systems,
we note that only the source order MTU n-gram
model has a major contribution to the overall
score of a given candidate. This suggests that the
three distinct models, despite their different walk
orders, are somewhat redundant. We plan to
consider other approaches for conditioning on
context. Furthermore phrasal channel models, in
spite of the laundry list of problems presented
here, have a significant impact on translation
quality. We hope to replace them with effective
models without the brittleness and sparsity issues
of heavy lexicalization.
</bodyText>
<sectionHeader confidence="0.998477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975036">
Banchs, Rafael, Josep Crego, Adrià de Gispert, Patrik
Lambert, and Jose Mariño. 2005. Statistical machine
translation of Euparl data by using bilingual n-grams. In
Proceedings of ACL Workshop on Building and Using
Parallel Texts.
Brown, Peter, Vincent Della Pietra, Stephen Della Pietra, and
Robert Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computational
Linguistics 19(2): 263-311.
Callison-Burch, Chris, Colin Bannard, and Josh Schroeder.
2005. Scaling phrase-based machine translation to larger
corpora and longer phrases. In Proceedings of ACL.
Chiang, David. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL.
Heidorn, George. 2000. “Intelligent writing assistance”. In
Dale et al. Handbook of Natural Language Processing,
Marcel Dekker.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase based translation. In Proceedings of
NAACL.
Koehn, Philipp. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of AMTA.
Och, Franz Josef and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1): 19-51.
Och, Franz Josef and Hermann Ney. 2004. The Alignment
Template approach to statistical machine translation,
Computational Linguistics, 30(4):417-450.
Och, Franz Josef. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of
machine translation. In Proceedings of ACL.
Quirk, Chris and Arul Menezes. 2005. Dependency tree
translation: syntactically-informed phrasal SMT. In
Proceedings of ACL.
Stolcke, Andreas. 1998. Entropy-based pruning of backoff
language models. In Proceedings of DARPA Broadcast
News Transcription and Understanding.
Vogel, Stephan, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venugopal, Bing Zhao, Alex Waibel. 2003. The
CMU statistical machine translation system. In
Proceedings of MT Summit.
Zens, Richard, and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In Proceedings of ACL.
Zhang, Ying and Stephan Vogel. 2005. An efficient phrase-
to-phrase alignment model for arbitrarily long phrase and
large corpora. In Proceedings of EAMT.
</reference>
<page confidence="0.9987">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.659796">
<title confidence="0.9860645">Do we need phrases? Challenging the conventional wisdom in Machine Translation</title>
<author confidence="0.895028">Quirk</author>
<affiliation confidence="0.902621">Microsoft</affiliation>
<address confidence="0.8756885">One Microsoft Redmond, WA 98052</address>
<email confidence="0.999656">chrisq@microsoft.com</email>
<email confidence="0.999656">arulm@microsoft.com</email>
<abstract confidence="0.999416923076923">We begin by exploring theoretical and practical issues with phrasal SMT, several of which are addressed by syntax-based SMT. Next, to address problems not handled by syntax, we propose the concept of a Minimal Translation Unit (MTU) and develop MTU sequence models. Finally we incorporate these models into a syntax-based SMT system and demonstrate that it improves on the state of the art translation quality within a theoretically more desirable framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rafael Banchs</author>
<author>Josep Crego</author>
<author>Adrià de Gispert</author>
<author>Patrik Lambert</author>
<author>Jose Mariño</author>
</authors>
<title>Statistical machine translation of Euparl data by using bilingual n-grams.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Building and Using Parallel Texts.</booktitle>
<marker>Banchs, Crego, de Gispert, Lambert, Mariño, 2005</marker>
<rawString>Banchs, Rafael, Josep Crego, Adrià de Gispert, Patrik Lambert, and Jose Mariño. 2005. Statistical machine translation of Euparl data by using bilingual n-grams. In Proceedings of ACL Workshop on Building and Using Parallel Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Vincent Della Pietra</author>
<author>Stephen Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="20611" citStr="Brown et al. 1993" startWordPosition="3204" endWordPosition="3207">e and target order MTU n-gram models) on partial candidates. This decoder is unchanged from our previous work: the MTU n-gram models are simply incorporated as feature functions in the log-linear combination. In the experiments section the MTU models are referred to as model set (1). 3.3. Other translation models Phrasal channel models We can estimate traditional channel models using maximum likelihood or lexical weighting: fDirectMLE fInverseMLE )= ∏ ∏ p t s ( |) (σ,τ)∈ treelets(A) t∈τ s∈ σ (σ,τ)∈ treelets(A)s∈σ t∈τ We use word probability tables p(t |s) and p(s |t) estimated by IBM Model 1 (Brown et al. 1993). Such models can be built over phrases if used in a phrasal decoder or over treelets if used in a treelet decoder. These models are referred to as set (2). Word-based models A target language model using modified KneserNey smoothing captures fluency; a word count feature offsets the target LM preference for shorter selections; and a treelet/phrase count helps bias toward translations using fewer phrases. These models are referred to as set (3). || T ) =∏ P(ti |ti−n ) Syntactic models As in Quirk and Menezes (2005), we include a linguistically-informed order model that predicts the head-relati</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter, Vincent Della Pietra, Stephen Della Pietra, and Robert Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling phrase-based machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="8817" citStr="Callison-Burch et al. 2005" startWordPosition="1340" endWordPosition="1343">ase lengths. Given that storing all of these phrases leads to very large phrase tables, many research systems simply limit the phrases gathered to those that could possibly influence some test set. However, this is not feasible for true production MT systems, since the data to be translated is unknown. 2. Previous work 2.1. Delayed phrase construction To avoid the major practical problem of phrasal SMT—namely large phrase tables, most of which are not useful to any one sentence—one can instead construct phrase tables on the fly using an indexed form of the training data (Zhang and Vogel 2005; Callison-Burch et al. 2005). However, this does not relieve any of the theoretical problems with phrase-based SMT. 2.2. Syntax-based SMT Two recent systems have attempted to address the contiguity limitation and global re-ordering problem using syntax-based approaches. Hierarchical phrases Recent work in the use of hierarchical phrases (Chiang 2005) improves the ability to capture linguistic generalizations, and also removes the limitation to contiguous phrases. Hierarchical phrases differ from standard phrases in one important way: in addition to lexical items, a phrase pair may contain indexed placeholders, where each</context>
</contexts>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Callison-Burch, Chris, Colin Bannard, and Josh Schroeder. 2005. Scaling phrase-based machine translation to larger corpora and longer phrases. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1067" citStr="Chiang 2005" startWordPosition="156" endWordPosition="157">evelop MTU sequence models. Finally we incorporate these models into a syntax-based SMT system and demonstrate that it improves on the state of the art translation quality within a theoretically more desirable framework. 1. Introduction The last several years have seen phrasal statistical machine translation (SMT) systems outperform word-based approaches by a wide margin (Koehn 2003). Unfortunately the use of phrases in SMT is beset by a number of difficult theoretical and practical problems, which we attempt to characterize below. Recent research into syntaxbased SMT (Quirk and Menezes 2005; Chiang 2005) has produced promising results in addressing some of the problems; research motivated by other statistical models has helped to address others (Banchs et al. 2005). We refine and unify two threads of research in an attempt to address all of these problems simultaneously. Such an approach proves both theoretically more desirable and empirically superior. In brief, Phrasal SMT systems employ phrase pairs automatically extracted from parallel corpora. To translate, a source sentence is first partitioned into a sequence of phrases I = s1...sI. Each source phrase si is then translated into a targe</context>
<context position="9141" citStr="Chiang 2005" startWordPosition="1387" endWordPosition="1388">e construction To avoid the major practical problem of phrasal SMT—namely large phrase tables, most of which are not useful to any one sentence—one can instead construct phrase tables on the fly using an indexed form of the training data (Zhang and Vogel 2005; Callison-Burch et al. 2005). However, this does not relieve any of the theoretical problems with phrase-based SMT. 2.2. Syntax-based SMT Two recent systems have attempted to address the contiguity limitation and global re-ordering problem using syntax-based approaches. Hierarchical phrases Recent work in the use of hierarchical phrases (Chiang 2005) improves the ability to capture linguistic generalizations, and also removes the limitation to contiguous phrases. Hierarchical phrases differ from standard phrases in one important way: in addition to lexical items, a phrase pair may contain indexed placeholders, where each index must occur exactly once on each side. Such a formulation leads to a formally syntax-based translation approach, where translation is viewed as a parallel parsing problem over a grammar with one non-terminal symbol. This approach significantly outperforms a phrasal SMT baseline in controlled experimentation. Hierarch</context>
<context position="28484" citStr="Chiang (2005)" startWordPosition="4497" endWordPosition="4498"> we consider it a promising step in that direction. An immediate practical benefit is that it allows systems to use much smaller phrases (and hence smaller phrase tables) with little or no loss in quality. This result is particularly important for syntax-based systems, or any system that allows discontiguous phrases. Given a fixed length limit, the number of surface phrases extracted from any sentence pair of length n where all words are uniquely aligned is O(n), but the number of treelets is potentially exponential in the number of children; and the number of rules with two gaps extracted by Chiang (2005) is potentially O(n3). Our results using MTUs suggest that such systems can avoid unwieldy, poorly estimated long phrases and instead anchor decoding on shorter, more tractable knowledge units such as MTUs, incorporating channel model information and contextual knowledge with an MTU n-gram model. Much future work does remain. From inspecting the model weights of the best systems, we note that only the source order MTU n-gram model has a major contribution to the overall score of a given candidate. This suggests that the three distinct models, despite their different walk orders, are somewhat r</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, David. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Heidorn</author>
</authors>
<title>Intelligent writing assistance”.</title>
<date>2000</date>
<booktitle>In Dale et al. Handbook of Natural Language Processing,</booktitle>
<publisher>Marcel Dekker.</publisher>
<marker>Heidorn, 2000</marker>
<rawString>Heidorn, George. 2000. “Intelligent writing assistance”. In Dale et al. Handbook of Natural Language Processing, Marcel Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="4675" citStr="Koehn et al. (2003)" startWordPosition="700" endWordPosition="703">rases. Neither phrasal SMT nor alignment templates allow discontiguous translation pairs. Global re-ordering Phrases do capture local reordering, but provide no global re-ordering strategy, and the number of possible orderings to be considered is not lessened significantly. Given a sentence of n words, if the average target phrase length is 4 words (which is unusually high), then the reordering space is reduced from n! to only (n/4)!: still impractical for exact search in most sentences. Systems must therefore impose some limits on phrasal reordering, often hard limits based on distance as in Koehn et al. (2003) or some linguistically motivated constraint, such as ITG (Zens and Ney, 2004). Since these phrases are not bound by or even related to syntactic constituents, linguistic generalizations (such as SVO becoming SOV, or prepositions becoming postpositions) are not easily incorporated into the movement models. Probability estimation To estimate the translation probability of a phrase pair, several approaches are used, often concurrently as features in a log-linear model. Conditional probabilities can be estimated by maximum likelihood estimation. Yet the phrases most likely to contribute important</context>
<context position="21628" citStr="Koehn et al. 2003" startWordPosition="3376" endWordPosition="3379"> phrases. These models are referred to as set (3). || T ) =∏ P(ti |ti−n ) Syntactic models As in Quirk and Menezes (2005), we include a linguistically-informed order model that predicts the head-relative position of each node independently, and a tree-based bigram target language model; these models are referred to as set (4). ∏ ( P position t∈ P t parent t ( |( )) t∈ 4. Experimental setup We evaluate the translation quality of the system using the BLEU metric (Papineni et al., 02) under a variety of configurations. As an additional baseline, we compare against a phrasal SMT decoder, Pharaoh (Koehn et al. 2003). 4.1. Data Two language pairs were used for this comparison: English to French, and English to Japanese. The data was selected from technical software documentation including software manuals and product support articles; Table 4.1 presents the major characteristics of this data. 4.2. Training We parsed the source (English) side of the corpora using NLPWIN, a broad-coverage rulebased parser able to produce syntactic analyses at varying levels of depth (Heidorn 2002). For the purposes of these experiments we used a dependency tree output with part-of-speech tags and unstemmed surface words. Wo</context>
<context position="24120" citStr="Koehn et al., 2003" startWordPosition="3823" endWordPosition="3826"> (2),(3) 46.4±2.1 34.3 ±0.9 Model sets (2),(3),(4) 48.7±2.1 34.9 ±0.9 Model sets (1),(3),(4) 49.6±2.1 33.9 ±0.8 Model sets (1)-(4) 50.5±2.1 36.2 ±0.9 Table 5.1. Broad system comparison. etc) on a development test set of 200 held-out sentences each with a single reference translation. Parameters were individually estimated for each distinct configuration. Pharaoh The same GIZA++ alignments as above were used in the Pharaoh decoder (Koehn 2004). We used the heuristic combination described in (Och and Ney 2003) and extracted phrasal translation pairs from this combined alignment as described in (Koehn et al., 2003). Aside from MTU models and syntactic models (Pharaoh uses its own ordering approach), the same models were used: MLE and lexical weighting channel models, target LM, and phrase and word count. Model weights were also trained following Och (2003). 5. Results We begin with a broad brush comparison of systems in Table 5.1. Throughout this section, treelet and phrase sizes are measured in terms of MTUs, not words. By default, all systems (including Pharaoh) use treelets or phrases of up to four MTUs, and MTU bigram models. The first results reiterate that the introduction of discontiguous mapping</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase based translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="10995" citStr="Koehn 2004" startWordPosition="1656" endWordPosition="1657">on Another means of extending phrase-based translation is to incorporate source language syntactic information. In Quirk and Menezes (2005) we presented an approach to phrasal SMT based on a parsed dependency tree representation of the source language. We use a source dependency parser and project a target dependency tree using a word-based alignment, after which we extract tree-based phrases (‘treelets’) and train a tree-based ordering model. We showed that using treelets and a tree-based ordering model results in significantly better translations than a leading phrase-based system (Pharaoh, Koehn 2004), keeping all other models identical. Like the hierarchical phrase approach, treelet translation succeeds in improving the global reordering search and allowing discontiguous phrases, but does not solve the partitioning or estimation problems. While we found our treelet system more resistant to degradation at smaller phrase sizes than the phrase-based system, it nevertheless suffered significantly at very small phrase sizes. Thus it is also subject to practical problems of size, and again these problems are exacerbated since there are potentially an exponential number of treelets. 2.3. Bilingu</context>
<context position="23947" citStr="Koehn 2004" startWordPosition="3798" endWordPosition="3799"> without discontiguous mappings Model sets (2),(3) 45.1±2.1 33.2±0.9 Model sets (2),(3),(4) 48.4±2.0 34.8±0.9 Treelet decoder, with discontiguous mappings Model sets (2),(3) 46.4±2.1 34.3 ±0.9 Model sets (2),(3),(4) 48.7±2.1 34.9 ±0.9 Model sets (1),(3),(4) 49.6±2.1 33.9 ±0.8 Model sets (1)-(4) 50.5±2.1 36.2 ±0.9 Table 5.1. Broad system comparison. etc) on a development test set of 200 held-out sentences each with a single reference translation. Parameters were individually estimated for each distinct configuration. Pharaoh The same GIZA++ alignments as above were used in the Pharaoh decoder (Koehn 2004). We used the heuristic combination described in (Och and Ney 2003) and extracted phrasal translation pairs from this combined alignment as described in (Koehn et al., 2003). Aside from MTU models and syntactic models (Pharaoh uses its own ordering approach), the same models were used: MLE and lexical weighting channel models, target LM, and phrase and word count. Model weights were also trained following Och (2003). 5. Results We begin with a broad brush comparison of systems in Table 5.1. Throughout this section, treelet and phrase sizes are measured in terms of MTUs, not words. By default, </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Koehn, Philipp. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="22284" citStr="Och and Ney 2003" startWordPosition="3476" endWordPosition="3479">d for this comparison: English to French, and English to Japanese. The data was selected from technical software documentation including software manuals and product support articles; Table 4.1 presents the major characteristics of this data. 4.2. Training We parsed the source (English) side of the corpora using NLPWIN, a broad-coverage rulebased parser able to produce syntactic analyses at varying levels of depth (Heidorn 2002). For the purposes of these experiments we used a dependency tree output with part-of-speech tags and unstemmed surface words. Word alignments were produced by GIZA++ (Och and Ney 2003) with a standard training regimen of five iterations of Model 1, five iterations of the HMM Model, and five iterations of Model 4, in both directions. These alignments were combined heuristically as described in our previous work. We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs, train the order model, and train MTU models. The target language models were trained using only the target side of the corpus. Finally we trained model weights by maximizing BLEU (Och 2003) and set decoder optimization parameters (n-best list size, </context>
<context position="24014" citStr="Och and Ney 2003" startWordPosition="3807" endWordPosition="3810">3.2±0.9 Model sets (2),(3),(4) 48.4±2.0 34.8±0.9 Treelet decoder, with discontiguous mappings Model sets (2),(3) 46.4±2.1 34.3 ±0.9 Model sets (2),(3),(4) 48.7±2.1 34.9 ±0.9 Model sets (1),(3),(4) 49.6±2.1 33.9 ±0.8 Model sets (1)-(4) 50.5±2.1 36.2 ±0.9 Table 5.1. Broad system comparison. etc) on a development test set of 200 held-out sentences each with a single reference translation. Parameters were individually estimated for each distinct configuration. Pharaoh The same GIZA++ alignments as above were used in the Pharaoh decoder (Koehn 2004). We used the heuristic combination described in (Och and Ney 2003) and extracted phrasal translation pairs from this combined alignment as described in (Koehn et al., 2003). Aside from MTU models and syntactic models (Pharaoh uses its own ordering approach), the same models were used: MLE and lexical weighting channel models, target LM, and phrase and word count. Model weights were also trained following Och (2003). 5. Results We begin with a broad brush comparison of systems in Table 5.1. Throughout this section, treelet and phrase sizes are measured in terms of MTUs, not words. By default, all systems (including Pharaoh) use treelets or phrases of up to fo</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1): 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template approach to statistical machine translation,</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--4</pages>
<contexts>
<context position="2015" citStr="Och and Ney 2004" startWordPosition="302" endWordPosition="305">and empirically superior. In brief, Phrasal SMT systems employ phrase pairs automatically extracted from parallel corpora. To translate, a source sentence is first partitioned into a sequence of phrases I = s1...sI. Each source phrase si is then translated into a target phrase ti. Finally the target phrases are permuted, and the translation is read off in order. Beam search is used to approximate the optimal translation. We refer the reader to Keohn et al. (2003) for a detailed description. Unless otherwise noted, the following discussion is generally applicable to Alignment Template systems (Och and Ney 2004) as well. 1.1. Advantages of phrasal SMT Non-compositionality Phrases capture the translations of idiomatic and other non-compositional fixed phrases as a unit, side-stepping the need to awkwardly reconstruct them word by word. While many words can be translated into a single target word, common everyday phrases such as the English password translating as the French mot de passe cannot be easily subdivided. Allowing such translations to be first class entities simplifies translation implementation and improves translation quality. Local re-ordering Phrases provide memorized re-ordering decisio</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2004. The Alignment Template approach to statistical machine translation, Computational Linguistics, 30(4):417-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22824" citStr="Och 2003" startWordPosition="3565" endWordPosition="3566">ace words. Word alignments were produced by GIZA++ (Och and Ney 2003) with a standard training regimen of five iterations of Model 1, five iterations of the HMM Model, and five iterations of Model 4, in both directions. These alignments were combined heuristically as described in our previous work. We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs, train the order model, and train MTU models. The target language models were trained using only the target side of the corpus. Finally we trained model weights by maximizing BLEU (Och 2003) and set decoder optimization parameters (n-best list size, timeouts fInverseM1 = ∏ ∏ S T A p s t ( , , ) ( |) = ∏ ( , , ) S T A ( , ) σ τ ∈ treelets(A) = ( , , ) S T A ∏ c (*, ) τ ( , ) σ τ ∈ treelets(A) ( , ) σ τ c ( , ) σ τ c (,*) σ c ( S T A , , fDirectM1 ( S T A , , ftargetLM fwordcount (S, T, A) = |T fphrasecount (S,T,A) = |treelets(A) | i = 1 forder ( , , ) = ftreeLM S T A = ( , , ) S T A ∏ T T ( ) |, , ) t S T A 14 EF EJ Phrasal decoder (Pharaoh) 45.8±2.0 32.9±0.9 Model sets (2),(3) Treelet decoder, without discontiguous mappings Model sets (2),(3) 45.1±2.1 33.2±0.9 Model sets (2),(3),</context>
<context position="24366" citStr="Och (2003)" startWordPosition="3864" endWordPosition="3865">single reference translation. Parameters were individually estimated for each distinct configuration. Pharaoh The same GIZA++ alignments as above were used in the Pharaoh decoder (Koehn 2004). We used the heuristic combination described in (Och and Ney 2003) and extracted phrasal translation pairs from this combined alignment as described in (Koehn et al., 2003). Aside from MTU models and syntactic models (Pharaoh uses its own ordering approach), the same models were used: MLE and lexical weighting channel models, target LM, and phrase and word count. Model weights were also trained following Och (2003). 5. Results We begin with a broad brush comparison of systems in Table 5.1. Throughout this section, treelet and phrase sizes are measured in terms of MTUs, not words. By default, all systems (including Pharaoh) use treelets or phrases of up to four MTUs, and MTU bigram models. The first results reiterate that the introduction of discontiguous mappings and especially a linguistically motivated order model (model set (4)) can improve translation quality. Replacing the standard channel models (model set (2)) with MTU bigram models (model set (1)) does not Table 5.2. Varying MTU n-gram model ord</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz Josef. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Dependency tree translation: syntactically-informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1053" citStr="Quirk and Menezes 2005" startWordPosition="152" endWordPosition="155">slation Unit (MTU) and develop MTU sequence models. Finally we incorporate these models into a syntax-based SMT system and demonstrate that it improves on the state of the art translation quality within a theoretically more desirable framework. 1. Introduction The last several years have seen phrasal statistical machine translation (SMT) systems outperform word-based approaches by a wide margin (Koehn 2003). Unfortunately the use of phrases in SMT is beset by a number of difficult theoretical and practical problems, which we attempt to characterize below. Recent research into syntaxbased SMT (Quirk and Menezes 2005; Chiang 2005) has produced promising results in addressing some of the problems; research motivated by other statistical models has helped to address others (Banchs et al. 2005). We refine and unify two threads of research in an attempt to address all of these problems simultaneously. Such an approach proves both theoretically more desirable and empirically superior. In brief, Phrasal SMT systems employ phrase pairs automatically extracted from parallel corpora. To translate, a source sentence is first partitioned into a sequence of phrases I = s1...sI. Each source phrase si is then translate</context>
<context position="10523" citStr="Quirk and Menezes (2005)" startWordPosition="1584" endWordPosition="1587">dering information is bound in a deeply lexicalized form. Yet they do not address the phrase probability estimation problem; nor do they provide a means of modeling phenomena across phrase boundaries. The practical problems with phrase-based translation systems are further exacerbated, since the number of translation rules with up to two non-adjacent non-terminals in a 1-1 monotone sentence pair of n source and target words is O(n6), as compared to O(n2) phrases. Treelet Translation Another means of extending phrase-based translation is to incorporate source language syntactic information. In Quirk and Menezes (2005) we presented an approach to phrasal SMT based on a parsed dependency tree representation of the source language. We use a source dependency parser and project a target dependency tree using a word-based alignment, after which we extract tree-based phrases (‘treelets’) and train a tree-based ordering model. We showed that using treelets and a tree-based ordering model results in significantly better translations than a leading phrase-based system (Pharaoh, Koehn 2004), keeping all other models identical. Like the hierarchical phrase approach, treelet translation succeeds in improving the globa</context>
<context position="14493" citStr="Quirk and Menezes 2005" startWordPosition="2213" endWordPosition="2216">context that models interactions between all adjacent tuples, obviating the need for overlapping mappings. These tuple channel models still must address practical issues such as model size, though much work has been done to shrink language models with minimal impact to perplexity (e.g. Stolcke 1998), which these models could immediately leverage. Furthermore, these models do not address the contiguity problem or the global reordering problem. 3. Translation by MTUs In this paper, we address all four theoretical problems using a novel combination of our syntactically-informed treelet approach (Quirk and Menezes 2005) and a modified version of bilingual n-gram channel models (Banchs et al. 2005). As in our previous work, we first parse the sentence into a dependency tree. After this initial parse, we use a global search to find a candidate that maximizes a log-linear model, where these candidates consist of a target word sequence annotated with a dependency structure, a word alignment, and a treelet decomposition. We begin by exploring minimal translation units and the models that concern them. 3.1. Minimal Translation Units Minimal Translation Units (MTUs) are related to the tuples of Banchs et al. (2005)</context>
<context position="21131" citStr="Quirk and Menezes (2005)" startWordPosition="3293" endWordPosition="3296">)s∈σ t∈τ We use word probability tables p(t |s) and p(s |t) estimated by IBM Model 1 (Brown et al. 1993). Such models can be built over phrases if used in a phrasal decoder or over treelets if used in a treelet decoder. These models are referred to as set (2). Word-based models A target language model using modified KneserNey smoothing captures fluency; a word count feature offsets the target LM preference for shorter selections; and a treelet/phrase count helps bias toward translations using fewer phrases. These models are referred to as set (3). || T ) =∏ P(ti |ti−n ) Syntactic models As in Quirk and Menezes (2005), we include a linguistically-informed order model that predicts the head-relative position of each node independently, and a tree-based bigram target language model; these models are referred to as set (4). ∏ ( P position t∈ P t parent t ( |( )) t∈ 4. Experimental setup We evaluate the translation quality of the system using the BLEU metric (Papineni et al., 02) under a variety of configurations. As an additional baseline, we compare against a phrasal SMT decoder, Pharaoh (Koehn et al. 2003). 4.1. Data Two language pairs were used for this comparison: English to French, and English to Japanes</context>
</contexts>
<marker>Quirk, Menezes, 2005</marker>
<rawString>Quirk, Chris and Arul Menezes. 2005. Dependency tree translation: syntactically-informed phrasal SMT. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proceedings of DARPA Broadcast News Transcription and Understanding.</booktitle>
<contexts>
<context position="14170" citStr="Stolcke 1998" startWordPosition="2169" endWordPosition="2170"> segmented into much smaller tuples, most often pairs of single words. Furthermore the failure to model a partitioning probability is much more defensible when the partitions are much smaller. Secondly, n-gram language model probabilities provide a robust means of estimating phrasal translation probabilities in context that models interactions between all adjacent tuples, obviating the need for overlapping mappings. These tuple channel models still must address practical issues such as model size, though much work has been done to shrink language models with minimal impact to perplexity (e.g. Stolcke 1998), which these models could immediately leverage. Furthermore, these models do not address the contiguity problem or the global reordering problem. 3. Translation by MTUs In this paper, we address all four theoretical problems using a novel combination of our syntactically-informed treelet approach (Quirk and Menezes 2005) and a modified version of bilingual n-gram channel models (Banchs et al. 2005). As in our previous work, we first parse the sentence into a dependency tree. After this initial parse, we use a global search to find a candidate that maximizes a log-linear model, where these can</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Stolcke, Andreas. 1998. Entropy-based pruning of backoff language models. In Proceedings of DARPA Broadcast News Transcription and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>Ying Zhang, Fei Huang, Alicia Tribble, Ashish Venugopal, Bing Zhao, Alex Waibel.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<marker>Vogel, 2003</marker>
<rawString>Vogel, Stephan, Ying Zhang, Fei Huang, Alicia Tribble, Ashish Venugopal, Bing Zhao, Alex Waibel. 2003. The CMU statistical machine translation system. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Zens, Ney, 2003</marker>
<rawString>Zens, Richard, and Hermann Ney. 2003. A comparative study on reordering constraints in statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>An efficient phraseto-phrase alignment model for arbitrarily long phrase and large corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="8788" citStr="Zhang and Vogel 2005" startWordPosition="1336" endWordPosition="1339">it with increasing phrase lengths. Given that storing all of these phrases leads to very large phrase tables, many research systems simply limit the phrases gathered to those that could possibly influence some test set. However, this is not feasible for true production MT systems, since the data to be translated is unknown. 2. Previous work 2.1. Delayed phrase construction To avoid the major practical problem of phrasal SMT—namely large phrase tables, most of which are not useful to any one sentence—one can instead construct phrase tables on the fly using an indexed form of the training data (Zhang and Vogel 2005; Callison-Burch et al. 2005). However, this does not relieve any of the theoretical problems with phrase-based SMT. 2.2. Syntax-based SMT Two recent systems have attempted to address the contiguity limitation and global re-ordering problem using syntax-based approaches. Hierarchical phrases Recent work in the use of hierarchical phrases (Chiang 2005) improves the ability to capture linguistic generalizations, and also removes the limitation to contiguous phrases. Hierarchical phrases differ from standard phrases in one important way: in addition to lexical items, a phrase pair may contain ind</context>
</contexts>
<marker>Zhang, Vogel, 2005</marker>
<rawString>Zhang, Ying and Stephan Vogel. 2005. An efficient phraseto-phrase alignment model for arbitrarily long phrase and large corpora. In Proceedings of EAMT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>