<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9963465">
Combining Intra- and Multi-sentential Rhetorical Parsing for
Document-level Discourse Analysis
</title>
<author confidence="0.77285">
Shafiq Joty∗ Giuseppe Carenini, Raymond Ng, Yashar Mehdad
</author>
<email confidence="0.473482">
sjoty@qf.org.qa {carenini, rng, mehdad}@cs.ubc.ca
</email>
<affiliation confidence="0.982840333333333">
Qatar Computing Research Institute Department of Computer Science
Qatar Foundation University of British Columbia
Doha, Qatar Vancouver, Canada
</affiliation>
<sectionHeader confidence="0.978028" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983785714286">
We propose a novel approach for develop-
ing a two-stage document-level discourse
parser. Our parser builds a discourse tree
by applying an optimal parsing algorithm
to probabilities inferred from two Con-
ditional Random Fields: one for intra-
sentential parsing and the other for multi-
sentential parsing. We present two ap-
proaches to combine these two stages of
discourse parsing effectively. A set of
empirical evaluations over two different
datasets demonstrates that our discourse
parser significantly outperforms the state-
of-the-art, often by a wide margin.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986573984126984">
Discourse of any kind is not formed by inde-
pendent and isolated textual units, but by related
and structured units. Discourse analysis seeks
to uncover such structures underneath the surface
of the text, and has been shown to be benefi-
cial for text summarization (Louis et al., 2010;
Marcu, 2000b), sentence compression (Sporleder
and Lapata, 2005), text generation (Prasad et al.,
2005), sentiment analysis (Somasundaran, 2010)
and question answering (Verberne et al., 2007).
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988), one of the most influential the-
ories of discourse, represents texts by labeled hier-
archical structures, called Discourse Trees (DTs),
as exemplified by a sample DT in Figure 1. The
leaves of a DT correspond to contiguous Elemen-
tary Discourse Units (EDUs) (six in the exam-
ple). Adjacent EDUs are connected by rhetori-
cal relations (e.g., Elaboration, Contrast), form-
ing larger discourse units (represented by internal
∗This work was conducted at the University of British
Columbia, Vancouver, Canada.
nodes), which in turn are also subject to this re-
lation linking. Discourse units linked by a rhetori-
cal relation are further distinguished based on their
relative importance in the text: nucleus being the
central part, whereas satellite being the peripheral
one. Discourse analysis in RST involves two sub-
tasks: discourse segmentation is the task of identi-
fying the EDUs, and discourse parsing is the task
of linking the discourse units into a labeled tree.
While recent advances in automatic discourse
segmentation and sentence-level discourse parsing
have attained accuracies close to human perfor-
mance (Fisher and Roark, 2007; Joty et al., 2012),
discourse parsing at the document-level still poses
significant challenges (Feng and Hirst, 2012) and
the performance of the existing document-level
parsers (Hernault et al., 2010; Subba and Di-
Eugenio, 2009) is still considerably inferior com-
pared to human gold-standard. This paper aims
to reduce this performance gap and take discourse
parsing one step further. To this end, we address
three key limitations of existing parsers as follows.
First, existing discourse parsers typically model
the structure and the labels of a DT separately
in a pipeline fashion, and also do not consider
the sequential dependencies between the DT con-
stituents, which has been recently shown to be crit-
ical (Feng and Hirst, 2012). To address this limi-
tation, as the first contribution, we propose a novel
document-level discourse parser based on proba-
bilistic discriminative parsing models, represented
as Conditional Random Fields (CRFs) (Sutton et
al., 2007), to infer the probability of all possible
DT constituents. The CRF models effectively rep-
resent the structure and the label of a DT con-
stituent jointly, and whenever possible, capture the
sequential dependencies between the constituents.
Second, existing parsers apply greedy and sub-
optimal parsing algorithms to build the DT for a
document. To cope with this limitation, our CRF
models support a probabilistic bottom-up parsing
</bodyText>
<page confidence="0.984839">
486
</page>
<note confidence="0.9594795">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 486–496,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.872603">
Figure 1: Discourse tree for two sentences in RST-DT. Each of the sentences contains three EDUs. The
second sentence has a well-formed discourse tree, but the first sentence does not have one.
</figureCaption>
<figure confidence="0.997989428571428">
Attribution
But he
added:
Contrast
Same-Unit
(1)
Contrast
&amp;quot;Some people use the purchasers’
index as a leading indicator,
some use it as a
coincident indicator.
it missed altogether
last month.&amp;quot; &lt;P&gt;
Elaboration
But the thing it’s
supposed to measure
-- manufacturing
strength --
(2) (3)
(6)
(4) (5)
</figure>
<bodyText confidence="0.989673180327869">
algorithm which is non-greedy and optimal.
Third, existing discourse parsers do not dis-
criminate between intra-sentential (i.e., building
the DTs for the individual sentences) and multi-
sentential parsing (i.e., building the DT for the
document). However, we argue that distinguish-
ing between these two conditions can result in
more effective parsing. Two separate parsing
models could exploit the fact that rhetorical re-
lations are distributed differently intra-sententially
vs. multi-sententially. Also, they could indepen-
dently choose their own informative features. As
another key contribution of our work, we devise
two different parsing components: one for intra-
sentential parsing, the other for multi-sentential
parsing. This provides for scalable, modular and
flexible solutions, that can exploit the strong cor-
relation observed between the text structure (sen-
tence boundaries) and the structure of the DT.
In order to develop a complete and robust dis-
course parser, we combine our intra-sentential
and multi-sentential parsers in two different ways.
Since most sentences have a well-formed dis-
course sub-tree in the full document-level DT (for
example, the second sentence in Figure 1), our first
approach constructs a DT for every sentence us-
ing our intra-sentential parser, and then runs the
multi-sentential parser on the resulting sentence-
level DTs. However, this approach would disre-
gard those cases where rhetorical structures vio-
late sentence boundaries. For example, consider
the first sentence in Figure 1. It does not have a
well-formed sub-tree because the unit containing
EDUs 2 and 3 merges with the next sentence and
only then is the resulting unit merged with EDU
1. Our second approach, in an attempt of dealing
with these cases, builds sentence-level sub-trees
by applying the intra-sentential parser on a sliding
window covering two adjacent sentences and by
then consolidating the results produced by over-
lapping windows. After that, the multi-sentential
parser takes all these sentence-level sub-trees and
builds a full rhetorical parse for the document.
While previous approaches have been tested on
only one corpus, we evaluate our approach on
texts from two very different genres: news articles
and instructional how-to-do manuals. The results
demonstrate that our contributions provide con-
sistent and statistically significant improvements
over previous approaches. Our final result com-
pares very favorably to the result of state-of-the-art
models in document-level discourse parsing.
In the rest of the paper, after discussing related
work in Section 2, we present our discourse pars-
ing framework in Section 3. In Section 4, we de-
scribe the intra- and multi-sentential parsing com-
ponents. Section 5 presents the two approaches
to combine the two stages of parsing. The exper-
iments and error analysis, followed by future di-
rections are discussed in Section 6. Finally, we
summarize our contributions in Section 7.
</bodyText>
<sectionHeader confidence="0.999408" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999929933333333">
The idea of staging document-level discourse
parsing on top of sentence-level discourse parsing
was investigated in (Marcu, 2000a; LeThanh et al.,
2004). These approaches mainly rely on discourse
markers (or cues), and use hand-coded rules to
build DTs for sentences first, then for paragraphs,
and so on. However, often rhetorical relations
are not explicitly signaled by discourse markers
(Marcu and Echihabi, 2002), and discourse struc-
tures do not always correspond to paragraph struc-
tures (Sporleder and Lascarides, 2004). Therefore,
rather than relying on hand-coded rules based on
discourse markers, recent approaches employ su-
pervised machine learning techniques with a large
set of informative features.
</bodyText>
<footnote confidence="0.539847">
Hernault et al., (2010) presents the publicly
available HILDA parser. Given the EDUs in a doc-
</footnote>
<page confidence="0.995748">
487
</page>
<figure confidence="0.989931260869565">
30
25
20
15
10
5
0
Multi-sentential
Intra-sentential
Sentences
segmented
into EDUs
Document-level
discourse tree
Algorithm
model
Intra-sentential
parser
Algorithm
model
Multi-sentential
parser
Elaboration Joint AttributionSame-Unit Contrast Explanation
</figure>
<figureCaption confidence="0.9975515">
Figure 2: Distributions of six most frequent relations in
intra-sentential and multi-sentential parsing scenarios.
</figureCaption>
<bodyText confidence="0.999896875">
ument, HILDA iteratively employs two Support
Vector Machine (SVM) classifiers in pipeline to
build the DT. In each iteration, a binary classifier
first decides which of the adjacent units to merge,
then a multi-class classifier connects the selected
units with an appropriate relation label. They eval-
uate their approach on the RST-DT corpus (Carl-
son et al., 2002) of news articles. On a different
genre of instructional texts, Subba and Di-Eugenio
(2009) propose a shift-reduce parser that relies on
a classifier for relation labeling. Their classifier
uses Inductive Logic Programming (ILP) to learn
first-order logic rules from a set of features includ-
ing compositional semantics. In this work, we ad-
dress the limitations of these models (described in
Section 1) introducing our novel discourse parser.
</bodyText>
<sectionHeader confidence="0.989958" genericHeader="method">
3 Our Discourse Parsing Framework
</sectionHeader>
<bodyText confidence="0.999977333333333">
Given a document with sentences already seg-
mented into EDUs, the discourse parsing prob-
lem is determining which discourse units (EDUs
or larger units) to relate (i.e., the structure), and
how to relate them (i.e., the labels or the discourse
relations) in the resulting DT. Since we already
have an accurate sentence-level discourse parser
(Joty et al., 2012), a straightforward approach to
document-level parsing could be to simply apply
this parser to the whole document. However this
strategy would be problematic because of scalabil-
ity and modeling issues. Note that the number of
valid trees grows exponentially with the number
of EDUs in a document.1 Therefore, an exhaus-
tive search over the valid trees is often unfeasible,
even for relatively small documents.
For modeling, the problem is two-fold. On the
one hand, it appears that rhetorical relations are
distributed differently intra-sententially vs. multi-
sententially. For example, Figure 2 shows a com-
parison between the two distributions of six most
</bodyText>
<footnote confidence="0.9962905">
1For n + 1 EDUs, the number of valid discourse trees is
actually the Catalan number Cn.
</footnote>
<figureCaption confidence="0.998964">
Figure 3: Discourse parsing framework.
</figureCaption>
<bodyText confidence="0.999917627906977">
frequent relations on a development set containing
20 randomly selected documents from RST-DT.
Notice that relations Attribution and Same-Unit
are more frequent than Joint in intra-sentential
case, whereas Joint is more frequent than the other
two in multi-sentential case. On the other hand,
different kinds of features are applicable and in-
formative for intra-sentential vs. multi-sentential
parsing. For example, syntactic features like dom-
inance sets (Soricut and Marcu, 2003) are ex-
tremely useful for sentence-level parsing, but are
not even applicable in multi-sentential case. Like-
wise, lexical chain features (Sporleder and Las-
carides, 2004), that are useful for multi-sentential
parsing, are not applicable at the sentence level.
Based on these observations, our discourse
parsing framework comprises two separate mod-
ules: an intra-sentential parser and a multi-
sentential parser (Figure 3). First, the intra-
sentential parser produces one or more discourse
sub-trees for each sentence. Then, the multi-
sentential parser generates a full DT for the doc-
ument from these sub-trees. Both of our parsers
have the same two components: a parsing model
assigns a probability to every possible DT, and
a parsing algorithm identifies the most probable
DT among the candidate DTs in that scenario.
While the two models are rather different, the
same parsing algorithm is shared by the two mod-
ules. Staging multi-sentential parsing on top of
intra-sentential parsing in this way allows us to ex-
ploit the strong correlation between the text struc-
ture and the DT structure as explained in detail in
Section 5. Before describing our parsing models
and the parsing algorithm, we introduce some ter-
minology that we will use throughout the paper.
Following (Joty et al., 2012), a DT can be for-
mally represented as a set of constituents of the
form R[i, m, j], referring to a rhetorical relation
R between the discourse unit containing EDUs i
through m and the unit containing EDUs m+1
through j. For example, the DT for the sec-
ond sentence in Figure 1 can be represented as
</bodyText>
<page confidence="0.994327">
488
</page>
<bodyText confidence="0.9871334">
{Elaboration-NS[4,4,5], Same-Unit-NN[4,5,6]}.
Notice that a relation R also specifies the nuclear-
ity statuses of the discourse units involved, which
can be one of Nucleus-Satellite (NS), Satellite-
Nucleus (SN) and Nucleus-Nucleus (NN).
</bodyText>
<sectionHeader confidence="0.753859" genericHeader="method">
4 Parsing Models and Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.9999815">
The job of our intra-sentential and multi-sentential
parsing models is to assign a probability to each
of the constituents of all possible DTs at the sen-
tence level and at the document level, respectively.
Formally, given the model parameters O, for each
possible constituent R[i, m, j] in a candidate DT
at the sentence or document level, the parsing
model estimates P(R[i, m, j]|O), which specifies
a joint distribution over the label R and the struc-
ture [i, m, j] of the constituent.
</bodyText>
<subsectionHeader confidence="0.901061">
4.1 Intra-Sentential Parsing Model
</subsectionHeader>
<bodyText confidence="0.9997360625">
Recently, we proposed a novel parsing model
for sentence-level discourse parsing (Joty et
al., 2012), that outperforms previous approaches
by effectively modeling sequential dependencies
along with structure and labels jointly. Below we
briefly describe the parsing model, and show how
it is applied to obtain the probabilities of all possi-
ble DT constituents at the sentence level.
Figure 4 shows the intra-sentential parsing
model expressed as a Dynamic Conditional Ran-
dom Field (DCRF) (Sutton et al., 2007). The ob-
served nodes Uj in a sequence represent the dis-
course units (EDUs or larger units). The first layer
of hidden nodes are the structure nodes, where
SjE{0,1} denotes whether two adjacent discourse
units Uj−1 and Uj should be connected or not.
The second layer of hidden nodes are the relation
nodes, with RjE{1... M} denoting the relation
between two adjacent units Uj−1 and Uj, where M
is the total number of relations in the relation set.
The connections between adjacent nodes in a hid-
den layer encode sequential dependencies between
the respective hidden nodes, and can enforce con-
straints such as the fact that a Sj= 1 must not fol-
low a Sj−1= 1. The connections between the two
hidden layers model the structure and the relation
of a DT (sentence-level) constituent jointly.
To obtain the probability of the constituents
of all candidate DTs for a sentence, we apply
the parsing model recursively at different levels
of the DT and compute the posterior marginals
over the relation-structure pairs. To illustrate the
</bodyText>
<figureCaption confidence="0.902981">
Figure 4: A chain-structured DCRF as our intra-
sentential parsing model.
</figureCaption>
<bodyText confidence="0.999598130434783">
process, let us assume that the sentence contains
four EDUs. At the first (bottom) level, when all
the units are the EDUs, there is only one possible
unit sequence to which we apply our DCRF
model (Figure 5(a)). We compute the posterior
marginals P(R2, S2=1|e1, e2, e3, e4, O), P(R3,
S3=1|e1, e2, e3, e4, O) and P(R4, S4=1|e1, e2, e3,
e4, O) to obtain the probability of the con-
stituents R[1,1,2], R[2, 2, 3] and R[3, 3, 4],
respectively. At the second level, there are
three possible unit sequences (e1:2, e3, e4),
(e1,e2:3, e4) and (e1,e2,e3:4). Figure 5(b) shows
their corresponding DCRFs. The posterior
marginals P(R3, S3=1|e1:2,e3,e4,O), P(R2:3
S2:3=1|e1,e2:3,e4,O), P(R4, S4=1|e1,e2:3,e4,O)
and P(R3:4, S3:4=1|e1,e2,e3:4,O) computed from
the three sequences correspond to the probability
of the constituents R[1, 2,3], R[1,1, 3], R[2, 3, 4]
and R[2, 2, 4], respectively. Similarly, we attain
the probability of the constituents R[1,1, 4],
R[1, 2, 4] and R[1, 3, 4] by computing their
respective posterior marginals from the three
possible sequences at the third (top) level.
</bodyText>
<figureCaption confidence="0.80694375">
Figure 5: Our parsing model applied to the sequences at
different levels of a sentence-level DT. (a) Only possible se-
quence at the first level, (b) Three possible sequences at the
second level, (c) Three possible sequences at the third level.
</figureCaption>
<bodyText confidence="0.999958">
At this point what is left to be explained is
how we generate all possible sequences for a
given number of EDUs in a sentence. Algorithm
1 demonstrates how we do that. More specifi-
cally, to compute the probabilities of each DT con-
</bodyText>
<equation confidence="0.886941333333333">
U1
U U U U U
2 3 j t-1 t
S2 S3 Sj St-1 St
R R R R R
2 3 j t-1 t
</equation>
<figure confidence="0.993722">
Unit
sequence
at level i
Structure
sequence
Relation
sequence
(a) (i) (ii)
R2:4
R3:4
R4
R2
R3:4
S2:4
S3:4
S4
S2
S3:4
e e e
1 e e e
2:4 1:2 3:4 1:3 4
e1 e2 e3:4
(i) (ii) (iii)
(c)
(iii)
(b)
e1 e e
2
R2 R3
S S3
2
3
e4
R4
S4
e e e4 e
1:2 3 1
S 3
R 3
R4
S4
S2:3
e
R2:3
2:3
R4
S4
e4
</figure>
<page confidence="0.982921">
489
</page>
<construct confidence="0.6353052">
stituent R[i, k, j], we need to generate sequences
like (e1, · · · , ei−1, ei:k, ek+1:j, ej+1, · · · , en) for
1 ≤ i ≤ k &lt; j ≤ n. In doing so, we may
generate some duplicate sequences. Clearly, the
sequence (e1, ··· , ei−1, ei:i, ei+1:j, ej+1, ··· , en)
</construct>
<bodyText confidence="0.9848005">
for 1 ≤ i ≤ k &lt; j &lt; n is already considered
for computing the probability of R[i + 1, j, j + 1].
Therefore, it is a duplicate sequence that we ex-
clude from our list of all possible sequences.
</bodyText>
<figure confidence="0.653730352941177">
Input: Sequence of EDUs: (e1, e2, · · · , en)
Output: List of sequences: L
for i = 1 → n − 1 do
for j = i + 1 → n do
if j == n then
for k = i → j − 1 do
L.append
((e1,.., ei−1, ei:k, ek+1:j, ej+1, .., en))
end
else
for k = i + 1 → j − 1 do
/L/.append
11e1 , .., ei−1, ei:k, ek+1:j, ej+1, .., en))
end
end
end
end
</figure>
<figureCaption confidence="0.56522">
Algorithm 1: Generating all possible sequences
</figureCaption>
<bodyText confidence="0.987224">
for a sentence with n EDUs.
Once we obtain the probability of all possible
DT constituents, the discourse sub-trees for the
sentences are built by applying an optimal prob-
abilistic parsing algorithm (Section 4.4) using one
of the methods described in Section 5.
</bodyText>
<subsectionHeader confidence="0.966372">
4.2 Multi-Sentential Parsing Model
</subsectionHeader>
<bodyText confidence="0.999909">
Given the discourse units (sub-trees) for all the
sentences of a document, a simple approach to
build the rhetorical tree of the document would be
to apply a new DCRF model, similar to the one
in Figure 4 (with different parameters), to all the
possible sequences generated from these units to
infer the probability of all possible higher-order
constituents. However, the number of possible se-
quences and their length increase with the number
of sentences in a document. For example, assum-
ing that each sentence has a well-formed DT, for
a document with n sentences, Algorithm 1 gener-
ates O(n3) sequences, where the sequence at the
bottom level has n units, each of the sequences at
the second level has n-1 units, and so on. Since
the model in Figure 4 has a “fat” chain structure,
</bodyText>
<figureCaption confidence="0.998171">
Figure 6: A CRF as a multi-sentential parsing model.
</figureCaption>
<bodyText confidence="0.998794190476191">
we could use forwards-backwards algorithm for
exact inference in this model (Sutton and McCal-
lum, 2012). However, forwards-backwards on a
sequence containing T units costs O(TM2) time,
where M is the number of relations in our rela-
tion set. This makes the chain-structured DCRF
model impractical for multi-sentential parsing of
long documents, since learning requires to run in-
ference on every training sequence with an overall
time complexity of O(TM2n3) per document.
Our model for multi-sentential parsing is shown
in Figure 6. The two observed nodes Ut−1 and
Ut are two adjacent discourse units. The (hidden)
structure node S∈{0, 1} denotes whether the two
units should be connected or not. The hidden node
R∈{1... M} represents the relation between the
two units. Notice that like the previous model, this
is also an undirected graphical model. It becomes
a CRF if we directly model the hidden (output)
variables by conditioning its clique potential (or
factor) φ on the observed (input) variables:
</bodyText>
<equation confidence="0.9971405">
1
P(Rt, St|x, O) = Z(x, O)φ(Rt, St|x, O) (1)
</equation>
<bodyText confidence="0.99996425">
where x represents input features extracted from
the observed variables Ut−1 and Ut, and Z(x, O)
is the partition function. We use a log-linear rep-
resentation of the factor:
</bodyText>
<equation confidence="0.96801">
φ(Rt, St|x, O) = exp(OT f(Rt, St, x)) (2)
</equation>
<bodyText confidence="0.998865">
where f(Rt, St, x) is a feature vector derived from
the input features x and the labels Rt and St, and
O is the corresponding weight vector. Although,
this model is similar in spirit to the model in Fig-
ure 4, we now break the chain structure, which
makes the inference much faster (i.e., complex-
ity of O(M2)). Breaking the chain structure also
allows us to balance the data for training (equal
number instances with S=1 and S=0), which dra-
matically reduces the learning time of the model.
We apply our model to all possible adjacent
units at all levels for the multi-sentential case, and
</bodyText>
<figure confidence="0.992449">
Relation
Structure
Adjacent Units
at level i
Rt
St
U U
t-1 t
</figure>
<page confidence="0.991438">
490
</page>
<bodyText confidence="0.999565">
compute the posterior marginals of the relation-
structure pairs P(Rt, St=1|Ut_1, Ut, O) to obtain
the probability of all possible DT constituents.
</bodyText>
<subsectionHeader confidence="0.99407">
4.3 Features Used in our Parsing Models
</subsectionHeader>
<bodyText confidence="0.999939347826087">
Table 1 summarizes the features used in our pars-
ing models, which are extracted from two adjacent
units Ut_1 and Ut. Since most of these features are
adopted from previous studies (Joty et al., 2012;
Hernault et al., 2010), we briefly describe them.
Organizational features include the length of
the units as the number of EDUs and tokens.
It also includes the distances of the units from
the beginning and end of the sentence (or text in
the multi-sentential case). Text structural fea-
tures indirectly capture the correlation between
text structure and rhetorical structure by counting
the number of sentence and paragraph boundaries
in the units. Discourse markers (e.g., because, al-
though) carry informative clues for rhetorical re-
lations (Marcu, 2000a). Rather than using a fixed
list of discourse markers, we use an empirically
learned lexical N-gram dictionary following (Joty
et al., 2012). This approach has been shown to
be more robust and flexible across domains (Bi-
ran and Rambow, 2011; Hernault et al., 2010). We
also include part-of-speech (POS) tags for the be-
ginning and end N tokens in a unit.
</bodyText>
<sectionHeader confidence="0.856304" genericHeader="method">
8 Organizational features Intra &amp; Multi-Sentential
</sectionHeader>
<bodyText confidence="0.8430215">
Number of EDUs in unit 1 (or unit 2).
Number of tokens in unit 1 (or unit 2).
Distance of unit 1 in EDUs to the beginning (or to the end).
Distance of unit 2 in EDUs to the beginning (or to the end).
</bodyText>
<sectionHeader confidence="0.721611" genericHeader="method">
4 Text structural features Multi-Sentential
</sectionHeader>
<bodyText confidence="0.872432428571429">
Number of sentences in unit 1 (or unit 2).
Number of paragraphs in unit 1 (or unit 2).
8N-gram features NE{1, 2, 3} Intra &amp; Multi-Sentential
Beginning (or end) lexical N-grams in unit 1.
Beginning (or end) lexical N-grams in unit 2.
Beginning (or end) POS N-grams in unit 1.
Beginning (or end) POS N-grams in unit 2.
</bodyText>
<sectionHeader confidence="0.734078" genericHeader="method">
5 Dominance set features Intra-Sentential
</sectionHeader>
<bodyText confidence="0.994460333333333">
Syntactic labels of the head node and the attachment node.
Lexical heads of the head node and the attachment node.
Dominance relationship between the two units.
</bodyText>
<sectionHeader confidence="0.669056" genericHeader="method">
8 Lexical chain features Multi-Sentential
</sectionHeader>
<footnote confidence="0.454131">
Number of chains start in unit 1 and end in unit 2.
Number of chains start (or end) in unit 1 (or in unit 2).
Number of chains skipping both unit 1 and unit 2.
Number of chains skipping unit 1 (or unit 2).
2 Contextual features Intra &amp; Multi-Sentential
</footnote>
<table confidence="0.2306725">
Previous and next feature vectors.
2 Substructure features Intra &amp; Multi-Sentential
</table>
<tableCaption confidence="0.685603">
Root nodes of the left and right rhetorical sub-trees.
Table 1: Features used in our parsing models.
</tableCaption>
<bodyText confidence="0.998796578947368">
Lexico-syntactic features dominance sets
(Soricut and Marcu, 2003) are very effective for
intra-sentential parsing. We include syntactic
labels and lexical heads of head and attachment
nodes along with their dominance relationship
as features. Lexical chains (Morris and Hirst,
1991) are sequences of semantically related words
that can indicate topic shifts. Features extracted
from lexical chains have been shown to be useful
for finding paragraph-level discourse structure
(Sporleder and Lascarides, 2004). We compute
lexical chains for a document following the ap-
proach proposed in (Galley and McKeown, 2003),
that extracts lexical chains after performing word
sense disambiguation. Following (Joty et al.,
2012), we also encode contextual and rhetorical
sub-structure features in our models. The rhetori-
cal sub-structure features incorporate hierarchical
dependencies between DT constituents.
</bodyText>
<subsectionHeader confidence="0.997867">
4.4 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.999829545454545">
Given the probability of all possible DT con-
stituents in the intra-sentential and multi-sentential
scenarios, the job of the parsing algorithm is to
find the most probable DT for that scenario. Fol-
lowing (Joty et al., 2012), we implement a prob-
abilistic CKY-like bottom-up algorithm for com-
puting the most likely parse using dynamic pro-
gramming. Specifically, with n discourse units,
we use the upper-triangular portion of the nxn
dynamic programming table D. Given Ux(0) and
Ux(1) are the start and end EDU Ids of unit Ux:
</bodyText>
<equation confidence="0.995657">
D[i,j] = P(R[Ui(0), Uk(1), Uj(1)]) (3)
</equation>
<bodyText confidence="0.891488428571429">
where, k = argmax P(R[Ui(0), Up(1), Uj(1)]).
i&lt;p&lt;j
Note that, in contrast to previous studies on
document-level parsing (Hernault et al., 2010;
Subba and Di-Eugenio, 2009; Marcu, 2000b),
which use a greedy algorithm, our approach finds
a discourse tree that is globally optimal.
</bodyText>
<sectionHeader confidence="0.983682" genericHeader="method">
5 Document-level Parsing Approaches
</sectionHeader>
<bodyText confidence="0.999973666666667">
Now that we have presented our intra-sentential
and our multi-sentential parsers, we are ready to
describe how they can be effectively combined to
perform document-level discourse analysis. Re-
call that a key motivation for a two-stage parsing is
that it allows us to capture the correlation between
text structure and discourse structure in a scalable,
modular and flexible way. Below we describe two
different approaches to model this correlation.
</bodyText>
<page confidence="0.997012">
491
</page>
<subsectionHeader confidence="0.855761">
5.1 1S-1S (1 Sentence-1 Sub-tree)
</subsectionHeader>
<bodyText confidence="0.999038538461538">
A key finding from several previous studies on
sentence-level discourse analysis is that most sen-
tences have a well-formed discourse sub-tree in
the full document-level DT (Joty et al., 2012;
Fisher and Roark, 2007). For example, Figure 7(a)
shows 10 EDUs in 3 sentences (see boxes), where
the DTs for the sentences obey their respective
sentence boundaries. The 1S-1S approach aims to
maximally exploit this finding. It first constructs
a DT for every sentence using our intra-sentential
parser, and then it provides our multi-sentential
parser with the sentence-level DTs to build the
rhetorical parse for the whole document.
</bodyText>
<figureCaption confidence="0.978976">
Figure 7: Two possible DTs for three sentences.
</figureCaption>
<subsectionHeader confidence="0.999844">
5.2 Sliding Window
</subsectionHeader>
<bodyText confidence="0.999920921052632">
While the assumption made by 1S-1S clearly sim-
plifies the parsing process, it totally ignores the
cases where discourse structures violate sentence
boundaries. For example, in the DT shown in Fig-
ure 7(b), sentence S2 does not have a well-formed
sub-tree because some of its units attach to the
left (4-5, 6) and some to the right (7). Vliet and
Redeker (2011) call these cases as ‘leaky’ bound-
aries. Even though less than 5% of the sentences
have leaky boundaries in RST-DT, in other corpora
this can be true for a larger portion of the sen-
tences. For example, we observe over 12% sen-
tences with leaky boundaries in the Instructional
corpus of (Subba and Di-Eugenio, 2009). How-
ever, we notice that in most cases where discourse
structures violate sentence boundaries, its units are
merged with the units of its adjacent sentences, as
in Figure 7(b). For example, this is true for 75%
cases in our development set containing 20 news
articles from RST-DT and for 79% cases in our
development set containing 20 how-to-do manuals
from the Instructional corpus. Based on this obser-
vation, we propose a sliding window approach.
In this approach, our intra-sentential parser
works with a window of two consecutive sen-
tences, and builds a DT for the two sentences. For
example, given the three sentences in Figure 7, our
intra-sentential parser constructs a DT for S1-S2
and a DT for S2-S3. In this process, each sentence
in a document except the first and the last will be
associated with two DTs: one with the previous
sentence (say DTp) and one with the next (say
DTn). In other words, for each non-boundary sen-
tence, we will have two decisions: one from DTp
and one from DTn. Our parser consolidates the
two decisions and generates one or more sub-trees
for each sentence by checking the following three
mutually exclusive conditions one after another:
</bodyText>
<listItem confidence="0.993314777777778">
• Same in both: If the sentence has the same (in
terms of both structure and labels) well-formed
sub-tree in both DTp and DTn, we take this sub-
tree for the sentence. For example, in Figure 8(a),
S2 has the same sub-tree in the two DTs, i.e. a DT
for S1-S2 and a DT for S2-S3. The two decisions
agree on the DT for the sentence.
• Different but no cross: If the sentence has a
well-formed sub-tree in both DTp and DTn, but
</listItem>
<bodyText confidence="0.982892714285714">
the two sub-trees vary either in structure or in la-
bels, we pick the most probable one. For example,
consider the DT for S1-S2 in Figure 8(a) and the
DT for S2-S3 in Figure 8(b). In both cases S2 has
a well-formed sub-tree, but they differ in structure.
We pick the sub-tree which has the higher proba-
bility in the two dynamic programming tables.
</bodyText>
<figureCaption confidence="0.972282">
Figure 8: Extracting sub-trees for S2.
</figureCaption>
<listItem confidence="0.972127">
• Cross: If either or both of DTp and DTn seg-
</listItem>
<bodyText confidence="0.9860865">
ment the sentence into multiple sub-trees, we pick
the one with more sub-trees. For example, con-
sider the two DTs in Figure 8(c). In the DT for
S1-S2, S2 has three sub-trees (4-5,6,7), whereas
in the DT for S2-S3, it has two (4-6,7). So, we ex-
tract the three sub-trees for S2 from the first DT. If
the sentence has the same number of sub-trees in
both DTp and DTn, we pick the one with higher
probability in the dynamic programming tables.
At the end, the multi-sentential parser takes all
these sentence-level sub-trees for a document, and
builds a full rhetorical parse for the document.
</bodyText>
<figure confidence="0.999664175">
(b)
(a)
1 2 3
S1
4 5 6 7
S
2
8 9 10
S
3
1 2 3
S1
?
4 5 6 7
? ?
S
2
8 9 10
S
3
1 2 3
4 5 6 7
4 5 6 7
8 9 10
S
(i) (c) 2 (ii)
S1
S2
S3
S2
S3
(b)
4 5 6 7
8 9 10
1 2 3
4 5 6 7
4 5 6 7
8 9 10
S1 S2 S2 S3
(a)
</figure>
<page confidence="0.995269">
492
</page>
<sectionHeader confidence="0.994809" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.992582">
6.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999972421052632">
While previous studies on document-level parsing
only report their results on a particular corpus, to
show the generality of our method, we experiment
with texts from two very different genres. Our
first corpus is the standard RST-DT (Carlson et
al., 2002), which consists of 385 Wall Street Jour-
nal articles, and is partitioned into a training set
of 347 documents and a test set of 38 documents.
53 documents, selected from both sets were anno-
tated by two annotators, based on which we mea-
sure human agreement. In RST-DT, the original 25
rhetorical relations defined by (Mann and Thomp-
son, 1988) are further divided into a set of 18
coarser relation classes with 78 finer-grained rela-
tions. Our second corpus is the Instructional cor-
pus prepared by (Subba and Di-Eugenio, 2009),
which contains 176 how-to-do manuals on home-
repair. The corpus was annotated with 26 informa-
tional relations (e.g., Preparation-Act, Act-Goal).
</bodyText>
<subsectionHeader confidence="0.998586">
6.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999991131578947">
We experiment with our discourse parser on the
two datasets using our two different parsing ap-
proaches, namely 1S-1S and the sliding window.
We compare our approach with HILDA (Hernault
et al., 2010) on RST-DT, and with the ILP-based
approach of (Subba and Di-Eugenio, 2009) on the
Instructional corpus, since they are the state-of-
the-art on the respective genres. On RST-DT, the
standard split was used for training and testing
purposes. The results for HILDA were obtained
by running the system with default settings on the
same inputs we provided to our system. Since we
could not run the ILP-based system of (Subba and
Di-Eugenio, 2009) (not publicly available) on the
Instructional corpus, we report the performances
presented in their paper. They used 151 documents
for training and 25 documents for testing. Since
we did not have access to their particular split,
we took 5 random samples of 151 documents for
training and 25 documents for testing, and report
the average performance over the 5 test sets.
To evaluate the parsing performance, we use
the standard unlabeled (i.e., hierarchical spans)
and labeled (i.e., nuclearity and relation) preci-
sion, recall and F-score as described in (Marcu,
2000b). To compare with previous studies, our
experiments on RST-DT use the 18 coarser rela-
tions. After attaching the nuclearity statuses (NS,
SN, NN) to these relations, we get 41 distinct re-
lations. Following (Subba and Di-Eugenio, 2009)
on the Instructional corpus, we use 26 relations,
and treat the reversals of non-commutative rela-
tions as separate relations. That is, Goal-Act and
Act-Goal are considered as two different relations.
Attaching the nuclearity statuses to these relations
gives 76 distinct relations. Analogous to previous
studies, we map the n-ary relations (e.g., Joint)
into nested right-branching binary relations.
</bodyText>
<subsectionHeader confidence="0.980167">
6.3 Results and Error Analysis
</subsectionHeader>
<bodyText confidence="0.999983942857143">
Table 2 presents F-score parsing results for our
parsers and the existing systems on the two cor-
pora.2 On both corpora, our parser, namely, 1S-1S
(TSP 1-1) and sliding window (TSP SW), outper-
form existing systems by a wide margin (p&lt;7.1e-
05).3 On RST-DT, our parsers achieve absolute
F-score improvements of 8%, 9.4% and 11.4%
in span, nuclearity and relation, respectively, over
HILDA. This represents relative error reductions
of 32%, 23% and 21% in span, nuclearity and rela-
tion, respectively. Our results are also close to the
upper bound, i.e. human agreement on this corpus.
On the Instructional genre, our parsers deliver
absolute F-score improvements of 10.5%, 13.6%
and 8.14% in span, nuclearity and relations, re-
spectively, over the ILP-based approach. Our
parsers, therefore, reduce errors by 36%, 27% and
13% in span, nuclearity and relations, respectively.
If we compare the performance of our parsers
on the two corpora, we observe higher results
on RST-DT. This can be explained in at least
two ways. First, the Instructional corpus has a
smaller amount of data with a larger set of rela-
tions (76 when nuclearity attached). Second, some
frequent relations are (semantically) very similar
(e.g., Preparation-Act, Step1-Step2), which makes
it difficult even for the human annotators to distin-
guish them (Subba and Di-Eugenio, 2009).
Comparison between our two models reveals
that TSP SW significantly outperforms TSP 1-1
only in finding the right structure on both corpora
(p&lt;0.01). Not surprisingly, the improvement is
higher on the Instructional corpus. A likely ex-
planation is that the Instructional corpus contains
more leaky boundaries (12%), allowing the sliding
</bodyText>
<footnote confidence="0.989668">
2Precision, Recall and F-score are the same when manual
segmentation is used (see Marcu, (2000b), page 143).
3Since we did not have access to the output or to the sys-
tem of (Subba and Di-Eugenio, 2009), we were not able to
perform a significance test on the Instructional corpus.
</footnote>
<page confidence="0.995001">
493
</page>
<table confidence="0.99567">
RST-DT Instructional
Metrics HILDA TSP 1-1 TSP SW Human ILP TSP 1-1 TSP SW
Span 74.68 82.47* 82.74*† 88.70 70.35 79.67 80.88†
Nuclearity 58.99 68.43* 68.40* 77.72 49.47 63.03 63.10
Relation 44.32 55.73* 55.71* 65.75 35.44 43.52 43.58
</table>
<tableCaption confidence="0.9914815">
Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA
(with p&lt;7.1e-05) are denoted by *. Significant differences between TSP 1-1 and TSP SW (with p&lt;0.01) are denoted by †.
</tableCaption>
<figure confidence="0.872046736842105">
T-C T-O T-CM M-M CMP EV SU CND EN CA TE EX BA CO JO S-U AT EL
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 7
0 0 0 10 0 0 0 0 0 0 0 1 1 0 0 0 1 3
0 0 0 1 4 0 0 1 0 1 0 3 3 0 1 1 0 2
0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 2 11
0 0 0 0 0 0 8 0 0 0 0 0 0 0 1 0 0 12
0 0 0 0 0 0 0 22 0 0 0 0 1 3 0 0 3 2
0 0 0 0 0 0 0 1 24 1 0 0 0 0 0 0 1 7
0 0 0 0 0 0 0 0 2 3 0 4 2 2 7 0 3 11
0 0 0 1 0 0 0 1 2 0 7 1 9 1 9 0 3 4
0 0 0 1 0 0 0 0 1 5 0 12 0 1 3 0 3 12
0 0 0 1 0 0 0 1 0 1 4 1 19 2 6 1 5 12
0 0 0 1 2 0 0 2 0 1 3 2 2 33 7 0 0 9
0 0 0 0 0 0 1 2 0 1 1 1 1 2 57 1 0 13
0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 85 1 0
0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 272 9
0 1 0 0 0 0 0 0 14 6 1 8 1 0 8 2 2 359
</figure>
<figureCaption confidence="0.962649666666667">
Figure 9: Confusion matrix for relation labels on the
RST-DT test set. Y-axis represents true and X-axis repre-
sents predicted relations. The relations are Topic-Change
(T-C), Topic-Comment (T-CM), Textual Organization (T-
O), Manner-Means (M-M), Comparison (CMP), Evaluation
(EV), Summary (SU), Condition (CND), Enablement (EN),
Cause (CA), Temporal (TE), Explanation (EX), Background
(BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu-
tion (AT) and Elaboration (EL).
</figureCaption>
<bodyText confidence="0.999990444444445">
window approach to be more effective in finding
those, without inducing much noise for the labels.
This clearly demonstrates the potential of TSP SW
for datasets with even more leaky boundaries e.g.,
the Dutch (Vliet and Redeker, 2011) and the Ger-
man Potsdam (Stede, 2004) corpora.
Error analysis reveals that although TSP SW
finds more correct structures, a corresponding im-
provement in labeling relations is not present be-
cause in a few cases, it tends to induce noise from
the neighboring sentences for the labels. For ex-
ample, when parsing was performed on the first
sentence in Figure 1 in isolation using 1S-1S, our
parser rightly identifies the Contrast relation be-
tween EDUs 2 and 3. But, when it is considered
with its neighboring sentences by the sliding win-
dow, the parser labels it as Elaboration. A promis-
ing strategy to deal with this and similar problems
that we plan to explore in future, is to apply both
approaches to each sentence and combine them by
consolidating three probabilistic decisions, i.e. the
one from 1S-1S and the two from sliding window.
To further analyze the errors made by our parser
on the hardest task of relation labeling, Figure 9
presents the confusion matrix for TSP 1-1 on the
RST-DT test set. The relation labels are ordered
according to their frequency in the RST-DT train-
ing set. In general, the errors are produced by two
different causes acting together: (i) imbalanced
distribution of the relations, and (ii) semantic sim-
ilarity between the relations. The most frequent
relation Elaboration tends to mislead others es-
pecially, the ones which are semantically similar
(e.g., Explanation, Background) and less frequent
(e.g., Summary, Evaluation). The relations which
are semantically similar mislead each other (e.g.,
Temporal:Background, Cause:Explanation).
These observations suggest two ways to im-
prove our parser. We would like to employ a more
robust method (e.g., ensemble methods with bag-
ging) to deal with the imbalanced distribution of
relations, along with taking advantage of a richer
semantic knowledge (e.g., compositional seman-
tics) to cope with the errors caused by semantic
similarity between the rhetorical relations.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999849916666667">
In this paper, we have presented a novel discourse
parser that applies an optimal parsing algorithm
to probabilities inferred from two CRF models:
one for intra-sentential parsing and the other for
multi-sentential parsing. The two models exploit
their own informative feature sets and the distribu-
tional variations of the relations in the two parsing
conditions. We have also presented two novel ap-
proaches to combine them effectively. Empirical
evaluations on two different genres demonstrate
that our approach yields substantial improvement
over existing methods in discourse parsing.
</bodyText>
<sectionHeader confidence="0.998809" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998331">
We are grateful to Frank Tompa and the anony-
mous reviewers for their comments, and the
NSERC BIN and CGS-D for financial support.
</bodyText>
<figure confidence="0.999436277777778">
T-C
T-O
T-CM
M-M
CMP
EV
SU
CND
EN
CA
TE
EX
BA
CO
JO
S-U
AT
EL
</figure>
<page confidence="0.997599">
494
</page>
<sectionHeader confidence="0.995699" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928651376147">
O. Biran and O. Rambow. 2011. Identifying Justi-
fications in Written Dialogs by Classifying Text as
Argumentative. International Journal of Semantic
Computing, 5(4):363–381.
L. Carlson, D. Marcu, and M. Okurowski. 2002. RST
Discourse Treebank (RST-DT) LDC2002T07. Lin-
guistic Data Consortium, Philadelphia.
V. Feng and G. Hirst. 2012. Text-level Discourse Pars-
ing with Rich Linguistic Features. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, ACL ’12, pages 60–68,
Jeju Island, Korea. Association for Computational
Linguistics.
S. Fisher and B. Roark. 2007. The Utility of Parse-
derived Features for Automatic Discourse Segmen-
tation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, ACL
’07, pages 488–495, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
M. Galley and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining. In Pro-
ceedings of the 18th International Joint Conference
on Artificial Intelligence, IJCAI ’07, pages 1486–
1488, Acapulco, Mexico.
H. Hernault, H. Prendinger, D. duVerle, and
M. Ishizuka. 2010. HILDA: A Discourse Parser
Using Support Vector Machine Classification. Dia-
logue and Discourse, 1(3):1–33.
S. Joty, G. Carenini, and R. T. Ng. 2012. A Novel
Discriminative Framework for Sentence-Level Dis-
course Analysis. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ’12, pages 904–
915, Jeju Island, Korea. Association for Computa-
tional Linguistics.
H. LeThanh, G. Abeysinghe, and C. Huyck. 2004.
Generating Discourse Structures for Written Texts.
In Proceedings of the 20th international confer-
ence on Computational Linguistics, COLING ’04,
Geneva, Switzerland. Association for Computa-
tional Linguistics.
A. Louis, A. Joshi, and A. Nenkova. 2010. Discourse
Indicators for Content Selection in Summarization.
In Proceedings of the 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
SIGDIAL ’10, pages 147–156, Tokyo, Japan. Asso-
ciation for Computational Linguistics.
W. Mann and S. Thompson. 1988. Rhetorical Struc-
ture Theory: Toward a Functional Theory of Text
Organization. Text, 8(3):243–281.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. In
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ’02, pages
368–375. Association for Computational Linguis-
tics.
D. Marcu. 2000a. The Rhetorical Parsing of Unre-
stricted Texts: A Surface-based Approach. Compu-
tational Linguistics, 26:395–448.
D. Marcu. 2000b. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press,
Cambridge, MA, USA.
J. Morris and G. Hirst. 1991. Lexical Cohesion
Computed by Thesaural Relations as an Indicator
of Structure of Text. Computational Linguistics,
17(1):21–48.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki,
and B. Webber. 2005. The Penn Discourse Tree-
Bank as a Resource for Natural Language Gener-
ation. In Proceedings of the Corpus Linguistics
Workshop on Using Corpora for Natural Language
Generation, pages 25–32, Birmingham, U.K.
S. Somasundaran, 2010. Discourse-Level Relations for
Opinion Analysis. PhD thesis, University of Pitts-
burgh.
R. Soricut and D. Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. In Proceedings of the 2003 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology, NAACL-HLT ’03, pages 149–
156, Edmonton, Canada. Association for Computa-
tional Linguistics.
C. Sporleder and M. Lapata. 2005. Discourse Chunk-
ing and its Application to Sentence Compression.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing, pages 257–264, Van-
couver, British Columbia, Canada. Association for
Computational Linguistics.
C. Sporleder and A. Lascarides. 2004. Combining Hi-
erarchical Clustering and Machine Learning to Pre-
dict High-Level Discourse Structure. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ’04, Geneva, Switzer-
land. Association for Computational Linguistics.
M. Stede. 2004. The Potsdam Commentary Corpus.
In Proceedings of the ACL-04 Workshop on Dis-
course Annotation, Barcelona. Association for Com-
putational Linguistics.
R. Subba and B. Di-Eugenio. 2009. An Effective Dis-
course Parser that Uses Rich Linguistic Information.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT-NAACL ’09, pages 566–574, Boul-
der, Colorado. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.987947">
495
</page>
<reference confidence="0.999879947368421">
C. Sutton and A. McCallum. 2012. An Introduction
to Conditional Random Fields. Foundations and
Trends in Machine Learning, 4(4):267–373.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting
Sequence Data. Journal of Machine Learning Re-
search (JMLR), 8:693–723.
S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
2007. Evaluating Discourse-based Answer Extrac-
tion for Why-question Answering. In Proceedings
of the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 735–736, Amsterdam, The Nether-
lands. ACM.
N. Vliet and G. Redeker. 2011. Complex Sentences as
Leaky Units in Discourse Parsing. In Proceedings
of Constraints in Discourse, Agay-Saint Raphael,
September.
</reference>
<page confidence="0.999125">
496
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.547625">
<title confidence="0.9907045">Combining Intraand Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</title>
<author confidence="0.678626">Raymond Ng Carenini</author>
<author confidence="0.678626">Yashar Mehdad</author>
<email confidence="0.922731">rng,</email>
<affiliation confidence="0.978402">Qatar Computing Research Institute Department of Computer Science Qatar Foundation University of British Columbia</affiliation>
<address confidence="0.980415">Doha, Qatar Vancouver, Canada</address>
<abstract confidence="0.996030333333334">We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Biran</author>
<author>O Rambow</author>
</authors>
<title>Identifying Justifications in Written Dialogs by Classifying Text as Argumentative.</title>
<date>2011</date>
<journal>International Journal of Semantic Computing,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="22200" citStr="Biran and Rambow, 2011" startWordPosition="3594" endWordPosition="3598">the units from the beginning and end of the sentence (or text in the multi-sentential case). Text structural features indirectly capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the units. Discourse markers (e.g., because, although) carry informative clues for rhetorical relations (Marcu, 2000a). Rather than using a fixed list of discourse markers, we use an empirically learned lexical N-gram dictionary following (Joty et al., 2012). This approach has been shown to be more robust and flexible across domains (Biran and Rambow, 2011; Hernault et al., 2010). We also include part-of-speech (POS) tags for the beginning and end N tokens in a unit. 8 Organizational features Intra &amp; Multi-Sentential Number of EDUs in unit 1 (or unit 2). Number of tokens in unit 1 (or unit 2). Distance of unit 1 in EDUs to the beginning (or to the end). Distance of unit 2 in EDUs to the beginning (or to the end). 4 Text structural features Multi-Sentential Number of sentences in unit 1 (or unit 2). Number of paragraphs in unit 1 (or unit 2). 8N-gram features NE{1, 2, 3} Intra &amp; Multi-Sentential Beginning (or end) lexical N-grams in unit 1. Begi</context>
</contexts>
<marker>Biran, Rambow, 2011</marker>
<rawString>O. Biran and O. Rambow. 2011. Identifying Justifications in Written Dialogs by Classifying Text as Argumentative. International Journal of Semantic Computing, 5(4):363–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M Okurowski</author>
</authors>
<date>2002</date>
<booktitle>RST Discourse Treebank (RST-DT) LDC2002T07. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="9173" citStr="Carlson et al., 2002" startWordPosition="1370" endWordPosition="1374">e Algorithm model Intra-sentential parser Algorithm model Multi-sentential parser Elaboration Joint AttributionSame-Unit Contrast Explanation Figure 2: Distributions of six most frequent relations in intra-sentential and multi-sentential parsing scenarios. ument, HILDA iteratively employs two Support Vector Machine (SVM) classifiers in pipeline to build the DT. In each iteration, a binary classifier first decides which of the adjacent units to merge, then a multi-class classifier connects the selected units with an appropriate relation label. They evaluate their approach on the RST-DT corpus (Carlson et al., 2002) of news articles. On a different genre of instructional texts, Subba and Di-Eugenio (2009) propose a shift-reduce parser that relies on a classifier for relation labeling. Their classifier uses Inductive Logic Programming (ILP) to learn first-order logic rules from a set of features including compositional semantics. In this work, we address the limitations of these models (described in Section 1) introducing our novel discourse parser. 3 Our Discourse Parsing Framework Given a document with sentences already segmented into EDUs, the discourse parsing problem is determining which discourse un</context>
<context position="30370" citStr="Carlson et al., 2002" startWordPosition="5011" endWordPosition="5014">lti-sentential parser takes all these sentence-level sub-trees for a document, and builds a full rhetorical parse for the document. (b) (a) 1 2 3 S1 4 5 6 7 S 2 8 9 10 S 3 1 2 3 S1 ? 4 5 6 7 ? ? S 2 8 9 10 S 3 1 2 3 4 5 6 7 4 5 6 7 8 9 10 S (i) (c) 2 (ii) S1 S2 S3 S2 S3 (b) 4 5 6 7 8 9 10 1 2 3 4 5 6 7 4 5 6 7 8 9 10 S1 S2 S2 S3 (a) 492 6 Experiments 6.1 Corpora While previous studies on document-level parsing only report their results on a particular corpus, to show the generality of our method, we experiment with texts from two very different genres. Our first corpus is the standard RST-DT (Carlson et al., 2002), which consists of 385 Wall Street Journal articles, and is partitioned into a training set of 347 documents and a test set of 38 documents. 53 documents, selected from both sets were annotated by two annotators, based on which we measure human agreement. In RST-DT, the original 25 rhetorical relations defined by (Mann and Thompson, 1988) are further divided into a set of 18 coarser relation classes with 78 finer-grained relations. Our second corpus is the Instructional corpus prepared by (Subba and Di-Eugenio, 2009), which contains 176 how-to-do manuals on homerepair. The corpus was annotate</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2002</marker>
<rawString>L. Carlson, D. Marcu, and M. Okurowski. 2002. RST Discourse Treebank (RST-DT) LDC2002T07. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Feng</author>
<author>G Hirst</author>
</authors>
<title>Text-level Discourse Parsing with Rich Linguistic Features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL ’12,</booktitle>
<pages>60--68</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="2713" citStr="Feng and Hirst, 2012" startWordPosition="397" endWordPosition="400">istinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequential dependencies between the DT constituents, which has been recently shown to be critical (Feng a</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>V. Feng and G. Hirst. 2012. Text-level Discourse Parsing with Rich Linguistic Features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL ’12, pages 60–68, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fisher</author>
<author>B Roark</author>
</authors>
<title>The Utility of Parsederived Features for Automatic Discourse Segmentation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, ACL ’07,</booktitle>
<pages>488--495</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2594" citStr="Fisher and Roark, 2007" startWordPosition="380" endWordPosition="383">, which in turn are also subject to this relation linking. Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not</context>
<context position="26109" citStr="Fisher and Roark, 2007" startWordPosition="4218" endWordPosition="4221">s, we are ready to describe how they can be effectively combined to perform document-level discourse analysis. Recall that a key motivation for a two-stage parsing is that it allows us to capture the correlation between text structure and discourse structure in a scalable, modular and flexible way. Below we describe two different approaches to model this correlation. 491 5.1 1S-1S (1 Sentence-1 Sub-tree) A key finding from several previous studies on sentence-level discourse analysis is that most sentences have a well-formed discourse sub-tree in the full document-level DT (Joty et al., 2012; Fisher and Roark, 2007). For example, Figure 7(a) shows 10 EDUs in 3 sentences (see boxes), where the DTs for the sentences obey their respective sentence boundaries. The 1S-1S approach aims to maximally exploit this finding. It first constructs a DT for every sentence using our intra-sentential parser, and then it provides our multi-sentential parser with the sentence-level DTs to build the rhetorical parse for the whole document. Figure 7: Two possible DTs for three sentences. 5.2 Sliding Window While the assumption made by 1S-1S clearly simplifies the parsing process, it totally ignores the cases where discourse </context>
</contexts>
<marker>Fisher, Roark, 2007</marker>
<rawString>S. Fisher and B. Roark. 2007. The Utility of Parsederived Features for Automatic Discourse Segmentation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, ACL ’07, pages 488–495, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
</authors>
<title>Improving Word Sense Disambiguation in Lexical Chaining.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI ’07,</booktitle>
<pages>1486--1488</pages>
<location>Acapulco,</location>
<contexts>
<context position="24221" citStr="Galley and McKeown, 2003" startWordPosition="3930" endWordPosition="3933">sing models. Lexico-syntactic features dominance sets (Soricut and Marcu, 2003) are very effective for intra-sentential parsing. We include syntactic labels and lexical heads of head and attachment nodes along with their dominance relationship as features. Lexical chains (Morris and Hirst, 1991) are sequences of semantically related words that can indicate topic shifts. Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004). We compute lexical chains for a document following the approach proposed in (Galley and McKeown, 2003), that extracts lexical chains after performing word sense disambiguation. Following (Joty et al., 2012), we also encode contextual and rhetorical sub-structure features in our models. The rhetorical sub-structure features incorporate hierarchical dependencies between DT constituents. 4.4 Parsing Algorithm Given the probability of all possible DT constituents in the intra-sentential and multi-sentential scenarios, the job of the parsing algorithm is to find the most probable DT for that scenario. Following (Joty et al., 2012), we implement a probabilistic CKY-like bottom-up algorithm for compu</context>
</contexts>
<marker>Galley, McKeown, 2003</marker>
<rawString>M. Galley and K. McKeown. 2003. Improving Word Sense Disambiguation in Lexical Chaining. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI ’07, pages 1486– 1488, Acapulco, Mexico. H. Hernault, H. Prendinger, D. duVerle, and M. Ishizuka. 2010. HILDA: A Discourse Parser Using Support Vector Machine Classification. Dialogue and Discourse, 1(3):1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Joty</author>
<author>G Carenini</author>
<author>R T Ng</author>
</authors>
<title>A Novel Discriminative Framework for Sentence-Level Discourse Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>904--915</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="2614" citStr="Joty et al., 2012" startWordPosition="384" endWordPosition="387"> subject to this relation linking. Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequen</context>
<context position="10008" citStr="Joty et al., 2012" startWordPosition="1501" endWordPosition="1504">mming (ILP) to learn first-order logic rules from a set of features including compositional semantics. In this work, we address the limitations of these models (described in Section 1) introducing our novel discourse parser. 3 Our Discourse Parsing Framework Given a document with sentences already segmented into EDUs, the discourse parsing problem is determining which discourse units (EDUs or larger units) to relate (i.e., the structure), and how to relate them (i.e., the labels or the discourse relations) in the resulting DT. Since we already have an accurate sentence-level discourse parser (Joty et al., 2012), a straightforward approach to document-level parsing could be to simply apply this parser to the whole document. However this strategy would be problematic because of scalability and modeling issues. Note that the number of valid trees grows exponentially with the number of EDUs in a document.1 Therefore, an exhaustive search over the valid trees is often unfeasible, even for relatively small documents. For modeling, the problem is two-fold. On the one hand, it appears that rhetorical relations are distributed differently intra-sententially vs. multisententially. For example, Figure 2 shows </context>
<context position="12555" citStr="Joty et al., 2012" startWordPosition="1898" endWordPosition="1901">odel assigns a probability to every possible DT, and a parsing algorithm identifies the most probable DT among the candidate DTs in that scenario. While the two models are rather different, the same parsing algorithm is shared by the two modules. Staging multi-sentential parsing on top of intra-sentential parsing in this way allows us to exploit the strong correlation between the text structure and the DT structure as explained in detail in Section 5. Before describing our parsing models and the parsing algorithm, we introduce some terminology that we will use throughout the paper. Following (Joty et al., 2012), a DT can be formally represented as a set of constituents of the form R[i, m, j], referring to a rhetorical relation R between the discourse unit containing EDUs i through m and the unit containing EDUs m+1 through j. For example, the DT for the second sentence in Figure 1 can be represented as 488 {Elaboration-NS[4,4,5], Same-Unit-NN[4,5,6]}. Notice that a relation R also specifies the nuclearity statuses of the discourse units involved, which can be one of Nucleus-Satellite (NS), SatelliteNucleus (SN) and Nucleus-Nucleus (NN). 4 Parsing Models and Parsing Algorithm The job of our intra-sen</context>
<context position="21402" citStr="Joty et al., 2012" startWordPosition="3469" endWordPosition="3472">and S=0), which dramatically reduces the learning time of the model. We apply our model to all possible adjacent units at all levels for the multi-sentential case, and Relation Structure Adjacent Units at level i Rt St U U t-1 t 490 compute the posterior marginals of the relationstructure pairs P(Rt, St=1|Ut_1, Ut, O) to obtain the probability of all possible DT constituents. 4.3 Features Used in our Parsing Models Table 1 summarizes the features used in our parsing models, which are extracted from two adjacent units Ut_1 and Ut. Since most of these features are adopted from previous studies (Joty et al., 2012; Hernault et al., 2010), we briefly describe them. Organizational features include the length of the units as the number of EDUs and tokens. It also includes the distances of the units from the beginning and end of the sentence (or text in the multi-sentential case). Text structural features indirectly capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the units. Discourse markers (e.g., because, although) carry informative clues for rhetorical relations (Marcu, 2000a). Rather than using a fixed list of discou</context>
<context position="24325" citStr="Joty et al., 2012" startWordPosition="3944" endWordPosition="3947">ntential parsing. We include syntactic labels and lexical heads of head and attachment nodes along with their dominance relationship as features. Lexical chains (Morris and Hirst, 1991) are sequences of semantically related words that can indicate topic shifts. Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004). We compute lexical chains for a document following the approach proposed in (Galley and McKeown, 2003), that extracts lexical chains after performing word sense disambiguation. Following (Joty et al., 2012), we also encode contextual and rhetorical sub-structure features in our models. The rhetorical sub-structure features incorporate hierarchical dependencies between DT constituents. 4.4 Parsing Algorithm Given the probability of all possible DT constituents in the intra-sentential and multi-sentential scenarios, the job of the parsing algorithm is to find the most probable DT for that scenario. Following (Joty et al., 2012), we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse using dynamic programming. Specifically, with n discourse units, we use the u</context>
<context position="26084" citStr="Joty et al., 2012" startWordPosition="4214" endWordPosition="4217">i-sentential parsers, we are ready to describe how they can be effectively combined to perform document-level discourse analysis. Recall that a key motivation for a two-stage parsing is that it allows us to capture the correlation between text structure and discourse structure in a scalable, modular and flexible way. Below we describe two different approaches to model this correlation. 491 5.1 1S-1S (1 Sentence-1 Sub-tree) A key finding from several previous studies on sentence-level discourse analysis is that most sentences have a well-formed discourse sub-tree in the full document-level DT (Joty et al., 2012; Fisher and Roark, 2007). For example, Figure 7(a) shows 10 EDUs in 3 sentences (see boxes), where the DTs for the sentences obey their respective sentence boundaries. The 1S-1S approach aims to maximally exploit this finding. It first constructs a DT for every sentence using our intra-sentential parser, and then it provides our multi-sentential parser with the sentence-level DTs to build the rhetorical parse for the whole document. Figure 7: Two possible DTs for three sentences. 5.2 Sliding Window While the assumption made by 1S-1S clearly simplifies the parsing process, it totally ignores t</context>
</contexts>
<marker>Joty, Carenini, Ng, 2012</marker>
<rawString>S. Joty, G. Carenini, and R. T. Ng. 2012. A Novel Discriminative Framework for Sentence-Level Discourse Analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 904– 915, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H LeThanh</author>
<author>G Abeysinghe</author>
<author>C Huyck</author>
</authors>
<title>Generating Discourse Structures for Written Texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04,</booktitle>
<institution>Geneva, Switzerland. Association for Computational Linguistics.</institution>
<contexts>
<context position="7784" citStr="LeThanh et al., 2004" startWordPosition="1171" endWordPosition="1174">level discourse parsing. In the rest of the paper, after discussing related work in Section 2, we present our discourse parsing framework in Section 3. In Section 4, we describe the intra- and multi-sentential parsing components. Section 5 presents the two approaches to combine the two stages of parsing. The experiments and error analysis, followed by future directions are discussed in Section 6. Finally, we summarize our contributions in Section 7. 2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al., (2010) presents the pub</context>
</contexts>
<marker>LeThanh, Abeysinghe, Huyck, 2004</marker>
<rawString>H. LeThanh, G. Abeysinghe, and C. Huyck. 2004. Generating Discourse Structures for Written Texts. In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04, Geneva, Switzerland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Joshi</author>
<author>A Nenkova</author>
</authors>
<title>Discourse Indicators for Content Selection in Summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’10,</booktitle>
<pages>147--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Tokyo, Japan.</location>
<contexts>
<context position="1211" citStr="Louis et al., 2010" startWordPosition="171" endWordPosition="174">rasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaborat</context>
</contexts>
<marker>Louis, Joshi, Nenkova, 2010</marker>
<rawString>A. Louis, A. Joshi, and A. Nenkova. 2010. Discourse Indicators for Content Selection in Summarization. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’10, pages 147–156, Tokyo, Japan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a Functional Theory of Text Organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1465" citStr="Mann and Thompson, 1988" startWordPosition="204" endWordPosition="207">r significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by internal ∗This work was conducted at the University of British Columbia, Vancouver, Canada. nodes), which in turn are also subject to this relation linking. Discourse units linked by a rhetor</context>
<context position="30711" citStr="Mann and Thompson, 1988" startWordPosition="5070" endWordPosition="5074"> 6 Experiments 6.1 Corpora While previous studies on document-level parsing only report their results on a particular corpus, to show the generality of our method, we experiment with texts from two very different genres. Our first corpus is the standard RST-DT (Carlson et al., 2002), which consists of 385 Wall Street Journal articles, and is partitioned into a training set of 347 documents and a test set of 38 documents. 53 documents, selected from both sets were annotated by two annotators, based on which we measure human agreement. In RST-DT, the original 25 rhetorical relations defined by (Mann and Thompson, 1988) are further divided into a set of 18 coarser relation classes with 78 finer-grained relations. Our second corpus is the Instructional corpus prepared by (Subba and Di-Eugenio, 2009), which contains 176 how-to-do manuals on homerepair. The corpus was annotated with 26 informational relations (e.g., Preparation-Act, Act-Goal). 6.2 Experimental Setup We experiment with our discourse parser on the two datasets using our two different parsing approaches, namely 1S-1S and the sliding window. We compare our approach with HILDA (Hernault et al., 2010) on RST-DT, and with the ILP-based approach of (Su</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. Mann and S. Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>A Echihabi</author>
</authors>
<title>An Unsupervised Approach to Recognizing Discourse Relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>368--375</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8049" citStr="Marcu and Echihabi, 2002" startWordPosition="1211" endWordPosition="1214">oaches to combine the two stages of parsing. The experiments and error analysis, followed by future directions are discussed in Section 6. Finally, we summarize our contributions in Section 7. 2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al., (2010) presents the publicly available HILDA parser. Given the EDUs in a doc487 30 25 20 15 10 5 0 Multi-sentential Intra-sentential Sentences segmented into EDUs Document-level discourse tree Algorithm model Intra-sentential parser Algorithm model Multi-sentential parser Elaboration Joi</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>D. Marcu and A. Echihabi. 2002. An Unsupervised Approach to Recognizing Discourse Relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 368–375. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Rhetorical Parsing of Unrestricted Texts: A Surface-based Approach.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--395</pages>
<contexts>
<context position="1224" citStr="Marcu, 2000" startWordPosition="175" endWordPosition="176"> and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast</context>
<context position="7760" citStr="Marcu, 2000" startWordPosition="1169" endWordPosition="1170">s in document-level discourse parsing. In the rest of the paper, after discussing related work in Section 2, we present our discourse parsing framework in Section 3. In Section 4, we describe the intra- and multi-sentential parsing components. Section 5 presents the two approaches to combine the two stages of parsing. The experiments and error analysis, followed by future directions are discussed in Section 6. Finally, we summarize our contributions in Section 7. 2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al.,</context>
<context position="21958" citStr="Marcu, 2000" startWordPosition="3557" endWordPosition="3558">es are adopted from previous studies (Joty et al., 2012; Hernault et al., 2010), we briefly describe them. Organizational features include the length of the units as the number of EDUs and tokens. It also includes the distances of the units from the beginning and end of the sentence (or text in the multi-sentential case). Text structural features indirectly capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the units. Discourse markers (e.g., because, although) carry informative clues for rhetorical relations (Marcu, 2000a). Rather than using a fixed list of discourse markers, we use an empirically learned lexical N-gram dictionary following (Joty et al., 2012). This approach has been shown to be more robust and flexible across domains (Biran and Rambow, 2011; Hernault et al., 2010). We also include part-of-speech (POS) tags for the beginning and end N tokens in a unit. 8 Organizational features Intra &amp; Multi-Sentential Number of EDUs in unit 1 (or unit 2). Number of tokens in unit 1 (or unit 2). Distance of unit 1 in EDUs to the beginning (or to the end). Distance of unit 2 in EDUs to the beginning (or to the</context>
<context position="25275" citStr="Marcu, 2000" startWordPosition="4092" endWordPosition="4093">gorithm is to find the most probable DT for that scenario. Following (Joty et al., 2012), we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse using dynamic programming. Specifically, with n discourse units, we use the upper-triangular portion of the nxn dynamic programming table D. Given Ux(0) and Ux(1) are the start and end EDU Ids of unit Ux: D[i,j] = P(R[Ui(0), Uk(1), Uj(1)]) (3) where, k = argmax P(R[Ui(0), Up(1), Uj(1)]). i&lt;p&lt;j Note that, in contrast to previous studies on document-level parsing (Hernault et al., 2010; Subba and Di-Eugenio, 2009; Marcu, 2000b), which use a greedy algorithm, our approach finds a discourse tree that is globally optimal. 5 Document-level Parsing Approaches Now that we have presented our intra-sentential and our multi-sentential parsers, we are ready to describe how they can be effectively combined to perform document-level discourse analysis. Recall that a key motivation for a two-stage parsing is that it allows us to capture the correlation between text structure and discourse structure in a scalable, modular and flexible way. Below we describe two different approaches to model this correlation. 491 5.1 1S-1S (1 Se</context>
<context position="32271" citStr="Marcu, 2000" startWordPosition="5322" endWordPosition="5323">sed system of (Subba and Di-Eugenio, 2009) (not publicly available) on the Instructional corpus, we report the performances presented in their paper. They used 151 documents for training and 25 documents for testing. Since we did not have access to their particular split, we took 5 random samples of 151 documents for training and 25 documents for testing, and report the average performance over the 5 test sets. To evaluate the parsing performance, we use the standard unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) precision, recall and F-score as described in (Marcu, 2000b). To compare with previous studies, our experiments on RST-DT use the 18 coarser relations. After attaching the nuclearity statuses (NS, SN, NN) to these relations, we get 41 distinct relations. Following (Subba and Di-Eugenio, 2009) on the Instructional corpus, we use 26 relations, and treat the reversals of non-commutative relations as separate relations. That is, Goal-Act and Act-Goal are considered as two different relations. Attaching the nuclearity statuses to these relations gives 76 distinct relations. Analogous to previous studies, we map the n-ary relations (e.g., Joint) into neste</context>
<context position="34713" citStr="Marcu, (2000" startWordPosition="5699" endWordPosition="5700">ations are (semantically) very similar (e.g., Preparation-Act, Step1-Step2), which makes it difficult even for the human annotators to distinguish them (Subba and Di-Eugenio, 2009). Comparison between our two models reveals that TSP SW significantly outperforms TSP 1-1 only in finding the right structure on both corpora (p&lt;0.01). Not surprisingly, the improvement is higher on the Instructional corpus. A likely explanation is that the Instructional corpus contains more leaky boundaries (12%), allowing the sliding 2Precision, Recall and F-score are the same when manual segmentation is used (see Marcu, (2000b), page 143). 3Since we did not have access to the output or to the system of (Subba and Di-Eugenio, 2009), we were not able to perform a significance test on the Instructional corpus. 493 RST-DT Instructional Metrics HILDA TSP 1-1 TSP SW Human ILP TSP 1-1 TSP SW Span 74.68 82.47* 82.74*† 88.70 70.35 79.67 80.88† Nuclearity 58.99 68.43* 68.40* 77.72 49.47 63.03 63.10 Relation 44.32 55.73* 55.71* 65.75 35.44 43.52 43.58 Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA (with p&lt;7.1e-05) are denoted by *. Significant diffe</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000a. The Rhetorical Parsing of Unrestricted Texts: A Surface-based Approach. Computational Linguistics, 26:395–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1224" citStr="Marcu, 2000" startWordPosition="175" endWordPosition="176"> and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast</context>
<context position="7760" citStr="Marcu, 2000" startWordPosition="1169" endWordPosition="1170">s in document-level discourse parsing. In the rest of the paper, after discussing related work in Section 2, we present our discourse parsing framework in Section 3. In Section 4, we describe the intra- and multi-sentential parsing components. Section 5 presents the two approaches to combine the two stages of parsing. The experiments and error analysis, followed by future directions are discussed in Section 6. Finally, we summarize our contributions in Section 7. 2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al.,</context>
<context position="21958" citStr="Marcu, 2000" startWordPosition="3557" endWordPosition="3558">es are adopted from previous studies (Joty et al., 2012; Hernault et al., 2010), we briefly describe them. Organizational features include the length of the units as the number of EDUs and tokens. It also includes the distances of the units from the beginning and end of the sentence (or text in the multi-sentential case). Text structural features indirectly capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the units. Discourse markers (e.g., because, although) carry informative clues for rhetorical relations (Marcu, 2000a). Rather than using a fixed list of discourse markers, we use an empirically learned lexical N-gram dictionary following (Joty et al., 2012). This approach has been shown to be more robust and flexible across domains (Biran and Rambow, 2011; Hernault et al., 2010). We also include part-of-speech (POS) tags for the beginning and end N tokens in a unit. 8 Organizational features Intra &amp; Multi-Sentential Number of EDUs in unit 1 (or unit 2). Number of tokens in unit 1 (or unit 2). Distance of unit 1 in EDUs to the beginning (or to the end). Distance of unit 2 in EDUs to the beginning (or to the</context>
<context position="25275" citStr="Marcu, 2000" startWordPosition="4092" endWordPosition="4093">gorithm is to find the most probable DT for that scenario. Following (Joty et al., 2012), we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse using dynamic programming. Specifically, with n discourse units, we use the upper-triangular portion of the nxn dynamic programming table D. Given Ux(0) and Ux(1) are the start and end EDU Ids of unit Ux: D[i,j] = P(R[Ui(0), Uk(1), Uj(1)]) (3) where, k = argmax P(R[Ui(0), Up(1), Uj(1)]). i&lt;p&lt;j Note that, in contrast to previous studies on document-level parsing (Hernault et al., 2010; Subba and Di-Eugenio, 2009; Marcu, 2000b), which use a greedy algorithm, our approach finds a discourse tree that is globally optimal. 5 Document-level Parsing Approaches Now that we have presented our intra-sentential and our multi-sentential parsers, we are ready to describe how they can be effectively combined to perform document-level discourse analysis. Recall that a key motivation for a two-stage parsing is that it allows us to capture the correlation between text structure and discourse structure in a scalable, modular and flexible way. Below we describe two different approaches to model this correlation. 491 5.1 1S-1S (1 Se</context>
<context position="32271" citStr="Marcu, 2000" startWordPosition="5322" endWordPosition="5323">sed system of (Subba and Di-Eugenio, 2009) (not publicly available) on the Instructional corpus, we report the performances presented in their paper. They used 151 documents for training and 25 documents for testing. Since we did not have access to their particular split, we took 5 random samples of 151 documents for training and 25 documents for testing, and report the average performance over the 5 test sets. To evaluate the parsing performance, we use the standard unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) precision, recall and F-score as described in (Marcu, 2000b). To compare with previous studies, our experiments on RST-DT use the 18 coarser relations. After attaching the nuclearity statuses (NS, SN, NN) to these relations, we get 41 distinct relations. Following (Subba and Di-Eugenio, 2009) on the Instructional corpus, we use 26 relations, and treat the reversals of non-commutative relations as separate relations. That is, Goal-Act and Act-Goal are considered as two different relations. Attaching the nuclearity statuses to these relations gives 76 distinct relations. Analogous to previous studies, we map the n-ary relations (e.g., Joint) into neste</context>
<context position="34713" citStr="Marcu, (2000" startWordPosition="5699" endWordPosition="5700">ations are (semantically) very similar (e.g., Preparation-Act, Step1-Step2), which makes it difficult even for the human annotators to distinguish them (Subba and Di-Eugenio, 2009). Comparison between our two models reveals that TSP SW significantly outperforms TSP 1-1 only in finding the right structure on both corpora (p&lt;0.01). Not surprisingly, the improvement is higher on the Instructional corpus. A likely explanation is that the Instructional corpus contains more leaky boundaries (12%), allowing the sliding 2Precision, Recall and F-score are the same when manual segmentation is used (see Marcu, (2000b), page 143). 3Since we did not have access to the output or to the system of (Subba and Di-Eugenio, 2009), we were not able to perform a significance test on the Instructional corpus. 493 RST-DT Instructional Metrics HILDA TSP 1-1 TSP SW Human ILP TSP 1-1 TSP SW Span 74.68 82.47* 82.74*† 88.70 70.35 79.67 80.88† Nuclearity 58.99 68.43* 68.40* 77.72 49.47 63.03 63.10 Relation 44.32 55.73* 55.71* 65.75 35.44 43.52 43.58 Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA (with p&lt;7.1e-05) are denoted by *. Significant diffe</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000b. The Theory and Practice of Discourse Parsing and Summarization. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical Cohesion Computed by Thesaural Relations as an Indicator of Structure of Text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="23892" citStr="Morris and Hirst, 1991" startWordPosition="3882" endWordPosition="3885">in unit 2). Number of chains skipping both unit 1 and unit 2. Number of chains skipping unit 1 (or unit 2). 2 Contextual features Intra &amp; Multi-Sentential Previous and next feature vectors. 2 Substructure features Intra &amp; Multi-Sentential Root nodes of the left and right rhetorical sub-trees. Table 1: Features used in our parsing models. Lexico-syntactic features dominance sets (Soricut and Marcu, 2003) are very effective for intra-sentential parsing. We include syntactic labels and lexical heads of head and attachment nodes along with their dominance relationship as features. Lexical chains (Morris and Hirst, 1991) are sequences of semantically related words that can indicate topic shifts. Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004). We compute lexical chains for a document following the approach proposed in (Galley and McKeown, 2003), that extracts lexical chains after performing word sense disambiguation. Following (Joty et al., 2012), we also encode contextual and rhetorical sub-structure features in our models. The rhetorical sub-structure features incorporate hierarchical dependencies between DT</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>J. Morris and G. Hirst. 1991. Lexical Cohesion Computed by Thesaural Relations as an Indicator of Structure of Text. Computational Linguistics, 17(1):21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>A Joshi</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse TreeBank as a Resource for Natural Language Generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Corpus Linguistics Workshop on Using Corpora for Natural Language Generation,</booktitle>
<pages>25--32</pages>
<location>Birmingham, U.K.</location>
<contexts>
<context position="1316" citStr="Prasad et al., 2005" startWordPosition="185" endWordPosition="188">ese two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by internal ∗This work was conducted at the U</context>
</contexts>
<marker>Prasad, Joshi, Dinesh, Lee, Miltsakaki, Webber, 2005</marker>
<rawString>R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, and B. Webber. 2005. The Penn Discourse TreeBank as a Resource for Natural Language Generation. In Proceedings of the Corpus Linguistics Workshop on Using Corpora for Natural Language Generation, pages 25–32, Birmingham, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
</authors>
<title>Discourse-Level Relations for Opinion Analysis.</title>
<date>2010</date>
<tech>PhD thesis,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="1357" citStr="Somasundaran, 2010" startWordPosition="191" endWordPosition="192">ively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by internal ∗This work was conducted at the University of British Columbia, Vancouver,</context>
</contexts>
<marker>Somasundaran, 2010</marker>
<rawString>S. Somasundaran, 2010. Discourse-Level Relations for Opinion Analysis. PhD thesis, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence Level Discourse Parsing Using Syntactic and Lexical Information.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL-HLT ’03,</booktitle>
<pages>149--156</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Edmonton, Canada.</location>
<contexts>
<context position="11270" citStr="Soricut and Marcu, 2003" startWordPosition="1693" endWordPosition="1696">ions of six most 1For n + 1 EDUs, the number of valid discourse trees is actually the Catalan number Cn. Figure 3: Discourse parsing framework. frequent relations on a development set containing 20 randomly selected documents from RST-DT. Notice that relations Attribution and Same-Unit are more frequent than Joint in intra-sentential case, whereas Joint is more frequent than the other two in multi-sentential case. On the other hand, different kinds of features are applicable and informative for intra-sentential vs. multi-sentential parsing. For example, syntactic features like dominance sets (Soricut and Marcu, 2003) are extremely useful for sentence-level parsing, but are not even applicable in multi-sentential case. Likewise, lexical chain features (Sporleder and Lascarides, 2004), that are useful for multi-sentential parsing, are not applicable at the sentence level. Based on these observations, our discourse parsing framework comprises two separate modules: an intra-sentential parser and a multisentential parser (Figure 3). First, the intrasentential parser produces one or more discourse sub-trees for each sentence. Then, the multisentential parser generates a full DT for the document from these sub-t</context>
<context position="23675" citStr="Soricut and Marcu, 2003" startWordPosition="3851" endWordPosition="3854">d node and the attachment node. Dominance relationship between the two units. 8 Lexical chain features Multi-Sentential Number of chains start in unit 1 and end in unit 2. Number of chains start (or end) in unit 1 (or in unit 2). Number of chains skipping both unit 1 and unit 2. Number of chains skipping unit 1 (or unit 2). 2 Contextual features Intra &amp; Multi-Sentential Previous and next feature vectors. 2 Substructure features Intra &amp; Multi-Sentential Root nodes of the left and right rhetorical sub-trees. Table 1: Features used in our parsing models. Lexico-syntactic features dominance sets (Soricut and Marcu, 2003) are very effective for intra-sentential parsing. We include syntactic labels and lexical heads of head and attachment nodes along with their dominance relationship as features. Lexical chains (Morris and Hirst, 1991) are sequences of semantically related words that can indicate topic shifts. Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004). We compute lexical chains for a document following the approach proposed in (Galley and McKeown, 2003), that extracts lexical chains after performing word s</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R. Soricut and D. Marcu. 2003. Sentence Level Discourse Parsing Using Syntactic and Lexical Information. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL-HLT ’03, pages 149– 156, Edmonton, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>M Lapata</author>
</authors>
<title>Discourse Chunking and its Application to Sentence Compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>257--264</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="1277" citStr="Sporleder and Lapata, 2005" startWordPosition="179" endWordPosition="182">rsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by int</context>
</contexts>
<marker>Sporleder, Lapata, 2005</marker>
<rawString>C. Sporleder and M. Lapata. 2005. Discourse Chunking and its Application to Sentence Compression. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 257–264, Vancouver, British Columbia, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>A Lascarides</author>
</authors>
<title>Combining Hierarchical Clustering and Machine Learning to Predict High-Level Discourse Structure.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04,</booktitle>
<institution>Geneva, Switzerland. Association for Computational Linguistics.</institution>
<contexts>
<context position="8157" citStr="Sporleder and Lascarides, 2004" startWordPosition="1227" endWordPosition="1230">rections are discussed in Section 6. Finally, we summarize our contributions in Section 7. 2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al., (2010) presents the publicly available HILDA parser. Given the EDUs in a doc487 30 25 20 15 10 5 0 Multi-sentential Intra-sentential Sentences segmented into EDUs Document-level discourse tree Algorithm model Intra-sentential parser Algorithm model Multi-sentential parser Elaboration Joint AttributionSame-Unit Contrast Explanation Figure 2: Distributions of six most frequent relations in intra</context>
<context position="11439" citStr="Sporleder and Lascarides, 2004" startWordPosition="1717" endWordPosition="1721"> on a development set containing 20 randomly selected documents from RST-DT. Notice that relations Attribution and Same-Unit are more frequent than Joint in intra-sentential case, whereas Joint is more frequent than the other two in multi-sentential case. On the other hand, different kinds of features are applicable and informative for intra-sentential vs. multi-sentential parsing. For example, syntactic features like dominance sets (Soricut and Marcu, 2003) are extremely useful for sentence-level parsing, but are not even applicable in multi-sentential case. Likewise, lexical chain features (Sporleder and Lascarides, 2004), that are useful for multi-sentential parsing, are not applicable at the sentence level. Based on these observations, our discourse parsing framework comprises two separate modules: an intra-sentential parser and a multisentential parser (Figure 3). First, the intrasentential parser produces one or more discourse sub-trees for each sentence. Then, the multisentential parser generates a full DT for the document from these sub-trees. Both of our parsers have the same two components: a parsing model assigns a probability to every possible DT, and a parsing algorithm identifies the most probable </context>
<context position="24117" citStr="Sporleder and Lascarides, 2004" startWordPosition="3913" endWordPosition="3916">ra &amp; Multi-Sentential Root nodes of the left and right rhetorical sub-trees. Table 1: Features used in our parsing models. Lexico-syntactic features dominance sets (Soricut and Marcu, 2003) are very effective for intra-sentential parsing. We include syntactic labels and lexical heads of head and attachment nodes along with their dominance relationship as features. Lexical chains (Morris and Hirst, 1991) are sequences of semantically related words that can indicate topic shifts. Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004). We compute lexical chains for a document following the approach proposed in (Galley and McKeown, 2003), that extracts lexical chains after performing word sense disambiguation. Following (Joty et al., 2012), we also encode contextual and rhetorical sub-structure features in our models. The rhetorical sub-structure features incorporate hierarchical dependencies between DT constituents. 4.4 Parsing Algorithm Given the probability of all possible DT constituents in the intra-sentential and multi-sentential scenarios, the job of the parsing algorithm is to find the most probable DT for that scen</context>
</contexts>
<marker>Sporleder, Lascarides, 2004</marker>
<rawString>C. Sporleder and A. Lascarides. 2004. Combining Hierarchical Clustering and Machine Learning to Predict High-Level Discourse Structure. In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04, Geneva, Switzerland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stede</author>
</authors>
<title>The Potsdam Commentary Corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL-04 Workshop on Discourse Annotation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona.</location>
<contexts>
<context position="36852" citStr="Stede, 2004" startWordPosition="6259" endWordPosition="6260">ons. The relations are Topic-Change (T-C), Topic-Comment (T-CM), Textual Organization (TO), Manner-Means (M-M), Comparison (CMP), Evaluation (EV), Summary (SU), Condition (CND), Enablement (EN), Cause (CA), Temporal (TE), Explanation (EX), Background (BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribution (AT) and Elaboration (EL). window approach to be more effective in finding those, without inducing much noise for the labels. This clearly demonstrates the potential of TSP SW for datasets with even more leaky boundaries e.g., the Dutch (Vliet and Redeker, 2011) and the German Potsdam (Stede, 2004) corpora. Error analysis reveals that although TSP SW finds more correct structures, a corresponding improvement in labeling relations is not present because in a few cases, it tends to induce noise from the neighboring sentences for the labels. For example, when parsing was performed on the first sentence in Figure 1 in isolation using 1S-1S, our parser rightly identifies the Contrast relation between EDUs 2 and 3. But, when it is considered with its neighboring sentences by the sliding window, the parser labels it as Elaboration. A promising strategy to deal with this and similar problems th</context>
</contexts>
<marker>Stede, 2004</marker>
<rawString>M. Stede. 2004. The Potsdam Commentary Corpus. In Proceedings of the ACL-04 Workshop on Discourse Annotation, Barcelona. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Subba</author>
<author>B Di-Eugenio</author>
</authors>
<title>An Effective Discourse Parser that Uses Rich Linguistic Information.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL ’09,</booktitle>
<pages>566--574</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado.</location>
<contexts>
<context position="9264" citStr="Subba and Di-Eugenio (2009)" startWordPosition="1385" endWordPosition="1388">boration Joint AttributionSame-Unit Contrast Explanation Figure 2: Distributions of six most frequent relations in intra-sentential and multi-sentential parsing scenarios. ument, HILDA iteratively employs two Support Vector Machine (SVM) classifiers in pipeline to build the DT. In each iteration, a binary classifier first decides which of the adjacent units to merge, then a multi-class classifier connects the selected units with an appropriate relation label. They evaluate their approach on the RST-DT corpus (Carlson et al., 2002) of news articles. On a different genre of instructional texts, Subba and Di-Eugenio (2009) propose a shift-reduce parser that relies on a classifier for relation labeling. Their classifier uses Inductive Logic Programming (ILP) to learn first-order logic rules from a set of features including compositional semantics. In this work, we address the limitations of these models (described in Section 1) introducing our novel discourse parser. 3 Our Discourse Parsing Framework Given a document with sentences already segmented into EDUs, the discourse parsing problem is determining which discourse units (EDUs or larger units) to relate (i.e., the structure), and how to relate them (i.e., t</context>
<context position="25262" citStr="Subba and Di-Eugenio, 2009" startWordPosition="4088" endWordPosition="4091">s, the job of the parsing algorithm is to find the most probable DT for that scenario. Following (Joty et al., 2012), we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse using dynamic programming. Specifically, with n discourse units, we use the upper-triangular portion of the nxn dynamic programming table D. Given Ux(0) and Ux(1) are the start and end EDU Ids of unit Ux: D[i,j] = P(R[Ui(0), Uk(1), Uj(1)]) (3) where, k = argmax P(R[Ui(0), Up(1), Uj(1)]). i&lt;p&lt;j Note that, in contrast to previous studies on document-level parsing (Hernault et al., 2010; Subba and Di-Eugenio, 2009; Marcu, 2000b), which use a greedy algorithm, our approach finds a discourse tree that is globally optimal. 5 Document-level Parsing Approaches Now that we have presented our intra-sentential and our multi-sentential parsers, we are ready to describe how they can be effectively combined to perform document-level discourse analysis. Recall that a key motivation for a two-stage parsing is that it allows us to capture the correlation between text structure and discourse structure in a scalable, modular and flexible way. Below we describe two different approaches to model this correlation. 491 5.</context>
<context position="27261" citStr="Subba and Di-Eugenio, 2009" startWordPosition="4410" endWordPosition="4413">mplifies the parsing process, it totally ignores the cases where discourse structures violate sentence boundaries. For example, in the DT shown in Figure 7(b), sentence S2 does not have a well-formed sub-tree because some of its units attach to the left (4-5, 6) and some to the right (7). Vliet and Redeker (2011) call these cases as ‘leaky’ boundaries. Even though less than 5% of the sentences have leaky boundaries in RST-DT, in other corpora this can be true for a larger portion of the sentences. For example, we observe over 12% sentences with leaky boundaries in the Instructional corpus of (Subba and Di-Eugenio, 2009). However, we notice that in most cases where discourse structures violate sentence boundaries, its units are merged with the units of its adjacent sentences, as in Figure 7(b). For example, this is true for 75% cases in our development set containing 20 news articles from RST-DT and for 79% cases in our development set containing 20 how-to-do manuals from the Instructional corpus. Based on this observation, we propose a sliding window approach. In this approach, our intra-sentential parser works with a window of two consecutive sentences, and builds a DT for the two sentences. For example, gi</context>
<context position="30893" citStr="Subba and Di-Eugenio, 2009" startWordPosition="5101" endWordPosition="5104">ith texts from two very different genres. Our first corpus is the standard RST-DT (Carlson et al., 2002), which consists of 385 Wall Street Journal articles, and is partitioned into a training set of 347 documents and a test set of 38 documents. 53 documents, selected from both sets were annotated by two annotators, based on which we measure human agreement. In RST-DT, the original 25 rhetorical relations defined by (Mann and Thompson, 1988) are further divided into a set of 18 coarser relation classes with 78 finer-grained relations. Our second corpus is the Instructional corpus prepared by (Subba and Di-Eugenio, 2009), which contains 176 how-to-do manuals on homerepair. The corpus was annotated with 26 informational relations (e.g., Preparation-Act, Act-Goal). 6.2 Experimental Setup We experiment with our discourse parser on the two datasets using our two different parsing approaches, namely 1S-1S and the sliding window. We compare our approach with HILDA (Hernault et al., 2010) on RST-DT, and with the ILP-based approach of (Subba and Di-Eugenio, 2009) on the Instructional corpus, since they are the state-ofthe-art on the respective genres. On RST-DT, the standard split was used for training and testing pu</context>
<context position="32506" citStr="Subba and Di-Eugenio, 2009" startWordPosition="5357" endWordPosition="5360">nce we did not have access to their particular split, we took 5 random samples of 151 documents for training and 25 documents for testing, and report the average performance over the 5 test sets. To evaluate the parsing performance, we use the standard unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) precision, recall and F-score as described in (Marcu, 2000b). To compare with previous studies, our experiments on RST-DT use the 18 coarser relations. After attaching the nuclearity statuses (NS, SN, NN) to these relations, we get 41 distinct relations. Following (Subba and Di-Eugenio, 2009) on the Instructional corpus, we use 26 relations, and treat the reversals of non-commutative relations as separate relations. That is, Goal-Act and Act-Goal are considered as two different relations. Attaching the nuclearity statuses to these relations gives 76 distinct relations. Analogous to previous studies, we map the n-ary relations (e.g., Joint) into nested right-branching binary relations. 6.3 Results and Error Analysis Table 2 presents F-score parsing results for our parsers and the existing systems on the two corpora.2 On both corpora, our parser, namely, 1S-1S (TSP 1-1) and sliding </context>
<context position="34281" citStr="Subba and Di-Eugenio, 2009" startWordPosition="5632" endWordPosition="5635">s, respectively, over the ILP-based approach. Our parsers, therefore, reduce errors by 36%, 27% and 13% in span, nuclearity and relations, respectively. If we compare the performance of our parsers on the two corpora, we observe higher results on RST-DT. This can be explained in at least two ways. First, the Instructional corpus has a smaller amount of data with a larger set of relations (76 when nuclearity attached). Second, some frequent relations are (semantically) very similar (e.g., Preparation-Act, Step1-Step2), which makes it difficult even for the human annotators to distinguish them (Subba and Di-Eugenio, 2009). Comparison between our two models reveals that TSP SW significantly outperforms TSP 1-1 only in finding the right structure on both corpora (p&lt;0.01). Not surprisingly, the improvement is higher on the Instructional corpus. A likely explanation is that the Instructional corpus contains more leaky boundaries (12%), allowing the sliding 2Precision, Recall and F-score are the same when manual segmentation is used (see Marcu, (2000b), page 143). 3Since we did not have access to the output or to the system of (Subba and Di-Eugenio, 2009), we were not able to perform a significance test on the Inst</context>
</contexts>
<marker>Subba, Di-Eugenio, 2009</marker>
<rawString>R. Subba and B. Di-Eugenio. 2009. An Effective Discourse Parser that Uses Rich Linguistic Information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL ’09, pages 566–574, Boulder, Colorado. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>An Introduction to Conditional Random Fields. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="19203" citStr="Sutton and McCallum, 2012" startWordPosition="3093" endWordPosition="3097">ty of all possible higher-order constituents. However, the number of possible sequences and their length increase with the number of sentences in a document. For example, assuming that each sentence has a well-formed DT, for a document with n sentences, Algorithm 1 generates O(n3) sequences, where the sequence at the bottom level has n units, each of the sequences at the second level has n-1 units, and so on. Since the model in Figure 4 has a “fat” chain structure, Figure 6: A CRF as a multi-sentential parsing model. we could use forwards-backwards algorithm for exact inference in this model (Sutton and McCallum, 2012). However, forwards-backwards on a sequence containing T units costs O(TM2) time, where M is the number of relations in our relation set. This makes the chain-structured DCRF model impractical for multi-sentential parsing of long documents, since learning requires to run inference on every training sequence with an overall time complexity of O(TM2n3) per document. Our model for multi-sentential parsing is shown in Figure 6. The two observed nodes Ut−1 and Ut are two adjacent discourse units. The (hidden) structure node S∈{0, 1} denotes whether the two units should be connected or not. The hidd</context>
</contexts>
<marker>Sutton, McCallum, 2012</marker>
<rawString>C. Sutton and A. McCallum. 2012. An Introduction to Conditional Random Fields. Foundations and Trends in Machine Learning, 4(4):267–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
<author>K Rohanimanesh</author>
</authors>
<title>Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>8--693</pages>
<contexts>
<context position="3559" citStr="Sutton et al., 2007" startWordPosition="529" endWordPosition="532">take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequential dependencies between the DT constituents, which has been recently shown to be critical (Feng and Hirst, 2012). To address this limitation, as the first contribution, we propose a novel document-level discourse parser based on probabilistic discriminative parsing models, represented as Conditional Random Fields (CRFs) (Sutton et al., 2007), to infer the probability of all possible DT constituents. The CRF models effectively represent the structure and the label of a DT constituent jointly, and whenever possible, capture the sequential dependencies between the constituents. Second, existing parsers apply greedy and suboptimal parsing algorithms to build the DT for a document. To cope with this limitation, our CRF models support a probabilistic bottom-up parsing 486 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 486–496, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Compu</context>
<context position="14162" citStr="Sutton et al., 2007" startWordPosition="2157" endWordPosition="2160">stribution over the label R and the structure [i, m, j] of the constituent. 4.1 Intra-Sentential Parsing Model Recently, we proposed a novel parsing model for sentence-level discourse parsing (Joty et al., 2012), that outperforms previous approaches by effectively modeling sequential dependencies along with structure and labels jointly. Below we briefly describe the parsing model, and show how it is applied to obtain the probabilities of all possible DT constituents at the sentence level. Figure 4 shows the intra-sentential parsing model expressed as a Dynamic Conditional Random Field (DCRF) (Sutton et al., 2007). The observed nodes Uj in a sequence represent the discourse units (EDUs or larger units). The first layer of hidden nodes are the structure nodes, where SjE{0,1} denotes whether two adjacent discourse units Uj−1 and Uj should be connected or not. The second layer of hidden nodes are the relation nodes, with RjE{1... M} denoting the relation between two adjacent units Uj−1 and Uj, where M is the total number of relations in the relation set. The connections between adjacent nodes in a hidden layer encode sequential dependencies between the respective hidden nodes, and can enforce constraints </context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>C. Sutton, A. McCallum, and K. Rohanimanesh. 2007. Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data. Journal of Machine Learning Research (JMLR), 8:693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Verberne</author>
<author>L Boves</author>
<author>N Oostdijk</author>
<author>P Coppen</author>
</authors>
<title>Evaluating Discourse-based Answer Extraction for Why-question Answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>735--736</pages>
<publisher>ACM.</publisher>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="1404" citStr="Verberne et al., 2007" startWordPosition="196" endWordPosition="199">wo different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by internal ∗This work was conducted at the University of British Columbia, Vancouver, Canada. nodes), which in turn are also subject</context>
</contexts>
<marker>Verberne, Boves, Oostdijk, Coppen, 2007</marker>
<rawString>S. Verberne, L. Boves, N. Oostdijk, and P. Coppen. 2007. Evaluating Discourse-based Answer Extraction for Why-question Answering. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 735–736, Amsterdam, The Netherlands. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Vliet</author>
<author>G Redeker</author>
</authors>
<title>Complex Sentences as Leaky Units in Discourse Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of Constraints in Discourse,</booktitle>
<location>Agay-Saint Raphael,</location>
<contexts>
<context position="26948" citStr="Vliet and Redeker (2011)" startWordPosition="4355" endWordPosition="4358">structs a DT for every sentence using our intra-sentential parser, and then it provides our multi-sentential parser with the sentence-level DTs to build the rhetorical parse for the whole document. Figure 7: Two possible DTs for three sentences. 5.2 Sliding Window While the assumption made by 1S-1S clearly simplifies the parsing process, it totally ignores the cases where discourse structures violate sentence boundaries. For example, in the DT shown in Figure 7(b), sentence S2 does not have a well-formed sub-tree because some of its units attach to the left (4-5, 6) and some to the right (7). Vliet and Redeker (2011) call these cases as ‘leaky’ boundaries. Even though less than 5% of the sentences have leaky boundaries in RST-DT, in other corpora this can be true for a larger portion of the sentences. For example, we observe over 12% sentences with leaky boundaries in the Instructional corpus of (Subba and Di-Eugenio, 2009). However, we notice that in most cases where discourse structures violate sentence boundaries, its units are merged with the units of its adjacent sentences, as in Figure 7(b). For example, this is true for 75% cases in our development set containing 20 news articles from RST-DT and fo</context>
<context position="36815" citStr="Vliet and Redeker, 2011" startWordPosition="6250" endWordPosition="6253">sents true and X-axis represents predicted relations. The relations are Topic-Change (T-C), Topic-Comment (T-CM), Textual Organization (TO), Manner-Means (M-M), Comparison (CMP), Evaluation (EV), Summary (SU), Condition (CND), Enablement (EN), Cause (CA), Temporal (TE), Explanation (EX), Background (BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribution (AT) and Elaboration (EL). window approach to be more effective in finding those, without inducing much noise for the labels. This clearly demonstrates the potential of TSP SW for datasets with even more leaky boundaries e.g., the Dutch (Vliet and Redeker, 2011) and the German Potsdam (Stede, 2004) corpora. Error analysis reveals that although TSP SW finds more correct structures, a corresponding improvement in labeling relations is not present because in a few cases, it tends to induce noise from the neighboring sentences for the labels. For example, when parsing was performed on the first sentence in Figure 1 in isolation using 1S-1S, our parser rightly identifies the Contrast relation between EDUs 2 and 3. But, when it is considered with its neighboring sentences by the sliding window, the parser labels it as Elaboration. A promising strategy to d</context>
</contexts>
<marker>Vliet, Redeker, 2011</marker>
<rawString>N. Vliet and G. Redeker. 2011. Complex Sentences as Leaky Units in Discourse Parsing. In Proceedings of Constraints in Discourse, Agay-Saint Raphael, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>