<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000210">
<title confidence="0.999387">
Soft Syntactic Constraints for Word Alignment
through Discriminative Training
</title>
<author confidence="0.998537">
Colin Cherry
</author>
<affiliation confidence="0.881594333333333">
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
</affiliation>
<email confidence="0.977802">
colinc@cs.ualberta.ca
</email>
<author confidence="0.922923">
Dekang Lin
</author>
<affiliation confidence="0.908718">
Google Inc.
</affiliation>
<address confidence="0.5777755">
1600 Amphitheatre Parkway
Mountain View, CA, USA, 94043
</address>
<email confidence="0.996156">
lindek@google.com
</email>
<sectionHeader confidence="0.99736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996415">
Word alignment methods can gain valu-
able guidance by ensuring that their align-
ments maintain cohesion with respect to
the phrases specified by a monolingual de-
pendency tree. However, this hard con-
straint can also rule out correct alignments,
and its utility decreases as alignment mod-
els become more complex. We use a pub-
licly available structured output SVM to
create a max-margin syntactic aligner with
a soft cohesion constraint. The resulting
aligner is the first, to our knowledge, to use
a discriminative learning method to train
an ITG bitext parser.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999345625">
Given a parallel sentence pair, or bitext, bilin-
gual word alignment finds word-to-word connec-
tions across languages. Originally introduced as a
byproduct of training statistical translation models
in (Brown et al., 1993), word alignment has be-
come the first step in training most statistical trans-
lation systems, and alignments are useful to a host
of other tasks. The dominant IBM alignment mod-
els (Och and Ney, 2003) use minimal linguistic in-
tuitions: sentences are treated as flat strings. These
carefully designed generative models are difficult
to extend, and have resisted the incorporation of
intuitively useful features, such as morphology.
There have been many attempts to incorporate
syntax into alignment; we will not present a com-
plete list here. Some methods parse two flat strings
at once using a bitext grammar (Wu, 1997). Others
parse one of the two strings before alignment be-
gins, and align the resulting tree to the remaining
string (Yamada and Knight, 2001). The statisti-
cal models associated with syntactic aligners tend
to be very different from their IBM counterparts.
They model operations that are meaningful at a
syntax level, like re-ordering children, but ignore
features that have proven useful in IBM models,
such as the preference to align words with simi-
lar positions, and the HMM preference for links to
appear near one another (Vogel et al., 1996).
Recently, discriminative learning technology
for structured output spaces has enabled several
discriminative word alignment solutions (Liu et
al., 2005; Moore, 2005; Taskar et al., 2005). Dis-
criminative learning allows easy incorporation of
any feature one might have access to during the
alignment search. Because the features are han-
dled so easily, discriminative methods use features
that are not tied directly to the search: the search
and the model become decoupled.
In this work, we view synchronous parsing only
as a vehicle to expose syntactic features to a dis-
criminative model. This allows us to include the
constraints that would usually be imposed by a
tree-to-string alignment method as a feature in our
model, creating a powerful soft constraint. We
add our syntactic features to an already strong
flat-string discriminative solution, and we show
that they provide new information resulting in im-
proved alignments.
</bodyText>
<sectionHeader confidence="0.995211" genericHeader="method">
2 Constrained Alignment
</sectionHeader>
<bodyText confidence="0.999954666666667">
Let an alignment be the complete structure that
connects two parallel sentences, and a link be
one of the word-to-word connections that make
up an alignment. All word alignment methods
benefit from some set of constraints. These limit
the alignment search space and encourage com-
petition between potential links. The IBM mod-
els (Brown et al., 1993) benefit from a one-to-
many constraint, where each target word has ex-
</bodyText>
<page confidence="0.988652">
105
</page>
<note confidence="0.969139">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 105–112,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999822">
Figure 1: A cohesion constraint violation.
</figureCaption>
<bodyText confidence="0.999888428571429">
actly one generator in the source. Methods like
competitive linking (Melamed, 2000) and maxi-
mum matching (Taskar et al., 2005) use a one-to-
one constraint, where words in either sentence can
participate in at most one link. Throughout this pa-
per we assume a one-to-one constraint in addition
to any syntax constraints.
</bodyText>
<subsectionHeader confidence="0.979357">
2.1 Cohesion Constraint
</subsectionHeader>
<bodyText confidence="0.999893545454546">
Suppose we are given a parse tree for one of the
two sentences in our sentence pair. We will re-
fer to the parsed language as English, and the
unparsed language as Foreign. Given this infor-
mation, a reasonable expectation is that English
phrases will move together when projected onto
Foreign. When this occurs, the alignment is said
to maintain phrasal cohesion.
Fox (2002) measured phrasal cohesion in gold
standard alignments by counting crossings. Cross-
ings occur when the projections of two disjoint
phrases overlap. For example, Figure 1 shows a
head-modifier crossing: the projection of the the
tax subtree, impˆot ... le, is interrupted by the pro-
jection of its head, cause. Alignments with no
crossings maintain phrasal cohesion. Fox’s exper-
iments show that cohesion is generally maintained
for French-English, and that dependency trees pro-
duce the highest degree of cohesion among the
tested structures.
Cherry and Lin (2003) use the phrasal cohesion
of a dependency tree as a constraint on a beam
search aligner. This constraint produces a sig-
nificant reduction in alignment error rate. How-
ever, as Fox (2002) showed, even in a language
pair as close as French-English, there are situa-
tions where phrasal cohesion should not be main-
tained. These include incorrect parses, systematic
violations such as not —* ne ... pas, paraphrases,
and linguistic exceptions.
We aim to create an alignment system that
obeys cohesion constraints most of the time, but
can violate them when necessary. Unfortunately,
Cherry and Lin’s beam search solution does not
lend itself to a soft cohesion constraint. The im-
perfect beam search may not be able to find the
optimal alignment under a soft constraint. Further-
more, it is not clear what penalty to assign to cross-
ings, or how to learn such a penalty from an iter-
ative training process. The remainder of this pa-
per will develop a complete alignment search that
is aware of cohesion violations, and use discrimi-
native learning technology to assign a meaningful
penalty to those violations.
</bodyText>
<sectionHeader confidence="0.956123" genericHeader="method">
3 Syntax-aware Alignment Search
</sectionHeader>
<bodyText confidence="0.999983882352941">
We require an alignment search that can find the
globally best alignment under its current objective
function, and can account for phrasal cohesion in
this objective. IBM Models 1 and 2, HMM (Vo-
gel et al., 1996), and weighted maximum matching
alignment all conduct complete searches, but they
would not be amenable to monitoring the syntac-
tic interactions of links. The tree-to-string models
of (Yamada and Knight, 2001) naturally consider
syntax, but special modeling considerations are
needed to allow any deviations from the provided
tree (Gildea, 2003). The Inversion Transduction
Grammar or ITG formalism, described in (Wu,
1997), is well suited for our purposes. ITGs per-
form string-to-string alignment, but do so through
a parsing algorithm that will allow us to inform the
objective function of our dependency tree.
</bodyText>
<subsectionHeader confidence="0.992058">
3.1 Inversion Transduction Grammar
</subsectionHeader>
<bodyText confidence="0.958331181818182">
An ITG aligns bitext through synchronous pars-
ing. Both sentences are decomposed into con-
stituent phrases simultaneously, producing a word
alignment as a byproduct. Viewed generatively, an
ITG writes to two streams at once. Terminal pro-
ductions produce a token in each stream, or a token
in one stream with the null symbol 0 in the other.
We will use standard ITG notation: A —* e/f in-
dicates that the token e is produced on the English
stream, while f is produced on the Foreign stream.
To allow for some degree of movement during
translation, non-terminal productions are allowed
to be either straight or inverted. Straight pro-
ductions, with their non-terminals inside square
brackets [...], produce their symbols in the same
order on both streams. Inverted productions, in-
dicated by angled brackets (...), have their non-
terminals produced in the given order on the En-
glish stream, but this order is reversed in the For-
eign stream.
the tax causes unrest
l&apos; impTMt cause le malaise
</bodyText>
<page confidence="0.879434">
106
</page>
<figureCaption confidence="0.69544775">
the Canadian agriculture industry
l&apos; industrie agricole Canadienne
Figure 2: An example of an ITG alignment. A
horizontal bar across an arc indicates an inversion.
</figureCaption>
<bodyText confidence="0.972934333333333">
An ITG chart parser provides a polynomial-
time algorithm to conduct a complete enumeration
of all alignments that are possible according to its
grammar. We will use a binary bracketing ITG, the
simplest interesting grammar in this formalism:
A — [AA]  |(AA)  |e/f
This grammar enforces its own weak cohesion
constraint: for every possible alignment, a corre-
sponding binary constituency tree must exist for
which the alignment maintains phrasal cohesion.
Figure 2 shows a word alignment and the corre-
sponding tree found by an ITG parser. Wu (1997)
provides anecdotal evidence that only incorrect
alignments are eliminated by ITG constraints. In
our French-English data set, an ITG rules out
only 0.3% of necessary links beyond those already
eliminated by the one-to-one constraint (Cherry
and Lin, 2006).
</bodyText>
<subsectionHeader confidence="0.966824">
3.2 Dependency-augmented ITG
</subsectionHeader>
<bodyText confidence="0.9982534">
An ITG will search all alignments that conform
to a possible binary constituency tree. We wish
to confine that search to a specific n-array depen-
dency tree. Fortunately, Wu (1997) provides a
method to have an ITG respect a known partial
structure. One can seed the ITG parse chart so that
spans that do not agree with the provided structure
are assigned a value of —oc before parsing begins.
The result is that no constituent is ever constructed
with any of these invalid spans.
In the case of phrasal cohesion, the invalid spans
correspond to spans of the English sentence that
interrupt the phrases established by the provided
dependency tree. To put this notion formally, we
first define some terms: given a subtree T[Z,k],
where i is the left index of the leftmost leaf in T[Z,k]
and k is the right index of its rightmost leaf, we say
any index j E (i, k) is internal to T[Z,k]. Similarly,
any index x E/ [i, k] is external to T[Z,k]. An in-
valid span is any span for which our provided tree
</bodyText>
<figureCaption confidence="0.8914636">
Figure 3: Illustration of invalid spans. [j�, j] and
[j, k] are legal, while [x1, j] and [j, x2] are not.
the tax causes unrest
Figure 4: The invalid spans induced by a depen-
dency tree.
</figureCaption>
<bodyText confidence="0.999919107142857">
has a subtree T[Z,k] such that one endpoint of the
span is internal to T[Z k] while the other is external
to it. Figure 3 illustrates this definition, while Fig-
ure 4 shows the invalid spans induced by a simple
dependency tree.
With these invalid spans in place, the ITG can
no longer merge part of a dependency subtree with
anything other than another part of the same sub-
tree. Since all ITG movement can be explained
by inversions, this constrained ITG cannot in-
terrupt one dependency phrase with part of an-
other. Therefore, the phrasal cohesion of the in-
put dependency tree is maintained. Note that this
will not search the exact same alignment space
as a cohesion-constrained beam search; instead it
uses the union of the cohesion constraint and the
weaker ITG constraints (Cherry and Lin, 2006).
Transforming this form of the cohesion con-
straint into a soft constraint is straight-forward.
Instead of overriding the parser so it cannot use
invalid English spans, we will note the invalid
spans and assign the parser a penalty should it
use them. The value of this penalty will be de-
termined through discriminative training, as de-
scribed in Section 4. Since the penalty is avail-
able within the dynamic programming algorithm,
the parser will be able to incorporate it to find a
globally optimal alignment.
</bodyText>
<sectionHeader confidence="0.995143" genericHeader="method">
4 Discriminative Training
</sectionHeader>
<bodyText confidence="0.999759">
To discriminatively train our alignment systems,
we adopt the Support Vector Machine (SVM) for
</bodyText>
<equation confidence="0.952537666666667">
T
T[i,k]
x1 i j&apos; j k x2
</equation>
<page confidence="0.98717">
107
</page>
<bodyText confidence="0.999653">
Structured Output (Tsochantaridis et al., 2004).
We have selected this system for its high degree of
modularity, and because it has an API freely avail-
able1. We will summarize the learning mechanism
briefly in this section, but readers should refer to
(Tsochantaridis et al., 2004) for more details.
SVM learning is most easily expressed as a con-
strained numerical optimization problem. All con-
straints mentioned in this section are constraints
on this optimizer, and have nothing to do with the
cohesion constraint from Section 2.
</bodyText>
<subsectionHeader confidence="0.90215">
4.1 SVM for Structured Output
</subsectionHeader>
<bodyText confidence="0.999848875">
Traditional SVMs attempt to find a linear sepa-
rator that creates the largest possible margin be-
tween two classes of vectors. Structured output
SVMs attempt to separate the correct structure
from all incorrect structures by the largest possible
margin, for all training instances. This may sound
like a much more difficult problem, but with a few
assumptions in place, the task begins to look very
similar to a traditional SVM.
As in most discriminative training methods, we
begin by assuming that a candidate structure y,
built for an input instance x, can be adequately de-
scribed using a feature vector IF(x, y). We also as-
sume that our 1F(x, y) decomposes in such a way
that the features can guide a search to recover the
structure y from x. That is:
</bodyText>
<equation confidence="0.886564">
struct(x; w) = argmaxyEY (w,&apos;F(x,y)) (1)
</equation>
<bodyText confidence="0.996917352941177">
is computable, where Y is the set of all possible
structures, and w is a vector that assigns weights
to each component of IF(x, y). w is the parameter
vector we will learn using our SVM.
Now the learning task begins to look straight-
forward: we are working with vectors, and the
task of building a structure y has been recast as
an argmax operator. Our learning goal is to find a
w so that the correct structure is found:
Vi, Vy E Y \ yi : (w, &apos;Pi(yi)) &gt; (w, &apos;Pi(y)) (2)
where xi is the ith training example, yi is its
correct structure, and 1&amp;i(y) is short-hand for
IF(xi, y). As several w will fulfill (2) in a linearly
separable training set, the unique max-margin ob-
jective is defined to be the w that maximizes the
minimum distance between yi and the incorrect
structures in Y.
</bodyText>
<footnote confidence="0.976507">
1At http://svmlight.joachims.org/svm struct.html
</footnote>
<bodyText confidence="0.999835851851852">
This learning framework also incorporates a no-
tion of structured loss. In a standard vector clas-
sification problem, there is 0-1 loss: a vector is
either classified correctly or it is not. In the struc-
tured case, some incorrect structures can be bet-
ter than others. For example, having the argmax
select an alignment missing only one link is bet-
ter than selecting one with no correct links and a
dozen wrong ones. A loss function A(yi, y) quan-
tifies just how incorrect a particular structure y is.
Though Tsochantaridis et al. (2004) provide sev-
eral ways to incorporate loss into the SVM ob-
jective, we will use margin re-scaling, as it corre-
sponds to loss usage in another max-margin align-
ment approach (Taskar et al., 2005). In margin
re-scaling, high loss structures must be separated
from the correct structure by a larger margin than
low loss structures.
To allow some misclassifications during train-
ing, a soft-margin requirement replaces our max-
margin objective. A slack variable �i is introduced
for each training example xi, to allow the learner
to violate the margin at a penalty. The magnitude
of this penalty to determined by a hand-tuned pa-
rameter C. After a few transformations (Tsochan-
taridis et al., 2004), the soft-margin learning ob-
jective can be formulated as a quadratic program:
</bodyText>
<equation confidence="0.9989656">
�������� � Cn
1
�i=1
Vi, Vy E Y \ yi : (4)
(w, `Pi(yi) − &apos;Pi(y)) &gt; A(yi,y) − �i
</equation>
<bodyText confidence="0.9997887">
Note how the slack variables �i allow some in-
correct structures to be built. Also note that the
loss A(yi, y) determines the size of the margin be-
tween structures.
Unfortunately, (4) provides one constraint for
every possible structure for every training exam-
ple. Enumerating these constraints explicitly is in-
feasible, but in reality, only a subset of these con-
straints are necessary to achieve the same objec-
tive. Re-organizing (4) produces:
</bodyText>
<equation confidence="0.9762938">
Vi, Vy E Y \ yi :
�i &gt; A(yi,y) − (w,I&amp;i(yi) − &apos;Pi(y)) (5)
which is equivalent to:
Vi :�i &gt; max
yEY\yi
</equation>
<bodyText confidence="0.975168">
where costi is defined as:
</bodyText>
<equation confidence="0.99945375">
costi(y; w) = A(yi,y) − (w, &apos;Pi(yi) − &apos;Pi(y))
mint���
�i, s.t. Vi�i &gt; 0 (3)
costi(y; w) (6)
</equation>
<page confidence="0.988664">
108
</page>
<bodyText confidence="0.9996146">
Provided that the max cost structure can be found
in polynomial time, we have all the components
needed for a constraint generation approach to this
optimization problem.
Constraint generation places an outer loop
around an optimizer that minimizes (3) repeatedly
for a growing set of constraints. It begins by min-
imizing (3) with an empty constraint set in place
of (4). This provides values for w~ and ~ξ. The max
cost structure
</bodyText>
<equation confidence="0.997043">
y� = argmaxyEY\yicosti(y; ~w)
</equation>
<bodyText confidence="0.9996955">
is found for i = 1 with the current ~w. If the re-
sulting costi(y; ~w) is greater than the current value
of ξi, then this represents a violated constraint2 in
our complete objective, and a new constraint of
the form ξi ≥ costi(y; ~w) is added to the con-
straint set. The algorithm then iterates: the opti-
mizer minimizes (3) again with the new constraint
set, and solves the max cost problem for i = i + 1
with the new ~w, growing the constraint set if nec-
essary. Note that the constraints on ξ change with
~w, as cost is a function of ~w. Once the end of
the training set is reached, the learner loops back
to the beginning. Learning ends when the entire
training set can be processed without needing to
add any constraints. It can be shown that this
will occur within a polynomial number of itera-
tions (Tsochantaridis et al., 2004).
With this framework in place, one need only fill
in the details to create an SVM for a new struc-
tured output space:
</bodyText>
<listItem confidence="0.998633222222222">
1. A IF(x, y) function to transform instance-
structure pairs into feature vectors
2. A search to find the best structure given a
weight vector: argmaxy h~w, IF(x, y)i. This
has no role in training, but it is necessary to
use the learned weights.
3. A structured loss function A(y, y)
4. A search to find the max cost structure:
argmaxycosti(y; w)
</listItem>
<subsectionHeader confidence="0.689926">
4.2 SVMs for Alignment
</subsectionHeader>
<bodyText confidence="0.99917475">
Using the Structured SVM API, we have created
two SVM word aligners: a baseline that uses
weighted maximum matching for its argmax op-
erator, and a dependency-augmented ITG that will
</bodyText>
<footnote confidence="0.957648">
2Generally the test to see if �i &gt; costi(y; w) is approxi-
mated as �i &gt; costi(y; w) + E for a small constant E.
</footnote>
<bodyText confidence="0.99967425">
satisfy our requirements for an aligner with a soft
cohesion constraint. Our x becomes a bilingual
sentence-pair, while our y becomes an alignment,
represented by a set of links.
</bodyText>
<subsectionHeader confidence="0.506505">
4.2.1 Weighed Maximum Matching
</subsectionHeader>
<bodyText confidence="0.994983">
Given a bipartite graph with edge values, the
weighted maximum matching algorithm (West,
2001) will find the matching with maximum
summed edge values. To create a matching align-
ment solution, we reproduce the approach of
(Taskar et al., 2005) within the framework de-
scribed in Section 4.1:
</bodyText>
<listItem confidence="0.992962777777778">
1. We define a feature vector ψ for each poten-
tial link l in x, and IF in terms of y’s compo-
nent links: &apos;F(x, y) = ElEy ψ(l).
2. Our structure search is the matching algo-
rithm. The input bipartite graph has an edge
for each l. Each edge is given the value
v(l) ← h~w, ψ(l)i.
3. We adopt the weighted Hamming loss in de-
scribed (Taskar et al., 2005):
</listItem>
<equation confidence="0.808063">
0(y, y) = co|y − y |+ c,|y − y|
</equation>
<bodyText confidence="0.9867975">
where co is an omission penalty and c, is a
commission penalty.
</bodyText>
<listItem confidence="0.582376333333333">
4. Our max cost search corresponds to their
loss-augmented matching problem. The in-
put graph is modified to prefer costly links:
</listItem>
<bodyText confidence="0.9710564">
∀l ∈/ y : v(l) ← h~w, ψ(l)i + c,
∀l ∈ y : v(l) ← h~w, ψ(l)i − co
Note that our max cost search could not have been
implemented as loss-augmented matching had we
selected one of the other loss objectives presented
in (Tsochantaridis et al., 2004) in place of margin
rescaling.
We use the same feature representation ψ(l) as
(Taskar et al., 2005), with some small exceptions.
Let l = (Ej, Fk) be a potential link between the
jth word of English sentence E and the kth word
of Foreign sentence F. To measure correlation be-
tween Ej and Fk we use conditional link proba-
bility (Cherry and Lin, 2003) in place of the Dice
coefficient:
</bodyText>
<equation confidence="0.974401">
#links(Ej, Fk) − d
cor(Ej, Fk) = #cooccurrences(Ej, Fk)
</equation>
<bodyText confidence="0.994267666666667">
where the link counts are determined by word-
aligning 50K sentence pairs with another match-
ing SVM that uses the φ2 measure (Gale and
</bodyText>
<page confidence="0.998121">
109
</page>
<bodyText confidence="0.995039666666667">
Church, 1991) in place of Dice. The 02 measure
requires only co-occurrence counts. d is an abso-
lute discount parameter as in (Moore, 2005). Also,
we omit the IBM Model 4 Prediction features, as
we wish to know how well we can do without re-
sorting to traditional word alignment techniques.
Otherwise, the features remain the same,
including distance features that measure
abs ( E − k ); orthographic features; word
frequencies; common-word features; a bias term
set always to 1; and an HMM approximation
cor(Ej+1, Fk+1).
</bodyText>
<subsectionHeader confidence="0.566765">
4.2.2 Soft Dependency-augmented ITG
</subsectionHeader>
<bodyText confidence="0.9998298">
Because of the modularity of the structured out-
put SVM, our SVM ITG re-uses a large amount
infrastructure from the matching solution. We
essentially plug an ITG parser in the place of
the matching algorithm, and add features to take
advantage of information made available by the
parser. x remains a sentence pair, and y becomes
an ITG parse tree that decomposes x and speci-
fies an alignment. Our required components are as
follows:
</bodyText>
<listItem confidence="0.989530285714286">
1. We define a feature vector OT on instances
of production rules, r. IF is a function of
the decomposition specified by y: IF(x, y) =
ErEy OT (r).
2. The structure search is a weighted ITG parser
that maximizes summed production scores.
Each instance of a production rule r is as-
signed a score of (w, OT (r))
3. Loss is unchanged, defined in terms of the
alignment induced by y.
4. A loss-augmented ITG is used to find the max
cost. Productions of the form A —* e/f
that correspond to links have their scores aug-
mented as in the matching system.
</listItem>
<bodyText confidence="0.999646208333334">
The OT vector has two new features in addition to
those present in the matching system’s O. These
features can be active only for non-terminal pro-
ductions, which have the form A —* [AA]  |(AA).
One feature indicates an inverted production A —*
(AA), while the other indicates the use of an in-
valid span according to a provided English depen-
dency tree, as described in Section 3.2. These
are the only features that can be active for non-
terminal productions.
A terminal production rl that corresponds to a
link l is given that link’s features from the match-
ing system: OT (rl) = O(l). Terminal productions
ro corresponding to unaligned tokens are given
blank feature vectors: OT(ro) = 0.
The SVM requires complete IF vectors for the
correct training structures. Unfortunately, our
training set contains gold standard alignments, not
ITG parse trees. The gold standard is divided into
sure and possible link sets 5 and P (Och and Ney,
2003). Links in 5 must be included in a correct
alignment, while P links are optional. We create
ITG trees from the gold standard using the follow-
ing sorted priorities during tree construction:
</bodyText>
<listItem confidence="0.9995856">
• maximize the number of links from 5
• minimize the number of English dependency
span violations
• maximize the number of links from P
• minimize the number of inversions
</listItem>
<bodyText confidence="0.999411">
This creates trees that represent high scoring align-
ments, using a minimal number of invalid spans.
Only the span and inversion counts of these trees
will be used in training, so we need not achieve a
perfect tree structure. We still evaluate all methods
with the original alignment gold standard.
</bodyText>
<sectionHeader confidence="0.994861" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999978833333333">
We conduct two experiments. The first tests
the dependency-augmented ITG described in Sec-
tion 3.2 as an aligner with hard cohesion con-
straints. The second tests our discriminative ITG
with soft cohesion constraints against two strong
baselines.
</bodyText>
<subsectionHeader confidence="0.992847">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999898066666667">
We conduct our experiments using French-English
Hansard data. Our 02 scores, link probabilities
and word frequency counts are determined using a
sentence-aligned bitext consisting of 50K sentence
pairs. Our training set for the discriminative align-
ers is the first 100 sentence pairs from the French-
English gold standard provided for the 2003 WPT
workshop (Mihalcea and Pedersen, 2003). For
evaluation we compare to the remaining 347 gold
standard pairs using the alignment evaluation met-
rics: precision, recall and alignment error rate or
AER (Och and Ney, 2003). SVM learning param-
eters are tuned using the 37-pair development set
provided with this data. English dependency trees
are provided by Minipar (Lin, 1994).
</bodyText>
<page confidence="0.999248">
110
</page>
<tableCaption confidence="0.99569225">
Table 2: The performance of SVM-trained align-
ers with various degrees of cohesion constraint.
Table 1: The effect of hard cohesion constraints on
a simple unsupervised link score.
</tableCaption>
<table confidence="0.999665125">
Search Prec Rec AER
Matching 0.723 0.845 0.231
ITG 0.764 0.860 0.200
D-ITG 0.830 0.873 0.153
Method Prec Rec AER
Matching 0.916 0.860 0.110
D-ITG 0.940 0.854 0.100
SD-ITG 0.944 0.878 0.086
</table>
<subsectionHeader confidence="0.992145">
5.2 Hard Constraint Performance
</subsectionHeader>
<bodyText confidence="0.9967342">
The goal of this experiment is to empirically con-
firm that the English spans marked invalid by
Section 3.2’s dependency-augmented ITG provide
useful guidance to an aligner. To do so, we
compare an ITG with hard cohesion constraints,
an unconstrained ITG, and a weighted maximum
matching aligner. All aligners use the same sim-
ple objective function. They maximize summed
link values v(l), where v(l) is defined as follows
for an l = (Ej, Fk):
</bodyText>
<equation confidence="0.93314">
v(l) = 02(Ej, Fk) − 10−5 abs − k
(JEJ JFJ/
</equation>
<bodyText confidence="0.999504533333333">
All three aligners link based on 02 correlation
scores, breaking ties in favor of closer pairs. This
allows us to evaluate the hard constraints outside
the context of supervised learning.
Table 1 shows the results of this experiment.
We can see that switching the search method
from weighted maximum matching to a cohesion-
constrained ITG (D-ITG) has produced a 34% rel-
ative reduction in alignment error rate. The bulk
of this improvement results from a substantial in-
crease in precision, though recall has also gone up.
This indicates that these cohesion constraints are a
strong alignment feature. The ITG row shows that
the weaker ITG constraints are also valuable, but
the cohesion constraint still improves on them.
</bodyText>
<subsectionHeader confidence="0.996171">
5.3 Soft Constraint Performance
</subsectionHeader>
<bodyText confidence="0.999803111111111">
We now test the performance of our SVM ITG
with soft cohesion constraint, or SD-ITG, which
is described in Section 4.2.2. We will test against
two strong baselines. The first baseline, matching
is the matching SVM described in Section 4.2.1,
which is a re-implementation of the state-of-the-
art work in (Taskar et al., 2005)3. The second
baseline, D-ITG is an ITG aligner with hard co-
hesion constraints, but which uses the weights
</bodyText>
<footnote confidence="0.9105035">
3Though it is arguably lacking one of its strongest fea-
tures: the output of GIZA++ (Och and Ney, 2003)
</footnote>
<bodyText confidence="0.999261404761905">
trained by the matching SVM to assign link val-
ues. This is the most straight-forward way to com-
bine discriminative training with the hard syntactic
constraints.
The results are shown in Table 2. The first thing
to note is that our Matching baseline is achieving
scores in line with (Taskar et al., 2005), which re-
ports an AER of 0.107 using similar features and
the same training and test sets.
The effect of the hard cohesion constraint has
been greatly diminished after discriminative train-
ing. Matching and D-ITG correspond to the the
entries of the same name in Table 1, only with a
much stronger, learned value function v(l). How-
ever, in place of a 34% relative error reduction, the
hard constraints in the D-ITG produce only a 9%
reduction from 0.110 to 0.100. Also note that this
time the hard constraints result in a reduction in
recall. This indicates that the hard cohesion con-
straint is providing little guidance not provided by
other features, and that it is actually eliminating
more sure links than it is helping to find.
The soft-constrained SD-ITG, which has access
to the D-ITG’s invalid spans as a feature during
SVM training, is fairing substantially better. Its
AER of 0.086 represents a 22% relative error re-
duction compared to the matching system. The
improved error rate is caused by gains in both pre-
cision and recall. This indicates that the invalid
span feature is doing more than just ruling out
links; perhaps it is de-emphasizing another, less
accurate feature’s role. The SD-ITG overrides the
cohesion constraint in only 41 of the 347 test sen-
tences, so we can see that it is indeed a soft con-
straint: it is obeyed nearly all the time, but it can be
broken when necessary. The SD-ITG achieves by
far the strongest ITG alignment result reported on
this French-English set; surpassing the 0.16 AER
reported in (Zhang and Gildea, 2004).
Training times for this system are quite low; un-
supervised statistics can be collected quickly over
a large set, while only the 100-sentence training
</bodyText>
<page confidence="0.996204">
111
</page>
<bodyText confidence="0.9999076">
set needs to be iteratively aligned. Our match-
ing SVM trains in minutes on a single-processor
machine, while the SD-ITG trains in roughly one
hour. The ITG is the bottleneck, so training time
could be improved by optimizing the parser.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999891363636364">
Several other aligners have used discriminative
training. Our work borrows heavily from (Taskar
et al., 2005), which uses a max-margin approach
with a weighted maximum matching aligner.
(Moore, 2005) uses an averaged perceptron for
training with a customized beam search. (Liu et
al., 2005) uses a log-linear model with a greedy
search. To our knowledge, ours is the first align-
ment approach to use this highly modular struc-
tured SVM, and the first discriminative method to
use an ITG for the base aligner.
(Gildea, 2003) presents another aligner with a
soft syntactic constraint. This work adds a cloning
operation to the tree-to-string generative model in
(Yamada and Knight, 2001). This allows subtrees
to move during translation. As the model is gen-
erative, it is much more difficult to incorporate a
wide variety of features as we do here. In (Zhang
and Gildea, 2004), this model was tested on the
same annotated French-English sentence pairs that
we divided into training and test sets for our exper-
iments; it achieved an AER of 0.15.
</bodyText>
<sectionHeader confidence="0.9995" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999987125">
We have presented a discriminative, syntactic
word alignment method. Discriminative training
is conducted using a highly modular SVM for
structured output, which allows code reuse be-
tween the syntactic aligner and a maximum match-
ing baseline. An ITG parser is used for the align-
ment search, exposing two syntactic features: the
use of inverted productions, and the use of spans
that would not be available in a tree-to-string sys-
tem. This second feature creates a soft phrasal co-
hesion constraint. Discriminative training allows
us to maintain all of the features that are useful to
the maximum matching baseline in addition to the
new syntactic features. We have shown that these
features produce a 22% relative reduction in error
rate with respect to a strong flat-string model.
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999950603448276">
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263–312.
C. Cherry and D. Lin. 2003. A probability model to improve
word alignment. In Meeting of the Association for Com-
putational Linguistics, pages 88–95, Sapporo, Japan, July.
C. Cherry and D. Lin. 2006. A comparison of syntacti-
cally motivated word alignment spaces. In Proceedings
of EACL, pages 145–152, Trento, Italy, April.
H. J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP, pages 304–311.
W. A. Gale and K. W. Church. 1991. Identifying word cor-
respondences in parallel texts. In 4th Speech and Natural
Language Workshop, pages 152–157. DARPA.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In Meeting of the Association for Computa-
tional Linguistics, pages 80–87, Sapporo, Japan.
D. Lin. 1994. Principar - an efficient, broad-coverage,
principle-based parser. In Proceedings of COLING, pages
42–48, Kyoto, Japan.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word
alignment. In Meeting of the Association for Computa-
tional Linguistics, pages 459–466, Ann Arbor, USA.
I. D. Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221–
249.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In HLT-NAACL Workshop on
Building and Using Parallel Texts, pages 1–10, Edmon-
ton, Canada.
R. Moore. 2005. A discriminative framework for bilingual
word alignment. In Proceedings of HLT-EMNLP, pages
81–88, Vancouver, Canada, October.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational Lin-
guistics, 29(1):19–52, March.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discrimi-
native matching approach to word alignment. In Proceed-
ings of HLT-EMNLP, pages 73–80, Vancouver, Canada.
I. Tsochantaridis, T. Hofman, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdependent
and structured output spaces. In Proceedings of ICML,
pages 823–830.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceedings
of COLING, pages 836–841, Copenhagen, Denmark.
D. West. 2001. Introduction to Graph Theory. Prentice Hall,
2nd edition.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377–403.
K. Yamada and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Meeting of the Association for
Computational Linguistics, pages 523–530.
H. Zhang and D. Gildea. 2004. Syntax-based alignment:
Supervised or unsupervised? In Proceedings of COLING,
Geneva, Switzerland, August.
</reference>
<page confidence="0.998298">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.873373">
<title confidence="0.9989655">Soft Syntactic Constraints for Word Alignment through Discriminative Training</title>
<author confidence="0.999187">Colin Cherry</author>
<affiliation confidence="0.9999735">Department of Computing Science University of Alberta</affiliation>
<address confidence="0.997814">Edmonton, AB, Canada, T6G 2E8</address>
<email confidence="0.995012">colinc@cs.ualberta.ca</email>
<author confidence="0.914442">Dekang Lin</author>
<affiliation confidence="0.998074">Google Inc.</affiliation>
<address confidence="0.999006">1600 Amphitheatre Parkway Mountain View, CA, USA, 94043</address>
<email confidence="0.999775">lindek@google.com</email>
<abstract confidence="0.9978458">Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree. However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex. We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1096" citStr="Brown et al., 1993" startWordPosition="159" endWordPosition="162">However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex. We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser. 1 Introduction Given a parallel sentence pair, or bitext, bilingual word alignment finds word-to-word connections across languages. Originally introduced as a byproduct of training statistical translation models in (Brown et al., 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext gr</context>
<context position="3560" citStr="Brown et al., 1993" startWordPosition="552" endWordPosition="555">ignment method as a feature in our model, creating a powerful soft constraint. We add our syntactic features to an already strong flat-string discriminative solution, and we show that they provide new information resulting in improved alignments. 2 Constrained Alignment Let an alignment be the complete structure that connects two parallel sentences, and a link be one of the word-to-word connections that make up an alignment. All word alignment methods benefit from some set of constraints. These limit the alignment search space and encourage competition between potential links. The IBM models (Brown et al., 1993) benefit from a one-tomany constraint, where each target word has ex105 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 105–112, Sydney, July 2006. c�2006 Association for Computational Linguistics Figure 1: A cohesion constraint violation. actly one generator in the source. Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al., 2005) use a one-toone constraint, where words in either sentence can participate in at most one link. Throughout this paper we assume a one-to-one constraint in addition to any syntax constraints. 2.1 Cohesion Con</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>D Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="5102" citStr="Cherry and Lin (2003)" startWordPosition="795" endWordPosition="798">e alignment is said to maintain phrasal cohesion. Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings. Crossings occur when the projections of two disjoint phrases overlap. For example, Figure 1 shows a head-modifier crossing: the projection of the the tax subtree, impˆot ... le, is interrupted by the projection of its head, cause. Alignments with no crossings maintain phrasal cohesion. Fox’s experiments show that cohesion is generally maintained for French-English, and that dependency trees produce the highest degree of cohesion among the tested structures. Cherry and Lin (2003) use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner. This constraint produces a significant reduction in alignment error rate. However, as Fox (2002) showed, even in a language pair as close as French-English, there are situations where phrasal cohesion should not be maintained. These include incorrect parses, systematic violations such as not —* ne ... pas, paraphrases, and linguistic exceptions. We aim to create an alignment system that obeys cohesion constraints most of the time, but can violate them when necessary. Unfortunately, Cherry and Lin’s beam se</context>
<context position="19682" citStr="Cherry and Lin, 2003" startWordPosition="3328" endWordPosition="3331">ied to prefer costly links: ∀l ∈/ y : v(l) ← h~w, ψ(l)i + c, ∀l ∈ y : v(l) ← h~w, ψ(l)i − co Note that our max cost search could not have been implemented as loss-augmented matching had we selected one of the other loss objectives presented in (Tsochantaridis et al., 2004) in place of margin rescaling. We use the same feature representation ψ(l) as (Taskar et al., 2005), with some small exceptions. Let l = (Ej, Fk) be a potential link between the jth word of English sentence E and the kth word of Foreign sentence F. To measure correlation between Ej and Fk we use conditional link probability (Cherry and Lin, 2003) in place of the Dice coefficient: #links(Ej, Fk) − d cor(Ej, Fk) = #cooccurrences(Ej, Fk) where the link counts are determined by wordaligning 50K sentence pairs with another matching SVM that uses the φ2 measure (Gale and 109 Church, 1991) in place of Dice. The 02 measure requires only co-occurrence counts. d is an absolute discount parameter as in (Moore, 2005). Also, we omit the IBM Model 4 Prediction features, as we wish to know how well we can do without resorting to traditional word alignment techniques. Otherwise, the features remain the same, including distance features that measure a</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>C. Cherry and D. Lin. 2003. A probability model to improve word alignment. In Meeting of the Association for Computational Linguistics, pages 88–95, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>D Lin</author>
</authors>
<title>A comparison of syntactically motivated word alignment spaces.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>145--152</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="9040" citStr="Cherry and Lin, 2006" startWordPosition="1434" endWordPosition="1437">eting ITG, the simplest interesting grammar in this formalism: A — [AA] |(AA) |e/f This grammar enforces its own weak cohesion constraint: for every possible alignment, a corresponding binary constituency tree must exist for which the alignment maintains phrasal cohesion. Figure 2 shows a word alignment and the corresponding tree found by an ITG parser. Wu (1997) provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints. In our French-English data set, an ITG rules out only 0.3% of necessary links beyond those already eliminated by the one-to-one constraint (Cherry and Lin, 2006). 3.2 Dependency-augmented ITG An ITG will search all alignments that conform to a possible binary constituency tree. We wish to confine that search to a specific n-array dependency tree. Fortunately, Wu (1997) provides a method to have an ITG respect a known partial structure. One can seed the ITG parse chart so that spans that do not agree with the provided structure are assigned a value of —oc before parsing begins. The result is that no constituent is ever constructed with any of these invalid spans. In the case of phrasal cohesion, the invalid spans correspond to spans of the English sent</context>
<context position="11050" citStr="Cherry and Lin, 2006" startWordPosition="1793" endWordPosition="1796">nvalid spans induced by a simple dependency tree. With these invalid spans in place, the ITG can no longer merge part of a dependency subtree with anything other than another part of the same subtree. Since all ITG movement can be explained by inversions, this constrained ITG cannot interrupt one dependency phrase with part of another. Therefore, the phrasal cohesion of the input dependency tree is maintained. Note that this will not search the exact same alignment space as a cohesion-constrained beam search; instead it uses the union of the cohesion constraint and the weaker ITG constraints (Cherry and Lin, 2006). Transforming this form of the cohesion constraint into a soft constraint is straight-forward. Instead of overriding the parser so it cannot use invalid English spans, we will note the invalid spans and assign the parser a penalty should it use them. The value of this penalty will be determined through discriminative training, as described in Section 4. Since the penalty is available within the dynamic programming algorithm, the parser will be able to incorporate it to find a globally optimal alignment. 4 Discriminative Training To discriminatively train our alignment systems, we adopt the Su</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>C. Cherry and D. Lin. 2006. A comparison of syntactically motivated word alignment spaces. In Proceedings of EACL, pages 145–152, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="4541" citStr="Fox (2002)" startWordPosition="711" endWordPosition="712">(Taskar et al., 2005) use a one-toone constraint, where words in either sentence can participate in at most one link. Throughout this paper we assume a one-to-one constraint in addition to any syntax constraints. 2.1 Cohesion Constraint Suppose we are given a parse tree for one of the two sentences in our sentence pair. We will refer to the parsed language as English, and the unparsed language as Foreign. Given this information, a reasonable expectation is that English phrases will move together when projected onto Foreign. When this occurs, the alignment is said to maintain phrasal cohesion. Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings. Crossings occur when the projections of two disjoint phrases overlap. For example, Figure 1 shows a head-modifier crossing: the projection of the the tax subtree, impˆot ... le, is interrupted by the projection of its head, cause. Alignments with no crossings maintain phrasal cohesion. Fox’s experiments show that cohesion is generally maintained for French-English, and that dependency trees produce the highest degree of cohesion among the tested structures. Cherry and Lin (2003) use the phrasal cohesion of a dependen</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>H. J. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of EMNLP, pages 304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>Identifying word correspondences in parallel texts.</title>
<date>1991</date>
<booktitle>In 4th Speech and Natural Language Workshop,</booktitle>
<pages>152--157</pages>
<publisher>DARPA.</publisher>
<marker>Gale, Church, 1991</marker>
<rawString>W. A. Gale and K. W. Church. 1991. Identifying word correspondences in parallel texts. In 4th Speech and Natural Language Workshop, pages 152–157. DARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation.</title>
<date>2003</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="6788" citStr="Gildea, 2003" startWordPosition="1072" endWordPosition="1073">lty to those violations. 3 Syntax-aware Alignment Search We require an alignment search that can find the globally best alignment under its current objective function, and can account for phrasal cohesion in this objective. IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring the syntactic interactions of links. The tree-to-string models of (Yamada and Knight, 2001) naturally consider syntax, but special modeling considerations are needed to allow any deviations from the provided tree (Gildea, 2003). The Inversion Transduction Grammar or ITG formalism, described in (Wu, 1997), is well suited for our purposes. ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree. 3.1 Inversion Transduction Grammar An ITG aligns bitext through synchronous parsing. Both sentences are decomposed into constituent phrases simultaneously, producing a word alignment as a byproduct. Viewed generatively, an ITG writes to two streams at once. Terminal productions produce a token in each stream, or a token in one str</context>
<context position="29020" citStr="Gildea, 2003" startWordPosition="4904" endWordPosition="4905">the bottleneck, so training time could be improved by optimizing the parser. 6 Related Work Several other aligners have used discriminative training. Our work borrows heavily from (Taskar et al., 2005), which uses a max-margin approach with a weighted maximum matching aligner. (Moore, 2005) uses an averaged perceptron for training with a customized beam search. (Liu et al., 2005) uses a log-linear model with a greedy search. To our knowledge, ours is the first alignment approach to use this highly modular structured SVM, and the first discriminative method to use an ITG for the base aligner. (Gildea, 2003) presents another aligner with a soft syntactic constraint. This work adds a cloning operation to the tree-to-string generative model in (Yamada and Knight, 2001). This allows subtrees to move during translation. As the model is generative, it is much more difficult to incorporate a wide variety of features as we do here. In (Zhang and Gildea, 2004), this model was tested on the same annotated French-English sentence pairs that we divided into training and test sets for our experiments; it achieved an AER of 0.15. 7 Conclusion We have presented a discriminative, syntactic word alignment method</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>D. Gildea. 2003. Loosely tree-based alignment for machine translation. In Meeting of the Association for Computational Linguistics, pages 80–87, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principar - an efficient, broad-coverage, principle-based parser.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>42--48</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="24053" citStr="Lin, 1994" startWordPosition="4068" endWordPosition="4069">quency counts are determined using a sentence-aligned bitext consisting of 50K sentence pairs. Our training set for the discriminative aligners is the first 100 sentence pairs from the FrenchEnglish gold standard provided for the 2003 WPT workshop (Mihalcea and Pedersen, 2003). For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER (Och and Ney, 2003). SVM learning parameters are tuned using the 37-pair development set provided with this data. English dependency trees are provided by Minipar (Lin, 1994). 110 Table 2: The performance of SVM-trained aligners with various degrees of cohesion constraint. Table 1: The effect of hard cohesion constraints on a simple unsupervised link score. Search Prec Rec AER Matching 0.723 0.845 0.231 ITG 0.764 0.860 0.200 D-ITG 0.830 0.873 0.153 Method Prec Rec AER Matching 0.916 0.860 0.110 D-ITG 0.940 0.854 0.100 SD-ITG 0.944 0.878 0.086 5.2 Hard Constraint Performance The goal of this experiment is to empirically confirm that the English spans marked invalid by Section 3.2’s dependency-augmented ITG provide useful guidance to an aligner. To do so, we compare</context>
</contexts>
<marker>Lin, 1994</marker>
<rawString>D. Lin. 1994. Principar - an efficient, broad-coverage, principle-based parser. In Proceedings of COLING, pages 42–48, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Log-linear models for word alignment.</title>
<date>2005</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>459--466</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="2410" citStr="Liu et al., 2005" startWordPosition="368" endWordPosition="371">ting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft co</context>
<context position="28789" citStr="Liu et al., 2005" startWordPosition="4861" endWordPosition="4864">ollected quickly over a large set, while only the 100-sentence training 111 set needs to be iteratively aligned. Our matching SVM trains in minutes on a single-processor machine, while the SD-ITG trains in roughly one hour. The ITG is the bottleneck, so training time could be improved by optimizing the parser. 6 Related Work Several other aligners have used discriminative training. Our work borrows heavily from (Taskar et al., 2005), which uses a max-margin approach with a weighted maximum matching aligner. (Moore, 2005) uses an averaged perceptron for training with a customized beam search. (Liu et al., 2005) uses a log-linear model with a greedy search. To our knowledge, ours is the first alignment approach to use this highly modular structured SVM, and the first discriminative method to use an ITG for the base aligner. (Gildea, 2003) presents another aligner with a soft syntactic constraint. This work adds a cloning operation to the tree-to-string generative model in (Yamada and Knight, 2001). This allows subtrees to move during translation. As the model is generative, it is much more difficult to incorporate a wide variety of features as we do here. In (Zhang and Gildea, 2004), this model was t</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word alignment. In Meeting of the Association for Computational Linguistics, pages 459–466, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<pages>249</pages>
<contexts>
<context position="3909" citStr="Melamed, 2000" startWordPosition="604" endWordPosition="605">d a link be one of the word-to-word connections that make up an alignment. All word alignment methods benefit from some set of constraints. These limit the alignment search space and encourage competition between potential links. The IBM models (Brown et al., 1993) benefit from a one-tomany constraint, where each target word has ex105 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 105–112, Sydney, July 2006. c�2006 Association for Computational Linguistics Figure 1: A cohesion constraint violation. actly one generator in the source. Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al., 2005) use a one-toone constraint, where words in either sentence can participate in at most one link. Throughout this paper we assume a one-to-one constraint in addition to any syntax constraints. 2.1 Cohesion Constraint Suppose we are given a parse tree for one of the two sentences in our sentence pair. We will refer to the parsed language as English, and the unparsed language as Foreign. Given this information, a reasonable expectation is that English phrases will move together when projected onto Foreign. When this occurs, the alignment is said to maint</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. D. Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221– 249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In HLT-NAACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>1--10</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="23720" citStr="Mihalcea and Pedersen, 2003" startWordPosition="4013" endWordPosition="4016">s. The first tests the dependency-augmented ITG described in Section 3.2 as an aligner with hard cohesion constraints. The second tests our discriminative ITG with soft cohesion constraints against two strong baselines. 5.1 Experimental setup We conduct our experiments using French-English Hansard data. Our 02 scores, link probabilities and word frequency counts are determined using a sentence-aligned bitext consisting of 50K sentence pairs. Our training set for the discriminative aligners is the first 100 sentence pairs from the FrenchEnglish gold standard provided for the 2003 WPT workshop (Mihalcea and Pedersen, 2003). For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER (Och and Ney, 2003). SVM learning parameters are tuned using the 37-pair development set provided with this data. English dependency trees are provided by Minipar (Lin, 1994). 110 Table 2: The performance of SVM-trained aligners with various degrees of cohesion constraint. Table 1: The effect of hard cohesion constraints on a simple unsupervised link score. Search Prec Rec AER Matching 0.723 0.845 0.231 ITG 0.764 0.860 0.200 D-ITG 0.830 </context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>R. Mihalcea and T. Pedersen. 2003. An evaluation exercise for word alignment. In HLT-NAACL Workshop on Building and Using Parallel Texts, pages 1–10, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>81--88</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="2423" citStr="Moore, 2005" startWordPosition="372" endWordPosition="373">emaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft constraint. We </context>
<context position="20048" citStr="Moore, 2005" startWordPosition="3394" endWordPosition="3395">ith some small exceptions. Let l = (Ej, Fk) be a potential link between the jth word of English sentence E and the kth word of Foreign sentence F. To measure correlation between Ej and Fk we use conditional link probability (Cherry and Lin, 2003) in place of the Dice coefficient: #links(Ej, Fk) − d cor(Ej, Fk) = #cooccurrences(Ej, Fk) where the link counts are determined by wordaligning 50K sentence pairs with another matching SVM that uses the φ2 measure (Gale and 109 Church, 1991) in place of Dice. The 02 measure requires only co-occurrence counts. d is an absolute discount parameter as in (Moore, 2005). Also, we omit the IBM Model 4 Prediction features, as we wish to know how well we can do without resorting to traditional word alignment techniques. Otherwise, the features remain the same, including distance features that measure abs ( E − k ); orthographic features; word frequencies; common-word features; a bias term set always to 1; and an HMM approximation cor(Ej+1, Fk+1). 4.2.2 Soft Dependency-augmented ITG Because of the modularity of the structured output SVM, our SVM ITG re-uses a large amount infrastructure from the matching solution. We essentially plug an ITG parser in the place o</context>
<context position="28698" citStr="Moore, 2005" startWordPosition="4848" endWordPosition="4849"> 2004). Training times for this system are quite low; unsupervised statistics can be collected quickly over a large set, while only the 100-sentence training 111 set needs to be iteratively aligned. Our matching SVM trains in minutes on a single-processor machine, while the SD-ITG trains in roughly one hour. The ITG is the bottleneck, so training time could be improved by optimizing the parser. 6 Related Work Several other aligners have used discriminative training. Our work borrows heavily from (Taskar et al., 2005), which uses a max-margin approach with a weighted maximum matching aligner. (Moore, 2005) uses an averaged perceptron for training with a customized beam search. (Liu et al., 2005) uses a log-linear model with a greedy search. To our knowledge, ours is the first alignment approach to use this highly modular structured SVM, and the first discriminative method to use an ITG for the base aligner. (Gildea, 2003) presents another aligner with a soft syntactic constraint. This work adds a cloning operation to the tree-to-string generative model in (Yamada and Knight, 2001). This allows subtrees to move during translation. As the model is generative, it is much more difficult to incorpor</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>R. Moore. 2005. A discriminative framework for bilingual word alignment. In Proceedings of HLT-EMNLP, pages 81–88, Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1294" citStr="Och and Ney, 2003" startWordPosition="194" endWordPosition="197">x-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser. 1 Introduction Given a parallel sentence pair, or bitext, bilingual word alignment finds word-to-word connections across languages. Originally introduced as a byproduct of training statistical translation models in (Brown et al., 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with </context>
<context position="22382" citStr="Och and Ney, 2003" startWordPosition="3798" endWordPosition="3801">rovided English dependency tree, as described in Section 3.2. These are the only features that can be active for nonterminal productions. A terminal production rl that corresponds to a link l is given that link’s features from the matching system: OT (rl) = O(l). Terminal productions ro corresponding to unaligned tokens are given blank feature vectors: OT(ro) = 0. The SVM requires complete IF vectors for the correct training structures. Unfortunately, our training set contains gold standard alignments, not ITG parse trees. The gold standard is divided into sure and possible link sets 5 and P (Och and Ney, 2003). Links in 5 must be included in a correct alignment, while P links are optional. We create ITG trees from the gold standard using the following sorted priorities during tree construction: • maximize the number of links from 5 • minimize the number of English dependency span violations • maximize the number of links from P • minimize the number of inversions This creates trees that represent high scoring alignments, using a minimal number of invalid spans. Only the span and inversion counts of these trees will be used in training, so we need not achieve a perfect tree structure. We still evalu</context>
<context position="23898" citStr="Och and Ney, 2003" startWordPosition="4042" endWordPosition="4045">s against two strong baselines. 5.1 Experimental setup We conduct our experiments using French-English Hansard data. Our 02 scores, link probabilities and word frequency counts are determined using a sentence-aligned bitext consisting of 50K sentence pairs. Our training set for the discriminative aligners is the first 100 sentence pairs from the FrenchEnglish gold standard provided for the 2003 WPT workshop (Mihalcea and Pedersen, 2003). For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER (Och and Ney, 2003). SVM learning parameters are tuned using the 37-pair development set provided with this data. English dependency trees are provided by Minipar (Lin, 1994). 110 Table 2: The performance of SVM-trained aligners with various degrees of cohesion constraint. Table 1: The effect of hard cohesion constraints on a simple unsupervised link score. Search Prec Rec AER Matching 0.723 0.845 0.231 ITG 0.764 0.860 0.200 D-ITG 0.830 0.873 0.153 Method Prec Rec AER Matching 0.916 0.860 0.110 D-ITG 0.940 0.854 0.100 SD-ITG 0.944 0.878 0.086 5.2 Hard Constraint Performance The goal of this experiment is to empi</context>
<context position="26229" citStr="Och and Ney, 2003" startWordPosition="4428" endWordPosition="4431">, but the cohesion constraint still improves on them. 5.3 Soft Constraint Performance We now test the performance of our SVM ITG with soft cohesion constraint, or SD-ITG, which is described in Section 4.2.2. We will test against two strong baselines. The first baseline, matching is the matching SVM described in Section 4.2.1, which is a re-implementation of the state-of-theart work in (Taskar et al., 2005)3. The second baseline, D-ITG is an ITG aligner with hard cohesion constraints, but which uses the weights 3Though it is arguably lacking one of its strongest features: the output of GIZA++ (Och and Ney, 2003) trained by the matching SVM to assign link values. This is the most straight-forward way to combine discriminative training with the hard syntactic constraints. The results are shown in Table 2. The first thing to note is that our Matching baseline is achieving scores in line with (Taskar et al., 2005), which reports an AER of 0.107 using similar features and the same training and test sets. The effect of the hard cohesion constraint has been greatly diminished after discriminative training. Matching and D-ITG correspond to the the entries of the same name in Table 1, only with a much stronge</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–52, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-Julien</author>
<author>D Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>73--80</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2445" citStr="Taskar et al., 2005" startWordPosition="374" endWordPosition="377">ng (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft constraint. We add our syntactic feat</context>
<context position="3952" citStr="Taskar et al., 2005" startWordPosition="610" endWordPosition="613">onnections that make up an alignment. All word alignment methods benefit from some set of constraints. These limit the alignment search space and encourage competition between potential links. The IBM models (Brown et al., 1993) benefit from a one-tomany constraint, where each target word has ex105 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 105–112, Sydney, July 2006. c�2006 Association for Computational Linguistics Figure 1: A cohesion constraint violation. actly one generator in the source. Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al., 2005) use a one-toone constraint, where words in either sentence can participate in at most one link. Throughout this paper we assume a one-to-one constraint in addition to any syntax constraints. 2.1 Cohesion Constraint Suppose we are given a parse tree for one of the two sentences in our sentence pair. We will refer to the parsed language as English, and the unparsed language as Foreign. Given this information, a reasonable expectation is that English phrases will move together when projected onto Foreign. When this occurs, the alignment is said to maintain phrasal cohesion. Fox (2002) measured p</context>
<context position="14620" citStr="Taskar et al., 2005" startWordPosition="2409" endWordPosition="2412">on problem, there is 0-1 loss: a vector is either classified correctly or it is not. In the structured case, some incorrect structures can be better than others. For example, having the argmax select an alignment missing only one link is better than selecting one with no correct links and a dozen wrong ones. A loss function A(yi, y) quantifies just how incorrect a particular structure y is. Though Tsochantaridis et al. (2004) provide several ways to incorporate loss into the SVM objective, we will use margin re-scaling, as it corresponds to loss usage in another max-margin alignment approach (Taskar et al., 2005). In margin re-scaling, high loss structures must be separated from the correct structure by a larger margin than low loss structures. To allow some misclassifications during training, a soft-margin requirement replaces our maxmargin objective. A slack variable �i is introduced for each training example xi, to allow the learner to violate the margin at a penalty. The magnitude of this penalty to determined by a hand-tuned parameter C. After a few transformations (Tsochantaridis et al., 2004), the soft-margin learning objective can be formulated as a quadratic program: �������� � Cn 1 �i=1 Vi, </context>
<context position="18467" citStr="Taskar et al., 2005" startWordPosition="3092" endWordPosition="3095">rgmax operator, and a dependency-augmented ITG that will 2Generally the test to see if �i &gt; costi(y; w) is approximated as �i &gt; costi(y; w) + E for a small constant E. satisfy our requirements for an aligner with a soft cohesion constraint. Our x becomes a bilingual sentence-pair, while our y becomes an alignment, represented by a set of links. 4.2.1 Weighed Maximum Matching Given a bipartite graph with edge values, the weighted maximum matching algorithm (West, 2001) will find the matching with maximum summed edge values. To create a matching alignment solution, we reproduce the approach of (Taskar et al., 2005) within the framework described in Section 4.1: 1. We define a feature vector ψ for each potential link l in x, and IF in terms of y’s component links: &apos;F(x, y) = ElEy ψ(l). 2. Our structure search is the matching algorithm. The input bipartite graph has an edge for each l. Each edge is given the value v(l) ← h~w, ψ(l)i. 3. We adopt the weighted Hamming loss in described (Taskar et al., 2005): 0(y, y) = co|y − y |+ c,|y − y| where co is an omission penalty and c, is a commission penalty. 4. Our max cost search corresponds to their loss-augmented matching problem. The input graph is modified to</context>
<context position="26020" citStr="Taskar et al., 2005" startWordPosition="4391" endWordPosition="4394">bstantial increase in precision, though recall has also gone up. This indicates that these cohesion constraints are a strong alignment feature. The ITG row shows that the weaker ITG constraints are also valuable, but the cohesion constraint still improves on them. 5.3 Soft Constraint Performance We now test the performance of our SVM ITG with soft cohesion constraint, or SD-ITG, which is described in Section 4.2.2. We will test against two strong baselines. The first baseline, matching is the matching SVM described in Section 4.2.1, which is a re-implementation of the state-of-theart work in (Taskar et al., 2005)3. The second baseline, D-ITG is an ITG aligner with hard cohesion constraints, but which uses the weights 3Though it is arguably lacking one of its strongest features: the output of GIZA++ (Och and Ney, 2003) trained by the matching SVM to assign link values. This is the most straight-forward way to combine discriminative training with the hard syntactic constraints. The results are shown in Table 2. The first thing to note is that our Matching baseline is achieving scores in line with (Taskar et al., 2005), which reports an AER of 0.107 using similar features and the same training and test s</context>
<context position="28608" citStr="Taskar et al., 2005" startWordPosition="4833" endWordPosition="4836">result reported on this French-English set; surpassing the 0.16 AER reported in (Zhang and Gildea, 2004). Training times for this system are quite low; unsupervised statistics can be collected quickly over a large set, while only the 100-sentence training 111 set needs to be iteratively aligned. Our matching SVM trains in minutes on a single-processor machine, while the SD-ITG trains in roughly one hour. The ITG is the bottleneck, so training time could be improved by optimizing the parser. 6 Related Work Several other aligners have used discriminative training. Our work borrows heavily from (Taskar et al., 2005), which uses a max-margin approach with a weighted maximum matching aligner. (Moore, 2005) uses an averaged perceptron for training with a customized beam search. (Liu et al., 2005) uses a log-linear model with a greedy search. To our knowledge, ours is the first alignment approach to use this highly modular structured SVM, and the first discriminative method to use an ITG for the base aligner. (Gildea, 2003) presents another aligner with a soft syntactic constraint. This work adds a cloning operation to the tree-to-string generative model in (Yamada and Knight, 2001). This allows subtrees to </context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discriminative matching approach to word alignment. In Proceedings of HLT-EMNLP, pages 73–80, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofman</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>823--830</pages>
<contexts>
<context position="11756" citStr="Tsochantaridis et al., 2004" startWordPosition="1910" endWordPosition="1913">straight-forward. Instead of overriding the parser so it cannot use invalid English spans, we will note the invalid spans and assign the parser a penalty should it use them. The value of this penalty will be determined through discriminative training, as described in Section 4. Since the penalty is available within the dynamic programming algorithm, the parser will be able to incorporate it to find a globally optimal alignment. 4 Discriminative Training To discriminatively train our alignment systems, we adopt the Support Vector Machine (SVM) for T T[i,k] x1 i j&apos; j k x2 107 Structured Output (Tsochantaridis et al., 2004). We have selected this system for its high degree of modularity, and because it has an API freely available1. We will summarize the learning mechanism briefly in this section, but readers should refer to (Tsochantaridis et al., 2004) for more details. SVM learning is most easily expressed as a constrained numerical optimization problem. All constraints mentioned in this section are constraints on this optimizer, and have nothing to do with the cohesion constraint from Section 2. 4.1 SVM for Structured Output Traditional SVMs attempt to find a linear separator that creates the largest possible</context>
<context position="14429" citStr="Tsochantaridis et al. (2004)" startWordPosition="2375" endWordPosition="2378">between yi and the incorrect structures in Y. 1At http://svmlight.joachims.org/svm struct.html This learning framework also incorporates a notion of structured loss. In a standard vector classification problem, there is 0-1 loss: a vector is either classified correctly or it is not. In the structured case, some incorrect structures can be better than others. For example, having the argmax select an alignment missing only one link is better than selecting one with no correct links and a dozen wrong ones. A loss function A(yi, y) quantifies just how incorrect a particular structure y is. Though Tsochantaridis et al. (2004) provide several ways to incorporate loss into the SVM objective, we will use margin re-scaling, as it corresponds to loss usage in another max-margin alignment approach (Taskar et al., 2005). In margin re-scaling, high loss structures must be separated from the correct structure by a larger margin than low loss structures. To allow some misclassifications during training, a soft-margin requirement replaces our maxmargin objective. A slack variable �i is introduced for each training example xi, to allow the learner to violate the margin at a penalty. The magnitude of this penalty to determined</context>
<context position="17235" citStr="Tsochantaridis et al., 2004" startWordPosition="2876" endWordPosition="2879"> of the form ξi ≥ costi(y; ~w) is added to the constraint set. The algorithm then iterates: the optimizer minimizes (3) again with the new constraint set, and solves the max cost problem for i = i + 1 with the new ~w, growing the constraint set if necessary. Note that the constraints on ξ change with ~w, as cost is a function of ~w. Once the end of the training set is reached, the learner loops back to the beginning. Learning ends when the entire training set can be processed without needing to add any constraints. It can be shown that this will occur within a polynomial number of iterations (Tsochantaridis et al., 2004). With this framework in place, one need only fill in the details to create an SVM for a new structured output space: 1. A IF(x, y) function to transform instancestructure pairs into feature vectors 2. A search to find the best structure given a weight vector: argmaxy h~w, IF(x, y)i. This has no role in training, but it is necessary to use the learned weights. 3. A structured loss function A(y, y) 4. A search to find the max cost structure: argmaxycosti(y; w) 4.2 SVMs for Alignment Using the Structured SVM API, we have created two SVM word aligners: a baseline that uses weighted maximum matchi</context>
<context position="19334" citStr="Tsochantaridis et al., 2004" startWordPosition="3264" endWordPosition="3267">rtite graph has an edge for each l. Each edge is given the value v(l) ← h~w, ψ(l)i. 3. We adopt the weighted Hamming loss in described (Taskar et al., 2005): 0(y, y) = co|y − y |+ c,|y − y| where co is an omission penalty and c, is a commission penalty. 4. Our max cost search corresponds to their loss-augmented matching problem. The input graph is modified to prefer costly links: ∀l ∈/ y : v(l) ← h~w, ψ(l)i + c, ∀l ∈ y : v(l) ← h~w, ψ(l)i − co Note that our max cost search could not have been implemented as loss-augmented matching had we selected one of the other loss objectives presented in (Tsochantaridis et al., 2004) in place of margin rescaling. We use the same feature representation ψ(l) as (Taskar et al., 2005), with some small exceptions. Let l = (Ej, Fk) be a potential link between the jth word of English sentence E and the kth word of Foreign sentence F. To measure correlation between Ej and Fk we use conditional link probability (Cherry and Lin, 2003) in place of the Dice coefficient: #links(Ej, Fk) − d cor(Ej, Fk) = #cooccurrences(Ej, Fk) where the link counts are determined by wordaligning 50K sentence pairs with another matching SVM that uses the φ2 measure (Gale and 109 Church, 1991) in place o</context>
</contexts>
<marker>Tsochantaridis, Hofman, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofman, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of ICML, pages 823–830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2257" citStr="Vogel et al., 1996" startWordPosition="349" endWordPosition="352">e methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allow</context>
<context position="6443" citStr="Vogel et al., 1996" startWordPosition="1019" endWordPosition="1023">timal alignment under a soft constraint. Furthermore, it is not clear what penalty to assign to crossings, or how to learn such a penalty from an iterative training process. The remainder of this paper will develop a complete alignment search that is aware of cohesion violations, and use discriminative learning technology to assign a meaningful penalty to those violations. 3 Syntax-aware Alignment Search We require an alignment search that can find the globally best alignment under its current objective function, and can account for phrasal cohesion in this objective. IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring the syntactic interactions of links. The tree-to-string models of (Yamada and Knight, 2001) naturally consider syntax, but special modeling considerations are needed to allow any deviations from the provided tree (Gildea, 2003). The Inversion Transduction Grammar or ITG formalism, described in (Wu, 1997), is well suited for our purposes. ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our depe</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of COLING, pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D West</author>
</authors>
<title>Introduction to Graph Theory.</title>
<date>2001</date>
<publisher>Prentice Hall,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="18319" citStr="West, 2001" startWordPosition="3069" endWordPosition="3070">for Alignment Using the Structured SVM API, we have created two SVM word aligners: a baseline that uses weighted maximum matching for its argmax operator, and a dependency-augmented ITG that will 2Generally the test to see if �i &gt; costi(y; w) is approximated as �i &gt; costi(y; w) + E for a small constant E. satisfy our requirements for an aligner with a soft cohesion constraint. Our x becomes a bilingual sentence-pair, while our y becomes an alignment, represented by a set of links. 4.2.1 Weighed Maximum Matching Given a bipartite graph with edge values, the weighted maximum matching algorithm (West, 2001) will find the matching with maximum summed edge values. To create a matching alignment solution, we reproduce the approach of (Taskar et al., 2005) within the framework described in Section 4.1: 1. We define a feature vector ψ for each potential link l in x, and IF in terms of y’s component links: &apos;F(x, y) = ElEy ψ(l). 2. Our structure search is the matching algorithm. The input bipartite graph has an edge for each l. Each edge is given the value v(l) ← h~w, ψ(l)i. 3. We adopt the weighted Hamming loss in described (Taskar et al., 2005): 0(y, y) = co|y − y |+ c,|y − y| where co is an omission</context>
</contexts>
<marker>West, 2001</marker>
<rawString>D. West. 2001. Introduction to Graph Theory. Prentice Hall, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1712" citStr="Wu, 1997" startWordPosition="261" endWordPosition="262">alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for stru</context>
<context position="6866" citStr="Wu, 1997" startWordPosition="1083" endWordPosition="1084">earch that can find the globally best alignment under its current objective function, and can account for phrasal cohesion in this objective. IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring the syntactic interactions of links. The tree-to-string models of (Yamada and Knight, 2001) naturally consider syntax, but special modeling considerations are needed to allow any deviations from the provided tree (Gildea, 2003). The Inversion Transduction Grammar or ITG formalism, described in (Wu, 1997), is well suited for our purposes. ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree. 3.1 Inversion Transduction Grammar An ITG aligns bitext through synchronous parsing. Both sentences are decomposed into constituent phrases simultaneously, producing a word alignment as a byproduct. Viewed generatively, an ITG writes to two streams at once. Terminal productions produce a token in each stream, or a token in one stream with the null symbol 0 in the other. We will use standard ITG notation: A </context>
<context position="8784" citStr="Wu (1997)" startWordPosition="1398" endWordPosition="1399">alignment. A horizontal bar across an arc indicates an inversion. An ITG chart parser provides a polynomialtime algorithm to conduct a complete enumeration of all alignments that are possible according to its grammar. We will use a binary bracketing ITG, the simplest interesting grammar in this formalism: A — [AA] |(AA) |e/f This grammar enforces its own weak cohesion constraint: for every possible alignment, a corresponding binary constituency tree must exist for which the alignment maintains phrasal cohesion. Figure 2 shows a word alignment and the corresponding tree found by an ITG parser. Wu (1997) provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints. In our French-English data set, an ITG rules out only 0.3% of necessary links beyond those already eliminated by the one-to-one constraint (Cherry and Lin, 2006). 3.2 Dependency-augmented ITG An ITG will search all alignments that conform to a possible binary constituency tree. We wish to confine that search to a specific n-array dependency tree. Fortunately, Wu (1997) provides a method to have an ITG respect a known partial structure. One can seed the ITG parse chart so that spans that do not agree </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="1853" citStr="Yamada and Knight, 2001" startWordPosition="283" endWordPosition="286">ther tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discri</context>
<context position="6652" citStr="Yamada and Knight, 2001" startWordPosition="1051" endWordPosition="1054">ll develop a complete alignment search that is aware of cohesion violations, and use discriminative learning technology to assign a meaningful penalty to those violations. 3 Syntax-aware Alignment Search We require an alignment search that can find the globally best alignment under its current objective function, and can account for phrasal cohesion in this objective. IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring the syntactic interactions of links. The tree-to-string models of (Yamada and Knight, 2001) naturally consider syntax, but special modeling considerations are needed to allow any deviations from the provided tree (Gildea, 2003). The Inversion Transduction Grammar or ITG formalism, described in (Wu, 1997), is well suited for our purposes. ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree. 3.1 Inversion Transduction Grammar An ITG aligns bitext through synchronous parsing. Both sentences are decomposed into constituent phrases simultaneously, producing a word alignment as a byproduc</context>
<context position="29182" citStr="Yamada and Knight, 2001" startWordPosition="4926" endWordPosition="4929">ur work borrows heavily from (Taskar et al., 2005), which uses a max-margin approach with a weighted maximum matching aligner. (Moore, 2005) uses an averaged perceptron for training with a customized beam search. (Liu et al., 2005) uses a log-linear model with a greedy search. To our knowledge, ours is the first alignment approach to use this highly modular structured SVM, and the first discriminative method to use an ITG for the base aligner. (Gildea, 2003) presents another aligner with a soft syntactic constraint. This work adds a cloning operation to the tree-to-string generative model in (Yamada and Knight, 2001). This allows subtrees to move during translation. As the model is generative, it is much more difficult to incorporate a wide variety of features as we do here. In (Zhang and Gildea, 2004), this model was tested on the same annotated French-English sentence pairs that we divided into training and test sets for our experiments; it achieved an AER of 0.15. 7 Conclusion We have presented a discriminative, syntactic word alignment method. Discriminative training is conducted using a highly modular SVM for structured output, which allows code reuse between the syntactic aligner and a maximum match</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Meeting of the Association for Computational Linguistics, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Syntax-based alignment: Supervised or unsupervised?</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="28092" citStr="Zhang and Gildea, 2004" startWordPosition="4749" endWordPosition="4752">mpared to the matching system. The improved error rate is caused by gains in both precision and recall. This indicates that the invalid span feature is doing more than just ruling out links; perhaps it is de-emphasizing another, less accurate feature’s role. The SD-ITG overrides the cohesion constraint in only 41 of the 347 test sentences, so we can see that it is indeed a soft constraint: it is obeyed nearly all the time, but it can be broken when necessary. The SD-ITG achieves by far the strongest ITG alignment result reported on this French-English set; surpassing the 0.16 AER reported in (Zhang and Gildea, 2004). Training times for this system are quite low; unsupervised statistics can be collected quickly over a large set, while only the 100-sentence training 111 set needs to be iteratively aligned. Our matching SVM trains in minutes on a single-processor machine, while the SD-ITG trains in roughly one hour. The ITG is the bottleneck, so training time could be improved by optimizing the parser. 6 Related Work Several other aligners have used discriminative training. Our work borrows heavily from (Taskar et al., 2005), which uses a max-margin approach with a weighted maximum matching aligner. (Moore,</context>
<context position="29371" citStr="Zhang and Gildea, 2004" startWordPosition="4960" endWordPosition="4963">ustomized beam search. (Liu et al., 2005) uses a log-linear model with a greedy search. To our knowledge, ours is the first alignment approach to use this highly modular structured SVM, and the first discriminative method to use an ITG for the base aligner. (Gildea, 2003) presents another aligner with a soft syntactic constraint. This work adds a cloning operation to the tree-to-string generative model in (Yamada and Knight, 2001). This allows subtrees to move during translation. As the model is generative, it is much more difficult to incorporate a wide variety of features as we do here. In (Zhang and Gildea, 2004), this model was tested on the same annotated French-English sentence pairs that we divided into training and test sets for our experiments; it achieved an AER of 0.15. 7 Conclusion We have presented a discriminative, syntactic word alignment method. Discriminative training is conducted using a highly modular SVM for structured output, which allows code reuse between the syntactic aligner and a maximum matching baseline. An ITG parser is used for the alignment search, exposing two syntactic features: the use of inverted productions, and the use of spans that would not be available in a tree-to</context>
</contexts>
<marker>Zhang, Gildea, 2004</marker>
<rawString>H. Zhang and D. Gildea. 2004. Syntax-based alignment: Supervised or unsupervised? In Proceedings of COLING, Geneva, Switzerland, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>