<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002125">
<title confidence="0.975451">
Choosing an Evaluation Metric for Parser Design
</title>
<author confidence="0.894128">
Woodley Packard
</author>
<email confidence="0.973951">
sweaglesw@sweaglesw.org
</email>
<sectionHeader confidence="0.995368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862272727273">
This paper seeks to quantitatively evaluate the
degree to which a number of popular met-
rics provide overlapping information to parser
designers. Two routine tasks are considered:
optimizing a machine learning regularization
parameter and selecting an optimal machine
learning feature set. The main result is that the
choice of evaluation metric used to optimize
these problems (with one exception among
popular metrics) has little effect on the solu-
tion to the optimization.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9931298">
The question of how best to evaluate the perfor-
mance of a parser has received considerable atten-
tion. Numerous metrics have been proposed, and
their relative merits have been debated. In this pa-
per, we seek to quantitatively evaluate the degree to
which a number of popular metrics provide overlap-
ping information for two concrete subtasks of the
parser design problem.
The motivation for this study was to confirm our
suspicion that parsing models that performed well
under one metric were likely to perform well un-
der other metrics, thereby validating the widespread
practice of using just a single metric when conduct-
ing research on improving parser performance. Our
results are cautiously optimistic on this front.&apos;
We use the problem of selecting the best per-
former from a large space of varied but related parse
&apos;Note that we are not suggesting that these metrics provide
redundant information for other uses, e.g. predicting utility for
any particular downstream task.
</bodyText>
<page confidence="0.987727">
29
</page>
<bodyText confidence="0.9999619">
disambiguation models (“parsers” henceforth) as the
setting for our study. The parsers are all conditional
log-linear disambiguators with quadratic regulariza-
tion, coupled to the English Resource Grammar
(ERG) (Flickinger, 2000), a broad-coverage HPSG-
based hand-built grammar of English. Analyses
from the ERG consist of a syntax tree together with
an underspecified logical formula called an MRS
(Copestake et al., 2005).
The parsers differ from each other along two di-
mensions: the feature templates employed, and the
degree of regularization used. There are 57 differ-
ent sets of traditional and novel feature templates
collecting a variety of syntactic and semantic data
about candidate ERG analyses. For each set of fea-
ture templates, parsers were trained with 41 different
values for the quadratic regularization parameter, for
a total of 2337 different parsers.
The WeScience Treebank of about 9100 sentences
(Ytrestøl et al., 2009) was used both for training and
testing the parsers, with 10-fold cross validation.
We break down the problem of selecting the best
parser into two tasks. The first task is to identify
the optimal value for the regularization parameter
for each set of feature templates. The second task
is to compare the different sets of feature templates
to each other, considering only the optimal value of
the regularization parameter for each, and select the
overall best. We attack each task with each of 14
metrics, and discuss the results.
</bodyText>
<sectionHeader confidence="0.996453" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.991321">
Comparisons of parser metrics have been under-
taken in the past. Carroll et al (1998) describe a
</bodyText>
<note confidence="0.3809615">
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 29–34,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999789047619048">
broad range of parser evaluation metrics, and com-
ment on their advantages and disadvantages, but do
not offer a quantitative comparison. A number of pa-
pers such as Clark and Curran (2007) have explored
the difficulty of parser comparison across different
underlying formalisms.
Crouch et al (2002) compare two variant
dependency-based metrics in some detail on a single
LFG-based parsing model, concluding that despite
some differences in the metrics’ strategies, they of-
fer similar views on the performance of their parser.
The literature specifically seeking to quantita-
tively compare a broad range of metrics across a
large array of parsers is small. Emms (2008) de-
scribes the tree-distance metric and compares the
rankings induced by several variants of that met-
ric and PARSEVAL on a collection of six statisti-
cal parsers, finding broad compatibility, but observ-
ing frequent disagreement about the relative ranks
of two parsers whose scores were only marginally
different.
</bodyText>
<sectionHeader confidence="0.996105" genericHeader="method">
3 Metrics
</sectionHeader>
<bodyText confidence="0.9984825">
In our setup, the overall score a metric assigns to
a parser is the average of the scores awarded for
the parser’s analyses of each sentence in the tree-
bank (termed macro-averaging, in contrast to micro-
averaging which is also common). For sentences
where the parser selects several candidate analyses
as tied best analyses, the actual metric score used is
the average value of the metric applied to the differ-
ent tied best analyses. Fourteen metrics are consid-
ered:
</bodyText>
<listItem confidence="0.997835">
• Exact Tree Match (ETM) (Toutanova et al.,
2005) - 100% if the returned tree is identical
to the gold tree, and 0% otherwise.
• Exact MRS Match (EMM) - 100% if the re-
turned MRS is equivalent to the gold MRS, and
0% otherwise.
• Average Crossing Brackets (AXB) - the num-
ber of brackets (constituents) in the returned
tree that overlap incompatibly with some
bracket in the gold tree. Sign-inverted for com-
parability to the other metrics.
• Zero Crossing Brackets (ZXB) - 100% if the
AXB score is 0, and 0% otherwise.
• Labeled PARSEVAL (LP) (Abney et al., 1991)
- the harmonic mean (F1) of the precision and
recall for comparing the set of labeled brack-
ets in the returned tree with the set of labeled
brackets in the gold tree. Labels are rule names.
• Unlabeled PARSEVAL (UP) - identical to LP,
except ignoring the labels on the brackets.
• Labeled Syntactic Dependencies (LSD) (Buch-
holz and Marsi, 2006) - the F1 for comparing
the sets of directed bilexical syntactic depen-
dencies extracted from the returned and gold
trees, labeled by the rule name that joins the
dependent to the dependee.
• Unlabeled Syntactic Dependencies (USD) -
identical to LSD, except ignoring the labels.
• Labeled Elementary Dependencies (LED) - the
F1 for comparing the sets of elementary depen-
dency triples (Oepen and Lønning, 2006) ex-
tracted from the returned and gold MRS. These
annotations are similar in spirit to those used in
the PARC 700 Dependency Bank (King et al.,
2003) and other semantic dependency evalua-
tion schemes.
• Unlabeled Elementary Dependencies (UED) -
identical to LED, except ignoring all labeling
information other than the input positions in-
volved.
• Leaf Ancestor (LA) (Sampson and Babarczy,
2003) - the average of the edit distances be-
tween the paths through the returned and gold
trees from root to each leaf.
• Lexeme Name Match (LNM) - the percentage
of input words parsed with the gold lexeme2.
• Part-of-Speech Match (POS) - the percentage
of input words parsed with the gold part of
speech.
• Node Count Match (NCM) - 100% if the gold
and returned trees have exactly the same num-
ber of nodes, and 0% otherwise.
</listItem>
<bodyText confidence="0.64758375">
2In the ERG, lexemes are detailed descriptions of the syn-
tactic and semantic properties of individual words. There can
be multiple candidate lexemes for each word with the same part
of speech.
</bodyText>
<page confidence="0.988926">
30
</page>
<note confidence="0.389951">
Regularized Performance of pcfg baseline
</note>
<figureCaption confidence="0.999165">
Figure 1: ETM for “pcfg baseline”
</figureCaption>
<figure confidence="0.98803">
Z-Score Comparison of Metrics
1
0.5
Z-Scares 0
-0.5
-1
-1.5
0.001 0.01 0.1 1 10 100 1000
Regularization
</figure>
<figureCaption confidence="0.908013">
Figure 2: Z-scores for all metrics for “pcfg baseline”
</figureCaption>
<figure confidence="0.993741357142857">
0.001 0.01 0.1 1 10 100 1000
Regularization Variance Parameter
Exact Match Accuracy (%)
42
40
38
36
34
32
30
28
26
24
pcfg baseline
</figure>
<bodyText confidence="0.999782333333333">
Note that the last three metrics are not commonly
used in parser evaluation, and we have no reason
to expect them to be particularly informative. They
were included for variety – in a sense serving as con-
trols, to see how informative a very unsophisticated
metric can be.
</bodyText>
<sectionHeader confidence="0.846257" genericHeader="method">
4 Optimizing the Regularization
Parameter
</sectionHeader>
<bodyText confidence="0.988402730769231">
The first half of our problem is: given a set of fea-
ture templates T, determine the optimal value for the
regularization parameter A. We interpret the word
“optimal” relative to each of our 14 metrics. This is
quite straightforward: to optimize relative to metric
µ, we simply evaluate µ(M(T, A)) for each value of
A, where M(T, A) is a parser trained using feature
templates T and regularization parameter A, and de-
clare the value of A yielding the greatest value of
µ the winner. Figure 1 shows values of the ETM
as a function of the regularization parameter A for
T = “pcfg baseline”3; as can easily be seen, the op-
timal value is approximately �Aµ = 2.
We are interested in how �Aµ varies with different
choices of µ. Figure 2 shows all 14 metrics as func-
tions of A for the same T = “pcfg baseline.” The
actual scores from the metrics vary broadly, so the
vertical axes of the superimposed plots have been
rescaled to allow for easier comparison.
A priori we might expect the optimal �µ to be
3Note that we are not actually considering a PCFG here; in-
stead we are looking at a conditional log-linear model whose
features are shaped like PCFG configurations.
quite different for different µ, but this does not turn
out to be the case. The curves for all of the met-
rics peak in roughly the same place, with one no-
ticeable outlier (AXB). The actual peak4 regulariza-
tion parameters for the 14 metrics were all in the
range [1.8, 3.9] except for the outlier AXB, which
was 14.8.
Relative to the range under consideration, the op-
timal regularization parameters can be seen by in-
spection to depend very little on the metric. Near the
optima, the graphs are all quite flat, and we calcu-
lated that by choosing the optimal regularization pa-
rameter according to any of the metrics (with the ex-
ception of the outlier AXB), the maximum increase
in error rate visible through the other metrics was
1.6%. If we ignore LNM, POS and NCM (the non-
standard metrics we included for variety) in addition
to AXB, the maximum increase in error rate result-
ing from using an alternate metric to optimize the
regularization parameter drops to 0.41%.
“pcfg baseline” is just one of 57 sets of feature
templates. However, the situation is essentially the
same with each of the remaining 56. The average
maximum error rate increase observed across all of
the sets of feature templates when optimizing on any
metric (including AXB, LNM, POS and NCM) was
2.54%; on the worst single set of feature templates it
was 6.7%. Excluding AXB, the average maximum
error rate increase was 1.7%. Additionally exclud-
</bodyText>
<footnote confidence="0.99601475">
4Due to noisiness near the tops of the graphs, the reported
optimum regularization parameters are actually the averages of
the best 3 values. We attribute the noise to the limited size of
our corpus.
</footnote>
<page confidence="0.9998">
31
</page>
<bodyText confidence="0.997990875">
ing LNM, POS and NCM it was 0.81%.
Given the size of the evaluation corpus we are
using, the significance of an error rate increase of
0.81% is very marginal. We conclude that, at least
in circumstances similar to ours, the choice of met-
ric used to optimize regularization parameters is not
important, provided we avoid AXB and the variety
metrics LNM, POS and NCM.
</bodyText>
<sectionHeader confidence="0.69632" genericHeader="method">
5 Choosing a Set of Feature Templates
</sectionHeader>
<bodyText confidence="0.99967728">
The second half of our problem is: given a col-
lection T of different sets of feature templates, se-
lect the optimal performer. Again, we interpret
the word “optimal” relative to each of our 14 met-
rics, and the selection is straightforward: given
a metric µ, we first form a set of parsers P =
{M(T, arg maxλ µ(M(T, λ))) : T E T } and then
select arg maxp,P µ(p). That is, we train parsers
using the µ-optimal regularization parameter for
each T E T , and then select the µ-optimal parser
from that set.
In our experiments, all 14 of the metrics ranked
the same set of feature templates as best.
It is also interesting to inspect the order that each
metric imposes on P. There was some disagree-
ment between the metrics about this order. We com-
puted pairwise Spearman rank correlations coeffi-
cients5 for the different metrics. As with the task
of choosing a regularization parameter, the metrics
AXB, LNM, POS and NCM were outliers. The av-
erage pairwise Spearman rank correlation exclud-
ing these metrics was 0.859 and the minimum was
0.761.
An alternate method of quantifying the degree of
agreement is described below.
</bodyText>
<subsectionHeader confidence="0.94294">
5.1 Epsila
</subsectionHeader>
<bodyText confidence="0.999979">
Consider two metrics µ : P H R and ρ : P H R.
Assume for simplicity that for both µ and ρ, larger
values are better and 100 is perfect. If x, y E P
then the error rate reduction from y to x under µ
</bodyText>
<equation confidence="0.918152">
is µ*(x, y) = µ(x)−µ(y)
100�µ(y) . Let 5µ,ρ be the smallest
</equation>
<footnote confidence="0.9143772">
number such that Vx, y E P : µ*(x, y) &gt; Eµ,ρ �
5The Spearman rank correlation coefficient of two metrics
is defined as the Pearson correlation coefficient of the ranks the
metrics assign to the elements of P. It takes values between −1
and 1, with larger values indicating higher ranking agreement.
</footnote>
<bodyText confidence="0.9969395">
ρ*(x, y) &gt; 0. Informally, this says for all pairs of
parsers x and y, if x is at least cµ,ρ better than y when
evaluated under µ, then we are guaranteed that x is
at least a tiny bit better than y when evaluated under
ρ. For an unrestricted domain of parsers, we are not
guaranteed that such epsila exist or are small enough
to be interesting. However, since our P is finite, we
can find an c that will provide the required property
at least within P.
Eµ,ρ serves as a measure of how similar µ and ρ
are: if cµ,ρ is small, then small improvements seen
under µ will be visible as improvements under ρ,
whereas if cµ,ρ is large, then small improvements
seen under µ may in fact be regressions when evalu-
ating with ρ.
We computed pairwise epsila for our 14 metrics.
A large portion of pairwise epsila were around 5%,
with some being considerably smaller or larger.
</bodyText>
<subsectionHeader confidence="0.997309">
5.2 Clustering
</subsectionHeader>
<bodyText confidence="0.999817142857143">
In order to make sense of the idea that these ep-
sila provide a similarity measure, we applied Quality
Threshold clustering (Heyer et al., 1999) to discover
maximal clusters of metrics within which all pair-
wise epsila are smaller than a given threshold. Small
thresholds produce many small clusters, while larger
thresholds produce fewer, larger clusters.
At a 1% threshold, almost all of the metrics form
singleton clusters; that is, a 1% error rate reduction
on any given metric is generally not enough to guar-
antee that any other metrics will see any error reduc-
tion at all. The exceptions were that {ETM, EMM}
formed a cluster, and {UED, LED} formed a cluster.
Increasing the threshold to 3%, a new cluster
{USD, LSD} forms (indicating that a 3% error rate
reduction in USD always is visible as some level
of error rate reduction in LSD, and vice versa), and
ZXB joins the {ETM, EMM} cluster.
By the time we reach a 5% threshold, the major-
ity (7 out of 11) of the “standard” parser evaluation
metrics have merged into a single cluster, consisting
of {ETM, EMM, ZXB, LA, LSD, UED, LED}. The
PARSEVAL metrics form a cluster of their own {UP,
LP}.
Increasing the threshold even more to 10% causes
10 out of 11 “standard” evaluation metrics to cluster
together; the only holdout is AXB (average number
of crossing brackets), which does not join the cluster
</bodyText>
<page confidence="0.9951">
32
</page>
<figure confidence="0.994869310344828">
Z-Score Comparison of Feature Sets Z-Score Comparison of Metrics
Z-8����.
-0.2
0.8
0.6
0.4
0.2
1.8
1.6
1.4
1.2
2
0
1
Z������.
-0.2
0.8
0.6
0.4
0.2
1.8
1.6
1.4
1.2
2
0
1
2 4 6 8 10 12 14 0 10 20 30 40 50 60
Metric Feature Set
</figure>
<figureCaption confidence="0.8979804">
Figure 3: Z-scores for all feature sets on the Y axis (one
line per feature set); different metrics on the X axis. The
“control” metrics and the outlier AXB are on the far right
end.
even at a 20% threshold.
</figureCaption>
<subsectionHeader confidence="0.983507">
5.3 Visualization
</subsectionHeader>
<bodyText confidence="0.999984681818182">
To qualitatively illustrate the degree of variation in
scores attributable to differences in metric as op-
posed to differences in feature sets, and the extent of
the metrics’ agreements in ranking the feature sets,
we plotted linearly rescaled scores from the metrics
(at their optimum regularization parameter value) in
two ways.
In Figure 3, the scores of each feature set are plot-
ted as a function of which metric is being used. To
the extent that the lines are horizontal, the metrics
provide identical information. To the extent that the
lines do not cross, the metrics agree about the rela-
tive ordering of the feature sets. Note that the three
control metrics and the outlier metric AXB are plot-
ted on the far right of the figure, and show signifi-
cantly more line crossings.
In Figure 4, the score from each metric is plot-
ted as a function of which feature set is being evalu-
ated, sorted in increasing order of the LP metric. As
can be seen, the increasing trend of the LP metric
is clearly mirrored in all the other metrics graphed,
although there is a degree of variability.
</bodyText>
<sectionHeader confidence="0.998972" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.630169">
From both subtasks, we saw that the Average Cross-
ing Brackets metric (AXB) is a serious outlier. We
cannot say whether it provides complementary in-
</bodyText>
<figureCaption confidence="0.991018">
Figure 4: Z-scores for all metrics except AXB, LNM,
POS and NCM on the Y axis (one line per metric); dif-
ferent feature sets on the X axis.
</figureCaption>
<bodyText confidence="0.999968387096774">
formation or actually misleading information; in-
deed, that might depend on the nature of the down-
stream application.
We can say with confidence that for the subtask of
optimizing a regularization parameter, there is very
little difference between the popular metrics {ETM,
EMM, ZXB, LA, LP, UP, LSD, USD, LED, UED}.
For the subtask of choosing the optimal set of fea-
ture templates, there was even greater agreement: all
14 metrics arrived at the same result. Although they
did not impose the exact same rankings, the rankings
were similar. It is interesting (and entertaining) that
even the three “control” metrics (LNM, POS and
NCM) selected the same optimal feature set. It is
particularly surprising that even the absurdly simple
NCM metric, which does nothing but check whether
two trees have the same number of nodes, irrespec-
tive of their structure or labels, when averaged over
thousands of items, can identify the best feature set.
Our findings agree with (Crouch et al., 2002)’s
suggestion that different metrics can offer similar
views on error rate reduction.
Clustering based on epsila at the 5% and 10%
thresholds showed interesting insights as well. We
demonstrated that a 5% error rate reduction as seen
on any of {ETM, EMM, ZXB, LA, LSD, UED,
LED} is also visible from the others (although the
popular PARSEVAL metrics were outliers at this
threshold). This has the encouraging implication
that a decision made on the basis of strong evidence
from just one metric is not likely to be contradicted
</bodyText>
<page confidence="0.997011">
33
</page>
<bodyText confidence="0.999988944444445">
by evaluations by other metrics. However, we must
point out that the precise values of these thresholds
are dependent on our setup. They would likely be
larger if a significantly larger number of parsers or a
significantly more varied group of parsers were con-
sidered, and conversely would perhaps be smaller if
a larger evaluation corpus were used (reducing the
noise).
Our data only directly apply to the tasks of se-
lecting the value of the regularization parameter and
selecting feature templates for a conditional log-
likelihood model for parsing with the ERG. How-
ever, we expect the results to generalize at least to
similar tasks with other precision grammars, and
probably treebank-derived parsers as well. Explo-
ration of how well these results hold for other tasks
and for other types of parsers is an excellent subject
for future research.
</bodyText>
<sectionHeader confidence="0.998145" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999759893939394">
S. Abney, D. Flickinger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, et al. 1991. Procedure for quan-
titatively comparing the syntactic coverage of English
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306–311. Association
for Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149–164,
New York City, June. Association for Computational
Linguistics.
J. Carroll, T. Briscoe, and A. Sanfilippo. 1998. Parser
evaluation: a survey and a new proposal. In Proceed-
ings of the 1st International Conference on Language
Resources and Evaluation, pages 447–454.
S. Clark and J. Curran. 2007. Formalism-independent
parser evaluation with CCG and DepBank. In An-
nual Meeting-Association for Computational Linguis-
tics, volume 45, page 248.
A. Copestake, D. Flickinger, C. Pollard, and I.A. Sag.
2005. Minimal recursion semantics: An introduction.
Research on Language &amp; Computation, 3(4):281–332.
R. Crouch, R.M. Kaplan, T.H. King, and S. Riezler.
2002. A comparison of evaluation metrics for a broad-
coverage stochastic parser. In Beyond PARSEVAL
workshop at 3rd Int. Conference on Language Re-
sources an Evaluation (LREC 2002).
Martin Emms. 2008. Tree distance and some other
variants of evalb. In Bente Maegaard Joseph Mari-
ani Jan Odjik Stelios Piperidis Daniel Tapias Nicoletta
Calzolari (Conference Chair), Khalid Choukri, edi-
tor, Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC’08),
Marrakech, Morocco, may. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6(01):15–28.
L.J. Heyer, S. Kruglyak, and S. Yooseph. 1999. Explor-
ing expression data: identification and analysis of co-
expressed genes. Genome research, 9(11):1106.
T.H. King, R. Crouch, S. Riezler, M. Dalrymple, and
R. Kaplan. 2003. The PARC 700 dependency
bank. In Proceedings of the EACL03: 4th Interna-
tional Workshop on Linguistically Interpreted Corpora
(LINC-03), pages 1–8.
S. Oepen and J.T. Lønning. 2006. Discriminant-based
MRS banking. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006).
G. Sampson and A. Babarczy. 2003. A test of the leaf-
ancestor metric for parse accuracy. Natural Language
Engineering, 9(04):365–380.
K. Toutanova, C.D. Manning, D. Flickinger, and
S. Oepen. 2005. Stochastic HPSG parse disambigua-
tion using the Redwoods corpus. Research on Lan-
guage &amp; Computation, 3(1):83–105.
Gisle Ytrestøl, Dan Flickinger, and Stephan Oepen.
2009. Extracting and Annotating Wikipedia Sub-
Domains. Towards a New eScience Community Re-
source. In Proceedings of the Seventh Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries, Groningen, The Netherlands.
</reference>
<page confidence="0.99932">
34
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.893773">
<title confidence="0.998783">Choosing an Evaluation Metric for Parser Design</title>
<author confidence="0.977409">Woodley Packard</author>
<email confidence="0.988348">sweaglesw@sweaglesw.org</email>
<abstract confidence="0.99370225">This paper seeks to quantitatively evaluate the degree to which a number of popular metrics provide overlapping information to parser designers. Two routine tasks are considered: optimizing a machine learning regularization parameter and selecting an optimal machine learning feature set. The main result is that the choice of evaluation metric used to optimize these problems (with one exception among popular metrics) has little effect on the solution to the optimization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>D Flickinger</author>
<author>C Gdaniec</author>
<author>C Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
</authors>
<title>Procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>306--311</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5317" citStr="Abney et al., 1991" startWordPosition="847" endWordPosition="850">rent tied best analyses. Fourteen metrics are considered: • Exact Tree Match (ETM) (Toutanova et al., 2005) - 100% if the returned tree is identical to the gold tree, and 0% otherwise. • Exact MRS Match (EMM) - 100% if the returned MRS is equivalent to the gold MRS, and 0% otherwise. • Average Crossing Brackets (AXB) - the number of brackets (constituents) in the returned tree that overlap incompatibly with some bracket in the gold tree. Sign-inverted for comparability to the other metrics. • Zero Crossing Brackets (ZXB) - 100% if the AXB score is 0, and 0% otherwise. • Labeled PARSEVAL (LP) (Abney et al., 1991) - the harmonic mean (F1) of the precision and recall for comparing the set of labeled brackets in the returned tree with the set of labeled brackets in the gold tree. Labels are rule names. • Unlabeled PARSEVAL (UP) - identical to LP, except ignoring the labels on the brackets. • Labeled Syntactic Dependencies (LSD) (Buchholz and Marsi, 2006) - the F1 for comparing the sets of directed bilexical syntactic dependencies extracted from the returned and gold trees, labeled by the rule name that joins the dependent to the dependee. • Unlabeled Syntactic Dependencies (USD) - identical to LSD, excep</context>
</contexts>
<marker>Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, 1991</marker>
<rawString>S. Abney, D. Flickinger, C. Gdaniec, C. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, et al. 1991. Procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of the workshop on Speech and Natural Language, pages 306–311. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="5662" citStr="Buchholz and Marsi, 2006" startWordPosition="907" endWordPosition="911">kets (constituents) in the returned tree that overlap incompatibly with some bracket in the gold tree. Sign-inverted for comparability to the other metrics. • Zero Crossing Brackets (ZXB) - 100% if the AXB score is 0, and 0% otherwise. • Labeled PARSEVAL (LP) (Abney et al., 1991) - the harmonic mean (F1) of the precision and recall for comparing the set of labeled brackets in the returned tree with the set of labeled brackets in the gold tree. Labels are rule names. • Unlabeled PARSEVAL (UP) - identical to LP, except ignoring the labels on the brackets. • Labeled Syntactic Dependencies (LSD) (Buchholz and Marsi, 2006) - the F1 for comparing the sets of directed bilexical syntactic dependencies extracted from the returned and gold trees, labeled by the rule name that joins the dependent to the dependee. • Unlabeled Syntactic Dependencies (USD) - identical to LSD, except ignoring the labels. • Labeled Elementary Dependencies (LED) - the F1 for comparing the sets of elementary dependency triples (Oepen and Lønning, 2006) extracted from the returned and gold MRS. These annotations are similar in spirit to those used in the PARC 700 Dependency Bank (King et al., 2003) and other semantic dependency evaluation sc</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149–164, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>T Briscoe</author>
<author>A Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<contexts>
<context position="3133" citStr="Carroll et al (1998)" startWordPosition="488" endWordPosition="491"> both for training and testing the parsers, with 10-fold cross validation. We break down the problem of selecting the best parser into two tasks. The first task is to identify the optimal value for the regularization parameter for each set of feature templates. The second task is to compare the different sets of feature templates to each other, considering only the optimal value of the regularization parameter for each, and select the overall best. We attack each task with each of 14 metrics, and discuss the results. 2 Prior Work Comparisons of parser metrics have been undertaken in the past. Carroll et al (1998) describe a Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 29–34, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics broad range of parser evaluation metrics, and comment on their advantages and disadvantages, but do not offer a quantitative comparison. A number of papers such as Clark and Curran (2007) have explored the difficulty of parser comparison across different underlying formalisms. Crouch et al (2002) compare two variant dependency-based metrics in some detail on a single LFG-based parsing model, concluding that despite some differ</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>J. Carroll, T. Briscoe, and A. Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the 1st International Conference on Language Resources and Evaluation, pages 447–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
</authors>
<title>Formalism-independent parser evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Annual Meeting-Association for Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>248</pages>
<contexts>
<context position="3490" citStr="Clark and Curran (2007)" startWordPosition="543" endWordPosition="546">ring only the optimal value of the regularization parameter for each, and select the overall best. We attack each task with each of 14 metrics, and discuss the results. 2 Prior Work Comparisons of parser metrics have been undertaken in the past. Carroll et al (1998) describe a Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 29–34, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics broad range of parser evaluation metrics, and comment on their advantages and disadvantages, but do not offer a quantitative comparison. A number of papers such as Clark and Curran (2007) have explored the difficulty of parser comparison across different underlying formalisms. Crouch et al (2002) compare two variant dependency-based metrics in some detail on a single LFG-based parsing model, concluding that despite some differences in the metrics’ strategies, they offer similar views on the performance of their parser. The literature specifically seeking to quantitatively compare a broad range of metrics across a large array of parsers is small. Emms (2008) describes the tree-distance metric and compares the rankings induced by several variants of that metric and PARSEVAL on a</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>S. Clark and J. Curran. 2007. Formalism-independent parser evaluation with CCG and DepBank. In Annual Meeting-Association for Computational Linguistics, volume 45, page 248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1987" citStr="Copestake et al., 2005" startWordPosition="300" endWordPosition="303"> large space of varied but related parse &apos;Note that we are not suggesting that these metrics provide redundant information for other uses, e.g. predicting utility for any particular downstream task. 29 disambiguation models (“parsers” henceforth) as the setting for our study. The parsers are all conditional log-linear disambiguators with quadratic regularization, coupled to the English Resource Grammar (ERG) (Flickinger, 2000), a broad-coverage HPSGbased hand-built grammar of English. Analyses from the ERG consist of a syntax tree together with an underspecified logical formula called an MRS (Copestake et al., 2005). The parsers differ from each other along two dimensions: the feature templates employed, and the degree of regularization used. There are 57 different sets of traditional and novel feature templates collecting a variety of syntactic and semantic data about candidate ERG analyses. For each set of feature templates, parsers were trained with 41 different values for the quadratic regularization parameter, for a total of 2337 different parsers. The WeScience Treebank of about 9100 sentences (Ytrestøl et al., 2009) was used both for training and testing the parsers, with 10-fold cross validation.</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>A. Copestake, D. Flickinger, C. Pollard, and I.A. Sag. 2005. Minimal recursion semantics: An introduction. Research on Language &amp; Computation, 3(4):281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Crouch</author>
<author>R M Kaplan</author>
<author>T H King</author>
<author>S Riezler</author>
</authors>
<title>A comparison of evaluation metrics for a broadcoverage stochastic parser.</title>
<date>2002</date>
<booktitle>In Beyond PARSEVAL workshop at 3rd Int. Conference on Language Resources an Evaluation (LREC</booktitle>
<contexts>
<context position="3600" citStr="Crouch et al (2002)" startWordPosition="558" endWordPosition="561">task with each of 14 metrics, and discuss the results. 2 Prior Work Comparisons of parser metrics have been undertaken in the past. Carroll et al (1998) describe a Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 29–34, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics broad range of parser evaluation metrics, and comment on their advantages and disadvantages, but do not offer a quantitative comparison. A number of papers such as Clark and Curran (2007) have explored the difficulty of parser comparison across different underlying formalisms. Crouch et al (2002) compare two variant dependency-based metrics in some detail on a single LFG-based parsing model, concluding that despite some differences in the metrics’ strategies, they offer similar views on the performance of their parser. The literature specifically seeking to quantitatively compare a broad range of metrics across a large array of parsers is small. Emms (2008) describes the tree-distance metric and compares the rankings induced by several variants of that metric and PARSEVAL on a collection of six statistical parsers, finding broad compatibility, but observing frequent disagreement about</context>
<context position="17680" citStr="Crouch et al., 2002" startWordPosition="3049" endWordPosition="3052">ature templates, there was even greater agreement: all 14 metrics arrived at the same result. Although they did not impose the exact same rankings, the rankings were similar. It is interesting (and entertaining) that even the three “control” metrics (LNM, POS and NCM) selected the same optimal feature set. It is particularly surprising that even the absurdly simple NCM metric, which does nothing but check whether two trees have the same number of nodes, irrespective of their structure or labels, when averaged over thousands of items, can identify the best feature set. Our findings agree with (Crouch et al., 2002)’s suggestion that different metrics can offer similar views on error rate reduction. Clustering based on epsila at the 5% and 10% thresholds showed interesting insights as well. We demonstrated that a 5% error rate reduction as seen on any of {ETM, EMM, ZXB, LA, LSD, UED, LED} is also visible from the others (although the popular PARSEVAL metrics were outliers at this threshold). This has the encouraging implication that a decision made on the basis of strong evidence from just one metric is not likely to be contradicted 33 by evaluations by other metrics. However, we must point out that the </context>
</contexts>
<marker>Crouch, Kaplan, King, Riezler, 2002</marker>
<rawString>R. Crouch, R.M. Kaplan, T.H. King, and S. Riezler. 2002. A comparison of evaluation metrics for a broadcoverage stochastic parser. In Beyond PARSEVAL workshop at 3rd Int. Conference on Language Resources an Evaluation (LREC 2002).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Martin Emms</author>
</authors>
<title>Tree distance and some other variants of evalb.</title>
<date>2008</date>
<booktitle>In Bente Maegaard Joseph Mariani Jan Odjik Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Conference Chair), Khalid Choukri, editor, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="3968" citStr="Emms (2008)" startWordPosition="617" endWordPosition="618">on their advantages and disadvantages, but do not offer a quantitative comparison. A number of papers such as Clark and Curran (2007) have explored the difficulty of parser comparison across different underlying formalisms. Crouch et al (2002) compare two variant dependency-based metrics in some detail on a single LFG-based parsing model, concluding that despite some differences in the metrics’ strategies, they offer similar views on the performance of their parser. The literature specifically seeking to quantitatively compare a broad range of metrics across a large array of parsers is small. Emms (2008) describes the tree-distance metric and compares the rankings induced by several variants of that metric and PARSEVAL on a collection of six statistical parsers, finding broad compatibility, but observing frequent disagreement about the relative ranks of two parsers whose scores were only marginally different. 3 Metrics In our setup, the overall score a metric assigns to a parser is the average of the scores awarded for the parser’s analyses of each sentence in the treebank (termed macro-averaging, in contrast to microaveraging which is also common). For sentences where the parser selects seve</context>
</contexts>
<marker>Emms, 2008</marker>
<rawString>Martin Emms. 2008. Tree distance and some other variants of evalb. In Bente Maegaard Joseph Mariani Jan Odjik Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Conference Chair), Khalid Choukri, editor, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http://www.lrecconf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>01</issue>
<contexts>
<context position="1794" citStr="Flickinger, 2000" startWordPosition="272" endWordPosition="273">t a single metric when conducting research on improving parser performance. Our results are cautiously optimistic on this front.&apos; We use the problem of selecting the best performer from a large space of varied but related parse &apos;Note that we are not suggesting that these metrics provide redundant information for other uses, e.g. predicting utility for any particular downstream task. 29 disambiguation models (“parsers” henceforth) as the setting for our study. The parsers are all conditional log-linear disambiguators with quadratic regularization, coupled to the English Resource Grammar (ERG) (Flickinger, 2000), a broad-coverage HPSGbased hand-built grammar of English. Analyses from the ERG consist of a syntax tree together with an underspecified logical formula called an MRS (Copestake et al., 2005). The parsers differ from each other along two dimensions: the feature templates employed, and the degree of regularization used. There are 57 different sets of traditional and novel feature templates collecting a variety of syntactic and semantic data about candidate ERG analyses. For each set of feature templates, parsers were trained with 41 different values for the quadratic regularization parameter,</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering, 6(01):15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L J Heyer</author>
<author>S Kruglyak</author>
<author>S Yooseph</author>
</authors>
<title>Exploring expression data: identification and analysis of coexpressed genes.</title>
<date>1999</date>
<journal>Genome research,</journal>
<pages>9--11</pages>
<contexts>
<context position="13646" citStr="Heyer et al., 1999" startWordPosition="2335" endWordPosition="2338">l provide the required property at least within P. Eµ,ρ serves as a measure of how similar µ and ρ are: if cµ,ρ is small, then small improvements seen under µ will be visible as improvements under ρ, whereas if cµ,ρ is large, then small improvements seen under µ may in fact be regressions when evaluating with ρ. We computed pairwise epsila for our 14 metrics. A large portion of pairwise epsila were around 5%, with some being considerably smaller or larger. 5.2 Clustering In order to make sense of the idea that these epsila provide a similarity measure, we applied Quality Threshold clustering (Heyer et al., 1999) to discover maximal clusters of metrics within which all pairwise epsila are smaller than a given threshold. Small thresholds produce many small clusters, while larger thresholds produce fewer, larger clusters. At a 1% threshold, almost all of the metrics form singleton clusters; that is, a 1% error rate reduction on any given metric is generally not enough to guarantee that any other metrics will see any error reduction at all. The exceptions were that {ETM, EMM} formed a cluster, and {UED, LED} formed a cluster. Increasing the threshold to 3%, a new cluster {USD, LSD} forms (indicating that</context>
</contexts>
<marker>Heyer, Kruglyak, Yooseph, 1999</marker>
<rawString>L.J. Heyer, S. Kruglyak, and S. Yooseph. 1999. Exploring expression data: identification and analysis of coexpressed genes. Genome research, 9(11):1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H King</author>
<author>R Crouch</author>
<author>S Riezler</author>
<author>M Dalrymple</author>
<author>R Kaplan</author>
</authors>
<title>The PARC 700 dependency bank.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL03: 4th International Workshop on Linguistically Interpreted Corpora (LINC-03),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6218" citStr="King et al., 2003" startWordPosition="1001" endWordPosition="1004">abeled Syntactic Dependencies (LSD) (Buchholz and Marsi, 2006) - the F1 for comparing the sets of directed bilexical syntactic dependencies extracted from the returned and gold trees, labeled by the rule name that joins the dependent to the dependee. • Unlabeled Syntactic Dependencies (USD) - identical to LSD, except ignoring the labels. • Labeled Elementary Dependencies (LED) - the F1 for comparing the sets of elementary dependency triples (Oepen and Lønning, 2006) extracted from the returned and gold MRS. These annotations are similar in spirit to those used in the PARC 700 Dependency Bank (King et al., 2003) and other semantic dependency evaluation schemes. • Unlabeled Elementary Dependencies (UED) - identical to LED, except ignoring all labeling information other than the input positions involved. • Leaf Ancestor (LA) (Sampson and Babarczy, 2003) - the average of the edit distances between the paths through the returned and gold trees from root to each leaf. • Lexeme Name Match (LNM) - the percentage of input words parsed with the gold lexeme2. • Part-of-Speech Match (POS) - the percentage of input words parsed with the gold part of speech. • Node Count Match (NCM) - 100% if the gold and returne</context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>T.H. King, R. Crouch, S. Riezler, M. Dalrymple, and R. Kaplan. 2003. The PARC 700 dependency bank. In Proceedings of the EACL03: 4th International Workshop on Linguistically Interpreted Corpora (LINC-03), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>J T Lønning</author>
</authors>
<title>Discriminant-based MRS banking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="6070" citStr="Oepen and Lønning, 2006" startWordPosition="974" endWordPosition="977"> of labeled brackets in the gold tree. Labels are rule names. • Unlabeled PARSEVAL (UP) - identical to LP, except ignoring the labels on the brackets. • Labeled Syntactic Dependencies (LSD) (Buchholz and Marsi, 2006) - the F1 for comparing the sets of directed bilexical syntactic dependencies extracted from the returned and gold trees, labeled by the rule name that joins the dependent to the dependee. • Unlabeled Syntactic Dependencies (USD) - identical to LSD, except ignoring the labels. • Labeled Elementary Dependencies (LED) - the F1 for comparing the sets of elementary dependency triples (Oepen and Lønning, 2006) extracted from the returned and gold MRS. These annotations are similar in spirit to those used in the PARC 700 Dependency Bank (King et al., 2003) and other semantic dependency evaluation schemes. • Unlabeled Elementary Dependencies (UED) - identical to LED, except ignoring all labeling information other than the input positions involved. • Leaf Ancestor (LA) (Sampson and Babarczy, 2003) - the average of the edit distances between the paths through the returned and gold trees from root to each leaf. • Lexeme Name Match (LNM) - the percentage of input words parsed with the gold lexeme2. • Par</context>
</contexts>
<marker>Oepen, Lønning, 2006</marker>
<rawString>S. Oepen and J.T. Lønning. 2006. Discriminant-based MRS banking. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
<author>A Babarczy</author>
</authors>
<title>A test of the leafancestor metric for parse accuracy.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>04</issue>
<contexts>
<context position="6462" citStr="Sampson and Babarczy, 2003" startWordPosition="1037" endWordPosition="1040">o the dependee. • Unlabeled Syntactic Dependencies (USD) - identical to LSD, except ignoring the labels. • Labeled Elementary Dependencies (LED) - the F1 for comparing the sets of elementary dependency triples (Oepen and Lønning, 2006) extracted from the returned and gold MRS. These annotations are similar in spirit to those used in the PARC 700 Dependency Bank (King et al., 2003) and other semantic dependency evaluation schemes. • Unlabeled Elementary Dependencies (UED) - identical to LED, except ignoring all labeling information other than the input positions involved. • Leaf Ancestor (LA) (Sampson and Babarczy, 2003) - the average of the edit distances between the paths through the returned and gold trees from root to each leaf. • Lexeme Name Match (LNM) - the percentage of input words parsed with the gold lexeme2. • Part-of-Speech Match (POS) - the percentage of input words parsed with the gold part of speech. • Node Count Match (NCM) - 100% if the gold and returned trees have exactly the same number of nodes, and 0% otherwise. 2In the ERG, lexemes are detailed descriptions of the syntactic and semantic properties of individual words. There can be multiple candidate lexemes for each word with the same pa</context>
</contexts>
<marker>Sampson, Babarczy, 2003</marker>
<rawString>G. Sampson and A. Babarczy. 2003. A test of the leafancestor metric for parse accuracy. Natural Language Engineering, 9(04):365–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
<author>D Flickinger</author>
<author>S Oepen</author>
</authors>
<title>Stochastic HPSG parse disambiguation using the Redwoods corpus.</title>
<date>2005</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="4805" citStr="Toutanova et al., 2005" startWordPosition="753" endWordPosition="756">disagreement about the relative ranks of two parsers whose scores were only marginally different. 3 Metrics In our setup, the overall score a metric assigns to a parser is the average of the scores awarded for the parser’s analyses of each sentence in the treebank (termed macro-averaging, in contrast to microaveraging which is also common). For sentences where the parser selects several candidate analyses as tied best analyses, the actual metric score used is the average value of the metric applied to the different tied best analyses. Fourteen metrics are considered: • Exact Tree Match (ETM) (Toutanova et al., 2005) - 100% if the returned tree is identical to the gold tree, and 0% otherwise. • Exact MRS Match (EMM) - 100% if the returned MRS is equivalent to the gold MRS, and 0% otherwise. • Average Crossing Brackets (AXB) - the number of brackets (constituents) in the returned tree that overlap incompatibly with some bracket in the gold tree. Sign-inverted for comparability to the other metrics. • Zero Crossing Brackets (ZXB) - 100% if the AXB score is 0, and 0% otherwise. • Labeled PARSEVAL (LP) (Abney et al., 1991) - the harmonic mean (F1) of the precision and recall for comparing the set of labeled b</context>
</contexts>
<marker>Toutanova, Manning, Flickinger, Oepen, 2005</marker>
<rawString>K. Toutanova, C.D. Manning, D. Flickinger, and S. Oepen. 2005. Stochastic HPSG parse disambiguation using the Redwoods corpus. Research on Language &amp; Computation, 3(1):83–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gisle Ytrestøl</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Extracting and Annotating Wikipedia SubDomains. Towards a New eScience Community Resource.</title>
<date>2009</date>
<booktitle>In Proceedings of the Seventh International Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Groningen, The Netherlands.</location>
<contexts>
<context position="2504" citStr="Ytrestøl et al., 2009" startWordPosition="381" endWordPosition="384">t of a syntax tree together with an underspecified logical formula called an MRS (Copestake et al., 2005). The parsers differ from each other along two dimensions: the feature templates employed, and the degree of regularization used. There are 57 different sets of traditional and novel feature templates collecting a variety of syntactic and semantic data about candidate ERG analyses. For each set of feature templates, parsers were trained with 41 different values for the quadratic regularization parameter, for a total of 2337 different parsers. The WeScience Treebank of about 9100 sentences (Ytrestøl et al., 2009) was used both for training and testing the parsers, with 10-fold cross validation. We break down the problem of selecting the best parser into two tasks. The first task is to identify the optimal value for the regularization parameter for each set of feature templates. The second task is to compare the different sets of feature templates to each other, considering only the optimal value of the regularization parameter for each, and select the overall best. We attack each task with each of 14 metrics, and discuss the results. 2 Prior Work Comparisons of parser metrics have been undertaken in t</context>
</contexts>
<marker>Ytrestøl, Flickinger, Oepen, 2009</marker>
<rawString>Gisle Ytrestøl, Dan Flickinger, and Stephan Oepen. 2009. Extracting and Annotating Wikipedia SubDomains. Towards a New eScience Community Resource. In Proceedings of the Seventh International Workshop on Treebanks and Linguistic Theories, Groningen, The Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>