<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000510">
<title confidence="0.959579">
Frequency Estimates for Statistical Word Similarity Measures
</title>
<note confidence="0.823845333333333">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 165-172
Edmonton, May-June 2003
</note>
<author confidence="0.980226">
Egidio Terra
</author>
<affiliation confidence="0.9986615">
School of Computer Science
University of Waterloo
</affiliation>
<email confidence="0.989901">
elterra@math.uwaterloo.ca
</email>
<author confidence="0.983093">
C. L. A. Clarke
</author>
<affiliation confidence="0.9986845">
School of Computer Science
University of Waterloo
</affiliation>
<email confidence="0.991529">
claclark@plg2.uwaterloo.ca
</email>
<sectionHeader confidence="0.995573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951428571429">
Statistical measures of word similarity have ap-
plication in many areas of natural language pro-
cessing, such as language modeling and in-
formation retrieval. We report a comparative
study of two methods for estimating word co-
occurrence frequencies required by word sim-
ilarity measures. Our frequency estimates are
generated from a terabyte-sized corpus of Web
data, and we study the impact of corpus size
on the effectiveness of the measures. We base
the evaluation on one TOEFL question set and
two practice questions sets, each consisting of
a number of multiple choice questions seek-
ing the best synonym for a given target word.
For two question sets, a context for the target
word is provided, and we examine a number of
word similarity measures that exploit this con-
text. Our best combination of similarity mea-
sure and frequency estimation method answers
6-8% more questions than the best results pre-
viously reported for the same question sets.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998538592592593">
Many different statistical tests have been proposed to
measure the strength of word similarity or word associ-
ation in natural language texts (Dunning, 1993; Church
and Hanks, 1990; Dagan et al., 1999). These tests attempt
to measure dependence between words by using statistics
taken from a large corpus. In this context, a key assump-
tion is that similarity between words is a consequence of
word co-occurrence, or that the closeness of the words
in text is indicative of some kind of relationship between
them, such as synonymy or antonymy.
Although word sequences in natural language are un-
likely to be independent, these statistical tests provide
quantitative information that can be used to compare
pairs of co-occurring words. Also, despite the fact that
word co-occurrence is a simple idea, there are a vari-
ety of ways to estimate word co-occurrence frequencies
from text. Two words can appear close to each other
in the same document, passage, paragraph, sentence or
fixed-size window. The boundaries for determining co-
occurrence will affect the estimates and as a consequence
the word similarity measures.
Statistical word similarity measures play an impor-
tant role in information retrieval and in many other natu-
ral language applications, such as the automatic creation
of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin,
1998) and word sense disambiguation (Yarowsky, 1992;
Li and Abe, 1998). Pantel and Lin (2002) use word sim-
ilarity to create groups of related words, in order to dis-
cover word senses directly from text. Recently, Tan et
al. (2002) provide an analysis on different measures of
independence in the context of association rules.
Word similarity is also used in language modeling ap-
plications. Rosenfeld (1996) uses word similarity as a
constraint in a maximum entropy model which reduces
the perplexity on a test set by 23%. Brown et al. (1992)
use a word similarity measure for language modeling
in an interpolated model, grouping similar words into
classes. Dagan et al. (1999) use word similarity to assign
probabilities to unseen bigrams by using similar bigrams,
which reduces perplexity up to 20% in held out data.
In information retrieval, word similarity can be used to
identify terms for pseudo-relevance feedback (Harman,
1992; Buckley et al., 1995; Xu and Croft, 2000; Vechto-
mova and Robertson, 2000). Xu and Croft (2000) expand
queries under a pseudo-relevance feedback model by us-
ing similar words from documents retrieved and improve
effectiveness by more than 20% on an 11-point average
precision.
Landauer and Dumais (1997) applied word similarity
measures to answer TOEFL (Test Of English as a For-
eign Language) synonym questions using Latent Seman-
tic Analysis. Turney (2001) performed an evaluation of a
specific word similarity measure using the same TOEFL
questions and compared the results with those obtained
</bodyText>
<listItem confidence="0.481660666666667">
_ “The results of the test were quite [unambiguous].”
_ ‘unambiguous’
_ ‘clear’,‘doubtful’,‘surprising’, ‘illegal’
</listItem>
<figureCaption confidence="0.971202">
Figure 1: Finding the best synonym option in presence of context
</figureCaption>
<figure confidence="0.9382965">
_ ‘boast’
_ ‘brag’,‘yell’,‘complain’,‘explain’
</figure>
<figureCaption confidence="0.998447">
Figure 2: Finding the best synonym
</figureCaption>
<bodyText confidence="0.997575189189189">
by Landauer and Dumais.
In our investigation of frequency estimates for word
similarity measures, we compare the results of sev-
eral different measures and frequency estimates to solve
human-oriented language tests. Our investigation is
based in part on the questions used by Landauer and Du-
mais, and by Turney. An example of such tests is the
determination of the best synonym in a set of alternatives
for a specific target word in a
context , as shown in figure 1.
Ideally, the context can provide support to choose best al-
ternative for each question. We also investigate questions
where no context is available, as shown in figure 2. These
questions provides an easy way to assess the performance
of measures and the co-occurrence frequency estimation
methods used to compute them.
Although word similarity has been used in many dif-
ferent applications, to the best of our knowledge, ours is
the first comparative investigation of the impact of co-
occurrence frequency estimation on the performance of
word similarity measures. In this paper, we provide a
comprehensive study of some of the most widely used
similarity measures with frequency estimates taken from
a terabyte-sized corpus of Web data, both in the presence
of context and not. In addition, we investigate frequency
estimates for co-occurrence that are based both on docu-
ments and on a variety of different window sizes, and ex-
amine the impact of the corpus size on the frequency es-
timates. In questions where context is available, we also
investigate the effect of adding more words from context.
The remainder of this paper is organized as follows:
In section 2 we briefly introduce some of the most
commonly used methods for measuring word similarity.
In section 3 we present methods to assess word co-
occurrence frequencies. Section 4 presents our experi-
mental evaluation, which is followed by a discussion of
the results in section 5.
</bodyText>
<sectionHeader confidence="0.936584" genericHeader="method">
2 Measuring Word Similarity
</sectionHeader>
<bodyText confidence="0.999869133333333">
The notion for co-occurrence of two words can depicted
by a contingency table, as shown in table 1. Each dimen-
sion represents a random discrete variable with range
(presence or absence of wordin a given
text window or document). Each cell in the table repre-
sent the joint frequency , where
is the maximum number of co-occurrences. Un-
der an independence assumption, the values of the cells
in the contingency table are calculated using the prob-
abilities in table 2. The methods described below per-
form different measures of how distant observed values
are from expected values under an independence assump-
tion. Tan et al. (2002) indicate that the difference between
the methods arise from non-uniform marginals and how
the methods react to this non-uniformity.
</bodyText>
<tableCaption confidence="0.999978">
Table 1: Contingency table
Table 2: Probabilities under independence
</tableCaption>
<bodyText confidence="0.9873644">
Occasionally, a context is available and can pro-
vide support for the co-occurrence and alternative meth-
ods can be used to exploit this context. The procedures to
estimate , as well , will be described in
section 3.
</bodyText>
<subsectionHeader confidence="0.99724">
2.1 Similarity between two words
</subsectionHeader>
<bodyText confidence="0.9999825">
We first present methods to measure the similarity be-
tween two words and when no context is available.
</bodyText>
<subsectionHeader confidence="0.885755">
2.1.1 Pointwise Mutual Information
</subsectionHeader>
<bodyText confidence="0.9996878">
This measure for word similarity was first used in this
context by Church and Hanks (1990). The measure is
given by equation 1 and is called Pointwise Mutual Infor-
mation. It is a straightforward transformation of the inde-
pendence assumption (on a specific point),
, into a ratio. Positive values indicate that
words occur together more than would be expected under
an independence assumption. Negative values indicate
that one word tends to appear only when the other does
not. Values close to zero indicate independence.
</bodyText>
<subsectionHeader confidence="0.830214">
2.1.2 -test
</subsectionHeader>
<bodyText confidence="0.999660714285714">
This test is directly derived from observed and ex-
pected values in the contingency tables.
Thestatistic determines a specific way to calculate
the difference between values expected under indepen-
dence and observed ones, as depicted in equation 2. The
values correspond to the observed frequency esti-
mates.
</bodyText>
<subsectionHeader confidence="0.674145">
2.1.3 Likelihood ratio
</subsectionHeader>
<bodyText confidence="0.9995115">
The likelihood ratio test provides an alternative to
check two simple hypotheses based on parameters of a
distribution. Dunning (1993) used a likelihood ratio to
test word similarity under the assumption that the words
in text have a binomial distribution.
Two hypotheses used are: H1:
(i.e. they occur independently); and
H2: (i.e. not independent).
These two conditionals are used as sample in the like-
lihood function , where
in this particular case represents the parameter of
the binomial distribution . Under hypothe-
sis H1, , and for H2,
.
Equation 3 represents the likelihood ratio. Asymptoti-
cally, isdistributed.
</bodyText>
<subsectionHeader confidence="0.958745">
2.1.4 Average Mutual Information
</subsectionHeader>
<bodyText confidence="0.99995025">
This measure corresponds to the expected value of two
random variables using the same equation as PMI. Av-
erage mutual information was used as a word similarity
measure by Rosenfeld (1996) and is given by equation 4.
</bodyText>
<subsectionHeader confidence="0.997232">
2.2 Context supported similarity
</subsectionHeader>
<bodyText confidence="0.99454375">
Similarity between two words can also be in-
ferred from a context (if given). Given a context
, and are related if their
co-occurrence with words in context are similar.
</bodyText>
<subsectionHeader confidence="0.930542">
2.2.1 Cosine of Pointwise Mutual Information
</subsectionHeader>
<bodyText confidence="0.999959461538461">
The PMI between each context word and form a
vector. The elements in the vector represents the similar-
ity weights of and . The cosine value between the
two vectors corresponding to and represents the
similarity between the two words in the specified context,
as depicted in equation 5.
Values closer to one indicate more similarity whereas
values close to zero represent less similarity. Lesk (1969)
was one of the first to apply the cosine measure to word
similarity, but did not use pointwise mutual information
to compute the weights. Pantel (2002) used the cosine
of pointwise mutual information to uncover word sense
from text.
</bodyText>
<subsectionHeader confidence="0.40717">
2.2.2 norm
</subsectionHeader>
<bodyText confidence="0.9996362">
In this method the conditional probability of each word
in given (and ) is computed. The accumu-
lated distance between the conditionals for all words in
context represents the similarity between the two words,
as shown in equation 6. This method was proposed as an
alternative word similarity measure in language modeling
to overcome zero-frequency problems of bigrams (Dagan
et al., 1999).
In this measure, a smaller value indicates a greater sim-
ilarity.
</bodyText>
<subsectionHeader confidence="0.639593">
2.2.3 Contextual Average Mutual Information
</subsectionHeader>
<bodyText confidence="0.9930008">
The conditional probabilities between each word in
the context and the two words and are used
to calculate the mutual information of the conditionals
(equation 7). This method was also used in Dagan et.
al. (1999).
</bodyText>
<subsubsectionHeader confidence="0.535292">
2.2.4 Contextual Jensen-Shannon Divergence
</subsubsectionHeader>
<bodyText confidence="0.95129672">
This is an alternative to the Mutual Information for-
mula (equation 8). It helps to avoid zero frequency prob-
lem by averaging the two distributions and also provides
a symmetric measure (AMIC is not symmetric). This
method was also used in Dagan et. al. (1999).
2.2.5 Pointwise Mutual Information of Multiple
words
Turney (2001) proposes a different formula for Point-
wise Mutual Information when context is available, as de-
picted in equation 9. The context is represented by ,
which is any subset of the context . In fact, Turney ar-
gued that bigger sets are worse because they narrow
the estimate and as consequence can be affected by noise.
As a consequence, Turney used only one wordfrom
the context, discarding the remaining words. The chosen
word was the one that has biggest pointwise information
with . Moreover, ( ) is fixed when the method
is used to find the best for , so is also
fixed and can be ignored, which transforms the equation
into the conditional .
It is interesting to note that the equation
is not the traditional n-gram model since no ordering is
imposed on the words and also due to the fact that the
words in this formula can be separated from one another
by other words.
</bodyText>
<listItem confidence="0.2100245">
(9)
2.2.6 Other measures of word similarities
</listItem>
<bodyText confidence="0.997535333333333">
Many other measures for word similarities exists. Tan
et al. (2002) present a comparative study with 21 different
measures. Lillian (2001) proposes a new word similarity
measure in the context of language modeling, performing
an comparative evaluation with other 7 similarity mea-
sures.
</bodyText>
<sectionHeader confidence="0.998956" genericHeader="method">
3 Co-occurrence Estimates
</sectionHeader>
<bodyText confidence="0.999916333333334">
We now discuss some alternatives to estimate word co-
occurrence frequencies from an available corpus. All
probabilities mentioned in previous section can be es-
timated from these frequencies. We describe two dif-
ferent approaches: a window-oriented approach and a
document-oriented approach.
</bodyText>
<subsectionHeader confidence="0.992992">
3.1 Window-oriented approach
</subsectionHeader>
<bodyText confidence="0.998408586206897">
Letbe the frequency of and the co-occurrence fre-
quency of and be denoted by . Let be
the size of the corpus in words. In the window-oriented
approach, individual word frequencies are the corpus fre-
quencies. The maximum likelihood estimate (MLE) for
in the corpus is .
The joint frequency is estimated by the number
of windows where the two words co-occur. The window
size may vary, Church and Hanks (1990) used windows
of size 2 and 5. Brown et al. (1992) used windows con-
taining 1001 words. Dunning (1993) also used windows
of size 2, which corresponds to word bigrams. Let the
number of windows of sizein the corpus be . Recall
that is the maximum number of co-occurrences,
i.e. in the windows-oriented approach.
The MLE of the co-occurrence probability is given by
.
In most common case, windows are overlapping, and
in this case . The total frequency of win-
dows for co-occurrence should be adjusted to reflect the
multiple counts of the same co-occurrence. One method
to account for overlap is to divide the total count of win-
dows by . This method also reinforces
closer co-occurrences by assigning them a larger weight.
Smoothing techniques can be applied to address the
zero-frequencyproblem, or alternatively, the window size
can be increased, which also increases the chance of co-
occurrence. To avoid inconsistency, windows do not to
cross document boundaries.
</bodyText>
<subsectionHeader confidence="0.995532">
3.2 Document-oriented approach
</subsectionHeader>
<bodyText confidence="0.99982605">
In information retrieval, one commonly uses document
statistics rather than individual word statistics. In an
document-oriented approach, the frequency of a word
is denoted byand corresponds to the number of doc-
uments in which the word appears, regardless of how fre-
quently it occurs in each document. The number of docu-
ments is denoted by . The MLE for an individual word
in document oriented approach is .
The co-occurrence frequency of two words and
, denoted by , is the number of documents
where the words co-occur. If we require only that the
words co-occur in the same document, no distinction is
made between distantly occurring words and adjacent
words. This distortion can be reduced by imposing a
maximal distance for co-occurrence, (i.e. a fixed-sized
window), but the frequency will still be the number of
documents where the two words co-occur within this dis-
tance. The MLE for the co-occurrence in this approach
is , since in the
document-oriented approach.
</bodyText>
<subsectionHeader confidence="0.999381">
3.3 Syntax based approach
</subsectionHeader>
<bodyText confidence="0.999644761904762">
An alternative to the Window and Document-oriented ap-
proach is to use syntactical information (Grefenstette,
1993). For this purpose, a Parser or Part-Of-Speech tag-
ger must be applied to the text and only the interesting
pairs of words in correct syntactical categories used. In
this case, the fixed window can be superseded by the re-
sult of the syntax analysis or tagging process and the fre-
quency of the pairs can be used directly. Alternatively,
the number of documents that contain the pair can also
be used. However, the nature of the language tests in this
work make it impractical to be applied. First, the alter-
natives are not in a context, and as such can have more
than one part-of-speech tag. Occasionally, it is possible
to infer that the syntactic category of the alternatives from
context of the target word , if there is such a context
. When the alternatives, or the target word , are mul-
tiwords then the problem is harder, as depicted in the first
example of figure 7. Also, both parsers and POS tagger
make mistakes, thus introducing error. Finally, the size
of the corpus used and its nature intensify the parser/POS
taggers problems.
</bodyText>
<figureCaption confidence="0.999947">
Figure 3: Results for TOEFL test set Figure 4: Impact of corpus size on TOEFL test set
Figure 5: Results for TS1 and no context Figure 6: Results for TS1 and context
</figureCaption>
<sectionHeader confidence="0.998225" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999492921875">
We evaluate the methods and frequency estimates using 3
test sets. The first test set is a set of TOEFL questions first
used by Landauer and Dumais (1997) and also by Tur-
ney (2001). This test set contains 80 synonym questions
and for each question one and four alternative op-
tions ( ) are given. The other two test sets, which
we will refer to as TS1 and TS2, are practice questions
for the TOEFL. These two test sets also contain four al-
ternatives options, , and is given in context
(within a sentence). TS1 has 50 questions and was also
used by Turney (2001). TS2 has 60 questions extracted
from a TOEFL practice guide (King and Stanley, 1989).
For all test sets the answer to each question is known and
unique. For comparison purposes, we also use TS1 and
TS2 with no context.
For the three test sets, TOEFL, TS1 and TS2 without
context, we applied the word and document-oriented fre-
quency estimates presented. We investigated a variety of
window sizes, varying the window size from 2 to 256 by
powers of 2.
The labels used in figures 3, 5, 6, 8, 9, 10, 12 are com-
posed from a keyword indicating the frequency estimate
used (W-window oriented; and DR-document retrieval
oriented) and a keyword indicating the word similarity
measure. For no-context measures the keywords are:
PMI-Pointwise Mutual Information; CHI-Chi-Squared;
MI-Average mutual information; and LL-Log-likelihood.
For the measures with context: CP-Cosine pointwise mu-
tual information; L1-L1 norm; AMIC-Average Mutual
Information in the presence of context; IRAD-Jensen-
Shannon Divergence; and PMIC- - Pointwise Mutual
Information with words of context.
For TS1 and TS2 with context, we also investigate Tur-
ney’s hypothesis that the outcome of adding more words
from is negative, using DR-PMIC. The result of this
experiment is shown in figures 10 and 12 for TS1 and
TS2 respectively.
It is important to note that in some of the questions,
or one or more of the ’s are multi-word strings.
For these questions, we assume that the strings may be
treated as collocations and use them “as is”, adjusting the
size of the windows by the collocation size when appli-
cable.
The corpus used for the experiments is a terabyte of
Web data crawled from the general web in 2001. In order
to balance the contents of the corpus, a breadth-first order
search was used from a initial seed set of URLs represent-
ing the home page of 2392 universities and other educa-
tional organizations (Clarke et al., 2002). No duplicate
pages are included in the collection and the crawler also
did not allow a large number of pages from the same site
to be downloaded simultaneously. Overall, the collection
contains 53 billion words and 77 million documents.
A key characteristic of this corpus is that it consists of
HTML files. These files have a focus on the presentation,
and not necessarily on the style of writing. Parsing or
tagging these files can be a hard process and prone to in-
troduction of error in rates bigger than traditional corpora
used in NLP or Information Retrieval.
We also investigate the impact of the collection size on
=“The country is plagued by [turmoil].”
= ‘constant change’,‘utter confusion’,‘bad weather’,‘fuel shortages’
= “[For] all their protestations, they heeded the judge’s ruling.”
= ‘In spite of’,‘Because of’,‘On behalf of’,‘without’
</bodyText>
<figureCaption confidence="0.9998105">
Figure 7: Examples of harder questions in TS2
Figure 8: Results for TS2 and no context Figure 9: Results for TS2 and context
</figureCaption>
<bodyText confidence="0.983124">
the results, as depicted in figures 4, 11 and 13 for TOEFL,
TS1 and TS2 test sets, respectively.
</bodyText>
<sectionHeader confidence="0.998614" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999437507246377">
The results for the TOEFL questions are presented in
figure 3. The best performance found is 81.25% of the
questions correctly answered. That result used DR-PMI
with a window size of 16-32 words. This is an im-
provement over the results presented by Landauer and
Dumais (1997) using Latent Semantic Analysis, where
64.5% of the questions were answered correctly, and Tur-
ney (2001), using pointwise mutual information and doc-
ument retrieval, where the best result was 73.75%.
Although we use a similar method (DR-PMI), the dif-
ference between the results presented here and Turney’s
results may be due to differences in the corpora and dif-
ferences in the queries. Turney uses Altavista and we
used our own crawl of web data. We can not compare
the collections since we do not know how Altavista col-
lection is created. As for the queries, we have more con-
trol over the queries since we can precisely specify the
window size and we also do not know how queries are
evaluated in Altavista.
PMI performs best overall, regardless of estimates used
(DR or W). W-CHI performs up to 80% when using win-
dow estimates, outperforming DR-CHI. MI and LL yield
exactly the same results (and the same ranking of the al-
ternatives), which suggests that the binomial distribution
is a good approximation for word occurrence in text.
The results for MI and PMI indicate that, for the two
discrete random variables and (and range
), no further gain is achieved by calculating the
expectation in the divergence. Recall that the divergence
formula has an embedded expectation to be calculated be-
tween the joint probability of these two random variables
and their independence. The peak of information is ex-
actly where both words co-occur, i.e. when
and , and not any of the other three possible
combinations.
Similar trends are seen when using TS1 and no con-
text, as depicted in figure 5. PMI is best overall, and DR-
PMI and W-PMI outperform each other with different
windows sizes. W-CHI has good performance in small
windows sizes. MI and LL yield identical (poor) results,
being worst than chance for some window sizes. Tur-
ney (2001) also uses this test set without context, achiev-
ing 66% peak performance compared with our best per-
formance of 72% (DR-PMI).
In the test set TS2 with no context, the trend seen be-
tween TOEFL and TS1 is repeated, as shown in figure 8.
PMI is best overall but W-CHI performs better than PMI
in three cases. DR-CHI performs poorly for small win-
dows sizes. MI and LL also perform poorly in compari-
son with PMI. The peak performance is 75%, using DR-
PMI with a window size of 64.
The result are not what we expected when context is
used in TS1 and TS2. In TS1, figure 6, only one of
the measures, DR-PMIC-1, outperforms the results from
non-context measures, having a peak of 80% correct an-
swers. The condition for the best result (one word from
context and a window size of 8) is similar to the one
used for the best score reported by Turney. L1, AMIC
and IRAD perform poorly, worst than chance for some
window sizes. One difference in the results is that for
DR-PMIC-1 only the best word from context was used,
while the other methods used all words but stopwords.
We examine the context and discovered that using more
words degrades the performance of DR-PMIC in all dif-
ferent windows sizes but, even using all words except
stopwords, the result from DR-PMIC is better than any
other contextual measure - 76% correct answers in TS1
(with DR-PMIC and a window size of 8).
For TS2, no measure using context was able to perform
</bodyText>
<figureCaption confidence="0.9968675">
Figure 10: Influence from the context on TS1 Figure 11: Impact of corpus size on TS1
Figure 12: Influence from the context on TS2 Figure 13: Impact of corpus size on TS2
</figureCaption>
<bodyText confidence="0.999907583333333">
better than the non-contextual measures. DR-PMIC-1
performs better overall but has worse performance than
DR-CP with a window size of 8. In this test set, the per-
formance of DR-CP is better than W-CP. L1 performs
better than AMIC but both have poor results, IRAD is
never better than chance. The context in TS2 has more
words than TS1 but the questions seem to be harder, as
shown in figure 7. In some of the TS2 questions, the tar-
get word or one of the alternatives uses functional words.
We also investigate the influence of more words from
context in TS2, as depicted in figure 12, where the trends
seen with TS1 are repeated.
The results in TS1 and TS2 suggest that the available
context is not very useful or that it is not being used prop-
erly.
Finally, we selected the method that yields the best
performance for each test set to analyze the impact
of the corpus size on performance, as shown in fig-
ures 4, 11 and 13. For TS1 we use W-PMI with a win-
dow size of 2 (W-PMI2) when no context is used and
DR-PMIC-1 with a window size of 8 (DR-PMIC8-1)
when context is used. For those measures, very little im-
provement is noticed after 500 GBytes for DR-PMIC8-1,
roughly half of the collection size. No apparent improve-
ment is achieved after 300-400 GBytes for W-PMI2. For
TS2 we use DR-PMI with a window size of 64 (DR-
PMI64) when no context is used, and DR-PMIC-1 with
a windows size of 64 (DR-PMIC64-1) when context is
used. It is clear that for TS2 no substantial improve-
ment in DR-PMI64 and DR-PMIC64-1 is achieved by
increasing the corpus size to values bigger than 300-400
GBytes. The most interesting impact of corpus size was
on TOEFL test set using DR-PMI with a window size of
16 (DR-PMI16). Using the full corpus is no better than
using 5% of the corpus, and the best result, 82.5% correct
answers, is achieved when using 85-95% of corpus size.
</bodyText>
<sectionHeader confidence="0.99936" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999275416666667">
Using a large corpus and human-oriented tests we de-
scribe a comprehensive study of word similarity mea-
sures and co-occurrence estimates, including variants on
corpus size. Without any parameter training, we were
able to correctly answer at least 75% questions in all test
sets. From all combinations of estimates and measures,
document retrieval with a maximum window of 16 words
and pointwise mutual information performs best on aver-
age in the three test sets used. However, both document or
windows-oriented approach for frequency estimates pro-
duce similar results in average. The impact of the corpus
size is not very conclusive, it suggests that the increase
in the corpus size normally reaches an asymptote, but the
points where this occurs is distinct among different mea-
sures and frequency estimates.
Our results outperform the previously reported results
on test sets when no context is used, being able to cor-
rectly answer 81.25% of TOEFL synonym questions,
compared with a previous best result of 73.5%. A hu-
man average score on the same type of questions is
64.5% (Landauer and Dumais, 1997). We also perform
better than previous work on another test set used as prac-
tice questions for TOEFL, obtaining 80% correct answers
compared to a best result of 74% from previous work.
</bodyText>
<sectionHeader confidence="0.996585" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992361">
This work was made possible also in part by PUC/RS and
Ministry of Education of Brazil through CAPES agency.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903930555556">
P. F. Brown, P. V. deSouza, R. L. Mercer, T. J. Watson,
V. J. Della Pietra, and J. C. Lai. 1992. Class-based n-
gram models of natural language. Computational Lin-
guistics, 18:467–479.
C. Buckley, G. Salton, J. Allan, and A. Singhal. 1995.
Automatic query expansion using smart: Trec 3. In
The third Text REtrieval Conference, Gaithersburg,
MD.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22–29.
C.L.A. Clarke, G.V. Cormack, M. Laszlo, T.R. Lynam,
and E.L. Terra. 2002. The impact of corpus size on
question answering performance. In Proceedings of
2002 SIGIR conference, Tampere, Finland.
I. Dagan, L. Lee, and F. C. N. Pereira. 1999. Similarity-
based models of word cooccurrence probabilities. Ma-
chine Learning, 34(1-3):43–69.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19:61–74.
G. Grefenstette. 1993. Automatic theasurus generation
from raw text using knowledge-poor techniques. In
Making sense of Words. 9th Annual Conference of the
UW Centre for the New OED and text Research.
D. Harman. 1992. Relevance feedback revisited. In
Proceedings of 1992 SIGIR conference, Copenhagen,
Denmark.
C. King and N. Stanley. 1989. Building Skills for the
TOEFL. Thomas Nelson and Sons Ltd, second edition.
T. K. Landauer and S. T. Dumais. 1997. A solution
to plato’s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211–240.
Lillian Lee. 2001. On the effectiveness of the skew di-
vergence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65–72.
M. E. Lesk. 1969. Word-word associations in doc-
ument retrieval systems. American Documentation,
20(1):27–38, January.
Hang Li and Naoki Abe. 1998. Word clustering and dis-
ambiguation based on co-occurence data. In COLING-
ACL, pages 749–755.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL, pages 768–774.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining, pages
613–619.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. computer
speech and language. Computer Speech and Lan-
guage, 10:187–228.
P.-N. Tan, V. Kumar, and J. Srivastava. 2002. Selecting
the right interestingness measure for association pat-
terns. In Proceedings ofACMSIGKDD Conference on
Knowledge Discovery and Data Mining, pages 32–41.
P. D. Turney. 2001. Mining the Web for synonyms:
PMI–IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML-2001), pages 491–502.
O. Vechtomova and S. Robertson. 2000. Integration
of collocation statistics into the probabilistic retrieval
model. In 22nd Annual Colloquium on Information
Retrieval Research, Cambridge, England.
J. Xu and B. Croft. 2000. Improving the effectiveness of
information retrieval. ACM Transactions on Informa-
tion Systems, 18(1):79–112.
David Yarowsky. 1992. Word-sense disambiguation us-
ing statistical models of Roget’s categories trained on
large corpora. In Proceedings of COLING-92, pages
454–460, Nantes, France, July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235375">
<title confidence="0.785708">Frequency Estimates for Statistical Word Similarity Measures</title>
<note confidence="0.842468">Proceedings of HLT-NAACL 2003 Main Papers , pp. 165-172 Edmonton, May-June 2003</note>
<author confidence="0.794206">Egidio</author>
<affiliation confidence="0.998358">School of Computer University of</affiliation>
<email confidence="0.943773">elterra@math.uwaterloo.ca</email>
<author confidence="0.824447">C L A</author>
<affiliation confidence="0.998732">School of Computer University of</affiliation>
<email confidence="0.859244">claclark@plg2.uwaterloo.ca</email>
<abstract confidence="0.999157818181818">Statistical measures of word similarity have application in many areas of natural language processing, such as language modeling and information retrieval. We report a comparative study of two methods for estimating word cooccurrence frequencies required by word similarity measures. Our frequency estimates are generated from a terabyte-sized corpus of Web data, and we study the impact of corpus size on the effectiveness of the measures. We base the evaluation on one TOEFL question set and two practice questions sets, each consisting of a number of multiple choice questions seeking the best synonym for a given target word. For two question sets, a context for the target word is provided, and we examine a number of word similarity measures that exploit this context. Our best combination of similarity measure and frequency estimation method answers 6-8% more questions than the best results previously reported for the same question sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>T J Watson</author>
<author>V J Della Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based ngram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="3170" citStr="Brown et al. (1992)" startWordPosition="499" endWordPosition="502">the automatic creation of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin, 1998) and word sense disambiguation (Yarowsky, 1992; Li and Abe, 1998). Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text. Recently, Tan et al. (2002) provide an analysis on different measures of independence in the context of association rules. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes. Dagan et al. (1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data. In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al., 1995; Xu and Croft, 2000; Vechtomova and Robertson, 2000). Xu and Croft (2000) expand queries under a pseudo-relevance feedback model by using similar words from documents retrieved and impr</context>
<context position="13272" citStr="Brown et al. (1992)" startWordPosition="2137" endWordPosition="2140">e estimated from these frequencies. We describe two different approaches: a window-oriented approach and a document-oriented approach. 3.1 Window-oriented approach Letbe the frequency of and the co-occurrence frequency of and be denoted by . Let be the size of the corpus in words. In the window-oriented approach, individual word frequencies are the corpus frequencies. The maximum likelihood estimate (MLE) for in the corpus is . The joint frequency is estimated by the number of windows where the two words co-occur. The window size may vary, Church and Hanks (1990) used windows of size 2 and 5. Brown et al. (1992) used windows containing 1001 words. Dunning (1993) also used windows of size 2, which corresponds to word bigrams. Let the number of windows of sizein the corpus be . Recall that is the maximum number of co-occurrences, i.e. in the windows-oriented approach. The MLE of the co-occurrence probability is given by . In most common case, windows are overlapping, and in this case . The total frequency of windows for co-occurrence should be adjusted to reflect the multiple counts of the same co-occurrence. One method to account for overlap is to divide the total count of windows by . This method als</context>
</contexts>
<marker>Brown, deSouza, Mercer, Watson, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. deSouza, R. L. Mercer, T. J. Watson, V. J. Della Pietra, and J. C. Lai. 1992. Class-based ngram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
<author>G Salton</author>
<author>J Allan</author>
<author>A Singhal</author>
</authors>
<title>Automatic query expansion using smart: Trec 3.</title>
<date>1995</date>
<booktitle>In The third Text REtrieval Conference,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="3584" citStr="Buckley et al., 1995" startWordPosition="563" endWordPosition="566">larity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes. Dagan et al. (1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data. In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al., 1995; Xu and Croft, 2000; Vechtomova and Robertson, 2000). Xu and Croft (2000) expand queries under a pseudo-relevance feedback model by using similar words from documents retrieved and improve effectiveness by more than 20% on an 11-point average precision. Landauer and Dumais (1997) applied word similarity measures to answer TOEFL (Test Of English as a Foreign Language) synonym questions using Latent Semantic Analysis. Turney (2001) performed an evaluation of a specific word similarity measure using the same TOEFL questions and compared the results with those obtained _ “The results of the test </context>
</contexts>
<marker>Buckley, Salton, Allan, Singhal, 1995</marker>
<rawString>C. Buckley, G. Salton, J. Allan, and A. Singhal. 1995. Automatic query expansion using smart: Trec 3. In The third Text REtrieval Conference, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="1474" citStr="Church and Hanks, 1990" startWordPosition="224" endWordPosition="227">s, each consisting of a number of multiple choice questions seeking the best synonym for a given target word. For two question sets, a context for the target word is provided, and we examine a number of word similarity measures that exploit this context. Our best combination of similarity measure and frequency estimation method answers 6-8% more questions than the best results previously reported for the same question sets. 1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts (Dunning, 1993; Church and Hanks, 1990; Dagan et al., 1999). These tests attempt to measure dependence between words by using statistics taken from a large corpus. In this context, a key assumption is that similarity between words is a consequence of word co-occurrence, or that the closeness of the words in text is indicative of some kind of relationship between them, such as synonymy or antonymy. Although word sequences in natural language are unlikely to be independent, these statistical tests provide quantitative information that can be used to compare pairs of co-occurring words. Also, despite the fact that word co-occurrence </context>
<context position="7655" citStr="Church and Hanks (1990)" startWordPosition="1221" endWordPosition="1224"> from non-uniform marginals and how the methods react to this non-uniformity. Table 1: Contingency table Table 2: Probabilities under independence Occasionally, a context is available and can provide support for the co-occurrence and alternative methods can be used to exploit this context. The procedures to estimate , as well , will be described in section 3. 2.1 Similarity between two words We first present methods to measure the similarity between two words and when no context is available. 2.1.1 Pointwise Mutual Information This measure for word similarity was first used in this context by Church and Hanks (1990). The measure is given by equation 1 and is called Pointwise Mutual Information. It is a straightforward transformation of the independence assumption (on a specific point), , into a ratio. Positive values indicate that words occur together more than would be expected under an independence assumption. Negative values indicate that one word tends to appear only when the other does not. Values close to zero indicate independence. 2.1.2 -test This test is directly derived from observed and expected values in the contingency tables. Thestatistic determines a specific way to calculate the differenc</context>
<context position="13222" citStr="Church and Hanks (1990)" startWordPosition="2126" endWordPosition="2129"> All probabilities mentioned in previous section can be estimated from these frequencies. We describe two different approaches: a window-oriented approach and a document-oriented approach. 3.1 Window-oriented approach Letbe the frequency of and the co-occurrence frequency of and be denoted by . Let be the size of the corpus in words. In the window-oriented approach, individual word frequencies are the corpus frequencies. The maximum likelihood estimate (MLE) for in the corpus is . The joint frequency is estimated by the number of windows where the two words co-occur. The window size may vary, Church and Hanks (1990) used windows of size 2 and 5. Brown et al. (1992) used windows containing 1001 words. Dunning (1993) also used windows of size 2, which corresponds to word bigrams. Let the number of windows of sizein the corpus be . Recall that is the maximum number of co-occurrences, i.e. in the windows-oriented approach. The MLE of the co-occurrence probability is given by . In most common case, windows are overlapping, and in this case . The total frequency of windows for co-occurrence should be adjusted to reflect the multiple counts of the same co-occurrence. One method to account for overlap is to divi</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K.W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L A Clarke</author>
<author>G V Cormack</author>
<author>M Laszlo</author>
<author>T R Lynam</author>
<author>E L Terra</author>
</authors>
<title>The impact of corpus size on question answering performance.</title>
<date>2002</date>
<booktitle>In Proceedings of 2002 SIGIR conference,</booktitle>
<location>Tampere, Finland.</location>
<contexts>
<context position="19006" citStr="Clarke et al., 2002" startWordPosition="3108" endWordPosition="3111">S2 respectively. It is important to note that in some of the questions, or one or more of the ’s are multi-word strings. For these questions, we assume that the strings may be treated as collocations and use them “as is”, adjusting the size of the windows by the collocation size when applicable. The corpus used for the experiments is a terabyte of Web data crawled from the general web in 2001. In order to balance the contents of the corpus, a breadth-first order search was used from a initial seed set of URLs representing the home page of 2392 universities and other educational organizations (Clarke et al., 2002). No duplicate pages are included in the collection and the crawler also did not allow a large number of pages from the same site to be downloaded simultaneously. Overall, the collection contains 53 billion words and 77 million documents. A key characteristic of this corpus is that it consists of HTML files. These files have a focus on the presentation, and not necessarily on the style of writing. Parsing or tagging these files can be a hard process and prone to introduction of error in rates bigger than traditional corpora used in NLP or Information Retrieval. We also investigate the impact o</context>
</contexts>
<marker>Clarke, Cormack, Laszlo, Lynam, Terra, 2002</marker>
<rawString>C.L.A. Clarke, G.V. Cormack, M. Laszlo, T.R. Lynam, and E.L. Terra. 2002. The impact of corpus size on question answering performance. In Proceedings of 2002 SIGIR conference, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F C N Pereira</author>
</authors>
<title>Similaritybased models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1495" citStr="Dagan et al., 1999" startWordPosition="228" endWordPosition="231">number of multiple choice questions seeking the best synonym for a given target word. For two question sets, a context for the target word is provided, and we examine a number of word similarity measures that exploit this context. Our best combination of similarity measure and frequency estimation method answers 6-8% more questions than the best results previously reported for the same question sets. 1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts (Dunning, 1993; Church and Hanks, 1990; Dagan et al., 1999). These tests attempt to measure dependence between words by using statistics taken from a large corpus. In this context, a key assumption is that similarity between words is a consequence of word co-occurrence, or that the closeness of the words in text is indicative of some kind of relationship between them, such as synonymy or antonymy. Although word sequences in natural language are unlikely to be independent, these statistical tests provide quantitative information that can be used to compare pairs of co-occurring words. Also, despite the fact that word co-occurrence is a simple idea, the</context>
<context position="3305" citStr="Dagan et al. (1999)" startWordPosition="520" endWordPosition="523">and Abe, 1998). Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text. Recently, Tan et al. (2002) provide an analysis on different measures of independence in the context of association rules. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes. Dagan et al. (1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data. In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al., 1995; Xu and Croft, 2000; Vechtomova and Robertson, 2000). Xu and Croft (2000) expand queries under a pseudo-relevance feedback model by using similar words from documents retrieved and improve effectiveness by more than 20% on an 11-point average precision. Landauer and Dumais (1997) applied word similarity measures to ans</context>
<context position="10576" citStr="Dagan et al., 1999" startWordPosition="1690" endWordPosition="1693">e first to apply the cosine measure to word similarity, but did not use pointwise mutual information to compute the weights. Pantel (2002) used the cosine of pointwise mutual information to uncover word sense from text. 2.2.2 norm In this method the conditional probability of each word in given (and ) is computed. The accumulated distance between the conditionals for all words in context represents the similarity between the two words, as shown in equation 6. This method was proposed as an alternative word similarity measure in language modeling to overcome zero-frequency problems of bigrams (Dagan et al., 1999). In this measure, a smaller value indicates a greater similarity. 2.2.3 Contextual Average Mutual Information The conditional probabilities between each word in the context and the two words and are used to calculate the mutual information of the conditionals (equation 7). This method was also used in Dagan et. al. (1999). 2.2.4 Contextual Jensen-Shannon Divergence This is an alternative to the Mutual Information formula (equation 8). It helps to avoid zero frequency problem by averaging the two distributions and also provides a symmetric measure (AMIC is not symmetric). This method was also </context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>I. Dagan, L. Lee, and F. C. N. Pereira. 1999. Similaritybased models of word cooccurrence probabilities. Machine Learning, 34(1-3):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--61</pages>
<contexts>
<context position="1450" citStr="Dunning, 1993" startWordPosition="222" endWordPosition="223">e questions sets, each consisting of a number of multiple choice questions seeking the best synonym for a given target word. For two question sets, a context for the target word is provided, and we examine a number of word similarity measures that exploit this context. Our best combination of similarity measure and frequency estimation method answers 6-8% more questions than the best results previously reported for the same question sets. 1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts (Dunning, 1993; Church and Hanks, 1990; Dagan et al., 1999). These tests attempt to measure dependence between words by using statistics taken from a large corpus. In this context, a key assumption is that similarity between words is a consequence of word co-occurrence, or that the closeness of the words in text is indicative of some kind of relationship between them, such as synonymy or antonymy. Although word sequences in natural language are unlikely to be independent, these statistical tests provide quantitative information that can be used to compare pairs of co-occurring words. Also, despite the fact </context>
<context position="8562" citStr="Dunning (1993)" startWordPosition="1364" endWordPosition="1365">sumption. Negative values indicate that one word tends to appear only when the other does not. Values close to zero indicate independence. 2.1.2 -test This test is directly derived from observed and expected values in the contingency tables. Thestatistic determines a specific way to calculate the difference between values expected under independence and observed ones, as depicted in equation 2. The values correspond to the observed frequency estimates. 2.1.3 Likelihood ratio The likelihood ratio test provides an alternative to check two simple hypotheses based on parameters of a distribution. Dunning (1993) used a likelihood ratio to test word similarity under the assumption that the words in text have a binomial distribution. Two hypotheses used are: H1: (i.e. they occur independently); and H2: (i.e. not independent). These two conditionals are used as sample in the likelihood function , where in this particular case represents the parameter of the binomial distribution . Under hypothesis H1, , and for H2, . Equation 3 represents the likelihood ratio. Asymptotically, isdistributed. 2.1.4 Average Mutual Information This measure corresponds to the expected value of two random variables using the </context>
<context position="13323" citStr="Dunning (1993)" startWordPosition="2147" endWordPosition="2148">erent approaches: a window-oriented approach and a document-oriented approach. 3.1 Window-oriented approach Letbe the frequency of and the co-occurrence frequency of and be denoted by . Let be the size of the corpus in words. In the window-oriented approach, individual word frequencies are the corpus frequencies. The maximum likelihood estimate (MLE) for in the corpus is . The joint frequency is estimated by the number of windows where the two words co-occur. The window size may vary, Church and Hanks (1990) used windows of size 2 and 5. Brown et al. (1992) used windows containing 1001 words. Dunning (1993) also used windows of size 2, which corresponds to word bigrams. Let the number of windows of sizein the corpus be . Recall that is the maximum number of co-occurrences, i.e. in the windows-oriented approach. The MLE of the co-occurrence probability is given by . In most common case, windows are overlapping, and in this case . The total frequency of windows for co-occurrence should be adjusted to reflect the multiple counts of the same co-occurrence. One method to account for overlap is to divide the total count of windows by . This method also reinforces closer co-occurrences by assigning the</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19:61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Automatic theasurus generation from raw text using knowledge-poor techniques.</title>
<date>1993</date>
<booktitle>In Making sense of Words. 9th Annual Conference of the UW Centre for the New OED and text Research.</booktitle>
<contexts>
<context position="2605" citStr="Grefenstette, 1993" startWordPosition="405" endWordPosition="406">to compare pairs of co-occurring words. Also, despite the fact that word co-occurrence is a simple idea, there are a variety of ways to estimate word co-occurrence frequencies from text. Two words can appear close to each other in the same document, passage, paragraph, sentence or fixed-size window. The boundaries for determining cooccurrence will affect the estimates and as a consequence the word similarity measures. Statistical word similarity measures play an important role in information retrieval and in many other natural language applications, such as the automatic creation of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin, 1998) and word sense disambiguation (Yarowsky, 1992; Li and Abe, 1998). Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text. Recently, Tan et al. (2002) provide an analysis on different measures of independence in the context of association rules. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for </context>
<context position="15330" citStr="Grefenstette, 1993" startWordPosition="2472" endWordPosition="2473"> where the words co-occur. If we require only that the words co-occur in the same document, no distinction is made between distantly occurring words and adjacent words. This distortion can be reduced by imposing a maximal distance for co-occurrence, (i.e. a fixed-sized window), but the frequency will still be the number of documents where the two words co-occur within this distance. The MLE for the co-occurrence in this approach is , since in the document-oriented approach. 3.3 Syntax based approach An alternative to the Window and Document-oriented approach is to use syntactical information (Grefenstette, 1993). For this purpose, a Parser or Part-Of-Speech tagger must be applied to the text and only the interesting pairs of words in correct syntactical categories used. In this case, the fixed window can be superseded by the result of the syntax analysis or tagging process and the frequency of the pairs can be used directly. Alternatively, the number of documents that contain the pair can also be used. However, the nature of the language tests in this work make it impractical to be applied. First, the alternatives are not in a context, and as such can have more than one part-of-speech tag. Occasional</context>
</contexts>
<marker>Grefenstette, 1993</marker>
<rawString>G. Grefenstette. 1993. Automatic theasurus generation from raw text using knowledge-poor techniques. In Making sense of Words. 9th Annual Conference of the UW Centre for the New OED and text Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harman</author>
</authors>
<title>Relevance feedback revisited.</title>
<date>1992</date>
<booktitle>In Proceedings of 1992 SIGIR conference,</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="3562" citStr="Harman, 1992" startWordPosition="561" endWordPosition="562">les. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes. Dagan et al. (1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data. In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al., 1995; Xu and Croft, 2000; Vechtomova and Robertson, 2000). Xu and Croft (2000) expand queries under a pseudo-relevance feedback model by using similar words from documents retrieved and improve effectiveness by more than 20% on an 11-point average precision. Landauer and Dumais (1997) applied word similarity measures to answer TOEFL (Test Of English as a Foreign Language) synonym questions using Latent Semantic Analysis. Turney (2001) performed an evaluation of a specific word similarity measure using the same TOEFL questions and compared the results with those obtained _ “Th</context>
</contexts>
<marker>Harman, 1992</marker>
<rawString>D. Harman. 1992. Relevance feedback revisited. In Proceedings of 1992 SIGIR conference, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C King</author>
<author>N Stanley</author>
</authors>
<title>Building Skills for the TOEFL. Thomas Nelson and Sons Ltd, second edition.</title>
<date>1989</date>
<contexts>
<context position="17195" citStr="King and Stanley, 1989" startWordPosition="2807" endWordPosition="2810">s and frequency estimates using 3 test sets. The first test set is a set of TOEFL questions first used by Landauer and Dumais (1997) and also by Turney (2001). This test set contains 80 synonym questions and for each question one and four alternative options ( ) are given. The other two test sets, which we will refer to as TS1 and TS2, are practice questions for the TOEFL. These two test sets also contain four alternatives options, , and is given in context (within a sentence). TS1 has 50 questions and was also used by Turney (2001). TS2 has 60 questions extracted from a TOEFL practice guide (King and Stanley, 1989). For all test sets the answer to each question is known and unique. For comparison purposes, we also use TS1 and TS2 with no context. For the three test sets, TOEFL, TS1 and TS2 without context, we applied the word and document-oriented frequency estimates presented. We investigated a variety of window sizes, varying the window size from 2 to 256 by powers of 2. The labels used in figures 3, 5, 6, 8, 9, 10, 12 are composed from a keyword indicating the frequency estimate used (W-window oriented; and DR-document retrieval oriented) and a keyword indicating the word similarity measure. For no-c</context>
</contexts>
<marker>King, Stanley, 1989</marker>
<rawString>C. King and N. Stanley. 1989. Building Skills for the TOEFL. Thomas Nelson and Sons Ltd, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="3865" citStr="Landauer and Dumais (1997)" startWordPosition="607" endWordPosition="610">ed model, grouping similar words into classes. Dagan et al. (1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data. In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al., 1995; Xu and Croft, 2000; Vechtomova and Robertson, 2000). Xu and Croft (2000) expand queries under a pseudo-relevance feedback model by using similar words from documents retrieved and improve effectiveness by more than 20% on an 11-point average precision. Landauer and Dumais (1997) applied word similarity measures to answer TOEFL (Test Of English as a Foreign Language) synonym questions using Latent Semantic Analysis. Turney (2001) performed an evaluation of a specific word similarity measure using the same TOEFL questions and compared the results with those obtained _ “The results of the test were quite [unambiguous].” _ ‘unambiguous’ _ ‘clear’,‘doubtful’,‘surprising’, ‘illegal’ Figure 1: Finding the best synonym option in presence of context _ ‘boast’ _ ‘brag’,‘yell’,‘complain’,‘explain’ Figure 2: Finding the best synonym by Landauer and Dumais. In our investigation o</context>
<context position="16704" citStr="Landauer and Dumais (1997)" startWordPosition="2716" endWordPosition="2719">alternatives, or the target word , are multiwords then the problem is harder, as depicted in the first example of figure 7. Also, both parsers and POS tagger make mistakes, thus introducing error. Finally, the size of the corpus used and its nature intensify the parser/POS taggers problems. Figure 3: Results for TOEFL test set Figure 4: Impact of corpus size on TOEFL test set Figure 5: Results for TS1 and no context Figure 6: Results for TS1 and context 4 Experiments We evaluate the methods and frequency estimates using 3 test sets. The first test set is a set of TOEFL questions first used by Landauer and Dumais (1997) and also by Turney (2001). This test set contains 80 synonym questions and for each question one and four alternative options ( ) are given. The other two test sets, which we will refer to as TS1 and TS2, are practice questions for the TOEFL. These two test sets also contain four alternatives options, , and is given in context (within a sentence). TS1 has 50 questions and was also used by Turney (2001). TS2 has 60 questions extracted from a TOEFL practice guide (King and Stanley, 1989). For all test sets the answer to each question is known and unique. For comparison purposes, we also use TS1</context>
<context position="20383" citStr="Landauer and Dumais (1997)" startWordPosition="3336" endWordPosition="3339">r protestations, they heeded the judge’s ruling.” = ‘In spite of’,‘Because of’,‘On behalf of’,‘without’ Figure 7: Examples of harder questions in TS2 Figure 8: Results for TS2 and no context Figure 9: Results for TS2 and context the results, as depicted in figures 4, 11 and 13 for TOEFL, TS1 and TS2 test sets, respectively. 5 Results and Discussion The results for the TOEFL questions are presented in figure 3. The best performance found is 81.25% of the questions correctly answered. That result used DR-PMI with a window size of 16-32 words. This is an improvement over the results presented by Landauer and Dumais (1997) using Latent Semantic Analysis, where 64.5% of the questions were answered correctly, and Turney (2001), using pointwise mutual information and document retrieval, where the best result was 73.75%. Although we use a similar method (DR-PMI), the difference between the results presented here and Turney’s results may be due to differences in the corpora and differences in the queries. Turney uses Altavista and we used our own crawl of web data. We can not compare the collections since we do not know how Altavista collection is created. As for the queries, we have more control over the queries si</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>On the effectiveness of the skew divergence for statistical language analysis.</title>
<date>2001</date>
<booktitle>In Artificial Intelligence and Statistics</booktitle>
<pages>65--72</pages>
<marker>Lee, 2001</marker>
<rawString>Lillian Lee. 2001. On the effectiveness of the skew divergence for statistical language analysis. In Artificial Intelligence and Statistics 2001, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Lesk</author>
</authors>
<title>Word-word associations in document retrieval systems.</title>
<date>1969</date>
<journal>American Documentation,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="9943" citStr="Lesk (1969)" startWordPosition="1589" endWordPosition="1590">milarity between two words can also be inferred from a context (if given). Given a context , and are related if their co-occurrence with words in context are similar. 2.2.1 Cosine of Pointwise Mutual Information The PMI between each context word and form a vector. The elements in the vector represents the similarity weights of and . The cosine value between the two vectors corresponding to and represents the similarity between the two words in the specified context, as depicted in equation 5. Values closer to one indicate more similarity whereas values close to zero represent less similarity. Lesk (1969) was one of the first to apply the cosine measure to word similarity, but did not use pointwise mutual information to compute the weights. Pantel (2002) used the cosine of pointwise mutual information to uncover word sense from text. 2.2.2 norm In this method the conditional probability of each word in given (and ) is computed. The accumulated distance between the conditionals for all words in context represents the similarity between the two words, as shown in equation 6. This method was proposed as an alternative word similarity measure in language modeling to overcome zero-frequency problem</context>
</contexts>
<marker>Lesk, 1969</marker>
<rawString>M. E. Lesk. 1969. Word-word associations in document retrieval systems. American Documentation, 20(1):27–38, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Word clustering and disambiguation based on co-occurence data. In</title>
<date>1998</date>
<booktitle>COLINGACL,</booktitle>
<pages>749--755</pages>
<contexts>
<context position="2623" citStr="Li and Abe, 1998" startWordPosition="407" endWordPosition="410">co-occurring words. Also, despite the fact that word co-occurrence is a simple idea, there are a variety of ways to estimate word co-occurrence frequencies from text. Two words can appear close to each other in the same document, passage, paragraph, sentence or fixed-size window. The boundaries for determining cooccurrence will affect the estimates and as a consequence the word similarity measures. Statistical word similarity measures play an important role in information retrieval and in many other natural language applications, such as the automatic creation of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin, 1998) and word sense disambiguation (Yarowsky, 1992; Li and Abe, 1998). Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text. Recently, Tan et al. (2002) provide an analysis on different measures of independence in the context of association rules. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling </context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Word clustering and disambiguation based on co-occurence data. In COLINGACL, pages 749–755.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="2635" citStr="Lin, 1998" startWordPosition="411" endWordPosition="412">. Also, despite the fact that word co-occurrence is a simple idea, there are a variety of ways to estimate word co-occurrence frequencies from text. Two words can appear close to each other in the same document, passage, paragraph, sentence or fixed-size window. The boundaries for determining cooccurrence will affect the estimates and as a consequence the word similarity measures. Statistical word similarity measures play an important role in information retrieval and in many other natural language applications, such as the automatic creation of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin, 1998) and word sense disambiguation (Yarowsky, 1992; Li and Abe, 1998). Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text. Recently, Tan et al. (2002) provide an analysis on different measures of independence in the context of association rules. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interp</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In COLING-ACL, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<contexts>
<context position="2723" citStr="Pantel and Lin (2002)" startWordPosition="423" endWordPosition="426">a variety of ways to estimate word co-occurrence frequencies from text. Two words can appear close to each other in the same document, passage, paragraph, sentence or fixed-size window. The boundaries for determining cooccurrence will affect the estimates and as a consequence the word similarity measures. Statistical word similarity measures play an important role in information retrieval and in many other natural language applications, such as the automatic creation of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin, 1998) and word sense disambiguation (Yarowsky, 1992; Li and Abe, 1998). Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text. Recently, Tan et al. (2002) provide an analysis on different measures of independence in the context of association rules. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes. Dagan et al. (1999) use word similari</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002. Discovering word senses from text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 613–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling. computer speech and language. Computer Speech and Language,</title>
<date>1996</date>
<pages>10--187</pages>
<contexts>
<context position="3035" citStr="Rosenfeld (1996)" startWordPosition="476" endWordPosition="477">l word similarity measures play an important role in information retrieval and in many other natural language applications, such as the automatic creation of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin, 1998) and word sense disambiguation (Yarowsky, 1992; Li and Abe, 1998). Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text. Recently, Tan et al. (2002) provide an analysis on different measures of independence in the context of association rules. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes. Dagan et al. (1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data. In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al., 1995; Xu and Croft, 2000; Vechtomova and Robertson, 200</context>
<context position="9268" citStr="Rosenfeld (1996)" startWordPosition="1477" endWordPosition="1478">xt have a binomial distribution. Two hypotheses used are: H1: (i.e. they occur independently); and H2: (i.e. not independent). These two conditionals are used as sample in the likelihood function , where in this particular case represents the parameter of the binomial distribution . Under hypothesis H1, , and for H2, . Equation 3 represents the likelihood ratio. Asymptotically, isdistributed. 2.1.4 Average Mutual Information This measure corresponds to the expected value of two random variables using the same equation as PMI. Average mutual information was used as a word similarity measure by Rosenfeld (1996) and is given by equation 4. 2.2 Context supported similarity Similarity between two words can also be inferred from a context (if given). Given a context , and are related if their co-occurrence with words in context are similar. 2.2.1 Cosine of Pointwise Mutual Information The PMI between each context word and form a vector. The elements in the vector represents the similarity weights of and . The cosine value between the two vectors corresponding to and represents the similarity between the two words in the specified context, as depicted in equation 5. Values closer to one indicate more sim</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. computer speech and language. Computer Speech and Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-N Tan</author>
<author>V Kumar</author>
<author>J Srivastava</author>
</authors>
<title>Selecting the right interestingness measure for association patterns.</title>
<date>2002</date>
<booktitle>In Proceedings ofACMSIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>32--41</pages>
<contexts>
<context position="2859" citStr="Tan et al. (2002)" startWordPosition="448" endWordPosition="451">ge, paragraph, sentence or fixed-size window. The boundaries for determining cooccurrence will affect the estimates and as a consequence the word similarity measures. Statistical word similarity measures play an important role in information retrieval and in many other natural language applications, such as the automatic creation of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin, 1998) and word sense disambiguation (Yarowsky, 1992; Li and Abe, 1998). Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text. Recently, Tan et al. (2002) provide an analysis on different measures of independence in the context of association rules. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes. Dagan et al. (1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data. In informati</context>
<context position="6977" citStr="Tan et al. (2002)" startWordPosition="1112" endWordPosition="1115"> for co-occurrence of two words can depicted by a contingency table, as shown in table 1. Each dimension represents a random discrete variable with range (presence or absence of wordin a given text window or document). Each cell in the table represent the joint frequency , where is the maximum number of co-occurrences. Under an independence assumption, the values of the cells in the contingency table are calculated using the probabilities in table 2. The methods described below perform different measures of how distant observed values are from expected values under an independence assumption. Tan et al. (2002) indicate that the difference between the methods arise from non-uniform marginals and how the methods react to this non-uniformity. Table 1: Contingency table Table 2: Probabilities under independence Occasionally, a context is available and can provide support for the co-occurrence and alternative methods can be used to exploit this context. The procedures to estimate , as well , will be described in section 3. 2.1 Similarity between two words We first present methods to measure the similarity between two words and when no context is available. 2.1.1 Pointwise Mutual Information This measure</context>
<context position="12254" citStr="Tan et al. (2002)" startWordPosition="1976" endWordPosition="1979"> context, discarding the remaining words. The chosen word was the one that has biggest pointwise information with . Moreover, ( ) is fixed when the method is used to find the best for , so is also fixed and can be ignored, which transforms the equation into the conditional . It is interesting to note that the equation is not the traditional n-gram model since no ordering is imposed on the words and also due to the fact that the words in this formula can be separated from one another by other words. (9) 2.2.6 Other measures of word similarities Many other measures for word similarities exists. Tan et al. (2002) present a comparative study with 21 different measures. Lillian (2001) proposes a new word similarity measure in the context of language modeling, performing an comparative evaluation with other 7 similarity measures. 3 Co-occurrence Estimates We now discuss some alternatives to estimate word cooccurrence frequencies from an available corpus. All probabilities mentioned in previous section can be estimated from these frequencies. We describe two different approaches: a window-oriented approach and a document-oriented approach. 3.1 Window-oriented approach Letbe the frequency of and the co-occ</context>
</contexts>
<marker>Tan, Kumar, Srivastava, 2002</marker>
<rawString>P.-N. Tan, V. Kumar, and J. Srivastava. 2002. Selecting the right interestingness measure for association patterns. In Proceedings ofACMSIGKDD Conference on Knowledge Discovery and Data Mining, pages 32–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI–IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>491--502</pages>
<contexts>
<context position="4018" citStr="Turney (2001)" startWordPosition="633" endWordPosition="634">es perplexity up to 20% in held out data. In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al., 1995; Xu and Croft, 2000; Vechtomova and Robertson, 2000). Xu and Croft (2000) expand queries under a pseudo-relevance feedback model by using similar words from documents retrieved and improve effectiveness by more than 20% on an 11-point average precision. Landauer and Dumais (1997) applied word similarity measures to answer TOEFL (Test Of English as a Foreign Language) synonym questions using Latent Semantic Analysis. Turney (2001) performed an evaluation of a specific word similarity measure using the same TOEFL questions and compared the results with those obtained _ “The results of the test were quite [unambiguous].” _ ‘unambiguous’ _ ‘clear’,‘doubtful’,‘surprising’, ‘illegal’ Figure 1: Finding the best synonym option in presence of context _ ‘boast’ _ ‘brag’,‘yell’,‘complain’,‘explain’ Figure 2: Finding the best synonym by Landauer and Dumais. In our investigation of frequency estimates for word similarity measures, we compare the results of several different measures and frequency estimates to solve human-oriented </context>
<context position="11272" citStr="Turney (2001)" startWordPosition="1802" endWordPosition="1803"> Average Mutual Information The conditional probabilities between each word in the context and the two words and are used to calculate the mutual information of the conditionals (equation 7). This method was also used in Dagan et. al. (1999). 2.2.4 Contextual Jensen-Shannon Divergence This is an alternative to the Mutual Information formula (equation 8). It helps to avoid zero frequency problem by averaging the two distributions and also provides a symmetric measure (AMIC is not symmetric). This method was also used in Dagan et. al. (1999). 2.2.5 Pointwise Mutual Information of Multiple words Turney (2001) proposes a different formula for Pointwise Mutual Information when context is available, as depicted in equation 9. The context is represented by , which is any subset of the context . In fact, Turney argued that bigger sets are worse because they narrow the estimate and as consequence can be affected by noise. As a consequence, Turney used only one wordfrom the context, discarding the remaining words. The chosen word was the one that has biggest pointwise information with . Moreover, ( ) is fixed when the method is used to find the best for , so is also fixed and can be ignored, which transf</context>
<context position="16730" citStr="Turney (2001)" startWordPosition="2723" endWordPosition="2725">multiwords then the problem is harder, as depicted in the first example of figure 7. Also, both parsers and POS tagger make mistakes, thus introducing error. Finally, the size of the corpus used and its nature intensify the parser/POS taggers problems. Figure 3: Results for TOEFL test set Figure 4: Impact of corpus size on TOEFL test set Figure 5: Results for TS1 and no context Figure 6: Results for TS1 and context 4 Experiments We evaluate the methods and frequency estimates using 3 test sets. The first test set is a set of TOEFL questions first used by Landauer and Dumais (1997) and also by Turney (2001). This test set contains 80 synonym questions and for each question one and four alternative options ( ) are given. The other two test sets, which we will refer to as TS1 and TS2, are practice questions for the TOEFL. These two test sets also contain four alternatives options, , and is given in context (within a sentence). TS1 has 50 questions and was also used by Turney (2001). TS2 has 60 questions extracted from a TOEFL practice guide (King and Stanley, 1989). For all test sets the answer to each question is known and unique. For comparison purposes, we also use TS1 and TS2 with no context. </context>
<context position="20487" citStr="Turney (2001)" startWordPosition="3353" endWordPosition="3355">mples of harder questions in TS2 Figure 8: Results for TS2 and no context Figure 9: Results for TS2 and context the results, as depicted in figures 4, 11 and 13 for TOEFL, TS1 and TS2 test sets, respectively. 5 Results and Discussion The results for the TOEFL questions are presented in figure 3. The best performance found is 81.25% of the questions correctly answered. That result used DR-PMI with a window size of 16-32 words. This is an improvement over the results presented by Landauer and Dumais (1997) using Latent Semantic Analysis, where 64.5% of the questions were answered correctly, and Turney (2001), using pointwise mutual information and document retrieval, where the best result was 73.75%. Although we use a similar method (DR-PMI), the difference between the results presented here and Turney’s results may be due to differences in the corpora and differences in the queries. Turney uses Altavista and we used our own crawl of web data. We can not compare the collections since we do not know how Altavista collection is created. As for the queries, we have more control over the queries since we can precisely specify the window size and we also do not know how queries are evaluated in Altavi</context>
<context position="22217" citStr="Turney (2001)" startWordPosition="3651" endWordPosition="3653">at the divergence formula has an embedded expectation to be calculated between the joint probability of these two random variables and their independence. The peak of information is exactly where both words co-occur, i.e. when and , and not any of the other three possible combinations. Similar trends are seen when using TS1 and no context, as depicted in figure 5. PMI is best overall, and DRPMI and W-PMI outperform each other with different windows sizes. W-CHI has good performance in small windows sizes. MI and LL yield identical (poor) results, being worst than chance for some window sizes. Turney (2001) also uses this test set without context, achieving 66% peak performance compared with our best performance of 72% (DR-PMI). In the test set TS2 with no context, the trend seen between TOEFL and TS1 is repeated, as shown in figure 8. PMI is best overall but W-CHI performs better than PMI in three cases. DR-CHI performs poorly for small windows sizes. MI and LL also perform poorly in comparison with PMI. The peak performance is 75%, using DRPMI with a window size of 64. The result are not what we expected when context is used in TS1 and TS2. In TS1, figure 6, only one of the measures, DR-PMIC-1</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. D. Turney. 2001. Mining the Web for synonyms: PMI–IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Vechtomova</author>
<author>S Robertson</author>
</authors>
<title>Integration of collocation statistics into the probabilistic retrieval model.</title>
<date>2000</date>
<booktitle>In 22nd Annual Colloquium on Information Retrieval Research,</booktitle>
<location>Cambridge, England.</location>
<contexts>
<context position="3637" citStr="Vechtomova and Robertson, 2000" startWordPosition="571" endWordPosition="575">pplications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes. Dagan et al. (1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data. In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al., 1995; Xu and Croft, 2000; Vechtomova and Robertson, 2000). Xu and Croft (2000) expand queries under a pseudo-relevance feedback model by using similar words from documents retrieved and improve effectiveness by more than 20% on an 11-point average precision. Landauer and Dumais (1997) applied word similarity measures to answer TOEFL (Test Of English as a Foreign Language) synonym questions using Latent Semantic Analysis. Turney (2001) performed an evaluation of a specific word similarity measure using the same TOEFL questions and compared the results with those obtained _ “The results of the test were quite [unambiguous].” _ ‘unambiguous’ _ ‘clear’,</context>
</contexts>
<marker>Vechtomova, Robertson, 2000</marker>
<rawString>O. Vechtomova and S. Robertson. 2000. Integration of collocation statistics into the probabilistic retrieval model. In 22nd Annual Colloquium on Information Retrieval Research, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>B Croft</author>
</authors>
<title>Improving the effectiveness of information retrieval.</title>
<date>2000</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="3604" citStr="Xu and Croft, 2000" startWordPosition="567" endWordPosition="570"> language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes. Dagan et al. (1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data. In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al., 1995; Xu and Croft, 2000; Vechtomova and Robertson, 2000). Xu and Croft (2000) expand queries under a pseudo-relevance feedback model by using similar words from documents retrieved and improve effectiveness by more than 20% on an 11-point average precision. Landauer and Dumais (1997) applied word similarity measures to answer TOEFL (Test Of English as a Foreign Language) synonym questions using Latent Semantic Analysis. Turney (2001) performed an evaluation of a specific word similarity measure using the same TOEFL questions and compared the results with those obtained _ “The results of the test were quite [unambigu</context>
</contexts>
<marker>Xu, Croft, 2000</marker>
<rawString>J. Xu and B. Croft. 2000. Improving the effectiveness of information retrieval. ACM Transactions on Information Systems, 18(1):79–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget’s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>454--460</pages>
<location>Nantes, France,</location>
<contexts>
<context position="2681" citStr="Yarowsky, 1992" startWordPosition="417" endWordPosition="418">rrence is a simple idea, there are a variety of ways to estimate word co-occurrence frequencies from text. Two words can appear close to each other in the same document, passage, paragraph, sentence or fixed-size window. The boundaries for determining cooccurrence will affect the estimates and as a consequence the word similarity measures. Statistical word similarity measures play an important role in information retrieval and in many other natural language applications, such as the automatic creation of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin, 1998) and word sense disambiguation (Yarowsky, 1992; Li and Abe, 1998). Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text. Recently, Tan et al. (2002) provide an analysis on different measures of independence in the context of association rules. Word similarity is also used in language modeling applications. Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%. Brown et al. (1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into clas</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget’s categories trained on large corpora. In Proceedings of COLING-92, pages 454–460, Nantes, France, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>