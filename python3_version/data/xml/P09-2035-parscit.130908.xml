<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.093049">
<title confidence="0.958252">
Sub-Sentence Division for Tree-Based Machine Translation
</title>
<author confidence="0.910974">
Hao Xiong*, Wenwen Xu+, Haitao Mi*, Yang Liu* and Qun Liu*
</author>
<affiliation confidence="0.9180055">
*Key Lab. of Intelligent Information Processing
+Key Lab. of Computer System and Architecture
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.870039">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.999333">
{xionghao,xuwenwen,htmi,yliu,liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997924052631579">
Tree-based statistical machine translation
models have made significant progress in re-
cent years, especially when replacing 1-best
trees with packed forests. However, as the
parsing accuracy usually goes down dramati-
cally with the increase of sentence length,
translating long sentences often takes long
time and only produces degenerate transla-
tions. We propose a new method named sub-
sentence division that reduces the decoding
time and improves the translation quality for
tree-based translation. Our approach divides
long sentences into several sub-sentences by
exploiting tree structures. Large-scale ex-
periments on the NIST 2008 Chinese-to-
English test set show that our approach
achieves an absolute improvement of 1.1
BLEU points over the baseline system in
50% less time.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999011894736842">
Tree-based statistical machine translation
models in days have witness promising progress
in recent years, such as tree-to-string models (Liu
et al., 2006; Huang et al., 2006), tree-to-tree
models (Quirk et al.,2005;Zhang et al., 2008).
Especially, when incorporated with forest, the
correspondent forest-based tree-to-string models
(Mi et al., 2008; Zhang et al., 2009), tree-to-tree
models (Liu et al., 2009) have achieved a prom-
ising improvements over correspondent tree-
based systems. However, when we translate long
sentences, we argue that two major issues will be
raised. On one hand, parsing accuracy will be
lower as the length of sentence grows. It will in-
evitably hurt the translation quality (Quirk and
Corston-Oliver, 2006; Mi and Huang, 2008). On
the other hand, decoding on long sentences will
be time consuming, especially for forest ap-
proaches. So splitting long sentences into sub-
</bodyText>
<figureCaption confidence="0.98814">
Figure 1. Main framework of our method
</figureCaption>
<bodyText confidence="0.99836375">
sentences becomes a natural way in MT litera-
ture.
A simple way is to split long sentences by
punctuations. However, without concerning
about the original whole tree structures, this ap-
proach will result in ill-formed sub-trees which
don’t respect to original structures. In this paper,
we present a new approach, which pays more
attention to parse trees on the long sentences. We
firstly parse the long sentences into trees, and
then divide them accordingly into sub-sentences,
which will be translated independently (Section
3). Finally, we combine sub translations into a
full translation (Section 4). Large-scale experi-
ments (Section 5) show that the BLEU score
achieved by our approach is 1.1 higher than di-
rect decoding and 0.3 higher than always split-
ting on commas on the 2008 NIST MT Chinese-
English test set. Moreover, our approach has re-
duced decoding time significantly.
</bodyText>
<sectionHeader confidence="0.990555" genericHeader="introduction">
2 Framework
</sectionHeader>
<bodyText confidence="0.992">
Our approach works in following steps.
</bodyText>
<listItem confidence="0.999895333333333">
(1) Split a long sentence into sub-sentences.
(2) Translate all the sub-sentences respectively.
(3) Combine the sub-translations.
</listItem>
<bodyText confidence="0.98371475">
Figure 1 illustrates the main idea of our ap-
proach. The crucial issues of our method are how
to divide long sentences and how to combine the
sub-translations.
</bodyText>
<sectionHeader confidence="0.960013" genericHeader="method">
3 Sub Sentence Division
</sectionHeader>
<bodyText confidence="0.9997626">
Long sentences could be very complicated in
grammar and sentence structure, thereby creating
an obstacle for translation. Consequently, we
need to break them into shorter and easier
clauses. To divide sentences by punctuation is
</bodyText>
<page confidence="0.971905">
137
</page>
<note confidence="0.980234">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 137–140,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.9999925">
Figure 2. An undividable parse tree
Figure 3. A dividable parse tree
</figureCaption>
<bodyText confidence="0.998483793103448">
one of the most commonly used methods. How-
ever, simply applying this method might damage
the accuracy of parsing. As a result, the strategy
we proposed is to operate division while con-
cerning the structure of parse tree.
As sentence division should not influence the
accuracy of parsing, we have to be very cautious
about sentences whose division might decrease
the accuracy of parsing. Figure 2(a) shows an
example of the parse tree of an undividable sen-
tence.
As can be seen in Figure 2, when we divide
the sentence by comma, it would break the struc-
ture of “VP” sub-tree and result in a ill-formed
sub-tree “VP” (right sub-tree), which don’t have
a subject and don’t respect to original tree struc-
tures.
Consequently, the key issue of sentence divi-
sion is finding the sentences that can be divided
without loosing parsing accuracy. Figure 2(b)
shows the parse tree of a sentence that can be
divided by punctuation, as sub-sentences divided
by comma are independent. The reference trans-
lation of the sentence in figure 3 is
Less than two hours earlier, a Palestinian took
on a shooting spree on passengers in the town of
Kfar Saba in northern Israel.
Pseudocode 1 Check Sub Sentence Divi-
sion Algorithm
</bodyText>
<listItem confidence="0.99861275">
1: procedure CheckSubSentence(sent)
2: for each word i in sent
3: if(i is a comma)
4: left={words in left side of i};
//words between last comma and cur-
rent comma i
5: right={words in right side of i};
//words between i and next comma or
semicolon, period, question mark
6: isDividePunct[i]=true;
7: for each j in left
8: if(( LCA(j, i)!=parent[i])
9: isDividePunct[i]=false;
10: break;
11: for each j in right
12: if(( LCA(j, i)!=parent[i])
13: isDividePunct[i]=false;
14: break;
15: function LCA(i, j)
16: return lowest common ancestor(i, j);
</listItem>
<bodyText confidence="0.999652709677419">
It demonstrates that this long sentence can be
divided into two sub-sentences, providing a good
support to our division.
In addition to dividable sentences and non-
dividable sentences, there are sentences contain-
ing more than one comma, some of which are
dividable and some are not. However, this does
not prove to be a problem, as we process each
comma independently. In other words, we only
split the dividable part of this kind of sentences,
leaving the non-dividable part unchanged.
To find the sentences that can be divided, we
present a new method and provide its pseudo
code. Firstly, we divide a sentence by its commas.
For each word in the sub-sentence on the left
side of a comma, we compute its lowest common
ancestor (LCA) with the comma. And we process
the words in the sub-sentence on the right side of
the comma in the same way. Finally, we check if
all the LCA we have computed are comma’s par-
ent node. If all the LCA are the comma’s parent
node, the sub-sentences are independent.
As shown in figure 3, the LCA (AD TNJ�q ,
PU ,), is “IP” ,which is the parent node of
“PU ,”; and the LCA (NR I�kt3�q , PU ,) is
also “IP”. Till we have checked all the LCA of
each word and comma, we finally find that all
the LCA are “IP”. As a result, this sentence can
be divided without loosing parsing accuracy.
LCA can be computed by using union-set (Tar-
jan, 1971) in lineal time. Concerning the
</bodyText>
<page confidence="0.984945">
138
</page>
<table confidence="0.626491222222222">
sub-sentence 1: 强卓指出
Translation 1: Johndroe said A1
Translation 2: Johndroe pointed out A2
Translation 3: Qiang Zhuo said A3
comma 1: ,
Translation: punctuation translation (white
space, that ... )
sub-sentence 2: 两位总统也对昨日签署的
美国━南韩自由贸易协议表示欢迎
</table>
<figureCaption confidence="0.981730647058824">
Translation 1: the two presidents also wel-
comed the US-South Korea free trade
agreement that was signed yesterday B1
Translation 2: the two presidents also ex-
pressed welcome to the US – South Korea
free trade agreement signed yesterday B2
comma 2: ,
Translation: punctuation translation (white
space, that ... )
sub-sentence 3:并将致力确保两国国会批
准此一协议。
Translation 1: and would work to ensure
that the congresses of both countries ap-
prove this agreement. C1
Translation 2: and will make efforts to en-
sure the Congress to approve this agreement
of the two countries. C2
</figureCaption>
<tableCaption confidence="0.971927">
Table 1. Sub translation example
</tableCaption>
<bodyText confidence="0.998976066666666">
implementation complexity, we have reduced the
problem to range minimum query problem
(Bender et al., 2005) with a time complexity of
ο(1) for querying.
Above all, our approach for sub sentence
works as follows:
(1)Split a sentence by semi-colon if there is
one.
(2)Parse a sentence if it contains a comma,
generating k-best parses (Huang Chiang, 2005)
with k=10.
(3)Use the algorithm in pseudocode 1 to
check the sentence and divide it if there are
more than 5 parse trees indicates that the sen-
tence is dividable.
</bodyText>
<sectionHeader confidence="0.984908" genericHeader="method">
4 Sub Translation Combining
</sectionHeader>
<bodyText confidence="0.999854">
For sub translation combining, we mainly use the
best-first expansion idea from cube pruning
(Huang and Chiang, 2007) to combine sub-
translations and generate the whole k-best trans-
lations. We first select the best translation from
sub translation sets, and then use an interpolation
</bodyText>
<table confidence="0.9998115">
Test Set 02 05 08
No Sent Division 34.56 31.26 24.53
Split by Comma 34.59 31.23 25.39
Our Approach 34.86 31.23 25.69
</table>
<tableCaption confidence="0.978684">
Table 2. BLEU results (case sensitive)
</tableCaption>
<table confidence="0.99992875">
Test Set 02 05 08
No Sent Division 28 h 36 h 52 h
Split by Comma 18h 23h 29h
Our Approach 18 h 22 h 26 h
</table>
<tableCaption confidence="0.970714">
Table 3. Decoding time of our experiments
(h means hours)
</tableCaption>
<bodyText confidence="0.995570181818182">
language model for rescoring (Huang and Chiang,
2007).
For example, we split the following sentence “强
卓指出,两位总统也对昨日签署的美国━南韩自由
贸易协议表示欢迎,并将致力确保两国国会批准此
一协议。” into three sub-sentences and generate
some translations, and the results are displayed in
Table 1.
As seen in Table 1, for each sub-sentence,
there are one or more versions of translation. For
convenience, we label the three translation ver-
sions of sub-sentence 1 as A1, A2, and A3, re-
spectively. Similarly, B1, B2, C1, C2 are also
labels of translation. We push the A1, white
space, B1, white space, C1 into the cube, and
then generate the final translation.
According to cube pruning algorithm, we will
generate other translations until we get the best
list we need. Finally, we rescore the k-best list
using interpolation language model and find the
best translation which is A1 that B1 white space
C1.
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99974">
5.1 Data preparation
</subsectionHeader>
<bodyText confidence="0.999908846153846">
We conduct our experiments on Chinese-English
translation, and use the Chinese parser of Xiong
et al. (2005) to parse the source sentences. And
our decoder is based on forest-based tree-to-
string translation model (Mi et al. 2008).
Our training corpus consists of 2.56 million
sentence pairs. Forest-based rule extractor (Mi
and Huang 2008) is used with a pruning thresh-
old p=3. And we use SRI Language Modeling
Toolkit (Stolcke, 2002) to train two 5-gram lan-
guage models with Kneser-Ney smoothing on the
English side of the training corpus and the Xin-
hua portion of Gigaword corpora respectively.
</bodyText>
<page confidence="0.997683">
139
</page>
<bodyText confidence="0.999970625">
We use 2006 NIST MT Evaluation test set as
development set, and 2002, 2005 and 2008 NIST
MT Evaluation test sets as test sets. We also use
minimum error-rate training (Och, 2003) to tune
our feature weights. We evaluate our results with
case-sensitive BLEU-4 metric (Papineni et al.,
2002). The pruning threshold p for parse forest in
decoding time is 12.
</bodyText>
<subsectionHeader confidence="0.632466">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999910727272727">
The final BLEU results are shown in Table 2, our
approach has achieved a BLEU score that is 1.1
higher than direct decoding and 0.3 higher than
always splitting on commas.
The decoding time results are presented in Ta-
ble 3. The search space of our experiment is ex-
tremely large due to the large pruning threshold
(p=12), thus resulting in a long decoding time.
However, our approach has reduced the decoding
time by 50% over direct decoding, and 10% over
always splitting on commas.
</bodyText>
<sectionHeader confidence="0.998751" genericHeader="conclusions">
6 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999888">
We have presented a new sub-sentence division
method and achieved some good results. In the
future, we will extend our work from decoding to
training time, where we divide the bilingual sen-
tences accordingly.
</bodyText>
<sectionHeader confidence="0.965646" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999883">
The authors were supported by National Natural
Science Foundation of China, Contracts 0873167
and 60736014, and 863 State Key Project
No.2006AA010108. We thank Liang Huang for
his insightful suggestions.
</bodyText>
<sectionHeader confidence="0.998521" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997600888888889">
Bender, Farach-Colton, Pemmasani, Skiena, Sumazin,
Lowest common ancestors in trees and di-
rected acyclic graphs. J. Algorithms 57(2), 75–
94 (2005)
Liang Huang and David Chiang. 2005. Better kbest
Parsing. In Proceedings of IWPT-2005.
Liang Huang and David Chiang. 2007. Forest res-
coring: Fast decoding with integrated lan-
guage models. In Proceedings of ACL.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with ex-
tended domain of locality. In Proceedings of
AMTA
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127-133.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String alignments template for statistical ma-
chine translation. In Proceedings of ACL.
Yang Liu, Yajuan Lv and Qun Liu.2009. Improving
Tree-to-Tree Translation with Packed Forests.To
appear in Proceedings of ACL/IJCNLP..
Daniel Marcu, Wei Wang, AbdessamadEchihabi, and
Kevin Knight. 2006. Statistical Machine Trans-
lation with syntactifiedtarget language
phrases. In Proceedings of EMNLP.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT.
Haitao Mi and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of
EMNLP.
Franz J. Och. 2003. Minimum error rate training
in statistical machine translation. In Proceed-
ings of ACL, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In
Proceedings of ACL, pages 311–318,.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically
informed phrasal SMT. In Proceedings of ACL.
Chris Quirk and Simon Corston-Oliver. 2006. The
impact of parse quality on syntactically-
informed statistical machine translation. In
Proceedings of EMNLP.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of
ICSLP, volume 30, pages 901–904.
Georgianna Tarjan, Depth First Search and Linear
Graph Algorithms. SIAM J. Comp. 1:2, pp. 146–
160, 1972.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin.2005. Parsing the Penn Chinese Treebank
with semantic knowledge. In Proceedings of
IJCNLP.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree transla-
tion model. In Proceedings of ACL.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and
Chew Lim Tan. 2009. Forest-based Tree Sequence
to String Translation Model. To appear in Proceed-
ings of ACL/IJCNLP
</reference>
<page confidence="0.997534">
140
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.758666">
<title confidence="0.999783">Sub-Sentence Division for Tree-Based Machine Translation</title>
<author confidence="0.95656">Wenwen Haitao Yang</author>
<affiliation confidence="0.9500215">Lab. of Intelligent Information Processing Lab. of Computer System and Architecture Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.997946">P.O. Box 2704, Beijing 100190, China</address>
<email confidence="0.987562">xionghao@ict.ac.cn</email>
<email confidence="0.987562">xuwenwen@ict.ac.cn</email>
<email confidence="0.987562">htmi@ict.ac.cn</email>
<email confidence="0.987562">yliu@ict.ac.cn</email>
<email confidence="0.987562">liuqun@ict.ac.cn</email>
<abstract confidence="0.9965062">Tree-based statistical machine translation models have made significant progress in recent years, especially when replacing 1-best trees with packed forests. However, as the parsing accuracy usually goes down dramatically with the increase of sentence length, translating long sentences often takes long time and only produces degenerate translations. We propose a new method named subsentence division that reduces the decoding time and improves the translation quality for tree-based translation. Our approach divides long sentences into several sub-sentences by exploiting tree structures. Large-scale experiments on the NIST 2008 Chinese-to- English test set show that our approach achieves an absolute improvement of 1.1 BLEU points over the baseline system in 50% less time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Farach-Colton Bender</author>
<author>Skiena Pemmasani</author>
</authors>
<title>Sumazin, Lowest common ancestors in trees and directed acyclic graphs.</title>
<date>2005</date>
<journal>J. Algorithms</journal>
<volume>57</volume>
<issue>2</issue>
<marker>Bender, Pemmasani, 2005</marker>
<rawString>Bender, Farach-Colton, Pemmasani, Skiena, Sumazin, Lowest common ancestors in trees and directed acyclic graphs. J. Algorithms 57(2), 75– 94 (2005)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better kbest Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT-2005.</booktitle>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better kbest Parsing. In Proceedings of IWPT-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Fast decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="8422" citStr="Huang and Chiang, 2007" startWordPosition="1363" endWordPosition="1366">uced the problem to range minimum query problem (Bender et al., 2005) with a time complexity of ο(1) for querying. Above all, our approach for sub sentence works as follows: (1)Split a sentence by semi-colon if there is one. (2)Parse a sentence if it contains a comma, generating k-best parses (Huang Chiang, 2005) with k=10. (3)Use the algorithm in pseudocode 1 to check the sentence and divide it if there are more than 5 parse trees indicates that the sentence is dividable. 4 Sub Translation Combining For sub translation combining, we mainly use the best-first expansion idea from cube pruning (Huang and Chiang, 2007) to combine subtranslations and generate the whole k-best translations. We first select the best translation from sub translation sets, and then use an interpolation Test Set 02 05 08 No Sent Division 34.56 31.26 24.53 Split by Comma 34.59 31.23 25.39 Our Approach 34.86 31.23 25.69 Table 2. BLEU results (case sensitive) Test Set 02 05 08 No Sent Division 28 h 36 h 52 h Split by Comma 18h 23h 29h Our Approach 18 h 22 h 26 h Table 3. Decoding time of our experiments (h means hours) language model for rescoring (Huang and Chiang, 2007). For example, we split the following sentence “强 卓指出,两位总统也对昨日</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Fast decoding with integrated language models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA</booktitle>
<contexts>
<context position="1334" citStr="Huang et al., 2006" startWordPosition="185" endWordPosition="188">. We propose a new method named subsentence division that reduces the decoding time and improves the translation quality for tree-based translation. Our approach divides long sentences into several sub-sentences by exploiting tree structures. Large-scale experiments on the NIST 2008 Chinese-toEnglish test set show that our approach achieves an absolute improvement of 1.1 BLEU points over the baseline system in 50% less time. 1 Introduction Tree-based statistical machine translation models in days have witness promising progress in recent years, such as tree-to-string models (Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008). On the other hand,</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of AMTA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>127--133</pages>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-toString alignments template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1313" citStr="Liu et al., 2006" startWordPosition="181" endWordPosition="184">erate translations. We propose a new method named subsentence division that reduces the decoding time and improves the translation quality for tree-based translation. Our approach divides long sentences into several sub-sentences by exploiting tree structures. Large-scale experiments on the NIST 2008 Chinese-toEnglish test set show that our approach achieves an absolute improvement of 1.1 BLEU points over the baseline system in 50% less time. 1 Introduction Tree-based statistical machine translation models in days have witness promising progress in recent years, such as tree-to-string models (Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-toString alignments template for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yang Liu</author>
</authors>
<title>Yajuan Lv and Qun Liu.2009. Improving Tree-to-Tree Translation with Packed Forests.To appear in</title>
<booktitle>Proceedings of ACL/IJCNLP..</booktitle>
<marker>Liu, </marker>
<rawString>Yang Liu, Yajuan Lv and Qun Liu.2009. Improving Tree-to-Tree Translation with Packed Forests.To appear in Proceedings of ACL/IJCNLP..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>AbdessamadEchihabi</author>
<author>Kevin Knight</author>
</authors>
<title>Statistical Machine Translation with syntactifiedtarget language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Marcu, Wang, AbdessamadEchihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, AbdessamadEchihabi, and Kevin Knight. 2006. Statistical Machine Translation with syntactifiedtarget language phrases. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL: HLT.</booktitle>
<contexts>
<context position="1508" citStr="Mi et al., 2008" startWordPosition="207" endWordPosition="210">tences into several sub-sentences by exploiting tree structures. Large-scale experiments on the NIST 2008 Chinese-toEnglish test set show that our approach achieves an absolute improvement of 1.1 BLEU points over the baseline system in 50% less time. 1 Introduction Tree-based statistical machine translation models in days have witness promising progress in recent years, such as tree-to-string models (Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008). On the other hand, decoding on long sentences will be time consuming, especially for forest approaches. So splitting long sentences into subFigure 1. Main framework of our method sentences bec</context>
<context position="10032" citStr="Mi et al. 2008" startWordPosition="1636" endWordPosition="1639">We push the A1, white space, B1, white space, C1 into the cube, and then generate the final translation. According to cube pruning algorithm, we will generate other translations until we get the best list we need. Finally, we rescore the k-best list using interpolation language model and find the best translation which is A1 that B1 white space C1. 5 Experiments 5.1 Data preparation We conduct our experiments on Chinese-English translation, and use the Chinese parser of Xiong et al. (2005) to parse the source sentences. And our decoder is based on forest-based tree-tostring translation model (Mi et al. 2008). Our training corpus consists of 2.56 million sentence pairs. Forest-based rule extractor (Mi and Huang 2008) is used with a pruning threshold p=3. And we use SRI Language Modeling Toolkit (Stolcke, 2002) to train two 5-gram language models with Kneser-Ney smoothing on the English side of the training corpus and the Xinhua portion of Gigaword corpora respectively. 139 We use 2006 NIST MT Evaluation test set as development set, and 2002, 2005 and 2008 NIST MT Evaluation test sets as test sets. We also use minimum error-rate training (Och, 2003) to tune our feature weights. We evaluate our resu</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1914" citStr="Mi and Huang, 2008" startWordPosition="273" endWordPosition="276"> (Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008). On the other hand, decoding on long sentences will be time consuming, especially for forest approaches. So splitting long sentences into subFigure 1. Main framework of our method sentences becomes a natural way in MT literature. A simple way is to split long sentences by punctuations. However, without concerning about the original whole tree structures, this approach will result in ill-formed sub-trees which don’t respect to original structures. In this paper, we present a new approach, which pays more attention to parse trees on the long sentences. We firstly parse the long sentences into t</context>
<context position="10142" citStr="Mi and Huang 2008" startWordPosition="1652" endWordPosition="1655">cording to cube pruning algorithm, we will generate other translations until we get the best list we need. Finally, we rescore the k-best list using interpolation language model and find the best translation which is A1 that B1 white space C1. 5 Experiments 5.1 Data preparation We conduct our experiments on Chinese-English translation, and use the Chinese parser of Xiong et al. (2005) to parse the source sentences. And our decoder is based on forest-based tree-tostring translation model (Mi et al. 2008). Our training corpus consists of 2.56 million sentence pairs. Forest-based rule extractor (Mi and Huang 2008) is used with a pruning threshold p=3. And we use SRI Language Modeling Toolkit (Stolcke, 2002) to train two 5-gram language models with Kneser-Ney smoothing on the English side of the training corpus and the Xinhua portion of Gigaword corpora respectively. 139 We use 2006 NIST MT Evaluation test set as development set, and 2002, 2005 and 2008 NIST MT Evaluation test sets as test sets. We also use minimum error-rate training (Och, 2003) to tune our feature weights. We evaluate our results with case-sensitive BLEU-4 metric (Papineni et al., 2002). The pruning threshold p for parse forest in dec</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="10582" citStr="Och, 2003" startWordPosition="1731" endWordPosition="1732">orest-based tree-tostring translation model (Mi et al. 2008). Our training corpus consists of 2.56 million sentence pairs. Forest-based rule extractor (Mi and Huang 2008) is used with a pruning threshold p=3. And we use SRI Language Modeling Toolkit (Stolcke, 2002) to train two 5-gram language models with Kneser-Ney smoothing on the English side of the training corpus and the Xinhua portion of Gigaword corpora respectively. 139 We use 2006 NIST MT Evaluation test set as development set, and 2002, 2005 and 2008 NIST MT Evaluation test sets as test sets. We also use minimum error-rate training (Och, 2003) to tune our feature weights. We evaluate our results with case-sensitive BLEU-4 metric (Papineni et al., 2002). The pruning threshold p for parse forest in decoding time is 12. 5.2 Results The final BLEU results are shown in Table 2, our approach has achieved a BLEU score that is 1.1 higher than direct decoding and 0.3 higher than always splitting on commas. The decoding time results are presented in Table 3. The search space of our experiment is extremely large due to the large pruning threshold (p=12), thus resulting in a long decoding time. However, our approach has reduced the decoding ti</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="10693" citStr="Papineni et al., 2002" startWordPosition="1746" endWordPosition="1749">million sentence pairs. Forest-based rule extractor (Mi and Huang 2008) is used with a pruning threshold p=3. And we use SRI Language Modeling Toolkit (Stolcke, 2002) to train two 5-gram language models with Kneser-Ney smoothing on the English side of the training corpus and the Xinhua portion of Gigaword corpora respectively. 139 We use 2006 NIST MT Evaluation test set as development set, and 2002, 2005 and 2008 NIST MT Evaluation test sets as test sets. We also use minimum error-rate training (Och, 2003) to tune our feature weights. We evaluate our results with case-sensitive BLEU-4 metric (Papineni et al., 2002). The pruning threshold p for parse forest in decoding time is 12. 5.2 Results The final BLEU results are shown in Table 2, our approach has achieved a BLEU score that is 1.1 higher than direct decoding and 0.3 higher than always splitting on commas. The decoding time results are presented in Table 3. The search space of our experiment is extremely large due to the large pruning threshold (p=12), thus resulting in a long decoding time. However, our approach has reduced the decoding time by 50% over direct decoding, and 10% over always splitting on commas. 6 Conclusion &amp; Future Work We have pre</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Simon Corston-Oliver</author>
</authors>
<title>The impact of parse quality on syntacticallyinformed statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1893" citStr="Quirk and Corston-Oliver, 2006" startWordPosition="269" endWordPosition="272">s, such as tree-to-string models (Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008). On the other hand, decoding on long sentences will be time consuming, especially for forest approaches. So splitting long sentences into subFigure 1. Main framework of our method sentences becomes a natural way in MT literature. A simple way is to split long sentences by punctuations. However, without concerning about the original whole tree structures, this approach will result in ill-formed sub-trees which don’t respect to original structures. In this paper, we present a new approach, which pays more attention to parse trees on the long sentences. We firstly parse the </context>
</contexts>
<marker>Quirk, Corston-Oliver, 2006</marker>
<rawString>Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntacticallyinformed statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<volume>30</volume>
<pages>901--904</pages>
<contexts>
<context position="10237" citStr="Stolcke, 2002" startWordPosition="1671" endWordPosition="1672">e need. Finally, we rescore the k-best list using interpolation language model and find the best translation which is A1 that B1 white space C1. 5 Experiments 5.1 Data preparation We conduct our experiments on Chinese-English translation, and use the Chinese parser of Xiong et al. (2005) to parse the source sentences. And our decoder is based on forest-based tree-tostring translation model (Mi et al. 2008). Our training corpus consists of 2.56 million sentence pairs. Forest-based rule extractor (Mi and Huang 2008) is used with a pruning threshold p=3. And we use SRI Language Modeling Toolkit (Stolcke, 2002) to train two 5-gram language models with Kneser-Ney smoothing on the English side of the training corpus and the Xinhua portion of Gigaword corpora respectively. 139 We use 2006 NIST MT Evaluation test set as development set, and 2002, 2005 and 2008 NIST MT Evaluation test sets as test sets. We also use minimum error-rate training (Och, 2003) to tune our feature weights. We evaluate our results with case-sensitive BLEU-4 metric (Papineni et al., 2002). The pruning threshold p for parse forest in decoding time is 12. 5.2 Results The final BLEU results are shown in Table 2, our approach has ach</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgianna Tarjan</author>
</authors>
<title>Depth First Search and Linear Graph Algorithms.</title>
<date>1972</date>
<journal>SIAM J. Comp.</journal>
<volume>1</volume>
<pages>146--160</pages>
<marker>Tarjan, 1972</marker>
<rawString>Georgianna Tarjan, Depth First Search and Linear Graph Algorithms. SIAM J. Comp. 1:2, pp. 146– 160, 1972.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Deyi Xiong</author>
</authors>
<title>Shuanglong Li, Qun Liu, and Shouxun Lin.2005. Parsing the Penn Chinese Treebank with semantic knowledge.</title>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<marker>Xiong, </marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.2005. Parsing the Penn Chinese Treebank with semantic knowledge. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1394" citStr="Zhang et al., 2008" startWordPosition="193" endWordPosition="196">duces the decoding time and improves the translation quality for tree-based translation. Our approach divides long sentences into several sub-sentences by exploiting tree structures. Large-scale experiments on the NIST 2008 Chinese-toEnglish test set show that our approach achieves an absolute improvement of 1.1 BLEU points over the baseline system in 50% less time. 1 Introduction Tree-based statistical machine translation models in days have witness promising progress in recent years, such as tree-to-string models (Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008). On the other hand, decoding on long sentences will be time consuming, especial</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Aiti Aw and Chew Lim Tan.</title>
<date>2009</date>
<booktitle>Proceedings of ACL/IJCNLP</booktitle>
<contexts>
<context position="1529" citStr="Zhang et al., 2009" startWordPosition="211" endWordPosition="214">al sub-sentences by exploiting tree structures. Large-scale experiments on the NIST 2008 Chinese-toEnglish test set show that our approach achieves an absolute improvement of 1.1 BLEU points over the baseline system in 50% less time. 1 Introduction Tree-based statistical machine translation models in days have witness promising progress in recent years, such as tree-to-string models (Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008). On the other hand, decoding on long sentences will be time consuming, especially for forest approaches. So splitting long sentences into subFigure 1. Main framework of our method sentences becomes a natural way in</context>
</contexts>
<marker>Zhang, Zhang, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew Lim Tan. 2009. Forest-based Tree Sequence to String Translation Model. To appear in Proceedings of ACL/IJCNLP</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>