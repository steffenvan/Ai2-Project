<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001161">
<note confidence="0.862934">
Proceedings of EACL &apos;99
</note>
<title confidence="0.996741">
An annotation scheme for discourse-level argumentation
in research articles
</title>
<author confidence="0.999054">
Simone Teufelt and Jean Carlettat and Marc Moenst
</author>
<affiliation confidence="0.997961">
IFICRC Language Technology Group and
t Human Communication Research Centre
Division of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.99271">
S.Teufel@ed.ac.uk, J.Carletta@ed.ac.uk, M.Moens@ed.ac.uk
</email>
<sectionHeader confidence="0.983122" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99979825">
In order to build robust automatic ab-
stracting systems, there is a need for bet-
ter training resources than are currently
available. In this paper, we introduce
an annotation scheme for scientific ar-
ticles which can be used to build such
a resource in a consistent way. The
seven categories of the scheme are based
on rhetorical moves of argumentation.
Our experimental results show that the
scheme is stable, reproducible and intu-
itive to use.
</bodyText>
<sectionHeader confidence="0.995487" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999888380952381">
Current approaches to automatic summariza-
tion cannot create coherent, flexible automatic
summaries. Sentence selection techniques (e.g.
Brandow et al., 1995; Kupiec et al. 1995) pro-
duce extracts which can be incoherent and which,
because of the generality of the methodology,
can give under-informative results; fact extrac-
tion techniques (e.g. Rau et al., 1989, Young and
Hayes, 1985) are tailored to particular domains,
but have not really scaled up from restricted texts
and restricted domains to larger domains and un-
restricted text. Sparck Jones (1998) argues that
taking into account the structure of a text will
help when summarizing the text.
The problem with sentence selection is that it
relies on extracting sentences out of context, but
the meaning of extracted material tends to depend
on where in the text the extracted sentence was
found. However, sentence selection still has the
distinct advantage of robustness.
We think sentence selection could be improved
substantially if the global rhetorical context of the
extracted material was taken into account more.
Marcu (1997) makes a similar point based on
rhetorical relations as defined by Rhetorical Struc-
ture Theory (RST, (Mann and Thompson, 1987)).
In contrast to this approach, we stress the impor-
tance of rhetorical moves which are global to the
argumentation of the paper, as opposed to local
RST—type moves. For example, sentences which
describe weaknesses of previous approaches can
provide a good characterization of the scientific
articles in which they occur, since they are likely
to also be a description of the problem that pa-
per is intending to solve. Take a sentence like
&amp;quot;Unfortunately, this work does not solve problem
X&amp;quot;: if X is a shortcoming in someone else&apos;s work,
this usually means that the current paper will try
to solve X. Sentence extraction methods can lo-
cate sentences like these, e.g. using a cue phrase
method (Paice, 1990).
But a very similar-looking sentence can play a
completely different argumentative role in a sci-
entific text: when it occurs in the section &amp;quot;Future
Work&amp;quot;, it might refer to a minor weakness in the
work presented in the source paper (i.e. of the au-
thor&apos;s own solution). In that case, the sentence is
not a good characterization of the paper.
Our approach to automatic text summarization
is to find important sentences in a source text by
determining their most likely argumentative role.
In order to create an automatic process to do so,
either by symbolic or machine learning techniques,
we need training material: a collection of texts (in
this case, scientific articles) where each sentence
is annotated with information about the argumen-
tative role that sentence plays in the paper. Cur-
rently, no such resource is available. We developed
an annotation scheme as a starting point for build-
ing up such a resource, which we will describe in
section 2. In section 3, we use content analysis
techniques to test the annotation scheme&apos;s relia-
bility.
</bodyText>
<sectionHeader confidence="0.768084" genericHeader="method">
2 The annotation scheme
</sectionHeader>
<bodyText confidence="0.977631666666667">
We wanted the scheme to cover one text type,
namely research articles, but from different pre-
sentational traditions and subject matters, so that
</bodyText>
<page confidence="0.993511">
110
</page>
<bodyText confidence="0.995815689189189">
Proceedings of EACL &apos;99
we can use it for text summarization in a range of
fields. This means we cannot rely on similarities
in external presentation, e.g. section structure and
typical linguistic formulaic expressions.
Previous discourse-level annotation schemes
(e.g. Liddy, 1991; Kircz, 1991) show that infor-
mation retrieval can profit from added rhetorical
information in scientific texts. However, the def-
initions of the categories in these schemes relies
on domain dependent knowledge like typical re-
search methodology, and are thus too specific for
our purposes.
General frameworks of text structure and argu-
mentation, like Cohen&apos;s (1984) theoretical frame-
work for general argumentation and Rhetorical
Structure Theory (Mann and Thompson, 1987),
are theoretically applicable to many different
kinds of text types. However, we believe that re-
stricting ourselves to the text type of research ar-
ticles will give us an advantage over such general
schemes, because it will allow us to rely on com-
municative goals typically occurring within that
text type.
Swales&apos; (1990) CARS (Creating a Research
Space) model provides a description at the right
level for our purposes. Swales claims that the
regularities in the argumentative structure of re-
search article introductions follow from the au-
thors&apos; primary communicative goal: namely to
convince their audience that they have provided
a contribution to science. From this goal follow
highly predictable subgoals which he calls argu-
mentative moves (&amp;quot;recurring and regularized com-
municative events&amp;quot;). An example for such a move
is &amp;quot;Indication of a gap&amp;quot;, where the author argues
that there is a weakness in an earlier approach
which needs to be solved.
Swales&apos; model has been used extensively by dis-
course analysts and researchers in the field of En-
glish for Specific Purposes, for tasks as varied as
teaching English as a foreign language, human
translation and citation analysis (Myers, 1992;
Thompson and Ye, 1991; Duszak, 1994), but al-
ways for manual analysis by a single person. Our
annotation scheme is based on Swales&apos; model but
we needed to modify it. Firstly, the CARS model
only applies to introductions of research articles,
so we needed new moves to cover the other paper
sections; secondly, we needed more precise guide-
lines to make the scheme applicable to reliable an-
notation for several non-discourse analysts (and
for potential automatic annotation).
For the development of our scheme, we used
computational linguistics articles. The papers in
our collection cover a challenging range of sub-
ject matters due to the interdisciplinarity of the
field, such as logic programming, statistical lan-
guage modelling, theoretical semantics and com-
putational psycholinguistics. Because the research
methodology and tradition of presentation is so
different in these fields, we would expect the
scheme to be equally applicable in a range of dis-
ciplines other than those named.
Our annotation scheme consists of the seven
categories shown in Figure 1. There are two ver-
sions of the annotation scheme. The basic scheme
provides a distinction between three textual seg-
ments which we think is a necessary precondi-
tion for argumentatively-justified summarization.
This distinction is concerned with the attribution
of authorship to scientific ideas and solutions de-
scribed in the text. Authors need to make clear,
and readers need to understand:
</bodyText>
<listItem confidence="0.999345142857143">
• which sections describe generally accepted
statements (BACKGROUND);
• which ideas are attributed to some other, spe-
cific piece of research outside the given paper,
including own previous work (OTHER);
• and which statements are the authors&apos; own
new contributions (OWN).
</listItem>
<bodyText confidence="0.999913428571429">
The full annotation scheme consists of the ba-
sic scheme plus four other categories, which are
based on Swales&apos; moves. The most important of
these is AIM (Swales&apos; move &amp;quot;Explicit statements
of research goal&amp;quot;), as these moves are good char-
acterizations of the entire paper. We are inter-
ested in how far humans can be trained to con-
sistently annotate these sentences; similar experi-
ments where subjects selected one or several &apos;most
relevant&apos; sentences from a paper have traditionally
reported low agreement (Rath et al., 1961). There
is also the category TEXTUAL ( Swales&apos; move &amp;quot;In-
dicate structure&amp;quot;), which provides helpful infor-
mation about section structure, and two moves
having to do with attitude towards previous re-
search, namely BASIS and CONTRAST.
The relative simplicity of the scheme was a com-
promise between two demands: we wanted the
scheme to contain enough information for auto-
matic summarization, but still be practicable for
hand coding.
Annotation proceeds sentence by sentence ac-
cording to the decision tree given in Figure 2. No
instructions about the use of cue phrases were
given, although some of the example sentences
given in the guidelines contained cue phrases. The
categorisation task resembles the judgements per-
formed e.g. in dialogue act coding (Carletta et al.,
</bodyText>
<page confidence="0.996834">
111
</page>
<table confidence="0.846965944444445">
Proceedings of EACL &apos;99
BASIC BACKGROUND Sentences describing some (generally accepted) background FULL
SCHEME knowledge SCHEME
OTHER Sentences describing aspects of some specific other research in a
neutral way (excluding contrastive or BASIS statements)
OWN Sentences describing any aspect of the own work presented in
this paper - except what is covered by AIM or TEXTUAL, e.g.
details of solution (methodology), limitations, and further work.
AIM Sentences best portraying the particular (main) research goal of
the article
TEXTUAL Explicit statements about the textual section structure of the
paper
CONTRAST Sentences contrasting own work to other work; sentences point-
ing out weaknesses in other research; sentences stating that the
research task of the current paper has never been done before;
direct comparisons
BASIS Statements that the own work uses some other work as its basis
or starting point, or gets support from this other work
</table>
<figureCaption confidence="0.999469">
Figure 1: Overview of the annotation scheme
</figureCaption>
<bodyText confidence="0.952965666666667">
1997; Alexandersson et al., 1995; Jurafsky et al.,
1997), but our task is more difficult since it re-
quires more subjective interpretation.
</bodyText>
<sectionHeader confidence="0.964996" genericHeader="method">
3 Annotation experiment
</sectionHeader>
<bodyText confidence="0.999478596491228">
Our annotation scheme is based on the intuition
that its categories provide an adequate and in-
tuitive description of scientific texts. But this
intuition alone is not enough of a justification:
we believe that our claims, like claims about any
other descriptive account of textual interpreta-
tion, should be substantiated by demonstrating
that other humans can apply this interpretation
consistently to actual texts.
We did three studies. Study I and II were de-
signed to find out if the two versions of the an-
notation scheme (basic vs. full) can be learned by
human coders with a significant amount of train-
ing. We are interested in two formal properties of
the annotation scheme: stability and reproducibil-
ity (Krippendorff, 1980). Stability, the extent to
which one annotator will produce the same classi-
fications at different times, is important because
an instable annotation scheme can never be re-
producible. Reproducibility, the extent to which
different annotators will produce the same clas-
sifications, is important because it measures the
consistency of shared understandings (or mean-
ing) held between annotators.
We use the Kappa coefficient K (Siegel and
Castellan, 1988) to measure stability and repro-
ducibility among k annotators on N items. In
our experiment, the items are sentences. Kappa
is a better measurement of agreement than raw
percentage agreement (Carletta, 1996) because it
factors out the level of agreement which would
be reached by random annotators using the same
distribution of categories as the real coders. No
matter how many items or annotators, or how the
categories are distributed, K=0 when there is no
agreement other than what would be expected by
chance, and K=1 when agreement is perfect. We
expect high random agreement for our annotation
scheme because so many sentences fall into the
OWN category.
Studies I and II will determine how far we can
trust in the human-annotated training material
for both learning and evaluation of the automatic
method. The outcome of Study II (full annota-
tion scheme) is crucial to the task, as some of the
categories specific to the full annotation scheme
(particularly ADA) add considerable value to the
information contained in the training material.
Study III tries to answer the question whether
the considerable training effort used in Studies I
and II can be reduced. If it were the case that
coders with hardly any task-specific training can
produce similar results to highly trained coders,
the training material could be acquired in a more
efficient way. A positive outcome of Study III
would also strengthen claims about the intuitivity
of the category definitions.
</bodyText>
<page confidence="0.98996">
112
</page>
<table confidence="0.869573090909091">
Proceedings of EACL &apos;99
Does this sentence refer to own
work (excluding previous work
of the same author)?
YE NO
Does this sentence contain material
that describes the specific aim
described in the paper?
Does the sentence describe general
background, including phenomena
to be explained or linguistic example sentences?
YES NO YES NO
AIM Does this sentence make BACKGROUND Does it describe a negative aspect
reference to the structure of the other work, or a contrast
of the paper? or comparison of the own work to it?
YES NO YES NO
TEXTUAL
OWN CONTRAST Does this sentence mention
the other work as basis of
or support for own work?
YES NO
OTHER
</table>
<figureCaption confidence="0.997093">
Figure 2: Decision tree for annotation
</figureCaption>
<bodyText confidence="0.999740111111111">
Our materials consist of 48 computational lin-
guistics papers (22 for Study I, 26 for Study II),
taken from the Computation and Language E-
Print Archive (http://xxx.lanl.gov/cmp-lg/).
We chose papers that had been presented at COL-
ING, ANLP or ACL conferences (including stu-
dent sessions), or ACL-sponsored workshops, and
been put onto the archive between April 1994 and
April 1995.
</bodyText>
<subsectionHeader confidence="0.999762">
3.1 Studies I and II
</subsectionHeader>
<bodyText confidence="0.999967471698113">
For Studies I and II, we used three highly trained
annotators. The annotators (two graduate stu-
dents and the first author) can be considered
skilled at extracting information from scientific
papers but they were not experts in all of the sub-
domains of the papers they annotated. The anno-
tators went through a substantial amount of train-
ing, including the reading of coding instructions
for the two versions of the scheme (6 pages for the
basic scheme and 17 pages for the full scheme),
four training papers and weekly discussions, in
which previous annotations were discussed. How-
ever, annotators were not allowed to change any
previous decisions. For the stability figures (intra-
annotator agreement), annotators re-coded 6 ran-
domly chosen papers 6 weeks after the end of the
annotation experiment. Skim-reading and anno-
tation of an average length paper (3800 words)
typically took the annotators 20-30 minutes.
During the annotation phase, one of the pa-
pers turned out to be a review paper. This paper
caused the annotators difficulty as the scheme was
not intended to cover reviews. Thus, we discarded
this paper from the analysis.
The results show that the basic annotation
scheme is stable (K=.83, .79, .81; N=1248; k=2
for all three annotators) and reproducible (K=.78,
N=4031, k=3). This reconfirms that trained an-
notators are capable of making the basic dis-
tinction between own work, specific other work,
and general background. The full annotation
scheme is stable (K=.82, .81, .76; N=1220; k=2
for all three annotators) and reproducible (K=.71,
N=4261, k=3). Because of the increased cogni-
tive difficulty of the task, the decrease in stability
and reproducibility in comparison to Study I is
acceptable. Leaving the coding developer out of
the coder pool for Study II did not change the re-
sults (K=.71, N=4261, k=2), suggesting that the
training conveyed her intentions fairly well.
We collected informal comments from our an-
notators about how natural the task felt, but did
not conduct a formal evaluation of subjective per-
ception of the difficulty of the task. As a general
approach in our analysis, we wanted to look at the
trends in the data as our main information source.
Figure 3 reports how well the four non-basic cat-
egories could be distinguished from all other cat-
egories, measured by Krippendorff&apos;s diagnostics
for category distinctions (i.e. collapsing all other
distinctions). When compared to the overall re-
producibility of .71, we notice that the annota-
tors were good at distinguishing AIM and TEX-
</bodyText>
<page confidence="0.996781">
113
</page>
<figure confidence="0.992029411764706">
Proceedings of EACL &apos;99
0.9
0.8
K 0.7
0.6
0.5
0.8
0.7
0.6
0.5
K 0.4
0.3
0.2
0.1
0
CONTRAST AIM BASIS TEXTUAL
none low high
</figure>
<figureCaption confidence="0.9831805">
Figure 3: Reproducibility diagnostics: non-basic
categories (Study II)
</figureCaption>
<figure confidence="0.997977142857143">
4
a)
cci 3
02
z 1 -
0
03
</figure>
<figureCaption confidence="0.9674565">
Figure 4: Distribution by reproducibility (Study
II)
</figureCaption>
<bodyText confidence="0.99984675">
TUAL. This is an important result: as ADA sen-
tences constitute the best characterization of the
research paper for the summarization task we are
particularly interested in having them annotated
consistently in our training material. The anno-
tators were less good at determining BASIS and
CONTRAST. This might have to do with the loca-
tion of those types of sentences in the paper: AIM
and TEXTUAL are usually found at the beginning
or end of the introduction section, whereas CON-
TRAST, and even more so BASIS, are usually in-
terspersed within longer stretches of OWN. As a
result, these categories are more exposed to lapses
of attention during annotation.
If we blur the less important distinctions be-
tween CONTRAST, OTHER, and BACKGROUND,
the reproducibility of the scheme increases to
K=.75. Structuring our training set in this way
seems to be a good compromise for our task, be-
cause with high reliability, it would still give us
the crucial distinctions contained in the basic an-
notation scheme, plus the highly important AIM
sentences, plus the useful TEXTUAL and BASIS
sentences.
The variation in reproducibility across papers is
large, both in Study I and Study II (cf. the quasi-
bimodal distribution shown in Figure 4). Some
hypotheses for why this might be so are the fol-
</bodyText>
<figureCaption confidence="0.995138">
Figure 5: Effect of self-citation ratio on repro-
ducibility (Study I)
</figureCaption>
<bodyText confidence="0.570564">
lowing:
</bodyText>
<listItem confidence="0.6534485">
• One problem our annotators reported was a
difficulty in distinguishing OTHER work from
</listItem>
<bodyText confidence="0.939462888888889">
OWN work, due to the fact that some authors
did not express a clear distinction between
previous own work (which, according to our
instructions, had to be coded as OTHER) and
current, new work. This was particularly the
case where authors had published several pa-
pers about different aspects of one piece of
research. We found a correlation with self ci-
tation ratio (ratio of self citations to all cita-
tions in running text): papers with many self
citations are more difficult to annotate than
papers that have few or no self citations (cf.
Figure 5).
• Another persistent problematic distinction
for our annotators was that between OWN
and BACKGROUND. This could be a sign that
some authors aimed their papers at an expert
audience, and thus thought it unnecessary to
signal clearly which statements are commonly
agreed in the field, as opposed to their own
new claims. If a paper is written in such a
way, it can indeed only be understood with
a considerable amount of domain knowledge,
which our annotators did not have.
• There is also a difference in reproducibil-
ity between papers from different conference
types, as Figure 6 suggests. Out of our 25 pa-
pers, 4 were presented in student sessions, 4
came from workshops, the remaining 16 ones
were main conference papers. Student session
papers are easiest to annotate, which might
be due to the fact that they are shorter and
have a simpler structure, with less mentions
of previous research. Main conference pa-
pers dedicate more space to describing and
0.4 05 06 07 08 09
</bodyText>
<page confidence="0.956646">
114
</page>
<figure confidence="0.441358">
Proceedings of EACL &apos;99
</figure>
<figureCaption confidence="0.9978645">
Figure 6: Effect of conference type on repro-
ducibility (Study II)
</figureCaption>
<bodyText confidence="0.999808375">
criticising other people&apos;s work than student
or workshop papers (on average about one
fourth of the paper). They seem to be care-
fully prepared (and thus easy to annotate);
conference authors must express themselves
more clearly than workshop authors because
they are reporting finished work to a wider
audience.
</bodyText>
<subsectionHeader confidence="0.999324">
3.2 Study III
</subsectionHeader>
<bodyText confidence="0.999941413043478">
For Study III, we used a different subject pool:
18 subjects with no prior annotation training. All
of them had a graduate degree in Cognitive Sci-
ence, with two exceptions: one was a graduate
student in Sociology of Science, and one was a sec-
retary. Subjects were given only minimal instruc-
tions (1 page A4), and the decision tree in Fig-
ure 2. Each annotator was randomly assigned to a
group of six, all of whom independently annotated
the same single paper. These three papers were
randomly chosen from the set of papers for which
our trained annotators had previously achieved
good reproducibility in Study II (K=.65,N=205,
k=3; K=.85,N=192,k=3; K=.87,N=144,k=3, re-
spectively).
Reproducibility varied considerably between
groups (K=.35, N=205, k=6; K=.49, N=192,
k=6; K=.72, N=144, k=6). Kappa is designed
to abstract over the number of coders. Lower reli-
ablity for Study III as compared to Studies I and
II is not an artefact of how K was calculated.
Some subjects in Group 1 and 2 did not un-
derstand the instructions as intended - we must
conclude that our very short instructions did not
provide enough information for consistent anno-
tation. This is not surprising, given that human
indexers (whose task is very similar to the task
introduced here) are highly skilled professionals.
However, part of this result can be attributed to
the papers: Group 3, which annotated the pa-
per found to be most reproducible in Study II,
performed almost as well as trained annotators;
Group 1, which performed worst, also happened
to have the paper with the lowest reproducibil-
ity. In Groups 1 and 2, the most similar three
annotators reached a respectable reproducibility
(K=.5, N=205, k=3; K=.63, N=192, k=3). That,
together with the good performance of Group 3,
seems to show that the instructions did at least
convey some of the meaning of the categories.
It is remarkable that the two subjects who had
no training in computational linguistics performed
reasonably well: they were not part of the circle
of the three most similar subjects in their groups,
but they were also not performing worse than the
other two annotators.
</bodyText>
<sectionHeader confidence="0.997859" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999737342105263">
It is an interesting question how far shallow (hu-
man and automatic) information extraction meth-
ods, i.e. those using no domain knowledge, can be
successful in a task such as ours. We believe that
argumentative structure has so many reliable lin-
guistic or non-linguistic correlates on the surface
- physical layout being one of these correlates,
others are linguistic indicators like &amp;quot;to our knowl-
edge&amp;quot; and the relative order of the individual ar-
gumentative moves - that it should be possible to
detect the line of argumentation of a text without
much world knowledge. The two non-experts in
the subject pool of Study III, who must have used
some other information besides computational lin-
guistics knowledge, performed satisfactorily - a
fact that seems to confirm the promise of shallow
methods..
Overall, reproducibility and stability for trained
annotators does not quite reach the levels found
for, for instance, the best dialogue act coding
schemes (around K=.80). Our annotation re-
quires more subjective judgments and is possi-
bly more cognitively complex. Our reproducibility
and stability results are in the range which Krip-
pendorff (1980) describes as giving marginally sig-
nificant results for reasonable size data sets when
correlating two coded variables which would show
a clear correlation if there were prefectly agree-
ment. That is, the coding contains enough signal
to be found among the noise of disagreement.
Of course, our requirements are rather less
stringent than Krippendorff&apos;s because only one
coded variable is involved, although coding is ex-
pensive enough that simply building larger data
sets is not an attractive option. Overall, we find
the level of agreement which we achieved accept-
able. However, as with all coding schemes, its
usefulness will only be clarified by the final appli-
</bodyText>
<figure confidence="0.9941848">
0.8
0.7
0.6
0.5
Mae&amp;quot; cont. Student Workshop
</figure>
<page confidence="0.957667">
115
</page>
<bodyText confidence="0.984880958333333">
Proceedings of EACL &apos;99
cation.
The single most surprising result of the experi-
ments is the large variation in reproducibility be-
tween papers. Intuitively, the reason for this are
qualitative differences in individual writing style
- annotators reported that some papers are bet-
ter structured and better written than others, and
that some authors tend to write more clearly than
others. It would be interesting to compare our re-
producibility results to independent quality judge-
ments of the papers, in order to determine if our
experiments can indeed measure the clarity of sci-
entific argumentation.
Most of the problems we identified in our stud-
ies have to do with a lack of distinction between
own and other people&apos;s work (or own previous
work). Because our scheme discriminates based
on these properties, as well as being useful for
summarizing research papers, it might be used for
automatically detecting whether a paper is a re-
view, a position paper, an evaluation paper or a
&apos;pure&apos; research article by looking at the relative
frequencies of automatically annotated categories.
</bodyText>
<sectionHeader confidence="0.999219" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999529">
We have introduced an annotation scheme for re-
search articles which marks the aims of the pa-
per in relation to past literature. We have ar-
gued that this scheme is useful for building better
abstracts, and have conducted some experiments
which show that the annotation scheme can be
learned by trained annotators and subsequently
applied in a consistent way. Because the scheme
is reliable, hand-annotated data can be used to
train a system which applies the scheme automat-
ically to unseen text.
The novel aspects of our scheme are that it ap-
plies to different kinds of scientific research arti-
cles, because it relies on the form and meaning
of argumentative aspects found in the text type
rather than on contents or physical format. As
such, it should be independent of article length
and article discipline. In the future, we plan
to show this by applying our scheme to journal
and conference articles from a range of disciplines.
Practical reasons have kept us from using journal
articles as data so far (namely the difficulty of cor-
pus collection and the increased length and subse-
quent time effort of human experiments), but we
are particularly interested in them as they can be
expected to be of higher quality. As the basic ar-
gumentation is the same as in conference articles,
our scheme should be applicable to journal arti-
cles at least as consistently as to the papers in our
current collection.
</bodyText>
<sectionHeader confidence="0.996086" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999891">
We wish to thank our annotators, Vasilis
Karaiskos and Ann Wilson, for their patience and
diligence in this work, and for their insightful, crit-
ical, and very useful observations.
The first author is supported by an EPSRC stu-
dentship.
</bodyText>
<sectionHeader confidence="0.998402" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999646302325581">
Jan Alexandersson, Elisabeth Maier, and Norbert Re-
ithinger. 1995. A robust and efficient three-layered
dialogue component for a speech-to-speech transla-
tion system. In Proceedings of the Seventh Euro-
pean Meeting of the ACL, pages 188-193.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications
by sentence selection. Information Processing and
Management, 31 (5):675-685.
Jean Carletta, Amy Isard, Stephen Isard, Jacque-
line C. Kowtko, Gwyneth Doherty-Sneddon, and
Anne H. Anderson. 1997. The reliability of a dia-
logue structure coding scheme. Computational Lin-
guistics, 23(1):13-31.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
Linguistics, 22(2):249-254.
Robin Cohen. 1984. A computational theory of the
function of clue words in argument understanding.
In Proceedings of COLING-84, pages 251-255.
Anna Duszak. 1994. Academic discourse and intellec-
tual styles. Journal of Pragmatics, 21:291-313.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca, 1997. Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual.
University of Colorado, Institute of Cognitive Sci-
ence. TR-97-02.
Joost G. Kircz. 1991. The rhetorical structure of sci-
entific articles: the case for argumentational analy-
sis in information retrieval. Journal of Documenta-
tion, 47(4):354-372.
Klaus Krippendorff. 1980. Content analysis: an in-
troduction to its methodology. Sage Commtext se-
ries; 5. Sage, Beverly Hills London.
Julian Kupiec, Jan 0. Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Pro-
ceedings of the 18th ACM-SIGIR Conference, Asso-
ciation for Computing Machinery, Special Interest
Group Information Retrieval, pages 68-73.
Elizabeth DuRoss Liddy. 1991. The discourse-level
structure of empirical abstracts: an exploratory
study. Information Processing and Management,
27(1):55-81.
</reference>
<page confidence="0.989857">
116
</page>
<reference confidence="0.999324976190476">
Proceedings of EACL &apos;99
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: description and con-
struction of text structures. In G. Kempen, edi-
tor, Natural Language Generation: New Results in
Artificial Intelligence, Psychology and Linguistics,
pages 85-95, Dordrecht. Nijhoff.
Daniel Marcu. 1997. From discourse structures to
text summaries. In Inderjeet Mani and Mark T.
Maybury, editors, Proceedings of the workshop on
Intelligent Scalable Text Summarization, in associ-
ation with ACL/EACL-97.
Greg Myers. 1992. In this paper we report... - speech
acts and scientific facts. Journal of Pragmatics,
17(4):295-313.
Chris D. Paice. 1990. Constructing literature ab-
stracts by computer: techniques and prospects.
Information Processing and Management, 26:171-
186.
G.J Rath, A. Resnick, and T. R. Savage. 1961. The
formation of abstracts by the selection of sentences.
American Documentation, 12(2):139-143.
Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989.
Information extraction and text processing using
linguistic knowledge acquisition. Information Pro-
cessing and Management, 25(4):419-428.
Sidney Siegel and N.J. Jr. Castellan. 1988. Non-
parametric statistics for the Behavioral Sciences.
McGraw-Hill, second edition edition.
Karen Sparck Jones. 1998. Automatic summarising:
factors and directions. In ACL/EACL-97 Work-
shop &apos;Intelligent Scalable Text Summarization&apos;.
John Swales. 1990. Genre analysis: English in aca-
demic and research settings. Cambridge University
Press.
Geoff Thompson and Yiyun Ye. 1991. Evaluation in
the reporting verbs used in academic papers. Ap-
plied Linguistics, 12(4):365-382.
Sheryl R. Young and Phillip J. Hayes. 1985. Auto-
matic classification and summarization of banking
telexes. In Proceedings of the Second Conference on
Artificial Intelligence Applications.
</reference>
<page confidence="0.998141">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.453285">
<note confidence="0.588077">Proceedings of EACL &apos;99</note>
<title confidence="0.982836">An annotation scheme for discourse-level argumentation in research articles</title>
<author confidence="0.917377">Teufelt Carlettat Moenst</author>
<affiliation confidence="0.9812345">IFICRC Language Technology Group and t Human Communication Research Centre Division of Informatics University of Edinburgh</affiliation>
<email confidence="0.994378">S.Teufel@ed.ac.uk,J.Carletta@ed.ac.uk,M.Moens@ed.ac.uk</email>
<abstract confidence="0.991497461538462">In order to build robust automatic abstracting systems, there is a need for better training resources than are currently available. In this paper, we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way. The seven categories of the scheme are based on rhetorical moves of argumentation. Our experimental results show that the scheme is stable, reproducible and intuitive to use.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jan Alexandersson</author>
<author>Elisabeth Maier</author>
<author>Norbert Reithinger</author>
</authors>
<title>A robust and efficient three-layered dialogue component for a speech-to-speech translation system.</title>
<date>1995</date>
<booktitle>In Proceedings of the Seventh European Meeting of the ACL,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="9921" citStr="Alexandersson et al., 1995" startWordPosition="1558" endWordPosition="1561">ion (methodology), limitations, and further work. AIM Sentences best portraying the particular (main) research goal of the article TEXTUAL Explicit statements about the textual section structure of the paper CONTRAST Sentences contrasting own work to other work; sentences pointing out weaknesses in other research; sentences stating that the research task of the current paper has never been done before; direct comparisons BASIS Statements that the own work uses some other work as its basis or starting point, or gets support from this other work Figure 1: Overview of the annotation scheme 1997; Alexandersson et al., 1995; Jurafsky et al., 1997), but our task is more difficult since it requires more subjective interpretation. 3 Annotation experiment Our annotation scheme is based on the intuition that its categories provide an adequate and intuitive description of scientific texts. But this intuition alone is not enough of a justification: we believe that our claims, like claims about any other descriptive account of textual interpretation, should be substantiated by demonstrating that other humans can apply this interpretation consistently to actual texts. We did three studies. Study I and II were designed to</context>
</contexts>
<marker>Alexandersson, Maier, Reithinger, 1995</marker>
<rawString>Jan Alexandersson, Elisabeth Maier, and Norbert Reithinger. 1995. A robust and efficient three-layered dialogue component for a speech-to-speech translation system. In Proceedings of the Seventh European Meeting of the ACL, pages 188-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Brandow</author>
<author>Karl Mitze</author>
<author>Lisa F Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1995</date>
<journal>Information Processing and Management,</journal>
<volume>31</volume>
<pages>5--675</pages>
<contexts>
<context position="954" citStr="Brandow et al., 1995" startWordPosition="134" endWordPosition="137">ract In order to build robust automatic abstracting systems, there is a need for better training resources than are currently available. In this paper, we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way. The seven categories of the scheme are based on rhetorical moves of argumentation. Our experimental results show that the scheme is stable, reproducible and intuitive to use. 1 Introduction Current approaches to automatic summarization cannot create coherent, flexible automatic summaries. Sentence selection techniques (e.g. Brandow et al., 1995; Kupiec et al. 1995) produce extracts which can be incoherent and which, because of the generality of the methodology, can give under-informative results; fact extraction techniques (e.g. Rau et al., 1989, Young and Hayes, 1985) are tailored to particular domains, but have not really scaled up from restricted texts and restricted domains to larger domains and unrestricted text. Sparck Jones (1998) argues that taking into account the structure of a text will help when summarizing the text. The problem with sentence selection is that it relies on extracting sentences out of context, but the mea</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Information Processing and Management, 31 (5):675-685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>Amy Isard</author>
<author>Stephen Isard</author>
<author>Jacqueline C Kowtko</author>
<author>Gwyneth Doherty-Sneddon</author>
<author>Anne H Anderson</author>
</authors>
<title>The reliability of a dialogue structure coding scheme.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<marker>Carletta, Isard, Isard, Kowtko, Doherty-Sneddon, Anderson, 1997</marker>
<rawString>Jean Carletta, Amy Isard, Stephen Isard, Jacqueline C. Kowtko, Gwyneth Doherty-Sneddon, and Anne H. Anderson. 1997. The reliability of a dialogue structure coding scheme. Computational Linguistics, 23(1):13-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="11439" citStr="Carletta, 1996" startWordPosition="1798" endWordPosition="1799">ator will produce the same classifications at different times, is important because an instable annotation scheme can never be reproducible. Reproducibility, the extent to which different annotators will produce the same classifications, is important because it measures the consistency of shared understandings (or meaning) held between annotators. We use the Kappa coefficient K (Siegel and Castellan, 1988) to measure stability and reproducibility among k annotators on N items. In our experiment, the items are sentences. Kappa is a better measurement of agreement than raw percentage agreement (Carletta, 1996) because it factors out the level of agreement which would be reached by random annotators using the same distribution of categories as the real coders. No matter how many items or annotators, or how the categories are distributed, K=0 when there is no agreement other than what would be expected by chance, and K=1 when agreement is perfect. We expect high random agreement for our annotation scheme because so many sentences fall into the OWN category. Studies I and II will determine how far we can trust in the human-annotated training material for both learning and evaluation of the automatic m</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22(2):249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cohen</author>
</authors>
<title>A computational theory of the function of clue words in argument understanding.</title>
<date>1984</date>
<booktitle>In Proceedings of COLING-84,</booktitle>
<pages>251--255</pages>
<marker>Cohen, 1984</marker>
<rawString>Robin Cohen. 1984. A computational theory of the function of clue words in argument understanding. In Proceedings of COLING-84, pages 251-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Duszak</author>
</authors>
<title>Academic discourse and intellectual styles.</title>
<date>1994</date>
<journal>Journal of Pragmatics,</journal>
<pages>21--291</pages>
<contexts>
<context position="5917" citStr="Duszak, 1994" startWordPosition="933" endWordPosition="934"> they have provided a contribution to science. From this goal follow highly predictable subgoals which he calls argumentative moves (&amp;quot;recurring and regularized communicative events&amp;quot;). An example for such a move is &amp;quot;Indication of a gap&amp;quot;, where the author argues that there is a weakness in an earlier approach which needs to be solved. Swales&apos; model has been used extensively by discourse analysts and researchers in the field of English for Specific Purposes, for tasks as varied as teaching English as a foreign language, human translation and citation analysis (Myers, 1992; Thompson and Ye, 1991; Duszak, 1994), but always for manual analysis by a single person. Our annotation scheme is based on Swales&apos; model but we needed to modify it. Firstly, the CARS model only applies to introductions of research articles, so we needed new moves to cover the other paper sections; secondly, we needed more precise guidelines to make the scheme applicable to reliable annotation for several non-discourse analysts (and for potential automatic annotation). For the development of our scheme, we used computational linguistics articles. The papers in our collection cover a challenging range of subject matters due to the</context>
</contexts>
<marker>Duszak, 1994</marker>
<rawString>Anna Duszak. 1994. Academic discourse and intellectual styles. Journal of Pragmatics, 21:291-313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL ShallowDiscourse-Function Annotation Coders</title>
<date>1997</date>
<pages>97--02</pages>
<institution>Manual. University of Colorado, Institute of Cognitive Science.</institution>
<contexts>
<context position="9945" citStr="Jurafsky et al., 1997" startWordPosition="1562" endWordPosition="1565">ns, and further work. AIM Sentences best portraying the particular (main) research goal of the article TEXTUAL Explicit statements about the textual section structure of the paper CONTRAST Sentences contrasting own work to other work; sentences pointing out weaknesses in other research; sentences stating that the research task of the current paper has never been done before; direct comparisons BASIS Statements that the own work uses some other work as its basis or starting point, or gets support from this other work Figure 1: Overview of the annotation scheme 1997; Alexandersson et al., 1995; Jurafsky et al., 1997), but our task is more difficult since it requires more subjective interpretation. 3 Annotation experiment Our annotation scheme is based on the intuition that its categories provide an adequate and intuitive description of scientific texts. But this intuition alone is not enough of a justification: we believe that our claims, like claims about any other descriptive account of textual interpretation, should be substantiated by demonstrating that other humans can apply this interpretation consistently to actual texts. We did three studies. Study I and II were designed to find out if the two ver</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca, 1997. Switchboard SWBD-DAMSL ShallowDiscourse-Function Annotation Coders Manual. University of Colorado, Institute of Cognitive Science. TR-97-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joost G Kircz</author>
</authors>
<title>The rhetorical structure of scientific articles: the case for argumentational analysis in information retrieval.</title>
<date>1991</date>
<journal>Journal of Documentation,</journal>
<pages>47--4</pages>
<contexts>
<context position="4235" citStr="Kircz, 1991" startWordPosition="669" endWordPosition="670"> a resource, which we will describe in section 2. In section 3, we use content analysis techniques to test the annotation scheme&apos;s reliability. 2 The annotation scheme We wanted the scheme to cover one text type, namely research articles, but from different presentational traditions and subject matters, so that 110 Proceedings of EACL &apos;99 we can use it for text summarization in a range of fields. This means we cannot rely on similarities in external presentation, e.g. section structure and typical linguistic formulaic expressions. Previous discourse-level annotation schemes (e.g. Liddy, 1991; Kircz, 1991) show that information retrieval can profit from added rhetorical information in scientific texts. However, the definitions of the categories in these schemes relies on domain dependent knowledge like typical research methodology, and are thus too specific for our purposes. General frameworks of text structure and argumentation, like Cohen&apos;s (1984) theoretical framework for general argumentation and Rhetorical Structure Theory (Mann and Thompson, 1987), are theoretically applicable to many different kinds of text types. However, we believe that restricting ourselves to the text type of researc</context>
</contexts>
<marker>Kircz, 1991</marker>
<rawString>Joost G. Kircz. 1991. The rhetorical structure of scientific articles: the case for argumentational analysis in information retrieval. Journal of Documentation, 47(4):354-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content analysis: an introduction to its methodology. Sage Commtext series; 5. Sage,</title>
<date>1980</date>
<location>Beverly Hills London.</location>
<contexts>
<context position="10782" citStr="Krippendorff, 1980" startWordPosition="1699" endWordPosition="1700">on of scientific texts. But this intuition alone is not enough of a justification: we believe that our claims, like claims about any other descriptive account of textual interpretation, should be substantiated by demonstrating that other humans can apply this interpretation consistently to actual texts. We did three studies. Study I and II were designed to find out if the two versions of the annotation scheme (basic vs. full) can be learned by human coders with a significant amount of training. We are interested in two formal properties of the annotation scheme: stability and reproducibility (Krippendorff, 1980). Stability, the extent to which one annotator will produce the same classifications at different times, is important because an instable annotation scheme can never be reproducible. Reproducibility, the extent to which different annotators will produce the same classifications, is important because it measures the consistency of shared understandings (or meaning) held between annotators. We use the Kappa coefficient K (Siegel and Castellan, 1988) to measure stability and reproducibility among k annotators on N items. In our experiment, the items are sentences. Kappa is a better measurement of</context>
<context position="23266" citStr="Krippendorff (1980)" startWordPosition="3762" endWordPosition="3764"> of a text without much world knowledge. The two non-experts in the subject pool of Study III, who must have used some other information besides computational linguistics knowledge, performed satisfactorily - a fact that seems to confirm the promise of shallow methods.. Overall, reproducibility and stability for trained annotators does not quite reach the levels found for, for instance, the best dialogue act coding schemes (around K=.80). Our annotation requires more subjective judgments and is possibly more cognitively complex. Our reproducibility and stability results are in the range which Krippendorff (1980) describes as giving marginally significant results for reasonable size data sets when correlating two coded variables which would show a clear correlation if there were prefectly agreement. That is, the coding contains enough signal to be found among the noise of disagreement. Of course, our requirements are rather less stringent than Krippendorff&apos;s because only one coded variable is involved, although coding is expensive enough that simply building larger data sets is not an attractive option. Overall, we find the level of agreement which we achieved acceptable. However, as with all coding s</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content analysis: an introduction to its methodology. Sage Commtext series; 5. Sage, Beverly Hills London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th ACM-SIGIR Conference, Association for Computing Machinery, Special Interest Group Information Retrieval,</booktitle>
<pages>68--73</pages>
<marker>Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan 0. Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the 18th ACM-SIGIR Conference, Association for Computing Machinery, Special Interest Group Information Retrieval, pages 68-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth DuRoss Liddy</author>
</authors>
<title>The discourse-level structure of empirical abstracts: an exploratory study.</title>
<date>1991</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>27--1</pages>
<contexts>
<context position="4221" citStr="Liddy, 1991" startWordPosition="667" endWordPosition="668">lding up such a resource, which we will describe in section 2. In section 3, we use content analysis techniques to test the annotation scheme&apos;s reliability. 2 The annotation scheme We wanted the scheme to cover one text type, namely research articles, but from different presentational traditions and subject matters, so that 110 Proceedings of EACL &apos;99 we can use it for text summarization in a range of fields. This means we cannot rely on similarities in external presentation, e.g. section structure and typical linguistic formulaic expressions. Previous discourse-level annotation schemes (e.g. Liddy, 1991; Kircz, 1991) show that information retrieval can profit from added rhetorical information in scientific texts. However, the definitions of the categories in these schemes relies on domain dependent knowledge like typical research methodology, and are thus too specific for our purposes. General frameworks of text structure and argumentation, like Cohen&apos;s (1984) theoretical framework for general argumentation and Rhetorical Structure Theory (Mann and Thompson, 1987), are theoretically applicable to many different kinds of text types. However, we believe that restricting ourselves to the text t</context>
</contexts>
<marker>Liddy, 1991</marker>
<rawString>Elizabeth DuRoss Liddy. 1991. The discourse-level structure of empirical abstracts: an exploratory study. Information Processing and Management, 27(1):55-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: description and construction of text structures.</title>
<date>1987</date>
<booktitle>Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics,</booktitle>
<pages>85--95</pages>
<editor>In G. Kempen, editor,</editor>
<location>Dordrecht. Nijhoff.</location>
<contexts>
<context position="2014" citStr="Mann and Thompson, 1987" startWordPosition="302" endWordPosition="305">e structure of a text will help when summarizing the text. The problem with sentence selection is that it relies on extracting sentences out of context, but the meaning of extracted material tends to depend on where in the text the extracted sentence was found. However, sentence selection still has the distinct advantage of robustness. We think sentence selection could be improved substantially if the global rhetorical context of the extracted material was taken into account more. Marcu (1997) makes a similar point based on rhetorical relations as defined by Rhetorical Structure Theory (RST, (Mann and Thompson, 1987)). In contrast to this approach, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to local RST—type moves. For example, sentences which describe weaknesses of previous approaches can provide a good characterization of the scientific articles in which they occur, since they are likely to also be a description of the problem that paper is intending to solve. Take a sentence like &amp;quot;Unfortunately, this work does not solve problem X&amp;quot;: if X is a shortcoming in someone else&apos;s work, this usually means that the current paper will try to solve X.</context>
<context position="4691" citStr="Mann and Thompson, 1987" startWordPosition="734" endWordPosition="737"> external presentation, e.g. section structure and typical linguistic formulaic expressions. Previous discourse-level annotation schemes (e.g. Liddy, 1991; Kircz, 1991) show that information retrieval can profit from added rhetorical information in scientific texts. However, the definitions of the categories in these schemes relies on domain dependent knowledge like typical research methodology, and are thus too specific for our purposes. General frameworks of text structure and argumentation, like Cohen&apos;s (1984) theoretical framework for general argumentation and Rhetorical Structure Theory (Mann and Thompson, 1987), are theoretically applicable to many different kinds of text types. However, we believe that restricting ourselves to the text type of research articles will give us an advantage over such general schemes, because it will allow us to rely on communicative goals typically occurring within that text type. Swales&apos; (1990) CARS (Creating a Research Space) model provides a description at the right level for our purposes. Swales claims that the regularities in the argumentative structure of research article introductions follow from the authors&apos; primary communicative goal: namely to convince their </context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical structure theory: description and construction of text structures. In G. Kempen, editor, Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, pages 85-95, Dordrecht. Nijhoff.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From discourse structures to text summaries.</title>
<date>1997</date>
<booktitle>In Inderjeet Mani and</booktitle>
<editor>Mark T. Maybury, editors,</editor>
<contexts>
<context position="1888" citStr="Marcu (1997)" startWordPosition="284" endWordPosition="285">restricted domains to larger domains and unrestricted text. Sparck Jones (1998) argues that taking into account the structure of a text will help when summarizing the text. The problem with sentence selection is that it relies on extracting sentences out of context, but the meaning of extracted material tends to depend on where in the text the extracted sentence was found. However, sentence selection still has the distinct advantage of robustness. We think sentence selection could be improved substantially if the global rhetorical context of the extracted material was taken into account more. Marcu (1997) makes a similar point based on rhetorical relations as defined by Rhetorical Structure Theory (RST, (Mann and Thompson, 1987)). In contrast to this approach, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to local RST—type moves. For example, sentences which describe weaknesses of previous approaches can provide a good characterization of the scientific articles in which they occur, since they are likely to also be a description of the problem that paper is intending to solve. Take a sentence like &amp;quot;Unfortunately, this work does not </context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. From discourse structures to text summaries. In Inderjeet Mani and Mark T. Maybury, editors, Proceedings of the workshop on Intelligent Scalable Text Summarization, in association with ACL/EACL-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Myers</author>
</authors>
<title>In this paper we report... - speech acts and scientific facts.</title>
<date>1992</date>
<journal>Journal of Pragmatics,</journal>
<pages>17--4</pages>
<contexts>
<context position="5879" citStr="Myers, 1992" startWordPosition="927" endWordPosition="928">mely to convince their audience that they have provided a contribution to science. From this goal follow highly predictable subgoals which he calls argumentative moves (&amp;quot;recurring and regularized communicative events&amp;quot;). An example for such a move is &amp;quot;Indication of a gap&amp;quot;, where the author argues that there is a weakness in an earlier approach which needs to be solved. Swales&apos; model has been used extensively by discourse analysts and researchers in the field of English for Specific Purposes, for tasks as varied as teaching English as a foreign language, human translation and citation analysis (Myers, 1992; Thompson and Ye, 1991; Duszak, 1994), but always for manual analysis by a single person. Our annotation scheme is based on Swales&apos; model but we needed to modify it. Firstly, the CARS model only applies to introductions of research articles, so we needed new moves to cover the other paper sections; secondly, we needed more precise guidelines to make the scheme applicable to reliable annotation for several non-discourse analysts (and for potential automatic annotation). For the development of our scheme, we used computational linguistics articles. The papers in our collection cover a challengi</context>
</contexts>
<marker>Myers, 1992</marker>
<rawString>Greg Myers. 1992. In this paper we report... - speech acts and scientific facts. Journal of Pragmatics, 17(4):295-313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
</authors>
<title>Constructing literature abstracts by computer: techniques and prospects.</title>
<date>1990</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>26--171</pages>
<contexts>
<context position="2720" citStr="Paice, 1990" startWordPosition="423" endWordPosition="424"> to the argumentation of the paper, as opposed to local RST—type moves. For example, sentences which describe weaknesses of previous approaches can provide a good characterization of the scientific articles in which they occur, since they are likely to also be a description of the problem that paper is intending to solve. Take a sentence like &amp;quot;Unfortunately, this work does not solve problem X&amp;quot;: if X is a shortcoming in someone else&apos;s work, this usually means that the current paper will try to solve X. Sentence extraction methods can locate sentences like these, e.g. using a cue phrase method (Paice, 1990). But a very similar-looking sentence can play a completely different argumentative role in a scientific text: when it occurs in the section &amp;quot;Future Work&amp;quot;, it might refer to a minor weakness in the work presented in the source paper (i.e. of the author&apos;s own solution). In that case, the sentence is not a good characterization of the paper. Our approach to automatic text summarization is to find important sentences in a source text by determining their most likely argumentative role. In order to create an automatic process to do so, either by symbolic or machine learning techniques, we need tra</context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Chris D. Paice. 1990. Constructing literature abstracts by computer: techniques and prospects. Information Processing and Management, 26:171-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J Rath</author>
<author>A Resnick</author>
<author>T R Savage</author>
</authors>
<title>The formation of abstracts by the selection of sentences.</title>
<date>1961</date>
<journal>American Documentation,</journal>
<pages>12--2</pages>
<contexts>
<context position="8127" citStr="Rath et al., 1961" startWordPosition="1279" endWordPosition="1282">wn previous work (OTHER); • and which statements are the authors&apos; own new contributions (OWN). The full annotation scheme consists of the basic scheme plus four other categories, which are based on Swales&apos; moves. The most important of these is AIM (Swales&apos; move &amp;quot;Explicit statements of research goal&amp;quot;), as these moves are good characterizations of the entire paper. We are interested in how far humans can be trained to consistently annotate these sentences; similar experiments where subjects selected one or several &apos;most relevant&apos; sentences from a paper have traditionally reported low agreement (Rath et al., 1961). There is also the category TEXTUAL ( Swales&apos; move &amp;quot;Indicate structure&amp;quot;), which provides helpful information about section structure, and two moves having to do with attitude towards previous research, namely BASIS and CONTRAST. The relative simplicity of the scheme was a compromise between two demands: we wanted the scheme to contain enough information for automatic summarization, but still be practicable for hand coding. Annotation proceeds sentence by sentence according to the decision tree given in Figure 2. No instructions about the use of cue phrases were given, although some of the exa</context>
</contexts>
<marker>Rath, Resnick, Savage, 1961</marker>
<rawString>G.J Rath, A. Resnick, and T. R. Savage. 1961. The formation of abstracts by the selection of sentences. American Documentation, 12(2):139-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa F Rau</author>
<author>Paul S Jacobs</author>
<author>Uri Zernik</author>
</authors>
<title>Information extraction and text processing using linguistic knowledge acquisition.</title>
<date>1989</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>25--4</pages>
<contexts>
<context position="1159" citStr="Rau et al., 1989" startWordPosition="167" endWordPosition="170"> which can be used to build such a resource in a consistent way. The seven categories of the scheme are based on rhetorical moves of argumentation. Our experimental results show that the scheme is stable, reproducible and intuitive to use. 1 Introduction Current approaches to automatic summarization cannot create coherent, flexible automatic summaries. Sentence selection techniques (e.g. Brandow et al., 1995; Kupiec et al. 1995) produce extracts which can be incoherent and which, because of the generality of the methodology, can give under-informative results; fact extraction techniques (e.g. Rau et al., 1989, Young and Hayes, 1985) are tailored to particular domains, but have not really scaled up from restricted texts and restricted domains to larger domains and unrestricted text. Sparck Jones (1998) argues that taking into account the structure of a text will help when summarizing the text. The problem with sentence selection is that it relies on extracting sentences out of context, but the meaning of extracted material tends to depend on where in the text the extracted sentence was found. However, sentence selection still has the distinct advantage of robustness. We think sentence selection cou</context>
</contexts>
<marker>Rau, Jacobs, Zernik, 1989</marker>
<rawString>Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. Information extraction and text processing using linguistic knowledge acquisition. Information Processing and Management, 25(4):419-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Castellan</author>
</authors>
<title>Nonparametric statistics for the Behavioral Sciences. McGraw-Hill, second edition edition.</title>
<date>1988</date>
<contexts>
<context position="11233" citStr="Castellan, 1988" startWordPosition="1766" endWordPosition="1767">coders with a significant amount of training. We are interested in two formal properties of the annotation scheme: stability and reproducibility (Krippendorff, 1980). Stability, the extent to which one annotator will produce the same classifications at different times, is important because an instable annotation scheme can never be reproducible. Reproducibility, the extent to which different annotators will produce the same classifications, is important because it measures the consistency of shared understandings (or meaning) held between annotators. We use the Kappa coefficient K (Siegel and Castellan, 1988) to measure stability and reproducibility among k annotators on N items. In our experiment, the items are sentences. Kappa is a better measurement of agreement than raw percentage agreement (Carletta, 1996) because it factors out the level of agreement which would be reached by random annotators using the same distribution of categories as the real coders. No matter how many items or annotators, or how the categories are distributed, K=0 when there is no agreement other than what would be expected by chance, and K=1 when agreement is perfect. We expect high random agreement for our annotation </context>
</contexts>
<marker>Castellan, 1988</marker>
<rawString>Sidney Siegel and N.J. Jr. Castellan. 1988. Nonparametric statistics for the Behavioral Sciences. McGraw-Hill, second edition edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>Automatic summarising: factors and directions.</title>
<date>1998</date>
<booktitle>In ACL/EACL-97 Workshop &apos;Intelligent Scalable Text Summarization&apos;.</booktitle>
<contexts>
<context position="1355" citStr="Jones (1998)" startWordPosition="200" endWordPosition="201">ble, reproducible and intuitive to use. 1 Introduction Current approaches to automatic summarization cannot create coherent, flexible automatic summaries. Sentence selection techniques (e.g. Brandow et al., 1995; Kupiec et al. 1995) produce extracts which can be incoherent and which, because of the generality of the methodology, can give under-informative results; fact extraction techniques (e.g. Rau et al., 1989, Young and Hayes, 1985) are tailored to particular domains, but have not really scaled up from restricted texts and restricted domains to larger domains and unrestricted text. Sparck Jones (1998) argues that taking into account the structure of a text will help when summarizing the text. The problem with sentence selection is that it relies on extracting sentences out of context, but the meaning of extracted material tends to depend on where in the text the extracted sentence was found. However, sentence selection still has the distinct advantage of robustness. We think sentence selection could be improved substantially if the global rhetorical context of the extracted material was taken into account more. Marcu (1997) makes a similar point based on rhetorical relations as defined by </context>
</contexts>
<marker>Jones, 1998</marker>
<rawString>Karen Sparck Jones. 1998. Automatic summarising: factors and directions. In ACL/EACL-97 Workshop &apos;Intelligent Scalable Text Summarization&apos;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Swales</author>
</authors>
<title>Genre analysis: English in academic and research settings.</title>
<date>1990</date>
<publisher>Cambridge University Press.</publisher>
<marker>Swales, 1990</marker>
<rawString>John Swales. 1990. Genre analysis: English in academic and research settings. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoff Thompson</author>
<author>Yiyun Ye</author>
</authors>
<title>Evaluation in the reporting verbs used in academic papers.</title>
<date>1991</date>
<journal>Applied Linguistics,</journal>
<pages>12--4</pages>
<contexts>
<context position="5902" citStr="Thompson and Ye, 1991" startWordPosition="929" endWordPosition="932">nce their audience that they have provided a contribution to science. From this goal follow highly predictable subgoals which he calls argumentative moves (&amp;quot;recurring and regularized communicative events&amp;quot;). An example for such a move is &amp;quot;Indication of a gap&amp;quot;, where the author argues that there is a weakness in an earlier approach which needs to be solved. Swales&apos; model has been used extensively by discourse analysts and researchers in the field of English for Specific Purposes, for tasks as varied as teaching English as a foreign language, human translation and citation analysis (Myers, 1992; Thompson and Ye, 1991; Duszak, 1994), but always for manual analysis by a single person. Our annotation scheme is based on Swales&apos; model but we needed to modify it. Firstly, the CARS model only applies to introductions of research articles, so we needed new moves to cover the other paper sections; secondly, we needed more precise guidelines to make the scheme applicable to reliable annotation for several non-discourse analysts (and for potential automatic annotation). For the development of our scheme, we used computational linguistics articles. The papers in our collection cover a challenging range of subject mat</context>
</contexts>
<marker>Thompson, Ye, 1991</marker>
<rawString>Geoff Thompson and Yiyun Ye. 1991. Evaluation in the reporting verbs used in academic papers. Applied Linguistics, 12(4):365-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheryl R Young</author>
<author>Phillip J Hayes</author>
</authors>
<title>Automatic classification and summarization of banking telexes.</title>
<date>1985</date>
<booktitle>In Proceedings of the Second Conference on Artificial Intelligence Applications.</booktitle>
<contexts>
<context position="1183" citStr="Young and Hayes, 1985" startWordPosition="171" endWordPosition="174"> to build such a resource in a consistent way. The seven categories of the scheme are based on rhetorical moves of argumentation. Our experimental results show that the scheme is stable, reproducible and intuitive to use. 1 Introduction Current approaches to automatic summarization cannot create coherent, flexible automatic summaries. Sentence selection techniques (e.g. Brandow et al., 1995; Kupiec et al. 1995) produce extracts which can be incoherent and which, because of the generality of the methodology, can give under-informative results; fact extraction techniques (e.g. Rau et al., 1989, Young and Hayes, 1985) are tailored to particular domains, but have not really scaled up from restricted texts and restricted domains to larger domains and unrestricted text. Sparck Jones (1998) argues that taking into account the structure of a text will help when summarizing the text. The problem with sentence selection is that it relies on extracting sentences out of context, but the meaning of extracted material tends to depend on where in the text the extracted sentence was found. However, sentence selection still has the distinct advantage of robustness. We think sentence selection could be improved substanti</context>
</contexts>
<marker>Young, Hayes, 1985</marker>
<rawString>Sheryl R. Young and Phillip J. Hayes. 1985. Automatic classification and summarization of banking telexes. In Proceedings of the Second Conference on Artificial Intelligence Applications.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>