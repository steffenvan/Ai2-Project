<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004041">
<note confidence="0.81551">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 107-110, Lisbon, Portugal, 2000.
</note>
<title confidence="0.985583">
Shallow Parsing by Inferencing with Classifiers*
</title>
<author confidence="0.996767">
Vasin Punyakanok and Dan Roth
</author>
<affiliation confidence="0.908344333333333">
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.996489">
fpunyakan, danrl@cs.uiuc.edu
</email>
<sectionHeader confidence="0.99733" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999815">
We study the problem of identifying phrase
structure. We formalize it as the problem of
combining the outcomes of several different clas-
sifiers in a way that provides a coherent in-
ference that satisfies some constraints, and de-
velop two general approaches for it. The first
is a Markovian approach that extends stan-
dard HMMs to allow the use of a rich obser-
vations structure and of general classifiers to
model state-observation dependencies. The sec-
ond is an extension of constraint satisfaction for-
malisms. We also develop efficient algorithms
under both models and study them experimen-
tally in the context of shallow parsing.&apos;
</bodyText>
<sectionHeader confidence="0.988305" genericHeader="method">
1 Identifying Phrase Structure
</sectionHeader>
<bodyText confidence="0.9997692">
The problem of identifying phrase structure can
be formalized as follows. Given an input string
o =&lt; oi, 02, , o7 &gt;, a phrase is a substring
of consecutive input symbols o, oi+i, , 0.
Some external mechanism is assumed to consis-
tently (or stochastically) annotate substrings as
phrases2. Our goal is to come up with a mech-
anism that, given an input string, identifies the
phrases in this string, this is a fundamental task
with applications in natural language (Church,
1988; Ramshaw and Marcus, 1995; Murioz et
al., 1999; Cardie and Pierce, 1998).
The identification mechanism works by using
classifiers that process the input string and rec-
ognize in the input string local signals which
</bodyText>
<footnote confidence="0.70567525">
* This research is supported by NSF grants IIS-9801638,
SBR-9873450 and IIS-9984168.
&apos;Full version is in (Punyakanok and Roth, 2000).
2We assume here a single type of phrase, and thus
</footnote>
<bodyText confidence="0.931192454545455">
each input symbol is either in a phrase or outside it. All
the methods we discuss can be extended to deal with
several kinds of phrases in a string, including different
kinds of phrases and embedded phrases.
are indicative to the existence of a phrase. Lo-
cal signals can indicate that an input symbol o
is inside or outside a phrase (JO modeling) or
they can indicate that an input symbol o opens
or closes a phrase (the OC modeling) or some
combination of the two. In any case, the lo-
cal signals can be combined to determine the
phrases in the input string. This process, how-
ever, needs to satisfy some constraints for the
resulting set of phrases to be legitimate. Sev-
eral types of constraints, such as length and or-
der can be formalized and incorporated into the
mechanisms studied here. For simplicity, we fo-
cus only on the most basic and common con-
straint - we assume that phrases do not overlap.
The goal is thus two-fold: to learn classifiers
that recognize the local signals and to combine
these in a ways that respects the constraints.
</bodyText>
<sectionHeader confidence="0.983263" genericHeader="method">
2 Markov Modeling
</sectionHeader>
<bodyText confidence="0.999269944444445">
HMM is a probabilistic finite state automaton
used to model the probabilistic generation of
sequential processes. The model consists of
a finite set S of states, a set 0 of observa-
tions, an initial state distribution Pi (s), a state-
transition distribution P(81.s&apos;) for s, ES and
an observation distribution P(ols) for o E 0
and s E S.3
In a supervised learning task, an observa-
tion sequence 0 =&lt; 01,02, ... on &gt; is super-
vised by a corresponding state sequence S =&lt;
Si, s2, • • • Sn &gt;• The supervision can also be sup-
plied, as described in Sec. 1, using the local sig-
nals. Constraints can be incorporated into the
HMM by constraining the state transition prob-
ability distribution P(.918&apos;). For example, set
P(s181) = 0 for all s, s&apos; such that the transition
from s&apos; to s is not allowed.
</bodyText>
<footnote confidence="0.973896">
3See (Rabiner, 1989) for a comprehensive tutorial.
</footnote>
<page confidence="0.998051">
107
</page>
<bodyText confidence="0.96495675">
Combining HMM and classifiers (artificial
neural networks) has been exploited in speech
recognition (Morgan and Bourlard, 1995), how-
ever, with some differences from this work.
</bodyText>
<subsectionHeader confidence="0.706351">
2.1 HMM with Classifiers
</subsectionHeader>
<bodyText confidence="0.999904555555556">
To recover the most likely state sequence in
HMM, we wish to estimate all the required
probability distributions. As in Sec. 1 we as-
sume to have local signals that indicate the
state. That is, we are given classifiers with
states as their outcomes. Formally, we assume
that Pt(sio) is given where t is the time step in
the sequence. In order to use this information
in the HMM framework, we compute
</bodyText>
<equation confidence="0.987602">
Pt(ols) = Pt(slo)Pt(o) Pt (s). (1)
</equation>
<bodyText confidence="0.9775005">
instead of observing the conditional probability
Pt(ols) directly from training data, we compute
it from the classifiers&apos; output. Pt(s) can be cal-
culated by Pt(s)
</bodyText>
<listItem confidence="0.817636555555555">
= Es, Es P(sisi)Pt—i(s1) where
• (s) and P(sis&apos;) are the two required distri-
bution for the HMM. For each t, we can treat
Pt(ols) in Eq. 1 as a constant Tit because our goal
is only to find the most likely sequence of states
for given observations which are the same for
all compared sequences. Therefore, to compute
the most likely sequence, standard dynamic pro-
gramming (Viterbi) can still be applied.
</listItem>
<subsectionHeader confidence="0.738553">
2.2 Projection based Markov Model
</subsectionHeader>
<bodyText confidence="0.9999689">
In HMMs, observations are allowed to depend
only on the current state and long term depen-
dencies are not modeled. Equivalently, from the
constraint point of view, the constraint struc-
ture is restricted by having a stationary proba-
bility distribution of a state given the previous
one. We attempt to relax this by allowing the
distribution of a state to depend, in addition
to the previous state, on the observation. For-
mally, we make the independence assumption:
</bodyText>
<equation confidence="0.9376415">
P(StiSt—i, St-2, • • • Si, ot, ot—i, • • • , 01)
= P(stist—i, ot)• (2)
Thus, we can find the most likely state sequence
S given 0 by maximizing
P(S10) = ll[P(StIS1, • • • , St-1,0)]P1(S11°)
t=2
H [P(st Ist_i , ot)]/A (si 101 )• (3)
t=2
</equation>
<bodyText confidence="0.999941230769231">
Hence, this model generalizes the standard
HMM by combining the state-transition prob-
ability and the observation probability into one
function. The most likely state sequence can
still be recovered using the dynamic program-
ming algorithm over the Eq.3.
In this model, the classifiers&apos; decisions are in-
corporated in the terms P(.91.51, o) and Pi (sio).
In learning these classifiers we project P (sis&apos; , o)
to many functions Ps, (slo) according to the pre-
vious states s&apos;. A similar approach has been
developed recently in the context of maximum
entropy classifiers in (McCallum et al., 2000).
</bodyText>
<sectionHeader confidence="0.979484" genericHeader="method">
3 Constraint Satisfaction with
Classifiers
</sectionHeader>
<bodyText confidence="0.987524027777778">
The approach is based on an extension of
the Boolean constraint satisfaction formal-
ism (Mackworth, 1992) to handle variables that
are outcomes of classifiers. As before, we as-
sume an observed string 0 =&lt; 01,02,.&gt;
and local classifiers that, w.l.o.g., take two dis-
tinct values, one indicating the openning a
phrase and a second indicating closing it (OC
modeling). The classifiers provide their outputs
in terms of the probability P(o) and P(c), given
the observation.
To formalize this, let E be the set of all possi-
ble phrases. All the non-overlapping constraints
can be encoded in: f
J Aez overlaps ej(--ieij).
Each solution to this formulae corresponds to a
legitimate set of phrases.
Our problem, however, is not simply to find
an assignment T : E —&gt; {0, 1} that satisfies f
but rather to optimize some criterion. Hence,
we associate a cost function c : E [0, 1]
with each variable, and then find a solution T
of f of minimum cost, e(l-) = E:11. r(ei)c(ei).
In phrase identification, the solution to the op-
timization problem corresponds to a shortest
path in a directed acyclic graph constructed
on the observation symbols, with legitimate
phrases (the variables in E) as its edges and
their costs as the weights. Each path in this
graph corresponds to a satisfying assignment
and the shortest path is the optimal solution.
A natural cost function is to use the classi-
fiers probabilities P(o) and P(c) and define, for
a phrase e = (o, c), c(e) = 1 — P (o)P (c) which
means that the error in selecting e is the er-
ror in selecting either o or c, and allowing those
</bodyText>
<page confidence="0.996484">
108
</page>
<bodyText confidence="0.9970595">
to overlap4. The constant in 1 — P(o)P(c) bi-
ases the minimization to prefers selecting a few
phrases, possibly no phrase, so instead we min-
imize —P(o)P(c).
</bodyText>
<sectionHeader confidence="0.972143" genericHeader="method">
4 Shallow Parsing
</sectionHeader>
<bodyText confidence="0.9998691">
The above mentioned approaches are evaluated
on shallow parsing tasks. We use the OC mod-
eling and learn two classifiers; one predicting
whether there should be a open in location t
or not, and the other whether there should a
close in location t or not. For technical reasons
it is easier to keep track of the constraints if
the cases and are separated according to
whether we are inside or outside a phrase. Con-
sequently, each classifier may output three pos-
sible outcomes 0, nOi, nOo (open, not open
inside, not open outside) and C, nCi, nCo,
resp. The state-transition diagram in figure 1
captures the order constraints. Our modeling of
the problem is a modification of our earlier work
on this topic that has been found to be quite
successful compared to other learning methods
attempted on this problem (Munoz et al., 1999)
and in particular, better than the IO modeling
of the problem (Munoz et al., 1999).
</bodyText>
<figureCaption confidence="0.923547">
Figure 1: State-transition diagramfor the
phrase recognition problem.
</figureCaption>
<bodyText confidence="0.9999564">
The classifier we use to learn the states as
a function of the observations is SNoW (Roth,
1998; Carleson et al., 1999), a multi-class clas-
sifier that is specifically tailored for large scale
learning tasks. The SNoW learning architec-
ture learns a sparse network of linear functions,
in which the targets (states, in this case) are
represented as linear functions over a common
feature space. Typically, SNoW is used as a
classifier, and predicts using a winner-take-all
</bodyText>
<footnote confidence="0.67561">
4Another solution in which the classifiers&apos; suggestions
inside each phrase are also accounted for is possible.
</footnote>
<bodyText confidence="0.9998195">
mechanism over the activation value of the tar-
get classes in this case. The activation value
itself is computed using a sigmoid function over
the linear sum. In this case, instead, we normal-
ize the activation levels of all targets to sum to 1
and output the outcomes for all targets (states).
We verified experimentally on the training data
that the output for each state is indeed a dis-
tribution function and can be used in further
processing as P(slo) (details omitted).
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999926025641026">
We experimented both with base noun phrases
(NP) and subject-verb patterns (SV) and show
results for two different representations of the
observations (that is, different feature sets for
the classifiers) - part of speech (POS) tags only
and POS with additional lexical information
(words). The data sets used are the standard
data sets for this problem (Ramshaw and Mar-
cus, 1995; Argamon et al., 1999; Munoz et
al., 1999; Tjong Kim Sang and Veenstra, 1999)
taken from the Wall Street Journal corpus in
the Penn Treebank (Marcus et al., 1993).
For each model we study three different clas-
sifiers. The simple classifier corresponds to the
standard HMM in which P(ols) is estimated di-
rectly from the data. The NB (naive Bayes) and
SNoW classifiers use the same feature set, con-
junctions of size 3 of POS tags (+ words) in a
window of size 6 around the target word.
The first important observation is that the
SV task is significantly more difficult than the
NP task. This is consistent for all models and
all features sets. When comparing between dif-
ferent models and features sets, it is clear that
the simple HMM formalism is not competitive
with the other two models. What is interest-
ing here is the very significant sensitivity to the
wider notion of observations (features) used by
the classifiers, despite the violation of the prob-
abilistic assumptions. For the easier NP task,
the HMM model is competitive with the oth-
ers when the classifiers used are NB or SNoW.
In particular, a significant improvement in both
probabilistic methods is achieved when their in-
put is given by SNoW.
Our two main methods, PMM and CSCL,
perform very well on predicting NP and SV
phrases with CSCL at least as good as any other
methods tried on these tasks. Both for NPs and
</bodyText>
<page confidence="0.999497">
109
</page>
<tableCaption confidence="0.985643">
Table 1: Results (F0=1) of different methods
</tableCaption>
<bodyText confidence="0.8783436">
and comparison to previous works on NP and
SV recognition. Notice that, in case of simple,
the data with lexical features are too sparse to
directly estimate the observation probability so
we leave these entries empty.
</bodyText>
<table confidence="0.999328928571429">
Method POS POS
+words
Model Classifier only
SNoW 90.64 92.89
HMM NB 90.50 92.26
Simple 87.83
SNoW 90.61 92.98
NP PMM NB 90.22 91.98
Simple 61.44
SNoW 90.87 92.88
CSCL NB 90.49 91.95
Simple 54.42
Ramshaw &amp; Marcus 90.6 92.0
Argamon et al. 91.6 N/A
Munoz et al. 90.6 92.8
Tjong Kim Sang
&amp; Veenstra N/A 92.37
SNoW 64.15 77.54
HMM NB 75.40 78.43
Simple 64.85
SNoW 74.98 86.07
SV PMM NB 74.80 84.80
Simple 40.18
SNoW 85.36 90.09
CSCL NB 80.63 88.28
Simple 59.27
Argamon et al. 86.5 N/A
Munoz et al. 88.1 92.0
</table>
<bodyText confidence="0.999872076923077">
SVs, CSCL performs better than the probabilis-
tic method, more significantly on the harder,
SV, task. We attribute it to CSCL&apos;s ability to
cope better with the length of the phrase and
the long term dependencies.
Our methods compare favorably with others
with the exception to SV in (Munoz et al.,
1999). Their method is fundamentally simi-
lar to our CSCL; however, they incorporated
the features from open in the close classifier al-
lowing to exploit the dependencies between two
classifiers. We believe that this is the main fac-
tor of the significant difference in performance.
</bodyText>
<sectionHeader confidence="0.997234" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999543">
We have addressed the problem of combining
the outcomes of several different classifiers in a
way that provides a coherent inference that sat-
isfies some constraints, While the probabilistic
approach extends standard and commonly used
techniques for sequential decisions, it seems
that the constraint satisfaction formalisms can
support complex constraints and dependencies
more flexibly. Future work will concentrate on
these formalisms.
</bodyText>
<sectionHeader confidence="0.998774" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999328826086956">
S. Argamon, I. Dagan, and Y. Krymolowski. 1999.
A memory-based approach to learning shallow
natural language patterns. Journal of Experimen-
tal and Theoretical Artificial Intelligence, 10:1-22.
C. Cardie and D. Pierce. 1998. Error-driven prun-
ing of treebanks grammars for base noun phrase
identification. In Proc. of ACL-98, pages 218-224.
A. Carleson, C. Cumby, J. Rosen, and D. Roth.
1999. The SNoW learning architecture. Tech. Re-
port UIUCDCS-R-99-2101, UIUC Computer Sci-
ence Department, May.
K. W. Church. 1988. A stochastic parts program
and noun phrase parser for unrestricted text. In
Proc. of ACL Conference on Applied Natural Lan-
guage Processing.
A. K. Mackworth. 1992. Constraint Satisfaction. In
Stuart C. Shapiro, editor, Encyclopedia of Artifi-
cial Intelligence, pages 285-293. Vol. 1, 2nd ed.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank Computational Lin-
guistics, 19(2):313-330, June.
A. McCallum, D. Freitag, and F. Pereira. 2000.
Maximum entropy Markov models for information
extraction and segmentation. In Proc. of ICML-
2000.
N. Morgan and H. Bourlard. 1995. Continuous
speech recognition. IEEE Signal Processing Mag-
azine, 12(3):25-42.
M. Mufioz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. In
Proc. of EMNLP-VLC&apos;99.
V. Punyakanok and D. Roth. 2000. Inference with
classifiers. Tech. Report UIUCDCS-R-2000-2181,
URIC Computer Science Department, July.
L. R. Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recog-
nition. Proc. of the IEEE, 77(2):257-285.
L. A. Ramshaw and M. P. Marcus. 1995. Text
chunking using transformation-based learning. In
Proc. of WVLC&apos;95.
D. Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A unified approach. In Proc.
of AAAI&apos;98, pages 806-813.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Rep-
resenting text chunks. In Proc. of EACL&apos;99.
</reference>
<page confidence="0.998398">
110
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000021">
<note confidence="0.956668">of CoNLL-2000 and LLL-2000, 107-110, Lisbon, Portugal, 2000.</note>
<title confidence="0.924046">Shallow Parsing by Inferencing with Classifiers*</title>
<author confidence="0.854941">Punyakanok</author>
<affiliation confidence="0.9988905">Department of Computer University of Illinois at</affiliation>
<address confidence="0.968969">Urbana, IL 61801,</address>
<email confidence="0.999908">fpunyakan,danrl@cs.uiuc.edu</email>
<abstract confidence="0.995577">We study the problem of identifying phrase structure. We formalize it as the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints, and develop two general approaches for it. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observations structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We also develop efficient algorithms under both models and study them experimentally in the context of shallow parsing.&apos; 1 Identifying Phrase Structure The problem of identifying phrase structure can be formalized as follows. Given an input string oi, 02, , &gt;, a substring consecutive input symbols , Some external mechanism is assumed to consistently (or stochastically) annotate substrings as Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Murioz et al., 1999; Cardie and Pierce, 1998). The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which * This research is supported by NSF grants IIS-9801638, SBR-9873450 and IIS-9984168. &apos;Full version is in (Punyakanok and Roth, 2000). assume here a single type of phrase, and thus each input symbol is either in a phrase or outside it. All the methods we discuss can be extended to deal with several kinds of phrases in a string, including different kinds of phrases and embedded phrases. are indicative to the existence of a phrase. Losignals can indicate that an input symbol is inside or outside a phrase (JO modeling) or can indicate that an input symbol or closes a phrase (the OC modeling) or some combination of the two. In any case, the local signals can be combined to determine the phrases in the input string. This process, however, needs to satisfy some constraints for the resulting set of phrases to be legitimate. Several types of constraints, such as length and order can be formalized and incorporated into the mechanisms studied here. For simplicity, we focus only on the most basic and common constraint we assume that phrases do not overlap. The goal is thus two-fold: to learn classifiers that recognize the local signals and to combine these in a ways that respects the constraints. 2 Markov Modeling HMM is a probabilistic finite state automaton used to model the probabilistic generation of sequential processes. The model consists of finite set states, a set 0 of observaan initial state distribution Pi statedistribution P(81.s&apos;) for observation distribution E In a supervised learning task, an observasequence 01,02, ... &gt; superby a corresponding state sequence =&lt; s2, • • • &gt;• supervision can also be supplied, as described in Sec. 1, using the local signals. Constraints can be incorporated into the HMM by constraining the state transition probability distribution P(.918&apos;). For example, set = 0 for all s&apos; that the transition not allowed. (Rabiner, 1989) for a comprehensive tutorial. 107 Combining HMM and classifiers (artificial neural networks) has been exploited in speech recognition (Morgan and Bourlard, 1995), however, with some differences from this work. 2.1 HMM with Classifiers To recover the most likely state sequence in HMM, we wish to estimate all the required probability distributions. As in Sec. 1 we assume to have local signals that indicate the state. That is, we are given classifiers with states as their outcomes. Formally, we assume is given where t is the time step in the sequence. In order to use this information in the HMM framework, we compute = (s). instead of observing the conditional probability from training data, we compute from the classifiers&apos; output. be calby Pt(s) Es (s) the two required distrifor the HMM. For each can treat Eq. 1 as a constant because our goal is only to find the most likely sequence of states for given observations which are the same for all compared sequences. Therefore, to compute the most likely sequence, standard dynamic programming (Viterbi) can still be applied. 2.2 Projection based Markov Model In HMMs, observations are allowed to depend only on the current state and long term dependencies are not modeled. Equivalently, from the constraint point of view, the constraint structure is restricted by having a stationary probability distribution of a state given the previous one. We attempt to relax this by allowing the distribution of a state to depend, in addition to the previous state, on the observation. Formally, we make the independence assumption: St-2, • • • Si, ot—i, • • , 01) P(stist—i, Thus, we can find the most likely state sequence given maximizing • • • , t=2 , ot)]/A )• t=2 Hence, this model generalizes the standard HMM by combining the state-transition probability and the observation probability into one function. The most likely state sequence can still be recovered using the dynamic programming algorithm over the Eq.3. In this model, the classifiers&apos; decisions are inin the terms Pi (sio). learning these classifiers we project (sis&apos; , o) many functions (slo) to the prestates similar approach has been developed recently in the context of maximum entropy classifiers in (McCallum et al., 2000). 3 Constraint Satisfaction with Classifiers The approach is based on an extension of the Boolean constraint satisfaction formalism (Mackworth, 1992) to handle variables that are outcomes of classifiers. As before, we asan observed string =&lt; 01,02,.&gt; and local classifiers that, w.l.o.g., take two distinct values, one indicating the openning a phrase and a second indicating closing it (OC modeling). The classifiers provide their outputs terms of the probability the observation. formalize this, let the set of all possible phrases. All the non-overlapping constraints be encoded in: overlaps ej(--ieij). Each solution to this formulae corresponds to a legitimate set of phrases. Our problem, however, is not simply to find assignment : —&gt; 1} that satisfies but rather to optimize some criterion. Hence, associate a cost function c : each variable, and then find a solution minimum cost, = In phrase identification, the solution to the optimization problem corresponds to a shortest path in a directed acyclic graph constructed on the observation symbols, with legitimate (the variables in its edges and their costs as the weights. Each path in this graph corresponds to a satisfying assignment and the shortest path is the optimal solution. A natural cost function is to use the classiprobabilities define, for phrase = (o, c), c(e) = 1 — P (o)P (c) means that the error in selecting e is the erin selecting either c, and allowing those 108 The constant in 1 — biases the minimization to prefers selecting a few phrases, possibly no phrase, so instead we min- 4 Shallow Parsing The above mentioned approaches are evaluated on shallow parsing tasks. We use the OC modeling and learn two classifiers; one predicting whether there should be a open in location t or not, and the other whether there should a in location not. For technical reasons it is easier to keep track of the constraints if the cases and are separated according whether we are inside or outside a phrase. Consequently, each classifier may output three posoutcomes nOi, nOo not open not open outside) and nCi, nCo, resp. The state-transition diagram in figure 1 captures the order constraints. Our modeling of the problem is a modification of our earlier work on this topic that has been found to be quite successful compared to other learning methods attempted on this problem (Munoz et al., 1999) and in particular, better than the IO modeling of the problem (Munoz et al., 1999). Figure 1: State-transition diagramfor the phrase recognition problem. The classifier we use to learn the states as a function of the observations is SNoW (Roth, 1998; Carleson et al., 1999), a multi-class classifier that is specifically tailored for large scale learning tasks. The SNoW learning architecture learns a sparse network of linear functions, in which the targets (states, in this case) are represented as linear functions over a common feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all solution in which the classifiers&apos; suggestions inside each phrase are also accounted for is possible. mechanism over the activation value of the target classes in this case. The activation value itself is computed using a sigmoid function over the linear sum. In this case, instead, we normalize the activation levels of all targets to sum to 1 and output the outcomes for all targets (states). We verified experimentally on the training data that the output for each state is indeed a distribution function and can be used in further as omitted). 5 Experiments We experimented both with base noun phrases (NP) and subject-verb patterns (SV) and show results for two different representations of the observations (that is, different feature sets for the classifiers) part of speech (POS) tags only and POS with additional lexical information (words). The data sets used are the standard data sets for this problem (Ramshaw and Marcus, 1995; Argamon et al., 1999; Munoz et al., 1999; Tjong Kim Sang and Veenstra, 1999) taken from the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993). For each model we study three different clas- The corresponds to the HMM in which estimated directly from the data. The NB (naive Bayes) and SNoW classifiers use the same feature set, conjunctions of size 3 of POS tags (+ words) in a window of size 6 around the target word. The first important observation is that the SV task is significantly more difficult than the NP task. This is consistent for all models and all features sets. When comparing between different models and features sets, it is clear that the simple HMM formalism is not competitive with the other two models. What is interesting here is the very significant sensitivity to the wider notion of observations (features) used by the classifiers, despite the violation of the probabilistic assumptions. For the easier NP task, the HMM model is competitive with the others when the classifiers used are NB or SNoW. In particular, a significant improvement in both probabilistic methods is achieved when their input is given by SNoW. Our two main methods, PMM and CSCL, perform very well on predicting NP and SV phrases with CSCL at least as good as any other methods tried on these tasks. Both for NPs and 109 1: Results of different methods and comparison to previous works on NP and recognition. Notice that, in case of the data with lexical features are too sparse to directly estimate the observation probability so we leave these entries empty.</abstract>
<title confidence="0.533666">Method POS +words</title>
<author confidence="0.414889">Model Classifier only</author>
<date confidence="0.194654">SNoW 90.64 92.89</date>
<note confidence="0.523061181818182">HMM NB 90.50 92.26 Simple 87.83 SNoW 90.61 92.98 NP PMM NB 90.22 91.98 Simple 61.44 SNoW 90.87 92.88 CSCL NB 90.49 91.95 Simple 54.42 Ramshaw &amp; Marcus 90.6 92.0 al. 91.6 N/A al. 90.6 92.8</note>
<author confidence="0.978045">Tjong Kim Sang</author>
<address confidence="0.867472">amp; Veenstra N/A 92.37</address>
<note confidence="0.763221111111111">SNoW 64.15 77.54 HMM NB 75.40 78.43 Simple 64.85 SNoW 74.98 86.07 SV PMM NB 74.80 84.80 Simple 40.18 SNoW 85.36 90.09 CSCL NB 80.63 88.28 Simple 59.27</note>
<abstract confidence="0.9813515">al. 86.5 N/A al. 88.1 92.0 CSCL better than the probabilistic method, more significantly on the harder, SV, task. We attribute it to CSCL&apos;s ability to cope better with the length of the phrase and the long term dependencies. Our methods compare favorably with others with the exception to SV in (Munoz et al., 1999). Their method is fundamentally similar to our CSCL; however, they incorporated the features from open in the close classifier allowing to exploit the dependencies between two classifiers. We believe that this is the main factor of the significant difference in performance. 6 Conclusion We have addressed the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints, While the probabilistic approach extends standard and commonly used techniques for sequential decisions, it seems that the constraint satisfaction formalisms can support complex constraints and dependencies more flexibly. Future work will concentrate on these formalisms.</abstract>
<note confidence="0.829416125">References S. Argamon, I. Dagan, and Y. Krymolowski. 1999. A memory-based approach to learning shallow language patterns. of Experimenand Theoretical Artificial Intelligence, C. Cardie and D. Pierce. 1998. Error-driven pruning of treebanks grammars for base noun phrase In of 218-224. A. Carleson, C. Cumby, J. Rosen, and D. Roth. 1999. The SNoW learning architecture. Tech. Report UIUCDCS-R-99-2101, UIUC Computer Science Department, May. K. W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proc. of ACL Conference on Applied Natural Language Processing. A. K. Mackworth. 1992. Constraint Satisfaction. In C. Shapiro, editor, of Artifi- Intelligence, 285-293. Vol. ed. M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of En- The Penn Treebank Lin- June. A. McCallum, D. Freitag, and F. Pereira. 2000.</note>
<title confidence="0.757031">Maximum entropy Markov models for information</title>
<abstract confidence="0.927086933333333">and segmentation. In of ICML- 2000. N. Morgan and H. Bourlard. 1995. Continuous recognition. Signal Processing Mag- M. Mufioz, V. Punyakanok, D. Roth, and D. Zimak. 1999. A learning approach to shallow parsing. In of V. Punyakanok and D. Roth. 2000. Inference with classifiers. Tech. Report UIUCDCS-R-2000-2181, URIC Computer Science Department, July. L. R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recogof the IEEE, L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-based learning. In</abstract>
<note confidence="0.790914428571429">Proc. of WVLC&apos;95. D. Roth. 1998. Learning to resolve natural lanambiguities: A unified approach. In AAAI&apos;98, 806-813. E. F. Tjong Kim Sang and J. Veenstra. 1999. Reptext chunks. In of EACL&apos;99. 110</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow natural language patterns.</title>
<date>1999</date>
<journal>Journal of Experimental and Theoretical Artificial Intelligence,</journal>
<pages>10--1</pages>
<contexts>
<context position="10501" citStr="Argamon et al., 1999" startWordPosition="1787" endWordPosition="1790">or all targets (states). We verified experimentally on the training data that the output for each state is indeed a distribution function and can be used in further processing as P(slo) (details omitted). 5 Experiments We experimented both with base noun phrases (NP) and subject-verb patterns (SV) and show results for two different representations of the observations (that is, different feature sets for the classifiers) - part of speech (POS) tags only and POS with additional lexical information (words). The data sets used are the standard data sets for this problem (Ramshaw and Marcus, 1995; Argamon et al., 1999; Munoz et al., 1999; Tjong Kim Sang and Veenstra, 1999) taken from the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993). For each model we study three different classifiers. The simple classifier corresponds to the standard HMM in which P(ols) is estimated directly from the data. The NB (naive Bayes) and SNoW classifiers use the same feature set, conjunctions of size 3 of POS tags (+ words) in a window of size 6 around the target word. The first important observation is that the SV task is significantly more difficult than the NP task. This is consistent for all models an</context>
</contexts>
<marker>Argamon, Dagan, Krymolowski, 1999</marker>
<rawString>S. Argamon, I. Dagan, and Y. Krymolowski. 1999. A memory-based approach to learning shallow natural language patterns. Journal of Experimental and Theoretical Artificial Intelligence, 10:1-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>D Pierce</author>
</authors>
<title>Error-driven pruning of treebanks grammars for base noun phrase identification.</title>
<date>1998</date>
<booktitle>In Proc. of ACL-98,</booktitle>
<pages>218--224</pages>
<contexts>
<context position="1504" citStr="Cardie and Pierce, 1998" startWordPosition="235" endWordPosition="238">imentally in the context of shallow parsing.&apos; 1 Identifying Phrase Structure The problem of identifying phrase structure can be formalized as follows. Given an input string o =&lt; oi, 02, , o7 &gt;, a phrase is a substring of consecutive input symbols o, oi+i, , 0. Some external mechanism is assumed to consistently (or stochastically) annotate substrings as phrases2. Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Murioz et al., 1999; Cardie and Pierce, 1998). The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which * This research is supported by NSF grants IIS-9801638, SBR-9873450 and IIS-9984168. &apos;Full version is in (Punyakanok and Roth, 2000). 2We assume here a single type of phrase, and thus each input symbol is either in a phrase or outside it. All the methods we discuss can be extended to deal with several kinds of phrases in a string, including different kinds of phrases and embedded phrases. are indicative to the existence of a phrase. Local signals can ind</context>
</contexts>
<marker>Cardie, Pierce, 1998</marker>
<rawString>C. Cardie and D. Pierce. 1998. Error-driven pruning of treebanks grammars for base noun phrase identification. In Proc. of ACL-98, pages 218-224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carleson</author>
<author>C Cumby</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>The SNoW learning architecture.</title>
<date>1999</date>
<tech>Tech. Report UIUCDCS-R-99-2101,</tech>
<institution>UIUC Computer Science Department,</institution>
<contexts>
<context position="9150" citStr="Carleson et al., 1999" startWordPosition="1567" endWordPosition="1570">, nOo (open, not open inside, not open outside) and C, nCi, nCo, resp. The state-transition diagram in figure 1 captures the order constraints. Our modeling of the problem is a modification of our earlier work on this topic that has been found to be quite successful compared to other learning methods attempted on this problem (Munoz et al., 1999) and in particular, better than the IO modeling of the problem (Munoz et al., 1999). Figure 1: State-transition diagramfor the phrase recognition problem. The classifier we use to learn the states as a function of the observations is SNoW (Roth, 1998; Carleson et al., 1999), a multi-class classifier that is specifically tailored for large scale learning tasks. The SNoW learning architecture learns a sparse network of linear functions, in which the targets (states, in this case) are represented as linear functions over a common feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all 4Another solution in which the classifiers&apos; suggestions inside each phrase are also accounted for is possible. mechanism over the activation value of the target classes in this case. The activation value itself is computed using a sigmoid function </context>
</contexts>
<marker>Carleson, Cumby, Rosen, Roth, 1999</marker>
<rawString>A. Carleson, C. Cumby, J. Rosen, and D. Roth. 1999. The SNoW learning architecture. Tech. Report UIUCDCS-R-99-2101, UIUC Computer Science Department, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proc. of ACL Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="1431" citStr="Church, 1988" startWordPosition="225" endWordPosition="226">p efficient algorithms under both models and study them experimentally in the context of shallow parsing.&apos; 1 Identifying Phrase Structure The problem of identifying phrase structure can be formalized as follows. Given an input string o =&lt; oi, 02, , o7 &gt;, a phrase is a substring of consecutive input symbols o, oi+i, , 0. Some external mechanism is assumed to consistently (or stochastically) annotate substrings as phrases2. Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Murioz et al., 1999; Cardie and Pierce, 1998). The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which * This research is supported by NSF grants IIS-9801638, SBR-9873450 and IIS-9984168. &apos;Full version is in (Punyakanok and Roth, 2000). 2We assume here a single type of phrase, and thus each input symbol is either in a phrase or outside it. All the methods we discuss can be extended to deal with several kinds of phrases in a string, including different kinds of phrases and embedded ph</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>K. W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proc. of ACL Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Mackworth</author>
</authors>
<title>Constraint Satisfaction. In</title>
<date>1992</date>
<booktitle>Encyclopedia of Artificial Intelligence,</booktitle>
<volume>1</volume>
<pages>285--293</pages>
<editor>Stuart C. Shapiro, editor,</editor>
<contexts>
<context position="6407" citStr="Mackworth, 1992" startWordPosition="1087" endWordPosition="1088">y into one function. The most likely state sequence can still be recovered using the dynamic programming algorithm over the Eq.3. In this model, the classifiers&apos; decisions are incorporated in the terms P(.91.51, o) and Pi (sio). In learning these classifiers we project P (sis&apos; , o) to many functions Ps, (slo) according to the previous states s&apos;. A similar approach has been developed recently in the context of maximum entropy classifiers in (McCallum et al., 2000). 3 Constraint Satisfaction with Classifiers The approach is based on an extension of the Boolean constraint satisfaction formalism (Mackworth, 1992) to handle variables that are outcomes of classifiers. As before, we assume an observed string 0 =&lt; 01,02,.&gt; and local classifiers that, w.l.o.g., take two distinct values, one indicating the openning a phrase and a second indicating closing it (OC modeling). The classifiers provide their outputs in terms of the probability P(o) and P(c), given the observation. To formalize this, let E be the set of all possible phrases. All the non-overlapping constraints can be encoded in: f J Aez overlaps ej(--ieij). Each solution to this formulae corresponds to a legitimate set of phrases. Our problem, how</context>
</contexts>
<marker>Mackworth, 1992</marker>
<rawString>A. K. Mackworth. 1992. Constraint Satisfaction. In Stuart C. Shapiro, editor, Encyclopedia of Artificial Intelligence, pages 285-293. Vol. 1, 2nd ed.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="10642" citStr="Marcus et al., 1993" startWordPosition="1812" endWordPosition="1815"> can be used in further processing as P(slo) (details omitted). 5 Experiments We experimented both with base noun phrases (NP) and subject-verb patterns (SV) and show results for two different representations of the observations (that is, different feature sets for the classifiers) - part of speech (POS) tags only and POS with additional lexical information (words). The data sets used are the standard data sets for this problem (Ramshaw and Marcus, 1995; Argamon et al., 1999; Munoz et al., 1999; Tjong Kim Sang and Veenstra, 1999) taken from the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993). For each model we study three different classifiers. The simple classifier corresponds to the standard HMM in which P(ols) is estimated directly from the data. The NB (naive Bayes) and SNoW classifiers use the same feature set, conjunctions of size 3 of POS tags (+ words) in a window of size 6 around the target word. The first important observation is that the SV task is significantly more difficult than the NP task. This is consistent for all models and all features sets. When comparing between different models and features sets, it is clear that the simple HMM formalism is not competitive </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank Computational Linguistics, 19(2):313-330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of ICML2000.</booktitle>
<contexts>
<context position="6258" citStr="McCallum et al., 2000" startWordPosition="1064" endWordPosition="1067">t_i , ot)]/A (si 101 )• (3) t=2 Hence, this model generalizes the standard HMM by combining the state-transition probability and the observation probability into one function. The most likely state sequence can still be recovered using the dynamic programming algorithm over the Eq.3. In this model, the classifiers&apos; decisions are incorporated in the terms P(.91.51, o) and Pi (sio). In learning these classifiers we project P (sis&apos; , o) to many functions Ps, (slo) according to the previous states s&apos;. A similar approach has been developed recently in the context of maximum entropy classifiers in (McCallum et al., 2000). 3 Constraint Satisfaction with Classifiers The approach is based on an extension of the Boolean constraint satisfaction formalism (Mackworth, 1992) to handle variables that are outcomes of classifiers. As before, we assume an observed string 0 =&lt; 01,02,.&gt; and local classifiers that, w.l.o.g., take two distinct values, one indicating the openning a phrase and a second indicating closing it (OC modeling). The classifiers provide their outputs in terms of the probability P(o) and P(c), given the observation. To formalize this, let E be the set of all possible phrases. All the non-overlapping co</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proc. of ICML2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Morgan</author>
<author>H Bourlard</author>
</authors>
<title>Continuous speech recognition.</title>
<date>1995</date>
<journal>IEEE Signal Processing Magazine,</journal>
<pages>12--3</pages>
<contexts>
<context position="3861" citStr="Morgan and Bourlard, 1995" startWordPosition="649" endWordPosition="652">upervised learning task, an observation sequence 0 =&lt; 01,02, ... on &gt; is supervised by a corresponding state sequence S =&lt; Si, s2, • • • Sn &gt;• The supervision can also be supplied, as described in Sec. 1, using the local signals. Constraints can be incorporated into the HMM by constraining the state transition probability distribution P(.918&apos;). For example, set P(s181) = 0 for all s, s&apos; such that the transition from s&apos; to s is not allowed. 3See (Rabiner, 1989) for a comprehensive tutorial. 107 Combining HMM and classifiers (artificial neural networks) has been exploited in speech recognition (Morgan and Bourlard, 1995), however, with some differences from this work. 2.1 HMM with Classifiers To recover the most likely state sequence in HMM, we wish to estimate all the required probability distributions. As in Sec. 1 we assume to have local signals that indicate the state. That is, we are given classifiers with states as their outcomes. Formally, we assume that Pt(sio) is given where t is the time step in the sequence. In order to use this information in the HMM framework, we compute Pt(ols) = Pt(slo)Pt(o) Pt (s). (1) instead of observing the conditional probability Pt(ols) directly from training data, we com</context>
</contexts>
<marker>Morgan, Bourlard, 1995</marker>
<rawString>N. Morgan and H. Bourlard. 1995. Continuous speech recognition. IEEE Signal Processing Magazine, 12(3):25-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mufioz</author>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>D Zimak</author>
</authors>
<title>A learning approach to shallow parsing.</title>
<date>1999</date>
<booktitle>In Proc. of EMNLP-VLC&apos;99.</booktitle>
<marker>Mufioz, Punyakanok, Roth, Zimak, 1999</marker>
<rawString>M. Mufioz, V. Punyakanok, D. Roth, and D. Zimak. 1999. A learning approach to shallow parsing. In Proc. of EMNLP-VLC&apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>Inference with classifiers.</title>
<date>2000</date>
<tech>Tech. Report UIUCDCS-R-2000-2181,</tech>
<institution>URIC Computer Science Department,</institution>
<contexts>
<context position="1778" citStr="Punyakanok and Roth, 2000" startWordPosition="277" endWordPosition="280">al mechanism is assumed to consistently (or stochastically) annotate substrings as phrases2. Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Murioz et al., 1999; Cardie and Pierce, 1998). The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which * This research is supported by NSF grants IIS-9801638, SBR-9873450 and IIS-9984168. &apos;Full version is in (Punyakanok and Roth, 2000). 2We assume here a single type of phrase, and thus each input symbol is either in a phrase or outside it. All the methods we discuss can be extended to deal with several kinds of phrases in a string, including different kinds of phrases and embedded phrases. are indicative to the existence of a phrase. Local signals can indicate that an input symbol o is inside or outside a phrase (JO modeling) or they can indicate that an input symbol o opens or closes a phrase (the OC modeling) or some combination of the two. In any case, the local signals can be combined to determine the phrases in the inp</context>
</contexts>
<marker>Punyakanok, Roth, 2000</marker>
<rawString>V. Punyakanok and D. Roth. 2000. Inference with classifiers. Tech. Report UIUCDCS-R-2000-2181, URIC Computer Science Department, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proc. of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="3699" citStr="Rabiner, 1989" startWordPosition="629" endWordPosition="630">itial state distribution Pi (s), a statetransition distribution P(81.s&apos;) for s, ES and an observation distribution P(ols) for o E 0 and s E S.3 In a supervised learning task, an observation sequence 0 =&lt; 01,02, ... on &gt; is supervised by a corresponding state sequence S =&lt; Si, s2, • • • Sn &gt;• The supervision can also be supplied, as described in Sec. 1, using the local signals. Constraints can be incorporated into the HMM by constraining the state transition probability distribution P(.918&apos;). For example, set P(s181) = 0 for all s, s&apos; such that the transition from s&apos; to s is not allowed. 3See (Rabiner, 1989) for a comprehensive tutorial. 107 Combining HMM and classifiers (artificial neural networks) has been exploited in speech recognition (Morgan and Bourlard, 1995), however, with some differences from this work. 2.1 HMM with Classifiers To recover the most likely state sequence in HMM, we wish to estimate all the required probability distributions. As in Sec. 1 we assume to have local signals that indicate the state. That is, we are given classifiers with states as their outcomes. Formally, we assume that Pt(sio) is given where t is the time step in the sequence. In order to use this informatio</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. of the IEEE, 77(2):257-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proc. of WVLC&apos;95.</booktitle>
<contexts>
<context position="1457" citStr="Ramshaw and Marcus, 1995" startWordPosition="227" endWordPosition="230">gorithms under both models and study them experimentally in the context of shallow parsing.&apos; 1 Identifying Phrase Structure The problem of identifying phrase structure can be formalized as follows. Given an input string o =&lt; oi, 02, , o7 &gt;, a phrase is a substring of consecutive input symbols o, oi+i, , 0. Some external mechanism is assumed to consistently (or stochastically) annotate substrings as phrases2. Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Murioz et al., 1999; Cardie and Pierce, 1998). The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which * This research is supported by NSF grants IIS-9801638, SBR-9873450 and IIS-9984168. &apos;Full version is in (Punyakanok and Roth, 2000). 2We assume here a single type of phrase, and thus each input symbol is either in a phrase or outside it. All the methods we discuss can be extended to deal with several kinds of phrases in a string, including different kinds of phrases and embedded phrases. are indicative to t</context>
<context position="10479" citStr="Ramshaw and Marcus, 1995" startWordPosition="1782" endWordPosition="1786"> and output the outcomes for all targets (states). We verified experimentally on the training data that the output for each state is indeed a distribution function and can be used in further processing as P(slo) (details omitted). 5 Experiments We experimented both with base noun phrases (NP) and subject-verb patterns (SV) and show results for two different representations of the observations (that is, different feature sets for the classifiers) - part of speech (POS) tags only and POS with additional lexical information (words). The data sets used are the standard data sets for this problem (Ramshaw and Marcus, 1995; Argamon et al., 1999; Munoz et al., 1999; Tjong Kim Sang and Veenstra, 1999) taken from the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993). For each model we study three different classifiers. The simple classifier corresponds to the standard HMM in which P(ols) is estimated directly from the data. The NB (naive Bayes) and SNoW classifiers use the same feature set, conjunctions of size 3 of POS tags (+ words) in a window of size 6 around the target word. The first important observation is that the SV task is significantly more difficult than the NP task. This is consis</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-based learning. In Proc. of WVLC&apos;95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In Proc. of AAAI&apos;98,</booktitle>
<pages>806--813</pages>
<contexts>
<context position="9126" citStr="Roth, 1998" startWordPosition="1565" endWordPosition="1566">comes 0, nOi, nOo (open, not open inside, not open outside) and C, nCi, nCo, resp. The state-transition diagram in figure 1 captures the order constraints. Our modeling of the problem is a modification of our earlier work on this topic that has been found to be quite successful compared to other learning methods attempted on this problem (Munoz et al., 1999) and in particular, better than the IO modeling of the problem (Munoz et al., 1999). Figure 1: State-transition diagramfor the phrase recognition problem. The classifier we use to learn the states as a function of the observations is SNoW (Roth, 1998; Carleson et al., 1999), a multi-class classifier that is specifically tailored for large scale learning tasks. The SNoW learning architecture learns a sparse network of linear functions, in which the targets (states, in this case) are represented as linear functions over a common feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all 4Another solution in which the classifiers&apos; suggestions inside each phrase are also accounted for is possible. mechanism over the activation value of the target classes in this case. The activation value itself is computed u</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proc. of AAAI&apos;98, pages 806-813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>J Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proc. of EACL&apos;99.</booktitle>
<contexts>
<context position="10557" citStr="Sang and Veenstra, 1999" startWordPosition="1797" endWordPosition="1800">on the training data that the output for each state is indeed a distribution function and can be used in further processing as P(slo) (details omitted). 5 Experiments We experimented both with base noun phrases (NP) and subject-verb patterns (SV) and show results for two different representations of the observations (that is, different feature sets for the classifiers) - part of speech (POS) tags only and POS with additional lexical information (words). The data sets used are the standard data sets for this problem (Ramshaw and Marcus, 1995; Argamon et al., 1999; Munoz et al., 1999; Tjong Kim Sang and Veenstra, 1999) taken from the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993). For each model we study three different classifiers. The simple classifier corresponds to the standard HMM in which P(ols) is estimated directly from the data. The NB (naive Bayes) and SNoW classifiers use the same feature set, conjunctions of size 3 of POS tags (+ words) in a window of size 6 around the target word. The first important observation is that the SV task is significantly more difficult than the NP task. This is consistent for all models and all features sets. When comparing between different mo</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>E. F. Tjong Kim Sang and J. Veenstra. 1999. Representing text chunks. In Proc. of EACL&apos;99.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>