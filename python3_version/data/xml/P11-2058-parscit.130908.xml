<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.261267">
<title confidence="0.9970235">
Modeling Wisdom of Crowds Using
Latent Mixture of Discriminative Experts
</title>
<author confidence="0.977682">
Derya Ozkan and Louis-Philippe Morency
</author>
<affiliation confidence="0.995213">
Institute for Creative Technologies
University of Southern California
</affiliation>
<email confidence="0.999391">
{ozkan,morency}@ict.usc.edu
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999777384615385">
In many computational linguistic scenarios,
training labels are subjectives making it nec-
essary to acquire the opinions of multiple an-
notators/experts, which is referred to as ”wis-
dom of crowds”. In this paper, we propose a
new approach for modeling wisdom of crowds
based on the Latent Mixture of Discrimina-
tive Experts (LMDE) model that can automat-
ically learn the prototypical patterns and hid-
den dynamic among different experts. Experi-
ments show improvement over state-of-the-art
approaches on the task of listener backchannel
prediction in dyadic conversations.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980132075472">
In many real life scenarios, it is hard to collect
the actual labels for training, because it is expen-
sive or the labeling is subjective. To address this
issue, a new direction of research appeared in the
last decade, taking full advantage of the ”wisdom of
crowds” (Surowiecki, 2004). In simple words, wis-
dom of crowds enables parallel acquisition of opin-
ions from multiple annotators/experts.
In this paper, we propose a new method to fuse
wisdom of crowds. Our approach is based on the
Latent Mixture of Discriminative Experts (LMDE)
model originally introduced for multimodal fu-
sion (Ozkan et al., 2010). In our Wisdom-LMDE
model, a discriminative expert is trained for each
crowd member. The key advantage of our compu-
tational model is that it can automatically discover
the prototypical patterns of experts and learn the dy-
namic between these patterns. An overview of our
approach is depicted in Figure 1.
We validate our model on the challenging task of
listener backchannel feedback prediction in dyadic
conversations. Backchannel feedback includes the
nods and paraverbals such as ”uh-huh” and ”mm-
hmm” that listeners produce as they are speaking.
Backchannels play a significant role in determining
the nature of a social exchange by showing rapport
and engagement (Gratch et al., 2007). When these
signals are positive, coordinated and reciprocated,
they can lead to feelings of rapport and promote
beneficial outcomes in diverse areas such as nego-
tiations and conflict resolution (Drolet and Morris,
2000), psychotherapeutic effectiveness (Tsui and
Schultz, 1985), improved test performance in class-
rooms (Fuchs, 1987) and improved quality of child
care (Burns, 1984). Supporting such fluid interac-
tions has become an important topic of virtual hu-
man research. In particular, backchannel feedback
has received considerable interest due to its perva-
siveness across languages and conversational con-
texts. By correctly predicting backchannel feed-
back, virtual agent and robots can have stronger
sense of rapport.
What makes backchannel prediction task well-
suited for our model is that listener feedback varies
between people and is often optional (listeners can
always decide to give feedback or not). A successful
computational model of backchannel must be able
to learn these variations among listeners. Wisdom-
LMDE is a generic approach designed to integrate
opinions from multiple listeners.
In our experiments, we validate the performance
of our approach using a dataset of 43 storytelling
dyadic interactions. Our analysis suggests three pro-
</bodyText>
<page confidence="0.989875">
335
</page>
<note confidence="0.7812005">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 335–340,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.9967648">
Latent Mixture of
Discriminative Experts
Wisdom of crowds
(listener backchannel)
y1 y2 y3 yn
h1h1 h2 h3 hn
x1
Speaker
x1
Time
Words
Look at listener
x1 x2 x3 xn
Pitch
Gaze
</figure>
<figureCaption confidence="0.9202685">
Figure 1: Left: Our approach applied to backchannel prediction: (1) multiple listeners experience the same series of
stimuli (pre-recorded speakers) and (2) a Wisdom-LMDE model is learned using this wisdom of crowds, associating
one expert for each listener. Right: Baseline models used in our experiments: a) Conditional Random Fields (CRF),
b) Latent Dynamic Conditional Random Fields (LDCRF), c) CRF Mixture of Experts (no latent variable)
</figureCaption>
<bodyText confidence="0.999043">
totypical patterns for backchannel feedback. By
automatically identifying these prototypical pat-
terns and learning the dynamic, our Wisdom-LMDE
model outperforms the previous approaches for lis-
tener backchannel prediction.
</bodyText>
<subsectionHeader confidence="0.989928">
1.1 Previous Work
</subsectionHeader>
<bodyText confidence="0.999988966666667">
Several researchers have developed models to pre-
dict when backchannel should happen. Ward and
Tsukahara (2000) propose a unimodal approach
where backchannels are associated with a region of
low pitch lasting 110ms during speech. Nishimura et
al. (2007) present a unimodal decision-tree approach
for producing backchannels based on prosodic fea-
tures. Cathcart et al. (2003) propose a unimodal
model based on pause duration and trigram part-of-
speech frequency.
Wisdom of crowds was first defined and used in
business world by Surowiecki (2004). Later, it has
been applied to other research areas as well. Raykar
et. al. (2010) proposed a probabilistic approach for
supervised learning tasks for which multiple annota-
tors provide labels but not an absolute gold standard.
Snow et. al. (2008) show that using non-expert la-
bels for training machine learning algorithms can be
as effective as using a gold standard annotation.
In this paper, we present a computational ap-
proach for listener backchannel prediction that ex-
ploits multiple listeners. Our model takes into ac-
count the differences in people’s reactions, and au-
tomatically learns the hidden structure among them.
The rest of the paper is organized as follows. In
Section 2, we present the wisdom acquisition pro-
cess. Then, we describe our Wisdom-LMDE model
in Section 3. Experimentals are presented in Sec-
tion 4. Finally, we conclude with discussions and
future works in Section 5.
</bodyText>
<sectionHeader confidence="0.965364" genericHeader="method">
2 Wisdom Acquisition
</sectionHeader>
<bodyText confidence="0.999865789473684">
It is known that culture, age and gender affect peo-
ple’s nonverbal behaviors (Linda L. Carli and Loe-
ber, 1995; Matsumoto, 2006). Therefore, there
might be variations among people’s reactions even
when experiencing the same situation. To effi-
ciently acquire responses from multiple listeners, we
employ the Parasocial Consensus Sampling (PCS)
paradigm (Huang et al., 2010), which is based on the
theory that people behave similarly when interact-
ing through a media (e.g., video conference). Huang
et al. (2010) showed that a virtual human driven by
PCS approach creates significantly more rapport and
is perceived as more believable than the virtual hu-
man driven by face-to-face interaction data (from ac-
tual listener). This result indicates that the parasocial
paradigm is a viable source of information for wis-
dom of crowds.
In practice, PCS is applied by having participants
watch pre-recorded speaker videos drawn from a
</bodyText>
<page confidence="0.99668">
336
</page>
<table confidence="0.9991465">
Listener1 Listener2 Listener3 Listener4 Listener5 Listener6 Listener7 Listener8 Listener9
pause POS:NN pause pause pause POS:NN Eyebrow up eye gaze lowness
label:sub pause POS:NN POS:NN dirdist:L1 pause dirdist:L8+ dirdist:R1 eye gaze
POS:NN label:pmod label:nmod low pitch low pitch low pitch POS:NN POS:JJ pause
</table>
<tableCaption confidence="0.996834">
Table 1: Most predictive features for each listener from our wisdom dataset. This analysis suggests three prototypical
patterns for backchannel feedback.
</tableCaption>
<bodyText confidence="0.999953285714286">
dyadic story-telling dataset. In our experiments,
we used 43 video-recorded dyadic interactions from
the RAPPORT1 dataset (Gratch et al., 2006). This
dataset was drawn from a study of face-to-face
narrative discourse (’quasi-monologic’ storytelling).
The videos of the actual listeners were manually an-
notated for backchannel feedback. For PCS wis-
dom acquisition, we recruited 9 participants, who
were told to pretend they are an active listener and
press the keyboard whenever they felt like provid-
ing backchannel feedback. This provides us the re-
sponses from multiple listeners all interacting with
the same speaker, hence the wisdom necessary to
model the variability among listeners.
</bodyText>
<sectionHeader confidence="0.937136" genericHeader="method">
3 Modeling Wisdom of Crowds
</sectionHeader>
<bodyText confidence="0.999988222222222">
Given the wisdom of multiple listeners, our goal is to
create a computational model of backchannel feed-
back. Although listener responses vary among indi-
viduals, we expect some patterns in these responses.
Therefore, we first analyze the most predictive fea-
tures for each listener and search for prototypical
patterns (in Section 3.1). Then, we present our
Wisdom-LMDE that allows to automatically learn
the hidden structure within listener responses.
</bodyText>
<subsectionHeader confidence="0.999197">
3.1 Wisdom Analysis
</subsectionHeader>
<bodyText confidence="0.99969675">
We analyzed our wisdom data to see the most rel-
evant speaker features when predicting responses
from each individual listener. (The complete list of
speaker features are described in Section 4.1.) We
used a feature ranking scheme based on a sparse
regularization technique, as described in (Ozkan and
Morency, 2010). It allows us to identify the speaker
features most predictive of each listener backchan-
nel feedback. The top 3 features for all 9 listeners
are listed in Table 1.
This analysis suggests three prototypical patterns.
For the first 3 listeners, pause in speech and syntac-
</bodyText>
<footnote confidence="0.945336">
1http://rapport.ict.usc.edu/
</footnote>
<bodyText confidence="0.999877461538462">
tic information (POS:NN) are more important. The
next 3 experts include a prosodic feature, low pitch,
which is coherent with earlier findings (Nishimura
et al., 2007; Ward and Tsukahara, 2000). It is inter-
esting to see that the last 3 experts incorporate visual
information when predicting backchannel feedback.
This is in line with Burgoon et al. (Burgoon et al.,
1995) work showing that speaker gestures are of-
ten correlated with listener feedback. These results
clearly suggest that variations be present among lis-
teners and some prototypical patterns may exist.
Based on these observations, we propose new com-
putational model for listener backchannel.
</bodyText>
<subsectionHeader confidence="0.988303">
3.2 Computational Model: Wisdom-LMDE
</subsectionHeader>
<bodyText confidence="0.999619608695652">
The goals of our computational model are to au-
tomatically discover the prototypical patterns of
backchannel feedback and learn the dynamic be-
tween these patterns. This will allow the compu-
tational model to accurately predict the responses of
a new listener even if he/she changes her backchan-
nel patterns in the middle of the interaction. It will
also improve generalization by allowing mixtures of
these prototypical patterns.
To achieve these goals, we propose a variant of the
Latent Mixture of Discriminative Experts (Ozkan et
al., 2010) which takes full advantage of the wisdom
of crowds. Our Wisdom-LMDE model is based on
a two step process: a Conditional Random Field
(CRF, see Figure 1a) is learned for each wisdom
listener, and the outputs of these expert models are
used as input to a Latent Dynamic Conditional Ran-
dom Field (LDCRF, see Figure 1b) model, which is
capable of learning the hidden structure within the
experts. In our Wisdom-LMDE, each expert cor-
responds to a different listener from the wisdom of
crowds. More details about training and inference of
LMDE can be found in Ozkan et al. (2010).
</bodyText>
<page confidence="0.996414">
337
</page>
<sectionHeader confidence="0.999674" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999804166666666">
To confirm the validity of our Wisdom-LMDE
model, we compare its performance with compu-
tational models previously proposed. As motivated
earlier, we focus our experiments on predicting lis-
tener backchannel since it is a well-suited task where
variability exists among listeners.
</bodyText>
<subsectionHeader confidence="0.998542">
4.1 Multimodal Speaker Features
</subsectionHeader>
<bodyText confidence="0.999975463414634">
The speaker videos were transcribed and annotated
to extract the following features:
Lexical: Some studies have suggested an asso-
ciation between lexical features and listener feed-
back (Cathcart et al., 2003). Therefore, we use all
the words (i.e., unigrams) spoken by the speaker.
Syntactic structure: Using a CRF part-of-speech
(POS) tagger and a data-driven left-to-right shift-
reduce dependency parser (Sagae and Tsujii, 2007)
we extract four types of features from a syntactic de-
pendency structure corresponding to the utterance:
POS tags and grammatical function for each word,
POS tag of the syntactic head, distance and direction
from each word to its syntactic head.
Prosody: Prosody refers to the rhythm, pitch and
intonation of speech. Several studies have demon-
strated that listener feedback is correlated with
a speaker’s prosody (Ward and Tsukahara, 2000;
Nishimura et al., 2007). Following this, we use
downslope in pitch, pitch regions lower than 26th
percentile, drop/rise and fast drop/rise in energy of
speech, vowel volume, pause.
Visual gestures: Gestures performed by the speaker
are often correlated with listener feedback (Burgoon
et al., 1995). Eye gaze, in particular, has often been
implicated as eliciting listener feedback. Thus, we
encode the following contextual features: speaker
looking at listener, smiling, moving eyebrows up
and frowning.
Although our current method for extracting these
features requires that the entire utterance to be avail-
able for processing, this provides us with a first
step towards integrating information about syntac-
tic structure in multimodal prediction models. Many
of these features could in principle be computed in-
crementally with only a slight degradation in accu-
racy, with the exception of features that require de-
pendency links where a word’s syntactic head is to
the right of the word itself. We leave an investiga-
tion that examines only syntactic features that can be
produced incrementally in real time as future work.
</bodyText>
<subsectionHeader confidence="0.948149">
4.2 Baseline Models
</subsectionHeader>
<bodyText confidence="0.994131333333333">
Consensus Classifier In our first baseline model, we
use consensus labels to train a CRF model, which
are constructed by a similar approach presented
in (Huang et al., 2010). The consensus threshold is
set to 3 (at least 3 listeners agree to give feedback at
a point) so that it contains approximately the same
number of head nods as the actual listener. See Fig-
ure 1 for a graphical representation of CRF model.
CRF Mixture of Experts To show the importance
of latent variable in our Wisdom-LMDE model, we
trained a CRF-based mixture of discriminative ex-
perts. This model is similar to the Logarithmic
Opinion Pool (LOP) CRF suggested by Smith et
al. (2005). Similar to our Wisdom-LMDE model,
the training is performed in two steps. A graphical
representation of a CRF Mixture of experts is given
in the Figure 1.
Actual Listener (AL) Classifiers This baseline model
consists of two models: CRF and LDCRF chains
(See Figure 1). To train these models, we use the
labels of the ”Actual Listeners” (AL) from the RAP-
PORT dataset.
Multimodal LMDE In this baseline model, we com-
pare our Wisdom LMDE to a multimodal LMDE,
where each expert refers to one of 5 different set of
multimodal features as presented in (Ozkan et al.,
2010): lexical, prosodic, part-of-speech, syntactic,
and visual.
Random Classifier Our last baseline model is a ran-
dom backchannel generator as desribed by Ward
and Tsukahara (2000). This model randomly gener-
ates backchannels whenever some pre-defined con-
ditions in the prosody of the speech is purveyed.
</bodyText>
<subsectionHeader confidence="0.99785">
4.3 Methodolgy
</subsectionHeader>
<bodyText confidence="0.999959">
We performed hold-out testing on a randomly se-
lected subset of 10 interactions. The training set
contains the remaining 33 interactions. Model pa-
rameters were validated by using a 3-fold cross-
validation strategy on the training set. Regulariza-
</bodyText>
<page confidence="0.999193">
338
</page>
<tableCaption confidence="0.9987885">
Table 2: Comparison of our Wisdom-LMDE model with previously proposed models. The last column shows the
paired one tailed t-test results comparing Wisdom LMDE to each model.
</tableCaption>
<bodyText confidence="0.998521076923077">
tion values used are 10k for k = -1,0,..,3. Numbers
of hidden states used in the LDCRF models were
2, 3 and 4. We use the hCRF library2 for training
of CRFs and LDCRFs. Our Wisdom-LMDE model
was implemented in Matlab based on the hCRF li-
brary. Following (Morency et al., 2008), we use
an encoding dictionary to represent our features.
The performance is measured by using the F-score,
which is the weighted harmonic mean of precision
and recall. A backchannel is predicted correctly if
a peak happens during an actual listener backchan-
nel with high enough probability. The threshold was
selected automatically during validation.
</bodyText>
<subsectionHeader confidence="0.959884">
4.4 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99969945">
Before reviewing the prediction results, is it impor-
tant to remember that backchannel feedback is an
optional phenomena, where the actual listener may
or may not decide on giving feedback (Ward and
Tsukahara, 2000). Therefore, results from predic-
tion tasks are expected to have lower accuracies as
opposed to recognition tasks where labels are di-
rectly observed (e.g., part-of-speech tagging).
Table 2 summarizes our experiments comparing
our Wisdom-LMDE model with state-of-the-art ap-
proaches for behavior prediction (see Section 4.2).
Our Wisdom-LMDE model achieves the best F1
score. Statistical t-test analysis show that Wisdom-
LMDE is significantly better than Consensus Clas-
sifier, AL Classifier (LDCRF), Multimodel LMDE
and Random Classifier.
The second best F1 score is achieved by CRF
Mixture of experts, which is the only model among
other baseline models that combines different lis-
tener labels in a late fusion manner. This result
</bodyText>
<footnote confidence="0.82999">
2http://sourceforge.net/projects/hrcf/
</footnote>
<bodyText confidence="0.999880583333333">
supports our claim that wisdom of clouds improves
learning of prediction models. CRF Mixture model
is a linear combination of the experts, whereas
Wisdom-LMDE enables different weighting of ex-
perts at different point in time. By using hidden
states, Wisdom-LMDE can automatically learn the
prototypical patterns between listeners.
One really interesting result is that the optimal
number of hidden states in the Wisdom-LMDE
model (after cross-validation) is 3. This is coherent
with our qualitative analysis in Section 3.1, where
we observed 3 prototypical patterns.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999975769230769">
In this paper, we proposed a new approach called
Wisdom-LMDE for modeling wisdom of crowds,
which automatically learns the hidden structure in
listener responses. We applied this method on
the task of listener backchannel feedback predic-
tion, and showed improvement over previous ap-
proaches. Both our qualitative analysis and exper-
imental results suggest that prototypical patterns ex-
ist when predicting listener backchannel feedback.
The Wisdom-LMDE is a generic model applicable
to multiple sequence labeling tasks (such as emotion
analysis and dialogue intent recognition), where la-
bels are subjective (i.e. small inter-coder reliability).
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999738142857143">
This material is based upon work supported by
the National Science Foundation under Grant No.
0917321 and the U.S. Army Research, Develop-
ment, and Engineering Command (RDE-COM).
The content does not necessarily reflect the position
or the policy of the Government, and no official en-
dorsement should be inferred.
</bodyText>
<page confidence="0.998633">
339
</page>
<sectionHeader confidence="0.996113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999771604938271">
Judee K. Burgoon, Lesa A. Stern, and Leesa Dillman.
1995. Interpersonal adaptation: Dyadic interaction
patterns. Cambridge University Press, Cambridge.
M. Burns. 1984. Rapport and relationships: The basis of
child care. Journal of Child Care, 4:47–57.
N. Cathcart, Jean Carletta, and Ewan Klein. 2003. A
shallow model of backchannel continuers in spoken
dialogue. In European Chapter of the Association for
Computational Linguistics. 51–58.
Aimee L. Drolet and Michael W. Morris. 2000. Rap-
port in conflict resolution: Accounting for how face-
to-face contact fosters mutual cooperation in mixed-
motive conflicts. Journal of Experimental Social Psy-
chology, 36(1):26–50.
D. Fuchs. 1987. Examiner familiarity effects on test per-
formance: Implications for training and practice. Top-
ics in Early Childhood Special Education, 7:90–104.
J. Gratch, A. Okhmatovskaia, F. Lamothe, S. Marsella,
M. Morales, R.J. Werf, and L.-P. Morency. 2006. Vir-
tual rapport. Proceedings of International Conference
on Intelligent Virtual Agents (IVA), Marina del Rey,
CA.
Jonathan Gratch, Ning Wang, Jillian Gerten, and Edward
Fast. 2007. Creating rapport with virtual agents. In
IVA.
L. Huang, L.-P. Morency, and J. Gratch:. 2010. Paraso-
cial consensus sampling: combining multiple perspec-
tives to learn virtual human behavior. In Interna-
tional Conference on Autonomous Agents and Multi-
agent Systems (AAMAS).
Suzanne J. LaFleur Linda L. Carli and Christopher C.
Loeber. 1995. Nonverbal behavior, gender, and influ-
ence. Journal of Personality and Social Psychology.
68, 1030-1041.
D. Matsumoto. 2006. Culture and Nonverbal Behav-
ior. The Sage Handbook of Nonverbal Communica-
tion, Sage Publications Inc.
L.-P. Morency, I. de Kok, and J. Gratch. 2008. Predict-
ing listener backchannels: A probabilistic multimodal
approach. In Proceedings of the Conference on Intel-
ligent Virutal Agents (IVA).
Ryota Nishimura, Norihide Kitaoka, and Seiichi Naka-
gawa. 2007. A spoken dialog system for chat-like
conversations considering response timing. Interna-
tional Conference on Text, Speech and Dialog. 599-
606.
D. Ozkan and L.-P. Morency. 2010. Concensus of self-
features for nonverbal behavior analysis. In Human
Behavior Understanding in conjucion with Interna-
tional Conference in Pattern Recognition.
D. Ozkan, K. Sagae, and L.-P. Morency. 2010. La-
tent mixture of discriminative experts for multimodal
prediction modeling. In International Conference on
Computational Linguistics (COLING).
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, Linda Moy, and David Blei. 2010. Learning
from crowds.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044–1050,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL,
pages 18–25.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2008.
Cheap and fast - but is it good? Evaluating non-expert
annotations for natural language tasks.
James Surowiecki. 2004. The Wisdom of Crowds: Why
the Many Are Smarter Than the Few and How Col-
lective Wisdom Shapes Business, Economies, Societies
and Nations. Doubleday.
P. Tsui and G.L. Schultz. 1985. Failure of rapport: Why
psychotheraputic engagement fails in the treatment of
asian clients. American Journal of Orthopsychiatry,
55:561–569.
N. Ward and W. Tsukahara. 2000. Prosodic fea-
tures which cue back-channel responses in english and
japanese. Journal of Pragmatics. 23, 1177–1207.
</reference>
<page confidence="0.998292">
340
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.838389">
<title confidence="0.986304">Modeling Wisdom of Crowds Latent Mixture of Discriminative Experts</title>
<author confidence="0.872518">Derya Ozkan</author>
<author confidence="0.872518">Louis-Philippe</author>
<affiliation confidence="0.992159">Institute for Creative University of Southern</affiliation>
<abstract confidence="0.997863214285714">In many computational linguistic scenarios, training labels are subjectives making it necessary to acquire the opinions of multiple annotators/experts, which is referred to as ”wisdom of crowds”. In this paper, we propose a new approach for modeling wisdom of crowds based on the Latent Mixture of Discriminative Experts (LMDE) model that can automatically learn the prototypical patterns and hidden dynamic among different experts. Experiments show improvement over state-of-the-art approaches on the task of listener backchannel prediction in dyadic conversations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Judee K Burgoon</author>
<author>Lesa A Stern</author>
<author>Leesa Dillman</author>
</authors>
<title>Interpersonal adaptation: Dyadic interaction patterns.</title>
<date>1995</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="9442" citStr="Burgoon et al., 1995" startWordPosition="1430" endWordPosition="1433">ost predictive of each listener backchannel feedback. The top 3 features for all 9 listeners are listed in Table 1. This analysis suggests three prototypical patterns. For the first 3 listeners, pause in speech and syntac1http://rapport.ict.usc.edu/ tic information (POS:NN) are more important. The next 3 experts include a prosodic feature, low pitch, which is coherent with earlier findings (Nishimura et al., 2007; Ward and Tsukahara, 2000). It is interesting to see that the last 3 experts incorporate visual information when predicting backchannel feedback. This is in line with Burgoon et al. (Burgoon et al., 1995) work showing that speaker gestures are often correlated with listener feedback. These results clearly suggest that variations be present among listeners and some prototypical patterns may exist. Based on these observations, we propose new computational model for listener backchannel. 3.2 Computational Model: Wisdom-LMDE The goals of our computational model are to automatically discover the prototypical patterns of backchannel feedback and learn the dynamic between these patterns. This will allow the computational model to accurately predict the responses of a new listener even if he/she chang</context>
<context position="12377" citStr="Burgoon et al., 1995" startWordPosition="1887" endWordPosition="1890"> grammatical function for each word, POS tag of the syntactic head, distance and direction from each word to its syntactic head. Prosody: Prosody refers to the rhythm, pitch and intonation of speech. Several studies have demonstrated that listener feedback is correlated with a speaker’s prosody (Ward and Tsukahara, 2000; Nishimura et al., 2007). Following this, we use downslope in pitch, pitch regions lower than 26th percentile, drop/rise and fast drop/rise in energy of speech, vowel volume, pause. Visual gestures: Gestures performed by the speaker are often correlated with listener feedback (Burgoon et al., 1995). Eye gaze, in particular, has often been implicated as eliciting listener feedback. Thus, we encode the following contextual features: speaker looking at listener, smiling, moving eyebrows up and frowning. Although our current method for extracting these features requires that the entire utterance to be available for processing, this provides us with a first step towards integrating information about syntactic structure in multimodal prediction models. Many of these features could in principle be computed incrementally with only a slight degradation in accuracy, with the exception of features</context>
</contexts>
<marker>Burgoon, Stern, Dillman, 1995</marker>
<rawString>Judee K. Burgoon, Lesa A. Stern, and Leesa Dillman. 1995. Interpersonal adaptation: Dyadic interaction patterns. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Burns</author>
</authors>
<title>Rapport and relationships: The basis of child care.</title>
<date>1984</date>
<journal>Journal of Child Care,</journal>
<pages>4--47</pages>
<contexts>
<context position="2479" citStr="Burns, 1984" startWordPosition="375" endWordPosition="376">and paraverbals such as ”uh-huh” and ”mmhmm” that listeners produce as they are speaking. Backchannels play a significant role in determining the nature of a social exchange by showing rapport and engagement (Gratch et al., 2007). When these signals are positive, coordinated and reciprocated, they can lead to feelings of rapport and promote beneficial outcomes in diverse areas such as negotiations and conflict resolution (Drolet and Morris, 2000), psychotherapeutic effectiveness (Tsui and Schultz, 1985), improved test performance in classrooms (Fuchs, 1987) and improved quality of child care (Burns, 1984). Supporting such fluid interactions has become an important topic of virtual human research. In particular, backchannel feedback has received considerable interest due to its pervasiveness across languages and conversational contexts. By correctly predicting backchannel feedback, virtual agent and robots can have stronger sense of rapport. What makes backchannel prediction task wellsuited for our model is that listener feedback varies between people and is often optional (listeners can always decide to give feedback or not). A successful computational model of backchannel must be able to lear</context>
</contexts>
<marker>Burns, 1984</marker>
<rawString>M. Burns. 1984. Rapport and relationships: The basis of child care. Journal of Child Care, 4:47–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cathcart</author>
<author>Jean Carletta</author>
<author>Ewan Klein</author>
</authors>
<title>A shallow model of backchannel continuers in spoken dialogue.</title>
<date>2003</date>
<booktitle>In European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>51--58</pages>
<contexts>
<context position="4788" citStr="Cathcart et al. (2003)" startWordPosition="712" endWordPosition="715">ariable) totypical patterns for backchannel feedback. By automatically identifying these prototypical patterns and learning the dynamic, our Wisdom-LMDE model outperforms the previous approaches for listener backchannel prediction. 1.1 Previous Work Several researchers have developed models to predict when backchannel should happen. Ward and Tsukahara (2000) propose a unimodal approach where backchannels are associated with a region of low pitch lasting 110ms during speech. Nishimura et al. (2007) present a unimodal decision-tree approach for producing backchannels based on prosodic features. Cathcart et al. (2003) propose a unimodal model based on pause duration and trigram part-ofspeech frequency. Wisdom of crowds was first defined and used in business world by Surowiecki (2004). Later, it has been applied to other research areas as well. Raykar et. al. (2010) proposed a probabilistic approach for supervised learning tasks for which multiple annotators provide labels but not an absolute gold standard. Snow et. al. (2008) show that using non-expert labels for training machine learning algorithms can be as effective as using a gold standard annotation. In this paper, we present a computational approach </context>
<context position="11418" citStr="Cathcart et al., 2003" startWordPosition="1741" endWordPosition="1744">ore details about training and inference of LMDE can be found in Ozkan et al. (2010). 337 4 Experiments To confirm the validity of our Wisdom-LMDE model, we compare its performance with computational models previously proposed. As motivated earlier, we focus our experiments on predicting listener backchannel since it is a well-suited task where variability exists among listeners. 4.1 Multimodal Speaker Features The speaker videos were transcribed and annotated to extract the following features: Lexical: Some studies have suggested an association between lexical features and listener feedback (Cathcart et al., 2003). Therefore, we use all the words (i.e., unigrams) spoken by the speaker. Syntactic structure: Using a CRF part-of-speech (POS) tagger and a data-driven left-to-right shiftreduce dependency parser (Sagae and Tsujii, 2007) we extract four types of features from a syntactic dependency structure corresponding to the utterance: POS tags and grammatical function for each word, POS tag of the syntactic head, distance and direction from each word to its syntactic head. Prosody: Prosody refers to the rhythm, pitch and intonation of speech. Several studies have demonstrated that listener feedback is co</context>
</contexts>
<marker>Cathcart, Carletta, Klein, 2003</marker>
<rawString>N. Cathcart, Jean Carletta, and Ewan Klein. 2003. A shallow model of backchannel continuers in spoken dialogue. In European Chapter of the Association for Computational Linguistics. 51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aimee L Drolet</author>
<author>Michael W Morris</author>
</authors>
<title>Rapport in conflict resolution: Accounting for how faceto-face contact fosters mutual cooperation in mixedmotive conflicts.</title>
<date>2000</date>
<journal>Journal of Experimental Social Psychology,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="2317" citStr="Drolet and Morris, 2000" startWordPosition="351" endWordPosition="354">picted in Figure 1. We validate our model on the challenging task of listener backchannel feedback prediction in dyadic conversations. Backchannel feedback includes the nods and paraverbals such as ”uh-huh” and ”mmhmm” that listeners produce as they are speaking. Backchannels play a significant role in determining the nature of a social exchange by showing rapport and engagement (Gratch et al., 2007). When these signals are positive, coordinated and reciprocated, they can lead to feelings of rapport and promote beneficial outcomes in diverse areas such as negotiations and conflict resolution (Drolet and Morris, 2000), psychotherapeutic effectiveness (Tsui and Schultz, 1985), improved test performance in classrooms (Fuchs, 1987) and improved quality of child care (Burns, 1984). Supporting such fluid interactions has become an important topic of virtual human research. In particular, backchannel feedback has received considerable interest due to its pervasiveness across languages and conversational contexts. By correctly predicting backchannel feedback, virtual agent and robots can have stronger sense of rapport. What makes backchannel prediction task wellsuited for our model is that listener feedback varie</context>
</contexts>
<marker>Drolet, Morris, 2000</marker>
<rawString>Aimee L. Drolet and Michael W. Morris. 2000. Rapport in conflict resolution: Accounting for how faceto-face contact fosters mutual cooperation in mixedmotive conflicts. Journal of Experimental Social Psychology, 36(1):26–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Fuchs</author>
</authors>
<title>Examiner familiarity effects on test performance: Implications for training and practice.</title>
<date>1987</date>
<booktitle>Topics in Early Childhood Special Education,</booktitle>
<pages>7--90</pages>
<contexts>
<context position="2430" citStr="Fuchs, 1987" startWordPosition="367" endWordPosition="368">rsations. Backchannel feedback includes the nods and paraverbals such as ”uh-huh” and ”mmhmm” that listeners produce as they are speaking. Backchannels play a significant role in determining the nature of a social exchange by showing rapport and engagement (Gratch et al., 2007). When these signals are positive, coordinated and reciprocated, they can lead to feelings of rapport and promote beneficial outcomes in diverse areas such as negotiations and conflict resolution (Drolet and Morris, 2000), psychotherapeutic effectiveness (Tsui and Schultz, 1985), improved test performance in classrooms (Fuchs, 1987) and improved quality of child care (Burns, 1984). Supporting such fluid interactions has become an important topic of virtual human research. In particular, backchannel feedback has received considerable interest due to its pervasiveness across languages and conversational contexts. By correctly predicting backchannel feedback, virtual agent and robots can have stronger sense of rapport. What makes backchannel prediction task wellsuited for our model is that listener feedback varies between people and is often optional (listeners can always decide to give feedback or not). A successful comput</context>
</contexts>
<marker>Fuchs, 1987</marker>
<rawString>D. Fuchs. 1987. Examiner familiarity effects on test performance: Implications for training and practice. Topics in Early Childhood Special Education, 7:90–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gratch</author>
<author>A Okhmatovskaia</author>
<author>F Lamothe</author>
<author>S Marsella</author>
<author>M Morales</author>
<author>R J Werf</author>
<author>L-P Morency</author>
</authors>
<title>Virtual rapport.</title>
<date>2006</date>
<booktitle>Proceedings of International Conference on Intelligent Virtual Agents (IVA),</booktitle>
<location>Marina del Rey, CA.</location>
<contexts>
<context position="7413" citStr="Gratch et al., 2006" startWordPosition="1116" endWordPosition="1119">om a 336 Listener1 Listener2 Listener3 Listener4 Listener5 Listener6 Listener7 Listener8 Listener9 pause POS:NN pause pause pause POS:NN Eyebrow up eye gaze lowness label:sub pause POS:NN POS:NN dirdist:L1 pause dirdist:L8+ dirdist:R1 eye gaze POS:NN label:pmod label:nmod low pitch low pitch low pitch POS:NN POS:JJ pause Table 1: Most predictive features for each listener from our wisdom dataset. This analysis suggests three prototypical patterns for backchannel feedback. dyadic story-telling dataset. In our experiments, we used 43 video-recorded dyadic interactions from the RAPPORT1 dataset (Gratch et al., 2006). This dataset was drawn from a study of face-to-face narrative discourse (’quasi-monologic’ storytelling). The videos of the actual listeners were manually annotated for backchannel feedback. For PCS wisdom acquisition, we recruited 9 participants, who were told to pretend they are an active listener and press the keyboard whenever they felt like providing backchannel feedback. This provides us the responses from multiple listeners all interacting with the same speaker, hence the wisdom necessary to model the variability among listeners. 3 Modeling Wisdom of Crowds Given the wisdom of multipl</context>
</contexts>
<marker>Gratch, Okhmatovskaia, Lamothe, Marsella, Morales, Werf, Morency, 2006</marker>
<rawString>J. Gratch, A. Okhmatovskaia, F. Lamothe, S. Marsella, M. Morales, R.J. Werf, and L.-P. Morency. 2006. Virtual rapport. Proceedings of International Conference on Intelligent Virtual Agents (IVA), Marina del Rey, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Gratch</author>
<author>Ning Wang</author>
<author>Jillian Gerten</author>
<author>Edward Fast</author>
</authors>
<title>Creating rapport with virtual agents.</title>
<date>2007</date>
<booktitle>In IVA.</booktitle>
<contexts>
<context position="2096" citStr="Gratch et al., 2007" startWordPosition="318" endWordPosition="321">ach crowd member. The key advantage of our computational model is that it can automatically discover the prototypical patterns of experts and learn the dynamic between these patterns. An overview of our approach is depicted in Figure 1. We validate our model on the challenging task of listener backchannel feedback prediction in dyadic conversations. Backchannel feedback includes the nods and paraverbals such as ”uh-huh” and ”mmhmm” that listeners produce as they are speaking. Backchannels play a significant role in determining the nature of a social exchange by showing rapport and engagement (Gratch et al., 2007). When these signals are positive, coordinated and reciprocated, they can lead to feelings of rapport and promote beneficial outcomes in diverse areas such as negotiations and conflict resolution (Drolet and Morris, 2000), psychotherapeutic effectiveness (Tsui and Schultz, 1985), improved test performance in classrooms (Fuchs, 1987) and improved quality of child care (Burns, 1984). Supporting such fluid interactions has become an important topic of virtual human research. In particular, backchannel feedback has received considerable interest due to its pervasiveness across languages and conver</context>
</contexts>
<marker>Gratch, Wang, Gerten, Fast, 2007</marker>
<rawString>Jonathan Gratch, Ning Wang, Jillian Gerten, and Edward Fast. 2007. Creating rapport with virtual agents. In IVA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>L-P Morency</author>
<author>J Gratch</author>
</authors>
<title>Parasocial consensus sampling: combining multiple perspectives to learn virtual human behavior.</title>
<date>2010</date>
<booktitle>In International Conference on Autonomous Agents and Multiagent Systems (AAMAS).</booktitle>
<contexts>
<context position="6245" citStr="Huang et al., 2010" startWordPosition="943" endWordPosition="946">ows. In Section 2, we present the wisdom acquisition process. Then, we describe our Wisdom-LMDE model in Section 3. Experimentals are presented in Section 4. Finally, we conclude with discussions and future works in Section 5. 2 Wisdom Acquisition It is known that culture, age and gender affect people’s nonverbal behaviors (Linda L. Carli and Loeber, 1995; Matsumoto, 2006). Therefore, there might be variations among people’s reactions even when experiencing the same situation. To efficiently acquire responses from multiple listeners, we employ the Parasocial Consensus Sampling (PCS) paradigm (Huang et al., 2010), which is based on the theory that people behave similarly when interacting through a media (e.g., video conference). Huang et al. (2010) showed that a virtual human driven by PCS approach creates significantly more rapport and is perceived as more believable than the virtual human driven by face-to-face interaction data (from actual listener). This result indicates that the parasocial paradigm is a viable source of information for wisdom of crowds. In practice, PCS is applied by having participants watch pre-recorded speaker videos drawn from a 336 Listener1 Listener2 Listener3 Listener4 Lis</context>
<context position="13395" citStr="Huang et al., 2010" startWordPosition="2048" endWordPosition="2051">about syntactic structure in multimodal prediction models. Many of these features could in principle be computed incrementally with only a slight degradation in accuracy, with the exception of features that require dependency links where a word’s syntactic head is to the right of the word itself. We leave an investigation that examines only syntactic features that can be produced incrementally in real time as future work. 4.2 Baseline Models Consensus Classifier In our first baseline model, we use consensus labels to train a CRF model, which are constructed by a similar approach presented in (Huang et al., 2010). The consensus threshold is set to 3 (at least 3 listeners agree to give feedback at a point) so that it contains approximately the same number of head nods as the actual listener. See Figure 1 for a graphical representation of CRF model. CRF Mixture of Experts To show the importance of latent variable in our Wisdom-LMDE model, we trained a CRF-based mixture of discriminative experts. This model is similar to the Logarithmic Opinion Pool (LOP) CRF suggested by Smith et al. (2005). Similar to our Wisdom-LMDE model, the training is performed in two steps. A graphical representation of a CRF Mix</context>
</contexts>
<marker>Huang, Morency, Gratch, 2010</marker>
<rawString>L. Huang, L.-P. Morency, and J. Gratch:. 2010. Parasocial consensus sampling: combining multiple perspectives to learn virtual human behavior. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne J LaFleur Linda L Carli</author>
<author>Christopher C Loeber</author>
</authors>
<title>Nonverbal behavior, gender, and influence.</title>
<date>1995</date>
<journal>Journal of Personality and Social Psychology.</journal>
<volume>68</volume>
<pages>1030--1041</pages>
<contexts>
<context position="5983" citStr="Carli and Loeber, 1995" startWordPosition="906" endWordPosition="910">t a computational approach for listener backchannel prediction that exploits multiple listeners. Our model takes into account the differences in people’s reactions, and automatically learns the hidden structure among them. The rest of the paper is organized as follows. In Section 2, we present the wisdom acquisition process. Then, we describe our Wisdom-LMDE model in Section 3. Experimentals are presented in Section 4. Finally, we conclude with discussions and future works in Section 5. 2 Wisdom Acquisition It is known that culture, age and gender affect people’s nonverbal behaviors (Linda L. Carli and Loeber, 1995; Matsumoto, 2006). Therefore, there might be variations among people’s reactions even when experiencing the same situation. To efficiently acquire responses from multiple listeners, we employ the Parasocial Consensus Sampling (PCS) paradigm (Huang et al., 2010), which is based on the theory that people behave similarly when interacting through a media (e.g., video conference). Huang et al. (2010) showed that a virtual human driven by PCS approach creates significantly more rapport and is perceived as more believable than the virtual human driven by face-to-face interaction data (from actual l</context>
</contexts>
<marker>Carli, Loeber, 1995</marker>
<rawString>Suzanne J. LaFleur Linda L. Carli and Christopher C. Loeber. 1995. Nonverbal behavior, gender, and influence. Journal of Personality and Social Psychology. 68, 1030-1041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Matsumoto</author>
</authors>
<title>Culture and Nonverbal Behavior. The Sage Handbook of Nonverbal Communication,</title>
<date>2006</date>
<publisher>Sage Publications Inc.</publisher>
<contexts>
<context position="6001" citStr="Matsumoto, 2006" startWordPosition="911" endWordPosition="912">ch for listener backchannel prediction that exploits multiple listeners. Our model takes into account the differences in people’s reactions, and automatically learns the hidden structure among them. The rest of the paper is organized as follows. In Section 2, we present the wisdom acquisition process. Then, we describe our Wisdom-LMDE model in Section 3. Experimentals are presented in Section 4. Finally, we conclude with discussions and future works in Section 5. 2 Wisdom Acquisition It is known that culture, age and gender affect people’s nonverbal behaviors (Linda L. Carli and Loeber, 1995; Matsumoto, 2006). Therefore, there might be variations among people’s reactions even when experiencing the same situation. To efficiently acquire responses from multiple listeners, we employ the Parasocial Consensus Sampling (PCS) paradigm (Huang et al., 2010), which is based on the theory that people behave similarly when interacting through a media (e.g., video conference). Huang et al. (2010) showed that a virtual human driven by PCS approach creates significantly more rapport and is perceived as more believable than the virtual human driven by face-to-face interaction data (from actual listener). This res</context>
</contexts>
<marker>Matsumoto, 2006</marker>
<rawString>D. Matsumoto. 2006. Culture and Nonverbal Behavior. The Sage Handbook of Nonverbal Communication, Sage Publications Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L-P Morency</author>
<author>I de Kok</author>
<author>J Gratch</author>
</authors>
<title>Predicting listener backchannels: A probabilistic multimodal approach.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Intelligent Virutal Agents (IVA).</booktitle>
<marker>Morency, de Kok, Gratch, 2008</marker>
<rawString>L.-P. Morency, I. de Kok, and J. Gratch. 2008. Predicting listener backchannels: A probabilistic multimodal approach. In Proceedings of the Conference on Intelligent Virutal Agents (IVA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryota Nishimura</author>
<author>Norihide Kitaoka</author>
<author>Seiichi Nakagawa</author>
</authors>
<title>A spoken dialog system for chat-like conversations considering response timing.</title>
<date>2007</date>
<booktitle>International Conference on Text, Speech and Dialog.</booktitle>
<pages>599--606</pages>
<contexts>
<context position="4668" citStr="Nishimura et al. (2007)" startWordPosition="695" endWordPosition="698">ditional Random Fields (CRF), b) Latent Dynamic Conditional Random Fields (LDCRF), c) CRF Mixture of Experts (no latent variable) totypical patterns for backchannel feedback. By automatically identifying these prototypical patterns and learning the dynamic, our Wisdom-LMDE model outperforms the previous approaches for listener backchannel prediction. 1.1 Previous Work Several researchers have developed models to predict when backchannel should happen. Ward and Tsukahara (2000) propose a unimodal approach where backchannels are associated with a region of low pitch lasting 110ms during speech. Nishimura et al. (2007) present a unimodal decision-tree approach for producing backchannels based on prosodic features. Cathcart et al. (2003) propose a unimodal model based on pause duration and trigram part-ofspeech frequency. Wisdom of crowds was first defined and used in business world by Surowiecki (2004). Later, it has been applied to other research areas as well. Raykar et. al. (2010) proposed a probabilistic approach for supervised learning tasks for which multiple annotators provide labels but not an absolute gold standard. Snow et. al. (2008) show that using non-expert labels for training machine learning</context>
<context position="9237" citStr="Nishimura et al., 2007" startWordPosition="1396" endWordPosition="1399">eatures are described in Section 4.1.) We used a feature ranking scheme based on a sparse regularization technique, as described in (Ozkan and Morency, 2010). It allows us to identify the speaker features most predictive of each listener backchannel feedback. The top 3 features for all 9 listeners are listed in Table 1. This analysis suggests three prototypical patterns. For the first 3 listeners, pause in speech and syntac1http://rapport.ict.usc.edu/ tic information (POS:NN) are more important. The next 3 experts include a prosodic feature, low pitch, which is coherent with earlier findings (Nishimura et al., 2007; Ward and Tsukahara, 2000). It is interesting to see that the last 3 experts incorporate visual information when predicting backchannel feedback. This is in line with Burgoon et al. (Burgoon et al., 1995) work showing that speaker gestures are often correlated with listener feedback. These results clearly suggest that variations be present among listeners and some prototypical patterns may exist. Based on these observations, we propose new computational model for listener backchannel. 3.2 Computational Model: Wisdom-LMDE The goals of our computational model are to automatically discover the p</context>
<context position="12102" citStr="Nishimura et al., 2007" startWordPosition="1846" endWordPosition="1849">the speaker. Syntactic structure: Using a CRF part-of-speech (POS) tagger and a data-driven left-to-right shiftreduce dependency parser (Sagae and Tsujii, 2007) we extract four types of features from a syntactic dependency structure corresponding to the utterance: POS tags and grammatical function for each word, POS tag of the syntactic head, distance and direction from each word to its syntactic head. Prosody: Prosody refers to the rhythm, pitch and intonation of speech. Several studies have demonstrated that listener feedback is correlated with a speaker’s prosody (Ward and Tsukahara, 2000; Nishimura et al., 2007). Following this, we use downslope in pitch, pitch regions lower than 26th percentile, drop/rise and fast drop/rise in energy of speech, vowel volume, pause. Visual gestures: Gestures performed by the speaker are often correlated with listener feedback (Burgoon et al., 1995). Eye gaze, in particular, has often been implicated as eliciting listener feedback. Thus, we encode the following contextual features: speaker looking at listener, smiling, moving eyebrows up and frowning. Although our current method for extracting these features requires that the entire utterance to be available for proce</context>
</contexts>
<marker>Nishimura, Kitaoka, Nakagawa, 2007</marker>
<rawString>Ryota Nishimura, Norihide Kitaoka, and Seiichi Nakagawa. 2007. A spoken dialog system for chat-like conversations considering response timing. International Conference on Text, Speech and Dialog. 599-606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ozkan</author>
<author>L-P Morency</author>
</authors>
<title>Concensus of selffeatures for nonverbal behavior analysis.</title>
<date>2010</date>
<booktitle>In Human Behavior Understanding in conjucion with International Conference in Pattern Recognition.</booktitle>
<contexts>
<context position="8772" citStr="Ozkan and Morency, 2010" startWordPosition="1324" endWordPosition="1327">e expect some patterns in these responses. Therefore, we first analyze the most predictive features for each listener and search for prototypical patterns (in Section 3.1). Then, we present our Wisdom-LMDE that allows to automatically learn the hidden structure within listener responses. 3.1 Wisdom Analysis We analyzed our wisdom data to see the most relevant speaker features when predicting responses from each individual listener. (The complete list of speaker features are described in Section 4.1.) We used a feature ranking scheme based on a sparse regularization technique, as described in (Ozkan and Morency, 2010). It allows us to identify the speaker features most predictive of each listener backchannel feedback. The top 3 features for all 9 listeners are listed in Table 1. This analysis suggests three prototypical patterns. For the first 3 listeners, pause in speech and syntac1http://rapport.ict.usc.edu/ tic information (POS:NN) are more important. The next 3 experts include a prosodic feature, low pitch, which is coherent with earlier findings (Nishimura et al., 2007; Ward and Tsukahara, 2000). It is interesting to see that the last 3 experts incorporate visual information when predicting backchanne</context>
</contexts>
<marker>Ozkan, Morency, 2010</marker>
<rawString>D. Ozkan and L.-P. Morency. 2010. Concensus of selffeatures for nonverbal behavior analysis. In Human Behavior Understanding in conjucion with International Conference in Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ozkan</author>
<author>K Sagae</author>
<author>L-P Morency</author>
</authors>
<title>Latent mixture of discriminative experts for multimodal prediction modeling.</title>
<date>2010</date>
<booktitle>In International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="1408" citStr="Ozkan et al., 2010" startWordPosition="210" endWordPosition="213">ntroduction In many real life scenarios, it is hard to collect the actual labels for training, because it is expensive or the labeling is subjective. To address this issue, a new direction of research appeared in the last decade, taking full advantage of the ”wisdom of crowds” (Surowiecki, 2004). In simple words, wisdom of crowds enables parallel acquisition of opinions from multiple annotators/experts. In this paper, we propose a new method to fuse wisdom of crowds. Our approach is based on the Latent Mixture of Discriminative Experts (LMDE) model originally introduced for multimodal fusion (Ozkan et al., 2010). In our Wisdom-LMDE model, a discriminative expert is trained for each crowd member. The key advantage of our computational model is that it can automatically discover the prototypical patterns of experts and learn the dynamic between these patterns. An overview of our approach is depicted in Figure 1. We validate our model on the challenging task of listener backchannel feedback prediction in dyadic conversations. Backchannel feedback includes the nods and paraverbals such as ”uh-huh” and ”mmhmm” that listeners produce as they are speaking. Backchannels play a significant role in determining</context>
<context position="10306" citStr="Ozkan et al., 2010" startWordPosition="1562" endWordPosition="1565">tational model for listener backchannel. 3.2 Computational Model: Wisdom-LMDE The goals of our computational model are to automatically discover the prototypical patterns of backchannel feedback and learn the dynamic between these patterns. This will allow the computational model to accurately predict the responses of a new listener even if he/she changes her backchannel patterns in the middle of the interaction. It will also improve generalization by allowing mixtures of these prototypical patterns. To achieve these goals, we propose a variant of the Latent Mixture of Discriminative Experts (Ozkan et al., 2010) which takes full advantage of the wisdom of crowds. Our Wisdom-LMDE model is based on a two step process: a Conditional Random Field (CRF, see Figure 1a) is learned for each wisdom listener, and the outputs of these expert models are used as input to a Latent Dynamic Conditional Random Field (LDCRF, see Figure 1b) model, which is capable of learning the hidden structure within the experts. In our Wisdom-LMDE, each expert corresponds to a different listener from the wisdom of crowds. More details about training and inference of LMDE can be found in Ozkan et al. (2010). 337 4 Experiments To con</context>
<context position="14448" citStr="Ozkan et al., 2010" startWordPosition="2232" endWordPosition="2235">ion Pool (LOP) CRF suggested by Smith et al. (2005). Similar to our Wisdom-LMDE model, the training is performed in two steps. A graphical representation of a CRF Mixture of experts is given in the Figure 1. Actual Listener (AL) Classifiers This baseline model consists of two models: CRF and LDCRF chains (See Figure 1). To train these models, we use the labels of the ”Actual Listeners” (AL) from the RAPPORT dataset. Multimodal LMDE In this baseline model, we compare our Wisdom LMDE to a multimodal LMDE, where each expert refers to one of 5 different set of multimodal features as presented in (Ozkan et al., 2010): lexical, prosodic, part-of-speech, syntactic, and visual. Random Classifier Our last baseline model is a random backchannel generator as desribed by Ward and Tsukahara (2000). This model randomly generates backchannels whenever some pre-defined conditions in the prosody of the speech is purveyed. 4.3 Methodolgy We performed hold-out testing on a randomly selected subset of 10 interactions. The training set contains the remaining 33 interactions. Model parameters were validated by using a 3-fold crossvalidation strategy on the training set. Regulariza338 Table 2: Comparison of our Wisdom-LMDE</context>
</contexts>
<marker>Ozkan, Sagae, Morency, 2010</marker>
<rawString>D. Ozkan, K. Sagae, and L.-P. Morency. 2010. Latent mixture of discriminative experts for multimodal prediction modeling. In International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas C Raykar</author>
<author>Shipeng Yu</author>
<author>Linda H Zhao</author>
</authors>
<title>Learning from crowds.</title>
<date>2010</date>
<location>Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, Linda</location>
<marker>Raykar, Yu, Zhao, 2010</marker>
<rawString>Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, Linda Moy, and David Blei. 2010. Learning from crowds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>1044--1050</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="11639" citStr="Sagae and Tsujii, 2007" startWordPosition="1773" endWordPosition="1776">oposed. As motivated earlier, we focus our experiments on predicting listener backchannel since it is a well-suited task where variability exists among listeners. 4.1 Multimodal Speaker Features The speaker videos were transcribed and annotated to extract the following features: Lexical: Some studies have suggested an association between lexical features and listener feedback (Cathcart et al., 2003). Therefore, we use all the words (i.e., unigrams) spoken by the speaker. Syntactic structure: Using a CRF part-of-speech (POS) tagger and a data-driven left-to-right shiftreduce dependency parser (Sagae and Tsujii, 2007) we extract four types of features from a syntactic dependency structure corresponding to the utterance: POS tags and grammatical function for each word, POS tag of the syntactic head, distance and direction from each word to its syntactic head. Prosody: Prosody refers to the rhythm, pitch and intonation of speech. Several studies have demonstrated that listener feedback is correlated with a speaker’s prosody (Ward and Tsukahara, 2000; Nishimura et al., 2007). Following this, we use downslope in pitch, pitch regions lower than 26th percentile, drop/rise and fast drop/rise in energy of speech, </context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044–1050, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Smith</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>Logarithmic opinion pools for conditional random fields.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="13880" citStr="Smith et al. (2005)" startWordPosition="2133" endWordPosition="2136">ine model, we use consensus labels to train a CRF model, which are constructed by a similar approach presented in (Huang et al., 2010). The consensus threshold is set to 3 (at least 3 listeners agree to give feedback at a point) so that it contains approximately the same number of head nods as the actual listener. See Figure 1 for a graphical representation of CRF model. CRF Mixture of Experts To show the importance of latent variable in our Wisdom-LMDE model, we trained a CRF-based mixture of discriminative experts. This model is similar to the Logarithmic Opinion Pool (LOP) CRF suggested by Smith et al. (2005). Similar to our Wisdom-LMDE model, the training is performed in two steps. A graphical representation of a CRF Mixture of experts is given in the Figure 1. Actual Listener (AL) Classifiers This baseline model consists of two models: CRF and LDCRF chains (See Figure 1). To train these models, we use the labels of the ”Actual Listeners” (AL) from the RAPPORT dataset. Multimodal LMDE In this baseline model, we compare our Wisdom LMDE to a multimodal LMDE, where each expert refers to one of 5 different set of multimodal features as presented in (Ozkan et al., 2010): lexical, prosodic, part-of-spe</context>
</contexts>
<marker>Smith, Cohn, Osborne, 2005</marker>
<rawString>A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic opinion pools for conditional random fields. In ACL, pages 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<marker>Snow, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Surowiecki</author>
</authors>
<title>The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations.</title>
<date>2004</date>
<publisher>Doubleday.</publisher>
<contexts>
<context position="1085" citStr="Surowiecki, 2004" startWordPosition="160" endWordPosition="161"> wisdom of crowds based on the Latent Mixture of Discriminative Experts (LMDE) model that can automatically learn the prototypical patterns and hidden dynamic among different experts. Experiments show improvement over state-of-the-art approaches on the task of listener backchannel prediction in dyadic conversations. 1 Introduction In many real life scenarios, it is hard to collect the actual labels for training, because it is expensive or the labeling is subjective. To address this issue, a new direction of research appeared in the last decade, taking full advantage of the ”wisdom of crowds” (Surowiecki, 2004). In simple words, wisdom of crowds enables parallel acquisition of opinions from multiple annotators/experts. In this paper, we propose a new method to fuse wisdom of crowds. Our approach is based on the Latent Mixture of Discriminative Experts (LMDE) model originally introduced for multimodal fusion (Ozkan et al., 2010). In our Wisdom-LMDE model, a discriminative expert is trained for each crowd member. The key advantage of our computational model is that it can automatically discover the prototypical patterns of experts and learn the dynamic between these patterns. An overview of our approa</context>
<context position="4957" citStr="Surowiecki (2004)" startWordPosition="741" endWordPosition="742"> previous approaches for listener backchannel prediction. 1.1 Previous Work Several researchers have developed models to predict when backchannel should happen. Ward and Tsukahara (2000) propose a unimodal approach where backchannels are associated with a region of low pitch lasting 110ms during speech. Nishimura et al. (2007) present a unimodal decision-tree approach for producing backchannels based on prosodic features. Cathcart et al. (2003) propose a unimodal model based on pause duration and trigram part-ofspeech frequency. Wisdom of crowds was first defined and used in business world by Surowiecki (2004). Later, it has been applied to other research areas as well. Raykar et. al. (2010) proposed a probabilistic approach for supervised learning tasks for which multiple annotators provide labels but not an absolute gold standard. Snow et. al. (2008) show that using non-expert labels for training machine learning algorithms can be as effective as using a gold standard annotation. In this paper, we present a computational approach for listener backchannel prediction that exploits multiple listeners. Our model takes into account the differences in people’s reactions, and automatically learns the hi</context>
</contexts>
<marker>Surowiecki, 2004</marker>
<rawString>James Surowiecki. 2004. The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations. Doubleday.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tsui</author>
<author>G L Schultz</author>
</authors>
<title>Failure of rapport: Why psychotheraputic engagement fails in the treatment of asian clients.</title>
<date>1985</date>
<journal>American Journal of Orthopsychiatry,</journal>
<pages>55--561</pages>
<contexts>
<context position="2375" citStr="Tsui and Schultz, 1985" startWordPosition="357" endWordPosition="360">g task of listener backchannel feedback prediction in dyadic conversations. Backchannel feedback includes the nods and paraverbals such as ”uh-huh” and ”mmhmm” that listeners produce as they are speaking. Backchannels play a significant role in determining the nature of a social exchange by showing rapport and engagement (Gratch et al., 2007). When these signals are positive, coordinated and reciprocated, they can lead to feelings of rapport and promote beneficial outcomes in diverse areas such as negotiations and conflict resolution (Drolet and Morris, 2000), psychotherapeutic effectiveness (Tsui and Schultz, 1985), improved test performance in classrooms (Fuchs, 1987) and improved quality of child care (Burns, 1984). Supporting such fluid interactions has become an important topic of virtual human research. In particular, backchannel feedback has received considerable interest due to its pervasiveness across languages and conversational contexts. By correctly predicting backchannel feedback, virtual agent and robots can have stronger sense of rapport. What makes backchannel prediction task wellsuited for our model is that listener feedback varies between people and is often optional (listeners can alwa</context>
</contexts>
<marker>Tsui, Schultz, 1985</marker>
<rawString>P. Tsui and G.L. Schultz. 1985. Failure of rapport: Why psychotheraputic engagement fails in the treatment of asian clients. American Journal of Orthopsychiatry, 55:561–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
<author>W Tsukahara</author>
</authors>
<title>Prosodic features which cue back-channel responses in english and japanese.</title>
<date>2000</date>
<journal>Journal of Pragmatics.</journal>
<volume>23</volume>
<pages>1177--1207</pages>
<contexts>
<context position="4526" citStr="Ward and Tsukahara (2000)" startWordPosition="673" endWordPosition="676">E model is learned using this wisdom of crowds, associating one expert for each listener. Right: Baseline models used in our experiments: a) Conditional Random Fields (CRF), b) Latent Dynamic Conditional Random Fields (LDCRF), c) CRF Mixture of Experts (no latent variable) totypical patterns for backchannel feedback. By automatically identifying these prototypical patterns and learning the dynamic, our Wisdom-LMDE model outperforms the previous approaches for listener backchannel prediction. 1.1 Previous Work Several researchers have developed models to predict when backchannel should happen. Ward and Tsukahara (2000) propose a unimodal approach where backchannels are associated with a region of low pitch lasting 110ms during speech. Nishimura et al. (2007) present a unimodal decision-tree approach for producing backchannels based on prosodic features. Cathcart et al. (2003) propose a unimodal model based on pause duration and trigram part-ofspeech frequency. Wisdom of crowds was first defined and used in business world by Surowiecki (2004). Later, it has been applied to other research areas as well. Raykar et. al. (2010) proposed a probabilistic approach for supervised learning tasks for which multiple an</context>
<context position="9264" citStr="Ward and Tsukahara, 2000" startWordPosition="1400" endWordPosition="1403"> Section 4.1.) We used a feature ranking scheme based on a sparse regularization technique, as described in (Ozkan and Morency, 2010). It allows us to identify the speaker features most predictive of each listener backchannel feedback. The top 3 features for all 9 listeners are listed in Table 1. This analysis suggests three prototypical patterns. For the first 3 listeners, pause in speech and syntac1http://rapport.ict.usc.edu/ tic information (POS:NN) are more important. The next 3 experts include a prosodic feature, low pitch, which is coherent with earlier findings (Nishimura et al., 2007; Ward and Tsukahara, 2000). It is interesting to see that the last 3 experts incorporate visual information when predicting backchannel feedback. This is in line with Burgoon et al. (Burgoon et al., 1995) work showing that speaker gestures are often correlated with listener feedback. These results clearly suggest that variations be present among listeners and some prototypical patterns may exist. Based on these observations, we propose new computational model for listener backchannel. 3.2 Computational Model: Wisdom-LMDE The goals of our computational model are to automatically discover the prototypical patterns of bac</context>
<context position="12077" citStr="Ward and Tsukahara, 2000" startWordPosition="1842" endWordPosition="1845">i.e., unigrams) spoken by the speaker. Syntactic structure: Using a CRF part-of-speech (POS) tagger and a data-driven left-to-right shiftreduce dependency parser (Sagae and Tsujii, 2007) we extract four types of features from a syntactic dependency structure corresponding to the utterance: POS tags and grammatical function for each word, POS tag of the syntactic head, distance and direction from each word to its syntactic head. Prosody: Prosody refers to the rhythm, pitch and intonation of speech. Several studies have demonstrated that listener feedback is correlated with a speaker’s prosody (Ward and Tsukahara, 2000; Nishimura et al., 2007). Following this, we use downslope in pitch, pitch regions lower than 26th percentile, drop/rise and fast drop/rise in energy of speech, vowel volume, pause. Visual gestures: Gestures performed by the speaker are often correlated with listener feedback (Burgoon et al., 1995). Eye gaze, in particular, has often been implicated as eliciting listener feedback. Thus, we encode the following contextual features: speaker looking at listener, smiling, moving eyebrows up and frowning. Although our current method for extracting these features requires that the entire utterance </context>
<context position="14624" citStr="Ward and Tsukahara (2000)" startWordPosition="2257" endWordPosition="2260"> of experts is given in the Figure 1. Actual Listener (AL) Classifiers This baseline model consists of two models: CRF and LDCRF chains (See Figure 1). To train these models, we use the labels of the ”Actual Listeners” (AL) from the RAPPORT dataset. Multimodal LMDE In this baseline model, we compare our Wisdom LMDE to a multimodal LMDE, where each expert refers to one of 5 different set of multimodal features as presented in (Ozkan et al., 2010): lexical, prosodic, part-of-speech, syntactic, and visual. Random Classifier Our last baseline model is a random backchannel generator as desribed by Ward and Tsukahara (2000). This model randomly generates backchannels whenever some pre-defined conditions in the prosody of the speech is purveyed. 4.3 Methodolgy We performed hold-out testing on a randomly selected subset of 10 interactions. The training set contains the remaining 33 interactions. Model parameters were validated by using a 3-fold crossvalidation strategy on the training set. Regulariza338 Table 2: Comparison of our Wisdom-LMDE model with previously proposed models. The last column shows the paired one tailed t-test results comparing Wisdom LMDE to each model. tion values used are 10k for k = -1,0,..</context>
<context position="16054" citStr="Ward and Tsukahara, 2000" startWordPosition="2487" endWordPosition="2490">ollowing (Morency et al., 2008), we use an encoding dictionary to represent our features. The performance is measured by using the F-score, which is the weighted harmonic mean of precision and recall. A backchannel is predicted correctly if a peak happens during an actual listener backchannel with high enough probability. The threshold was selected automatically during validation. 4.4 Results and Discussion Before reviewing the prediction results, is it important to remember that backchannel feedback is an optional phenomena, where the actual listener may or may not decide on giving feedback (Ward and Tsukahara, 2000). Therefore, results from prediction tasks are expected to have lower accuracies as opposed to recognition tasks where labels are directly observed (e.g., part-of-speech tagging). Table 2 summarizes our experiments comparing our Wisdom-LMDE model with state-of-the-art approaches for behavior prediction (see Section 4.2). Our Wisdom-LMDE model achieves the best F1 score. Statistical t-test analysis show that WisdomLMDE is significantly better than Consensus Classifier, AL Classifier (LDCRF), Multimodel LMDE and Random Classifier. The second best F1 score is achieved by CRF Mixture of experts, w</context>
</contexts>
<marker>Ward, Tsukahara, 2000</marker>
<rawString>N. Ward and W. Tsukahara. 2000. Prosodic features which cue back-channel responses in english and japanese. Journal of Pragmatics. 23, 1177–1207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>