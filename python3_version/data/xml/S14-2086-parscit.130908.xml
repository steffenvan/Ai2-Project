<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001175">
<title confidence="0.99465">
RTRGO: Enhancing the GU-MLT-LT System
for Sentiment Analysis of Short Messages
</title>
<note confidence="0.592792666666667">
Tobias G¨unther Jean Vancoppenolle Richard Johansson
Retresco GmbH ferret go GmbH University of Gothenburg
retresco.de ferret-go.com www.svenska.gu.se
</note>
<email confidence="0.981933">
email@tobias.io jean.vcop@gmail.com richard.johansson@gu.se
</email>
<sectionHeader confidence="0.993462" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801636363636">
This paper describes the enhancements
made to our GU-MLT-LT system (G¨unther
and Furrer, 2013) for the SemEval-2014
re-run of the SemEval-2013 shared task on
sentiment analysis in Twitter. The changes
include the usage of a Twitter-specific to-
kenizer, additional features and sentiment
lexica, feature weighting and random sub-
space learning. The improvements result
in an increase of 4.18 F-measure points on
this year’s Twitter test set, ranking 3rd.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999782523809524">
Automatic analysis of sentiment expressed in text
is an active research area in natural language pro-
cessing with obvious commercial interest. In the
simplest formulation of the problem, sentiment
analysis is framed as a categorization problem
over documents, where the set of categories is
typically a set of polarity values, such as posi-
tive, neutral, and negative. Many approaches to
document-level sentiment classification have been
proposed. For an overview see e.g. Liu (2012).
Text in social media and in particular microblog
messages are a challenging text genre for senti-
ment classification, as they introduce additional
problems such as short text length, spelling vari-
ation, special tokens, topic variation, language
style and multilingual content. Following Pang et
al. (2002), most sentiment analysis systems have
been based on standard text categorization tech-
niques, e.g. training a classifier using some sort of
bag-of-words feature representation. This is also
true for sentiment analysis of microblogs. Among
</bodyText>
<footnote confidence="0.90831375">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999477538461538">
the first to work specifically with Twitter1 data
were Go et al. (2009), who use emoticons as labels
for the messages. Similarly, Davidov et al. (2010),
Pak and Paroubek (2010), and Kouloumpis et al.
(2011) use this method of distant supervision to
overcome the data acquisition barrier. Barbosa
and Feng (2010) make use of three different senti-
ment detection websites to label messages and use
mostly non-lexical features to improve the robust-
ness of their classifier. Bermingham and Smeaton
(2010) investigate the impact of the shortness of
Tweets on sentiment analysis and Speriosu et al.
(2011) propagate information from seed labels
along a linked structure that includes Twitter’s
follower graph. There has also been work on
lexicon-based approaches to sentiment analysis of
microblogs, such as O’Connor et al. (2010), Thel-
wall et al. (2010) and Zhang et al. (2011). For a
detailed discussion see G¨unther (2013).
In 2013, the International Workshop on Se-
mantic Evaluation (SemEval) organized a shared
task on sentiment analysis in Twitter (Nakov et
al., 2013) to enable a better comparison of dif-
ferent approaches for sentiment analysis of mi-
croblogs. The shared task consisted of two sub-
tasks: one on recognizing contextual polarity of
a given subjective expression (Task A), and one
on document-level sentiment classification (Task
B). For both tasks, the training sets consisted of
manually labeled Twitter messages, while the test
sets consisted of a Twitter part and an SMS part
in order to test domain sensitivity. Among the
best performing systems were Mohammad et al.
(2013), G¨unther and Furrer (2013) and Becker et
al. (2013), who all train linear models on a vari-
ety of task-specific features. In this year the cor-
pus resources were used for a re-run of the shared
task (Rosenthal et al., 2014), introducing two new
Twitter test sets, as well as LiveJournal data.
</bodyText>
<footnote confidence="0.991839">
1A popular microblogging service on the internet, its mes-
sages are commonly referred to as “Tweets.”
</footnote>
<page confidence="0.910924">
497
</page>
<note confidence="0.8315175">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 497–502,
Dublin, Ireland, August 23-24, 2014.
</note>
<sectionHeader confidence="0.942403" genericHeader="method">
2 System Desciption
</sectionHeader>
<bodyText confidence="0.9998332">
This section describes the details of our sentiment
analysis system, focusing on the differences to our
last year’s implementation. This year we only par-
ticipated in the subtask on whole message polarity
classification (Subtask B).
</bodyText>
<subsectionHeader confidence="0.993033">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999980294117647">
For tokenization of the messages we use the
tokenizer of Owoputi et al. (2013)’s Twitter
NLP Tools2, which include a tokenizer and part-
of-speech tagger optimized for the usage with
Tweets. The tokenizer contains a regular expres-
sion grammar for recognizing emoticons, which is
an especially valuable property in the context of
sentiment analysis due to the high emotional ex-
pressiveness of emoticons.
It is well known that the way word tokens are
represented may have a significant impact on the
performance of a lexical classifier. This is par-
ticularly true in natural language processing of
social media, where we run into the problem of
spelling variation causing extreme lexical sparsity.
To deal with this issue we normalize the tokens
with the following technique: First, all tokens are
converted to lowercase and the hashtag sign (#) is
removed if present. If the token is not present in
an English word list or any of the used sentiment
lexica (see below), we remove all directly repeated
letters after the first repetition (e.g. greeeeaaat →
greeaat). If the resulting token is still not present
in any of the lexical resources, we allow no direct
repetition of letters at all. While this might lead
to lexical collisions in some cases (e.g. goooodd
→ goodd → god), it is an easy and efficient way
to remove some lexical sparsity. While generating
all possible combinations of deletions and check-
ing the resulting tokens against a lexical resource
is another option, a correct disambiguation of the
intended word would require a method making use
of context knowledge (e.g. goooodd → good, vs.
goooodd → god).
</bodyText>
<subsectionHeader confidence="0.959822">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.9760305">
We use the following set of features as input to our
supervised classifier:
</bodyText>
<listItem confidence="0.804280111111111">
• The normalized tokens as unigrams and bi-
grams, where stopword and punctuation to-
kens are excluded from bigrams
2http://www.ark.cs.cmu.edu/TweetNLP
• The word stems of the normalized tokens,
reducing inflected forms of a word to a com-
mon form. The stems were computed using
the Porter stemmer algorithm (Porter, 1980)
• The IDs of the token’s word clusters.
</listItem>
<bodyText confidence="0.5940315">
The clusters were generated by performing
Brown clustering (Brown et al., 1992) on
56,345,753 Tweets by Owoputi et al. (2013)
and are available online.2
</bodyText>
<listItem confidence="0.902892571428571">
• The presence of a hashtag or URL in the mes-
sage (one feature each)
• The presence of a question mark token in the
message
• We use the opinion lexicon by Bing Liu (Hu
and Liu, 2004), the MPQA subjectivity lex-
icon (Wiebe et al., 2005) and the Twitrratr
</listItem>
<bodyText confidence="0.997651222222222">
wordlist, which all provide a list of positive
and negative words, to compute a prior polar-
ity of the message. For each of the three sen-
timent lexica two features capture whether
the majority of the tokens in the message
were in the positive or negative sentiment list.
The same is done for hashtags using the NRC
hashtag sentiment lexicon (Mohammad et al.,
2013).
</bodyText>
<listItem confidence="0.879651217391304">
• We apply special handling to features in a
negation context. A token is considered as
negated if it occurs after a negation word (up
to the next punctuation). All token, stem and
word cluster features are marked with a nega-
tion prefix. Additionally, the polarity for to-
ken in a negation context is inverted when
computing the prior lexicon polarity.
• We use the part-of-speech tags computed by
the part-of-speech tagger of the Twitter NLP
tools by Owoputi et al. (2013) to exclude
certain tokens. Assuming they do not carry
any helpful sentiment information, no fea-
tures are computed for token recognized as
name (tag ˆ) or user mention (tag @).
• We also employ feature weighting to give
more importance to certain features and indi-
cation of emphasis by the author. Normally,
all features described above receive weight 1
if they are present and weight 0 if they are ab-
sent. For each of the following cases we add
+1 to the weight of a token’s unigram, stem
and word cluster features:
</listItem>
<page confidence="0.994933">
498
</page>
<bodyText confidence="0.999298909090909">
– The original (not normalized) token is
all uppercase
– The original token has more than three
adjacent repetitions of one letter
– The token is an adjective or emoticon
(according to its part-of-speech tag)
Furthermore, the score of each token is di-
vided in half, if the token occurs in a ques-
tion context. A token is considered to be in
a question context, if it occurs before a ques-
tion mark (up to the next punctuation).
</bodyText>
<subsectionHeader confidence="0.995584">
2.3 Machine Learning Methods
</subsectionHeader>
<bodyText confidence="0.999735685714286">
All training was done using the open-source ma-
chine learning toolkit scikit-learn3 (Pedregosa et
al., 2011). Just as in our last year’s system
we trained linear one-versus-all classifiers us-
ing stochastic gradient descent optimization with
hinge loss and elastic net regularization.4 For fur-
ther details see G¨unther and Furrer (2013). The
number of iterations was set to 1000 for the final
model and 100 for the experiments.
It is widely observed that training on a lot of
lexical features can lead to brittle NLP systems,
that are easily overfit to particular domains. In so-
cial media messages the brittleness is particularly
acute due to the wide variation in vocabulary and
style. While this problem can be eased by using
corpus-induced word representations such as the
previously introduced word cluster features, it can
also be addressed from a learning point of view.
Brittleness can be caused by the problem that very
strong features (e.g. emoticons) drown out the ef-
fect of other useful features.
The method of random subspace learning
(Søgaard and Johannsen, 2012) seeks to handle
this problem by forcing learning algorithms to pro-
duce models with more redundancy. It does this
by randomly corrupting training instances during
learning, so if some useful feature is correlated
with a strong feature, the learning algorithm has
a better chance to assign it a nonzero weight. We
implemented random subspace learning by train-
ing the classifier on a concatenation of 25 cor-
rupted copies of the training set. In a corrupted
copy, each feature was randomly disabled with a
probability of 0.2. Just as for the classifier, the hy-
perparameters were optimized empirically.
</bodyText>
<footnote confidence="0.99750775">
3Version 0.13.1, http://scikit-learn.org.
4SGDClassifier(penalty=’elasticnet’,
alpha=0.001, l1 ratio=0.85, n iter=1000,
class weight=’auto’)
</footnote>
<sectionHeader confidence="0.996493" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999819565217391">
For the experiments and the training of the final
model we used the joined training and develop-
ment sets of subtask B. We were able to retrieve
10368 Tweets, of which we merged all samples
labeled as objective into the neutral class. This re-
sulted in a training set of 3855 positive, 4889 neu-
tral and 1624 negative tweets. The results of the
experiments were obtained by performing 10-fold
cross-validation, predicting positive, negative and
neutral class. Just as in the evaluation of the shared
task the results are reported as average F-measure
(F1) between positive and negative class.
To be able to evaluate the contribution of the
different features groups to the final model we per-
form an ablation study. By disabling one feature
group at the time one can easily compare the per-
formance of the model without a certain feature to
the model using the complete feature set. In Ta-
ble 1 we present the results for the feature groups
bigrams (2gr), stems (stem), word clusters (wc),
sentiment lexica (lex), negation (neg), excluding
names and user mentions (excl), feature weighting
(wei) and random subspace learning (rssl).
</bodyText>
<table confidence="0.997850363636363">
Negative Rec Positive Rec Avg.
Prec Prec F1
ALL 54.80 71.67 76.70 75.41 69.08
-2gr -0.55 -0.49 -0.35 +0.20 -0.31
-stem -1.47 -1.72 -0.49 -0.03 -0.92
-wc -1.45 -1.60 -0.40 -1.66 -1.29
-lex -1.73 -5.11 +1.06 -2.75 -1.99
-neg -1.90 -3.14 -1.30 +0.36 -1.43
-excl +0.31 -0.99 +0.59 +0.08 +0.08
-wei -1.57 +0.43 -0.84 -0.34 -0.73
-rssl +2.04 -4.37 +1.38 -2.88 -0.67
</table>
<tableCaption confidence="0.999229">
Table 1: Feature ablation study
</tableCaption>
<bodyText confidence="0.999304166666667">
Looking at Table 1, we can see that removing
the sentiment lexica features causes the biggest
drop in performance. This is especially true for
the recall of the negative class, which is underrep-
resented in the training data and can thus profit the
most from prior domain knowledge. When com-
paring to the features of our last year’s system, it
becomes clear that the used sentiment lexica can
provide a much bigger gain in performance than
the previously used SentiWordNet. Even though
they are outperformed by the sentiment lexica, the
word cluster features still provide an additional in-
</bodyText>
<page confidence="0.998151">
499
</page>
<table confidence="0.9975425">
GU- MLT-LT (2013) Accuracy Fi pos/neg RTRGO Accuracy
Fi pos/neg Fi 3-class (2014)
Fi 3-class
Twitter2013 65.42 68.13 70.42 69.10 70.92 72.54
Twitter2014 65.77 66.59 69.40 69.95 69.99 72.53
SMS2013 62.65 66.93 69.09 67.51 72.15 75.54
LiveJournal2014 68.97 68.42 68.39 72.20 72.29 72.33
Twitter2014Sarcasm 54.11 56.91 58.14 47.09 49.34 51.16
</table>
<tableCaption confidence="0.997594">
Table 2: Final results of our submissions on the different test sets (Subtask B)
</tableCaption>
<bodyText confidence="0.99997025">
crease in performance and can, in contrast to sen-
timent lexica, be learned in a completely unsu-
pervised manner. Negation handling is an impor-
tant feature to boost the precision of the classifier,
while using random subspace learning increases
the recall of the classes, which indicates that the
technique indeed leads to more redundant models.
Another interesting question in sentiment anal-
ysis is, how machine learning methods com-
pare to simple methods only relying on sentiment
wordlists and how much training data is needed
to outperform them. Figure 1 shows the results
of a training size experiment, in which we tested
classifiers, trained on different portions of a train-
ing set, on the same test set (10-fold cross val-
idated). The two horizontal lines indicate the
performance of two simple classifiers, using the
Twitrratr wordlist (359 entries, labeled TRR) or
Bing Liu opinion lexicon (6789 entries, labeled
LIU) with a simple majority-vote strategy (choos-
ing the neutral class in case of no hits or no ma-
jority and including a polarity switch for token in
a negation context). The baseline of the machine
learning classifiers is a logistic regression
</bodyText>
<subsectionHeader confidence="0.575587">
Training samples per class
</subsectionHeader>
<figureCaption confidence="0.998757">
Figure 1: Training size experiment
</figureCaption>
<bodyText confidence="0.999860111111111">
classifier using only uni- and bigram features and
negation handling (labeled BOW). To this baseline
we add either the lexicon features for the Bing Liu
opinion lexicon and the Twitrratr wordlist (labeled
+LEX) or all other features described in section
2.2 excluding lexicon features (labeled +REST).
Looking at the results, we can see that a simple
bag of words classifier needs about 250 samples
of each class to outperform the TRR list and about
700 samples of each class to outperform the LIU
lexicon on the common test set. Adding the fea-
tures that can be obtained without having senti-
ment lexica available (+REST) reduces the needed
training samples about half. It is worth noting that
from a training set size of 1250 samples per class
the +REST-classifier is able to match the results of
the classifier combining bag of words and lexicon
features (+LEX).
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="evaluation">
4 Results and Conclusion
</sectionHeader>
<bodyText confidence="0.9999648">
The results of our system are presented in Table 2,
where the bold column marks the results relevant
to our submission to this year’s shared task. We
also give results for our last year’s system. Be-
side the average F-measure between positive and
negative class, on which the shared task is evalu-
ated, we also provide the results of both systems as
average F-measure over all three classes and accu-
racy to create possibilities for better comparison
to other research. In this paper we showed sev-
eral ways to improve a machine learning classifier
for the use of sentiment analysis in Twitter. Com-
pared to our last year’s system we were able to
increase the performance about several F-measure
points on all non-sarcastic datasets.
</bodyText>
<sectionHeader confidence="0.980159" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99811875">
We would like to thank the organizers of the
shared task for their effort, as well as the anony-
mous reviewers for their helpful comments on the
paper.
</bodyText>
<figure confidence="0.549168428571429">
50 250 500 750 1000 1250
Average 3−class F1
45 50 55 60 65
+LEX
+REST
BOW
TRR LIU
</figure>
<page confidence="0.976275">
500
</page>
<sectionHeader confidence="0.953756" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998972376146789">
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 36–44. Association for Computational Lin-
guistics.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. 2013. Avaya: Sentiment analysis on
twitter with self-training and polarity lexicon expan-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 333–
340, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Adam Bermingham and Alan F Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, pages 1833–1836. ACM.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 241–249. Association for Computa-
tional Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford.
Tobias G¨unther and Lenz Furrer. 2013. GU-MLT-
LT: Sentiment analysis of short messages using lin-
guistic features and stochastic gradient descent. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 328–332, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Tobias G¨unther. 2013. Sentiment analysis of mi-
croblogs. Master’s thesis, University of Gothenburg,
June.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg. In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media, pages 538–541.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Seman-
tic Evaluation Exercises (SemEval-2013), Atlanta,
Georgia, USA, June.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312–
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R Routledge, and Noah A Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media, pages 122–129.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL 2013.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of LREC, volume 2010.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Martin F Porter. 1980. An algorithm for suffix strip-
ping. Program: electronic library and information
systems, 14(3):130–137.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanova. 2014. Semeval-2014 task 9:
Sentiment analysis in twitter. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
</reference>
<page confidence="0.971158">
501
</page>
<reference confidence="0.999573916666667">
Anders Søgaard and Anders Johannsen. 2012. Robust
learning in random subspaces: Equipping NLP for
OOV effects. In COLING (Posters), pages 1171–
1180.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and
Jason Baldridge. 2011. Twitter polarity classifica-
tion with label propagation over lexical links and the
follower graph. In Proceedings of the First Work-
shop on Unsupervised Learning in NLP, pages 53–
63. Association for Computational Linguistics.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment
strength detection in short informal text. Journal of
the American Society for Information Science and
Technology, 61(12):2544–2558.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165–210.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-
ichun Hsu, and Bing Liu. 2011. Combining lex-
iconbased and learning-based methods for twitter
sentiment analysis. HP Laboratories, Technical Re-
port HPL-2011-89.
</reference>
<page confidence="0.997611">
502
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.536741">
<title confidence="0.9993765">RTRGO: Enhancing the GU-MLT-LT for Sentiment Analysis of Short Messages</title>
<author confidence="0.99987">Tobias G¨unther Jean Vancoppenolle Richard Johansson</author>
<affiliation confidence="0.619002">Retresco GmbH ferret go GmbH University of Gothenburg</affiliation>
<abstract confidence="0.988570785714286">retresco.de ferret-go.com www.svenska.gu.se email@tobias.io jean.vcop@gmail.com richard.johansson@gu.se Abstract This paper describes the enhancements made to our GU-MLT-LT system (G¨unther and Furrer, 2013) for the SemEval-2014 re-run of the SemEval-2013 shared task on sentiment analysis in Twitter. The changes include the usage of a Twitter-specific tokenizer, additional features and sentiment lexica, feature weighting and random subspace learning. The improvements result in an increase of 4.18 F-measure points on this year’s Twitter test set, ranking 3rd.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>36--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2316" citStr="Barbosa and Feng (2010)" startWordPosition="327" endWordPosition="330">bag-of-words feature representation. This is also true for sentiment analysis of microblogs. Among This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ the first to work specifically with Twitter1 data were Go et al. (2009), who use emoticons as labels for the messages. Similarly, Davidov et al. (2010), Pak and Paroubek (2010), and Kouloumpis et al. (2011) use this method of distant supervision to overcome the data acquisition barrier. Barbosa and Feng (2010) make use of three different sentiment detection websites to label messages and use mostly non-lexical features to improve the robustness of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approaches to sentiment analysis of microblogs, such as O’Connor et al. (2010), Thelwall et al. (2010) and Zhang et al. (2011). For a detailed discussion see G¨unther</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 36–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lee Becker</author>
<author>George Erhart</author>
<author>David Skiba</author>
<author>Valentine Matula</author>
</authors>
<title>Avaya: Sentiment analysis on twitter with self-training and polarity lexicon expansion.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>333--340</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3647" citStr="Becker et al. (2013)" startWordPosition="538" endWordPosition="541">ent analysis in Twitter (Nakov et al., 2013) to enable a better comparison of different approaches for sentiment analysis of microblogs. The shared task consisted of two subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B). For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter part and an SMS part in order to test domain sensitivity. Among the best performing systems were Mohammad et al. (2013), G¨unther and Furrer (2013) and Becker et al. (2013), who all train linear models on a variety of task-specific features. In this year the corpus resources were used for a re-run of the shared task (Rosenthal et al., 2014), introducing two new Twitter test sets, as well as LiveJournal data. 1A popular microblogging service on the internet, its messages are commonly referred to as “Tweets.” 497 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 497–502, Dublin, Ireland, August 23-24, 2014. 2 System Desciption This section describes the details of our sentiment analysis system, focusing on the differences t</context>
</contexts>
<marker>Becker, Erhart, Skiba, Matula, 2013</marker>
<rawString>Lee Becker, George Erhart, David Skiba, and Valentine Matula. 2013. Avaya: Sentiment analysis on twitter with self-training and polarity lexicon expansion. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 333– 340, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Bermingham</author>
<author>Alan F Smeaton</author>
</authors>
<title>Classifying sentiment in microblogs: is brevity an advantage?</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management,</booktitle>
<pages>1833--1836</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2504" citStr="Bermingham and Smeaton (2010)" startWordPosition="356" endWordPosition="359">e. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ the first to work specifically with Twitter1 data were Go et al. (2009), who use emoticons as labels for the messages. Similarly, Davidov et al. (2010), Pak and Paroubek (2010), and Kouloumpis et al. (2011) use this method of distant supervision to overcome the data acquisition barrier. Barbosa and Feng (2010) make use of three different sentiment detection websites to label messages and use mostly non-lexical features to improve the robustness of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approaches to sentiment analysis of microblogs, such as O’Connor et al. (2010), Thelwall et al. (2010) and Zhang et al. (2011). For a detailed discussion see G¨unther (2013). In 2013, the International Workshop on Semantic Evaluation (SemEval) organized a shared task on sentiment analysis in Twitter (Nakov et al., 2013) to enable a better comparison of</context>
</contexts>
<marker>Bermingham, Smeaton, 2010</marker>
<rawString>Adam Bermingham and Alan F Smeaton. 2010. Classifying sentiment in microblogs: is brevity an advantage? In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 1833–1836. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="6545" citStr="Brown et al., 1992" startWordPosition="1010" endWordPosition="1013"> require a method making use of context knowledge (e.g. goooodd → good, vs. goooodd → god). 2.2 Features We use the following set of features as input to our supervised classifier: • The normalized tokens as unigrams and bigrams, where stopword and punctuation tokens are excluded from bigrams 2http://www.ark.cs.cmu.edu/TweetNLP • The word stems of the normalized tokens, reducing inflected forms of a word to a common form. The stems were computed using the Porter stemmer algorithm (Porter, 1980) • The IDs of the token’s word clusters. The clusters were generated by performing Brown clustering (Brown et al., 1992) on 56,345,753 Tweets by Owoputi et al. (2013) and are available online.2 • The presence of a hashtag or URL in the message (one feature each) • The presence of a question mark token in the message • We use the opinion lexicon by Bing Liu (Hu and Liu, 2004), the MPQA subjectivity lexicon (Wiebe et al., 2005) and the Twitrratr wordlist, which all provide a list of positive and negative words, to compute a prior polarity of the message. For each of the three sentiment lexica two features capture whether the majority of the tokens in the message were in the positive or negative sentiment list. Th</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2156" citStr="Davidov et al. (2010)" startWordPosition="302" endWordPosition="305">Pang et al. (2002), most sentiment analysis systems have been based on standard text categorization techniques, e.g. training a classifier using some sort of bag-of-words feature representation. This is also true for sentiment analysis of microblogs. Among This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ the first to work specifically with Twitter1 data were Go et al. (2009), who use emoticons as labels for the messages. Similarly, Davidov et al. (2010), Pak and Paroubek (2010), and Kouloumpis et al. (2011) use this method of distant supervision to overcome the data acquisition barrier. Barbosa and Feng (2010) make use of three different sentiment detection websites to label messages and use mostly non-lexical features to improve the robustness of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approa</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 241–249. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<location>Stanford.</location>
<contexts>
<context position="2076" citStr="Go et al. (2009)" startWordPosition="289" endWordPosition="292">okens, topic variation, language style and multilingual content. Following Pang et al. (2002), most sentiment analysis systems have been based on standard text categorization techniques, e.g. training a classifier using some sort of bag-of-words feature representation. This is also true for sentiment analysis of microblogs. Among This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ the first to work specifically with Twitter1 data were Go et al. (2009), who use emoticons as labels for the messages. Similarly, Davidov et al. (2010), Pak and Paroubek (2010), and Kouloumpis et al. (2011) use this method of distant supervision to overcome the data acquisition barrier. Barbosa and Feng (2010) make use of three different sentiment detection websites to label messages and use mostly non-lexical features to improve the robustness of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that inc</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tobias G¨unther</author>
<author>Lenz Furrer</author>
</authors>
<title>GU-MLTLT: Sentiment analysis of short messages using linguistic features and stochastic gradient descent.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>328--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<marker>G¨unther, Furrer, 2013</marker>
<rawString>Tobias G¨unther and Lenz Furrer. 2013. GU-MLTLT: Sentiment analysis of short messages using linguistic features and stochastic gradient descent. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 328–332, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias G¨unther</author>
</authors>
<title>Sentiment analysis of microblogs. Master’s thesis,</title>
<date>2013</date>
<institution>University of Gothenburg,</institution>
<marker>G¨unther, 2013</marker>
<rawString>Tobias G¨unther. 2013. Sentiment analysis of microblogs. Master’s thesis, University of Gothenburg, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6802" citStr="Hu and Liu, 2004" startWordPosition="1061" endWordPosition="1064">n tokens are excluded from bigrams 2http://www.ark.cs.cmu.edu/TweetNLP • The word stems of the normalized tokens, reducing inflected forms of a word to a common form. The stems were computed using the Porter stemmer algorithm (Porter, 1980) • The IDs of the token’s word clusters. The clusters were generated by performing Brown clustering (Brown et al., 1992) on 56,345,753 Tweets by Owoputi et al. (2013) and are available online.2 • The presence of a hashtag or URL in the message (one feature each) • The presence of a question mark token in the message • We use the opinion lexicon by Bing Liu (Hu and Liu, 2004), the MPQA subjectivity lexicon (Wiebe et al., 2005) and the Twitrratr wordlist, which all provide a list of positive and negative words, to compute a prior polarity of the message. For each of the three sentiment lexica two features capture whether the majority of the tokens in the message were in the positive or negative sentiment list. The same is done for hashtags using the NRC hashtag sentiment lexicon (Mohammad et al., 2013). • We apply special handling to features in a negation context. A token is considered as negated if it occurs after a negation word (up to the next punctuation). All</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>538--541</pages>
<contexts>
<context position="2211" citStr="Kouloumpis et al. (2011)" startWordPosition="311" endWordPosition="314">have been based on standard text categorization techniques, e.g. training a classifier using some sort of bag-of-words feature representation. This is also true for sentiment analysis of microblogs. Among This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ the first to work specifically with Twitter1 data were Go et al. (2009), who use emoticons as labels for the messages. Similarly, Davidov et al. (2010), Pak and Paroubek (2010), and Kouloumpis et al. (2011) use this method of distant supervision to overcome the data acquisition barrier. Barbosa and Feng (2010) make use of three different sentiment detection websites to label messages and use mostly non-lexical features to improve the robustness of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approaches to sentiment analysis of microblogs, such as O’Con</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the omg. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media, pages 538–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1247" citStr="Liu (2012)" startWordPosition="173" endWordPosition="174">ng. The improvements result in an increase of 4.18 F-measure points on this year’s Twitter test set, ranking 3rd. 1 Introduction Automatic analysis of sentiment expressed in text is an active research area in natural language processing with obvious commercial interest. In the simplest formulation of the problem, sentiment analysis is framed as a categorization problem over documents, where the set of categories is typically a set of polarity values, such as positive, neutral, and negative. Many approaches to document-level sentiment classification have been proposed. For an overview see e.g. Liu (2012). Text in social media and in particular microblog messages are a challenging text genre for sentiment classification, as they introduce additional problems such as short text length, spelling variation, special tokens, topic variation, language style and multilingual content. Following Pang et al. (2002), most sentiment analysis systems have been based on standard text categorization techniques, e.g. training a classifier using some sort of bag-of-words feature representation. This is also true for sentiment analysis of microblogs. Among This work is licenced under a Creative Commons Attribut</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013),</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3594" citStr="Mohammad et al. (2013)" startWordPosition="529" endWordPosition="532"> Evaluation (SemEval) organized a shared task on sentiment analysis in Twitter (Nakov et al., 2013) to enable a better comparison of different approaches for sentiment analysis of microblogs. The shared task consisted of two subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B). For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter part and an SMS part in order to test domain sensitivity. Among the best performing systems were Mohammad et al. (2013), G¨unther and Furrer (2013) and Becker et al. (2013), who all train linear models on a variety of task-specific features. In this year the corpus resources were used for a re-run of the shared task (Rosenthal et al., 2014), introducing two new Twitter test sets, as well as LiveJournal data. 1A popular microblogging service on the internet, its messages are commonly referred to as “Tweets.” 497 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 497–502, Dublin, Ireland, August 23-24, 2014. 2 System Desciption This section describes the details of our sen</context>
<context position="7236" citStr="Mohammad et al., 2013" startWordPosition="1137" endWordPosition="1140">ne.2 • The presence of a hashtag or URL in the message (one feature each) • The presence of a question mark token in the message • We use the opinion lexicon by Bing Liu (Hu and Liu, 2004), the MPQA subjectivity lexicon (Wiebe et al., 2005) and the Twitrratr wordlist, which all provide a list of positive and negative words, to compute a prior polarity of the message. For each of the three sentiment lexica two features capture whether the majority of the tokens in the message were in the positive or negative sentiment list. The same is done for hashtags using the NRC hashtag sentiment lexicon (Mohammad et al., 2013). • We apply special handling to features in a negation context. A token is considered as negated if it occurs after a negation word (up to the next punctuation). All token, stem and word cluster features are marked with a negation prefix. Additionally, the polarity for token in a negation context is inverted when computing the prior lexicon polarity. • We use the part-of-speech tags computed by the part-of-speech tagger of the Twitter NLP tools by Owoputi et al. (2013) to exclude certain tokens. Assuming they do not carry any helpful sentiment information, no features are computed for token r</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3071" citStr="Nakov et al., 2013" startWordPosition="445" endWordPosition="448">s of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approaches to sentiment analysis of microblogs, such as O’Connor et al. (2010), Thelwall et al. (2010) and Zhang et al. (2011). For a detailed discussion see G¨unther (2013). In 2013, the International Workshop on Semantic Evaluation (SemEval) organized a shared task on sentiment analysis in Twitter (Nakov et al., 2013) to enable a better comparison of different approaches for sentiment analysis of microblogs. The shared task consisted of two subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B). For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter part and an SMS part in order to test domain sensitivity. Among the best performing systems were Mohammad et al. (2013), G¨unther and Furrer (2013) and Becker et al. (2013), who all train linear m</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312– 320, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>122--129</pages>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R Routledge, and Noah A Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the International AAAI Conference on Weblogs and Social Media, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>volume</volume>
<contexts>
<context position="2181" citStr="Pak and Paroubek (2010)" startWordPosition="306" endWordPosition="309">t sentiment analysis systems have been based on standard text categorization techniques, e.g. training a classifier using some sort of bag-of-words feature representation. This is also true for sentiment analysis of microblogs. Among This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ the first to work specifically with Twitter1 data were Go et al. (2009), who use emoticons as labels for the messages. Similarly, Davidov et al. (2010), Pak and Paroubek (2010), and Kouloumpis et al. (2011) use this method of distant supervision to overcome the data acquisition barrier. Barbosa and Feng (2010) make use of three different sentiment detection websites to label messages and use mostly non-lexical features to improve the robustness of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approaches to sentiment analysi</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of LREC, volume 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1553" citStr="Pang et al. (2002)" startWordPosition="216" endWordPosition="219"> of the problem, sentiment analysis is framed as a categorization problem over documents, where the set of categories is typically a set of polarity values, such as positive, neutral, and negative. Many approaches to document-level sentiment classification have been proposed. For an overview see e.g. Liu (2012). Text in social media and in particular microblog messages are a challenging text genre for sentiment classification, as they introduce additional problems such as short text length, spelling variation, special tokens, topic variation, language style and multilingual content. Following Pang et al. (2002), most sentiment analysis systems have been based on standard text categorization techniques, e.g. training a classifier using some sort of bag-of-words feature representation. This is also true for sentiment analysis of microblogs. Among This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ the first to work specifically with Twitter1 data were Go et al. (2009), who use emoticons as labels for the messages. Similarly, Davidov et al. (20</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping. Program: electronic library and information systems,</title>
<date>1980</date>
<pages>14--3</pages>
<contexts>
<context position="6425" citStr="Porter, 1980" startWordPosition="992" endWordPosition="993">resulting tokens against a lexical resource is another option, a correct disambiguation of the intended word would require a method making use of context knowledge (e.g. goooodd → good, vs. goooodd → god). 2.2 Features We use the following set of features as input to our supervised classifier: • The normalized tokens as unigrams and bigrams, where stopword and punctuation tokens are excluded from bigrams 2http://www.ark.cs.cmu.edu/TweetNLP • The word stems of the normalized tokens, reducing inflected forms of a word to a common form. The stems were computed using the Porter stemmer algorithm (Porter, 1980) • The IDs of the token’s word clusters. The clusters were generated by performing Brown clustering (Brown et al., 1992) on 56,345,753 Tweets by Owoputi et al. (2013) and are available online.2 • The presence of a hashtag or URL in the message (one feature each) • The presence of a question mark token in the message • We use the opinion lexicon by Bing Liu (Hu and Liu, 2004), the MPQA subjectivity lexicon (Wiebe et al., 2005) and the Twitrratr wordlist, which all provide a list of positive and negative words, to compute a prior polarity of the message. For each of the three sentiment lexica tw</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F Porter. 1980. An algorithm for suffix stripping. Program: electronic library and information systems, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanova</author>
</authors>
<title>Semeval-2014 task 9: Sentiment analysis in twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="3817" citStr="Rosenthal et al., 2014" startWordPosition="570" endWordPosition="573">wo subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B). For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter part and an SMS part in order to test domain sensitivity. Among the best performing systems were Mohammad et al. (2013), G¨unther and Furrer (2013) and Becker et al. (2013), who all train linear models on a variety of task-specific features. In this year the corpus resources were used for a re-run of the shared task (Rosenthal et al., 2014), introducing two new Twitter test sets, as well as LiveJournal data. 1A popular microblogging service on the internet, its messages are commonly referred to as “Tweets.” 497 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 497–502, Dublin, Ireland, August 23-24, 2014. 2 System Desciption This section describes the details of our sentiment analysis system, focusing on the differences to our last year’s implementation. This year we only participated in the subtask on whole message polarity classification (Subtask B). 2.1 Preprocessing For tokenization o</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanova, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanova. 2014. Semeval-2014 task 9: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Anders Johannsen</author>
</authors>
<title>Robust learning in random subspaces: Equipping NLP for OOV effects.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>1171--1180</pages>
<contexts>
<context position="9760" citStr="Søgaard and Johannsen, 2012" startWordPosition="1567" endWordPosition="1570">d that training on a lot of lexical features can lead to brittle NLP systems, that are easily overfit to particular domains. In social media messages the brittleness is particularly acute due to the wide variation in vocabulary and style. While this problem can be eased by using corpus-induced word representations such as the previously introduced word cluster features, it can also be addressed from a learning point of view. Brittleness can be caused by the problem that very strong features (e.g. emoticons) drown out the effect of other useful features. The method of random subspace learning (Søgaard and Johannsen, 2012) seeks to handle this problem by forcing learning algorithms to produce models with more redundancy. It does this by randomly corrupting training instances during learning, so if some useful feature is correlated with a strong feature, the learning algorithm has a better chance to assign it a nonzero weight. We implemented random subspace learning by training the classifier on a concatenation of 25 corrupted copies of the training set. In a corrupted copy, each feature was randomly disabled with a probability of 0.2. Just as for the classifier, the hyperparameters were optimized empirically. 3</context>
</contexts>
<marker>Søgaard, Johannsen, 2012</marker>
<rawString>Anders Søgaard and Anders Johannsen. 2012. Robust learning in random subspaces: Equipping NLP for OOV effects. In COLING (Posters), pages 1171– 1180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Speriosu</author>
<author>Nikita Sudan</author>
<author>Sid Upadhyay</author>
<author>Jason Baldridge</author>
</authors>
<title>Twitter polarity classification with label propagation over lexical links and the follower graph.</title>
<date>2011</date>
<booktitle>In Proceedings of the First Workshop on Unsupervised Learning in NLP,</booktitle>
<pages>53--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2603" citStr="Speriosu et al. (2011)" startWordPosition="372" endWordPosition="375">ns.org/licenses/by/4.0/ the first to work specifically with Twitter1 data were Go et al. (2009), who use emoticons as labels for the messages. Similarly, Davidov et al. (2010), Pak and Paroubek (2010), and Kouloumpis et al. (2011) use this method of distant supervision to overcome the data acquisition barrier. Barbosa and Feng (2010) make use of three different sentiment detection websites to label messages and use mostly non-lexical features to improve the robustness of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approaches to sentiment analysis of microblogs, such as O’Connor et al. (2010), Thelwall et al. (2010) and Zhang et al. (2011). For a detailed discussion see G¨unther (2013). In 2013, the International Workshop on Semantic Evaluation (SemEval) organized a shared task on sentiment analysis in Twitter (Nakov et al., 2013) to enable a better comparison of different approaches for sentiment analysis of microblogs. The shared task consisted of two subtas</context>
</contexts>
<marker>Speriosu, Sudan, Upadhyay, Baldridge, 2011</marker>
<rawString>Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Jason Baldridge. 2011. Twitter polarity classification with label propagation over lexical links and the follower graph. In Proceedings of the First Workshop on Unsupervised Learning in NLP, pages 53– 63. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
<author>Di Cai</author>
<author>Arvid Kappas</author>
</authors>
<title>Sentiment strength detection in short informal text.</title>
<date>2010</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>61</volume>
<issue>12</issue>
<marker>Thelwall, Buckley, Paltoglou, Di Cai, Kappas, 2010</marker>
<rawString>Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas. 2010. Sentiment strength detection in short informal text. Journal of the American Society for Information Science and Technology, 61(12):2544–2558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="6854" citStr="Wiebe et al., 2005" startWordPosition="1070" endWordPosition="1073">.cs.cmu.edu/TweetNLP • The word stems of the normalized tokens, reducing inflected forms of a word to a common form. The stems were computed using the Porter stemmer algorithm (Porter, 1980) • The IDs of the token’s word clusters. The clusters were generated by performing Brown clustering (Brown et al., 1992) on 56,345,753 Tweets by Owoputi et al. (2013) and are available online.2 • The presence of a hashtag or URL in the message (one feature each) • The presence of a question mark token in the message • We use the opinion lexicon by Bing Liu (Hu and Liu, 2004), the MPQA subjectivity lexicon (Wiebe et al., 2005) and the Twitrratr wordlist, which all provide a list of positive and negative words, to compute a prior polarity of the message. For each of the three sentiment lexica two features capture whether the majority of the tokens in the message were in the positive or negative sentiment list. The same is done for hashtags using the NRC hashtag sentiment lexicon (Mohammad et al., 2013). • We apply special handling to features in a negation context. A token is considered as negated if it occurs after a negation word (up to the next punctuation). All token, stem and word cluster features are marked wi</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ley Zhang</author>
<author>Riddhiman Ghosh</author>
<author>Mohamed Dekhil</author>
<author>Meichun Hsu</author>
<author>Bing Liu</author>
</authors>
<title>Combining lexiconbased and learning-based methods for twitter sentiment analysis. HP Laboratories,</title>
<date>2011</date>
<tech>Technical Report HPL-2011-89.</tech>
<contexts>
<context position="2876" citStr="Zhang et al. (2011)" startWordPosition="415" endWordPosition="418">rcome the data acquisition barrier. Barbosa and Feng (2010) make use of three different sentiment detection websites to label messages and use mostly non-lexical features to improve the robustness of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approaches to sentiment analysis of microblogs, such as O’Connor et al. (2010), Thelwall et al. (2010) and Zhang et al. (2011). For a detailed discussion see G¨unther (2013). In 2013, the International Workshop on Semantic Evaluation (SemEval) organized a shared task on sentiment analysis in Twitter (Nakov et al., 2013) to enable a better comparison of different approaches for sentiment analysis of microblogs. The shared task consisted of two subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B). For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter p</context>
</contexts>
<marker>Zhang, Ghosh, Dekhil, Hsu, Liu, 2011</marker>
<rawString>Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Meichun Hsu, and Bing Liu. 2011. Combining lexiconbased and learning-based methods for twitter sentiment analysis. HP Laboratories, Technical Report HPL-2011-89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>