<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000225">
<title confidence="0.994924">
Stream-based Translation Models for Statistical Machine Translation
</title>
<author confidence="0.997854">
Abby Levenberg Chris Callison-Burch Miles Osborne
</author>
<affiliation confidence="0.9978925">
School of Informatics Computer Science Department School of Informatics
University of Edinburgh Johns Hopkins University University of Edinburgh
</affiliation>
<email confidence="0.997116">
a.levenberg@ed.ac.uk ccb@cs.jhu.edu miles@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993892" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999601210526316">
Typical statistical machine translation sys-
tems are trained with static parallel corpora.
Here we account for scenarios with a continu-
ous incoming stream of parallel training data.
Such scenarios include daily governmental
proceedings, sustained output from transla-
tion agencies, or crowd-sourced translations.
We show incorporating recent sentence pairs
from the stream improves performance com-
pared with a static baseline. Since frequent
batch retraining is computationally demand-
ing we introduce a fast incremental alternative
using an online version of the EM algorithm.
To bound our memory requirements we use
a novel data-structure and associated training
regime. When compared to frequent batch re-
training, our online time and space-bounded
model achieves the same performance with
significantly less computational overhead.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998449363636363">
There is more parallel training data available to-
day than there has ever been and it keeps increas-
ing. For example, the European Parliament1 releases
new parallel data in 22 languages on a regular basis.
Project Syndicate2 translates editorials into seven
languages (including Arabic, Chinese and Russian)
every day. Existing translation systems often get
‘crowd-sourced’ improvements such as the option
to contribute a better translation to GoogleTrans-
late3. In these and many other instances, the data can
be viewed as an incoming unbounded stream since
</bodyText>
<footnote confidence="0.999940666666667">
1http://www.europarl.europa.eu
2http://www.project-syndicate.org
3http://www.translate.google.com
</footnote>
<bodyText confidence="0.999881259259259">
the corpus grows continually with time. Dealing
with such unbounded streams of parallel sentences
presents two challenges: making retraining efficient
and operating within a bounded amount of space.
Statistical Machine Translation (SMT) systems
are typically batch trained, often taking many CPU-
days of computation when using large volumes of
training material. Incorporating new data into these
models forces us to retrain from scratch. Clearly,
this makes rapidly adding newly translated sen-
tences into our models a daunting engineering chal-
lenge. We introduce an adaptive training regime us-
ing an online variant of EM that is capable of in-
crementally adding new parallel sentences without
incurring the burdens of full retraining.
For situations with large volumes of incoming
parallel sentences we are also forced to consider
placing space-bounds on our SMT system. We in-
troduce a dynamic suffix array which allows us to
add and delete parallel sentences, thereby maintain-
ing bounded space despite processing a potentially
high-rate input stream of unbounded length.
Taken as a whole we show that online translation
models operating within bounded space can perform
as well as systems which are batch-based and have
no space constraints thereby making our approach
suitable for stream-based translation.
</bodyText>
<sectionHeader confidence="0.983193" genericHeader="method">
2 Stepwise Online EM
</sectionHeader>
<bodyText confidence="0.999608333333333">
The EM algorithm is a common way of inducing
latent structure from unlabeled data in an unsuper-
vised manner (Dempster et al., 1977). Given a set
of unlabeled examples and an initial, often uniform
guess at a probability distribution over the latent
variables, the EM algorithm maximizes the marginal
</bodyText>
<page confidence="0.982255">
394
</page>
<note confidence="0.7528535">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 394–402,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999659375">
log-likelihood of the examples by repeatedly com-
puting the expectation of the conditional probability
of the latent data with respect to the current distri-
bution, and then maximizing the expectations over
the observations into a new distribution used in the
next iteration. EM (and related variants such as vari-
ational or sampling approaches) form the basis of
how SMT systems learn their translation models.
</bodyText>
<subsectionHeader confidence="0.994154">
2.1 Batch vs. Online EM
</subsectionHeader>
<bodyText confidence="0.997403657142857">
Computing an expectation for the conditional prob-
abilities requires collecting the sufficient statistics S
over the set of n unlabeled examples. In the case
of a multinomial distribution, S is comprised of the
counts over each conditional observation occurring
in the n examples. In traditional batch EM, we col-
lect the counts over the entire dataset of n unlabeled
training examples via the current ‘best-guess’ proba-
bility model Bt at iteration t (E-step) before normal-
izing the counts into probabilities B(S) (M-step)4.
After each iteration all the counts in the sufficient
statistics vector S are cleared and the count collec-
tion begins anew using the new distribution �Bt+1.
When we move to processing an incoming data
stream, however, the batch EM algorithm’s require-
ment that all data be available for each iteration be-
comes impractical since we do not have access to all
n examples at once. Instead we receive examples
from the input stream incrementally. For this reason
online EM algorithms have been developed to up-
date the probability model B� incrementally without
needing to store and iterate through all the unlabeled
training data repeatedly.
Various online EM algorithms have been investi-
gated (see Liang and Klein (2009) for an overview)
but our focus is on the stepwise online EM (sOEM)
algorithm (Cappe and Moulines, 2009). Instead
of iterating over the full set of training examples,
sOEM stochastically approximates the batch E-step
and incorporates the information from the newly
available streaming observations in steps. Each step
is called a mini-batch and is comprised of one or
more new examples encountered in the stream.
Unlike in batch EM, in sOEM the expected counts
are retained between EM iterations and not cleared.
</bodyText>
<footnote confidence="0.9899605">
4As the M-step can be computed in closed form we desig-
nate it in this work as B(S).
</footnote>
<table confidence="0.807771363636364">
Algorithm 1: Batch EM for Word Alignments
Input: {F(source),E (target)} sentence-pairs
Output: MLE BT over alignments a
�B0 +-MLE initialization;
for iteration k = 0, ... , T do
S +- 0; // reset counts
foreach (f, e) E IF, E} do // E-step
S +- S + � Pr(f, a′Ie; Bt);
a′∈a
end
Bt+1 +- Bt(S) ; // M-step
</table>
<subsectionHeader confidence="0.303945">
end
</subsectionHeader>
<bodyText confidence="0.999834222222222">
That is, for each new example we interpolate its ex-
pected count with the existing set of sufficient statis-
tics. For each step we use a stepsize parameter y
which mixes the information from the current ex-
ample with information gathered from all previous
examples. Over time the sOEM model probabilities
begin to stabilize and are guaranteed to converge to
a local maximum (Cappe and Moulines, 2009).
Note that the stepsize y has a dependence on the
current mini-batch. As we observe more incoming
data the model’s current probability distribution is
closer to the true distribution so the new observa-
tions receive less weight. From Liang and Klein
(2009), if we set the stepsize as yt = (t + 2)−α,
with 0.5 &lt; α &lt; 1, we can guarantee convergence in
the limit as n -* oc. If we set α low, y weighs the
newly observed statistics heavily whereas if y is low
new observations are down-weighted.
</bodyText>
<subsectionHeader confidence="0.999416">
2.2 Batch EM for Word Alignments
</subsectionHeader>
<bodyText confidence="0.999964923076923">
Batch EM is used in statistical machine translation
to estimate word alignment probabilities between
parallel sentences. From these alignments, bilingual
rules or phrase pairs can be extracted. Given a set
of parallel sentence examples, IF, E}, with F the
set of source sentences and E the corresponding tar-
get sentences, we want to find the latent alignments
a for a sentence pair (f, e) E IF, E} that defines
the most probable correspondence between words fj
and ez such that aj = i. We can induce these align-
ments using an HMM-based alignment model where
the probability of alignment aj is dependent only on
the previous alignment at aj−1 (Vogel et al., 1996).
</bodyText>
<page confidence="0.996927">
395
</page>
<figure confidence="0.981064">
Algorithm 2: sOEM Algorithm for Word Align-
ments
We can write
|f|
Pr(f, a  |e) = � H p(aj  |aj−1, |e|) · p(fj  |ea,)
a′∈a j=1
Input: mini-batches of sentence pairs
{M : M ⊂ {F(source), E(target)}}
</figure>
<bodyText confidence="0.994155958333333">
Input: stepsize weight α
where we assume a first-order dependence on previ-
ously aligned positions.
To find the most likely parameter weights for
the translation and alignment probabilities for the
HMM-based alignments, we employ the EM algo-
rithm via dynamic programming. Since HMMs have
multiple local minima, we seed the HMM-based
model probabilities with a better than random guess
using IBM Model 1 (Brown et al., 1993) as is stan-
dard. IBM Model 1 is of the same form as the
HMM-based model except it uses a uniform distri-
bution instead of a first-order dependency. Although
a series of more complex models are defined, IBM
Models 2 to Model 6 (Brown et al., 1993; Och and
Ney, 2003), researchers typically find that extract-
ing phrase pairs or translation grammar rules using
Model 1 and the HMM-based alignments results in
equivalently high translation quality. Nevertheless,
there is nothing in our approach which limits us to
using just Model 1 and the HMM model.
A high-level overview of the standard, batch EM
algorithm applied to HMM-based word alignment
model is shown in Algorithm 1.
</bodyText>
<subsectionHeader confidence="0.996513">
2.3 Stepwise EM for Word Alignments
</subsectionHeader>
<bodyText confidence="0.999986833333334">
Application of sOEM to HMM and Model 1 based
word aligning is straightforward. The process of
collecting the counts over the expected conditional
probabilities inside each iteration loop remains the
same as in the batch case. However, instead of clear-
ing the sufficient statistics between the iterations we
retain them and interpolate them with the batch of
counts gathered in the next iteration.
Algorithm 2 shows high level pseudocode of our
sOEM framework as applied to HMM-based word
alignments. Here we have an unbounded input
stream of source and target sentences {F, E} which
we do not have access to in its entirety at once.
Instead we observe mini-batches {M} comprised
of chronologically ordered strict subsets of the full
stream. To word align the sentences for each mini-
batch m ∈ M, we use the probability assigned by
the current model parameters and then interpolate
</bodyText>
<equation confidence="0.894708785714286">
BT over alignments a
B0 ←MLE initialization;
5 ← 0; k = 0;
foreach mini-batch {m : m ∈ M} do
for iteration t = 0, ... , T do
foreach (f, e) ∈ {m} do // E-step
�s� ← Pr(f, a′|e; Bt);
a′∈a
end
-y = (k + 2)−α; k = k + 1; // stepsize
5 ← -ys + (1 − -y)5; // interpolate
�Bt+1 ← �Bt(5) ; // M-step
end
end
</equation>
<bodyText confidence="0.99962075">
the newest sufficient statistics s� with our full count
vector S using an interpolation parameter -y. The in-
terpolation parameter -y has a dependency on how
far along the input stream we are processing.
</bodyText>
<sectionHeader confidence="0.993836" genericHeader="method">
3 Dynamic Suffix Arrays
</sectionHeader>
<bodyText confidence="0.968275095238096">
So far we have shown how to incrementally retrain
translation models. We now consider how we might
bound the space we use for them when processing
(potentially) unbounded streams of parallel data.
Suffix arrays are space-efficient data structures for
fast searching over large text strings (Manber and
Myers, 1990). Treating the entire corpus as a sin-
gle string, a suffix array holds in lexicographical or-
der (only) the starting index of each suffix of the
string. After construction, since the corpus is now
ordered, we can query the suffix array quickly us-
ing binary search to efficiently find all occurrences
of a particular token or sequence of tokens. Then we
can easily compute, on-the-fly, the statistics required
such as translation probabilities for a given source
phrase. Suffix arrays can also be compressed, which
make them highly attractive structures for represent-
ing massive translation models (Callison-Burch et
al., 2005; Lopez, 2008).
We need to delete items if we wish to maintain
Output: MLE
</bodyText>
<page confidence="0.96069">
396
</page>
<figureCaption confidence="0.9965645">
Figure 1: Streaming coverage conditions. In traditional
batch based modeling the coverage of a trained model
never changes. Unbounded coverage operates without
any memory constraints so the model is able to contin-
ually add data from the input stream. Bounded coverage
uses just a fixed window.
</figureCaption>
<bodyText confidence="0.999936416666667">
constant space when processing unbounded streams.
Standard suffix arrays are static, store a fixed corpus
and do not support deletions. Nevertheless, a dy-
namic variant of the suffix array does support dele-
tions as well as insertions and therefore can be used
in our stream-based approach (Salson et al., 2009).
Using a dynamic suffix array, we can compactly
represent the set of parallel sentences from which
we eventually extract grammar rules. Furthermore,
when incorporating new parallel sentences, we sim-
ply insert them into the array and, to maintain con-
stant space usage, we delete an equivalent number.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999939142857143">
In this section we describe the experiments con-
ducted comparing various batch trained translation
models (TMs) versus online incrementally retrained
TMs in a full SMT setting with different conditions
set on model coverage. We used publicly available
resources for all our tests. We start by showing that
recency motivates incremental retraining.
</bodyText>
<subsectionHeader confidence="0.999185">
4.1 Effects of Recency on SMT
</subsectionHeader>
<bodyText confidence="0.999769">
For language modeling, it is known that perfor-
mance can be improved using the criterion of re-
cency where training data is drawn from times
chronologically closer to the test data (Rosenfeld,
</bodyText>
<figure confidence="0.95283975">
2.5
2
Delta in BLEU scores 1.5
1
0.5
0
5 10 15 20 25 30 35
epochs
</figure>
<figureCaption confidence="0.999006">
Figure 2: Recency effects to SMT performance. De-
</figureCaption>
<bodyText confidence="0.947262125">
picted are the differences in BLEU scores for multiple
test points decoded by a static baseline system and a sys-
tem batched retrained on a fixed sized window prior to
the test point in question. The results are accentuated at
the end of the timeline when more time has passed con-
firming that recent data impacts translation performance.
1995). Given an incoming stream of parallel text,
we gauged the extent to which incorporating recent
data into a TM affects translation quality.
We used the Europarl corpus5 with the Fr-En lan-
guage pair using French as source and English as tar-
get. Europarl is released in the format of a daily par-
liamentary session per time-stamped file. The actual
dates of the full corpus are interspersed unevenly
(they do not convene daily) over a continuous time-
line corresponding to the parliament sessions from
April,1996 through October, 2006, but for concep-
tual simplicity we treated the corpus as a continual
input stream over consecutive days.
As a baseline we aligned the first 500k sentence
pairs from the beginning of the corpus timeline. We
extracted a grammar for and translated 36 held out
test documents that were evenly spaced along the re-
mainder of the Europarl timeline. These test docu-
ments effectively divided the remaining training data
into epochs and we used a sliding window over the
timeline to build 36 distinct, overlapping training
sets of 500k sentences each.
We then translated all 36 test points again using
a new grammar for each document extracted from
only the sentences contained in the epoch that was
before it. To explicitly test the effect of recency
</bodyText>
<footnote confidence="0.989833">
5Available at http://www.statmt.org/europarl
</footnote>
<figure confidence="0.997857933333333">
Test Points
Static
input stream
model coverage
Unb*unded
Test Points
input stream
epoch 1 epoch 2
model coverage
B*unded
Test Points
model coverage
epoch 2
input stream
sfiding windows
</figure>
<page confidence="0.98133">
397
</page>
<bodyText confidence="0.9999671">
on the TM all other factors of the SMT pipeline re-
mained constant including the language model and
the feature weights. Hence, the only change from
the static baseline to the epochs performance was the
TM data which was based on recency. Note that at
this stage we did not use any incremental retraining.
Results are shown in Figure 2 as the differences
in BLEU score (Papineni et al., 2001) between the
baseline TM versus the translation models trained
on material chronologically closer to the given test
point. The consistently positive deltas in BLEU
scores between the model that is never retrained and
the models that are retrained show that we achieve a
higher translation performance when using more up-
to-date TMs that incorporate recent sentence pairs.
As the chronological distance between the initial,
static model and the retrained models increases, we
see ever-increasing differences in translation perfor-
mance. This underlines the need to retrain transla-
tion models with timely material.
</bodyText>
<subsectionHeader confidence="0.9877105">
4.2 Unbounded and Bounded Translation
Model Retraining
</subsectionHeader>
<bodyText confidence="0.9998846">
Here we consider how to process a stream along two
main axes: by bounding time (batch versus incre-
mental retraining) and by bounding space (either us-
ing all the stream seen so far, or only using a fixed
sized sample of it).
To ensure the recency results reported above were
not limited to French-English, this time our paral-
lel input stream was generated from the German-
English language pair of Europarl with German as
source and English again as target. For testing we
held out a total of 22k sentences from 10 evenly
spaced intervals in the input stream which divided
the input stream into 10 epochs. Stream statistics for
three example epochs are shown in Table 1. We held
out 4.5k sentence pairs as development data to opti-
mize the feature function weights using minimum
error rate training (Och, 2003) and these weights
were used by all models. We used Joshua (Li et
al., 2009), a syntax-based decoder with a suffix array
implementation, and rule induction via the standard
Hiero grammar extraction heuristics (Chiang, 2007)
for the TMs. Note that nothing hinges on whether
we used a syntax or a phrase-based system.
We used a 5-gram, Kneser-Ney smoothed lan-
guage model (LM) trained on the initial segment of
</bodyText>
<table confidence="0.9985766">
Ep From–To Sent Pairs Source/Target
00 04/1996–12/2000 600k 15.0M/16.0M
03 02/2002–09/2002 70k 1.9M/2.0M
06 10/2003–03/2004 60k 1.6M/1.7M
10 03/2006–09/2006 73k 1.9M/2.0M
</table>
<tableCaption confidence="0.99002625">
Table 1: Date ranges, total sentence pairs, and source and
target word counts encountered in the input stream for
example epochs. Epoch 00 is baseline data that is also
used as a seed corpus for the online models.
</tableCaption>
<bodyText confidence="0.999943714285714">
the target side parallel data used in the first base-
line as described further in the next subsection. As
our initial experiments aim to isolate the effect of
changes to the TM on overall translation system per-
formance, our in-domain LM remains static for ev-
ery decoding run reported below until indicated.
We used the open-source toolkit GIZA++ (Och
and Ney, 2003) for all word alignments. For the
online adaptation experiments we modified Model
1 and the HMM model in GIZA++ to use the sOEM
algorithm. Batch baselines were aligned using the
standard version of GIZA++. We ran the batch and
incremental versions of Model 1 and HMM for the
same number of iterations each in both directions.
</bodyText>
<subsectionHeader confidence="0.996095">
4.3 Time and Space Bounds
</subsectionHeader>
<bodyText confidence="0.99982">
For both batch and sOEM we ran a number of ex-
periments listed below corresponding to the differ-
ent training scenarios diagrammed in Figure 1.
</bodyText>
<listItem confidence="0.935547875">
1. Static: We used the first half of the in-
put stream, approximately 600k sentences and
15/16 million source/target words, as parallel
training data. We then translated each of the 10
test sets using the static model. This is the tradi-
tional approach and the coverage of the model
never changes.
2. Unbounded Space: Batch or incremental re-
training with no memory constraint. For each
epoch in the stream, we retrained the TM us-
ing all the data from the beginning of the in-
put stream until just before the present with re-
spect to a given test point. As more time passes
our training data set grows so each batch run
of GIZA++ takes more time. Overall this is the
most computationally expensive approach.
</listItem>
<page confidence="0.996752">
398
</page>
<table confidence="0.9930598">
Baseline Unbounded Bounded
Epoch Test Date Test Sent. Train Sent. Rules Train Sent. Rules Train Sent. Rules
03 09/23/2002 1.0k 580k 4.0M 800k 5.0M 580k 4.2M
06 03/29/2004 1.5k 580k 5.0M 1.0M 7.0M 580k 5.5M
10 09/26/2006 3.5k 580k 8.5M 1.3M 14.0M 580k 10.0M
</table>
<tableCaption confidence="0.99175">
Table 2: Translation model statistics for example epochs and the next test dates grouped by experimental condition.
Test and Train Sent. is the number of sentence pairs in test and training data respectively. Rules is the count of unique
Hiero grammar rules extracted for the corresponding test set.
</tableCaption>
<figure confidence="0.935698">
1 2 3 4 5 6 7 8 9 10
epochs
</figure>
<figureCaption confidence="0.998842166666667">
Figure 3: Static vs. online TM performance. Gains in
translation performance measured by BLEU are achieved
when recent German-English sentence pairs are auto-
matically incorporated into the TM. Shown are relative
BLEU improvements for the online models against the
static baseline.
</figureCaption>
<bodyText confidence="0.996407035714286">
3. Bounded Space: Batch and incremental re-
training with an enforced memory constraint.
Here we batch or incrementally retrain using
a sliding window approach where the training
set size (the number of sentence pairs) remains
constant. In particular, we ensured that we
used the same number of sentences as the base-
line. Each batch run of GIZA++ takes approxi-
mately the same time.
The time for aligning in the sOEM model is unaf-
fected by the bounded/unbounded conditions since
we always only align the mini-batch of sentences
encountered in the last epoch. In contrast, for batch
EM we must realign all the sentences in our training
set from scratch to incorporate the new training data.
Similarly space usage for the batch training grows
with the training set size. For sOEM, in theory mem-
ory used is with respect to vocabulary size (which
grows slowly with the stream size) since we retain
count history for the entire stream. To make space
usage truly constant, we filter for just the needed
word pairs in the current epoch being aligned. This
effectively means that online EM is more mem-
ory efficient than the batch version. As our exper-
iments will show, the sufficient statistics kept be-
tween epochs by sOEM benefits performance com-
pared to the batch models which can only use infor-
mation present within the batch itself.
</bodyText>
<subsectionHeader confidence="0.996955">
4.4 Incremental Retraining Procedure
</subsectionHeader>
<bodyText confidence="0.999981307692308">
Our incremental adaptation procedure was as fol-
lows: after the latest mini-batch of sentences had
been aligned using sOEM we added all newly
aligned sentence pairs to the dynamic suffix ar-
rays. For the experiments where our memory was
bounded, we also deleted an equal number of sen-
tences from the suffix arrays before extracting the
Hiero grammar for the next test point. For the un-
bounded coverage experiments we deleted nothing
prior to grammar extraction. Table 2 presents statis-
tics for the number of training sentence pairs and
grammar rules extracted for each coverage condition
for various test points.
</bodyText>
<subsectionHeader confidence="0.681232">
4.5 Results
</subsectionHeader>
<bodyText confidence="0.999508181818182">
Figure 3 shows the results of the static baseline
against both the unbounded and bounded online EM
models. We can see that both the online models
outperform the static baseline. On average the un-
constrained model that contains more sentence pairs
for rule extraction slightly outperforms the bounded
condition which uses less data per epoch. However,
the static baseline and the bounded models both use
the same number of sentence-pairs for TM training.
We see there is a clear gain by incorporating recent
sentence-pairs made available by the stream.
</bodyText>
<figure confidence="0.998723363636364">
Delta in BLEU scores
0.8
0.6
0.4
0.2
1.6
1.4
1.2
1
unbounded
bounded
</figure>
<page confidence="0.993474">
399
</page>
<table confidence="0.9948692">
Static Baseline Retrained (Unbounded) Retrained (Bounded)
Test Date Batch Batch Online Batch Online
09/23/2002 26.10 26.60 26.43 26.19 26.40
03/29/2004 27.40 28.33 28.42 28.06 28.38
09/26/2006 28.56 29.74 29.75 29.73 29.80
</table>
<tableCaption confidence="0.8632644">
Table 3: Sample BLEU results for all baseline and online EM model conditions. The static baseline is a traditional
model that is never retrained. The batch unbounded and batch bounded models incorporate new data from the stream
but retraining is slow and computationally expensive (best results are bolded). In contrast both unbounded and bounded
online models incrementally retrain only the mini-batch of new sentences collected from the incoming stream so
quickly adopt the new data (best results are italicized).
</tableCaption>
<bodyText confidence="0.997268322580645">
Table 3 gives results of the online models com-
pared to the batch retrained models. For presentation
clarity we show only a sample of the full set of ten
test points though all results follow the pattern that
using more aligned sentences to derive our gram-
mar set resulted in slightly better performance ver-
sus a restricted training set. However, for the same
coverage constraints not only do we achieve com-
parable performance to batch retrained models us-
ing the sOEM method of incremental adaptation, we
are able to align and adopt new data from the input
stream orders of magnitude quicker since we only
align the mini-batch of sentences collected from the
last epoch. In the bounded condition, not only do
we benefit from quicker adaptation, we also see that
sOEM models slightly outperform the batch based
models due to the online algorithm employing a
longer history of count-based evidence to draw on
when aligning new sentence pairs.
Figure 4 shows two example test sentences that
benefited from the online TM adaptation. Trans-
lations from the online model produce more and
longer matching phrases for both sentences (e.g.,
“creation of such a”, “of the occupying forces”)
leading to more fluent output as well as the improve-
ments achieved in BLEU scores.
We experimented with a variety of interpolation
parameters (see Algorithm 2) but found no signifi-
cant difference between them (the biggest improve-
ment gained over all test points for all parameter set-
tings was less than 0.1% BLEU).
</bodyText>
<subsectionHeader confidence="0.994312">
4.6 Increasing LM Coverage
</subsectionHeader>
<bodyText confidence="0.998015">
A natural and interesting extension to the experi-
ments above is to use the target side of the incoming
stream to extend the LM coverage alongside the TM.
</bodyText>
<table confidence="0.99923325">
Test Date Static Unbounded Bounded
09/23/2002 26.46 27.11 26.96
03/29/2004 28.11 29.53 29.20
09/26/2006 29.53 30.94 30.88
</table>
<tableCaption confidence="0.752688333333333">
Table 4: Unbounded LM coverage improvements. Shown
are the BLEU scores for each experimental conditional
when we allow the LM coverage to increase.
</tableCaption>
<bodyText confidence="0.999785666666667">
It is well known that more LM coverage (via larger
training data sets) is beneficial to SMT performance
(Brants et al., 2007) so we investigated whether re-
cency gains for the TM were additive with recency
gains afforded by a LM.
To test this we added all the target side data from
the beginning of the stream to the most recent epoch
into the LM training set before each test point. We
then batch retrained6 and used the new LM with
greater coverage for the next decoding run. Experi-
ments were for the static baseline and online models.
Results are reported in Table 4. We can see that
increasing LM coverage is complimentary to adapt-
ing the TM with recent data. Comparing Tables
3 and 4, for the bounded condition, adapting only
the TM achieved an absolute improvement of +1.24
BLEU over the static baseline for the final test point.
We get another absolute gain of +1.08 BLEU by al-
lowing the LM coverage to adapt as well. Using an
online, adaptive model gives a total gain of +2.32
BLEU over a static baseline that does not adapt.
</bodyText>
<footnote confidence="0.994161">
6Although we batch retrain the LMs we could use an online
LM that incorporates new vocabulary from the input stream as
in Levenberg and Osborne (2009).
</footnote>
<page confidence="0.961537">
400
</page>
<note confidence="0.859428142857143">
Source: Die Kommission ist bereit, an der Schaffung eines solchen Rechtsrahmens unter Zugrundelegung von vier wesentlichen
Prinzipien mitzuwirken.
Reference: The commission is willing to cooperate in the creation of such a legal framework on the basis of four essential principles.
Static: The commission is prepared, in the creation of a legal framework, taking account of four fundamental principles them.
On(ine: The commission is prepared to participate in the creation of such a legal framework, based on four fundamental principles.
Source: Unser Standpunkt ist klar und allseits bekannt: Wir sind gegen den Krieg und die Besetzung des Irak durch die USA und das
Vereinigte K6nigreich, und wir verlangen den unverzOglichen Abzug der Besatzungsmachte aus diesem Land.
</note>
<tableCaption confidence="0.9950555">
Reference: Our position is clear and well known: we are against the war and the US-British occupation in Iraq and we demand the
immediate withdrawal of the occupying forces from that country.
Static: Our position is clear and we all know: we are against the war and the occupation of Iraq by the United States and the United
Kingdom, and we are calling for the immediate withdrawal of the besatzungsmachte from this country.
On(ine: Our position is clear and well known: we are against the war and the occupation of Iraq by the United States and the United
Kingdom, and we demand the immediate withdrawal of the occupying forces from this country.
</tableCaption>
<figureCaption confidence="0.9632445">
Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent
sentences. In both examples we get longer matching phrases in the online translation compared to the static one.
</figureCaption>
<sectionHeader confidence="0.999859" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<subsectionHeader confidence="0.884053">
5.1 Translation Model Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999998076923077">
Our work is related to domain adaptation for transla-
tion models. See, for example, Koehn and Schroeder
(2007) or Bertoldi and Federico (2009). Most tech-
niques center around using mixtures of translation
models. Once trained, these models generally never
change. They therefore fall under the batch training
regime. The focus of this work instead is on incre-
mental retraining and also on supporting bounded
memory consumption. Our experiments examine
updating model parameters in a single domain over
different periods in time. Naturally, we could also
use domain adaptation techniques to further improve
how we incorporate new samples.
</bodyText>
<subsectionHeader confidence="0.96991">
5.2 Online EM for SMT
</subsectionHeader>
<bodyText confidence="0.99985075">
For stepwise online EM for SMT models, the only
prior work we are aware of is Liang and Klein
(2009), where variations of online EM were exper-
imented with on various NLP tasks including word
alignments. They showed application of sOEM can
produce quicker convergence compared to the batch
EM algorithm. However, the model presented does
not incorporate any unseen data, instead iterating
over a static data set multiple times using sOEM.
For Liang and Klein (2009) incremental retraining
is simply an alternative way to use a fixed training
set.
</bodyText>
<subsectionHeader confidence="0.997827">
5.3 Streaming Language Models
</subsectionHeader>
<bodyText confidence="0.9999817">
Recent work in Levenberg and Osborne (2009) pre-
sented a streaming LM that was capable of adapt-
ing to an unbounded monolingual input stream in
constant space and time. The LM has the ability to
add or delete n-grams (and their counts) based on
feedback from the decoder after translation points.
The model was tested in an SMT setting and results
showed recent data benefited performance. How-
ever, adaptation was only to the LM and no tests
were conducted on the TM.
</bodyText>
<sectionHeader confidence="0.996435" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999116375">
We have presented an online EM approach for word
alignments. We have shown that, for a SMT system,
incorporating recent parallel data into a TM from an
input stream is beneficial to translation performance
compared to a traditional, static baseline.
Our strategy for populating the suffix array was
simply to use a first-in, first-out stack. For future
work we will investigate whether information pro-
vided by the incoming stream coupled with the feed-
back from the decoder allows for more sophisti-
cated adaptation strategies that reinforce useful word
alignments and delete bad or unused ones.
In the near future we also hope to test the online
EM setup in an application setting such as a com-
puter aided translation or crowdsourced generated
streams via Amazon’s Mechanical Turk.
</bodyText>
<page confidence="0.998351">
401
</page>
<sectionHeader confidence="0.996549" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9951536">
Research supported by EuroMatrixPlus funded by
the European Commission, by the DARPA GALE
program under Contract Nos. HR0011-06-2-0001
and HR0011-06-C-0022, and the NSF under grant
IIS-0713448.
</bodyText>
<sectionHeader confidence="0.998842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999226617977528">
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In WMT09: Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 182–189, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858–867.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263–311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL’05), pages 255–262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Olivier Cappe and Eric Moulines. 2009. Online EM al-
gorithm for latent data models. Journal Of The Royal
Statistical Society Series B, 71:593.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39:1–38.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224–227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In WMT09: Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation, pages 135–139, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Percy Liang and Dan Klein. 2009. Online EM for unsu-
pervised models. In North American Association for
Computational Linguistics (NAACL).
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505–512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Udi Manber and Gene Myers. 1990. Suffix arrays:
A new method for on-line string searches. In The
First Annual ACM-SIAM Symposium on Dicrete Algo-
rithms, pages 319–327.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ’03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160–167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ’02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311–318, Morristown, NJ,
USA. Association for Computational Linguistics.
Ronald Rosenfeld. 1995. Optimizing lexical and n-gram
coverage via judicious use of linguistic data. In In
Proc. European Conf. on Speech Technology, pages
1763–1766.
Mika¨el Salson, Thierry Lecroq, Martine L´eonard, and
Laurent Mouchard. 2009. Dynamic extended suffix
arrays. Journal ofDiscrete Algorithms, March.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics, pages 836–841, Morristown,
NJ, USA. Association for Computational Linguistics.
</reference>
<page confidence="0.998591">
402
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981041">
<title confidence="0.999752">Stream-based Translation Models for Statistical Machine Translation</title>
<author confidence="0.999991">Abby Levenberg Chris Callison-Burch Miles Osborne</author>
<affiliation confidence="0.9998645">School of Informatics Computer Science Department School of Informatics University of Edinburgh Johns Hopkins University University of Edinburgh</affiliation>
<email confidence="0.996273">a.levenberg@ed.ac.ukccb@cs.jhu.edumiles@inf.ed.ac.uk</email>
<abstract confidence="0.999243">Typical statistical machine translation systems are trained with static parallel corpora. Here we account for scenarios with a continuous incoming stream of parallel training data. Such scenarios include daily governmental proceedings, sustained output from translation agencies, or crowd-sourced translations. We show incorporating recent sentence pairs from the stream improves performance compared with a static baseline. Since frequent batch retraining is computationally demanding we introduce a fast incremental alternative using an online version of the EM algorithm. To bound our memory requirements we use a novel data-structure and associated training regime. When compared to frequent batch retraining, our online time and space-bounded model achieves the same performance with significantly less computational overhead.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In WMT09: Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>182--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="28434" citStr="Bertoldi and Federico (2009)" startWordPosition="4664" endWordPosition="4667">on is clear and well known: we are against the war and the occupation of Iraq by the United States and the United Kingdom, and we demand the immediate withdrawal of the occupying forces from this country. Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent sentences. In both examples we get longer matching phrases in the online translation compared to the static one. 5 Related Work 5.1 Translation Model Domain Adaptation Our work is related to domain adaptation for translation models. See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). Most techniques center around using mixtures of translation models. Once trained, these models generally never change. They therefore fall under the batch training regime. The focus of this work instead is on incremental retraining and also on supporting bounded memory consumption. Our experiments examine updating model parameters in a single domain over different periods in time. Naturally, we could also use domain adaptation techniques to further improve how we incorporate new samples. 5.2 Online EM for SMT For stepwise online EM for SMT models, the only prior work we are aware of is Liang</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In WMT09: Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 182–189, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>858--867</pages>
<contexts>
<context position="25526" citStr="Brants et al., 2007" startWordPosition="4177" endWordPosition="4180">arameter settings was less than 0.1% BLEU). 4.6 Increasing LM Coverage A natural and interesting extension to the experiments above is to use the target side of the incoming stream to extend the LM coverage alongside the TM. Test Date Static Unbounded Bounded 09/23/2002 26.46 27.11 26.96 03/29/2004 28.11 29.53 29.20 09/26/2006 29.53 30.94 30.88 Table 4: Unbounded LM coverage improvements. Shown are the BLEU scores for each experimental conditional when we allow the LM coverage to increase. It is well known that more LM coverage (via larger training data sets) is beneficial to SMT performance (Brants et al., 2007) so we investigated whether recency gains for the TM were additive with recency gains afforded by a LM. To test this we added all the target side data from the beginning of the stream to the most recent epoch into the LM training set before each test point. We then batch retrained6 and used the new LM with greater coverage for the next decoding run. Experiments were for the static baseline and online models. Results are reported in Table 4. We can see that increasing LM coverage is complimentary to adapting the TM with recent data. Comparing Tables 3 and 4, for the bounded condition, adapting </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="8438" citStr="Brown et al., 1993" startWordPosition="1328" endWordPosition="1331">395 Algorithm 2: sOEM Algorithm for Word Alignments We can write |f| Pr(f, a |e) = � H p(aj |aj−1, |e|) · p(fj |ea,) a′∈a j=1 Input: mini-batches of sentence pairs {M : M ⊂ {F(source), E(target)}} Input: stepsize weight α where we assume a first-order dependence on previously aligned positions. To find the most likely parameter weights for the translation and alignment probabilities for the HMM-based alignments, we employ the EM algorithm via dynamic programming. Since HMMs have multiple local minima, we seed the HMM-based model probabilities with a better than random guess using IBM Model 1 (Brown et al., 1993) as is standard. IBM Model 1 is of the same form as the HMM-based model except it uses a uniform distribution instead of a first-order dependency. Although a series of more complex models are defined, IBM Models 2 to Model 6 (Brown et al., 1993; Och and Ney, 2003), researchers typically find that extracting phrase pairs or translation grammar rules using Model 1 and the HMM-based alignments results in equivalently high translation quality. Nevertheless, there is nothing in our approach which limits us to using just Model 1 and the HMM model. A high-level overview of the standard, batch EM algo</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>255--262</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="11492" citStr="Callison-Burch et al., 2005" startWordPosition="1854" endWordPosition="1857">990). Treating the entire corpus as a single string, a suffix array holds in lexicographical order (only) the starting index of each suffix of the string. After construction, since the corpus is now ordered, we can query the suffix array quickly using binary search to efficiently find all occurrences of a particular token or sequence of tokens. Then we can easily compute, on-the-fly, the statistics required such as translation probabilities for a given source phrase. Suffix arrays can also be compressed, which make them highly attractive structures for representing massive translation models (Callison-Burch et al., 2005; Lopez, 2008). We need to delete items if we wish to maintain Output: MLE 396 Figure 1: Streaming coverage conditions. In traditional batch based modeling the coverage of a trained model never changes. Unbounded coverage operates without any memory constraints so the model is able to continually add data from the input stream. Bounded coverage uses just a fixed window. constant space when processing unbounded streams. Standard suffix arrays are static, store a fixed corpus and do not support deletions. Nevertheless, a dynamic variant of the suffix array does support deletions as well as inser</context>
</contexts>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Chris Callison-Burch, Colin Bannard, and Josh Schroeder. 2005. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 255–262, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Cappe</author>
<author>Eric Moulines</author>
</authors>
<title>Online EM algorithm for latent data models.</title>
<date>2009</date>
<journal>Journal Of The Royal Statistical Society Series B,</journal>
<pages>71--593</pages>
<contexts>
<context position="5434" citStr="Cappe and Moulines, 2009" startWordPosition="803" endWordPosition="806">g data stream, however, the batch EM algorithm’s requirement that all data be available for each iteration becomes impractical since we do not have access to all n examples at once. Instead we receive examples from the input stream incrementally. For this reason online EM algorithms have been developed to update the probability model B� incrementally without needing to store and iterate through all the unlabeled training data repeatedly. Various online EM algorithms have been investigated (see Liang and Klein (2009) for an overview) but our focus is on the stepwise online EM (sOEM) algorithm (Cappe and Moulines, 2009). Instead of iterating over the full set of training examples, sOEM stochastically approximates the batch E-step and incorporates the information from the newly available streaming observations in steps. Each step is called a mini-batch and is comprised of one or more new examples encountered in the stream. Unlike in batch EM, in sOEM the expected counts are retained between EM iterations and not cleared. 4As the M-step can be computed in closed form we designate it in this work as B(S). Algorithm 1: Batch EM for Word Alignments Input: {F(source),E (target)} sentence-pairs Output: MLE BT over </context>
</contexts>
<marker>Cappe, Moulines, 2009</marker>
<rawString>Olivier Cappe and Eric Moulines. 2009. Online EM algorithm for latent data models. Journal Of The Royal Statistical Society Series B, 71:593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="17105" citStr="Chiang, 2007" startWordPosition="2779" endWordPosition="2780">as source and English again as target. For testing we held out a total of 22k sentences from 10 evenly spaced intervals in the input stream which divided the input stream into 10 epochs. Stream statistics for three example epochs are shown in Table 1. We held out 4.5k sentence pairs as development data to optimize the feature function weights using minimum error rate training (Och, 2003) and these weights were used by all models. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Note that nothing hinges on whether we used a syntax or a phrase-based system. We used a 5-gram, Kneser-Ney smoothed language model (LM) trained on the initial segment of Ep From–To Sent Pairs Source/Target 00 04/1996–12/2000 600k 15.0M/16.0M 03 02/2002–09/2002 70k 1.9M/2.0M 06 10/2003–03/2004 60k 1.6M/1.7M 10 03/2006–09/2006 73k 1.9M/2.0M Table 1: Date ranges, total sentence pairs, and source and target word counts encountered in the input stream for example epochs. Epoch 00 is baseline data that is also used as a seed corpus for the online models. the target side parallel data </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>39--1</pages>
<contexts>
<context position="3288" citStr="Dempster et al., 1977" startWordPosition="463" endWordPosition="466">space-bounds on our SMT system. We introduce a dynamic suffix array which allows us to add and delete parallel sentences, thereby maintaining bounded space despite processing a potentially high-rate input stream of unbounded length. Taken as a whole we show that online translation models operating within bounded space can perform as well as systems which are batch-based and have no space constraints thereby making our approach suitable for stream-based translation. 2 Stepwise Online EM The EM algorithm is a common way of inducing latent structure from unlabeled data in an unsupervised manner (Dempster et al., 1977). Given a set of unlabeled examples and an initial, often uniform guess at a probability distribution over the latent variables, the EM algorithm maximizes the marginal 394 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 394–402, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics log-likelihood of the examples by repeatedly computing the expectation of the conditional probability of the latent data with respect to the current distribution, and then maximizing the expectations over the observations into a </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="28402" citStr="Koehn and Schroeder (2007)" startWordPosition="4659" endWordPosition="4662">is country. On(ine: Our position is clear and well known: we are against the war and the occupation of Iraq by the United States and the United Kingdom, and we demand the immediate withdrawal of the occupying forces from this country. Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent sentences. In both examples we get longer matching phrases in the online translation compared to the static one. 5 Related Work 5.1 Translation Model Domain Adaptation Our work is related to domain adaptation for translation models. See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). Most techniques center around using mixtures of translation models. Once trained, these models generally never change. They therefore fall under the batch training regime. The focus of this work instead is on incremental retraining and also on supporting bounded memory consumption. Our experiments examine updating model parameters in a single domain over different periods in time. Naturally, we could also use domain adaptation techniques to further improve how we incorporate new samples. 5.2 Online EM for SMT For stepwise online EM for SMT models, the only pri</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
</authors>
<title>Streambased randomised language models for SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="26584" citStr="Levenberg and Osborne (2009)" startWordPosition="4369" endWordPosition="4372">ed in Table 4. We can see that increasing LM coverage is complimentary to adapting the TM with recent data. Comparing Tables 3 and 4, for the bounded condition, adapting only the TM achieved an absolute improvement of +1.24 BLEU over the static baseline for the final test point. We get another absolute gain of +1.08 BLEU by allowing the LM coverage to adapt as well. Using an online, adaptive model gives a total gain of +2.32 BLEU over a static baseline that does not adapt. 6Although we batch retrain the LMs we could use an online LM that incorporates new vocabulary from the input stream as in Levenberg and Osborne (2009). 400 Source: Die Kommission ist bereit, an der Schaffung eines solchen Rechtsrahmens unter Zugrundelegung von vier wesentlichen Prinzipien mitzuwirken. Reference: The commission is willing to cooperate in the creation of such a legal framework on the basis of four essential principles. Static: The commission is prepared, in the creation of a legal framework, taking account of four fundamental principles them. On(ine: The commission is prepared to participate in the creation of such a legal framework, based on four fundamental principles. Source: Unser Standpunkt ist klar und allseits bekannt:</context>
<context position="29570" citStr="Levenberg and Osborne (2009)" startWordPosition="4844" endWordPosition="4847"> for SMT For stepwise online EM for SMT models, the only prior work we are aware of is Liang and Klein (2009), where variations of online EM were experimented with on various NLP tasks including word alignments. They showed application of sOEM can produce quicker convergence compared to the batch EM algorithm. However, the model presented does not incorporate any unseen data, instead iterating over a static data set multiple times using sOEM. For Liang and Klein (2009) incremental retraining is simply an alternative way to use a fixed training set. 5.3 Streaming Language Models Recent work in Levenberg and Osborne (2009) presented a streaming LM that was capable of adapting to an unbounded monolingual input stream in constant space and time. The LM has the ability to add or delete n-grams (and their counts) based on feedback from the decoder after translation points. The model was tested in an SMT setting and results showed recent data benefited performance. However, adaptation was only to the LM and no tests were conducted on the TM. 6 Conclusion and Future Work We have presented an online EM approach for word alignments. We have shown that, for a SMT system, incorporating recent parallel data into a TM from</context>
</contexts>
<marker>Levenberg, Osborne, 2009</marker>
<rawString>Abby Levenberg and Miles Osborne. 2009. Streambased randomised language models for SMT. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: an open source toolkit for parsingbased machine translation.</title>
<date>2009</date>
<booktitle>In WMT09: Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16958" citStr="Li et al., 2009" startWordPosition="2757" endWordPosition="2760">ve were not limited to French-English, this time our parallel input stream was generated from the GermanEnglish language pair of Europarl with German as source and English again as target. For testing we held out a total of 22k sentences from 10 evenly spaced intervals in the input stream which divided the input stream into 10 epochs. Stream statistics for three example epochs are shown in Table 1. We held out 4.5k sentence pairs as development data to optimize the feature function weights using minimum error rate training (Och, 2003) and these weights were used by all models. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Note that nothing hinges on whether we used a syntax or a phrase-based system. We used a 5-gram, Kneser-Ney smoothed language model (LM) trained on the initial segment of Ep From–To Sent Pairs Source/Target 00 04/1996–12/2000 600k 15.0M/16.0M 03 02/2002–09/2002 70k 1.9M/2.0M 06 10/2003–03/2004 60k 1.6M/1.7M 10 03/2006–09/2006 73k 1.9M/2.0M Table 1: Date ranges, total sentence pairs, and source and target word counts encountered in the </context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: an open source toolkit for parsingbased machine translation. In WMT09: Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135–139, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>Online EM for unsupervised models.</title>
<date>2009</date>
<booktitle>In North American Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="5330" citStr="Liang and Klein (2009)" startWordPosition="785" endWordPosition="788"> count collection begins anew using the new distribution �Bt+1. When we move to processing an incoming data stream, however, the batch EM algorithm’s requirement that all data be available for each iteration becomes impractical since we do not have access to all n examples at once. Instead we receive examples from the input stream incrementally. For this reason online EM algorithms have been developed to update the probability model B� incrementally without needing to store and iterate through all the unlabeled training data repeatedly. Various online EM algorithms have been investigated (see Liang and Klein (2009) for an overview) but our focus is on the stepwise online EM (sOEM) algorithm (Cappe and Moulines, 2009). Instead of iterating over the full set of training examples, sOEM stochastically approximates the batch E-step and incorporates the information from the newly available streaming observations in steps. Each step is called a mini-batch and is comprised of one or more new examples encountered in the stream. Unlike in batch EM, in sOEM the expected counts are retained between EM iterations and not cleared. 4As the M-step can be computed in closed form we designate it in this work as B(S). Alg</context>
<context position="6886" citStr="Liang and Klein (2009)" startWordPosition="1055" endWordPosition="1058">e interpolate its expected count with the existing set of sufficient statistics. For each step we use a stepsize parameter y which mixes the information from the current example with information gathered from all previous examples. Over time the sOEM model probabilities begin to stabilize and are guaranteed to converge to a local maximum (Cappe and Moulines, 2009). Note that the stepsize y has a dependence on the current mini-batch. As we observe more incoming data the model’s current probability distribution is closer to the true distribution so the new observations receive less weight. From Liang and Klein (2009), if we set the stepsize as yt = (t + 2)−α, with 0.5 &lt; α &lt; 1, we can guarantee convergence in the limit as n -* oc. If we set α low, y weighs the newly observed statistics heavily whereas if y is low new observations are down-weighted. 2.2 Batch EM for Word Alignments Batch EM is used in statistical machine translation to estimate word alignment probabilities between parallel sentences. From these alignments, bilingual rules or phrase pairs can be extracted. Given a set of parallel sentence examples, IF, E}, with F the set of source sentences and E the corresponding target sentences, we want t</context>
<context position="29051" citStr="Liang and Klein (2009)" startWordPosition="4763" endWordPosition="4766">2009). Most techniques center around using mixtures of translation models. Once trained, these models generally never change. They therefore fall under the batch training regime. The focus of this work instead is on incremental retraining and also on supporting bounded memory consumption. Our experiments examine updating model parameters in a single domain over different periods in time. Naturally, we could also use domain adaptation techniques to further improve how we incorporate new samples. 5.2 Online EM for SMT For stepwise online EM for SMT models, the only prior work we are aware of is Liang and Klein (2009), where variations of online EM were experimented with on various NLP tasks including word alignments. They showed application of sOEM can produce quicker convergence compared to the batch EM algorithm. However, the model presented does not incorporate any unseen data, instead iterating over a static data set multiple times using sOEM. For Liang and Klein (2009) incremental retraining is simply an alternative way to use a fixed training set. 5.3 Streaming Language Models Recent work in Levenberg and Osborne (2009) presented a streaming LM that was capable of adapting to an unbounded monolingua</context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>Percy Liang and Dan Klein. 2009. Online EM for unsupervised models. In North American Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Tera-scale translation models via pattern matching.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>505--512</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="11506" citStr="Lopez, 2008" startWordPosition="1858" endWordPosition="1859">pus as a single string, a suffix array holds in lexicographical order (only) the starting index of each suffix of the string. After construction, since the corpus is now ordered, we can query the suffix array quickly using binary search to efficiently find all occurrences of a particular token or sequence of tokens. Then we can easily compute, on-the-fly, the statistics required such as translation probabilities for a given source phrase. Suffix arrays can also be compressed, which make them highly attractive structures for representing massive translation models (Callison-Burch et al., 2005; Lopez, 2008). We need to delete items if we wish to maintain Output: MLE 396 Figure 1: Streaming coverage conditions. In traditional batch based modeling the coverage of a trained model never changes. Unbounded coverage operates without any memory constraints so the model is able to continually add data from the input stream. Bounded coverage uses just a fixed window. constant space when processing unbounded streams. Standard suffix arrays are static, store a fixed corpus and do not support deletions. Nevertheless, a dynamic variant of the suffix array does support deletions as well as insertions and ther</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Tera-scale translation models via pattern matching. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 505–512, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udi Manber</author>
<author>Gene Myers</author>
</authors>
<title>Suffix arrays: A new method for on-line string searches.</title>
<date>1990</date>
<booktitle>In The First Annual ACM-SIAM Symposium on Dicrete Algorithms,</booktitle>
<pages>319--327</pages>
<contexts>
<context position="10869" citStr="Manber and Myers, 1990" startWordPosition="1755" endWordPosition="1758">/ stepsize 5 ← -ys + (1 − -y)5; // interpolate �Bt+1 ← �Bt(5) ; // M-step end end the newest sufficient statistics s� with our full count vector S using an interpolation parameter -y. The interpolation parameter -y has a dependency on how far along the input stream we are processing. 3 Dynamic Suffix Arrays So far we have shown how to incrementally retrain translation models. We now consider how we might bound the space we use for them when processing (potentially) unbounded streams of parallel data. Suffix arrays are space-efficient data structures for fast searching over large text strings (Manber and Myers, 1990). Treating the entire corpus as a single string, a suffix array holds in lexicographical order (only) the starting index of each suffix of the string. After construction, since the corpus is now ordered, we can query the suffix array quickly using binary search to efficiently find all occurrences of a particular token or sequence of tokens. Then we can easily compute, on-the-fly, the statistics required such as translation probabilities for a given source phrase. Suffix arrays can also be compressed, which make them highly attractive structures for representing massive translation models (Call</context>
</contexts>
<marker>Manber, Myers, 1990</marker>
<rawString>Udi Manber and Gene Myers. 1990. Suffix arrays: A new method for on-line string searches. In The First Annual ACM-SIAM Symposium on Dicrete Algorithms, pages 319–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8702" citStr="Och and Ney, 2003" startWordPosition="1379" endWordPosition="1382"> previously aligned positions. To find the most likely parameter weights for the translation and alignment probabilities for the HMM-based alignments, we employ the EM algorithm via dynamic programming. Since HMMs have multiple local minima, we seed the HMM-based model probabilities with a better than random guess using IBM Model 1 (Brown et al., 1993) as is standard. IBM Model 1 is of the same form as the HMM-based model except it uses a uniform distribution instead of a first-order dependency. Although a series of more complex models are defined, IBM Models 2 to Model 6 (Brown et al., 1993; Och and Ney, 2003), researchers typically find that extracting phrase pairs or translation grammar rules using Model 1 and the HMM-based alignments results in equivalently high translation quality. Nevertheless, there is nothing in our approach which limits us to using just Model 1 and the HMM model. A high-level overview of the standard, batch EM algorithm applied to HMM-based word alignment model is shown in Algorithm 1. 2.3 Stepwise EM for Word Alignments Application of sOEM to HMM and Model 1 based word aligning is straightforward. The process of collecting the counts over the expected conditional probabili</context>
<context position="18039" citStr="Och and Ney, 2003" startWordPosition="2931" endWordPosition="2934">M/1.7M 10 03/2006–09/2006 73k 1.9M/2.0M Table 1: Date ranges, total sentence pairs, and source and target word counts encountered in the input stream for example epochs. Epoch 00 is baseline data that is also used as a seed corpus for the online models. the target side parallel data used in the first baseline as described further in the next subsection. As our initial experiments aim to isolate the effect of changes to the TM on overall translation system performance, our in-domain LM remains static for every decoding run reported below until indicated. We used the open-source toolkit GIZA++ (Och and Ney, 2003) for all word alignments. For the online adaptation experiments we modified Model 1 and the HMM model in GIZA++ to use the sOEM algorithm. Batch baselines were aligned using the standard version of GIZA++. We ran the batch and incremental versions of Model 1 and HMM for the same number of iterations each in both directions. 4.3 Time and Space Bounds For both batch and sOEM we ran a number of experiments listed below corresponding to the different training scenarios diagrammed in Figure 1. 1. Static: We used the first half of the input stream, approximately 600k sentences and 15/16 million sour</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16882" citStr="Och, 2003" startWordPosition="2744" endWordPosition="2745"> fixed sized sample of it). To ensure the recency results reported above were not limited to French-English, this time our parallel input stream was generated from the GermanEnglish language pair of Europarl with German as source and English again as target. For testing we held out a total of 22k sentences from 10 evenly spaced intervals in the input stream which divided the input stream into 10 epochs. Stream statistics for three example epochs are shown in Table 1. We held out 4.5k sentence pairs as development data to optimize the feature function weights using minimum error rate training (Och, 2003) and these weights were used by all models. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Note that nothing hinges on whether we used a syntax or a phrase-based system. We used a 5-gram, Kneser-Ney smoothed language model (LM) trained on the initial segment of Ep From–To Sent Pairs Source/Target 00 04/1996–12/2000 600k 15.0M/16.0M 03 02/2002–09/2002 70k 1.9M/2.0M 06 10/2003–03/2004 60k 1.6M/1.7M 10 03/2006–09/2006 73k 1.9M/2.0M Table 1: Date ranges,</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="15410" citStr="Papineni et al., 2001" startWordPosition="2503" endWordPosition="2506">ailable at http://www.statmt.org/europarl Test Points Static input stream model coverage Unb*unded Test Points input stream epoch 1 epoch 2 model coverage B*unded Test Points model coverage epoch 2 input stream sfiding windows 397 on the TM all other factors of the SMT pipeline remained constant including the language model and the feature weights. Hence, the only change from the static baseline to the epochs performance was the TM data which was based on recency. Note that at this stage we did not use any incremental retraining. Results are shown in Figure 2 as the differences in BLEU score (Papineni et al., 2001) between the baseline TM versus the translation models trained on material chronologically closer to the given test point. The consistently positive deltas in BLEU scores between the model that is never retrained and the models that are retrained show that we achieve a higher translation performance when using more upto-date TMs that incorporate recent sentence pairs. As the chronological distance between the initial, static model and the retrained models increases, we see ever-increasing differences in translation performance. This underlines the need to retrain translation models with timely</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Optimizing lexical and n-gram coverage via judicious use of linguistic data. In In</title>
<date>1995</date>
<booktitle>Proc. European Conf. on Speech Technology,</booktitle>
<pages>1763--1766</pages>
<marker>Rosenfeld, 1995</marker>
<rawString>Ronald Rosenfeld. 1995. Optimizing lexical and n-gram coverage via judicious use of linguistic data. In In Proc. European Conf. on Speech Technology, pages 1763–1766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Lecroq Mika¨el Salson</author>
<author>Martine L´eonard</author>
<author>Laurent Mouchard</author>
</authors>
<title>Dynamic extended suffix arrays. Journal ofDiscrete Algorithms,</title>
<date>2009</date>
<marker>Mika¨el Salson, L´eonard, Mouchard, 2009</marker>
<rawString>Mika¨el Salson, Thierry Lecroq, Martine L´eonard, and Laurent Mouchard. 2009. Dynamic extended suffix arrays. Journal ofDiscrete Algorithms, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>836--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7817" citStr="Vogel et al., 1996" startWordPosition="1223" endWordPosition="1226">nslation to estimate word alignment probabilities between parallel sentences. From these alignments, bilingual rules or phrase pairs can be extracted. Given a set of parallel sentence examples, IF, E}, with F the set of source sentences and E the corresponding target sentences, we want to find the latent alignments a for a sentence pair (f, e) E IF, E} that defines the most probable correspondence between words fj and ez such that aj = i. We can induce these alignments using an HMM-based alignment model where the probability of alignment aj is dependent only on the previous alignment at aj−1 (Vogel et al., 1996). 395 Algorithm 2: sOEM Algorithm for Word Alignments We can write |f| Pr(f, a |e) = � H p(aj |aj−1, |e|) · p(fj |ea,) a′∈a j=1 Input: mini-batches of sentence pairs {M : M ⊂ {F(source), E(target)}} Input: stepsize weight α where we assume a first-order dependence on previously aligned positions. To find the most likely parameter weights for the translation and alignment probabilities for the HMM-based alignments, we employ the EM algorithm via dynamic programming. Since HMMs have multiple local minima, we seed the HMM-based model probabilities with a better than random guess using IBM Model 1</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics, pages 836–841, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>