<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001129">
<title confidence="0.99861">
Quadratic-Time Dependency Parsing for Machine Translation
</title>
<author confidence="0.995124">
Michel Galley
</author>
<affiliation confidence="0.9844315">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.781694">
Stanford, CA 94305-9020
</address>
<email confidence="0.998976">
mgalley@cs.stanford.edu
</email>
<sectionHeader confidence="0.993898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993298421052632">
Efficiency is a prime concern in syntactic MT de-
coding, yet significant developments in statisti-
cal parsing with respect to asymptotic efficiency
haven’t yet been explored in MT. Recently,
McDonald et al. (2005b) formalized dependency
parsing as a maximum spanning tree (MST) prob-
lem, which can be solved in quadratic time relative
to the length of the sentence. They show that MST
parsing is almost as accurate as cubic-time depen-
dency parsing in the case of English, and that it
is more accurate with free word order languages.
This paper applies MST parsing to MT, and de-
scribes how it can be integrated into a phrase-based
decoder to compute dependency language model
scores. Our results show that augmenting a state-of-
the-art phrase-based system with this dependency
language model leads to significant improvements
in TER (0.92%) and BLEU (0.45%) scores on five
NIST Chinese-English evaluation test sets.
</bodyText>
<sectionHeader confidence="0.998856" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958428571429">
Hierarchical approaches to machine translation
have proven increasingly successful in recent
years (Chiang, 2005; Marcu et al., 2006; Shen
et al., 2008), and often outperform phrase-based
systems (Och and Ney, 2004; Koehn et al., 2003)
on target-language fluency and adequacy. How-
ever, their benefits generally come with high com-
putational costs, particularly when chart parsing,
such as CKY, is integrated with language models
of high orders (Wu, 1996). Indeed, synchronous
CFG parsing with m-grams runs in O(n3m) time,
where n is the length of the sentence.1
Furthermore, synchronous CFG approaches of-
ten only marginally outperform the most com-
</bodyText>
<footnote confidence="0.728414">
1The algorithmic complexity of (Wu, 1996) is
O(n3+4(m−1)), though Huang et al. (2005) present a
more efficient factorization inspired by (Eisner and Satta,
1999) that yields an overall complexity of O(n3+3(m−1)),
i.e., O(n3m). In comparison, phrase-based decoding can run
in linear time if a distortion limit is imposed. Of course, this
comparison holds only for approximate algorithms. Since
exact MT decoding is NP complete (Knight, 1999), there is
no exact search algorithm for either phrase-based or syntactic
MT that runs in polynomial time (unless P = NP).
</footnote>
<note confidence="0.45174525">
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9010
</note>
<email confidence="0.986867">
manning@cs.stanford.edu
</email>
<bodyText confidence="0.999894">
petitive phrase-based systems in large-scale ex-
periments such as NIST evaluations.2 This lack
of significant difference may not be completely
surprising. Indeed, researchers have shown that
gigantic language models are key to state-of-
the-art performance (Brants et al., 2007), and
the ability of phrase-based decoders to handle
large-size, high-order language models with no
consequence on asymptotic running time during
decoding presents a compelling advantage over
CKY decoders, whose time complexity grows pro-
hibitively large with higher-order language mod-
els.
While context-free decoding algorithms (CKY,
Earley, etc.) may sometimes appear too computa-
tionally expensive for high-end statistical machine
translation, there are many alternative parsing al-
gorithms that have seldom been explored in the
machine translation literature. The parsing liter-
ature presents faster alternatives for both phrase-
structure and dependency trees, e.g., O(n) shift-
reduce parsers and variants ((Ratnaparkhi, 1997;
Nivre, 2003), inter alia). While deterministic
parsers are often deemed inadequate for dealing
with ambiguities of natural language, highly accu-
rate O(n2) algorithms exist in the case of depen-
dency parsing. Building upon the theoretical work
of (Chu and Liu, 1965; Edmonds, 1967), McDon-
ald et al. (2005b) present a quadratic-time depen-
dency parsing algorithm that is just 0.7% less ac-
curate than “full-fledged” chart parsing (which, in
the case of dependency parsing, runs in time O(n3)
(Eisner, 1996)).
In this paper, we show how to exploit syn-
tactic dependency structure for better machine
translation, under the constraint that the depen-
</bodyText>
<footnote confidence="0.997316428571429">
2Results of the 2008 NIST Open MT evaluation
(http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/
mt08_official_results_v0.html) reveal that, while many of
the best systems in the Chinese-English and Arabic-English
tasks incorporate synchronous CFG models, score differ-
ences with the best phrase-based system were insignificantly
small.
</footnote>
<page confidence="0.949459">
773
</page>
<note confidence="0.9996165">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 773–781,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999890428571429">
dency structure is built as a by-product of phrase-
based decoding, without reliance on a dynamic-
programming or chart parsing algorithm such as
CKY or Earley. Adapting the approach of Mc-
Donald et al. (2005b) for machine translation, we
incrementally build dependency structure left-to-
right in time O(n2) during decoding. Most in-
terestingly, the time complexity of non-projective
dependency parsing remains quadratic as the or-
der of the language model increases. This pro-
vides a compelling advantage over previous de-
pendency language models for MT (Shen et al.,
2008), which use a 5-gram LM only during rerank-
ing. In our experiments, we build a competi-
tive baseline (Koehn et al., 2007) incorporating a
5-gram LM trained on a large part of Gigaword
and show that our dependency language model
provides improvements on five different test sets,
with an overall gain of 0.92 in TER and 0.45 in
BLEU scores. These results are found to be statis-
tically very significant (p :5 .01).
</bodyText>
<sectionHeader confidence="0.9217435" genericHeader="method">
2 Dependency parsing for machine
translation
</sectionHeader>
<bodyText confidence="0.999840928571428">
In this section, we review dependency parsing for-
mulated as a maximum spanning tree problem
(McDonald et al., 2005b), which can be solved in
quadratic time, and then present its adaptation and
novel application to phrase-based decoding.
Dependency models have recently gained con-
siderable interest in many NLP applications, in-
cluding machine translation (Ding and Palmer,
2005; Quirk et al., 2005; Shen et al., 2008). De-
pendency structure provides several compelling
advantages compared to other syntactic represen-
tations. First, dependency links are close to the se-
mantic relationships, which are more likely to be
consistent across languages. Indeed, Fox (2002)
found inter-lingual phrasal cohesion to be greater
than for a CFG when using a dependency rep-
resentation, for which she found only 12.6% of
head crossings and 9.2% modifier crossings. Sec-
ond, dependency trees contain exactly one node
per word, which contributes to cutting down the
search space during parsing: indeed, the task of
the parser is merely to connect existing nodes
rather than hypothesizing new ones. Finally, de-
pendency models are more flexible and account
for (non-projective) head-modifier relations that
CFG models fail to represent adequately, which
is problematic with certain types of grammatical
constructions and with free word order languages,
</bodyText>
<figure confidence="0.494280333333333">
who do you think they hired ?
WP VB PRP VB PRP VBD .
1 2 3 4 5 6 7
</figure>
<figureCaption confidence="0.995717333333333">
Figure 1: A dependency tree with directed edges going from
heads to modifiers. The edge between who and hired causes
this tree to be non-projective. Such a head-modifier relation-
ship is difficult to represent with a CFG, since all words di-
rectly or indirectly headed by hired (i.e., who, think, they, and
hired) do not constitute a contiguous sequence of words.
</figureCaption>
<bodyText confidence="0.999945921052631">
as we will see later in this section.
The most standardly used algorithm for parsing
with dependency grammars is presented in (Eis-
ner, 1996; Eisner and Satta, 1999). It runs in time
O(n3), where n is the length of the sentence. Their
algorithm exploits the special properties of depen-
dency trees to reduce the worst-case complexity of
bilexical parsing, which otherwise requires O(n4)
for bilexical constituency-based parsing. While it
seems difficult to improve the asymptotic running
time of the Eisner algorithm beyond what is pre-
sented in (Eisner and Satta, 1999), McDonald et
al. (2005b) show O(n2)-time parsing is possible if
trees are not required to be projective. This re-
laxation entails that dependencies may cross each
other rather than being required to be nested, as
shown in Fig. 1. More formally, a non-projective
tree is any tree that does not satisfy the following
definition of a projective tree:
Definition. Let x = x1 ···xn be an input sentence,
and let y be a rooted tree represented as a set
in which each element (i, j) E y is an ordered
pair of word indices of x that defines a depen-
dency relation between a head xi and a modifier
xj. By definition, the tree y is said to be projec-
tive if each dependency (i, j) satisfies the follow-
ing property: each word in xi+1 ···xj_1 (if i &lt; j)
or in xj+1 ···xi_1 (if j &lt; i) is a descendent of head
word xi.
This relaxation is key to computational effi-
ciency, since the parser does not need to keep
track of whether dependencies assemble into con-
tiguous spans. It is also linguistically desirable
in the case of free word order languages such as
Czech, Dutch, and German. Non-projective de-
pendency structures are sometimes even needed
for languages like English, e.g., in the case of the
wh-movement shown in Fig. 1. For languages
</bodyText>
<figure confidence="0.973804">
&lt;root&gt;
&lt;root&gt;
0
</figure>
<page confidence="0.996882">
774
</page>
<bodyText confidence="0.999925785714286">
with relatively rigid word order such as English,
there may be some concern that searching the
space of non-projective dependency trees, which
is considerably larger than the space of projective
dependency trees, would yield poor performance.
That is not the case: dependency accuracy for non-
projective parsing is 90.2% for English (McDon-
ald et al., 2005b), only 0.7% lower than a projec-
tive parser (McDonald et al., 2005a) that uses the
same set of features and learning algorithm. In the
case of dependency parsing for Czech, (McDonald
et al., 2005b) even outperforms projective parsing,
and was one of the top systems in the CoNLL-06
shared task in multilingual dependency parsing.
</bodyText>
<subsectionHeader confidence="0.966289">
2.1 O(n2)-time dependency parsing for MT
</subsectionHeader>
<bodyText confidence="0.999826833333333">
We now formalize weighted non-projective de-
pendency parsing similarly to (McDonald et al.,
2005b) and then describe a modified and more ef-
ficient version that can be integrated into a phrase-
based decoder.
Given the single-head constraint, parsing an in-
put sentence x = (x0,x1,••• ,xn) is reduced to la-
beling each word xj with an index i identifying its
head word xi. We include the dummy root symbol
x0 = (root) so that each word can be a modifier.
We score each dependency relation using a stan-
dard linear model
</bodyText>
<equation confidence="0.743658">
s(i, j) = X • f(i, j) (1)
</equation>
<bodyText confidence="0.999990333333333">
whose weight vector X is trained using
MIRA (Crammer and Singer, 2003) to opti-
mize dependency parsing accuracy (McDonald et
al., 2005a). As is commonly the case in statistical
parsing, the score of the full tree is decomposed
as the sum of the score of all edges:
</bodyText>
<equation confidence="0.979464">
s(x,y) = E X - f(i, j) (2)
(i,j)cy
</equation>
<bodyText confidence="0.999371045454546">
When there is no need to ensure projectivity, one
can independently select the highest scoring edge
(i, j) for each modifier xj, yet we generally want to
ensure that the resulting structure is a tree, i.e., that
it does not contain any circular dependencies. This
optimization problem is a known instance of the
maximum spanning tree (MST) problem. In our
case, the graph is directed—indeed, the equality
s(i, j) = s(j,i) is generally not true and would be
linguistically aberrant—so the problem constitutes
an instance of the less-known MST problem for
directed graphs. This problem is solved with the
Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu,
1965; Edmonds, 1967).
Formally, we represent the graph G = (V,E)
with a vertex set V = x = {x0,••• ,xnI and a set
of directed edges E = [0,n] x [1,n], in which each
edge (i, j), representing the dependency xi —* xj,
is assigned a score s(i, j). Finding the spanning
tree y C E rooted at x0 that maximizes s(x,y) as
defined in Equation 2 has a straightforward solu-
tion in O(n2 log(n)) time for dense graphs such as
G, though Tarjan (1977) shows that the problem
can be solved in O(n2). Hence, non-projective
dependency parsing is solved in quadratic time.
The main idea behind the CLE algorithm is to
first greedily select for each word xj the incom-
ing edge (i, j) with highest score, then to succes-
sively repeat the following two steps: (a) identify
a loop in the graph, and if there is none, halt; (b)
contract the loop into a single vertex, and update
scores for edges coming in and out of the loop.
Once all loops have been eliminated, the algorithm
maps back the maximum spanning tree of the con-
tracted graph onto the original graph G, and it can
be shown that this yields a spanning tree that is op-
timal with respect to G and s (Georgiadis, 2003).
The greedy approach of selecting the highest
scoring edge (i, j) for each modifier xj can
easily be applied left-to-right during phrase-based
decoding, which proceeds in the same order.
For each hypothesis expansion, our decoder
generates the following information for the new
hypothesis h:
</bodyText>
<listItem confidence="0.999989666666667">
• a partial translation x;
• a coverage set of input words c;
• a translation score 6.
</listItem>
<bodyText confidence="0.990204333333333">
In the case of non-projective dependency parsing,
we need to maintain additional information for
each word xj of the partial translation x:
</bodyText>
<listItem confidence="0.9999475">
• a predicted POS tag tj;
• a dependency score sj.
</listItem>
<bodyText confidence="0.9993695">
Dependency scores sj are initialized to ��.
Each time a new word is added to a partial hy-
pothesis, the decoder executes the routine shown
in Table 1. To avoid cluttering the pseudo-code,
we make here the simplifying assumption that
each hypothesis expansion adds exactly one word,
though the real implementation supports the case
of phrases of any length. Line 3 determines
whether the translation hypothesis is complete, in
which case it explicitly builds the graph G and
</bodyText>
<page confidence="0.992699">
775
</page>
<bodyText confidence="0.41191">
Decoding: hypothesis expansion step.
</bodyText>
<listItem confidence="0.974257222222222">
1. Inferer generates new hypothesis h = (x,c,a)
2. j ← |x|−1
3. tj ← tagger(xj−3,··· ,xj)
4. if complete(c)
5. Chu-Liu-Edmonds(h)
6. else
7. for i = 1 to j
8. sj = max(sj,s(i, j))
9. si = max(si,s(j,i))
</listItem>
<tableCaption confidence="0.998121">
Table 1: Hypothesis expansion with dependency scoring.
</tableCaption>
<bodyText confidence="0.999948304347826">
finds the maximum spanning tree. Note that it is
impractical to identify loops each time a new word
is added to a translation hypothesis, since this re-
quires explicitly storing the dense graph G, which
would require an O(n2) copy operation during
each hypothesis expansion; this would of course
increase time and space complexity (the max op-
eration in lines 8 and 9 only keeps the current best
scoring edges). If there is any loop, the depen-
dency score is adjusted in the last hypothesis ex-
pansion. In practice, we delay the computation of
dependency scores involving word xj until tag tj+1
is generated, since dependency parsing accuracy is
particularly low (−0.8%) when the next tag is un-
known.
We found that dependency scores with or with-
out loop elimination are generally close and highly
correlated, and that MT performance without fi-
nal loop removal was about the same (generally
less than 0.2% BLEU). While it seems that loopy
graphs are undesirable when the goal is to obtain a
syntactic analysis, that is not necessarily the case
when one just needs a language modeling score.
</bodyText>
<subsectionHeader confidence="0.798972">
2.2 Features for dependency parsing
</subsectionHeader>
<bodyText confidence="0.999470285714286">
In our experiments, we use sets of features that are
similar to the ones used in the McDonald parser,
though we make a key modification that yields an
asymptotic speedup that ensures a genuine O(n2)
running time.
The three feature sets that were used in our ex-
periments are shown in Table 2. We write h-word,
h-pos, m-word, m-pos to refer to head and modi-
fier words and POS tags, and append a numerical
value to shift the word offset either to the left or to
the right (e.g., h-pos+1 is the POS to the right of
the head word). We use the symbol ∧ to represent
feature conjunctions. Each feature in the table has
a distinct identifier, so that, e.g., the POS features
</bodyText>
<table confidence="0.99988345">
Unigram features:
h-word, h-pos, h-word ∧ h-pos,
m-word, m-pos, m-word ∧ m-pos
Bigram features:
h-word ∧ m-word, h-pos ∧ m-pos,
h-word ∧ h-pos ∧ m-word, h-word ∧ h-pos ∧ m-pos,
m-word ∧ m-pos ∧ h-word, m-word ∧ m-pos ∧ h-pos,
h-word ∧ h-pos ∧ m-word ∧ m-pos
Adjacent POS features:
h-pos ∧ h-pos+1 ∧ m-pos−1 ∧ m-pos,
h-pos ∧ h-pos+1 ∧ m-pos ∧ m-pos+1,
h-pos−1 ∧ h-pos ∧ m-pos−1 ∧ m-pos,
h-pos−1 ∧ h-pos ∧ m-pos ∧ m-pos+1
In-between POS features:
ifi&lt;j:
h-pos ∧ h-pos+k ∧ m-pos k ∈ [i,min(i+5, j)]
h-pos ∧ m-pos−k ∧ m-pos k ∈ [max(i, j− 5), j]
ifi&gt; j:
m-pos ∧ m-pos+k ∧ h-pos k ∈ [ j,min(j + 5,i)]
m-pos ∧ h-pos−k ∧ h-pos k ∈ [max(j,i− 5),i]
</table>
<tableCaption confidence="0.981048666666667">
Table 2: Features for dependency parsing. It is quite similar
to the McDonald (2005a) feature set, except that it does not
include the set of all POS tags that appear between each can-
didate head-modifier pair (i, j). This modification is essential
in order to make our parser run in true O(n2) time, as opposed
to (McDonald et al., 2005b).
</tableCaption>
<table confidence="0.999875666666667">
SOURCE IDS GENRE SENTENCES
English CTB 050–325 newswire 3027
English ATB all newswire 13628
OntoNotes all broadcast news 14056
WSJ 02–21 financial news 39832
Total 70543
</table>
<tableCaption confidence="0.9530625">
Table 3: Characteristics of our training data. The second col-
umn identifies documents and sections selected for training.
</tableCaption>
<bodyText confidence="0.992030388888889">
h-pos are all distinct from m-pos features.3
The primary difference between our feature sets
and the ones of McDonald et al. is that their set of
“in between POS features” includes the set of all
tags appearing between each pair of words. Ex-
tracting all these tags takes time O(n) for any arbi-
trary pair (i, j). Since i and j are both free vari-
ables, feature computation in (McDonald et al.,
2005b) takes time O(n3), even though parsing it-
self takes O(n2) time. To make our parser gen-
uinely O(n2), we modified the set of in-between
POS features in two ways. First, we restrict ex-
traction of in-between POS tags to those words
that appear within a window of five words rel-
ative to either the head or the modifier. While
this change alone ensures that feature extraction is
now O(1) for each word pair, this causes a fairly
high drop of performance (dependency accuracy
</bodyText>
<footnote confidence="0.8327864">
3In addition to these basic features, we follow McDonald
in conjoining most features with two extra pieces of infor-
mation: a boolean variable indicating whether the modifier
attaches to the left or to the right, and the binned distance
between the two words.
</footnote>
<page confidence="0.976097">
776
</page>
<table confidence="0.999704">
ALGORITHM TIME SETUP TRAINING TESTING ACCURACY
Projective O(n3) Parsing WSJ(02-21) WSJ(23) 90.60
Chu-Liu-Edmonds O(n3) Parsing WSJ(02-21) WSJ(23) 89.64
Chu-Liu-Edmonds O(n2) Parsing WSJ(02-21) WSJ(23) 89.32
Local classifier Parsing WSJ(02-21) WSJ(23) 89.15
Projective MT CTB(050-325) CTB(001-049) 86.33
Chu-Liu-Edmonds O(n3) MT CTB(050-325) CTB(001-049) 85.68
Chu-Liu-Edmonds O(n2) MT CTB(050-325) CTB(001-049) 85.43
Local classifier MT CTB(050-325) CTB(001-049) 85.22
Projective MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 87.40(**)
Chu-Liu-Edmonds O(n3) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.79
Chu-Liu-Edmonds O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.45(*)
Local classifier O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.29
</table>
<tableCaption confidence="0.997624666666667">
Table 4: Dependency parsing experiments on test sentences of any length. The projective parsing algorithm is the one imple-
mented as in (McDonald et al., 2005a), which is known as one of the top performing dependency parsers for English. The O(n3)
non-projective parser of (McDonald et al., 2005b) is slightly more accurate than our version, though ours runs in O(n2) time.
“Local classifier” refers to non-projective dependency parsing without removing loops as a post-processing step. The result
marked with (*) identifies the parser used for our MT experiments, which is only about 1% less accurate than a state-of-the-art
dependency parser (**).
</tableCaption>
<bodyText confidence="0.997954333333333">
on our test was down 0.9%). To make our gen-
uinely O(n2) parser almost as accurate as the non-
projective parser of McDonald et al., we conjoin
each in-between POS with its position relative to
(i, j). This relatively simple change reduces the
drop in accuracy to only 0.34%.4
</bodyText>
<sectionHeader confidence="0.987148" genericHeader="method">
3 Dependency parsing experiments
</sectionHeader>
<bodyText confidence="0.995866412698413">
In this section, we compare the performance of
our parsing model to the ones of McDonald et al.
Since our MT test sets include newswire, web, and
audio, we trained our parser on different genres.
Our training data includes newswire from the En-
glish translation treebank (LDC2007T02) and the
English-Arabic Treebank (LDC2006T10), which
are respectively translations of sections of the Chi-
nese treebank (CTB) and Arabic treebank (ATB).
We also trained the parser on the broadcast-
news treebank available in the OntoNotes corpus
(LDC2008T04), and added sections 02-21 of the
WSJ Penn treebank. Documents 001-040 of the
English CTB data were set aside to constitute a
test set for newswire texts. Our other test set is
the standard Section 23 of the Penn treebank. The
splits and amounts of data used for training are dis-
played in Table 3.
Parsing experiments are shown in Table 4. We
4We need to mention some practical considerations that
make feature computation fast enough for MT. Most features
are precomputed before actual decoding. All target-language
words to appear during beam search can be determined in ad-
vance, and all their unigram feature scores are precomputed.
For features conditioned on both head and modifier, scores
are cached whenever possible. The only features that are not
cached are the ones that include contextual POS tags, since
their miss rate is relatively high.
distinguish two experimental conditions: Parsing
and MT. For Parsing, sentences are cased and tok-
enization abides to the PTB segmentation as used
in the Penn treebank version 3. For the MT set-
ting, texts are all lower case, and tokenization
was changed to improve machine translation (e.g.,
most hyphenated words were split). For this set-
ting, we also had to harmonize the four treebanks.
The most crucial modification was to add NP in-
ternal bracketing to the WSJ (Vadas and Curran,
2007), since the three other treebanks contain that
information. Treebanks were also transformed to
be consistent with MT tokenization. We evaluate
MT parsing models on CTB rather than on WSJ,
since CTB contains newswire and is thus more
representative of MT evaluation conditions.
To obtain part-of-speech tags, we use a
state-of-the-art maximum-entropy (CMM) tagger
(Toutanova et al., 2003). In the Parsing setting, we
use its best configuration, which reaches a tagging
accuracy of 97.25% on standard WSJ test data. In
the MT setting, we need to use a less effective tag-
ger, since we cannot afford to perform Viterbi in-
ference as a by-product of phrase-based decoding.
Hence, we use a simpler tagging model that as-
signs tag ti to word xi by only using features of
words xi_3 ···xi, and that does not condition any
decision based on any preceding or next tags (ti_1,
etc.). Its performance is 95.02% on the WSJ, and
95.30% on the English CTB. Additional experi-
ments reveal two main contributing factors to this
drop on WSJ: tagging uncased texts reduces tag-
ging accuracy by about 1%, and using only word-
based features further reduces it by 0.6%.
Table 4 shows that the accuracy of our truly
</bodyText>
<page confidence="0.991341">
777
</page>
<bodyText confidence="0.996416678571428">
O(n2) parser is only .25% to .34% worse than
the O(n3) implementation of (McDonald et al.,
2005b).5 Compared to the state-of-the-art projec-
tive parser as implemented in (McDonald et al.,
2005a), performance is 1.28% lower on WSJ, but
only 0.95% when training on all our available data
and using the MT setting. Overall, we believe that
the drop of performance is a reasonable price to
pay considering the computational constraints im-
posed by integrating the dependency parser into an
MT decoder.
The table also shows a gain of more than 1% in
dependency accuracy by adding ATB, OntoNotes,
and WSJ to the English CTB training set. The
four sources were assigned non-uniform weights:
we set the weight of the CTB data to be 10 times
larger than the other corpora, which seems to work
best in our parsing experiments. While this im-
provement of 1% may seem relatively small con-
sidering that the amount of training data is more
than 20 times larger in the latter case, it is quite
consistent with previous findings in domain adap-
tation, which is known to be a difficult task. For
example, (Daume III, 2007) shows that training a
learning algorithm on the weighted union of dif-
ferent data sets (which is basically what we did)
performs almost as well as more involved domain
adaptation approaches.
</bodyText>
<sectionHeader confidence="0.991415" genericHeader="method">
4 Machine translation experiments
</sectionHeader>
<bodyText confidence="0.999655380281691">
In our experiments, we use a re-implementation
of the Moses phrase-based decoder (Koehn et
al., 2007). We use the standard features imple-
mented almost exactly as in Moses: four trans-
lation features (phrase-based translation probabil-
ities and lexically-weighted probabilities), word
penalty, phrase penalty, linear distortion, and lan-
guage model score. We also incorporated the lex-
icalized reordering features of Moses, in order to
experiment with a baseline that is stronger than the
default Moses configuration.
The language pair for our experiments is
Chinese-to-English. The training data consists of
about 28 million English words and 23.3 million
5Note that our results on WSJ are not exactly the same
as those reported in (McDonald et al., 2005b), since we used
slightly different head finding rules. To extract dependencies
from treebanks, we used the LTH Penn Converter (http://
nlp.cs.lth.se/pennconverter/), which extracts
dependencies that are almost identical to those used for the
CoNLL-2008 Shared Task. We constrain the converter not to
use functional tags found in the treebanks, in order to make it
possible to use automatically parsed texts (i.e., perform self-
training) in future work.
Chinese words drawn from various news parallel
corpora distributed by the Linguistic Data Con-
sortium (LDC). In order to provide experiments
comparable to previous work, we used the same
corpora as (Wang et al., 2007): LDC2002E18,
LDC2003E07, LDC2003E14, LDC2005E83,
LDC2005T06, LDC2006E26, LDC2006E8, and
LDC2006G05. Chinese words were automatically
segmented with a conditional random field (CRF)
classifier (Chang et al., 2008) that conforms to the
Chinese Treebank (CTB) standard.
In order to train a competitive baseline given our
computational resources, we built a large 5-gram
language model using the Xinhua and AFP sec-
tions of the Gigaword corpus (LDC2007T40) in
addition to the target side of the parallel data.
This data represents a total of about 700 mil-
lion words. We manually removed documents of
Gigaword that were released during periods that
overlap with those of our development and test
sets. The language model was smoothed with the
modified Kneser-Ney algorithm as implemented
in (Stolcke, 2002), and we only kept 4-grams and
5-grams that occurred at least three times in the
training data.6
For tuning and testing, we use the official NIST
MT evaluation data for Chinese from 2002 to 2008
(MT02 to MT08), which all have four English ref-
erences for each input sentence. We used the 1082
sentences of MT05 for tuning and all other sets for
testing. Parameter tuning was done with minimum
error rate training (Och, 2003), which was used
to maximize BLEU (Papineni et al., 2001). Since
MERT is prone to search errors, especially with
large numbers of parameters, we ran each tuning
experiment three times with different initial condi-
tions. We used n-best lists of size 200 and a beam
size of 200. In the final evaluations, we report re-
sults using both TER (Snover et al., 2006) and the
original BLEU metric as described in (Papineni et
al., 2001). All our evaluations are performed on
uncased texts.
The results for our translation experiments are
shown in Table 5. We compared two systems: one
with the set of features described earlier in this
section. The second system incorporates one ad-
ditional feature, which is the dependency language
</bodyText>
<footnote confidence="0.993878833333333">
6We found that sections of Gigaword other than Xinhua
and AFP provide almost no improvement in our experiments.
By leaving aside the other sections, we were able to increase
the order of the language model to 5-gram and perform rela-
tively little pruning. This LM required 16GB of RAM during
training.
</footnote>
<page confidence="0.977771">
778
</page>
<table confidence="0.9836435">
BLEU[%]
DEP. LM MT05 (tune) MT02 MT03 MT04 MT06 MT08
33.42 33.38 33.13 36.21 32.16
34.19 (+.77**) 33.85 (+.47) 33.73 (+.6*) 36.67 (+.46*) 32.84 (+.68**)
no 24.83
yes 24.91 (+.08)
TER[%]
DEP. LM MT05 (tune) MT02 MT03 MT04 MT06 MT08
no 57.41 58.07 57.32 56.09 57.24 61.96
yes 56.27 (−1.14**) 57.15 (− .92**) 56.09 (− 1.23**) 55.30 (− .79**) 56.05 (− 1.19**) 61.41 (−.55*)
MT05 (tune) MT02 MT03 MT04 MT06 MT08
Sentences 1082 878 919 1788 1664 1357
</table>
<tableCaption confidence="0.885676">
Table 5: MT experiments with and without a dependency language model. We use randomization tests (Riezler and Maxwell,
2005) to determine significance: differences marked with a (*) are significant at the p &lt; .05 level, and those marked as (**) are
significant at the p &lt; .01 level.
</tableCaption>
<bodyText confidence="0.999785435897436">
model score computed with the dependency pars-
ing algorithm described in Section 2. We used
the dependency model trained on the English CTB
and ATB treebank, WSJ, and OntoNotes.
We see that the Moses decoder with integrated
dependency language model systematically out-
performs the Moses baseline. For BLEU evalu-
ations, differences are significant in four out of
six cases, and in the case of TER, all differences
are significant. Regarding the small difference in
BLEU scores on MT08, we would like to point
out that tuning on MT05 and testing on MT08
had a rather adverse effect with respect to trans-
lation length: while the two systems are rela-
tively close in terms of BLEU scores (24.83 and
24.91, respectively), the dependency LM provides
a much bigger gain when evaluated with BLEU
precision (27.73 vs. 28.79), i.e., by ignoring the
brevity penalty. On the other hand, the difference
on MT08 is significant in terms of TER.
Table 6 provides experimental results on the
NIST test data (excluding the tuning set MT05) for
each of the three genres: newswire, web data, and
speech (broadcast news and conversation). The
last column displays results for all test sets com-
bined. Results do not suggest any noticeable dif-
ference between genres, and the dependency lan-
guage model provides significant gains on all gen-
res, despite the fact that this model was primarily
trained on news data.
We wish to emphasize that our positive re-
sults are particularly noteworthy because they are
achieved over a baseline incorporating a compet-
itive 5-gram language model. As is widely ac-
knowledged in the speech community, it can be
difficult to outperform high-order n-gram models
in large-scale experiments. Finally, we quantified
the effective running time of our phrase-based de-
coder with and without our dependency language
</bodyText>
<table confidence="0.97333825">
BLEU[%]
DEP. LM newswire web speech all
no 32.86 21.75 36.88 32.29
yes 33.19 22.64 37.51 32.74
(+0.33) (+0.89) (+0.63) (+0.45)
TER[%]
DEP. LM newswire web speech all
no 57.73 62.64 55.16 58.02
yes 56.73 61.97 54.26 57.10
(−1) (−0.67) (−0.9) (−0.92)
newswire web speech all
Sentences 4006 1149 1451 6606
</table>
<tableCaption confidence="0.90012625">
Table 6: Test set performances on MT02-MT04 and MT06-
MT08, where the data was broken down by genre. Given
the large amount of test data involved in this table, all these
results are statistically highly significant (p &lt; .01).
</tableCaption>
<figure confidence="0.9700445">
10 20 30 40 50 60 70 80 90
sentence length
</figure>
<figureCaption confidence="0.993229">
Figure 2: Running time of our phrase-based decoder with and
without quadratic-time dependency LM scoring.
</figureCaption>
<bodyText confidence="0.999854454545455">
model using MT05 (Fig. 2). In both settings, we
selected the best tuned model, which yield the per-
formance shown in the first column of Table 5.
Our decoder was run on an AMD Opteron Proces-
sor 2216 with 16GB of memory, and without re-
sorting to any rescoring method such as cube prun-
ing. In the case of English translations of 40 words
and shorter, the baseline system took 6.5 seconds
per sentence, whereas the dependency LM system
spent 15.6 seconds per sentence, i.e., 2.4 times the
baseline running time. In the case of translations
</bodyText>
<figure confidence="0.9976095">
seconds
160
140
120
100
80
60
40
20
0
depLM
baseline
</figure>
<page confidence="0.993245">
779
</page>
<bodyText confidence="0.999339666666667">
longer than 40 words, average speeds were respec-
tively 17.5 and 59.5 seconds per sentence, i.e., the
dependency was only 3.4 times slower.7
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.99996865">
Perhaps due to the high computational cost of syn-
chronous CFG decoding, there have been various
attempts to exploit syntactic knowledge and hier-
archical structure in other machine translation ex-
periments that do not require chart parsing. Using
a reranking framework, Och et al. (2004) found
that various types of syntactic features provided
only minor gains in performance, suggesting that
phrase-based systems (Och and Ney, 2004) should
exploit such information during rather than after
decoding. Wang et al. (2007) sidestep the need to
operate large-scale word order changes during de-
coding (and thus lessening the need for syntactic
decoding) by rearranging input words in the train-
ing data to match the syntactic structure of the
target language. Finally, Birch et al. (2007) ex-
ploit factored phrase-based translation models to
associate each word with a supertag, which con-
tains most of the information needed to build a full
parse. When combined with a supertag n-gram
language model, it helps enforce grammatical con-
straints on the target side.
There have been various attempts to reduce the
computational expense of syntactic decoding, in-
cluding multi-pass decoding approaches (Zhang
and Gildea, 2008; Petrov et al., 2008) and rescor-
ing approaches (Huang and Chiang, 2007). In the
latter paper, Huang and Chiang introduce rescor-
ing methods named “cube pruning” and “cube
growing”, which first use a baseline decoder (ei-
ther synchronous CFG or a phrase-based sys-
tem) and no LM to generate a hypergraph, and
then rescoring this hypergraph with a language
model. Huang and Chiang show significant speed
increases with little impact on translation quality.
We believe that their approach is orthogonal (and
possibly complementary) to our work, since our
paper proposes a new model for fully-integrated
decoding that increases MT performance, and
does not rely on rescoring.
</bodyText>
<footnote confidence="0.9638466">
7We note that our Java-based decoder is research rather
than industrial-strength code and that it could be substantially
optimized. Hence, we think the reader should pay more at-
tention to relative speed differences between the two systems
rather than absolute timings.
</footnote>
<sectionHeader confidence="0.892601" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999976363636364">
In this paper, we presented a non-projective de-
pendency parser whose time-complexity of O(n2)
improves upon the cubic time implementation of
(McDonald et al., 2005b), and does so with lit-
tle loss in dependency accuracy (.25% to .34%).
Since this parser does not need to enforce projec-
tivity constraints, it can easily be integrated into
a phrase-based decoder during search (rather than
during rescoring). We use dependency scores as
an extra feature in our MT experiments, and found
that our dependency model provides significant
gains over a competitive baseline that incorporates
a large 5-gram language model (0.92% TER and
0.45% BLEU absolute improvements).
We plan to pursue other research directions us-
ing dependency models discussed in this paper.
While we use a dependency language model to
exemplify the use of hierarchical structure within
phrase based decoders, we could extend this work
to incorporate dependency features of both source-
and target side. Since parsing of the source is rel-
atively inexpensive compared to the target side,
it would be relatively easy to condition head-
modifier dependencies not only on the two tar-
get words, but also on their corresponding Chi-
nese words and their relative positions in the Chi-
nese tree. This would enable the decoder to cap-
ture syntactic reordering without requiring trees to
be isomorphic or even projective. It would also
be interesting to apply these models to target lan-
guages that have free word order, which would
presumably benefit more from the flexibility of
non-projective dependency models.
</bodyText>
<sectionHeader confidence="0.994951" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999978444444444">
The authors wish to thank the anonymous review-
ers for their helpful comments on an earlier draft
of this paper, and Daniel Cer for his implementa-
tion of Phrasal, a phrase-based decoder similar to
Moses. This paper is based on work funded by
the Defense Advanced Research Projects Agency
through IBM. The content does not necessarily re-
flect the views of the U.S. Government, and no of-
ficial endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9974775">
A. Birch, M. Osborne, and P. Koehn. 2007. CCG su-
pertags in factored statistical machine translation. In
Proc. of the Workshop on Statistical Machine Trans-
lation, pages 9–16.
</reference>
<page confidence="0.983843">
780
</page>
<reference confidence="0.99989156097561">
T. Brants, A. Popat, P. Xu, F. Och, and J. Dean. 2007.
Large language models in machine translation. In
Proc. of EMNLP-CoNLL, pages 858–867.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of the ACL Workshop on
Statistical Machine Translation, pages 224–232.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263–270.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396–
1400.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951–991.
H. Daume III. 2007. Frustratingly easy domain adap-
tation. In Proc. of ACL, pages 256–263.
Y. Ding and M. Palmer. 2005. Machine translation us-
ing probabilistic synchronous dependency insertion
grammars. In Proc. of ACL, pages 541–548.
J. Edmonds. 1967. Optimum branchings. Research of
the National Bureau of Standards, 71B:233–240.
J. Eisner and G. Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proc. of ACL, pages 457–
464.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING, pages 340–345.
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of EMNLP, pages 304–311.
L. Georgiadis. 2003. Arborescence optimization prob-
lems solvable by Edmonds’ algorithm. Theoretical
Computer Science, 301(1-3):427–437.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL, pages 144–151.
L. Huang, H. Zhang, and D. Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks.
In Proc. of the International Workshop on Parsing
Technology, pages 65–73.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607–615.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL,
Demonstration Session.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntact-
ified target language phrases. In Proc. of EMNLP,
pages 44–52.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. of ACL, pages 91–98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005b. Non-projective dependency parsing using
spanning tree algorithms. In Proc. of HLT-EMNLP,
pages 523–530.
J. Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proc. of the Inter-
national Workshop on Parsing Technologies (IWPT
03), pages 149–160.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417–449.
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In Proceedings of HLT-NAACL.
F. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of ACL.
S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-
to-fine syntactic machine translation using language
projections. In Proc. of EMNLP, pages 108–116.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: syntactically informed
phrasal SMT. In Proc. of ACL, pages 271–279.
A. Ratnaparkhi. 1997. A linear observed time statis-
tical parser based on maximum entropy models. In
Proc. of EMNLP.
S. Riezler and J. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
MT. In Proc. of the ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization, pages 57–64.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
ofACL, pages 577–585.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA,
pages 223–231.
A. Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing (ICSLP–2002).
R. Tarjan. 1977. Finding optimum branchings. Net-
works, 7:25–35.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. of NAACL,
pages 173–180.
D. Vadas and J. Curran. 2007. Adding noun phrase
structure to the Penn treebank. In Proc. of ACL,
pages 240–247.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proc. of EMNLP-CoNLL, pages 737–745.
D. Wu. 1996. A polynomial-time algorithm for statis-
tical machine translation. In Proc. ofACL.
H. Zhang and D. Gildea. 2008. Efficient multi-pass
decoding for synchronous context free grammars. In
Proc. of ACL, pages 209–217.
</reference>
<page confidence="0.997902">
781
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955489">
<title confidence="0.999911">Quadratic-Time Dependency Parsing for Machine Translation</title>
<author confidence="0.999998">Michel Galley</author>
<affiliation confidence="0.999935">Computer Science Department Stanford University</affiliation>
<address confidence="0.999373">Stanford, CA 94305-9020</address>
<email confidence="0.999639">mgalley@cs.stanford.edu</email>
<abstract confidence="0.99752395">Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven’t yet been explored in MT. Recently, McDonald et al. (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence. They show that MST parsing is almost as accurate as cubic-time dependency parsing in the case of English, and that it is more accurate with free word order languages. This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores. Our results show that augmenting a state-ofthe-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Birch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>CCG supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="32633" citStr="Birch et al. (2007)" startWordPosition="5358" endWordPosition="5361">e in other machine translation experiments that do not require chart parsing. Using a reranking framework, Och et al. (2004) found that various types of syntactic features provided only minor gains in performance, suggesting that phrase-based systems (Och and Ney, 2004) should exploit such information during rather than after decoding. Wang et al. (2007) sidestep the need to operate large-scale word order changes during decoding (and thus lessening the need for syntactic decoding) by rearranging input words in the training data to match the syntactic structure of the target language. Finally, Birch et al. (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. When combined with a supertag n-gram language model, it helps enforce grammatical constraints on the target side. There have been various attempts to reduce the computational expense of syntactic decoding, including multi-pass decoding approaches (Zhang and Gildea, 2008; Petrov et al., 2008) and rescoring approaches (Huang and Chiang, 2007). In the latter paper, Huang and Chiang introduce rescoring methods named “cube pruning” and “cube </context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>A. Birch, M. Osborne, and P. Koehn. 2007. CCG supertags in factored statistical machine translation. In Proc. of the Workshop on Statistical Machine Translation, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Popat</author>
<author>P Xu</author>
<author>F Och</author>
<author>J Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="2706" citStr="Brants et al., 2007" startWordPosition="398" endWordPosition="401">comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305-9010 manning@cs.stanford.edu petitive phrase-based systems in large-scale experiments such as NIST evaluations.2 This lack of significant difference may not be completely surprising. Indeed, researchers have shown that gigantic language models are key to state-ofthe-art performance (Brants et al., 2007), and the ability of phrase-based decoders to handle large-size, high-order language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKY decoders, whose time complexity grows prohibitively large with higher-order language models. While context-free decoding algorithms (CKY, Earley, etc.) may sometimes appear too computationally expensive for high-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature. The parsing literature presents faster </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>T. Brants, A. Popat, P. Xu, F. Och, and J. Dean. 2007. Large language models in machine translation. In Proc. of EMNLP-CoNLL, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chang</author>
<author>M Galley</author>
<author>C Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proc. of the ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<contexts>
<context position="25848" citStr="Chang et al., 2008" startWordPosition="4218" endWordPosition="4221">sk. We constrain the converter not to use functional tags found in the treebanks, in order to make it possible to use automatically parsed texts (i.e., perform selftraining) in future work. Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC). In order to provide experiments comparable to previous work, we used the same corpora as (Wang et al., 2007): LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al., 2008) that conforms to the Chinese Treebank (CTB) standard. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in (Stolcke, 2002), and we onl</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>P. Chang, M. Galley, and C. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proc. of the ACL Workshop on Statistical Machine Translation, pages 224–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1215" citStr="Chiang, 2005" startWordPosition="178" endWordPosition="179">dependency parsing in the case of English, and that it is more accurate with free word order languages. This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores. Our results show that augmenting a state-ofthe-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets. 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1)), though </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<volume>14</volume>
<pages>1400</pages>
<contexts>
<context position="3694" citStr="Chu and Liu, 1965" startWordPosition="538" endWordPosition="541">o computationally expensive for high-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature. The parsing literature presents faster alternatives for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants ((Ratnaparkhi, 1997; Nivre, 2003), inter alia). While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n2) algorithms exist in the case of dependency parsing. Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967), McDonald et al. (2005b) present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than “full-fledged” chart parsing (which, in the case of dependency parsing, runs in time O(n3) (Eisner, 1996)). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen2Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English and Arabic-English tasks inc</context>
<context position="11376" citStr="Chu and Liu, 1965" startWordPosition="1813" endWordPosition="1816">ectivity, one can independently select the highest scoring edge (i, j) for each modifier xj, yet we generally want to ensure that the resulting structure is a tree, i.e., that it does not contain any circular dependencies. This optimization problem is a known instance of the maximum spanning tree (MST) problem. In our case, the graph is directed—indeed, the equality s(i, j) = s(j,i) is generally not true and would be linguistically aberrant—so the problem constitutes an instance of the less-known MST problem for directed graphs. This problem is solved with the Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu, 1965; Edmonds, 1967). Formally, we represent the graph G = (V,E) with a vertex set V = x = {x0,••• ,xnI and a set of directed edges E = [0,n] x [1,n], in which each edge (i, j), representing the dependency xi —* xj, is assigned a score s(i, j). Finding the spanning tree y C E rooted at x0 that maximizes s(x,y) as defined in Equation 2 has a straightforward solution in O(n2 log(n)) time for dense graphs such as G, though Tarjan (1977) shows that the problem can be solved in O(n2). Hence, non-projective dependency parsing is solved in quadratic time. The main idea behind the CLE algorithm is to firs</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y. J. Chu and T. H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396– 1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="10494" citStr="Crammer and Singer, 2003" startWordPosition="1663" endWordPosition="1666"> parsing for MT We now formalize weighted non-projective dependency parsing similarly to (McDonald et al., 2005b) and then describe a modified and more efficient version that can be integrated into a phrasebased decoder. Given the single-head constraint, parsing an input sentence x = (x0,x1,••• ,xn) is reduced to labeling each word xj with an index i identifying its head word xi. We include the dummy root symbol x0 = (root) so that each word can be a modifier. We score each dependency relation using a standard linear model s(i, j) = X • f(i, j) (1) whose weight vector X is trained using MIRA (Crammer and Singer, 2003) to optimize dependency parsing accuracy (McDonald et al., 2005a). As is commonly the case in statistical parsing, the score of the full tree is decomposed as the sum of the score of all edges: s(x,y) = E X - f(i, j) (2) (i,j)cy When there is no need to ensure projectivity, one can independently select the highest scoring edge (i, j) for each modifier xj, yet we generally want to ensure that the resulting structure is a tree, i.e., that it does not contain any circular dependencies. This optimization problem is a known instance of the maximum spanning tree (MST) problem. In our case, the graph</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>256--263</pages>
<marker>Daume, 2007</marker>
<rawString>H. Daume III. 2007. Frustratingly easy domain adaptation. In Proc. of ACL, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ding</author>
<author>M Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="5962" citStr="Ding and Palmer, 2005" startWordPosition="890" endWordPosition="893">language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p :5 .01). 2 Dependency parsing for machine translation In this section, we review dependency parsing formulated as a maximum spanning tree problem (McDonald et al., 2005b), which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding. Dependency models have recently gained considerable interest in many NLP applications, including machine translation (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency structure provides several compelling advantages compared to other syntactic representations. First, dependency links are close to the semantic relationships, which are more likely to be consistent across languages. Indeed, Fox (2002) found inter-lingual phrasal cohesion to be greater than for a CFG when using a dependency representation, for which she found only 12.6% of head crossings and 9.2% modifier crossings. Second, dependency trees contain exactly one node per word, which contributes to cutting down the search space during parsing: i</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Y. Ding and M. Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proc. of ACL, pages 541–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Research of the National Bureau of Standards,</journal>
<pages>71--233</pages>
<contexts>
<context position="3710" citStr="Edmonds, 1967" startWordPosition="542" endWordPosition="543">xpensive for high-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature. The parsing literature presents faster alternatives for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants ((Ratnaparkhi, 1997; Nivre, 2003), inter alia). While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n2) algorithms exist in the case of dependency parsing. Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967), McDonald et al. (2005b) present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than “full-fledged” chart parsing (which, in the case of dependency parsing, runs in time O(n3) (Eisner, 1996)). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen2Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English and Arabic-English tasks incorporate synchro</context>
<context position="11392" citStr="Edmonds, 1967" startWordPosition="1817" endWordPosition="1818">ndependently select the highest scoring edge (i, j) for each modifier xj, yet we generally want to ensure that the resulting structure is a tree, i.e., that it does not contain any circular dependencies. This optimization problem is a known instance of the maximum spanning tree (MST) problem. In our case, the graph is directed—indeed, the equality s(i, j) = s(j,i) is generally not true and would be linguistically aberrant—so the problem constitutes an instance of the less-known MST problem for directed graphs. This problem is solved with the Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu, 1965; Edmonds, 1967). Formally, we represent the graph G = (V,E) with a vertex set V = x = {x0,••• ,xnI and a set of directed edges E = [0,n] x [1,n], in which each edge (i, j), representing the dependency xi —* xj, is assigned a score s(i, j). Finding the spanning tree y C E rooted at x0 that maximizes s(x,y) as defined in Equation 2 has a straightforward solution in O(n2 log(n)) time for dense graphs such as G, though Tarjan (1977) shows that the problem can be solved in O(n2). Hence, non-projective dependency parsing is solved in quadratic time. The main idea behind the CLE algorithm is to first greedily selec</context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>J. Edmonds. 1967. Optimum branchings. Research of the National Bureau of Standards, 71B:233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and headautomaton grammars.</title>
<date>1999</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>457--464</pages>
<contexts>
<context position="1910" citStr="Eisner and Satta, 1999" startWordPosition="284" endWordPosition="287">sed systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1)), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1)), i.e., O(n3m). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305-9010 manning@cs.stanford.edu petitive phrase-based systems in large-scale experiments such as NIST evaluatio</context>
<context position="7510" citStr="Eisner and Satta, 1999" startWordPosition="1145" endWordPosition="1148">th free word order languages, who do you think they hired ? WP VB PRP VB PRP VBD . 1 2 3 4 5 6 7 Figure 1: A dependency tree with directed edges going from heads to modifiers. The edge between who and hired causes this tree to be non-projective. Such a head-modifier relationship is difficult to represent with a CFG, since all words directly or indirectly headed by hired (i.e., who, think, they, and hired) do not constitute a contiguous sequence of words. as we will see later in this section. The most standardly used algorithm for parsing with dependency grammars is presented in (Eisner, 1996; Eisner and Satta, 1999). It runs in time O(n3), where n is the length of the sentence. Their algorithm exploits the special properties of dependency trees to reduce the worst-case complexity of bilexical parsing, which otherwise requires O(n4) for bilexical constituency-based parsing. While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. (2005b) show O(n2)-time parsing is possible if trees are not required to be projective. This relaxation entails that dependencies may cross each other rather than being required t</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and headautomaton grammars. In Proc. of ACL, pages 457– 464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="3932" citStr="Eisner, 1996" startWordPosition="578" endWordPosition="579">for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants ((Ratnaparkhi, 1997; Nivre, 2003), inter alia). While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n2) algorithms exist in the case of dependency parsing. Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967), McDonald et al. (2005b) present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than “full-fledged” chart parsing (which, in the case of dependency parsing, runs in time O(n3) (Eisner, 1996)). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen2Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English and Arabic-English tasks incorporate synchronous CFG models, score differences with the best phrase-based system were insignificantly small. 773 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 773–781, Suntec, Singapore, 2-7 </context>
<context position="7485" citStr="Eisner, 1996" startWordPosition="1142" endWordPosition="1144">uctions and with free word order languages, who do you think they hired ? WP VB PRP VB PRP VBD . 1 2 3 4 5 6 7 Figure 1: A dependency tree with directed edges going from heads to modifiers. The edge between who and hired causes this tree to be non-projective. Such a head-modifier relationship is difficult to represent with a CFG, since all words directly or indirectly headed by hired (i.e., who, think, they, and hired) do not constitute a contiguous sequence of words. as we will see later in this section. The most standardly used algorithm for parsing with dependency grammars is presented in (Eisner, 1996; Eisner and Satta, 1999). It runs in time O(n3), where n is the length of the sentence. Their algorithm exploits the special properties of dependency trees to reduce the worst-case complexity of bilexical parsing, which otherwise requires O(n4) for bilexical constituency-based parsing. While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. (2005b) show O(n2)-time parsing is possible if trees are not required to be projective. This relaxation entails that dependencies may cross each other rat</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. of COLING, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="6249" citStr="Fox (2002)" startWordPosition="935" endWordPosition="936">ted as a maximum spanning tree problem (McDonald et al., 2005b), which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding. Dependency models have recently gained considerable interest in many NLP applications, including machine translation (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency structure provides several compelling advantages compared to other syntactic representations. First, dependency links are close to the semantic relationships, which are more likely to be consistent across languages. Indeed, Fox (2002) found inter-lingual phrasal cohesion to be greater than for a CFG when using a dependency representation, for which she found only 12.6% of head crossings and 9.2% modifier crossings. Second, dependency trees contain exactly one node per word, which contributes to cutting down the search space during parsing: indeed, the task of the parser is merely to connect existing nodes rather than hypothesizing new ones. Finally, dependency models are more flexible and account for (non-projective) head-modifier relations that CFG models fail to represent adequately, which is problematic with certain typ</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>H. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proc. of EMNLP, pages 304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Georgiadis</author>
</authors>
<title>Arborescence optimization problems solvable by Edmonds’ algorithm.</title>
<date>2003</date>
<journal>Theoretical Computer Science,</journal>
<pages>301--1</pages>
<contexts>
<context position="12523" citStr="Georgiadis, 2003" startWordPosition="2032" endWordPosition="2033">d in quadratic time. The main idea behind the CLE algorithm is to first greedily select for each word xj the incoming edge (i, j) with highest score, then to successively repeat the following two steps: (a) identify a loop in the graph, and if there is none, halt; (b) contract the loop into a single vertex, and update scores for edges coming in and out of the loop. Once all loops have been eliminated, the algorithm maps back the maximum spanning tree of the contracted graph onto the original graph G, and it can be shown that this yields a spanning tree that is optimal with respect to G and s (Georgiadis, 2003). The greedy approach of selecting the highest scoring edge (i, j) for each modifier xj can easily be applied left-to-right during phrase-based decoding, which proceeds in the same order. For each hypothesis expansion, our decoder generates the following information for the new hypothesis h: • a partial translation x; • a coverage set of input words c; • a translation score 6. In the case of non-projective dependency parsing, we need to maintain additional information for each word xj of the partial translation x: • a predicted POS tag tj; • a dependency score sj. Dependency scores sj are init</context>
</contexts>
<marker>Georgiadis, 2003</marker>
<rawString>L. Georgiadis. 2003. Arborescence optimization problems solvable by Edmonds’ algorithm. Theoretical Computer Science, 301(1-3):427–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="33134" citStr="Huang and Chiang, 2007" startWordPosition="5436" endWordPosition="5439">ing input words in the training data to match the syntactic structure of the target language. Finally, Birch et al. (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. When combined with a supertag n-gram language model, it helps enforce grammatical constraints on the target side. There have been various attempts to reduce the computational expense of syntactic decoding, including multi-pass decoding approaches (Zhang and Gildea, 2008; Petrov et al., 2008) and rescoring approaches (Huang and Chiang, 2007). In the latter paper, Huang and Chiang introduce rescoring methods named “cube pruning” and “cube growing”, which first use a baseline decoder (either synchronous CFG or a phrase-based system) and no LM to generate a hypergraph, and then rescoring this hypergraph with a language model. Huang and Chiang show significant speed increases with little impact on translation quality. We believe that their approach is orthogonal (and possibly complementary) to our work, since our paper proposes a new model for fully-integrated decoding that increases MT performance, and does not rely on rescoring. 7W</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. of ACL, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Machine translation as lexicalized parsing with hooks.</title>
<date>2005</date>
<booktitle>In Proc. of the International Workshop on Parsing Technology,</booktitle>
<pages>65--73</pages>
<contexts>
<context position="1834" citStr="Huang et al. (2005)" startWordPosition="273" endWordPosition="276">; Marcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1)), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1)), i.e., O(n3m). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305-9010 manning@cs.stanford.edu pet</context>
</contexts>
<marker>Huang, Zhang, Gildea, 2005</marker>
<rawString>L. Huang, H. Zhang, and D. Gildea. 2005. Machine translation as lexicalized parsing with hooks. In Proc. of the International Workshop on Parsing Technology, pages 65–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="2189" citStr="Knight, 1999" startWordPosition="328" endWordPosition="329">s CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1)), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1)), i.e., O(n3m). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305-9010 manning@cs.stanford.edu petitive phrase-based systems in large-scale experiments such as NIST evaluations.2 This lack of significant difference may not be completely surprising. Indeed, researchers have shown that gigantic language models are key to state-ofthe-art performance (Brants et al., 2007), and the ability of phrase-based decoders to handle large-size, high-order languag</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>K. Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of</booktitle>
<contexts>
<context position="1338" citStr="Koehn et al., 2003" startWordPosition="197" endWordPosition="200">pplies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores. Our results show that augmenting a state-ofthe-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets. 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1)), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall comp</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL. P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="1235" citStr="Marcu et al., 2006" startWordPosition="180" endWordPosition="183">sing in the case of English, and that it is more accurate with free word order languages. This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores. Our results show that augmenting a state-ofthe-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets. 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1)), though Huang et al. (2005) </context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. of EMNLP, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="3733" citStr="McDonald et al. (2005" startWordPosition="544" endWordPosition="548">h-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature. The parsing literature presents faster alternatives for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants ((Ratnaparkhi, 1997; Nivre, 2003), inter alia). While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n2) algorithms exist in the case of dependency parsing. Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967), McDonald et al. (2005b) present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than “full-fledged” chart parsing (which, in the case of dependency parsing, runs in time O(n3) (Eisner, 1996)). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen2Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English and Arabic-English tasks incorporate synchronous CFG models, score </context>
<context position="5700" citStr="McDonald et al., 2005" startWordPosition="852" endWordPosition="855">pendency language models for MT (Shen et al., 2008), which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p :5 .01). 2 Dependency parsing for machine translation In this section, we review dependency parsing formulated as a maximum spanning tree problem (McDonald et al., 2005b), which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding. Dependency models have recently gained considerable interest in many NLP applications, including machine translation (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency structure provides several compelling advantages compared to other syntactic representations. First, dependency links are close to the semantic relationships, which are more likely to be consistent across languages. Indeed, Fox (2002) found inter-lingual phrasal cohesion to be greater</context>
<context position="7936" citStr="McDonald et al. (2005" startWordPosition="1212" endWordPosition="1215"> contiguous sequence of words. as we will see later in this section. The most standardly used algorithm for parsing with dependency grammars is presented in (Eisner, 1996; Eisner and Satta, 1999). It runs in time O(n3), where n is the length of the sentence. Their algorithm exploits the special properties of dependency trees to reduce the worst-case complexity of bilexical parsing, which otherwise requires O(n4) for bilexical constituency-based parsing. While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. (2005b) show O(n2)-time parsing is possible if trees are not required to be projective. This relaxation entails that dependencies may cross each other rather than being required to be nested, as shown in Fig. 1. More formally, a non-projective tree is any tree that does not satisfy the following definition of a projective tree: Definition. Let x = x1 ···xn be an input sentence, and let y be a rooted tree represented as a set in which each element (i, j) E y is an ordered pair of word indices of x that defines a dependency relation between a head xi and a modifier xj. By definition, the tree y is sa</context>
<context position="9513" citStr="McDonald et al., 2005" startWordPosition="1491" endWordPosition="1495">ly desirable in the case of free word order languages such as Czech, Dutch, and German. Non-projective dependency structures are sometimes even needed for languages like English, e.g., in the case of the wh-movement shown in Fig. 1. For languages &lt;root&gt; &lt;root&gt; 0 774 with relatively rigid word order such as English, there may be some concern that searching the space of non-projective dependency trees, which is considerably larger than the space of projective dependency trees, would yield poor performance. That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English (McDonald et al., 2005b), only 0.7% lower than a projective parser (McDonald et al., 2005a) that uses the same set of features and learning algorithm. In the case of dependency parsing for Czech, (McDonald et al., 2005b) even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing. 2.1 O(n2)-time dependency parsing for MT We now formalize weighted non-projective dependency parsing similarly to (McDonald et al., 2005b) and then describe a modified and more efficient version that can be integrated into a phrasebased decoder. Given the single-head c</context>
<context position="16631" citStr="McDonald et al., 2005" startWordPosition="2753" endWordPosition="2756">s−1 ∧ h-pos ∧ m-pos−1 ∧ m-pos, h-pos−1 ∧ h-pos ∧ m-pos ∧ m-pos+1 In-between POS features: ifi&lt;j: h-pos ∧ h-pos+k ∧ m-pos k ∈ [i,min(i+5, j)] h-pos ∧ m-pos−k ∧ m-pos k ∈ [max(i, j− 5), j] ifi&gt; j: m-pos ∧ m-pos+k ∧ h-pos k ∈ [ j,min(j + 5,i)] m-pos ∧ h-pos−k ∧ h-pos k ∈ [max(j,i− 5),i] Table 2: Features for dependency parsing. It is quite similar to the McDonald (2005a) feature set, except that it does not include the set of all POS tags that appear between each candidate head-modifier pair (i, j). This modification is essential in order to make our parser run in true O(n2) time, as opposed to (McDonald et al., 2005b). SOURCE IDS GENRE SENTENCES English CTB 050–325 newswire 3027 English ATB all newswire 13628 OntoNotes all broadcast news 14056 WSJ 02–21 financial news 39832 Total 70543 Table 3: Characteristics of our training data. The second column identifies documents and sections selected for training. h-pos are all distinct from m-pos features.3 The primary difference between our feature sets and the ones of McDonald et al. is that their set of “in between POS features” includes the set of all tags appearing between each pair of words. Extracting all these tags takes time O(n) for any arbitrary pair </context>
<context position="19021" citStr="McDonald et al., 2005" startWordPosition="3112" endWordPosition="3115">TB(001-049) 85.68 Chu-Liu-Edmonds O(n2) MT CTB(050-325) CTB(001-049) 85.43 Local classifier MT CTB(050-325) CTB(001-049) 85.22 Projective MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 87.40(**) Chu-Liu-Edmonds O(n3) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.79 Chu-Liu-Edmonds O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.45(*) Local classifier O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.29 Table 4: Dependency parsing experiments on test sentences of any length. The projective parsing algorithm is the one implemented as in (McDonald et al., 2005a), which is known as one of the top performing dependency parsers for English. The O(n3) non-projective parser of (McDonald et al., 2005b) is slightly more accurate than our version, though ours runs in O(n2) time. “Local classifier” refers to non-projective dependency parsing without removing loops as a post-processing step. The result marked with (*) identifies the parser used for our MT experiments, which is only about 1% less accurate than a state-of-the-art dependency parser (**). on our test was down 0.9%). To make our genuinely O(n2) parser almost as accurate as the nonprojective parse</context>
<context position="22984" citStr="McDonald et al., 2005" startWordPosition="3767" endWordPosition="3770"> Hence, we use a simpler tagging model that assigns tag ti to word xi by only using features of words xi_3 ···xi, and that does not condition any decision based on any preceding or next tags (ti_1, etc.). Its performance is 95.02% on the WSJ, and 95.30% on the English CTB. Additional experiments reveal two main contributing factors to this drop on WSJ: tagging uncased texts reduces tagging accuracy by about 1%, and using only wordbased features further reduces it by 0.6%. Table 4 shows that the accuracy of our truly 777 O(n2) parser is only .25% to .34% worse than the O(n3) implementation of (McDonald et al., 2005b).5 Compared to the state-of-the-art projective parser as implemented in (McDonald et al., 2005a), performance is 1.28% lower on WSJ, but only 0.95% when training on all our available data and using the MT setting. Overall, we believe that the drop of performance is a reasonable price to pay considering the computational constraints imposed by integrating the dependency parser into an MT decoder. The table also shows a gain of more than 1% in dependency accuracy by adding ATB, OntoNotes, and WSJ to the English CTB training set. The four sources were assigned non-uniform weights: we set the we</context>
<context position="24965" citStr="McDonald et al., 2005" startWordPosition="4091" endWordPosition="4094">es implemented almost exactly as in Moses: four translation features (phrase-based translation probabilities and lexically-weighted probabilities), word penalty, phrase penalty, linear distortion, and language model score. We also incorporated the lexicalized reordering features of Moses, in order to experiment with a baseline that is stronger than the default Moses configuration. The language pair for our experiments is Chinese-to-English. The training data consists of about 28 million English words and 23.3 million 5Note that our results on WSJ are not exactly the same as those reported in (McDonald et al., 2005b), since we used slightly different head finding rules. To extract dependencies from treebanks, we used the LTH Penn Converter (http:// nlp.cs.lth.se/pennconverter/), which extracts dependencies that are almost identical to those used for the CoNLL-2008 Shared Task. We constrain the converter not to use functional tags found in the treebanks, in order to make it possible to use automatically parsed texts (i.e., perform selftraining) in future work. Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC). In order to provide experiments compar</context>
<context position="34193" citStr="McDonald et al., 2005" startWordPosition="5601" endWordPosition="5604">y complementary) to our work, since our paper proposes a new model for fully-integrated decoding that increases MT performance, and does not rely on rescoring. 7We note that our Java-based decoder is research rather than industrial-strength code and that it could be substantially optimized. Hence, we think the reader should pay more attention to relative speed differences between the two systems rather than absolute timings. 6 Conclusion and future work In this paper, we presented a non-projective dependency parser whose time-complexity of O(n2) improves upon the cubic time implementation of (McDonald et al., 2005b), and does so with little loss in dependency accuracy (.25% to .34%). Since this parser does not need to enforce projectivity constraints, it can easily be integrated into a phrase-based decoder during search (rather than during rescoring). We use dependency scores as an extra feature in our MT experiments, and found that our dependency model provides significant gains over a competitive baseline that incorporates a large 5-gram language model (0.92% TER and 0.45% BLEU absolute improvements). We plan to pursue other research directions using dependency models discussed in this paper. While w</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proc. of ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="3733" citStr="McDonald et al. (2005" startWordPosition="544" endWordPosition="548">h-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature. The parsing literature presents faster alternatives for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants ((Ratnaparkhi, 1997; Nivre, 2003), inter alia). While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n2) algorithms exist in the case of dependency parsing. Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967), McDonald et al. (2005b) present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than “full-fledged” chart parsing (which, in the case of dependency parsing, runs in time O(n3) (Eisner, 1996)). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen2Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English and Arabic-English tasks incorporate synchronous CFG models, score </context>
<context position="5700" citStr="McDonald et al., 2005" startWordPosition="852" endWordPosition="855">pendency language models for MT (Shen et al., 2008), which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p :5 .01). 2 Dependency parsing for machine translation In this section, we review dependency parsing formulated as a maximum spanning tree problem (McDonald et al., 2005b), which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding. Dependency models have recently gained considerable interest in many NLP applications, including machine translation (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency structure provides several compelling advantages compared to other syntactic representations. First, dependency links are close to the semantic relationships, which are more likely to be consistent across languages. Indeed, Fox (2002) found inter-lingual phrasal cohesion to be greater</context>
<context position="7936" citStr="McDonald et al. (2005" startWordPosition="1212" endWordPosition="1215"> contiguous sequence of words. as we will see later in this section. The most standardly used algorithm for parsing with dependency grammars is presented in (Eisner, 1996; Eisner and Satta, 1999). It runs in time O(n3), where n is the length of the sentence. Their algorithm exploits the special properties of dependency trees to reduce the worst-case complexity of bilexical parsing, which otherwise requires O(n4) for bilexical constituency-based parsing. While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. (2005b) show O(n2)-time parsing is possible if trees are not required to be projective. This relaxation entails that dependencies may cross each other rather than being required to be nested, as shown in Fig. 1. More formally, a non-projective tree is any tree that does not satisfy the following definition of a projective tree: Definition. Let x = x1 ···xn be an input sentence, and let y be a rooted tree represented as a set in which each element (i, j) E y is an ordered pair of word indices of x that defines a dependency relation between a head xi and a modifier xj. By definition, the tree y is sa</context>
<context position="9513" citStr="McDonald et al., 2005" startWordPosition="1491" endWordPosition="1495">ly desirable in the case of free word order languages such as Czech, Dutch, and German. Non-projective dependency structures are sometimes even needed for languages like English, e.g., in the case of the wh-movement shown in Fig. 1. For languages &lt;root&gt; &lt;root&gt; 0 774 with relatively rigid word order such as English, there may be some concern that searching the space of non-projective dependency trees, which is considerably larger than the space of projective dependency trees, would yield poor performance. That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English (McDonald et al., 2005b), only 0.7% lower than a projective parser (McDonald et al., 2005a) that uses the same set of features and learning algorithm. In the case of dependency parsing for Czech, (McDonald et al., 2005b) even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing. 2.1 O(n2)-time dependency parsing for MT We now formalize weighted non-projective dependency parsing similarly to (McDonald et al., 2005b) and then describe a modified and more efficient version that can be integrated into a phrasebased decoder. Given the single-head c</context>
<context position="16631" citStr="McDonald et al., 2005" startWordPosition="2753" endWordPosition="2756">s−1 ∧ h-pos ∧ m-pos−1 ∧ m-pos, h-pos−1 ∧ h-pos ∧ m-pos ∧ m-pos+1 In-between POS features: ifi&lt;j: h-pos ∧ h-pos+k ∧ m-pos k ∈ [i,min(i+5, j)] h-pos ∧ m-pos−k ∧ m-pos k ∈ [max(i, j− 5), j] ifi&gt; j: m-pos ∧ m-pos+k ∧ h-pos k ∈ [ j,min(j + 5,i)] m-pos ∧ h-pos−k ∧ h-pos k ∈ [max(j,i− 5),i] Table 2: Features for dependency parsing. It is quite similar to the McDonald (2005a) feature set, except that it does not include the set of all POS tags that appear between each candidate head-modifier pair (i, j). This modification is essential in order to make our parser run in true O(n2) time, as opposed to (McDonald et al., 2005b). SOURCE IDS GENRE SENTENCES English CTB 050–325 newswire 3027 English ATB all newswire 13628 OntoNotes all broadcast news 14056 WSJ 02–21 financial news 39832 Total 70543 Table 3: Characteristics of our training data. The second column identifies documents and sections selected for training. h-pos are all distinct from m-pos features.3 The primary difference between our feature sets and the ones of McDonald et al. is that their set of “in between POS features” includes the set of all tags appearing between each pair of words. Extracting all these tags takes time O(n) for any arbitrary pair </context>
<context position="19021" citStr="McDonald et al., 2005" startWordPosition="3112" endWordPosition="3115">TB(001-049) 85.68 Chu-Liu-Edmonds O(n2) MT CTB(050-325) CTB(001-049) 85.43 Local classifier MT CTB(050-325) CTB(001-049) 85.22 Projective MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 87.40(**) Chu-Liu-Edmonds O(n3) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.79 Chu-Liu-Edmonds O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.45(*) Local classifier O(n2) MT CTB(050-325), WSJ(02-21), ATB, OntoNotes CTB(001-049) 86.29 Table 4: Dependency parsing experiments on test sentences of any length. The projective parsing algorithm is the one implemented as in (McDonald et al., 2005a), which is known as one of the top performing dependency parsers for English. The O(n3) non-projective parser of (McDonald et al., 2005b) is slightly more accurate than our version, though ours runs in O(n2) time. “Local classifier” refers to non-projective dependency parsing without removing loops as a post-processing step. The result marked with (*) identifies the parser used for our MT experiments, which is only about 1% less accurate than a state-of-the-art dependency parser (**). on our test was down 0.9%). To make our genuinely O(n2) parser almost as accurate as the nonprojective parse</context>
<context position="22984" citStr="McDonald et al., 2005" startWordPosition="3767" endWordPosition="3770"> Hence, we use a simpler tagging model that assigns tag ti to word xi by only using features of words xi_3 ···xi, and that does not condition any decision based on any preceding or next tags (ti_1, etc.). Its performance is 95.02% on the WSJ, and 95.30% on the English CTB. Additional experiments reveal two main contributing factors to this drop on WSJ: tagging uncased texts reduces tagging accuracy by about 1%, and using only wordbased features further reduces it by 0.6%. Table 4 shows that the accuracy of our truly 777 O(n2) parser is only .25% to .34% worse than the O(n3) implementation of (McDonald et al., 2005b).5 Compared to the state-of-the-art projective parser as implemented in (McDonald et al., 2005a), performance is 1.28% lower on WSJ, but only 0.95% when training on all our available data and using the MT setting. Overall, we believe that the drop of performance is a reasonable price to pay considering the computational constraints imposed by integrating the dependency parser into an MT decoder. The table also shows a gain of more than 1% in dependency accuracy by adding ATB, OntoNotes, and WSJ to the English CTB training set. The four sources were assigned non-uniform weights: we set the we</context>
<context position="24965" citStr="McDonald et al., 2005" startWordPosition="4091" endWordPosition="4094">es implemented almost exactly as in Moses: four translation features (phrase-based translation probabilities and lexically-weighted probabilities), word penalty, phrase penalty, linear distortion, and language model score. We also incorporated the lexicalized reordering features of Moses, in order to experiment with a baseline that is stronger than the default Moses configuration. The language pair for our experiments is Chinese-to-English. The training data consists of about 28 million English words and 23.3 million 5Note that our results on WSJ are not exactly the same as those reported in (McDonald et al., 2005b), since we used slightly different head finding rules. To extract dependencies from treebanks, we used the LTH Penn Converter (http:// nlp.cs.lth.se/pennconverter/), which extracts dependencies that are almost identical to those used for the CoNLL-2008 Shared Task. We constrain the converter not to use functional tags found in the treebanks, in order to make it possible to use automatically parsed texts (i.e., perform selftraining) in future work. Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC). In order to provide experiments compar</context>
<context position="34193" citStr="McDonald et al., 2005" startWordPosition="5601" endWordPosition="5604">y complementary) to our work, since our paper proposes a new model for fully-integrated decoding that increases MT performance, and does not rely on rescoring. 7We note that our Java-based decoder is research rather than industrial-strength code and that it could be substantially optimized. Hence, we think the reader should pay more attention to relative speed differences between the two systems rather than absolute timings. 6 Conclusion and future work In this paper, we presented a non-projective dependency parser whose time-complexity of O(n2) improves upon the cubic time implementation of (McDonald et al., 2005b), and does so with little loss in dependency accuracy (.25% to .34%). Since this parser does not need to enforce projectivity constraints, it can easily be integrated into a phrase-based decoder during search (rather than during rescoring). We use dependency scores as an extra feature in our MT experiments, and found that our dependency model provides significant gains over a competitive baseline that incorporates a large 5-gram language model (0.92% TER and 0.45% BLEU absolute improvements). We plan to pursue other research directions using dependency models discussed in this paper. While w</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proc. of HLT-EMNLP, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proc. of the International Workshop on Parsing Technologies (IWPT 03),</booktitle>
<pages>149--160</pages>
<contexts>
<context position="3443" citStr="Nivre, 2003" startWordPosition="501" endWordPosition="502"> running time during decoding presents a compelling advantage over CKY decoders, whose time complexity grows prohibitively large with higher-order language models. While context-free decoding algorithms (CKY, Earley, etc.) may sometimes appear too computationally expensive for high-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature. The parsing literature presents faster alternatives for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants ((Ratnaparkhi, 1997; Nivre, 2003), inter alia). While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n2) algorithms exist in the case of dependency parsing. Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967), McDonald et al. (2005b) present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than “full-fledged” chart parsing (which, in the case of dependency parsing, runs in time O(n3) (Eisner, 1996)). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under t</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>J. Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proc. of the International Workshop on Parsing Technologies (IWPT 03), pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1317" citStr="Och and Ney, 2004" startWordPosition="193" endWordPosition="196">uages. This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores. Our results show that augmenting a state-ofthe-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets. 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1)), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that y</context>
<context position="32284" citStr="Och and Ney, 2004" startWordPosition="5302" endWordPosition="5305">0 60 40 20 0 depLM baseline 779 longer than 40 words, average speeds were respectively 17.5 and 59.5 seconds per sentence, i.e., the dependency was only 3.4 times slower.7 5 Related work Perhaps due to the high computational cost of synchronous CFG decoding, there have been various attempts to exploit syntactic knowledge and hierarchical structure in other machine translation experiments that do not require chart parsing. Using a reranking framework, Och et al. (2004) found that various types of syntactic features provided only minor gains in performance, suggesting that phrase-based systems (Och and Ney, 2004) should exploit such information during rather than after decoding. Wang et al. (2007) sidestep the need to operate large-scale word order changes during decoding (and thus lessening the need for syntactic decoding) by rearranging input words in the training data to match the syntactic structure of the target language. Finally, Birch et al. (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. When combined with a supertag n-gram language model, it helps enforce grammatical constraint</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="32138" citStr="Och et al. (2004)" startWordPosition="5281" endWordPosition="5284">dency LM system spent 15.6 seconds per sentence, i.e., 2.4 times the baseline running time. In the case of translations seconds 160 140 120 100 80 60 40 20 0 depLM baseline 779 longer than 40 words, average speeds were respectively 17.5 and 59.5 seconds per sentence, i.e., the dependency was only 3.4 times slower.7 5 Related work Perhaps due to the high computational cost of synchronous CFG decoding, there have been various attempts to exploit syntactic knowledge and hierarchical structure in other machine translation experiments that do not require chart parsing. Using a reranking framework, Och et al. (2004) found that various types of syntactic features provided only minor gains in performance, suggesting that phrase-based systems (Och and Ney, 2004) should exploit such information during rather than after decoding. Wang et al. (2007) sidestep the need to operate large-scale word order changes during decoding (and thus lessening the need for syntactic decoding) by rearranging input words in the training data to match the syntactic structure of the target language. Finally, Birch et al. (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains </context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="26859" citStr="Och, 2003" startWordPosition="4390" endWordPosition="4391"> released during periods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data.6 For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2002 to 2008 (MT02 to MT08), which all have four English references for each input sentence. We used the 1082 sentences of MT05 for tuning and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize BLEU (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment three times with different initial conditions. We used n-best lists of size 200 and a beam size of 200. In the final evaluations, we report results using both TER (Snover et al., 2006) and the original BLEU metric as described in (Papineni et al., 2001). All our evaluations are performed on uncased texts. The results for our translation experiments are shown in Table 5. We compared two systems: one with the set of features d</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="26916" citStr="Papineni et al., 2001" startWordPosition="4398" endWordPosition="4401">ose of our development and test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data.6 For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2002 to 2008 (MT02 to MT08), which all have four English references for each input sentence. We used the 1082 sentences of MT05 for tuning and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize BLEU (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment three times with different initial conditions. We used n-best lists of size 200 and a beam size of 200. In the final evaluations, we report results using both TER (Snover et al., 2006) and the original BLEU metric as described in (Papineni et al., 2001). All our evaluations are performed on uncased texts. The results for our translation experiments are shown in Table 5. We compared two systems: one with the set of features described earlier in this section. The second system incor</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Coarseto-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>108--116</pages>
<contexts>
<context position="33084" citStr="Petrov et al., 2008" startWordPosition="5428" endWordPosition="5431">ng the need for syntactic decoding) by rearranging input words in the training data to match the syntactic structure of the target language. Finally, Birch et al. (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. When combined with a supertag n-gram language model, it helps enforce grammatical constraints on the target side. There have been various attempts to reduce the computational expense of syntactic decoding, including multi-pass decoding approaches (Zhang and Gildea, 2008; Petrov et al., 2008) and rescoring approaches (Huang and Chiang, 2007). In the latter paper, Huang and Chiang introduce rescoring methods named “cube pruning” and “cube growing”, which first use a baseline decoder (either synchronous CFG or a phrase-based system) and no LM to generate a hypergraph, and then rescoring this hypergraph with a language model. Huang and Chiang show significant speed increases with little impact on translation quality. We believe that their approach is orthogonal (and possibly complementary) to our work, since our paper proposes a new model for fully-integrated decoding that increases </context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>S. Petrov, A. Haghighi, and D. Klein. 2008. Coarseto-fine syntactic machine translation using language projections. In Proc. of EMNLP, pages 108–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
<author>C Cherry</author>
</authors>
<title>Dependency treelet translation: syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="5982" citStr="Quirk et al., 2005" startWordPosition="894" endWordPosition="897"> improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p :5 .01). 2 Dependency parsing for machine translation In this section, we review dependency parsing formulated as a maximum spanning tree problem (McDonald et al., 2005b), which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding. Dependency models have recently gained considerable interest in many NLP applications, including machine translation (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency structure provides several compelling advantages compared to other syntactic representations. First, dependency links are close to the semantic relationships, which are more likely to be consistent across languages. Indeed, Fox (2002) found inter-lingual phrasal cohesion to be greater than for a CFG when using a dependency representation, for which she found only 12.6% of head crossings and 9.2% modifier crossings. Second, dependency trees contain exactly one node per word, which contributes to cutting down the search space during parsing: indeed, the task of t</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet translation: syntactically informed phrasal SMT. In Proc. of ACL, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="3429" citStr="Ratnaparkhi, 1997" startWordPosition="499" endWordPosition="500">uence on asymptotic running time during decoding presents a compelling advantage over CKY decoders, whose time complexity grows prohibitively large with higher-order language models. While context-free decoding algorithms (CKY, Earley, etc.) may sometimes appear too computationally expensive for high-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature. The parsing literature presents faster alternatives for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants ((Ratnaparkhi, 1997; Nivre, 2003), inter alia). While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n2) algorithms exist in the case of dependency parsing. Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967), McDonald et al. (2005b) present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than “full-fledged” chart parsing (which, in the case of dependency parsing, runs in time O(n3) (Eisner, 1996)). In this paper, we show how to exploit syntactic dependency structure for better machine transl</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>A. Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="28455" citStr="Riezler and Maxwell, 2005" startWordPosition="4659" endWordPosition="4662">uning. This LM required 16GB of RAM during training. 778 BLEU[%] DEP. LM MT05 (tune) MT02 MT03 MT04 MT06 MT08 33.42 33.38 33.13 36.21 32.16 34.19 (+.77**) 33.85 (+.47) 33.73 (+.6*) 36.67 (+.46*) 32.84 (+.68**) no 24.83 yes 24.91 (+.08) TER[%] DEP. LM MT05 (tune) MT02 MT03 MT04 MT06 MT08 no 57.41 58.07 57.32 56.09 57.24 61.96 yes 56.27 (−1.14**) 57.15 (− .92**) 56.09 (− 1.23**) 55.30 (− .79**) 56.05 (− 1.19**) 61.41 (−.55*) MT05 (tune) MT02 MT03 MT04 MT06 MT08 Sentences 1082 878 919 1788 1664 1357 Table 5: MT experiments with and without a dependency language model. We use randomization tests (Riezler and Maxwell, 2005) to determine significance: differences marked with a (*) are significant at the p &lt; .05 level, and those marked as (**) are significant at the p &lt; .01 level. model score computed with the dependency parsing algorithm described in Section 2. We used the dependency model trained on the English CTB and ATB treebank, WSJ, and OntoNotes. We see that the Moses decoder with integrated dependency language model systematically outperforms the Moses baseline. For BLEU evaluations, differences are significant in four out of six cases, and in the case of TER, all differences are significant. Regarding th</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>S. Riezler and J. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>J Xu</author>
<author>R Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1255" citStr="Shen et al., 2008" startWordPosition="184" endWordPosition="187">English, and that it is more accurate with free word order languages. This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores. Our results show that augmenting a state-ofthe-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets. 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1)), though Huang et al. (2005) present a more effic</context>
<context position="5130" citStr="Shen et al., 2008" startWordPosition="754" endWordPosition="757">tec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of McDonald et al. (2005b) for machine translation, we incrementally build dependency structure left-toright in time O(n2) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008), which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p :5 .01). 2 Dependency parsing for machine translation In this section, we review dependency parsing formulated as a maximum spanning tree problem (McDonald et al., 2005b), which can be solved in qua</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>L. Shen, J. Xu, and R. Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proc. ofACL, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="27215" citStr="Snover et al., 2006" startWordPosition="4451" endWordPosition="4454"> data for Chinese from 2002 to 2008 (MT02 to MT08), which all have four English references for each input sentence. We used the 1082 sentences of MT05 for tuning and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize BLEU (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment three times with different initial conditions. We used n-best lists of size 200 and a beam size of 200. In the final evaluations, we report results using both TER (Snover et al., 2006) and the original BLEU metric as described in (Papineni et al., 2001). All our evaluations are performed on uncased texts. The results for our translation experiments are shown in Table 5. We compared two systems: one with the set of features described earlier in this section. The second system incorporates one additional feature, which is the dependency language 6We found that sections of Gigaword other than Xinhua and AFP provide almost no improvement in our experiments. By leaving aside the other sections, we were able to increase the order of the language model to 5-gram and perform relati</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of AMTA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing (ICSLP–2002).</booktitle>
<contexts>
<context position="26436" citStr="Stolcke, 2002" startWordPosition="4315" endWordPosition="4316">ifier (Chang et al., 2008) that conforms to the Chinese Treebank (CTB) standard. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data.6 For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2002 to 2008 (MT02 to MT08), which all have four English references for each input sentence. We used the 1082 sentences of MT05 for tuning and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize BLEU (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment three</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Language Processing (ICSLP–2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tarjan</author>
</authors>
<title>Finding optimum branchings.</title>
<date>1977</date>
<journal>Networks,</journal>
<pages>7--25</pages>
<contexts>
<context position="11809" citStr="Tarjan (1977)" startWordPosition="1900" endWordPosition="1901">berrant—so the problem constitutes an instance of the less-known MST problem for directed graphs. This problem is solved with the Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu, 1965; Edmonds, 1967). Formally, we represent the graph G = (V,E) with a vertex set V = x = {x0,••• ,xnI and a set of directed edges E = [0,n] x [1,n], in which each edge (i, j), representing the dependency xi —* xj, is assigned a score s(i, j). Finding the spanning tree y C E rooted at x0 that maximizes s(x,y) as defined in Equation 2 has a straightforward solution in O(n2 log(n)) time for dense graphs such as G, though Tarjan (1977) shows that the problem can be solved in O(n2). Hence, non-projective dependency parsing is solved in quadratic time. The main idea behind the CLE algorithm is to first greedily select for each word xj the incoming edge (i, j) with highest score, then to successively repeat the following two steps: (a) identify a loop in the graph, and if there is none, halt; (b) contract the loop into a single vertex, and update scores for edges coming in and out of the loop. Once all loops have been eliminated, the algorithm maps back the maximum spanning tree of the contracted graph onto the original graph </context>
</contexts>
<marker>Tarjan, 1977</marker>
<rawString>R. Tarjan. 1977. Finding optimum branchings. Networks, 7:25–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="22084" citStr="Toutanova et al., 2003" startWordPosition="3605" endWordPosition="3608"> to improve machine translation (e.g., most hyphenated words were split). For this setting, we also had to harmonize the four treebanks. The most crucial modification was to add NP internal bracketing to the WSJ (Vadas and Curran, 2007), since the three other treebanks contain that information. Treebanks were also transformed to be consistent with MT tokenization. We evaluate MT parsing models on CTB rather than on WSJ, since CTB contains newswire and is thus more representative of MT evaluation conditions. To obtain part-of-speech tags, we use a state-of-the-art maximum-entropy (CMM) tagger (Toutanova et al., 2003). In the Parsing setting, we use its best configuration, which reaches a tagging accuracy of 97.25% on standard WSJ test data. In the MT setting, we need to use a less effective tagger, since we cannot afford to perform Viterbi inference as a by-product of phrase-based decoding. Hence, we use a simpler tagging model that assigns tag ti to word xi by only using features of words xi_3 ···xi, and that does not condition any decision based on any preceding or next tags (ti_1, etc.). Its performance is 95.02% on the WSJ, and 95.30% on the English CTB. Additional experiments reveal two main contribu</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of NAACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vadas</author>
<author>J Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn treebank.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>240--247</pages>
<contexts>
<context position="21697" citStr="Vadas and Curran, 2007" startWordPosition="3548" endWordPosition="3551">ssible. The only features that are not cached are the ones that include contextual POS tags, since their miss rate is relatively high. distinguish two experimental conditions: Parsing and MT. For Parsing, sentences are cased and tokenization abides to the PTB segmentation as used in the Penn treebank version 3. For the MT setting, texts are all lower case, and tokenization was changed to improve machine translation (e.g., most hyphenated words were split). For this setting, we also had to harmonize the four treebanks. The most crucial modification was to add NP internal bracketing to the WSJ (Vadas and Curran, 2007), since the three other treebanks contain that information. Treebanks were also transformed to be consistent with MT tokenization. We evaluate MT parsing models on CTB rather than on WSJ, since CTB contains newswire and is thus more representative of MT evaluation conditions. To obtain part-of-speech tags, we use a state-of-the-art maximum-entropy (CMM) tagger (Toutanova et al., 2003). In the Parsing setting, we use its best configuration, which reaches a tagging accuracy of 97.25% on standard WSJ test data. In the MT setting, we need to use a less effective tagger, since we cannot afford to p</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>D. Vadas and J. Curran. 2007. Adding noun phrase structure to the Penn treebank. In Proc. of ACL, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>M Collins</author>
<author>P Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>737--745</pages>
<contexts>
<context position="25635" citStr="Wang et al., 2007" startWordPosition="4193" endWordPosition="4196">les. To extract dependencies from treebanks, we used the LTH Penn Converter (http:// nlp.cs.lth.se/pennconverter/), which extracts dependencies that are almost identical to those used for the CoNLL-2008 Shared Task. We constrain the converter not to use functional tags found in the treebanks, in order to make it possible to use automatically parsed texts (i.e., perform selftraining) in future work. Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC). In order to provide experiments comparable to previous work, we used the same corpora as (Wang et al., 2007): LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05. Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al., 2008) that conforms to the Chinese Treebank (CTB) standard. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Giga</context>
<context position="32370" citStr="Wang et al. (2007)" startWordPosition="5315" endWordPosition="5318"> 17.5 and 59.5 seconds per sentence, i.e., the dependency was only 3.4 times slower.7 5 Related work Perhaps due to the high computational cost of synchronous CFG decoding, there have been various attempts to exploit syntactic knowledge and hierarchical structure in other machine translation experiments that do not require chart parsing. Using a reranking framework, Och et al. (2004) found that various types of syntactic features provided only minor gains in performance, suggesting that phrase-based systems (Och and Ney, 2004) should exploit such information during rather than after decoding. Wang et al. (2007) sidestep the need to operate large-scale word order changes during decoding (and thus lessening the need for syntactic decoding) by rearranging input words in the training data to match the syntactic structure of the target language. Finally, Birch et al. (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. When combined with a supertag n-gram language model, it helps enforce grammatical constraints on the target side. There have been various attempts to reduce the computational exp</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>C. Wang, M. Collins, and P. Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proc. of EMNLP-CoNLL, pages 737–745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="1556" citStr="Wu, 1996" startWordPosition="232" endWordPosition="233">y language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets. 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1)), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1)), i.e., O(n3m). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decod</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>D. Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Efficient multi-pass decoding for synchronous context free grammars.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>209--217</pages>
<contexts>
<context position="33062" citStr="Zhang and Gildea, 2008" startWordPosition="5424" endWordPosition="5427">coding (and thus lessening the need for syntactic decoding) by rearranging input words in the training data to match the syntactic structure of the target language. Finally, Birch et al. (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. When combined with a supertag n-gram language model, it helps enforce grammatical constraints on the target side. There have been various attempts to reduce the computational expense of syntactic decoding, including multi-pass decoding approaches (Zhang and Gildea, 2008; Petrov et al., 2008) and rescoring approaches (Huang and Chiang, 2007). In the latter paper, Huang and Chiang introduce rescoring methods named “cube pruning” and “cube growing”, which first use a baseline decoder (either synchronous CFG or a phrase-based system) and no LM to generate a hypergraph, and then rescoring this hypergraph with a language model. Huang and Chiang show significant speed increases with little impact on translation quality. We believe that their approach is orthogonal (and possibly complementary) to our work, since our paper proposes a new model for fully-integrated de</context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>H. Zhang and D. Gildea. 2008. Efficient multi-pass decoding for synchronous context free grammars. In Proc. of ACL, pages 209–217.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>