<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.991272">
Parse Correction with Specialized Models for Difficult Attachment Types
</title>
<author confidence="0.815306">
Enrique Henestroza Anguiano and Marie Candito
</author>
<affiliation confidence="0.771442">
Alpage (Universit´e Paris Diderot / INRIA)
</affiliation>
<address confidence="0.664954">
Paris, France
</address>
<email confidence="0.99458">
henestro@inria.fr, marie.candito@linguist.jussieu.fr
</email>
<sectionHeader confidence="0.99858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999648">
This paper develops a framework for syntac-
tic dependency parse correction. Dependen-
cies in an input parse tree are revised by se-
lecting, for a given dependent, the best gov-
ernor from within a small set of candidates.
We use a discriminative linear ranking model
to select the best governor from a group of
candidates for a dependent, and our model in-
cludes a rich feature set that encodes syntac-
tic structure in the input parse tree. The parse
correction framework is parser-agnostic, and
can correct attachments using either a generic
model or specialized models tailored to dif-
ficult attachment types like coordination and
pp-attachment. Our experiments show that
parse correction, combining a generic model
with specialized models for difficult attach-
ment types, can successfully improve the qual-
ity of predicted parse trees output by sev-
eral representative state-of-the-art dependency
parsers for French.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99217635483871">
In syntactic dependency parse correction, attach-
ments in an input parse tree are revised by selecting,
for a given dependent, the best governor from within
a small set of candidates. The motivation behind
parse correction is that attachment decisions, espe-
cially traditionally difficult ones like pp-attachment
and coordination, may require substantial contextual
information in order to be made accurately. Because
syntactic dependency parsers predict the parse tree
for an entire sentence, they may not be able to take
into account sufficient context when making attach-
ment decisions, due to computational complexity.
Assuming nonetheless that a predicted parse tree is
mostly accurate, parse correction can revise difficult
attachments by using the predicted tree’s syntactic
structure to restrict the set of candidate governors
and extract a rich set of features to help select among
them. Parse correction is also appealing because it
is parser-agnostic: it can be trained to correct the
output of any dependency parser.
In Section 2 we discuss work related to parse
correction, pp-attachment and coordination resolu-
tion. In Section 3 we discuss dependency struc-
ture and various statistical dependency parsing ap-
proaches. In Section 4 we introduce the parse cor-
rection framework, and Section 5 describes the fea-
tures and learning model used in our implementa-
tion. In Section 6 we present experiments in which
parse correction revises the predicted parse trees of
four state-of-the-art dependency parsers for French.
We provide concluding remarks in Section 7.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999071272727273">
Previous research directly concerning parse correc-
tion includes that of Attardi and Ciaramita (2007),
working on English and Swedish, who use an ap-
proach that considers a fixed set of revision rules:
each rule describes movements in the parse tree
leading from a dependent’s original governor to a
new governor, and a classifier is trained to select
the correct revision rule for a given dependent. One
drawback of this approach is that the classes lack
semantic coherence: a sequence of movements does
not necessarily have the same meaning across differ-
</bodyText>
<page confidence="0.935541">
1222
</page>
<note confidence="0.95774">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1222–1233,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999916315789474">
ent syntactic trees. Hall and Nov´ak (2005), working
on Czech, define a neighborhood of candidate gov-
ernors centered around the original governor of a de-
pendent, and a Maximum Entropy model determines
the probability of each candidate-dependent attach-
ment. We follow primarily from their work in our
use of neighborhoods to delimit the set of candidate
governors. Our main contributions are: specialized
corrective models for difficult attachment types (co-
ordination and pp-attachment) in addition to a gen-
eral corrective model; more sophisticated features,
feature combinations, and feature selection; and a
ranking model trained directly to select the true gov-
ernor from among a set of candidates.
There has also been other work on techniques
similar to parse correction. Attardi and Dell’Orletta
(2009) investigate reverse revision: a left-to-right
transition-based model is first used to parse a sen-
tence, then a right-to-left transition-based model is
run with additional features taken from the left-to-
right model’s predicted parse. This approach leads
to improved parsing results on a number of lan-
guages. While their approach is similar to parse cor-
rection in that it uses a predicted parse to inform a
subsequent processing step, this information is used
to improve a second parser rather than a model for
correcting errors. McDonald and Pereira (2006)
consider a method for recovering non-projective at-
tachments from a graph representation of a sentence,
in which an optimal projective parse tree has been
identified. The parse tree’s edges are allowed to be
rearranged in ways that introduce non-projectivity
in order to increase its overall score. This rearrange-
ment approach resembles parse correction because
it is a second step that can revise attachments made
in the first step, but it differs in a number of ways: it
is dependent on a graph-based parsing approach, it
does not model errors made by the parser, and it can
only output non-projective variants of the predicted
parse tree.
As a process that revises the output of a syntac-
tic parser, parse reranking is also similar to parse
correction. A well-studied subject (e.g. the work
of Charniak and Johnson (2005) and of Collins and
Koo (2005)), parse reranking is concerned with the
reordering of n-best ranked parse trees output by
a syntactic parser. Parse correction has a num-
ber of advantages compared to reranking: it can be
used with parsers that do not output n-best ranked
parses, it can be easily restricted to specific attach-
ment types, and its output space of parse trees is not
limited to those appearing in an n-best list. How-
ever, parse reranking has the advantage of selecting
the globally optimal parse for a sentence from an n-
best list, while parse correction makes only locally
optimal revisions in the predicted parse for a sen-
tence.
</bodyText>
<subsectionHeader confidence="0.953856">
2.1 Difficult Attachment Types
</subsectionHeader>
<bodyText confidence="0.999878513513513">
Research on pp-attachment traditionally formulates
the problem in isolation, as in the work of Pantel and
Lin (2000) and of Olteanu and Moldovan (2005).
Examples consist oftuples of the form (v, n1, p, n2),
where either v or n1 is the true governor of the
pp comprising p and n2, and the task is to choose
between v and n1. Recently, Atterer and Sch¨utze
(2007) have criticized this formulation as unrealistic
because it uses an oracle to select candidate gover-
nors, and they find that successful approaches for
the isolated problem perform no better than state-
of-the-art parsers on pp-attachment when evaluated
on full sentences. With parse correction, candi-
date governors are identified automatically with no
(v, n1, p, n2) restriction, and for several representa-
tive parsers we find that parse correction improves
pp-attachment performance.
Research on coordination resolution has also of-
ten formulated the problem in isolation. Resnik
(1999) uses semantic similarity to resolve noun-
phrase coordination of the form (n1, cc, n2, n3),
where the coordinating conjunction cc coordinates
either the heads n1 and n2 or the heads n1 and
n3. The same criticism as the one made by At-
terer and Sch¨utze (2007) for pp-attachment might
be applied to this approach to coordination reso-
lution. In another formulation, the input consists
of a raw sentence, and coordination structure is
then detected and disambiguated using discrimina-
tive learning models (Shimbo and Hara, 2007) or
coordination-specific parsers (Hara et al., 2009). Fi-
nally, other work has focused on introducing spe-
cialized features for coordination into existing syn-
tactic parsing models (Hogan, 2007). Our approach
is novel with respect to previous work by directly
modeling the correction of coordination errors made
by general-purpose dependency parsers.
</bodyText>
<page confidence="0.95708">
1223
</page>
<figure confidence="0.9908615">
ouvrit
Elle porte avec
la cl´e
la
</figure>
<figureCaption confidence="0.932113">
Figure 1: An unlabeled dependency tree for: Elle ouvrit
la porte avec la cl´e. (She opened the door with the key).
</figureCaption>
<sectionHeader confidence="0.989116" genericHeader="method">
3 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999956233333333">
Dependency syntax involves the representation of
syntactic information for a sentence in the form a
directed graph, whose edges encode word-to-word
relationships. An edge from a governor to a de-
pendent indicates, roughly, that the presence of the
dependent is syntactically legitimated by the gover-
nor. An important property of dependency syntax is
that each word, except for the root of the sentence,
has exactly one governor; dependency syntax is thus
represented by trees. Figure 1 shows an example
of an unlabeled dependency tree.1 For languages
like English or French, most sentences can be rep-
resented with a projective dependency tree: for any
edge from word g to word d, g dominates any inter-
vening word between g and d.
Dependency trees are appealing syntactic repre-
sentations, closer than constituency trees to the se-
mantic representations useful for NLP applications.
This is true even with the projectivity requirement,
which occasionally creates syntax-semantics mis-
matches. Dependency trees have recently seen a
surge of interest, particularly with the introduction
of supervised models for dependency parsing us-
ing linear classifiers. Such parsers fall into two
main categories: transition-based parsing and graph-
based parsing. Additionally, an alternative method
for obtaining the dependency parse for a sentence
is to parse the sentence with a constituency-based
parser and then use an automatic process to convert
the output into dependency structure.
</bodyText>
<footnote confidence="0.83390875">
1Edges are generally labeled with the surface grammatical
function that the dependent bears with respect to its governor.
In this paper we focus on unlabeled dependency parsing, setting
aside labeling as a separate task.
</footnote>
<subsectionHeader confidence="0.99018">
3.1 Transition-Based Parsing
</subsectionHeader>
<bodyText confidence="0.999992956521739">
In transition-based dependency parsing, whose sem-
inal works are that of Yamada and Matsumoto
(2003) and Nivre (2003), the parsing process ap-
plies a sequence of incremental actions, which typ-
ically manipulate a buffer position in the sentence
and a stack for built sub-structures. Actions are of
the type “read word from buffer”, “build a depen-
dencyfrom node on top of the stack to node that
begins the buffer”, etc. In a greedy version of this
process, the action to apply at each step is determin-
istically chosen to be the best-scoring action accord-
ing to a classifier, which is trained on a dependency
treebank converted into sequences of actions. The
strengths of this framework are O(n) time complex-
ity and a lack of restrictions on the locality of fea-
tures. A major drawback is its greedy behavior: it
can potentially make difficult attachment decisions
early in the processing of a sentence, without being
able to reconsider them when more information be-
comes available. Beamed versions of the algorithm
(Johansson and Nugues, 2006) partially address this
problem, but still do not provide a global optimiza-
tion for selecting the output parse tree.
</bodyText>
<subsectionHeader confidence="0.996815">
3.2 Graph-Based Parsing
</subsectionHeader>
<bodyText confidence="0.999787176470588">
In graph-based dependency parsing, whose seminal
work is that of McDonald et al. (2005), the parsing
process selects the globally optimal parse tree from
a graph containing attachments (directed edges) be-
tween each pair of words (nodes) in a sentence.
It finds the k-best scoring parse trees, both during
training and at parse time, where the score of a tree
is the sum of the scores of its factors (consisting of
one or more linked edges). While large factors are
desirable for capturing sophisticated linguistic con-
straints, they come at the cost of time complexity:
for the projective case, adaptations of Eisner’s algo-
rithm (Eisner, 1996) are O(n3) for 1-edge factors
(McDonald et al., 2005) or sibling 2-edge factors
(McDonald and Pereira, 2006), and O(n4) for gen-
eral 2-edge factors (Carreras, 2007) or 3-edge fac-
tors (Koo and Collins, 2010).
</bodyText>
<subsectionHeader confidence="0.995579">
3.3 Constituency-Based Parsing
</subsectionHeader>
<bodyText confidence="0.986468">
Beyond the two main approaches to dependency
parsing, there is also the approach of constituency-
</bodyText>
<page confidence="0.913974">
1224
</page>
<bodyText confidence="0.959200857142857">
based parsing followed by a conversion step to de- INPUT: Predicted parse tree T
pendency structure. We use the three-step parsing LOOP: For each chosen dependent d E D
architecture previously tested for French by Candito • Identify candidates Cd from T
et al. (2010a): (i) A constituency parse tree is out- • Predict cˆ = argmax S(c, d, T)
put by the BerkeleyParser, which has been trained to
learn a probabilistic context-free grammar with la-
tent annotations (Petrov et al., 2006) that has parsing
time complexity O(n3) (Matsuzaki et al., 2005); (ii)
A functional role labeler using a Maximum Entropy
model adds functional annotations to links between
a verb and its dependents; (iii) Constituency trees
are automatically converted into projective depen-
dency trees, with remaining unlabeled dependencies
assigned labels using a rule-based approach.
3.4 Baseline Parsers
In this paper, we use the following baseline parsers:
MaltParser (Nivre et al., 2007) for transition-based
parsing; MSTParser (McDonald et al., 2005) (with
sibling 2-edge factors) and BohnetParser (Bohnet,
2010) (with general 2-edge factors) for graph-based
parsing; and BerkeleyParser (Petrov et al., 2006) for
constituency-based parsing.
For MaltParser and MSTParser, we use the best
settings from a benchmarking of parsers for French
(Candito et al., 2010b), except that we remove un-
supervised word clusters as features. The parsing
models are thus trained using features including pre-
dicted part-of-speech tags, lemmas and morpholog-
ical features. For BohnetParser, we trained a new
model using these same predicted features. For
BerkelyParser, which was included in the bench-
marking experiments, we trained a model using the
so-called “desinflection” process that addresses data
sparseness due to morphological variation: both
at training and parsing time, terminal symbols are
word forms in which redundant morphological suf-
fixes are removed, provided the original part-of-
speech ambiguities are kept (Candito et al., 2010b).
All models are trained on the French Treebank
(FTB) (Abeill´e and Barrier, 2004), consisting of
12,351 sentences from the Le Monde newspaper, ei-
ther “desinflected” for the BerkeleyParser, or con-
verted to projective dependency trees (Candito et al.,
2010a) for the three dependency-native parsers.2 For
c ∈ Cd
• Update T{gov(d) ← ˆc}
OUTPUT: Corrected version of parse tree T
Figure 2: The parse correction algorithm.
the dependency-native models, features include pre-
dicted part-of-speech (POS) tags from the MElt tag-
ger (Denis and Sagot, 2009), as well as predicted
lemmas and morphological features from the Lefff
lexicon (Sagot, 2010). These models constitute the
state-of-the-art for French dependency parsing: un-
labeled attachment scores (UAS) on the FTB test set
are 89.78% for MaltParser, 91.04% for MSTParser,
91.78% for BohnetParser, and 90.73% for Berkeley-
Parser.
4 Parse Correction
The parse correction algorithm is a post-processing
step to dependency parsing, where attachments from
the predicted parse tree of a sentence are corrected
by considering alternative candidate governors for
each dependent. This process can be useful for at-
tachments made too early in transition-based pars-
ing, or with features that are too local in MST-based
parsing.
The input is the predicted parse T of a sentence.
From T a set D of dependent nodes are chosen for
attachment correction. For each d E D in left-to-
right sentence order, a set Cd of candidate governors
from T is identified, and then the highest scoring
c E Cd, using a function S(c,d,T), is assigned as
the new governor of d in T. Pseudo-code for parse
correction is shown in Figure 2.3
3Contrary to Hall and Nov´ak (2005), our iterative algorithm
(along with the fact that Cd never includes nodes that are domi-
nated by d) ensures that corrected structures are trees, so it does
not require additional processing to eliminate cycles and pre-
serve connectivity.
2The projectivity constraint is linguistically valid for most
French parses: the authors report &lt; 2% non-projective edges in
a hand-corrected subset of the converted FTB.
1225
</bodyText>
<subsectionHeader confidence="0.999342">
4.1 Choosing Dependents
</subsectionHeader>
<bodyText confidence="0.99999">
Various criteria may be used to choose the set D
of dependents to correct. In the work of Hall and
Nov´ak (2005) and of Attardi and Ciaramita (2007),
D contains all nodes in the input parse tree. How-
ever, one advantage of parse correction is its ability
to focus on specific attachment types, so an addi-
tional criterion for choosing dependents is to look
separately at those dependents that correspond to
difficult attachment types.
Analyzing errors made by the dependency parsers
introduced in Section 3 on the development set of
the FTB, we observe that two major sources of er-
ror across different parsers are coordination and pp-
attachment. Coordination accounts for around 10%
of incorrect attachments and has an error rate rang-
ing from 30 − 40%, while pp-attachment accounts
for around 30% of incorrect attachments and has an
error rate of around 15%.
In this paper, we pay special attention to coordina-
tion and pp-attachment. Given the FTB annotation
scheme, coordination can be corrected by changing
the governor (first conjunct) of the coordinating con-
junction that governs the second conjunct, and pp-
attachment can be corrected by changing the gover-
nor of the preposition that heads the pp.4 We thus
train specialized corrective models for when the de-
pendents are coordinating conjunctions and preposi-
tions, in addition to a generic corrective model that
can be applied to any dependent.5
</bodyText>
<subsectionHeader confidence="0.993323">
4.2 Identifying Candidate Governors
</subsectionHeader>
<bodyText confidence="0.978561914285714">
The set of candidate governors Cd for a dependent
d can be chosen in different ways. One method is
to let every other node in T be a candidate gover-
nor for d. However, parser error analysis has shown
that errors often occur in local contexts. Hall and
Nov´ak (2005) define a neighborhood as a set of
nodes Nm(d) around the original predicted gover-
nor co of d, where Nm(d) includes all nodes in the
4The FTB handles pp-attachment in a typical fashion, but
coordination may be handled differently by other schemes (e.g.
the coordinating conjunction governs both conjuncts).
5In our experiments, we never revise punctuation and clitic
dependents. Since punctuation attachments mostly carry little
meaning, they are often annotated inconsistently and ignored
in parsing evaluations (including ours). Clitics are not revised
because they have a very low attachment error rate (2%).
parse tree T within graph distance m of d that pass
through co. They find that around 2/3 of the incor-
rect attachments in the output of Czech parses can be
corrected by selecting the best governor from within
N3(d). Similarly, in oracle experiments reported in
section 6, we find that around 1/2 of coordination
and pp-attachments in the output of French parses
can be corrected by selecting the best governor from
within N3(d). We thus use neighborhoods to delimit
the set of candidate governors.
While one can simply assign Cd �-- Nm(d), we
add additional restrictions. First, in order to preserve
projectivity within T, we keep in Cd only those c
such that the update T{gov(d) �-- c} would result
in a projective tree.6 Additionally, we discard candi-
dates with certain POS categories that are very un-
likely to be governors: clitics and punctuation are
always discarded, while determiners are discarded if
the dependent is a preposition.
</bodyText>
<subsectionHeader confidence="0.999259">
4.3 Scoring Candidate Governors
</subsectionHeader>
<bodyText confidence="0.9999245">
A new governor cˆ for a dependent d is predicted by
selecting the highest scoring candidate c E Cd ac-
cording to a function S(c, d, T), which takes into
account features over c, d, and the parse tree T. We
use a linear model for our scoring function, which
allows for relatively fast training and prediction. Our
scoring function uses a weight vector w� E F, where
F is the feature space for dependents we wish to cor-
rect (either generic, or specialized for prepositions
or for coordinating conjunction), as well as the map-
ping 4) : CxDxT -+ F from combinations of candi-
date c E C, dependent d E D, and parse tree T E T,
to vectors in the feature space F. The scoring func-
tion returns the inner product of w� and 4)(c, d, T):
</bodyText>
<equation confidence="0.989362">
S(c,d,T) = w� · 4)(c, d, T) (1)
</equation>
<subsectionHeader confidence="0.994267">
4.4 Algorithm Complexity
</subsectionHeader>
<bodyText confidence="0.862898">
The time complexity of our algorithm is O(n) in
the length n of the input sentence, which is consis-
tent with past work on parse correction by Hall and
Nov´ak (2005) and by Attardi and Ciaramita (2007).
6We also keep candidates that would lead to a non-projective
tree, as long as it would be projective if we ignored punctuation.
This relaxation of the projectivity constraint leads to better or-
acle scores while retaining the key linguistic properties of pro-
jectivity.
</bodyText>
<page confidence="0.965645">
1226
</page>
<bodyText confidence="0.999938555555556">
Attachments for up to n dependents in a sentence
are deterministically corrected in one pass. For each
such dependent d, the algorithm uses a linear model
to select a new governor after extracting features for
a local set of candidate governors Cd, whose size
does not dependent on n in the average case.7 Lo-
cality in candidate governor identification and fea-
ture extraction preserves linear time complexity in
the overall algorithm.
</bodyText>
<sectionHeader confidence="0.980667" genericHeader="method">
5 Model Learning
</sectionHeader>
<bodyText confidence="0.999965">
We now discuss our training setup, features, and
learning approach for obtaining the weight vector w.
</bodyText>
<subsectionHeader confidence="0.980985">
5.1 Training Setup
</subsectionHeader>
<bodyText confidence="0.999783142857143">
The parse correction training set pairs gold parse
trees with corresponding predicted parse trees out-
put by a syntactic parser, and it is obtained us-
ing a jackknifing procedure to automatically parse
the gold-annotated training section of a dependency
treebank with a syntactic dependency parser.
We extract separate training sets for each type of
dependent we wish to correct (generic, prepositions,
coordinating conjunctions). Given p, then for each
token d we wish to correct in a sentence in the train-
ing section, we note its true governor gd in the gold
parse tree of the sentence, identify a set of candidate
governors Cd in the predicted parse T, and get fea-
ture vectors 14)(c, d, T) : c E Cd}.
</bodyText>
<subsectionHeader confidence="0.997652">
5.2 Feature Space
</subsectionHeader>
<bodyText confidence="0.999733545454545">
In order to learn an effective scoring function, we
use a rich feature space F that encodes syntactic con-
text surrounding a candidate-dependent pair (c, d)
within a parse tree T. Our primary features are indi-
cator functions for realizations of linguistic or tree-
based feature classes.8 From these primary features
we generate more complex feature combinations of
length up to P, which are then added to F. Each
combo represents a set of one or more primary fea-
tures, and is an indicator function that fires if and
only if all of its members do.
</bodyText>
<footnote confidence="0.913907666666667">
7Degenerate parse trees (e.g. flat trees) could lead to cases
where JCdJ=n, but for linguistically coherent parse trees JCdJ is
rather O(km), where k is the average -arity of syntactic parse
trees and m is the neighborhood distance used.
8For instance, there is a binary feature that is 1 if feature
class ”POS of c” takes on the value ”verb”, and 0 otherwise.
</footnote>
<subsectionHeader confidence="0.452434">
5.2.1 Primary Feature Classes
</subsectionHeader>
<bodyText confidence="0.999826666666667">
The primary feature classes we use are listed be-
low, grouped into categories corresponding to their
use in different corrective models (dobe is the object
of the dependent, cyo„ is the governor of the candi-
date, and cd−1 and cd+1 are the closest dependents
of c linearly to the left and right, respectively, of d).
</bodyText>
<table confidence="0.289971181818182">
Generic features (always included)
− POS, lemma, and number of dependents of c
− POS and dependency label of cd−1
− POS and dependency label of cd+1
− POS of cyo„
− POS and lemma of d
− POS of dobe and whether dobe has a determiner
− Whether c is the predicted governor of d
− Binned linear distance between c and d
− Linear direction of c with respect to d
− POS sequence for nodes on path from c to d
− Graph distance between c and d
− Whether there is punctuation between c and d
Features exclusive to coordination
Whether d would coordinate two conjuncts that:
− Have the same POS
− Have the same word form
− Have number agreement
− Are both nouns with the same cardinality
− Are both proper nouns or both common nouns
− Are both prepositions with the same word form
− Are both prepositions with object of same POS
</table>
<subsectionHeader confidence="0.337939">
Features exclusive to pp-attachment
</subsectionHeader>
<bodyText confidence="0.953429125">
− Whether d immediately follows a punctuation
− Whether d heads a pp likely to be the agent of
a passive verb
− If c is a coordinating conjunction, then whether
c would coordinate two prepositions with the
same word form, and whether there is at least
one open-category word linearly between c and
d (in which case c is an unlikely governor)
</bodyText>
<page confidence="0.968923">
1227
</page>
<bodyText confidence="0.9997546">
− If c is linearly after d, then whether there exists
a plausible rival candidate to the left of d (im-
plemented as whether there is a noun or adjec-
tive linearly before d, without any intervening
finite verb)
</bodyText>
<subsubsectionHeader confidence="0.678456">
5.2.2 Feature Selection
</subsubsectionHeader>
<bodyText confidence="0.999931052631579">
Feature combos allow our models to effectively
sidestep linearity constraints, at the cost of an expo-
nential increase in the size of the feature space F. In
order to accommodate combos, we use feature se-
lection to help reduce the resulting space.
Our first feature selection technique is to apply a
frequency threshold: if a feature or a combo appears
less than K times among instances in our training
set, we remove it from F. In addition to making the
feature space more tractable, frequency thresholding
makes our scoring function less reliant on rare fea-
tures and combos.
Following frequency thresholding, we employ an
additional technique using conditional entropy (CE)
that we term CE-reduction. Let Y be a random vari-
able for whether or not an attachment is true, and
let A be a random variable for different combos that
can appear in an attachment. We calculate the CE of
a combo a with respect to Y as follows,
</bodyText>
<equation confidence="0.9905295">
H(Y |A=a) = − � p(y|a) log p(y|a) (2)
y∈Y
</equation>
<bodyText confidence="0.9999774">
where the probability p(y|a) is approximated from
the training set as freq(a, y)/freq(a), with exam-
ple balancing used here to account for more false at-
tachments (Y = 0) than true ones (Y = 1) in our train-
ing set. Having calculated the CE of each combo,
we remove from F those combos for which a subset
combo (or feature) exists with equal or lesser CE.
This eliminates any overly specific combo a when
the extra features encoded in a, compared to some
subset b, do not help a explain Y any better than b.
</bodyText>
<subsectionHeader confidence="0.976442">
5.3 Ranking Model
</subsectionHeader>
<bodyText confidence="0.999582857142857">
The ranking setting for learning is used when a
model needs to discriminate between mutually ex-
clusive candidates that vary from instance to in-
stance. This is typically used in parse reranking
(Charniak and Johnson, 2005), where for each sen-
tence the model must select the correct parse from
within an n-best list. Denis and Baldridge (2007)
</bodyText>
<table confidence="0.602173666666667">
INPUT: Aggressiveness C, rounds R.
INITIALIZE: ~w0 ←(0,..., 0), ~wavg ←(0,..., 0)
REPEAT: R times
LOOP: Fort = 1, 2,.. . ,|X|
· Get feature vectors {~xt,c : c ∈ Cdt}
· Get true governor gt ∈ Cdt
</table>
<equation confidence="0.47524475">
· Let ht = argmax (~wt−1 · ~xt,c)
c∈Cdt −{gt}
· Let mt = (~wt−1 · ~xt,gt) − (~wt−1 · ~xt,ht)
IF: mt &lt; 1
· Let τt = min{ C , 1−mt
k~xt,gt−~xt,
· Set ~wt ← ~wt−1 + τt(~xt,gt − ~xt,ht)
ELSE:
· Set ~wt ← ~wt−1
· Set ~wavg ← ~wavg + ~wt
· Set ~w0 ← ~w|X|
OUTPUT: ~wavg/(R · |X|)
</equation>
<figureCaption confidence="0.999509">
Figure 3: Averaged PA-Ranking training algorithm.
</figureCaption>
<bodyText confidence="0.89982028">
also show that ranking outperforms a binary classifi-
cation approach to pronoun resolution (using a Max-
imum Entropy model), where for each pronominal
anaphor the model must select the correct antecedent
among candidates in a text.9
In our ranking approach to parse correction (PA-
Ranking), the weight vector is trained to select the
true governor from a set of candidates Cd for a de-
pendent d. The training set X is defined such that
the tth instance is a collection of feature vectors
{~xt,c = 4,(c, dt, Tt) : c ∈ Cdt}, where Cdt is the
candidate set for the dependent dt within the pre-
dicted parse Tt, and the class is the true governor gt.
Instances in which gt ∈6 Cdt are discarded.
PA-Ranking training is carried out using a varia-
tion of the Passive-Aggressive algorithm (Crammer
et al., 2006), which has been adapted to the rank-
ing setting, implemented using the Polka library.10
For each training iteration t, the margin is defined as
9We considered a binary training approach to parse correc-
tion in which the model is trained to independently classify can-
didates as true or false governors, as used by Hall and Nov´ak
(2005). However, we found that this approach performs no bet-
ter (and often worse) than the ranking approach, and is less ap-
propriate from a modeling standpoint.
</bodyText>
<equation confidence="0.6738276">
10http://polka.gforge.inria.fr/
I
htk2
1228
mt = (19t−1 · xt,gt) − (19t−1 · xt,ht), where ht is the
</equation>
<bodyText confidence="0.999799571428571">
highest scoring incorrect candidate. The algorithm
is passive because an update to the weight vector is
made if and only if mt &lt; 1, either for incorrect pre-
dictions (mt &lt; 0) or for correct predictions with in-
sufficient margin (0≤mt &lt;1). The new weight vec-
tor wt is as close as possible to wt−1, subject to the
aggressive constraint that the new margin be greater
than 1. We use weight averaging, so the final out-
put wavg is the average over the weight vectors after
each training step. Pseudo-code for the training al-
gorithm is shown in Figure 3. The rounds parameter
R determines the number of times to run through the
training set, and the aggressiveness parameter C sets
an upper limit on the update magnitude.
</bodyText>
<sectionHeader confidence="0.999801" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999964769230769">
We present experiments where we applied parse cor-
rection to the output of four state-of-the-art depen-
dency parsers for French. We conducted our eval-
uation on the FTB using the standard training, de-
velopment (dev), and test splits (containing 9,881,
1,235 and 1,235 sentences, respectively). To train
our parse correction models, we generated special-
ized training sets corresponding to each parser by
doing 10-fold jackknifing on the FTB training set
(cf. Section 5.1). Each parser was run on the FTB
dev and test sets, providing baseline unlabeled at-
tachment score (UAS) results and output parse trees
to be corrected.
</bodyText>
<subsectionHeader confidence="0.994704">
6.1 Oracles and Neighborhood Size
</subsectionHeader>
<bodyText confidence="0.999938533333333">
To determine candidate neighborhood size, we con-
sidered an oracle scoring function that always se-
lects the true governor of a dependent if it appears
in the set of candidate governors, and otherwise se-
lects the predicted governor. Results for this oracle
on the dev set are shown in Table 1. The baseline
corresponds to m=1, where the oracle just selects
the predicted governor. Incrementing m to 2 and
to 3 resulted in substantial gains in oracle UAS, but
further incrementing m to 4 resulted in a relatively
small additional gain. We found that average can-
didate set size increases about linearly in m, so we
decided to use m=3 in order to have a high UAS up-
per bound without adding candidates that are very
unlikely to be true governors.
</bodyText>
<table confidence="0.999848733333333">
Neighborhood Size (m)
Base 2 3 4
Berkeley Coords 67.2 76.5 82.8 84.8
Preps 82.9 88.5 92.2 93.2
Overall 90.1 94.0 96.0 96.5
Bohnet Coords 70.1 80.6 85.6 87.7
Preps 85.4 89.4 93.4 94.5
Overall 91.2 94.4 96.1 96.6
Malt Coords 60.9 72.2 78.2 80.5
Preps 82.6 88.1 92.6 93.7
Overall 89.3 93.2 95.1 95.8
MST Coords 63.6 73.7 80.7 84.4
Preps 84.7 89.4 93.4 94.4
Overall 90.2 93.7 95.6 96.2
MST Overall Reranking top-100 parses: 95.4
</table>
<tableCaption confidence="0.9900885">
Table 1: Parse correction oracle UAS (%) for differ-
ent neighborhood sizes, by dependent type (coordinating
conjunctions, prepositions, or all dependents). Also, a
reranking oracle for MSTParser using the top-100 parses.
</tableCaption>
<bodyText confidence="0.999952">
We also compared the oracle for parse correc-
tion with an oracle for parse reranking, in which the
parse with the highest UAS for a sentence is selected
from the top-100 parses output by MSTParser. We
found that for MSTParser, the oracle for parse cor-
rection using neighborhood size m=3 (95.6% UAS)
is comparable to the oracle for parse reranking using
the top-100 parses (95.4% UAS). This is an encour-
aging result, showing that parse correction is capable
of the same improvement as parse reranking without
needing to process an n-best list of parses.
</bodyText>
<subsectionHeader confidence="0.999812">
6.2 Feature Space Parameters
</subsectionHeader>
<bodyText confidence="0.999978866666667">
For the feature space F, we performed a grid search
to find good values for the parameters K (frequency
threshold), P (combo length), and CE-reduction.
We found that P=3 with CE-reduction allowed for
the most compactness without sacrificing correction
performance, for all of our corrective models. Ad-
ditionally, K=2 worked well for the coordinating
conjunction models, while K=10 worked well for
the preposition and generic models. CE-reduction
proved useful in greatly reducing the feature space
without lowering correction performance: it reduced
the size of the coordinating conjunction models from
400k to 65k features each, the preposition models
from 400k to 75k features each, and the generic
models from 800k to 200k features each.
</bodyText>
<page confidence="0.976848">
1229
</page>
<table confidence="0.999717733333333">
Corrective UAS (%)
Configuration
Coords Preps Overall
Berkeley Baseline 68.3 83.8 90.73
Generic 69.4 84.9* 91.13*
Specialized 71.5* 85.1* 91.23*
Bohnet Baseline 70.5 86.1 91.78
Generic 71.2 86.4 91.88
Specialized 72.7* 86.2 91.88
Malt Baseline 59.8 83.2 89.78
Generic 63.2* 84.5* 90.39*
Specialized 64.0* 85.0* 90.47*
MST Baseline 60.5 85.9 91.04
Generic 64.2* 86.2 91.25*
Specialized 68.0* 86.2 91.36*
</table>
<tableCaption confidence="0.970852">
Table 2: Coordinating conjunction, preposition, and over-
all UAS (%) by corrective configuration on the test set.
Significant improvements over the baseline starred.
</tableCaption>
<subsectionHeader confidence="0.99644">
6.3 Corrective Configurations
</subsectionHeader>
<bodyText confidence="0.999994176470588">
For our evaluation of parse correction, we compared
two different configurations: generic (corrects all
dependents using the generic model) and specialized
(corrects coordinating conjunctions and prepositions
using their respective specialized models, and cor-
rects other dependents using the generic model).
The PA-Ranking aggressiveness parameter C was
set to 1 for our experiments, while the rounds pa-
rameter R was tuned separately for each corrective
model using the dev set. For our final tests, we ap-
plied each combination of parser + corrective con-
figuration by sequentially revising all dependents in
the output parse that had a relevant POS tag given
the corrective configuration. In the FTB test set,
this amounted to an evaluation over 5,706 prepo-
sition tokens, 801 coordinating conjunction tokens,
and 31,404 overall (non-punctuation) tokens.11
</bodyText>
<subsectionHeader confidence="0.752476">
6.4 Results
</subsectionHeader>
<bodyText confidence="0.987789293103449">
Final results for the test set are shown in Table 2.
The overall UAS of each parser (except Bohnet-
Parser) was significantly improved under both cor-
rective configurations.12 The specialized configura-
11Since the MElt tagger and BerkeleyParser POS tagging ac-
curacies were around 97%, the sets of tokens considered for re-
vision differed slightly from the sets of tokens (with gold POS
tags) used to calculate UAS scores.
12We used McNemar’s Chi-squared test with p = 0.05 for all
significance tests.
tion performed as well as, and in most cases bet-
ter than, the generic configuration, indicating the
usefulness of specialized models and features for
difficult attachment types. Interestingly, the lower
the baseline parser’s UAS, the larger the overall
improvement from parse correction under the spe-
cialized configuration: MaltParser had the lowest
baseline and the highest error reduction (6.8%),
BerkeleyParser had the second-lowest baseline and
the second-highest error reduction (5.4%), MST-
Parser had the third-lowest baseline and the third-
highest error reduction (3.6%), and BohnetParser
had the highest baseline and the lowest error re-
duction (1.2%). It may be that the additional er-
rors made by a low-baseline parser, compared to a
high-baseline parser, involve relatively simpler at-
tachments that parse correction can better model.
Parse correction achieved significant improve-
ments for coordination resolution under the spe-
cialized configuration for each parser. MaltParser
and MSTParser had very low baseline coordinat-
ing conjunction UAS (around 60%), while Berke-
leyParser and BohnetParser had higher baselines
(around 70%). The highest error reduction was
achieved by MSTParser (19.0%), followed by Malt-
Parser (10.4%), BerkeleyParser (10.1%), and finally
BohnetParser (7.5%). The result for MSTParser was
surprising: although it had the second-highest base-
line overall UAS, it shared the lowest baseline coor-
dinating conjunction UAS and had the highest er-
ror reduction with parse correction. An explana-
tion for this result is that the annotation scheme for
coordination structure in the dependency FTB has
the first conjunct governing the coordinating con-
junction, which governs the second conjunct. Since
MSTParser is limited to sibling 2-edge factors (cf.
section 3), it is unable to jointly consider a full coor-
dination structure. BohnetParser, which uses general
2-edge factors, can consider full coordination struc-
tures and consequently has a much higher baseline
coordinating conjunction UAS than MSTParser.
Parse correction achieved significant but mod-
est improvements in pp-attachment performance un-
der the specialized configuration for MaltParser and
BerkeleyParser. However, parse correction did not
significantly improve pp-attachment performance
for MSTParser or BohnetParser, the two parsers that
had the highest baseline preposition UAS (around
</bodyText>
<page confidence="0.941296">
1230
</page>
<table confidence="0.999925214285714">
Modification Type
w-+c c-+w w-+w Mods
Berkeley Coords 40 14 33 10.9 %
Preps 118 39 41 3.5 %
Overall 228 67 104 1.3 %
Bohnet Coords 32 15 33 10.0 %
Preps 52 46 32 2.3 %
Overall 150 121 130 1.1 %
Malt Coords 55 21 56 16.5 %
Preps 149 50 76 4.8 %
Overall 390 172 293 2.4 %
MST Coords 80 20 51 18.9 %
Preps 64 45 26 2.4 %
Overall 183 88 117 1.1 %
</table>
<tableCaption confidence="0.9797408">
Table 3: Breakdown of modifications made under the
specialized configuration for each parser, by dependent
type. w-+c is wrong-to-correct, c-+w is correct-to-
wrong, w-+w is wrong-to-wrong, and Mods is the per-
centage of tokens modified.
</tableCaption>
<bodyText confidence="0.999951605263158">
86%). These results are a bit disappointing, but they
suggest that there may be a performance ceiling for
pp-attachment beyond which rich lexical informa-
tion (syntactic and semantic) or full sentence con-
texts are needed. For English, the average human
performance on pp-attachment for the (v, n1, p, n2)
problem formulation is just 88.2% when given only
the four head-words, but increases to 93.2% when
given the full sentence (Ratnaparkhi et al., 1994).
If similar levels of human performance exist for
French, additional sources of information may be
needed to improve pp-attachment performance.
In addition to evaluating UAS improvements for
parse correction, we took a closer look at the best
corrective configuration (specialized) and analyzed
the types of attachment modifications made (Ta-
ble 3). In most cases there were around 2−3 times
as many error-correcting modifications (w-+c) as
error-creating modifications (c-+w), and the overall
% of tokens modified was very low overall (around
1-2%). Parse correction is thus conservative in the
number of modifications made, and rather accurate
when it does decide to modify an attachment.
Finally, we compared the running times of the
four parsers, as well as that of parse correction, on
the test set using a 2.66 GHz Intel Core 2 Duo ma-
chine. BerkeleyParser took 600s, BohnetParser took
450s using both cores (800s using a single core),
MaltParser took 45s, and MSTParser took 1000s. A
rough version of parse correction in the specialized
configuration took around 200s (for each parser). An
interesting result is that parse correction improves
MaltParser the most while retaining an overall time
complexity of O(n), compared to O(n3) or higher
for the other parsers. This suggests that linear-time
transition-based parsing and parse correction could
combine to form an attractive system that improves
parsing performance while retaining high speed.
</bodyText>
<sectionHeader confidence="0.998866" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999978620689655">
We have developed a parse correction framework for
syntactic dependency parsing that uses specialized
models for difficult attachment types. Candidate
governors for a given dependent are identified in a
neighborhood around the predicted governor, and a
scoring function selects the best governor. We used
discriminative linear ranking models with features
encoding syntactic context, and we tested parse cor-
rection on coordination, pp-attachment, and generic
dependencies in the outputs of four representative
statistical dependency parsers for French. Parse cor-
rection achieved improvements in unlabeled attach-
ment score for three out of the four parsers, with
MaltParser seeing the greatest improvement. Since
both MaltParser and parse correction run in O(n)
time, a combined system could prove useful in situ-
ations where high parsing speed is required.
Future work on parse correction might focus on
developing specialized models for other difficult
attachment types, such as verb-phrase attachment
(verb dependents account for around 15% of incor-
rect attachments across all four parsers). Also, se-
lectional preferences and subcategorization frames
(from hand-built resources or extracted using distri-
butional methods) could make for useful features in
the pp-attachment corrective model; we suspect that
richer lexical information is needed in order to in-
crease the currently modest improvements achieved
by parse correction on pp-attachment.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9990995">
We would like to thank Pascal Denis for his help us-
ing the Polka library, and Alexis Nasr for his advice
and comments. This work was partially funded by
the ANR project Sequoia ANR-08-EMER-013.
</bodyText>
<page confidence="0.988443">
1231
</page>
<sectionHeader confidence="0.996351" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915742857143">
A. Abeill´e and N. Barrier. 2004. Enriching a French
treebank. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
Lisbon, Portugal, May.
G. Attardi and M. Ciaramita. 2007. Tree revision learn-
ing for dependency parsing. In Proceedings of the
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 388–
395, Rochester, New York, April.
G. Attardi and F. Dell’Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proceedings of the 2009 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 261–264, Boulder, Colorado,
June.
M. Atterer and H. Sch¨utze. 2007. Prepositional phrase
attachment without oracles. Computational Linguis-
tics, 33(4):469–476.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 89–97, Beijing, China, August.
M. Candito, B. Crabb´e, and P. Denis. 2010a. Statistical
French dependency parsing: Treebank conversion and
first results. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation, Valetta, Malta, May.
M. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010b. Benchmarking of statistical depen-
dency parsers for French. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 108–116, Beijing, China, August.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session ofEMNLP-CoNLL, pages
957–961, Prague, Czech Republic, June.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 173–180,
Ann Arbor, Michigan, June.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25–70.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. The Journal of Machine Learning Research,
7:551–585.
P. Denis and J. Baldridge. 2007. A ranking approach
to pronoun resolution. In Proceedings of the 20th In-
ternational Joint Conference on Artifical intelligence,
pages 1588–1593, Hyderabad, India, January.
P. Denis and B. Sagot. 2009. Coupling an annotated cor-
pus and a morphosyntactic lexicon for state-of-the-art
POS tagging with less human effort. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation, Hong Kong, China, De-
cember.
J.M. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of the 16th conference on Computational linguistics-
Volume 1, pages 340–345, Santa Cruz, California, Au-
gust.
K. Hall and V. Nov´ak. 2005. Corrective modeling for
non-projective dependency parsing. In Proceedings
of the Ninth International Workshop on Parsing Tech-
nologies, pages 42–52, Vancouver, British Columbia,
October.
K. Hara, M. Shimbo, H. Okuma, and Y. Matsumoto.
2009. Coordinate structure analysis with global struc-
tural constraints and alignment-based local features.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 967–975, Suntec, Singapore, Au-
gust.
D. Hogan. 2007. Coordinate noun phrase disambigua-
tion in a generative parsing model. In In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, number 1, page 680,
Prague, Czech Republic, June.
R. Johansson and P. Nugues. 2006. Investigating
multilingual dependency parsing. In Proceedings of
the Tenth Conference on Computational Natural Lan-
guage Learning, pages 206–210, New York City, New
York, June.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1–11, Uppsala, Sweden, July.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 75–82, Ann Arbor, Michigan,
June.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 81–88, Trento, Italy, April.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 91–98, Ann
Arbor, Michigan, June.
</reference>
<page confidence="0.830601">
1232
</page>
<reference confidence="0.999604204081633">
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02):95–135.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies, pages 149–
160, Nancy, France, April.
M. Olteanu and D. Moldovan. 2005. PP-attachment dis-
ambiguation using large context. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 273–280, Vancouver, British Columbia, Octo-
ber.
P. Pantel and D. Lin. 2000. An unsupervised approach
to prepositional phrase attachment using contextually
similar words. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics, volume 38, pages 101–108, Hong Kong, October.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 433–440, Sydney, Australia, July.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A max-
imum entropy model for prepositional phrase attach-
ment. In Proceedings of the Workshop on Human Lan-
guage Technology, pages 250–255, Plainsboro, New
Jersey, March.
P. Resnik. 1999. Semantic similarity in a taxonomy: An
information-based measure and its application to prob-
lems of ambiguity in natural language. Journal ofAr-
tificial Intelligence Research, 11(95):130.
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceedings
of the Seventh International Conference on Language
Resources and Evaluation, Valetta, Malta, May.
M. Shimbo and K. Hara. 2007. A discriminative learn-
ing model for coordinate conjunctions. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 610–619, Prague,
Czech Republic, June.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of the 8th International Workshop on Parsing
Technologies, pages 195–206, Nancy, France, April.
</reference>
<page confidence="0.973796">
1233
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.386051">
<title confidence="0.999965">Parse Correction with Specialized Models for Difficult Attachment Types</title>
<author confidence="0.978329">Enrique Henestroza Anguiano</author>
<author confidence="0.978329">Marie</author>
<affiliation confidence="0.870749">Alpage (Universit´e Paris Diderot /</affiliation>
<address confidence="0.506799">Paris,</address>
<email confidence="0.996922">henestro@inria.fr,marie.candito@linguist.jussieu.fr</email>
<abstract confidence="0.991154409090909">This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by several representative state-of-the-art dependency parsers for French.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeill´e</author>
<author>N Barrier</author>
</authors>
<title>Enriching a French treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation,</booktitle>
<location>Lisbon, Portugal,</location>
<marker>Abeill´e, Barrier, 2004</marker>
<rawString>A. Abeill´e and N. Barrier. 2004. Enriching a French treebank. In Proceedings of the Fourth International Conference on Language Resources and Evaluation, Lisbon, Portugal, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Attardi</author>
<author>M Ciaramita</author>
</authors>
<title>Tree revision learning for dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>388--395</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="2841" citStr="Attardi and Ciaramita (2007)" startWordPosition="422" endWordPosition="425">rk related to parse correction, pp-attachment and coordination resolution. In Section 3 we discuss dependency structure and various statistical dependency parsing approaches. In Section 4 we introduce the parse correction framework, and Section 5 describes the features and learning model used in our implementation. In Section 6 we present experiments in which parse correction revises the predicted parse trees of four state-of-the-art dependency parsers for French. We provide concluding remarks in Section 7. 2 Related Work Previous research directly concerning parse correction includes that of Attardi and Ciaramita (2007), working on English and Swedish, who use an approach that considers a fixed set of revision rules: each rule describes movements in the parse tree leading from a dependent’s original governor to a new governor, and a classifier is trained to select the correct revision rule for a given dependent. One drawback of this approach is that the classes lack semantic coherence: a sequence of movements does not necessarily have the same meaning across differ1222 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1222–1233, Edinburgh, Scotland, UK, July 27–31,</context>
<context position="16494" citStr="Attardi and Ciaramita (2007)" startWordPosition="2585" endWordPosition="2588">re 2.3 3Contrary to Hall and Nov´ak (2005), our iterative algorithm (along with the fact that Cd never includes nodes that are dominated by d) ensures that corrected structures are trees, so it does not require additional processing to eliminate cycles and preserve connectivity. 2The projectivity constraint is linguistically valid for most French parses: the authors report &lt; 2% non-projective edges in a hand-corrected subset of the converted FTB. 1225 4.1 Choosing Dependents Various criteria may be used to choose the set D of dependents to correct. In the work of Hall and Nov´ak (2005) and of Attardi and Ciaramita (2007), D contains all nodes in the input parse tree. However, one advantage of parse correction is its ability to focus on specific attachment types, so an additional criterion for choosing dependents is to look separately at those dependents that correspond to difficult attachment types. Analyzing errors made by the dependency parsers introduced in Section 3 on the development set of the FTB, we observe that two major sources of error across different parsers are coordination and ppattachment. Coordination accounts for around 10% of incorrect attachments and has an error rate ranging from 30 − 40%</context>
<context position="20610" citStr="Attardi and Ciaramita (2007)" startWordPosition="3284" endWordPosition="3287"> F, where F is the feature space for dependents we wish to correct (either generic, or specialized for prepositions or for coordinating conjunction), as well as the mapping 4) : CxDxT -+ F from combinations of candidate c E C, dependent d E D, and parse tree T E T, to vectors in the feature space F. The scoring function returns the inner product of w� and 4)(c, d, T): S(c,d,T) = w� · 4)(c, d, T) (1) 4.4 Algorithm Complexity The time complexity of our algorithm is O(n) in the length n of the input sentence, which is consistent with past work on parse correction by Hall and Nov´ak (2005) and by Attardi and Ciaramita (2007). 6We also keep candidates that would lead to a non-projective tree, as long as it would be projective if we ignored punctuation. This relaxation of the projectivity constraint leads to better oracle scores while retaining the key linguistic properties of projectivity. 1226 Attachments for up to n dependents in a sentence are deterministically corrected in one pass. For each such dependent d, the algorithm uses a linear model to select a new governor after extracting features for a local set of candidate governors Cd, whose size does not dependent on n in the average case.7 Locality in candida</context>
</contexts>
<marker>Attardi, Ciaramita, 2007</marker>
<rawString>G. Attardi and M. Ciaramita. 2007. Tree revision learning for dependency parsing. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, pages 388– 395, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Attardi</author>
<author>F Dell’Orletta</author>
</authors>
<title>Reverse revision and linear tree combination for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>261--264</pages>
<location>Boulder, Colorado,</location>
<marker>Attardi, Dell’Orletta, 2009</marker>
<rawString>G. Attardi and F. Dell’Orletta. 2009. Reverse revision and linear tree combination for dependency parsing. In Proceedings of the 2009 Conference of the North American Chapter of the Association for Computational Linguistics, pages 261–264, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Atterer</author>
<author>H Sch¨utze</author>
</authors>
<title>Prepositional phrase attachment without oracles.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>Atterer, Sch¨utze, 2007</marker>
<rawString>M. Atterer and H. Sch¨utze. 2007. Prepositional phrase attachment without oracles. Computational Linguistics, 33(4):469–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<contexts>
<context position="13307" citStr="Bohnet, 2010" startWordPosition="2086" endWordPosition="2087">t al., 2006) that has parsing time complexity O(n3) (Matsuzaki et al., 2005); (ii) A functional role labeler using a Maximum Entropy model adds functional annotations to links between a verb and its dependents; (iii) Constituency trees are automatically converted into projective dependency trees, with remaining unlabeled dependencies assigned labels using a rule-based approach. 3.4 Baseline Parsers In this paper, we use the following baseline parsers: MaltParser (Nivre et al., 2007) for transition-based parsing; MSTParser (McDonald et al., 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al., 2006) for constituency-based parsing. For MaltParser and MSTParser, we use the best settings from a benchmarking of parsers for French (Candito et al., 2010b), except that we remove unsupervised word clusters as features. The parsing models are thus trained using features including predicted part-of-speech tags, lemmas and morphological features. For BohnetParser, we trained a new model using these same predicted features. For BerkelyParser, which was included in the benchmarking experiments, we trained </context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>B. Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 89–97, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>B Crabb´e</author>
<author>P Denis</author>
</authors>
<title>Statistical French dependency parsing: Treebank conversion and first results.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation,</booktitle>
<location>Valetta, Malta,</location>
<marker>Candito, Crabb´e, Denis, 2010</marker>
<rawString>M. Candito, B. Crabb´e, and P. Denis. 2010a. Statistical French dependency parsing: Treebank conversion and first results. In Proceedings of the Seventh International Conference on Language Resources and Evaluation, Valetta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>J Nivre</author>
<author>P Denis</author>
<author>E Henestroza Anguiano</author>
</authors>
<title>Benchmarking of statistical dependency parsers for French.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>108--116</pages>
<location>Beijing, China,</location>
<contexts>
<context position="13554" citStr="Candito et al., 2010" startWordPosition="2120" endWordPosition="2123">automatically converted into projective dependency trees, with remaining unlabeled dependencies assigned labels using a rule-based approach. 3.4 Baseline Parsers In this paper, we use the following baseline parsers: MaltParser (Nivre et al., 2007) for transition-based parsing; MSTParser (McDonald et al., 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al., 2006) for constituency-based parsing. For MaltParser and MSTParser, we use the best settings from a benchmarking of parsers for French (Candito et al., 2010b), except that we remove unsupervised word clusters as features. The parsing models are thus trained using features including predicted part-of-speech tags, lemmas and morphological features. For BohnetParser, we trained a new model using these same predicted features. For BerkelyParser, which was included in the benchmarking experiments, we trained a model using the so-called “desinflection” process that addresses data sparseness due to morphological variation: both at training and parsing time, terminal symbols are word forms in which redundant morphological suffixes are removed, provided t</context>
</contexts>
<marker>Candito, Nivre, Denis, Anguiano, 2010</marker>
<rawString>M. Candito, J. Nivre, P. Denis, and E. Henestroza Anguiano. 2010b. Benchmarking of statistical dependency parsers for French. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 108–116, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL,</booktitle>
<pages>957--961</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="12048" citStr="Carreras, 2007" startWordPosition="1892" endWordPosition="1893">cted edges) between each pair of words (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituency1224 based parsing followed by a conversion step to de- INPUT: Predicted parse tree T pendency structure. We use the three-step parsing LOOP: For each chosen dependent d E D architecture previously tested for French by Candito • Identify candidates Cd from T et al. (2010a): (i) A constituency parse tree is out- • Predict cˆ = argmax S(c, d, T) put by the BerkeleyParser, which has been trained to learn a probabilistic context-</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL, pages 957–961, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5681" citStr="Charniak and Johnson (2005)" startWordPosition="870" endWordPosition="873">lowed to be rearranged in ways that introduce non-projectivity in order to increase its overall score. This rearrangement approach resembles parse correction because it is a second step that can revise attachments made in the first step, but it differs in a number of ways: it is dependent on a graph-based parsing approach, it does not model errors made by the parser, and it can only output non-projective variants of the predicted parse tree. As a process that revises the output of a syntactic parser, parse reranking is also similar to parse correction. A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. Parse correction has a number of advantages compared to reranking: it can be used with parsers that do not output n-best ranked parses, it can be easily restricted to specific attachment types, and its output space of parse trees is not limited to those appearing in an n-best list. However, parse reranking has the advantage of selecting the globally optimal parse for a sentence from an nbest list, while parse correction makes only locally optimal revision</context>
<context position="26568" citStr="Charniak and Johnson, 2005" startWordPosition="4336" endWordPosition="4339"> here to account for more false attachments (Y = 0) than true ones (Y = 1) in our training set. Having calculated the CE of each combo, we remove from F those combos for which a subset combo (or feature) exists with equal or lesser CE. This eliminates any overly specific combo a when the extra features encoded in a, compared to some subset b, do not help a explain Y any better than b. 5.3 Ranking Model The ranking setting for learning is used when a model needs to discriminate between mutually exclusive candidates that vary from instance to instance. This is typically used in parse reranking (Charniak and Johnson, 2005), where for each sentence the model must select the correct parse from within an n-best list. Denis and Baldridge (2007) INPUT: Aggressiveness C, rounds R. INITIALIZE: ~w0 ←(0,..., 0), ~wavg ←(0,..., 0) REPEAT: R times LOOP: Fort = 1, 2,.. . ,|X| · Get feature vectors {~xt,c : c ∈ Cdt} · Get true governor gt ∈ Cdt · Let ht = argmax (~wt−1 · ~xt,c) c∈Cdt −{gt} · Let mt = (~wt−1 · ~xt,gt) − (~wt−1 · ~xt,ht) IF: mt &lt; 1 · Let τt = min{ C , 1−mt k~xt,gt−~xt, · Set ~wt ← ~wt−1 + τt(~xt,gt − ~xt,ht) ELSE: · Set ~wt ← ~wt−1 · Set ~wavg ← ~wavg + ~wt · Set ~w0 ← ~w|X| OUTPUT: ~wavg/(R · |X|) Figure 3: </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173–180, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="5711" citStr="Collins and Koo (2005)" startWordPosition="876" endWordPosition="879"> introduce non-projectivity in order to increase its overall score. This rearrangement approach resembles parse correction because it is a second step that can revise attachments made in the first step, but it differs in a number of ways: it is dependent on a graph-based parsing approach, it does not model errors made by the parser, and it can only output non-projective variants of the predicted parse tree. As a process that revises the output of a syntactic parser, parse reranking is also similar to parse correction. A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. Parse correction has a number of advantages compared to reranking: it can be used with parsers that do not output n-best ranked parses, it can be easily restricted to specific attachment types, and its output space of parse trees is not limited to those appearing in an n-best list. However, parse reranking has the advantage of selecting the globally optimal parse for a sentence from an nbest list, while parse correction makes only locally optimal revisions in the predicted parse for a</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>M. Collins and T. Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="28004" citStr="Crammer et al., 2006" startWordPosition="4607" endWordPosition="4610">ect the correct antecedent among candidates in a text.9 In our ranking approach to parse correction (PARanking), the weight vector is trained to select the true governor from a set of candidates Cd for a dependent d. The training set X is defined such that the tth instance is a collection of feature vectors {~xt,c = 4,(c, dt, Tt) : c ∈ Cdt}, where Cdt is the candidate set for the dependent dt within the predicted parse Tt, and the class is the true governor gt. Instances in which gt ∈6 Cdt are discarded. PA-Ranking training is carried out using a variation of the Passive-Aggressive algorithm (Crammer et al., 2006), which has been adapted to the ranking setting, implemented using the Polka library.10 For each training iteration t, the margin is defined as 9We considered a binary training approach to parse correction in which the model is trained to independently classify candidates as true or false governors, as used by Hall and Nov´ak (2005). However, we found that this approach performs no better (and often worse) than the ranking approach, and is less appropriate from a modeling standpoint. 10http://polka.gforge.inria.fr/ I htk2 1228 mt = (19t−1 · xt,gt) − (19t−1 · xt,ht), where ht is the highest sco</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. The Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>A ranking approach to pronoun resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artifical intelligence,</booktitle>
<pages>1588--1593</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="26688" citStr="Denis and Baldridge (2007)" startWordPosition="4357" endWordPosition="4360">of each combo, we remove from F those combos for which a subset combo (or feature) exists with equal or lesser CE. This eliminates any overly specific combo a when the extra features encoded in a, compared to some subset b, do not help a explain Y any better than b. 5.3 Ranking Model The ranking setting for learning is used when a model needs to discriminate between mutually exclusive candidates that vary from instance to instance. This is typically used in parse reranking (Charniak and Johnson, 2005), where for each sentence the model must select the correct parse from within an n-best list. Denis and Baldridge (2007) INPUT: Aggressiveness C, rounds R. INITIALIZE: ~w0 ←(0,..., 0), ~wavg ←(0,..., 0) REPEAT: R times LOOP: Fort = 1, 2,.. . ,|X| · Get feature vectors {~xt,c : c ∈ Cdt} · Get true governor gt ∈ Cdt · Let ht = argmax (~wt−1 · ~xt,c) c∈Cdt −{gt} · Let mt = (~wt−1 · ~xt,gt) − (~wt−1 · ~xt,ht) IF: mt &lt; 1 · Let τt = min{ C , 1−mt k~xt,gt−~xt, · Set ~wt ← ~wt−1 + τt(~xt,gt − ~xt,ht) ELSE: · Set ~wt ← ~wt−1 · Set ~wavg ← ~wavg + ~wt · Set ~w0 ← ~w|X| OUTPUT: ~wavg/(R · |X|) Figure 3: Averaged PA-Ranking training algorithm. also show that ranking outperforms a binary classification approach to pronoun r</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. A ranking approach to pronoun resolution. In Proceedings of the 20th International Joint Conference on Artifical intelligence, pages 1588–1593, Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>B Sagot</author>
</authors>
<title>Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less human effort.</title>
<date>2009</date>
<booktitle>In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation,</booktitle>
<location>Hong Kong, China,</location>
<contexts>
<context position="14770" citStr="Denis and Sagot, 2009" startWordPosition="2303" endWordPosition="2306">ided the original part-ofspeech ambiguities are kept (Candito et al., 2010b). All models are trained on the French Treebank (FTB) (Abeill´e and Barrier, 2004), consisting of 12,351 sentences from the Le Monde newspaper, either “desinflected” for the BerkeleyParser, or converted to projective dependency trees (Candito et al., 2010a) for the three dependency-native parsers.2 For c ∈ Cd • Update T{gov(d) ← ˆc} OUTPUT: Corrected version of parse tree T Figure 2: The parse correction algorithm. the dependency-native models, features include predicted part-of-speech (POS) tags from the MElt tagger (Denis and Sagot, 2009), as well as predicted lemmas and morphological features from the Lefff lexicon (Sagot, 2010). These models constitute the state-of-the-art for French dependency parsing: unlabeled attachment scores (UAS) on the FTB test set are 89.78% for MaltParser, 91.04% for MSTParser, 91.78% for BohnetParser, and 90.73% for BerkeleyParser. 4 Parse Correction The parse correction algorithm is a post-processing step to dependency parsing, where attachments from the predicted parse tree of a sentence are corrected by considering alternative candidate governors for each dependent. This process can be useful f</context>
</contexts>
<marker>Denis, Sagot, 2009</marker>
<rawString>P. Denis and B. Sagot. 2009. Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less human effort. In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Hong Kong, China, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguisticsVolume 1,</booktitle>
<pages>340--345</pages>
<location>Santa Cruz, California,</location>
<contexts>
<context position="11885" citStr="Eisner, 1996" startWordPosition="1866" endWordPosition="1867">sing, whose seminal work is that of McDonald et al. (2005), the parsing process selects the globally optimal parse tree from a graph containing attachments (directed edges) between each pair of words (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituency1224 based parsing followed by a conversion step to de- INPUT: Predicted parse tree T pendency structure. We use the three-step parsing LOOP: For each chosen dependent d E D architecture previously tested for French by Candito • Identify candidates Cd from T et al. </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J.M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th conference on Computational linguisticsVolume 1, pages 340–345, Santa Cruz, California, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hall</author>
<author>V Nov´ak</author>
</authors>
<title>Corrective modeling for non-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies,</booktitle>
<pages>42--52</pages>
<location>Vancouver, British Columbia,</location>
<marker>Hall, Nov´ak, 2005</marker>
<rawString>K. Hall and V. Nov´ak. 2005. Corrective modeling for non-projective dependency parsing. In Proceedings of the Ninth International Workshop on Parsing Technologies, pages 42–52, Vancouver, British Columbia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hara</author>
<author>M Shimbo</author>
<author>H Okuma</author>
<author>Y Matsumoto</author>
</authors>
<title>Coordinate structure analysis with global structural constraints and alignment-based local features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>967--975</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="7873" citStr="Hara et al., 2009" startWordPosition="1227" endWordPosition="1230">ted the problem in isolation. Resnik (1999) uses semantic similarity to resolve nounphrase coordination of the form (n1, cc, n2, n3), where the coordinating conjunction cc coordinates either the heads n1 and n2 or the heads n1 and n3. The same criticism as the one made by Atterer and Sch¨utze (2007) for pp-attachment might be applied to this approach to coordination resolution. In another formulation, the input consists of a raw sentence, and coordination structure is then detected and disambiguated using discriminative learning models (Shimbo and Hara, 2007) or coordination-specific parsers (Hara et al., 2009). Finally, other work has focused on introducing specialized features for coordination into existing syntactic parsing models (Hogan, 2007). Our approach is novel with respect to previous work by directly modeling the correction of coordination errors made by general-purpose dependency parsers. 1223 ouvrit Elle porte avec la cl´e la Figure 1: An unlabeled dependency tree for: Elle ouvrit la porte avec la cl´e. (She opened the door with the key). 3 Dependency Parsing Dependency syntax involves the representation of syntactic information for a sentence in the form a directed graph, whose edges e</context>
</contexts>
<marker>Hara, Shimbo, Okuma, Matsumoto, 2009</marker>
<rawString>K. Hara, M. Shimbo, H. Okuma, and Y. Matsumoto. 2009. Coordinate structure analysis with global structural constraints and alignment-based local features. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 967–975, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hogan</author>
</authors>
<title>Coordinate noun phrase disambiguation in a generative parsing model. In</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, number 1,</booktitle>
<pages>680</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8012" citStr="Hogan, 2007" startWordPosition="1250" endWordPosition="1251">oordinating conjunction cc coordinates either the heads n1 and n2 or the heads n1 and n3. The same criticism as the one made by Atterer and Sch¨utze (2007) for pp-attachment might be applied to this approach to coordination resolution. In another formulation, the input consists of a raw sentence, and coordination structure is then detected and disambiguated using discriminative learning models (Shimbo and Hara, 2007) or coordination-specific parsers (Hara et al., 2009). Finally, other work has focused on introducing specialized features for coordination into existing syntactic parsing models (Hogan, 2007). Our approach is novel with respect to previous work by directly modeling the correction of coordination errors made by general-purpose dependency parsers. 1223 ouvrit Elle porte avec la cl´e la Figure 1: An unlabeled dependency tree for: Elle ouvrit la porte avec la cl´e. (She opened the door with the key). 3 Dependency Parsing Dependency syntax involves the representation of syntactic information for a sentence in the form a directed graph, whose edges encode word-to-word relationships. An edge from a governor to a dependent indicates, roughly, that the presence of the dependent is syntacti</context>
</contexts>
<marker>Hogan, 2007</marker>
<rawString>D. Hogan. 2007. Coordinate noun phrase disambiguation in a generative parsing model. In In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, number 1, page 680, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Investigating multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>206--210</pages>
<location>New York City, New York,</location>
<contexts>
<context position="11102" citStr="Johansson and Nugues, 2006" startWordPosition="1736" endWordPosition="1739">etc. In a greedy version of this process, the action to apply at each step is deterministically chosen to be the best-scoring action according to a classifier, which is trained on a dependency treebank converted into sequences of actions. The strengths of this framework are O(n) time complexity and a lack of restrictions on the locality of features. A major drawback is its greedy behavior: it can potentially make difficult attachment decisions early in the processing of a sentence, without being able to reconsider them when more information becomes available. Beamed versions of the algorithm (Johansson and Nugues, 2006) partially address this problem, but still do not provide a global optimization for selecting the output parse tree. 3.2 Graph-Based Parsing In graph-based dependency parsing, whose seminal work is that of McDonald et al. (2005), the parsing process selects the globally optimal parse tree from a graph containing attachments (directed edges) between each pair of words (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factor</context>
</contexts>
<marker>Johansson, Nugues, 2006</marker>
<rawString>R. Johansson and P. Nugues. 2006. Investigating multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 206–210, New York City, New York, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="12090" citStr="Koo and Collins, 2010" startWordPosition="1898" endWordPosition="1901">rds (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituency1224 based parsing followed by a conversion step to de- INPUT: Predicted parse tree T pendency structure. We use the three-step parsing LOOP: For each chosen dependent d E D architecture previously tested for French by Candito • Identify candidates Cd from T et al. (2010a): (i) A constituency parse tree is out- • Predict cˆ = argmax S(c, d, T) put by the BerkeleyParser, which has been trained to learn a probabilistic context-free grammar with latent annotations (Petr</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>75--82</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="12770" citStr="Matsuzaki et al., 2005" startWordPosition="2009" endWordPosition="2012">oaches to dependency parsing, there is also the approach of constituency1224 based parsing followed by a conversion step to de- INPUT: Predicted parse tree T pendency structure. We use the three-step parsing LOOP: For each chosen dependent d E D architecture previously tested for French by Candito • Identify candidates Cd from T et al. (2010a): (i) A constituency parse tree is out- • Predict cˆ = argmax S(c, d, T) put by the BerkeleyParser, which has been trained to learn a probabilistic context-free grammar with latent annotations (Petrov et al., 2006) that has parsing time complexity O(n3) (Matsuzaki et al., 2005); (ii) A functional role labeler using a Maximum Entropy model adds functional annotations to links between a verb and its dependents; (iii) Constituency trees are automatically converted into projective dependency trees, with remaining unlabeled dependencies assigned labels using a rule-based approach. 3.4 Baseline Parsers In this paper, we use the following baseline parsers: MaltParser (Nivre et al., 2007) for transition-based parsing; MSTParser (McDonald et al., 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and Ber</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 75–82, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>81--88</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="4858" citStr="McDonald and Pereira (2006)" startWordPosition="734" endWordPosition="737">techniques similar to parse correction. Attardi and Dell’Orletta (2009) investigate reverse revision: a left-to-right transition-based model is first used to parse a sentence, then a right-to-left transition-based model is run with additional features taken from the left-toright model’s predicted parse. This approach leads to improved parsing results on a number of languages. While their approach is similar to parse correction in that it uses a predicted parse to inform a subsequent processing step, this information is used to improve a second parser rather than a model for correcting errors. McDonald and Pereira (2006) consider a method for recovering non-projective attachments from a graph representation of a sentence, in which an optimal projective parse tree has been identified. The parse tree’s edges are allowed to be rearranged in ways that introduce non-projectivity in order to increase its overall score. This rearrangement approach resembles parse correction because it is a second step that can revise attachments made in the first step, but it differs in a number of ways: it is dependent on a graph-based parsing approach, it does not model errors made by the parser, and it can only output non-project</context>
<context position="11993" citStr="McDonald and Pereira, 2006" startWordPosition="1881" endWordPosition="1884">obally optimal parse tree from a graph containing attachments (directed edges) between each pair of words (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituency1224 based parsing followed by a conversion step to de- INPUT: Predicted parse tree T pendency structure. We use the three-step parsing LOOP: For each chosen dependent d E D architecture previously tested for French by Candito • Identify candidates Cd from T et al. (2010a): (i) A constituency parse tree is out- • Predict cˆ = argmax S(c, d, T) put by the BerkeleyParser, w</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–88, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="11330" citStr="McDonald et al. (2005)" startWordPosition="1772" endWordPosition="1775">ions. The strengths of this framework are O(n) time complexity and a lack of restrictions on the locality of features. A major drawback is its greedy behavior: it can potentially make difficult attachment decisions early in the processing of a sentence, without being able to reconsider them when more information becomes available. Beamed versions of the algorithm (Johansson and Nugues, 2006) partially address this problem, but still do not provide a global optimization for selecting the output parse tree. 3.2 Graph-Based Parsing In graph-based dependency parsing, whose seminal work is that of McDonald et al. (2005), the parsing process selects the globally optimal parse tree from a graph containing attachments (directed edges) between each pair of words (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3) for 1-edge factors (McDonald et al</context>
<context position="13245" citStr="McDonald et al., 2005" startWordPosition="2076" endWordPosition="2079"> a probabilistic context-free grammar with latent annotations (Petrov et al., 2006) that has parsing time complexity O(n3) (Matsuzaki et al., 2005); (ii) A functional role labeler using a Maximum Entropy model adds functional annotations to links between a verb and its dependents; (iii) Constituency trees are automatically converted into projective dependency trees, with remaining unlabeled dependencies assigned labels using a rule-based approach. 3.4 Baseline Parsers In this paper, we use the following baseline parsers: MaltParser (Nivre et al., 2007) for transition-based parsing; MSTParser (McDonald et al., 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al., 2006) for constituency-based parsing. For MaltParser and MSTParser, we use the best settings from a benchmarking of parsers for French (Candito et al., 2010b), except that we remove unsupervised word clusters as features. The parsing models are thus trained using features including predicted part-of-speech tags, lemmas and morphological features. For BohnetParser, we trained a new model using these same predicted features. For BerkelyParser, w</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 91–98, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K¨ubler, S. Marinov, and E. Marsi. 2007. MaltParser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies,</booktitle>
<pages>149--160</pages>
<location>Nancy, France,</location>
<contexts>
<context position="10180" citStr="Nivre (2003)" startWordPosition="1583" endWordPosition="1584">raphbased parsing. Additionally, an alternative method for obtaining the dependency parse for a sentence is to parse the sentence with a constituency-based parser and then use an automatic process to convert the output into dependency structure. 1Edges are generally labeled with the surface grammatical function that the dependent bears with respect to its governor. In this paper we focus on unlabeled dependency parsing, setting aside labeling as a separate task. 3.1 Transition-Based Parsing In transition-based dependency parsing, whose seminal works are that of Yamada and Matsumoto (2003) and Nivre (2003), the parsing process applies a sequence of incremental actions, which typically manipulate a buffer position in the sentence and a stack for built sub-structures. Actions are of the type “read word from buffer”, “build a dependencyfrom node on top of the stack to node that begins the buffer”, etc. In a greedy version of this process, the action to apply at each step is deterministically chosen to be the best-scoring action according to a classifier, which is trained on a dependency treebank converted into sequences of actions. The strengths of this framework are O(n) time complexity and a lac</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>J. Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies, pages 149– 160, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Olteanu</author>
<author>D Moldovan</author>
</authors>
<title>PP-attachment disambiguation using large context.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>273--280</pages>
<location>Vancouver, British Columbia,</location>
<contexts>
<context position="6504" citStr="Olteanu and Moldovan (2005)" startWordPosition="1009" endWordPosition="1012"> reranking: it can be used with parsers that do not output n-best ranked parses, it can be easily restricted to specific attachment types, and its output space of parse trees is not limited to those appearing in an n-best list. However, parse reranking has the advantage of selecting the globally optimal parse for a sentence from an nbest list, while parse correction makes only locally optimal revisions in the predicted parse for a sentence. 2.1 Difficult Attachment Types Research on pp-attachment traditionally formulates the problem in isolation, as in the work of Pantel and Lin (2000) and of Olteanu and Moldovan (2005). Examples consist oftuples of the form (v, n1, p, n2), where either v or n1 is the true governor of the pp comprising p and n2, and the task is to choose between v and n1. Recently, Atterer and Sch¨utze (2007) have criticized this formulation as unrealistic because it uses an oracle to select candidate governors, and they find that successful approaches for the isolated problem perform no better than stateof-the-art parsers on pp-attachment when evaluated on full sentences. With parse correction, candidate governors are identified automatically with no (v, n1, p, n2) restriction, and for seve</context>
</contexts>
<marker>Olteanu, Moldovan, 2005</marker>
<rawString>M. Olteanu and D. Moldovan. 2005. PP-attachment disambiguation using large context. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 273–280, Vancouver, British Columbia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>An unsupervised approach to prepositional phrase attachment using contextually similar words.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>38</volume>
<pages>101--108</pages>
<location>Hong Kong,</location>
<contexts>
<context position="6469" citStr="Pantel and Lin (2000)" startWordPosition="1003" endWordPosition="1006">ber of advantages compared to reranking: it can be used with parsers that do not output n-best ranked parses, it can be easily restricted to specific attachment types, and its output space of parse trees is not limited to those appearing in an n-best list. However, parse reranking has the advantage of selecting the globally optimal parse for a sentence from an nbest list, while parse correction makes only locally optimal revisions in the predicted parse for a sentence. 2.1 Difficult Attachment Types Research on pp-attachment traditionally formulates the problem in isolation, as in the work of Pantel and Lin (2000) and of Olteanu and Moldovan (2005). Examples consist oftuples of the form (v, n1, p, n2), where either v or n1 is the true governor of the pp comprising p and n2, and the task is to choose between v and n1. Recently, Atterer and Sch¨utze (2007) have criticized this formulation as unrealistic because it uses an oracle to select candidate governors, and they find that successful approaches for the isolated problem perform no better than stateof-the-art parsers on pp-attachment when evaluated on full sentences. With parse correction, candidate governors are identified automatically with no (v, n</context>
</contexts>
<marker>Pantel, Lin, 2000</marker>
<rawString>P. Pantel and D. Lin. 2000. An unsupervised approach to prepositional phrase attachment using contextually similar words. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, volume 38, pages 101–108, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="12706" citStr="Petrov et al., 2006" startWordPosition="1999" endWordPosition="2002">010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituency1224 based parsing followed by a conversion step to de- INPUT: Predicted parse tree T pendency structure. We use the three-step parsing LOOP: For each chosen dependent d E D architecture previously tested for French by Candito • Identify candidates Cd from T et al. (2010a): (i) A constituency parse tree is out- • Predict cˆ = argmax S(c, d, T) put by the BerkeleyParser, which has been trained to learn a probabilistic context-free grammar with latent annotations (Petrov et al., 2006) that has parsing time complexity O(n3) (Matsuzaki et al., 2005); (ii) A functional role labeler using a Maximum Entropy model adds functional annotations to links between a verb and its dependents; (iii) Constituency trees are automatically converted into projective dependency trees, with remaining unlabeled dependencies assigned labels using a rule-based approach. 3.4 Baseline Parsers In this paper, we use the following baseline parsers: MaltParser (Nivre et al., 2007) for transition-based parsing; MSTParser (McDonald et al., 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet, 2010</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>J Reynar</author>
<author>S Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology,</booktitle>
<pages>250--255</pages>
<location>Plainsboro, New Jersey,</location>
<contexts>
<context position="38056" citStr="Ratnaparkhi et al., 1994" startWordPosition="6218" endWordPosition="6221">configuration for each parser, by dependent type. w-+c is wrong-to-correct, c-+w is correct-towrong, w-+w is wrong-to-wrong, and Mods is the percentage of tokens modified. 86%). These results are a bit disappointing, but they suggest that there may be a performance ceiling for pp-attachment beyond which rich lexical information (syntactic and semantic) or full sentence contexts are needed. For English, the average human performance on pp-attachment for the (v, n1, p, n2) problem formulation is just 88.2% when given only the four head-words, but increases to 93.2% when given the full sentence (Ratnaparkhi et al., 1994). If similar levels of human performance exist for French, additional sources of information may be needed to improve pp-attachment performance. In addition to evaluating UAS improvements for parse correction, we took a closer look at the best corrective configuration (specialized) and analyzed the types of attachment modifications made (Table 3). In most cases there were around 2−3 times as many error-correcting modifications (w-+c) as error-creating modifications (c-+w), and the overall % of tokens modified was very low overall (around 1-2%). Parse correction is thus conservative in the numb</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In Proceedings of the Workshop on Human Language Technology, pages 250–255, Plainsboro, New Jersey, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language.</title>
<date>1999</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>11</volume>
<issue>95</issue>
<contexts>
<context position="7298" citStr="Resnik (1999)" startWordPosition="1137" endWordPosition="1138">, Atterer and Sch¨utze (2007) have criticized this formulation as unrealistic because it uses an oracle to select candidate governors, and they find that successful approaches for the isolated problem perform no better than stateof-the-art parsers on pp-attachment when evaluated on full sentences. With parse correction, candidate governors are identified automatically with no (v, n1, p, n2) restriction, and for several representative parsers we find that parse correction improves pp-attachment performance. Research on coordination resolution has also often formulated the problem in isolation. Resnik (1999) uses semantic similarity to resolve nounphrase coordination of the form (n1, cc, n2, n3), where the coordinating conjunction cc coordinates either the heads n1 and n2 or the heads n1 and n3. The same criticism as the one made by Atterer and Sch¨utze (2007) for pp-attachment might be applied to this approach to coordination resolution. In another formulation, the input consists of a raw sentence, and coordination structure is then detected and disambiguated using discriminative learning models (Shimbo and Hara, 2007) or coordination-specific parsers (Hara et al., 2009). Finally, other work has</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>P. Resnik. 1999. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal ofArtificial Intelligence Research, 11(95):130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sagot</author>
</authors>
<title>The Lefff, a freely available, accurate and large-coverage lexicon for French.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation,</booktitle>
<location>Valetta, Malta,</location>
<contexts>
<context position="14863" citStr="Sagot, 2010" startWordPosition="2319" endWordPosition="2320"> the French Treebank (FTB) (Abeill´e and Barrier, 2004), consisting of 12,351 sentences from the Le Monde newspaper, either “desinflected” for the BerkeleyParser, or converted to projective dependency trees (Candito et al., 2010a) for the three dependency-native parsers.2 For c ∈ Cd • Update T{gov(d) ← ˆc} OUTPUT: Corrected version of parse tree T Figure 2: The parse correction algorithm. the dependency-native models, features include predicted part-of-speech (POS) tags from the MElt tagger (Denis and Sagot, 2009), as well as predicted lemmas and morphological features from the Lefff lexicon (Sagot, 2010). These models constitute the state-of-the-art for French dependency parsing: unlabeled attachment scores (UAS) on the FTB test set are 89.78% for MaltParser, 91.04% for MSTParser, 91.78% for BohnetParser, and 90.73% for BerkeleyParser. 4 Parse Correction The parse correction algorithm is a post-processing step to dependency parsing, where attachments from the predicted parse tree of a sentence are corrected by considering alternative candidate governors for each dependent. This process can be useful for attachments made too early in transition-based parsing, or with features that are too loca</context>
</contexts>
<marker>Sagot, 2010</marker>
<rawString>B. Sagot. 2010. The Lefff, a freely available, accurate and large-coverage lexicon for French. In Proceedings of the Seventh International Conference on Language Resources and Evaluation, Valetta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shimbo</author>
<author>K Hara</author>
</authors>
<title>A discriminative learning model for coordinate conjunctions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>610--619</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7820" citStr="Shimbo and Hara, 2007" startWordPosition="1220" endWordPosition="1223">esearch on coordination resolution has also often formulated the problem in isolation. Resnik (1999) uses semantic similarity to resolve nounphrase coordination of the form (n1, cc, n2, n3), where the coordinating conjunction cc coordinates either the heads n1 and n2 or the heads n1 and n3. The same criticism as the one made by Atterer and Sch¨utze (2007) for pp-attachment might be applied to this approach to coordination resolution. In another formulation, the input consists of a raw sentence, and coordination structure is then detected and disambiguated using discriminative learning models (Shimbo and Hara, 2007) or coordination-specific parsers (Hara et al., 2009). Finally, other work has focused on introducing specialized features for coordination into existing syntactic parsing models (Hogan, 2007). Our approach is novel with respect to previous work by directly modeling the correction of coordination errors made by general-purpose dependency parsers. 1223 ouvrit Elle porte avec la cl´e la Figure 1: An unlabeled dependency tree for: Elle ouvrit la porte avec la cl´e. (She opened the door with the key). 3 Dependency Parsing Dependency syntax involves the representation of syntactic information for a</context>
</contexts>
<marker>Shimbo, Hara, 2007</marker>
<rawString>M. Shimbo and K. Hara. 2007. A discriminative learning model for coordinate conjunctions. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 610–619, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies,</booktitle>
<pages>195--206</pages>
<location>Nancy, France,</location>
<contexts>
<context position="10163" citStr="Yamada and Matsumoto (2003)" startWordPosition="1578" endWordPosition="1581">: transition-based parsing and graphbased parsing. Additionally, an alternative method for obtaining the dependency parse for a sentence is to parse the sentence with a constituency-based parser and then use an automatic process to convert the output into dependency structure. 1Edges are generally labeled with the surface grammatical function that the dependent bears with respect to its governor. In this paper we focus on unlabeled dependency parsing, setting aside labeling as a separate task. 3.1 Transition-Based Parsing In transition-based dependency parsing, whose seminal works are that of Yamada and Matsumoto (2003) and Nivre (2003), the parsing process applies a sequence of incremental actions, which typically manipulate a buffer position in the sentence and a stack for built sub-structures. Actions are of the type “read word from buffer”, “build a dependencyfrom node on top of the stack to node that begins the buffer”, etc. In a greedy version of this process, the action to apply at each step is deterministically chosen to be the best-scoring action according to a classifier, which is trained on a dependency treebank converted into sequences of actions. The strengths of this framework are O(n) time com</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies, pages 195–206, Nancy, France, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>