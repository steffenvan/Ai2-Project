<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.997162">
Automatic Editing in a Back-End Speech-to-Text System
</title>
<author confidence="0.870129">
Maximilian Bisani Paul Vozila Olivier Divay Jeff Adams
</author>
<affiliation confidence="0.67093">
Nuance Communications
</affiliation>
<address confidence="0.831799">
One Wayside Road
Burlington, MA 01803, U.S.A.
</address>
<email confidence="0.997409">
{maximilian.bisani,paul.vozila,olivier.divay,jeff.adams}@nuance.com
</email>
<sectionHeader confidence="0.982866" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999794333333333">
Written documents created through dictation
differ significantly from a true verbatim tran-
script of the recorded speech. This poses
an obstacle in automatic dictation systems as
speech recognition output needs to undergo
a fair amount of editing in order to turn it
into a document that complies with the cus-
tomary standards. We present an approach
that attempts to perform this edit from recog-
nized words to final document automatically
by learning the appropriate transformations
from example documents. This addresses a
number of problems in an integrated way,
which have so far been studied independently,
in particular automatic punctuation, text seg-
mentation, error correction and disfluency re-
pair. We study two different learning methods,
one based on rule induction and one based on
a probabilistic sequence model. Quantitative
evaluation shows that the probabilistic method
performs more accurately.
</bodyText>
<sectionHeader confidence="0.995148" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998954">
Large vocabulary speech recognition today achieves
a level of accuracy that makes it useful in the produc-
tion of written documents. Especially in the medical
and legal domains large volumes of text are tradi-
tionally produced by means of dictation. Here docu-
ment creation is typically a “back-end” process. The
author dictates all necessary information into a tele-
phone handset or a portable recording device and
is not concerned with the actual production of the
document any further. A transcriptionist will then
listen to the recorded dictation and produce a well-
formed document using a word processor. The goal
of introducing speech recognition in this process is
to create a draft document automatically, so that the
transcriptionist only has to verify the accuracy of the
document and to fix occasional recognition errors.
We observe that users try to spend as little time as
possible dictating. They usually focus only on the
content and rely on the transcriptionist to compose
a readable, syntactically correct, stylistically accept-
able and formally compliant document. For this rea-
son there is a considerable discrepancy between the
final document and what the speaker has said liter-
ally. In particular in medical reports we see differ-
ences of the following kinds:
</bodyText>
<listItem confidence="0.980105727272727">
• Punctuation marks are typically not verbalized.
• No instructions on the formatting of the report
are dictated. Section headings are not identified
as such.
• Frequently section headings are only implied.
(“vitals are” —* “PHYSICAL EXAMINATION:
VITAL SIGNS:”)
• Enumerated lists. Typically speakers use
phrases like “number one ... next number... ”,
which need to be turned into “1.... 2.... ”
• The dictation usually begins with a preamble
</listItem>
<bodyText confidence="0.8932406">
(e.g. “This is doctor Xyz ...”) which does not
appear in the report. Similarly there are typ-
ical phrases at the end of the dictation which
should not be transcribed (e.g. “End of dicta-
tion. Thank you.”)
</bodyText>
<page confidence="0.471335">
114
</page>
<note confidence="0.627495">
Proceedings of ACL-08: HLT, pages 114–120,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<listItem confidence="0.996111037037037">
• There are specific standards regarding the use
of medical terminology. Transcriptionists fre-
quently expand dictated abbreviations (e.g.
“CVA” —* “cerebrovascular accident”) or oth-
erwise use equivalent terms (e.g. “nonicteric
sclerae” —* “no scleral icterus”).
• The dictation typically has a more narrative
style (e.g. “She has no allergies.”, “I examined
him”). In contrast, the report is normally more
impersonal and structured (e.g. “ALLERGIES:
None.”, “he was examined”).
• For the sake of brevity, speakers frequently
omit function words. (“patient” —* “the pa-
tient”, “denies fever pain” —* “he denies any
fever or pain”)
• As the dictation is spontaneous, disfluencies are
quite frequent, in particular false starts, correc-
tions and repetitions. (e.g. “22-year-old fe-
male, sorry, male 22-year-old male” —* “22-
year-old male”)
• Instruction to the transcriptionist and so-called
normal reports, pre-defined text templates in-
voked by a short phrase like “This is a normal
chest x-ray.”
• In addition to the above, speech recognition
output has the usual share of recognition errors
some of which may occur systematically.
</listItem>
<bodyText confidence="0.99996162962963">
These phenomena pose a problem that goes beyond
the speech recognition task which has traditionally
focused on correctly identifying speech utterances.
Even with a perfectly accurate verbatim transcript of
the user’s utterances, the transcriptionist would need
to perform a significant amount of editing to obtain
a document conforming to the customary standards.
We need to look for what the user wants rather than
what he says.
Natural language processing research has ad-
dressed a number of these issues as individual prob-
lems: automatic punctuation (Liu et al., 2005),
text segmentation (Beeferman et al., 1999; Matusov
et al., 2003) disfluency repair (Heeman et al., 1996)
and error correction (Ringger and Allen, 1996;
Strzalkowski and Brandow, 1997; Peters and Drexel,
2004). The method we present in the following at-
tempts to address all this by a unified transforma-
tion model. The goal is simply stated as transform-
ing the recognition output into a text document. We
will first describe the general framework of learn-
ing transformations from example documents. In
the following two sections we will discuss a rule-
induction-based and a probabilistic transformation
method respectively. Finally we present experimen-
tal results in the context of medical transcription and
conclude with an assessment of both methods.
</bodyText>
<sectionHeader confidence="0.918871" genericHeader="method">
2 Text transformation
</sectionHeader>
<bodyText confidence="0.999969294117647">
In dictation and transcription management systems
corresponding pairs of recognition output and edited
and corrected documents are readily available. The
idea of transformation modeling, outlined in fig-
ure 1, is to learn to emulate the transcriptionist. To
this end we first process archived dictations with the
speech recognizer to create approximate verbatim
transcriptions. For each document this yields the
spoken or source word sequence S = s1 ... sM,
which is supposed to be a word-by-word transcrip-
tion of the user’s utterances, but which may actu-
ally contain recognition errors. The corresponding
final reports are cleaned (removal of page headers
etc.), tagged (identification of section headings and
enumerated lists) and tokenized, yielding the text or
target token sequence T = t1...tN for each docu-
ment. Generally, the token sequence corresponds
to the spoken form. (E.g. “25mg” is tokenized as
“twenty five milligrams”.) Tokens can be ordinary
words or special symbols representing line breaks,
section headings, etc. Specifically, we represent
each section heading by a single indivisible token,
even if the section name consists of multiple words.
Enumerations are represented by special tokens, too.
Different techniques can be applied to learn and ex-
ecute the actual transformation from S to T. Two
options are discussed in the following.
With the transformation model at hand, a draft
for a new document is created in three steps. First
the speech recognizer processes the audio recording
and produces the source word sequence S. Next,
the transformation step converts S into the target se-
quence T. Finally the transformation output T is
formatted into a text document. Formatting is the
</bodyText>
<figure confidence="0.990431107142857">
115
archived
dictations
recognize
V
transcripts
archived
documents
new
dictation
recognize
V
transcript
transform
V
tokens
format
V
draft
document
transformation
model
store
��
��
A
��
store
</figure>
<figureCaption confidence="0.988002">
Figure 1: Illustration of how text transformation is inte-
grated into a speech-to-text system.
</figureCaption>
<bodyText confidence="0.999791636363636">
inverse of tokenization and includes conversion of
number words to digits, rendition of paragraphs and
section headings, etc.
Before we turn to concrete transformation tech-
niques, we can make two general statements about
this problem. Firstly, in the absence of observa-
tions to the contrary, it is reasonable to leave words
unchanged. So, a priori the mapping should be
the identity. Secondly, the transformation is mostly
monotonous. Out-of-order sections do occur but are
the exception rather than the rule.
</bodyText>
<sectionHeader confidence="0.374999" genericHeader="method">
3 Transformation based learning
</sectionHeader>
<bodyText confidence="0.998229">
Following Strzalkowski and Brandow (1997) and
Peters and Drexel (2004) we have implemented
a transformation-based learning (TBL) algorithm
(Brill, 1995). This method iteratively improves the
match (as measured by token error rate) of a col-
lection of corresponding source and target token se-
quences by positing and applying a sequence of sub-
stitution rules. In each iteration the source and tar-
get tokens are aligned using a minimum edit dis-
tance criterion. We refer to maximal contiguous
subsequences of non-matching tokens as error re-
</bodyText>
<figure confidence="0.9840558">
��
��
��train
targets
��
tokenize
V manual
correction
final
document
</figure>
<bodyText confidence="0.999932108108108">
gions. These consist of paired sequences of source
and target tokens, where either sequence may be
empty. Each error region serves as a candidate sub-
stitution rule. Additionally we consider refinements
of these rules with varying amounts of contiguous
context tokens on either side. Deviating from Peters
and Drexel (2004), in the special case of an empty
target sequence, i.e. a deletion rule, we consider
deleting all (non-empty) contiguous subsequences
of the source sequence as well. For each candi-
date rule we accumulate two counts: the number of
exactly matching error regions and the number of
false alarms, i.e. when its left-hand-side matches
a sequence of already correct tokens. Rules are
ranked by the difference in these counts scaled by
the number of errors corrected by a single rule ap-
plication, which is the length of the corresponding
error region. This is an approximation to the to-
tal number of errors corrected by a rule, ignoring
rule interactions and non-local changes in the mini-
mum edit distance alignment. A subset of the top-
ranked non-overlapping rules satisfying frequency
and minimum impact constraints are selected and
the source sequences are updated by applying the se-
lected rules. Again deviating from Peters and Drexel
(2004), we consider two rules as overlapping if the
left-hand-side of one is a contiguous subsequence
of the other. This procedure is iterated until no ad-
ditional rules can be selected. The initial rule set
is populated by a small sequence of hand-crafted
rules (e.g. “impression colon” —* “IMPRESSION:”).
A user-independent baseline rule set is generated
by applying the algorithm to data from a collec-
tion of users. We construct speaker-dependent mod-
els by initializing the algorithm with the speaker-
independent rule set and applying it to data from the
given user.
</bodyText>
<sectionHeader confidence="0.992322" genericHeader="method">
4 Probabilistic model
</sectionHeader>
<bodyText confidence="0.99422425">
The canonical approach to text transformation fol-
lowing statistical decision theory is to maximize the
text document posterior probability given the spoken
document.
</bodyText>
<equation confidence="0.998113">
T* = argmax p(T|S) (1)
T
</equation>
<bodyText confidence="0.999194">
Obviously, the global model p(T|S) must be con-
structed from smaller scale observations on the cor-
</bodyText>
<page confidence="0.776659">
116
</page>
<bodyText confidence="0.999488461538462">
respondence between source and target words. We
use a 1-to-n alignment scheme. This means each
source word is assigned to a sequence of zero, one
or more target words. We denote the target words
assigned to source word si as τi. Each replacement
τi is a possibly empty sequence of target words. A
source word together with its replacement sequence
will be called a segment. We constrain the set of pos-
sible transformations by selecting a relatively small
set of allowable replacements A(s) to each source
word. This means we require τi E A(si). We use
the usual m-gram approximation to model the joint
probability of a transformation:
</bodyText>
<equation confidence="0.999184">
M
p(S, T) = p(si, τi|si−M+1, τi−M+1, ... si−1, τi−1)
i=1
</equation>
<bodyText confidence="0.996503289473684">
(2)
The work of Ringger and Allen (1996) is similar
in spirit to this method, but uses a factored source-
channel model. Note that the decision rule (1) is
over whole documents. Therefore we processes
complete documents at a time without prior segmen-
tation into sentences.
To estimate this model we first align all training
documents. That is, for each document, the tar-
get word sequence is segmented into M segments
T = τ1— ... ,..,τM. The criterion for this alignment
is to maximize the likelihood of a segment unigram
model. The alignment is performed by an expec-
tation maximization algorithm. Subsequent to the
alignment step, m-gram probabilities are estimated
by standard language modeling techniques. We cre-
ate speaker-specific models by linearly interpolating
an m-gram model based on data from the user with
a speaker-independent background m-gram model
trained on data pooled from a collection of users.
To select the allowable replacements for each
source word we count how often each particular tar-
get sequence is aligned to it in the training data. A
source target pair is selected if it occurs twice or
more times. Source words that were not observed
in training are immutable, i.e. the word itself is its
only allowable replacement A(s) = {(s)}. As an
example suppose “patient” was deleted 10 times, left
unchanged 105 times, replaced by “the patient” 113
times and once replaced by “she”. The word patient
would then have three allowables: A(patient) =
{(), (patient), (the, patient)}.)
The decision rule (1) minimizes the document er-
ror rate. A more appropriate loss function is the
number of source words that are replaced incor-
rectly. Therefore we use the following minimum
word risk (MWR) decision strategy, which mini-
mizes source word loss.
</bodyText>
<equation confidence="0.869264">
T* = (argmax p(τ1|S))-- ... ( argmax p(τM|S))
T1EA(si) TM EA(sM )
(3)
</equation>
<bodyText confidence="0.9998971">
This means for each source sequence position we
choose the replacement that has the highest poste-
rior probability p(τi|S) given the entire source se-
quence. To compute the posterior probabilities, first
a graph is created representing alternatives “around”
the most probable transform using beam search.
Then the forward-backward algorithm is applied to
compute edge posterior probabilities. Finally edge
posterior probabilities for each source position are
accumulated.
</bodyText>
<sectionHeader confidence="0.993423" genericHeader="evaluation">
5 Experimental evaluation
</sectionHeader>
<bodyText confidence="0.999987576923077">
The methods presented were evaluated on a set of
real-life medical reports dictated by 51 doctors. For
each doctor we use 30 reports as a test set. Trans-
formation models are trained on a disjoint set of re-
ports that predated the evaluation reports. The typ-
ical document length is between one hundred and
one thousand words. All dictations were recorded
via telephone. The speech recognizer works with
acoustic models that are specifically adapted for
each user, not using the test data, of course. It
is hard to quote the verbatim word error rate of
the recognizer, because this would require a care-
ful and time-consuming manual transcription of the
test set. The recognition output is auto-punctuated
by a method similar in spirit to the one proposed by
Liu et al. (2005) before being passed to the transfor-
mation model. This was done because we consid-
ered the auto-punctuation output as the status quo
ante which transformation modeling was to be com-
pared to. Neither of both transformation methods
actually relies on having auto-punctuated input. The
auto-punctuation step only inserts periods and com-
mas and the document is not explicitly segmented
into sentences. (The transformation step always ap-
plies to entire documents and the interpretation of a
period as a sentence boundary is left to the human
</bodyText>
<page confidence="0.9118">
117
</page>
<tableCaption confidence="0.966131333333333">
Table 1: Experimental evaluation of different text transformation techniques with different amounts of user-specific
data. Precision, recall, deletion, insertion and error rate values are given in percent and represent the average of 51
users, where the results for each user are the ratios of sums over 30 reports.
</tableCaption>
<table confidence="0.998275666666667">
method user sections punctuation all tokens errors
docs precision recall precision recall deletions insertions
none (only auto-punct) 0.00 0.00 66.68 71.21 11.32 27.48 45.32
TBL SI 69.18 44.43 73.90 67.22 11.41 17.73 34.99
3-gram SI 65.19 44.41 73.79 62.26 18.15 12.27 36.09
TBL 25 75.38 53.39 75.59 69.11 10.97 15.97 32.62
3-gram 25 80.90 59.37 78.88 69.81 11.50 12.09 28.87
TBL 50 76.67 56.18 76.11 69.81 10.81 15.53 31.92
3-gram 50 81.10 62.69 79.39 70.94 11.31 11.46 27.76
TBL 100 77.92 58.03 76.41 70.52 10.67 15.19 31.29
3-gram 100 81.69 64.36 79.35 71.38 11.48 10.82 27.12
3-gram without MWR 100 81.39 64.23 79.01 71.52 11.55 10.92 27.29
</table>
<bodyText confidence="0.999776333333333">
reader of the document.) For each doctor a back-
ground transformation model was constructed using
100 reports from each of the other users. This is re-
ferred to as the speaker-independent (SI) model. In
the case of the probabilistic model, all models were
3-gram models. User-specific models were created
by augmenting the SI model with 25, 50 or 100 re-
ports. One report from the test set is shown as an
example in the appendix.
</bodyText>
<subsectionHeader confidence="0.987108">
5.1 Evaluation metric
</subsectionHeader>
<bodyText confidence="0.999997966666667">
The output of the text transformation is aligned with
the corresponding tokenized report using a mini-
mum edit cost criterion. Alignments between sec-
tion headings and non-section headings are not per-
mitted. Likewise no alignment of punctuation and
non-punctuation tokens is allowed. Using the align-
ment we compute precision and recall for sections
headings and punctuation marks as well as the over-
all token error rate. It should be noted that the so de-
rived error rate is not comparable to word error rates
usually reported in speech recognition research. All
missing or erroneous section headings, punctuation
marks and line breaks are counted as errors. As
pointed out in the introduction the reference texts do
not represent a literal transcript of the dictation. Fur-
thermore the data were not cleaned manually. There
are, for example, instances of letter heads or page
numbers that were not correctly removed when the
text was extracted from the word processor’s file for-
mat. The example report shown in the appendix
features some of the typical differences between the
produced draft and the final report that may or may
not be judged as errors. (For example, the date of
the report was not given in the dictation, the sec-
tion names “laboratory data” and “laboratory evalu-
ation” are presumably equivalent and whether “sta-
ble” is preceded by a hyphen or a period in the last
section might not be important.) Nevertheless, the
numbers reported do permit a quantitative compari-
son between different methods.
</bodyText>
<subsectionHeader confidence="0.947811">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999894764705882">
Results are stated in table 1. In the baseline setup
no transformation is applied to the auto-punctuated
recognition output. Since many parts of the source
data do not need to be altered, this constitutes the
reference point for assessing the benefit of transfor-
mation modeling. For obvious reasons precision and
recall of section headings are zero. A high rate of
insertion errors is observed which can largely be at-
tributed to preambles. Both transformation methods
reduce the discrepancy between the draft document
and the final corrected document significantly. With
100 training documents per user the mean token er-
ror rate is reduced by up to 40% relative by the prob-
abilistic model. When user specific data is used, the
probabilistic approach performs consistently better
than TBL on all accounts. In particular it always
has much lower insertion rates reflecting its supe-
</bodyText>
<page confidence="0.860276">
118
</page>
<bodyText confidence="0.999949769230769">
rior ability to remove utterances that are not typi-
cally part of the report. On the other hand the prob-
abilistic model suffers from a slightly higher dele-
tion rate due to being overzealous in this regard.
In speaker independent mode, however, the deletion
rate is excessively high and leads to inferior overall
performance. Interestingly the precision of the au-
tomatic punctuation is increased by the transforma-
tion step, without compromising on recall, at least
when enough user specific training data is available.
The minimum word risk criterion (3) yields slightly
better results than the simpler document risk crite-
rion (1).
</bodyText>
<sectionHeader confidence="0.995541" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999754375">
Automatic text transformation brings speech recog-
nition output much closer to the end result desired
by the user of a back-end dictation system. It au-
tomatically punctuates, sections and rephrases the
document and thereby greatly enhances transcrip-
tionist productivity. The holistic approach followed
here is simpler and more comprehensive than a cas-
cade of more specialized methods. Whether or not
the holistic approach is also more accurate is not an
easy question to answer. Clearly the outcome would
depend on the specifics of the specialized methods
one would compare to, as well as the complexity
of the integrated transformation model one applies.
The simple models studied in this work admittedly
have little provisions for targeting specific transfor-
mation problems. For example the typical length of
a section is not taken into account. However, this is
not a limitation of the general approach. We have
observed that a simple probabilistic sequence model
performs consistently better than the transformation-
based learning approach. Even though neither of
both methods is novel, we deem this an important
finding since none of the previous publications we
know of in this domain allow this conclusion. While
the present experiments have used a separate auto-
punctuation step, future work will aim to eliminate
it by integrating the punctuation features into the
transformation step. In the future we plan to inte-
grate additional knowledge sources into our statis-
tical method in order to more specifically address
each of the various phenomena encountered in spon-
taneous dictation.
</bodyText>
<sectionHeader confidence="0.989961" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.948559035714286">
Beeferman, Doug, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation.
Machine Learning, 34(1-3):177 – 210.
Brill, Eric. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics, 21(4):543 – 565.
Heeman, Peter A., Kyung-ho Loken-Kim, and
James F. Allen. 1996. Combining the detec-
tion and correction of speech repairs. In Proc.
Int. Conf. Spoken Language Processing (ICSLP),
pages 362 – 365. Philadelphia, PA, USA.
Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and
Mary Harper. 2005. Using conditional random
fields for sentence boundary detection in speech.
In Proc. Annual Meeting of the ACL, pages 451 –
458. Ann Arbor, MI, USA.
Matusov, Evgeny, Jochen Peters, Carsten Meyer,
and Hermann Ney. 2003. Topic segmentation
using markov models on section level. In Proc.
IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU), pages 471 – 476.
IEEE, St. Thomas, U.S. Virgin Islands.
Peters, Jochen and Christina Drexel. 2004.
Transformation-based error correction for speech-
to-text systems. In Proc. Int. Conf. Spoken Lan-
guage Processing (ICSLP), pages 1449 – 1452.
Jeju Island, Korea.
</reference>
<bodyText confidence="0.8689078">
Ringger, Eric K. and James F. Allen. 1996. A fertil-
ity channel model for post-correction of continu-
ous speech recognition. In Proc. Int. Conf. Spoken
Language Processing (ICSLP), pages 897 – 900.
Philadelphia, PA, USA.
Strzalkowski, Tomek and Ronald Brandow. 1997.
A natural language correction model for contin-
uous speech recognition. In Proc. 5th Workshop
on Very Large Corpora (WVVLC-5):, pages 168 –
177. Beijing-Hong Kong.
</bodyText>
<page confidence="0.618595">
119
</page>
<table confidence="0.802646607142857">
Progress Note
DATE: July 26, 2005.
HISTORY OF PRESENT ILLNESS: The patient has no
complaints. She is ambulating without problems. No chest
pain or chest pressure. She still has some shortness of breath,
but overall has improved significantly.
PHYSICAL EXAMINATION:
VITAL SIGNS: Stable. She’s afebrile.
LUNGS: Decreased breath sounds at the bases with bilateral
rales and rhonchi.
HEART: Regular rate and rhythm. 2/6 crescendo, decrescendo
murmur at the right sternal border.
ABDOMEN: Soft, nontender and nondistended.
EXTREMITIES: 1+ pedal edema bilaterally.
NEUROLOGICAL EXAMINATION: Nonfocal.
LABORATORY EVALUATION: White count 5.7, H&amp;H 11.6 and
35.5, platelet count of 155, sodium 137, potassium 3.9,
chloride 100, co2 39, calcium 8.7, glucose 91, BUN and
creatinine 37 and 1.1.
IMPRESSION:
1. Chronic obstructive pulmonary disease exacerbation.
Continue breathing treatments.
2. Asthma exacerbation. Continue oral prednisone.
3. Bronchitis. Continue Levaquin.
4. Hypertension-stable.
5. Uncontrolled diabetes mellitus-improved.
6. Gastroesophageal reflux disease-stable.
7. Congestive heart failure-stable.
</table>
<bodyText confidence="0.8736202">
The patient is in stable condition and will be discharged to
name Nursing Home, and will be monitored on an outpatient
basis.
Final report produced by a human transcriptionist
without reference to the automatic draft.
SUBJECTIVE: The patient is without complaints. Has been
ambulating without problems. No chest pain, chest pressure,
still has some shortness of breath, but overall has improved
significantly.
PHYSICAL EXAMINATION:
VITAL SIGNS: Stable. She is afebrile.
LUNGS: Show decreased breath sounds at the bases with
bilateral rales and rhonchi.
HEART: Regular rate and rhythm 2/6 crescendo decrescendo
murmur at the right sternal border.
</bodyText>
<figure confidence="0.9133644375">
ABDOMEN: Soft, nontender, nondistended.
EXTREMITIES: Show 1+ pedal edema bilaterally.
NEUROLOGICAL: Nonfocal.
LABORATORY DATA: White count of 5.7, hemoglobin and
hematocrit 11.6 and 35.5, platelet count of 155, sodium 137,
potassium 3.9, chloride 100, CO2 39, calcium 8.7, glucose 91,
BUN and creatinine 37 and 1.1.
IMPRESSION:
1. Chronic obstructive pulmonary disease exacerbation.
Continue breathing treatments.
2. Asthma exacerbation. Continue oral prednisone.
3. Bronchitis. Continue Levaquin.
4. Hypertension. Stable.
5. Uncontrolled diabetes mellitus. Improved.
6. Gastroesophageal reflux disease, stable.
7. Congestive heart failure. Stable.
</figure>
<bodyText confidence="0.940710977272727">
PLAN: The patient is in stable condition and will be discharged
to name nursing home and will be monitored closely on an
outpatient basis.
Automatically generated draft (speech recognition
output after transformation and formatting)
Progress note
doctors name dictating a progress note on
first name last name patient without
complaints has been ambulating without
problems no chest pain chest pressure still
has some shortness of breath but overall has
improved significantly
vital signs are stable she is afebrile lungs
show decreased breath sounds at the bases
with bilateral rales and rhonchi heart is
regular rate and rhythm two over six
crescendo decrescendo murmur at the right
sternal border abdomen soft nontender
nondistended extremities show one plus
pedal edema bilaterally neurological exam is
nonfocal
white count of five point seven H. and H.
eleven point six and thirty five point five
platelet count of one fifty five sodium one
thirty seven potassium three point nine
chloride one hundred carbon dioxide thirty
nine calcium eight point seven glucose
ninety one BUN and creatinine thirty seven
and one point one
impression number one COPD exacerbation
continue breathing treatments number two
asthma exacerbation continue oral
prednisone number three bronchitis continue
Levaquin number four hypertension stable
number five uncontrolled diabetes mellitus
improved number six gastroesophageal
reflux disease stable number seven
congestive heart failure stable
new paragraph patient is in stable condition
and will be discharged to name nursing
home and will be monitored closely on an
outpatient basis progress note
Recognition output. Vertical space was
added to facilitate visual comparison.
</bodyText>
<figure confidence="0.3455335">
Appendix A. Example of a medical report
120
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919911">
<title confidence="0.999945">Automatic Editing in a Back-End Speech-to-Text System</title>
<author confidence="0.996819">Maximilian Bisani Paul Vozila Olivier Divay Jeff Adams</author>
<affiliation confidence="0.96529">Nuance Communications</affiliation>
<address confidence="0.9803315">One Wayside Road Burlington, MA 01803, U.S.A.</address>
<abstract confidence="0.999268272727273">Written documents created through dictation differ significantly from a true verbatim transcript of the recorded speech. This poses an obstacle in automatic dictation systems as speech recognition output needs to undergo a fair amount of editing in order to turn it into a document that complies with the customary standards. We present an approach that attempts to perform this edit from recognized words to final document automatically by learning the appropriate transformations from example documents. This addresses a number of problems in an integrated way, which have so far been studied independently, in particular automatic punctuation, text segmentation, error correction and disfluency repair. We study two different learning methods, one based on rule induction and one based on a probabilistic sequence model. Quantitative evaluation shows that the probabilistic method performs more accurately.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="4968" citStr="Beeferman et al., 1999" startWordPosition="754" endWordPosition="757">tically. These phenomena pose a problem that goes beyond the speech recognition task which has traditionally focused on correctly identifying speech utterances. Even with a perfectly accurate verbatim transcript of the user’s utterances, the transcriptionist would need to perform a significant amount of editing to obtain a document conforming to the customary standards. We need to look for what the user wants rather than what he says. Natural language processing research has addressed a number of these issues as individual problems: automatic punctuation (Liu et al., 2005), text segmentation (Beeferman et al., 1999; Matusov et al., 2003) disfluency repair (Heeman et al., 1996) and error correction (Ringger and Allen, 1996; Strzalkowski and Brandow, 1997; Peters and Drexel, 2004). The method we present in the following attempts to address all this by a unified transformation model. The goal is simply stated as transforming the recognition output into a text document. We will first describe the general framework of learning transformations from example documents. In the following two sections we will discuss a ruleinduction-based and a probabilistic transformation method respectively. Finally we present e</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Beeferman, Doug, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177 – 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>565</pages>
<contexts>
<context position="8385" citStr="Brill, 1995" startWordPosition="1279" endWordPosition="1280">on of paragraphs and section headings, etc. Before we turn to concrete transformation techniques, we can make two general statements about this problem. Firstly, in the absence of observations to the contrary, it is reasonable to leave words unchanged. So, a priori the mapping should be the identity. Secondly, the transformation is mostly monotonous. Out-of-order sections do occur but are the exception rather than the rule. 3 Transformation based learning Following Strzalkowski and Brandow (1997) and Peters and Drexel (2004) we have implemented a transformation-based learning (TBL) algorithm (Brill, 1995). This method iteratively improves the match (as measured by token error rate) of a collection of corresponding source and target token sequences by positing and applying a sequence of substitution rules. In each iteration the source and target tokens are aligned using a minimum edit distance criterion. We refer to maximal contiguous subsequences of non-matching tokens as error re�� �� ��train targets �� tokenize V manual correction final document gions. These consist of paired sequences of source and target tokens, where either sequence may be empty. Each error region serves as a candidate su</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, Eric. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543 – 565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>Kyung-ho Loken-Kim</author>
<author>James F Allen</author>
</authors>
<title>Combining the detection and correction of speech repairs.</title>
<date>1996</date>
<booktitle>In Proc. Int. Conf. Spoken Language Processing (ICSLP),</booktitle>
<pages>362--365</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5031" citStr="Heeman et al., 1996" startWordPosition="764" endWordPosition="767">ch recognition task which has traditionally focused on correctly identifying speech utterances. Even with a perfectly accurate verbatim transcript of the user’s utterances, the transcriptionist would need to perform a significant amount of editing to obtain a document conforming to the customary standards. We need to look for what the user wants rather than what he says. Natural language processing research has addressed a number of these issues as individual problems: automatic punctuation (Liu et al., 2005), text segmentation (Beeferman et al., 1999; Matusov et al., 2003) disfluency repair (Heeman et al., 1996) and error correction (Ringger and Allen, 1996; Strzalkowski and Brandow, 1997; Peters and Drexel, 2004). The method we present in the following attempts to address all this by a unified transformation model. The goal is simply stated as transforming the recognition output into a text document. We will first describe the general framework of learning transformations from example documents. In the following two sections we will discuss a ruleinduction-based and a probabilistic transformation method respectively. Finally we present experimental results in the context of medical transcription and</context>
</contexts>
<marker>Heeman, Loken-Kim, Allen, 1996</marker>
<rawString>Heeman, Peter A., Kyung-ho Loken-Kim, and James F. Allen. 1996. Combining the detection and correction of speech repairs. In Proc. Int. Conf. Spoken Language Processing (ICSLP), pages 362 – 365. Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Mary Harper</author>
</authors>
<title>Using conditional random fields for sentence boundary detection in speech.</title>
<date>2005</date>
<booktitle>In Proc. Annual Meeting of the ACL,</booktitle>
<pages>451--458</pages>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="4925" citStr="Liu et al., 2005" startWordPosition="748" endWordPosition="751">errors some of which may occur systematically. These phenomena pose a problem that goes beyond the speech recognition task which has traditionally focused on correctly identifying speech utterances. Even with a perfectly accurate verbatim transcript of the user’s utterances, the transcriptionist would need to perform a significant amount of editing to obtain a document conforming to the customary standards. We need to look for what the user wants rather than what he says. Natural language processing research has addressed a number of these issues as individual problems: automatic punctuation (Liu et al., 2005), text segmentation (Beeferman et al., 1999; Matusov et al., 2003) disfluency repair (Heeman et al., 1996) and error correction (Ringger and Allen, 1996; Strzalkowski and Brandow, 1997; Peters and Drexel, 2004). The method we present in the following attempts to address all this by a unified transformation model. The goal is simply stated as transforming the recognition output into a text document. We will first describe the general framework of learning transformations from example documents. In the following two sections we will discuss a ruleinduction-based and a probabilistic transformatio</context>
<context position="14762" citStr="Liu et al. (2005)" startWordPosition="2317" endWordPosition="2320">rmation models are trained on a disjoint set of reports that predated the evaluation reports. The typical document length is between one hundred and one thousand words. All dictations were recorded via telephone. The speech recognizer works with acoustic models that are specifically adapted for each user, not using the test data, of course. It is hard to quote the verbatim word error rate of the recognizer, because this would require a careful and time-consuming manual transcription of the test set. The recognition output is auto-punctuated by a method similar in spirit to the one proposed by Liu et al. (2005) before being passed to the transformation model. This was done because we considered the auto-punctuation output as the status quo ante which transformation modeling was to be compared to. Neither of both transformation methods actually relies on having auto-punctuated input. The auto-punctuation step only inserts periods and commas and the document is not explicitly segmented into sentences. (The transformation step always applies to entire documents and the interpretation of a period as a sentence boundary is left to the human 117 Table 1: Experimental evaluation of different text transform</context>
</contexts>
<marker>Liu, Stolcke, Shriberg, Harper, 2005</marker>
<rawString>Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and Mary Harper. 2005. Using conditional random fields for sentence boundary detection in speech. In Proc. Annual Meeting of the ACL, pages 451 – 458. Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Jochen Peters</author>
<author>Carsten Meyer</author>
<author>Hermann Ney</author>
</authors>
<title>Topic segmentation using markov models on section level.</title>
<date>2003</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<pages>471--476</pages>
<publisher>IEEE, St. Thomas, U.S. Virgin Islands.</publisher>
<contexts>
<context position="4991" citStr="Matusov et al., 2003" startWordPosition="758" endWordPosition="761"> pose a problem that goes beyond the speech recognition task which has traditionally focused on correctly identifying speech utterances. Even with a perfectly accurate verbatim transcript of the user’s utterances, the transcriptionist would need to perform a significant amount of editing to obtain a document conforming to the customary standards. We need to look for what the user wants rather than what he says. Natural language processing research has addressed a number of these issues as individual problems: automatic punctuation (Liu et al., 2005), text segmentation (Beeferman et al., 1999; Matusov et al., 2003) disfluency repair (Heeman et al., 1996) and error correction (Ringger and Allen, 1996; Strzalkowski and Brandow, 1997; Peters and Drexel, 2004). The method we present in the following attempts to address all this by a unified transformation model. The goal is simply stated as transforming the recognition output into a text document. We will first describe the general framework of learning transformations from example documents. In the following two sections we will discuss a ruleinduction-based and a probabilistic transformation method respectively. Finally we present experimental results in </context>
</contexts>
<marker>Matusov, Peters, Meyer, Ney, 2003</marker>
<rawString>Matusov, Evgeny, Jochen Peters, Carsten Meyer, and Hermann Ney. 2003. Topic segmentation using markov models on section level. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 471 – 476. IEEE, St. Thomas, U.S. Virgin Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jochen Peters</author>
<author>Christina Drexel</author>
</authors>
<title>Transformation-based error correction for speechto-text systems.</title>
<date>2004</date>
<booktitle>In Proc. Int. Conf. Spoken Language Processing (ICSLP),</booktitle>
<pages>1449--1452</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5135" citStr="Peters and Drexel, 2004" startWordPosition="779" endWordPosition="782"> with a perfectly accurate verbatim transcript of the user’s utterances, the transcriptionist would need to perform a significant amount of editing to obtain a document conforming to the customary standards. We need to look for what the user wants rather than what he says. Natural language processing research has addressed a number of these issues as individual problems: automatic punctuation (Liu et al., 2005), text segmentation (Beeferman et al., 1999; Matusov et al., 2003) disfluency repair (Heeman et al., 1996) and error correction (Ringger and Allen, 1996; Strzalkowski and Brandow, 1997; Peters and Drexel, 2004). The method we present in the following attempts to address all this by a unified transformation model. The goal is simply stated as transforming the recognition output into a text document. We will first describe the general framework of learning transformations from example documents. In the following two sections we will discuss a ruleinduction-based and a probabilistic transformation method respectively. Finally we present experimental results in the context of medical transcription and conclude with an assessment of both methods. 2 Text transformation In dictation and transcription manag</context>
<context position="8303" citStr="Peters and Drexel (2004)" startWordPosition="1267" endWordPosition="1270">ext system. inverse of tokenization and includes conversion of number words to digits, rendition of paragraphs and section headings, etc. Before we turn to concrete transformation techniques, we can make two general statements about this problem. Firstly, in the absence of observations to the contrary, it is reasonable to leave words unchanged. So, a priori the mapping should be the identity. Secondly, the transformation is mostly monotonous. Out-of-order sections do occur but are the exception rather than the rule. 3 Transformation based learning Following Strzalkowski and Brandow (1997) and Peters and Drexel (2004) we have implemented a transformation-based learning (TBL) algorithm (Brill, 1995). This method iteratively improves the match (as measured by token error rate) of a collection of corresponding source and target token sequences by positing and applying a sequence of substitution rules. In each iteration the source and target tokens are aligned using a minimum edit distance criterion. We refer to maximal contiguous subsequences of non-matching tokens as error re�� �� ��train targets �� tokenize V manual correction final document gions. These consist of paired sequences of source and target toke</context>
<context position="10096" citStr="Peters and Drexel (2004)" startWordPosition="1555" endWordPosition="1558">and-side matches a sequence of already correct tokens. Rules are ranked by the difference in these counts scaled by the number of errors corrected by a single rule application, which is the length of the corresponding error region. This is an approximation to the total number of errors corrected by a rule, ignoring rule interactions and non-local changes in the minimum edit distance alignment. A subset of the topranked non-overlapping rules satisfying frequency and minimum impact constraints are selected and the source sequences are updated by applying the selected rules. Again deviating from Peters and Drexel (2004), we consider two rules as overlapping if the left-hand-side of one is a contiguous subsequence of the other. This procedure is iterated until no additional rules can be selected. The initial rule set is populated by a small sequence of hand-crafted rules (e.g. “impression colon” —* “IMPRESSION:”). A user-independent baseline rule set is generated by applying the algorithm to data from a collection of users. We construct speaker-dependent models by initializing the algorithm with the speakerindependent rule set and applying it to data from the given user. 4 Probabilistic model The canonical ap</context>
</contexts>
<marker>Peters, Drexel, 2004</marker>
<rawString>Peters, Jochen and Christina Drexel. 2004. Transformation-based error correction for speechto-text systems. In Proc. Int. Conf. Spoken Language Processing (ICSLP), pages 1449 – 1452. Jeju Island, Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>