<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.673472">
American Journal of Computational Linguistics Microfiche 64
the FINITE STRING
</note>
<title confidence="0.872697">
NEWSLETTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS
</title>
<author confidence="0.928917">
VOLUME 14 - NUMBER 2 MAY 1977
</author>
<affiliation confidence="0.711306">
Report of the ACL 1977 Annual Meeting Panel on Speech
Understanding and Computational Linguistics&apos;&apos;: A Cri-
tical Examination of the ARPA Project,
</affiliation>
<figure confidence="0.163308666666667">
by-Stanley R. Petrick 2
Essays on Lexical Semantics, edited by V. Ju. Rozencvejg,
Reviewed by Ernst von Glasersfeld 16
Constituent and Pattern in Poetry, Archibald A. Hill,
Reviewed by James Joyce 34
CURRENT BIBLIOGRAPHY 42
</figure>
<affiliation confidence="0.607281444444444">
AMERNAN JOURNAL OF COMPUTATIONAL LINGUISTICS is published
by the Association for Computational Linguistics.
SECRETARY-TREASURER: Donald E. Walker, Stanford Research
Institute, Menlo Park, California 94025.
EDITOR: David G. Hays, 5048 Lake Shore Road, Hamburg, New
York, 14075
EDITORIAL ASSISTANT: William Benzon
Copyright 0977
Association for Computational Lingbistics
</affiliation>
<note confidence="0.658247">
American Journal of Computational Linguistics Microfiche 64 : 2
REPORT ON
1977 ACL PANEL ON
</note>
<sectionHeader confidence="0.681104" genericHeader="abstract">
SPEECH UNDERSTANDING AND
COMPUTATIONAL
LINGUISTICS:
A CRITICAL EXAMINATION OF THE ARPA PROJECT
</sectionHeader>
<author confidence="0.687191">
STANLEY R. PETR I CK
</author>
<affiliation confidence="0.52476175">
Jonathan Allen, Chairman
Jack Mostow, Carnegie-Mellon University
Donald Walker, Stanford Research Institute
William Woods, Bolt Beranek and Newman
</affiliation>
<bodyText confidence="0.995112">
Th p present report summarizes the statements of the panelists
and paraphrases the questions and answers that followed their
talks.
</bodyText>
<note confidence="0.939646">
REPORT OF THE ACL 1977 ANNUAL MEETING PANEL ON 3
SPEECH UNDERSTANDING AND COMPUTATIONAL LINGUISTICS
A CRITICAL EXAMINATION OF THE ARPA PROJECT
</note>
<author confidence="0.795008">
S. R. Petrick
IBM T J. Watson Research Center
</author>
<bodyText confidence="0.98883245">
This panel discussion was chaired by Jon Allen of MIT. The participants were Jack
Mostow of Carnegie-Mellon University (ably substituting for Raj Reddy on short notice), Don
Walker of the Stanford Research Institute, and Bill Woods of Bolt, Beranek and Newman
The panel format featured a half hour talk by each panelist followed by a period of discussion
centered around questions from the audience.
Jon Allen began by reminding the audience that the ARPA five year speech under-
standing project had just ended last October, and that this was an appropriate time to step
back and ask just what had been learned as a result of that project He then turned the
podium over to the panelists to address that issue.
Don Walker began by supplying some general background information on all the
ARPA-sponsored efforts before he tuened to the specific accomplishments of SRI. They were
all concerned with speech understanding rather than speech recognition, and they each
focussed on one or more specific tasks SRI, for example, was concerned with information
retrieval with respect to a navy ship data base. This concentration on specific tasks limited
generality somewhat. There was also a focus on producing an operational system. This was
good in some respects, but this emphasis on results did make it impossible to follow many
promising research paths. Performance requirements, such as size of vocabulary and process-
ing time, were not all realistic.
The approaches followed differed widely from group to group These diverse ap-
proaches were perhaps appropriate to the state of knowledge which existed, but they did limit
</bodyText>
<sectionHeader confidence="0.650766" genericHeader="method">
ARPA SPEECH PANEL 4
</sectionHeader>
<bodyText confidence="0.960646764705882">
the opportunity to share results. There were competitive aspects to the ARPA effort. In
addition, the necessity to attend lots of steering committee meetings hindered local progress.
The computing power made available for the ARPA-sponsored efforts was good but
still not enough It was not as much as the investigators had hoped for. Walker referred to
the money which had been expended and remarked that although it was substantial, it was not
enough. Not only would more computing power have been useful but more people and more
time would also have contributed to improved results.
Turning then to a more detailed examination of the SRI effort, Walker discussed four
system issues which had been dealt with: (a) integration of speech, syntax, semantics, etc.,
1 e., specifying contributions and interactions of multiple knowledge sources perspicuously and
efficiently, (b) cooperation, i.e., sharing information and avoiding duplication of effort, (c)
evaluation, i.e., rating alternatives and choosing what to do next, and (d) attention, i e.,
avoiding dissipation of resources over competing alternatives He also discussed some
component issues including those required for natural language specification, task modeltrig,
and dialog interactions
Typical sentences in the SRI information retrieval application were cited and discussed
These included
</bodyText>
<sectionHeader confidence="0.9893146" genericHeader="method">
WHAT IS THE LENGTH OF THE LAFAYETTE?
THE GEORGE WASHINGTON?
DID GENERAL DYNAMICS BUILD THE SHARK?
WHO OWNS IT?
WHICH FRIGATES WERE BUILT BY CAMMELL LAIRD COMPANY?
</sectionHeader>
<bodyText confidence="0.8898938">
The SRI system organization was discussed briefly Those portions of the system
concerned with acoustics, phonetics and phonology were handled by SDC on their own
computer while _higher levels such as syntax and semantics were treated by SRI Basically, a
flexible control facility provided for such alternatives as either building structure up from
acoustically determined words or building it top-down in a syntax-directed fashion. In any
</bodyText>
<note confidence="0.729153">
AR PA SPEECH PANEL 5
</note>
<bodyText confidence="0.956688333333333">
case, a semantic network was constructed using scores or the degree of phonetic match
involved with various alternatives.
Turning to the definition of the input language, Walker stated that it is accomplished
by means of linguistically motivated (ATN) rules that are general and extensible over a variety
of domains. These provide a means for adjusting (&apos;tuning&apos;) the language definition to particu-
lar domains without loss of generality. Syntactic, semantic, and discourse information is
combined within the rules that define words and phrases.
The SRI semantic component makes use of partitional semantic networks. It handles
higher-order logical predicates, especially quantifiers It provides deduction routines for
retrieval and inference that can access a supplementary relational data base in responding to a
user&apos;s query It also provides a network substructive that is converted to an English sentence
or phrase to answer a user&apos;s question.
SRI discourse modeling is based on in-depth studies of domain-oriented dialogs A
model of dialog context is encoded through the use of semantic partitions. Dialog context
(through semantic closeness and syntactic considerations) is also used to find the meanings of
elliptical expressions and the referents of definite noun phrases
System integration was briefly discussed. In the SRI system this provides for interac-
tion of information from various sources of knowledge — syntax, semantics, and discourse -- as
part of the language definition itself The SRI effort avoided commitment to a particular
system control strategy, allowing instead for flexible use of various strategies for putting
together words and phrases out of incomplete and uncertain fragments.
The SRI system control facility was the last topic discussed by Walker. The points
made about it included the following: Ratings are assigned to incomplete sentence structures
reflecting the highest possible continuation of each such structure. The priorities of available
alternative continuations are assigned as a result of those ratings. Data structures are organ-
ized for testing hypotheses about utterances in a manner that avoids duplication of effort.
Combinations of top-down, bottom-up, and bidirectional strategies are allowed. The flexibility
</bodyText>
<page confidence="0.589927">
6
</page>
<note confidence="0.8787175">
ARPA SPEECH PANEL
provided by the SRI system control made possible extensive experimental studies to evaluate
design alternatives. Control strategies which were evaluated included the following four paired
alternatives:
</note>
<listItem confidence="0.780349111111111">
1) Island driving versus left to right processing of utterances
2) Testing for all words versus testing for just good matches
3) Using sentential context versus ignoring it
4) Focussing., on selected alternatives versus continuing on a single successful path until it
succeeds or fails.
Experimental findings from these studies of alternatives included
a) Context checking is important; it helps priority setting
b) Testing for all words improves performance but it is costly. A more efficient mapper is
needed.
</listItem>
<bodyText confidence="0.8214365">
c) Island driving produced wild exponential branching on long sentences.
d) Focussing on selected alternatives is less effective than best first following of a single
path. Nevertheless, simple best-path-first parse algorithms are not good enough
e) There is a strong need for a phrase mapper for word coarticulation
f) A high false alarm rate was reported. Hit scores were better than false alarm scores
Bill Woods, the next speaker, began with a discussion of a speech spectrogram He
described early experiments in which two investigators, Ken Stevens and Dennis Klatt,
attempted to &amp;quot;read&amp;quot; spectrograms either on a segment by segment basis or making additional
use of their knowledge of English phonology and syntax. They correctly transcribed between
70 per cent and 75 per cent of the individual segments considered in isolation (some segments
were only partially identified, i.e., certain of their distinctive features were correctly identi-
fied), but attained 96 per, cent word accuracy in an experiment which used a computer
program as an aid to human classification together with the human skills and knowledge of
linguistic constraints they commanded.
A number of observations were made. They included the following:
ARPA SPEECH PANEL 7
</bodyText>
<listItem confidence="0.991140363636363">
(1) Small function words are highly unreliable anchors.
(2) Acoustically based matching alone is insufficient; accidental word matches outnumber
correct ones.
(3) Speech understanding is essentially a nondeterministic process.
(4) Sequential left-to-right scanning presents problems; the first and last words are often not
pronounced carefully; the first stressed word is a more reliable anchor There is a need
to recover from any garbled word, especially the first one
(5) The space of alternatives must be open-ended You might have to go way down the list
of likely alternatives to find the right one
(6) There is a strong need for merged representations. many similar hypotheses snare
common parts.
</listItem>
<bodyText confidence="0.997423375">
A discussion of the sources of knowledge in the BBN System followed.Acoustic-
phonetic, phonological, lexical, syntactic, semantic, discourse, and factual sources of knowledge
were considered.
A program to assign scores to each of a number of overlapping segments for each
possible candidate phoneme was described. Results of an experiment were described in which
the correct phoneme was identified 53 per cent of the time and the first or second choice was
correct 70 per cent of the time. Per cent of phonemes correct within the first N choices for
various values of N was reported as follows
</bodyText>
<equation confidence="0.57585">
1 2 3 4 5 6 7
</equation>
<bodyText confidence="0.9241356">
% correct 53 70 75 79 83 84 86
The BBN verification component was next discussed It takes an analysis-by-synthesis
approach to word matching. A synthesis-by-rule procedure is used to generate word temp-
lates, and these templates are compared with a parametric version of the signal to determine
the closeness of match. The pragmatic ATN-based grammar which served as the .syntactic
component was identified as that described by G. Brown in a paper presented earlier that day.
ARPA SPEECH PANEL 8
A diagram of the BBN HWIM System organiiation was discussed briefly. The basic
operation of the system was described as follows: (1) create initial theories (2) rank them,
(3) give them to the parser, (4) have the parser suggest the n best words (5) verify best
matches, (6) add results on to islands previously identified to form larger islands, and (7) let
these new events trigger new theories and continue this cycle.
Woods then presented transparencies citing in parallel columns the ARPA-imposed
19:71 goals and the degitee of success attained by the BBN HWIM System in meeting those,
goals. I include reproductions of those transparencies.
</bodyText>
<figure confidence="0.703872">
ARPA Goals (from 1971) HWIM (1976)
The system should: The system did:
1 Accept continuous speec-h. Accept connected speech
2 From many From 3
3 Cooperative speakers of Cooperative male general
the general American American speakers
dialect
4 In a quiet room In a somewhat quiet
computer terminal room
5 Over a good quality Over an ordinary close-talking
microphone talking microphone
6 Allowing slight tuning With no tuning of the system
of the system per speaker for the individual speaker
7 But rewring only And requiring no speaker
natural adaptation by adaptation
the user
</figure>
<footnote confidence="0.938662625">
8 Permitting a slightly On 1097 words with no post-
selected vocabulary selection
of 1000 -words
9 With, a highly artificial With a somewhat natural
syntax combined syntax and semantics
(average branching &gt; 100)
10 And a highly constrained And a well defined task.
task
</footnote>
<table confidence="0.999284">
ARPA SPEECH PANEL 9
11 With a siniple psycho- With no user model
logical model of the user
12 Providing graceful. inter And modest interaction
action capabilities
13 Tolerating less than 10% With 57% semantic error
semantic error
14 In a few times real time In about 1350 times real time
15 On a 100- Mips Machine On a .35 Mips PDP-10
16 With 2561t, 36-bit words
17 With a hierarchical system
organization
18 (Cost per utterance)
19 And be demonstrable in And was operational
1976 with a moderate 12 Octbber 1976
chance of success
</table>
<tableCaption confidence="0.989729">
I also include information from a summary of results transparency which casts further
light on the 57 per cent semantic error figure cited in, item 13. In discussing this summary
Woods stressed that the difficulty of recognition is very much affected by the branching factor
associated with the syntactic component in question and with the distinctiveness of the paths
allowed.
</tableCaption>
<table confidence="0.995234157894737">
ARRA SPEECH PANEL 10
Summary of Results 10/31/76
124 Utterances by 3 Speakers
Average Utterance = 6.1 words = 1_8 seconds
BIGDICT MIDDICT
(1097 words) (409 words)
Correct 41% 48%
Semantically OK 2% 4%
Incorrect
Close 23% 20%
So-so 10% 6%
Bad 5% 6%
Gave up (150 18% 14%
theories)
System Broke 1% 1 ok
Estimated average 196 67
branching (words)
Speed (times 1350 1050
real time)
</table>
<bodyText confidence="0.979961333333333">
Woods closed with a discussion of those natural language problem areas which had
been affected by results obtained through speech understanding research. They included the
following:
</bodyText>
<listItem confidence="0.9902592">
(1) Coping with ambiguity (BBN did not eliminate this by fiat)
(2) Error recovery (island driving is good in this respect)
(3) Parsing algorithms
(4) Interaction of syntax, semantics, and pragmatics
(5) Degrees of &amp;quot;grammaticality&amp;quot; (BBN was just getting into this. SRI somewhat mo,re so.)
</listItem>
<figure confidence="0.882185166666667">
ARPA SPEECH PANEL 11
(6) Grammar-writing methodologies (e.g., general categories and specific filters at SRI,
domain-specific categories at BBN, and a compromise between these as suggested in
the paper by Bobrow and Bates)
0) Discourse and dialog models
(8) Human performance and linguistic theory.
The final talk of the afternoon was given by Tom Moslow of Carnegie-Mellon
University. The CMU project featured a task domain concerned with information retrieval for
a set of AI abstracts. Vocabulary size was 1011 words and a branching factor of 9.53 was
cited, indicating a considerably more restricted syntax than that of the other efforts. Two
distinct systems, HARPY and HEARSAY-II were implemented, and the results cited for each
were:
HARPY
97% word accuracy
91% sentence accurac
95% semantic accuracy
27.9 Mips (about 30 times real speech rate)
HEARSAY-II
</figure>
<subsectionHeader confidence="0.699658857142857">
86% word accuracy
73% sentence accuracy
91% semantic accuracy
85.0 Mips (85 times real speech rate)
After a discussion of how signal variability and imperfect knowledge lead to a combi-
natorial explosion of alternative transcriptions, making speech understanding very difficult,
Mostow gave more detailed descriptions of both HEARSAY-II and HARPY.
</subsectionHeader>
<bodyText confidence="0.968401032258064">
HEARSAY-II is an asynchronous, data-directed, hypothesize and test model in which
cooperating, independent knowledge sources communicate by means of a &amp;quot;blackboard&amp;quot;. In
this model it is easy to deactivate a particular knowlpdge source, and this made it relatively
ARPA SPEECH PANEL 12
easy to evaluate the worth -of each such source. Focus of attention mechanisms were studied.
The -systern is island-driven and makes use of a task-specific pattern matching grammar
HARPY is quite different. It integrates knowledge sources into a uniform state-space
netwOrk at the phoneme level. All knowledge sources, high and low, must be integrated into
such h network. It was claimed to be very efficient for sufficiently restricted languages with a
sentence accuracy over 90 per cent on a 1 MIP PDP/10 in 10 to 30 times real time Proceed-
ing to a new task requires that a similar but new grammar be produced, which Mostow claimed
was not a major undertaking.
A general observation was made and illustrated by specific experience accrued at
CMU. This was that different uses of the same linguistic knowledge require different knowl-
edge representations. Precompiling knowledge into an appropriate data structure for a given
use is more efficient than interpreting a single global knowledge structure.
The philosophy of Language design adopted by CMU was, given a set of typical
sentences, to construct a sequence of increasingly larger languages each of which properly
contains its successor. The last member of this sequence must account for the given corpus.
In other words it was proposed that full linguistic constraints be relaxed at first to produce a
simpler model with a correspondingly more efficient recognition procedure; it was argued that
this provides a cheap filter on data, and the output of such a model can, in turn, be con-
strained by a more elaborate model. In illustration bf this approach with respect to HARPY, it
was suggested that a finite state net could serve as the simpler model and an ATN grammar as
the more elaborate model. A HEARSAY-II example was also discussed in which a Markov
model serves as the simpler model and a pragmatic template grammar as its more elaborate
extension.
The problem- of understanding ungrammatical speech was touched on briefly. The
approach suggested was to use a template-based grammar but to accept approximate template
realizations. If the normal grammar fails, find grammatical fragments and concatenate them if
permitted by general rules.
</bodyText>
<table confidence="0.432359">
ARPA SPEECH PANEL 13
</table>
<tableCaption confidence="0.8082049">
Search techniques for both HEARSAY-II and HARPY were discussed. The locus
search method of HARPY was cited as one of the main contributions of CMU research along
with the HEARSAY-II blackboard model and the results obtained on the interpretation of
ungrammatical utterances.
Future directions included writChsion of the system(s) to provide for more general
(larger and more flexible) input la nguagta and extension of the methods developed to such
other At tasks as the development of vision systems.
Following the prepared panel talks questions were directed to the panel members from
the floor. I cannot identify all the questioners so I will not attempt to supply names. I have
paraphrased questions and answers in most cases.
</tableCaption>
<table confidence="0.82797025">
Q: What linguistic constraints were imposed other than a 1000 word vocabulary?
A: Formal grammars of one type or another were used (for example, an ATN grammar for
the BBN effort). Sentences which were grammatical with respect to these formal
grammars were read by subjects.
Q: Was any use made of intonation?
A: (Woods) Some use of intonation constraints was implemented into the HWIM System
rather late in the program but was never tested in the final crunch.
Q: What results from speech work should help in text processing?
</table>
<tableCaption confidence="0.87782925">
A: (Woods) The development of a model for completing unfinished sentences. This has
certain psycholinguistic implications.
A: (Walker) Language definition involving general categories and pragmatic grammars for
one thing. The integration of all knowledge sources at the phrase level is also a signifi-
cant contribution.
A: (Mostow) The determination of how much milage can be squeezed out of cheap
methods. We expect to continue this type of speech research but not to do much more
on high level constraints.
</tableCaption>
<table confidence="0.364631">
Q: Why did,CMU do so much better than the others?
ARPA SPEECH PANEL 14
A: (Combined responses of all three panelists)
CMU picked a simpler domain perhaps and definitely used a much cruder (more
restrictive) grammar.
</table>
<subsectionHeader confidence="0.9095055">
The BBN LISP Sysfem and operating system were less efficient than CMU&apos;s
lower level programming.
</subsectionHeader>
<bodyText confidence="0.897699020408163">
BBN aimed for a tougher target (as evidenced by their more realistic grammar
and correspondingly higher branching factor) and didn&apos;t make it in spite of a ten &amp;quot;0 per
month improvement over the last few months.
The BBN results were intermediate bench marks along the way toward a target
which was never reached because of a lack of time whereas CMU was more realistic in
budgeting its time.
CMU had no real theory. It just threw in a new dictionary entry whenever
necessary.
CMU emphasis was placed on general Al techniques, not linguistic principles
BBN could only process 10 utterances per night whereas CMU could process 10
utterances per hour. Hence, CMU could consider more data SRI did not have a total
system avialable to them, being dependent on SDC for their front end, so they were at a
disadvantage in this respect. All speakers agreed that once real time speeds are achieved,
the user interaction possibilities will be attractive.
SRI not only had no opportunities for fine tuning, but their approach was
claimed to be even more linguistically motivated than BBN&apos;s. It was similar to BBN&apos;s
before the latter&apos;s switch to a pragmatic grammar.
SRI reported a 40/1 false alarm-to-hit ratio on the part of the acoustic compo-
nent supplied by SDC but still achieved close to 90% semantic success Its system was
estimated to run in 200-300 times real time on a vocabulary of 320 words and a
grammar with a static branching factor of 110.
ARPA SPEECH PANEL 15
Precomputation gives HARPY its efficiency bin intorporatioti of new knowledge
into HARPY makes new precomputatiOn necessary and it is expensive.
Q. Does anyone currently have a government contract to pursue speech recognition?
A: Not to anyone&apos;s knowledge, but mote general contracts may include some speech work.
Speech understanding is not dead but is currently in abeyance.
Q: Is the rumor true that NSF may pick up some speech work and therefore some projects
that would otherwise be funded will suffer?
A: It is true in the sense that people previously supported by ARPA funds are entering the
competitive pool for a static amount of research funds There is a &amp;quot;new player&amp;quot; however
in the form of the Sloan Foundation, which is supporting less capital intensive research,
is less systems-oriented, and is seeking to encourage cooperation between different
groups.
Q: What would you do different if you were starting over?
A: (Woods) I would do some things different but probably that woukl be wrong. I would
give more reassurance to new people starting work on the project that they were on the
right track. ARPA managers need such reassurance too. More intermediate results might
have kept up the interest of the sponsors. Too many decisions had to be made on the
basis of inadequate experimentaion.
A: (Walkei) Groups were brought together and educated and then° disbanded.
Q: I can hear the difference between /s/ and /z/, etc. Why don&apos;t you do some better
acoustic work so as to be able to make such discriminations reliably?
A: You &apos;&apos;hear&amp;quot; the difference in large part by means of higher constraints such as syntax,
semantics, and even pragmatics. It is well documented, that the information for identifi-
cation just isn&apos;t in the segment to be recognized in many cases.
Q: Are there any companies now working on speech recognition?
A: Yes. IBM is, and other companies such as Threshold Technology are working on word
template matching.
</bodyText>
<figure confidence="0.971335545454545">
American Journal of Computational Linguistics Microfiche 64 : .16
ESSAYS ON LEXICAL SEMANTICS,
VOLUME II
EDITED BY V. Ju. ROZENCVE)G
SPRAKF6RLAGET SITIPTOR
Box
S 104 65 Stockholm
1974
282 pages ISBN 91-7282-065-9
SKr 60
REVIEWED BY ERNST VON GLASERSFELD,
</figure>
<bodyText confidence="0.872474166666667">
Department of Psychology
University of Georgig-
Athens 30602
Since the editor did not provide an introduction to this second
volume, the only information we get about the authors of the 9
papers it contains is in a short blurb on the back cover of the
book.
The young linguists who gathered fifteen years around the
Laboratory of Machine Translation [in Moscow] were attracted by
the prospects of creative endeavor which the problem of automatic
tiansaltion offered: the problem itself and the search fOr a
way to solve it did not permit any dogma to dominate... They
wanted...to understand and clearly formulate how a person under-
stands the meaning of an utterance in&apos;the given language and
then constructs a message in another language that preserves the
original meaning.
Reading the papers, it quickly becomes clear that, though these
authors may indeed not have a common homogeneous theory of semantic
</bodyText>
<subsectionHeader confidence="0.381189">
Lexical Semantics 17
</subsectionHeader>
<bodyText confidence="0.99541196875">
analysis, they nevertheless are what I should call &amp;quot;dogmatic&amp;quot; in
their approach to meaning and how it functions in communication.
Most of them refrain from making their basic philosophical posi-
tion explicit; but, as I read them, I get the impression that they
would all subscribe to the proaranunatic statement pn the fitst page
of the first essay in the volume, Towards a Functioning &apos;Meaning-
Text&apos; Model of Language, by I. A. Melink and A. K. Llkovskij:
The main aspect of hUman language behaviour consists
undoubtedly in the speaker&apos;s transmitting to the hearer
certain information, or content, which the present authors
decided to refer to (for brevity&apos;s sake and rather indiscri-
minately) as MEANING. The hackneyed phrase &amp;quot;Language is a
means of human communication&amp;quot; implies nothing beyond the
fact that the speaker has a certain meaning in mind and
produces a text to convey it, and the hearer extracts the
intended meaning from the text. Language (de Saussure&apos;s
langue) functions here literally as a mechanism, a &apos;mean-
ing-text-meaning&apos; transformer. The linguistic model should
therefore aim at &apos;meaning-text&apos; transition as its core
operation.
At the risk of appearing pedantic, I am going to look rather
closely at this passage. It purports to lay out the foundation
on which models of language can be built and to establish the
point of view from which we are to examine language. It is easy
to agree that &amp;quot;language is a means of humah communication.&amp;quot; But
if we agree to that - and I, for one, certainly do - we shall be
obliged to take into account what we know about communication
and its limitations. And that, I would suggest, entails a cer-
tain amount of disagreement with what is said in the rest of the
quoted passage.
From a communication-theoretical point of view, to say that
text conveys meaning is at best a metaphor, and to say that the
</bodyText>
<subsectionHeader confidence="0.938843">
Lexical Semantics 18
</subsectionHeader>
<bodyText confidence="0.998181576923077">
receiver &amp;quot;extracts the meaning from the text&amp;quot; is a misrepresen-
tation. Even in simple telegraphic communication the receiver
does not, indeed could not, extract the meaning of the dot-and-
dash signals from those signals. The fact that, say, a single
dot means &amp;quot;e&amp;quot; can be extracted only from the conventionally
established listing that links the set of signals to the letters
of the alphabet. Such signals have been correctly described as
&amp;quot;selectional instructions&amp;quot;; that is to say, they tell the receiver
which letters he is to select from the list of letters which he
already has. A signal that has no conventional link (semantic
nexus) to a letter in the code list simply has no &amp;quot;meaning&amp;quot;. Al-
though on the level of telegraphy signal meaning is clearly a
much simpler affair than the meaning of sentences and texts (where
all sorts of combinatorial rules come into play), one important
aspect remains the same for any form and any level of communica-
tion: Meaning is under all circumstances something that the
receiver has to construct or, if you will, reconstruct, out of
material he already possesses. The signal, be it an electric
impulse or a string of phonemes, may tell him what to choose and
how to put it together, but it can never supply the material
itself. To put it very simply: Meaning does not travel between
a source and a destination or between a sender and a receiver;
what travels is a signal - and if communication is to be achieved,
that signal must be semantically linked to the same meaning at
both ends. This immediately raises the question: How can we
ever be sure that the sender&apos;s meaning and the receiver&apos;s meaning
</bodyText>
<subsectionHeader confidence="0.463569">
Lexical Semantics 19
</subsectionHeader>
<bodyText confidence="0.999876461538461">
are, in fact, the same? To answer that question we should need
more than a theory of language we should need a theory of know-
ledge, i.e. a full worked out epistemology.
Judgina by the essays collected in this volume, none of the
authors seems to be wholly aware of this indispensable connection
between a theory of meaning and a theory of knowledge. This fuzzi-
ness leads to a rather indiscriminate mixing of acute practical
observations and more or less inappropriate theoretical metaphors;
and this, in turn, makes for difficult reading - if by &amp;quot;reading&amp;quot;
we mean the endeavor to reconstruct what the author intended to
say.
Mel&apos;.6uk and cplkovskij come closest to my understanding of
meaning when they say &amp;quot;meaning appears as a construct, a bundle
of correspondences between actual (content-) equivalent utter-
rances...&amp;quot; or &amp;quot;the command of linguistic meaning manifests itself
in the speaker&apos;s ability to express one and the same idea in a
number of different ways ...&amp;quot; (p.2). The ability to paraphrase
thus becomes a criterion. for semantic competence and &amp;quot;designing
functioning paraphrasing systems&amp;quot; becomes a major concern of
semanticists. This, I believe, is indeed one way of successfully
avoiding epistemological problems. Without trying to achieve an
act of communication, the semantic analyst (provided he is a
proficient speaker of the language) can be himself decide whether
or not two or more arrauements of linguistic items (i.e., signals)
are semantically linked to one and the same conceptual structure
(idea). He can decide this by himself because the idea, the
</bodyText>
<subsectionHeader confidence="0.536882">
Lexical Semantics 20
</subsectionHeader>
<bodyText confidence="0.998663777777778">
representation of the linguistic items, and the linkage between
these linguistic and conceptual parts are all in his own head.
According to Melquk and •colkovskij &amp;quot;the first and principal
goal of present-day semantics [is) the construction of a meaning-
text model of language.&amp;quot; (p.3) One might think that such a
model would require a rather complete description of the &amp;quot;tdeas&amp;quot;
(i.e. the non-linguistic conceptual structures) on the one hand,
and on the other, of the linguistic items (morphemes, words,
syntactic structures) by means of wifich they can be &amp;quot;expressed&amp;quot;.
But the authors are careful to stress that they have singled out
an area of semantics &amp;quot;which is linguistic par excellence and aims
at modelling such a handli.ng of meaning as involves nothing but
a command of language and requires no special information about
the surrounding vorld.&amp;quot; (p.4) They add in parenthesis: Obvious-
lx, this diiStinction is hard to draw in many specific cases.&amp;quot;
Quite so. The distinction is indeed hard to draw because the two
kinds of information are not different in the way they are
purported to be different. &amp;quot;Command of languasie&amp;quot;, after all,
is knowledge of regularities and rules the language user gathers
in his experience of language, and &amp;quot;information about the surround-
ing world&amp;quot; id knowledge of regularities and rules he gathers in
other kinds of experience. According to one&apos;s epistemological
position, the &amp;quot;surrounding world&amp;quot; will be involved in both or in
neither,.
The few semantic representations Melquk and iolkovskij illus-
trate (e\g. a relational pattern for the situation designated by
expressions such as &amp;quot;to buy&amp;quot;, &amp;quot;to sell&amp;quot;, &amp;quot;to pay&amp;quot;, etc.) would
</bodyText>
<subsectionHeader confidence="0.484997">
Lexical Semantics 21
</subsectionHeader>
<bodyText confidence="0.999947038461538">
seem to be compatible with the rather more complete conceptual
analyses that have led Schank (1975) and Fillmore (1975) to speak
of scripts or frames respectively. But scripts and frames are
conceptua] scenarios derived from and dependent on actual experi-
ence, which is to say, they are representations of a surrounding
world&amp;quot;. Melquk and &amp;dkovskij suggest that experiential know-
ledge (&amp;quot;a huge encyclopedia and &apos;axioms of reality&apos; as well&amp;quot;,
p.17) is sdmething essentially different that should be kept &amp;quot;in
the background, but immediately available when necessary for all
kinds of checking operations, etc.&amp;quot; (p.17)
Their paraphrasing system, thus, is based not so much on
underlying conceptual structures but rather on the exchange of
linguistic items, an exchange which is to be carried out with the
help of a large number of substitution lists that take into
account certain relational properties of the lexical items they
contain. These relational properties are referred to as the
&amp;quot;syntax of semantic components&amp;quot; (p.44). Though the relations
comprised in this &amp;quot;syntaxoare many more than, say, the cases in
a,case grammar, they still seela to be derived from the structure
of language rather than from the Structure of non-linguistic
conceptualization; that is to say, they reflect relations between
items in language and only very indirectly, if at all, the concep-
tual relations by means of which we construct such entities as
objects, activities, processes, and states.
In short, the &amp;quot;meaning-text model&amp;quot;, because it approaches
meaning &amp;quot;as the invariant of synonymic transformation0 (p.41).
</bodyText>
<subsectionHeader confidence="0.36223">
Lexical Semantics 22
</subsectionHeader>
<bodyText confidence="0.912177">
will presumably lead to an extremely particularized mapping of
</bodyText>
<subsectionHeader confidence="0.768035">
what can be exchanged for what, on the level of linguistic ex-
</subsectionHeader>
<bodyText confidence="0.958994416666667">
pressions. In the last part of their paper the authors say that
it &amp;quot;constitutes a step towards experimental linguistics&amp;quot; (p. 42)
and that puts the effort into the proper perspective. Establish-
ing what is and what is not &amp;quot;synonymous&amp;quot; in a given language and
according to a given criterion of synonymy, is undoubtedly a
valuable enterprise. But the discovery and mapping of synonymous
expressions, acceptable paraphrases, and semantic equivalences,
though it certainly requires linguistic competence, does not auto-
matically provide a model for linguistic competence, let alone for
linguistic communication. Hence I find it difficult to agree that
this kind of investigation should be considered&apos;&amp;quot;the principal
goal of present-day semantics.
In the second paper of, the collection, Materdals for a
Russian-Somali Dictionary by A. K. solkovskij, the difficulty for
readers who don&apos;t know Russian will be compounded if they are also
ignorant of Somali. Though the translation, throughout the volume,
seems to be quit l adequate, tffe very fact that semantic analysis
(inevitably) often leads to language-specific results, means that
the reader has to take much of what is said on faith. Nevertheless
even if a particular analysis is not applidable to the language or
languages one knows, it may well stimulate ideas as to why and how
certain items should be analyzed.
In this paper olkovskij &amp;quot;proposes tentative entries for a
Russian-Somali dictionary consisting of 105 nouns occurring
</bodyText>
<subsectionHeader confidence="0.474877">
Lexical Semantics 23
</subsectionHeader>
<bodyText confidence="0.999651346153846">
frequently in political texts and compiled according to the recom-
mendations set forth above.&amp;quot; (p.56) The dictionary entries take
up 22 pages, i.e., two thirds of the paper. The brief&amp;quot;recommenda-
tions&amp;quot; contain an even briefer explanation of a particular trans-
lation problem: &amp;quot;Somali lacks the nominal transform of the verbal
phrase.&amp;quot; Hence, expressions such as the policy of the colonial-
ists have to be transformed into something like the policy pursued
by the colonialists, before they can be translated into Somali.
This is apparently complicated by the fact that Somali has a
variety of &amp;quot;empty verbs&amp;quot; and rather strict rules of usage that
determine with what items each of these can be used. The Problem
seems to be similar to the one English-speakers encounter when
they want to take a bath in Italy, because in Italian one does not
take a bath but makes it (fare il bagno). In Somali, however, the
problem is compounded by the fact that the. ,&amp;quot;empty&amp;quot; verbs mostly
require mahy more specifications, such as to whom the object
belongs, where it is, for what reason it is involved, etc. A
semanticist would be interested to know whether there are any
conceptual regularities br rules to be gleaned from the differen-
tial usage of these &amp;quot;empty&amp;quot; verbs in Somali and whether, in fact,
they are quite as empty as they seem. The paper does not attempt
any such analysis but gives, for each Russian head wprd, a selec-
tion of Somali constructions that have to be used when the word
is translated. Hence, this mini-dictionary, will no doubt be use-
ful to a student of Somali but it adds little to our knowledge of
the semantics of Somali or any other language.
</bodyText>
<subsectionHeader confidence="0.419173">
Lexical Semantics 24
</subsectionHeader>
<bodyText confidence="0.871609444444444">
L. N. Iordanskaja makes a courageous attempt to analyze A
Group_ of Russian Words Denoting EMotions. She briefly refers to
the basic classifications of Spinoza (joy, sorrow, and desire)
and Descartes (who added love, hatred, and admiration) and then
makes a radical division between &amp;quot;emotional states&amp;quot; and &amp;quot;emotional
attitudes&amp;quot;. Though the criterion for this division is not ex-
plained,, the definitions that follow suggest that &amp;quot;emotional
states&amp;quot; are alWays considered the result of a specific stimulus-
situation (actual .or envisioned) while &amp;quot;attitudes&amp;quot; are not. Intui-
tively this teems a sensible distinction to make; but, I believe,
it is somewhat misleading to proceed as though emotional states
could be treated without any reference to attitudes. Indeed, one
of the three characteristics the author uses to describe the mean-
ing of the chosen Russian emotion-words is the polar dimension of
&amp;quot;positive (pleasant)/negative(unpleasant)&amp;quot;, and I would argue that
in most, if not in all, cases it is precisely an &amp;quot;attitude&amp;quot; that
makes an individual evaluate something as &amp;quot;pleasant&amp;quot; or &amp;quot;unplea-
sant&amp;quot;. Apart from the limitation of the words that are being
analysed, there is nothing to disagree with in the paper. gs the
author herself says, &amp;quot;many (of the definitions) may probably be
refined.&amp;quot; Anyone familiar with Robert Plutchik&apos;s (1962) classifi-
cation of emotions will not find much novelty here.
In a 25-page essay, N. N. Leont&apos;eva deals with the almost
universal phenomenon of The Semantic Incompleteness of Texts, i.e.
with the remarkable fact that human users of language are so very
often able to construct a satisfactory meaning for linguistic
expressions when these expressions supply only incomplete
</bodyText>
<subsectionHeader confidence="0.502732">
Lexical Semantics 25
</subsectionHeader>
<bodyText confidence="0.999900884615385">
directives for such a construction. She differentiates syntactic
incompleteness (ellipsis), the referential indeterminacy of pro-
nouns (anaphoric expressions), the need for information that was
given earlier in the text, lack of one of the eltments required
by the relational configuration of a particular concept (e4g. the
knife cuts, where the agent is missing). and all the experientially
established relationships, causai and otherwise, that texts pre-
suppose in the reader.
Leont&apos;eva has compiled a set of 41 &amp;quot;semantic relations&amp;quot; and
half a dozen &amp;quot;metarelations&amp;quot; (relations relating relations) by
meahs of which the semantic connectivity of a text can be mapped
and tested for completeness. Each of the relations has two terms.
Wherever the text indicates a relation but not both the terms it
purports to relate, there is a case of semantic incompleteness
and an occasion to ask how the language user manages to infer or
retrieve the ited that is lacking in the linguistic expression.
This is clearly a very sensible pragmatic way to proceed because
it leads on the one side to progressive refinement of the master-
list of relations and, on the other, to insights into the as yet
largely uncharted inferential operations the ordinary language
&apos;user carries out when he &amp;quot;understands&amp;quot; a piece of language.
V. Ju. Rozencvejg in Notes on the Meaning of some French
Words, does much the same as Fillmore (1965) in his analysis of
111entailment rules&amp;quot;. The main part of the paper deals with aller
and venir while a short final section treats a wholly different
area, namely French particles of affirmation and negation.
</bodyText>
<subsectionHeader confidence="0.922986">
Lexical Semantics 26
</subsectionHeader>
<bodyText confidence="0.99974336">
Unlike Fillmore, who did not include the cotbinations of 22
and come with the infinitive of another verb, Rozencvejg does
consider the somewhat specialized infinitive constructions (e.g.
aller voir, venir se placer) and, I believe, it is the analysis
if these expressions that leads him to overrate the importance of
the &amp;quot;directional&amp;quot; element. Thus.he states that the sentence Olga
va (de Moscou) A Grenoble &amp;quot;communicates&amp;quot; to the addressee that the
speaker &amp;quot;Ls located at A (Moscow) or at some point between A and B
(Grenoble), but not at B or a point behind B or beyond B ...&amp;quot; If
this were the case, the language user would often have to have a
much more detailed map of the world in his head than he actually
has. In fact, a little late Rozencveig himself gives London as
example of a point &amp;quot;behind&amp;quot; Grenoble as seen from Moscow; actually
London and Grenoble are almost exactly equidistant from Moscow but
more than a third of that distance apart from each other.
Later in his paper, Rozencvejg documents the important obser-
vation that it is precisely by the use of the spatial implications
of these verbs of motion that a skillful writer influences the
visual representations of his readers - &amp;quot;not unlike a cameraman
who controls the perception of the viewer by appropriately posi-
tioning his camera.&amp;quot; (p.154) There is no doubt that this is so,
and it should alert the semantic analyst to the fact that the use
of aller and venir (as in the case of go and come in English) is
constrained rather by the addresse&apos;s location and direction of
view than by the speaker&apos;s.
</bodyText>
<subsectionHeader confidence="0.35284">
Semantic Analysis of Russian Negative Sentencesr by J. V.
Padaeva, illustrates in a semi-formal way the many difficulties
</subsectionHeader>
<bodyText confidence="0.954540838709678">
Lexical Semantics 27
encountered by the transformationaligt in his attempt to formulate
rules by means of which the various forms of negation in Russian
rAnd sgme other lahguages) might be derived. Some 30 pages of
Affssified examples (most of them in Russian, but some also in
Frencb) will provide a valuable survey for investigators of nega-
WEvon in other languages, especially if they can read Russian. The
autnor ends by saying:
&amp;quot;It is difficult to say whether one should, on the
basis of the complex problems connected with this trans-
fokmation, draw a pessimistic conclusion about the possi-
bilities of a formal description of language, or should
be satisfied with the depth of analysis required to solve
even the most simple concrete problem within the frame-
work of this theory.&amp;quot; (p.202)
This reader would suggest a third way out, namely that the
failure of transformational grammar may not necessarily imply that
language cannot be formally described by other theories.
The last three essays of the volume, covering some 170 pages,
are G. V. Dorofeev and Ju.S. Martem&apos;janov and examine different
aspects of the formalization of meaning and logical inferences
within and between sentences. The 4-line abstract that precedes
the first of the three papers iells us:
The present paper follows up the series of semantic
descriptions of groups of words pertaining to certain
situations of reality. The notions described here are
&amp;quot;obstacle&amp;quot;, &amp;quot;possible&amp;quot;, &amp;quot;way&amp;quot;, &amp;quot;actual way&amp;quot;, &amp;quot;leads to&amp;quot;
and some others. (p.207)
This at once tempts me^to ask: Well now, what will it be -
descriptions of words in terms of notions or descriptions of
notions in terms of words? - The next sentence, the first of the
essay proper, asserts that &amp;quot;to describe an informal meaning
Lexical Semantics, 28
formally means to substitute it completely, in all cases of its
use, with a new, formalized concept.&amp;quot; (p.207 Now I am really
baffled. If we apply this definition of &amp;quot;td describe formally&amp;quot;
to the above Abstract, we must conclude that in this paper the
notions &amp;quot;obstacle&amp;quot;, &amp;quot;possible&amp;quot;, etc., are going to be substituted
by &amp;quot;new, formalized concepts.&amp;quot; Is that really what the authors
intend to say? - Ap it turns out, it is not. What they are actual-
ly doing is, in a sense, the opposite and therefore quite sensible.
They try to grasp the notion or concept of, say, &amp;quot;obPtacle&amp;quot; and
then to find a formalized (logical) expression that renders it
without too much distortion. That is to say, they try to substi-
tute the word by a formal expression in such a way that the notion
or concept remains the same. For instance:
The statement &amp;quot;Some phenomenon b is an obstacle to
another phenomenon f is equivalent to: &amp;quot;If event b occurs,
event a does not take place&amp;quot;, or, in the notation off
mathematical logic:
b 12 . (p.209)
Though this is by no means their final formal represpntation
of &amp;quot;obstacle&amp;quot;, it does show what is being substituted for what and
that the final goal of the formalization is to be able to write
some sequence of conventional logical symbols (such as the one
given) for elie4y occurrence of the word &amp;quot;obstacle&amp;quot; without doing
too much violence to the concept or conceptual structure that con-
stitutes its meaning. &amp;quot;Semantic description&amp;quot;, in this paper then
means the replacing of ordinary-language words by logical formulas
which, intuitively, are deemed to be semantically equivalent, i.e.,
deemed to have the same conceptual content.
The authors note that-an atemporal (classical)logic does not
</bodyText>
<subsectionHeader confidence="0.374501">
Lexical Semantics 29
</subsectionHeader>
<bodyText confidence="0.988965554216867">
Burnish expressions that could successfully be substituted for
the words they are interested in. They introduce a time factor,
and that allows them to come up with a rather flexible, compre-
hensive notation. But even in its final and most elatoorate form,
which involves set-theoretical expressions such as &amp;quot;&apos;closure&amp;quot; and
&amp;quot;neighborhood&amp;quot;, the formalization is still based on certain
assumptions that seem unacceptable to a conceptual semanticibt.
To give one example, the concepts of &amp;quot;state&amp;quot; and &amp;quot;event&amp;quot; are
throughout differentiated by the assumption that the first takes
up a &amp;quot;period&amp;quot; (i.e., a stretch of time) while the second oerres-
ponds to a &amp;quot;moment&amp;quot; (i. e., a single point in time). It is easy
to show that what we call an &amp;quot;event&amp;quot; involves under all circum-
stances a change, and that, conceptually, a change requires a
sequence of at least two points. In fact, the difference between
state and event is not the respective presence or absence of a
lapse of time but rather that, with regard to a particular aspect,
an item is considered the same or not the same at two paints in
time.
The words analyzed in this essay are, in the translation,
listed in English only. That is unfortunate because the English
word &amp;quot;obstacle&amp;quot;, for instance, which serves as the main example,
does not really fit the authors&apos; formalization. In our language
it would not be correct to equate the meaning of &amp;quot;obstacle&amp;quot; with
that of &amp;quot;preclusion&amp;quot;. If b is an obstacle to E., it would often
be quite incorrect to formalize this state of affairs as &amp;quot;if b,
then not-g&amp;quot;. Obstacles need not preclude achievement, they merely
make it more difficult. After all, we have competitions called
Lexical Semantics 30
&amp;quot;obstacle races&amp;quot; and they would not be much fun if, by definition,
all competitors were precluded from reaching the goal.
In their second paper, using parts of the notation intro-
duced in the first, the authors set out &amp;quot;to reconstruct the impli-
cative and causal relations existing between the sentences of a
text ...&amp;quot; (p.225) The text they use to demonstrate their method
of is a Russian fable about a fox and a crane. The
highlights of the story are, %a) the fox invites the crane to a
dinner of porridge which the crane cannot eat because it is
spread on a plate, and (b) the crane then returns the invitation
and serves soup in a pitcher into which the fox cannot squeeze
his head. - It is clear that &amp;quot;obstacles&amp;quot; and their deliberate
creaeion are of paramount importance in this tale. But the
analysis goes very much farther. With the help of a large number
of pre-established formalized representations (same two dozen of
which are listed) for expressions that include difficult concepts
such as &amp;quot;A is vindictive&amp;quot;, &amp;quot;A is perspicacious&amp;quot;, &amp;quot;A is a friend
of B&amp;quot;, and the rather startling &amp;quot;A is a &apos;feed-back system&apos;&amp;quot;, some
50 inferences are derived from 10 sentences of the story. Thelse
inferences, we are told, are what makes the text a connected whole
I have no doubt that is what they do, but the explanations as to
how the formalized expressions have helped in bringing out the
connective implications are either inept or incomplete (or both).
One of the reasons for this failure is that the reader finds it
almost impossible to patch together the original unadulterated
fabel. There are lists and tables with comments, coded expres-
sions, translations, and the often wholly mysterious result of
Lexical Semantics 31
the analysis. (e.g. one &amp;quot;transition to deeper semantic strUcture
in terms of the basic notions used&amp;quot;, in the case of the sentence
&amp;quot;The Crane ate up all the soup himself&amp;quot; is the opaque statement:
&amp;quot;Fox is &apos;feed-back system&amp;quot;.)
It is clear that an enormous amount of work has gone into
this analysis and this reader feels that much of it would be very
interesting and useful if one could really get hold of it. It is
a great pity that the two authors, who refer to Fillmore&apos;s Entail-
ment Rules in a Semantic Theory, from which they certainly have
gained inspiration, did not also gain some of Fillmore&apos;s exemplary
clarity of exposition.
The third essay of this team of authors, Description of the
Meaning of Words for the Purpose of Inference, makes a few, dis-
tinctions that, I think, demonsirate the value of this kind of
work. &apos;iere is, among others, an illustration of the point that
the linguistic negation of a statement cannot always be replaced
by a simply logical negation, for if one says, for instance, &amp;quot;A
did not succeed in g&amp;quot;, there is, beyond the negation of success,
the definite implication that A attempted g, and this implication
is crucial for the proper comprehension or semantic analysis of
the statement.
This last example may serve to make explicit a point which
these three essays tend to obscure: It is not° the formalization
that leads to such semantic or conceptual insights but the analy-
sis that is a prerequisite to formalization. Once analysis and
formalization have been achieved, a suitable notation may help us
to keep track of details of the analysis that might otherwise get
</bodyText>
<subsectionHeader confidence="0.408853">
Lexical Semantics 32
</subsectionHeader>
<bodyText confidence="0.999626909090909">
lost again; but neither formalization nor notation can tell us
more about the meaning! of words than what a native speaker&apos;s
semantic or conceptual analysis has brought out. What kind of
formalization and notation we want to use will always depend on
what we want to do with it. The strictly linear notation used
here -may me very suitable for certain purposes - for the human
reader, however, a two-dimensional, more diagrammatic notation,
such as for instance the one created by Roger Schank (1975),
makes comprehension a good deal easier.
In conclusion, there is a lot of interesting material and
much suggestive exploration in this book but I would recommend it
only to readers who have the time and the patience to go through
every passage more than once. Some intricacies of semantic
analysis are difficult to follow no matter how lucidly they are
presented. Here the difficulties for the reader are greatly
increased by a general lack of clarity and organization in the
exposition. Much of this reads like research reports prepared
for constant collaborators or colleagues who are well acquainted
with the work. Finally, since this book was reproduced directly
from a type Script, it is a pity that the typewriter used for
the corrections was not the same as the original one and that so
many errors managed to survive.
</bodyText>
<table confidence="0.372672142857143">
Lexical Semantics 33
References
Fillmore, Charles J. Entailment rules in a semantic theory.
Report No. 10. Project on Linguistic Analysis. Columbus,
Ohio: Ohio State University, 1965.
Frame semantics and the nature of language. Annals of the
New York Academy of Sciences (1976) 280, 20-32.
</table>
<tableCaption confidence="0.22590575">
Plutchik, Robert The emotions: Facts, theories, and a new
model. New York: Random House, 1962.
Schank, Roger C. SAM - A story understander. Research Report
No. 43. New Haven, Connecticut: Yale University. 1975.
</tableCaption>
<note confidence="0.831526">
American Journal of Computational Linguistics Microfiche 64 : 34
</note>
<sectionHeader confidence="0.958943333333333" genericHeader="method">
CONSTITUENT AND PATTERN
IN POETRY
ARCHIBALD A. HILL
</sectionHeader>
<subsectionHeader confidence="0.8599905">
Prdfessor Emeritus of English and Linguistics
University of Texas, Austin
</subsectionHeader>
<table confidence="0.6847552">
THE UNIVERSITY OF TEXAS PRESS
Austin 78712
xiv + 157 pages 1976 LC 75-32582
$11.95 ISBN 0-292-72010-6
REVIEWED BY JAMES JOYCE
</table>
<subsectionHeader confidence="0.674893166666667">
Computer Sciences Division
Department of Electrical Engineering and Computing Science
University of California, Berkeley 94720
Constituent and Pattern in Poetry is a collection of
Professor Archibald Hill&apos;s essays on the application of
linguistic principles to short poems in English. The essays
</subsectionHeader>
<bodyText confidence="0.936625657894737">
are revisions, sometimes minor and sometimes major, of
prevpiously-published studies. They are, Hill tells us,
&amp;quot;concerned with poems as designs, and all fit into a
thought-out scheme of the relations of literature and
language.&amp;quot; The view of language he brings to this collection
is admittedly Bloomfieldian, and it might be argued that no
Pattern in Poetry 35
amount of revision could bring his structural linguistics-
based observations up to date. Yet his particular brand of
linguistics is not a point of contention here; indeed, the
book&apos;s strengths and weaknesses stem from other considera-
tions.
The strength of the dozen essays making up the book is
that an intelligent, sensitive reader of poetry is sharing
Insights with us. His sole dogma, it appears, is the invo-
cation of the Joos Law, &amp;quot;to choose the least meaning for any
unknown item and the maximal meaning for its context.&amp;quot; The
way this law is used reminds me very much of the basic prin-
ciple of New Criticism, as well as what seems to me to be a
basic tenet of scientific inquiry: that is, to make minimum
assumptions about the data that yield maximal insights or
organization. Usually the Joos law of semantics is applied
with good sense and profit to us all. A second principle
guiding Hill&apos;s reading of poetry is one I would like to see
writ large over each departmental office and lounge in every
university:
In a work we admire
we do not have to justify every line
to yet think it excellent.
The same might be said of linguistic studtes without violat-
ing the spirit of the statement or its applicability.
There are two things in this collection that disturb me;
one is what I feel is a design flaw, and the other is an
Pattern in Poetry 36
unfortunate choice of a measure. In several of the essays
diagrams are described or suggested in varying degrees of
detail (p. 21, for example)) but only one is produced. To
be sure, they are not promised by the usual means of a
reference to Figure 4, but their presence might have added
to the reader&apos;s comprehension of material and perhaps would
have saved Professor Hill the work of such detailed prose
where a diagram might be better.
A second, more substantive point, concerns his method
for deciding whether an image is invoked in a passage.
Although admitting he is using &amp;quot;a rough kind of calculation&amp;quot;
he appears not to realize just how inappropriate the measure
is. In a discussion of Frost&apos;s poem &amp;quot;Bereft&amp;quot; he cites
Leaves got up in a coil and hissed
Blindly struck at my knee and missed.
and observes that a lexical item, such as hissed, either
refers to a snake or does not. &amp;quot;The chance of a snake
reference by pure chance is then one out of two. I am of
course perfectly aware that an accurate weighting would be
very difficult, probably impossible.&amp;quot; (p. 96) The first
assumption he makes is that whether the word hissed refers
to a snake or not is a matter similar to whether a tossed
coin comes up heads or not. That is, the probability that
hissed refers to snake is 1/2, just as the probability that
a tossed coin coming up heads is 1/2. Hill admits there is
Pattern in Poetry 37
another way of calculating such probabilities, but continues
with the one described above because it is easier. tie then
discusses the probability that several &amp;quot;snake items&amp;quot; occur
in the same passage, taking the probabilities (each assigned
1/2) and multiplying them together to get (for four items) a
probability of 1/16 that the referances to a snake are by
chance. Since Hill says the order of the &amp;quot;snake items&amp;quot; is
significant, and it takes three items (out of the four) to
make a sequence, the 1/16 figure should be multiplied by
1/6, giving a final answer of 1/96 that the snake references
in the passage by Frost are by chance. I&apos;m afraid Professor
Hill&apos;s adviser on statistical matters has misperceived the
phenomenon being modelled probabilistically and combinatori-
cally, and Hill apparently has not understood fully what
assumptions about language such a model would require to be
applicable.
</bodyText>
<subsectionHeader confidence="0.530602">
Also, regarding the order of the &amp;quot;snake items&amp;quot;, Hill
</subsectionHeader>
<bodyText confidence="0.846013235294118">
finds it significant that it is: &amp;quot;got up in a coil&amp;quot;,
&amp;quot;hissed&amp;quot;, &amp;quot;struck&amp;quot;, and &amp;quot;missed&amp;quot;. For the order to figure
into the significance of the four &amp;quot;snake items&amp;quot;, there.has
to be a likelihood that the order of actions is natural
among snakes. I discussed the matter with a herpetologist
at San Francisco&apos;s Steinhart Aquarium, and learned that 1)
most snakes do not hiss, 2) and those that do use hissing as
a bluff rather than as a prelude to the attack. The
Pattern in Poetry 38
herpetologist suggested Frost may have used hissing to subh-
stitute for, say, a rattlesnake&apos;s rattling. As a summary of
his observations he suggested the passage from &amp;quot;Bereft&amp;quot; was
&amp;quot;a matter of what you want to interpret it being.&amp;quot; That is,
to a herpetologist the passage is not a clearly accurate
metaphor for snake behavior. Certainly snakes coil, hiss,
strike, and sometimes miss their strike, but the particular
order Hill cites as significant is an order that someone who
</bodyText>
<subsectionHeader confidence="0.540767">
knows snakes finds open to question.
</subsectionHeader>
<bodyText confidence="0.988572454545455">
Before the patient reader of this review turns off the
microfiche reader in disgust at what appears a compounding
of error by absurdity, let me state there is no question in
my mind the passage at issue evokes a snake; I am sure there
is no doubt in Hill&apos;s mind the passage evokes a snake. The
difference between the two of us lies in the soundness or
lack of soundness of the argument Hill uses to illustrate
his point. The significance of the order of &amp;quot;snake items,&amp;quot;
if taken into consideration as they reflect the activity of
snakes, does not count toward the probability that the image
is that of a snake, but Against it.
</bodyText>
<subsectionHeader confidence="0.670766">
What Hill might have done instead is a cluster analysis
</subsectionHeader>
<bodyText confidence="0.636146470588235">
of the &amp;quot;snake items&amp;quot; to see that the density of snake items
is greater than the density of other kinds of images. One
example of such use of cluster analysis is John Smith&apos;s work
oh imagery in Joyce&apos;s A Portraimt of the Artist as a young
Pattern in Poetry 39
Man, described in &amp;quot;Thematic Structure and Complexity,&amp;quot; Style
9 (Winter4 1975), pp. 32-45. Neither the passage from
&amp;quot;Bereft&amp;quot; nor the entire poem is large enough to support a
statistical analysis, but that does not prevent us from
applying the technique as a way of summarizing the data tor
us so long as we make no claims of probability. Numbers,
after all, can only guide interpretation. That Hill&apos;s 1959
essay should know about Smith&apos;s 1975 essay is, of course,
impossible; yet Hill&apos;s book is not just a reprint of the
essays, but a revision as well. And cluster analysis has
been known as a technique longer than either essay has been
in print.
</bodyText>
<subsectionHeader confidence="0.940744">
Yet Constituent and Pattern in Poetry is a book I found
</subsectionHeader>
<bodyText confidence="0.870914333333333">
enlightening on a number of points, and I believe it is a
valuable collection because of those insights. I very much
liked Chapter 8, &amp;quot;The Locus of the Literary Work,&amp;quot; and feel
it should be better known in literary circles than it is.
The chapter views a poem not as a work on paper, as both New
Critics and Structuralists would have us believe, but as a
pattern, m poetic structure analogous to langue; the in-
terpretations of the poem, however, are not parole, but con-
clusions about parole (p. 86). It is the text (or various
texts of the poem, which need not be written to be texts)
that Hill identifies as corresponding to parole in this
case. A diagram of this might be:
</bodyText>
<table confidence="0.993245827586207">
Pattern In Poetry 40
: The poem, or
1
Langue 1
The texts, or 1
1
Parole
1 Interpretations,:
1 or Conclusions 1
1 about Parole 1
1
I would want to add another part to this picture, one I
hope Professor Hill would not object to:
1 The poem, or 1
1
1 Langue 1
: The poet&apos;s oeuvre :
1
1 as context—M-7&apos;11
1
1 the poem 1
1 The texts, or :
1 1
1 Parole 1
oP
1 Interketations,1
1 or Conclusions
t about Parole 1
1
</table>
<subsectionHeader confidence="0.428507">
The pget&apos;s oeuvre exists between the poem as pattern in
</subsectionHeader>
<bodyText confidence="0.968948925925926">
the poet&apos;s mind and the texts of a poem&apos;s realization. The
oeuvre provides a context for composition at the moments of
Pattern in Poetry 41
creation; how many moments those number, and whether they
are contiguous or not, is irrelevant here. The various
texts of a given poem, whether versions of one person&apos;s poem
or from an oral tradition, begin as patterns in the mind and
rind realization as text influenced in composition by the
poet&apos;s oeuvre. Critics make conclusions about oeuvre when
they consult a concordance for data about a word or do
stylometric studies. Such studies ultimately allow us to
interpret a particular text by providing accurate models of
the poet&apos;s oeuvre; that is, we form better conclusions about
parole. Such studies as well allow us to make inferences
about langue, if that is the target of our study.
Hill states as one of the general ideas he holds strong-
ly that &amp;quot;the method of literary investigation, when properly
carried out, must not only rest on a linguistic base but
also go beyond it.&amp;quot; (p. x) This seems to me to be so reason-
able and appealing I am tempted to dismiss it as a common-
place; but when I remember other applications of linguistics
(or other disciplines, such as computing) to literature I am
reminded that though it should be a commonplace it decidedly
is not. As I said at the beginning of this review, an in-
telligent, sensitive reader of poetry is sharing insights
with us. And there is no need to defend every line to yet
think it excellent.
</bodyText>
<note confidence="0.752494">
American Journal of Computational Linguistics Microfiche 64 : 42
</note>
<table confidence="0.941045321428572">
CURRENT BIBLIOGRAPHY
GENERAL 43
PHONETICS-PHONOLOGY 47
PHONOLOGY , 49
RECOGNITION 50
Prosody 52
Linear Predittive 53
System 53
SIINTHESIS , 54
WRITING 55
RECOGNITION 55
Chinese 55
CHARACTER SETS 56
LEXICOGRAPHY-LEXICOLOGY . 56
THESAURI 56
GRAMMAR 57
PARSER 57
SEMANTICS-DISCOURSE &amp;quot; 60
GENERAL 60
THEORY 61
COMPREHENSION 65
System 66
MEMORY 66
Question Answering ...&amp;quot;.. 66
TEXT GRAMMAR 67
CONCEPTUAL DEPENDENCY 67
EXPRESSION 68
LINGUISTICS 69
</table>
<sectionHeader confidence="0.336692" genericHeader="method">
METHODS 69
</sectionHeader>
<figure confidence="0.980636963636364">
Mathematical 69
COMPUTATION
INFERENCE
PROGRAMMING
Languagect
INFORMATION STRUCTURES
PICTORIAL SYSTEMS
DOCUMENTATION
INDEXING
RETRIEVAL
THESAURI
TRANSLATION
SOCIAL-BEHAVIORAL SCIENCE
PSYCHOLOGY
Psycholinguistics
HUMANITIES
ANALYSIS
INSTRUCTION
BRAIN THEORY
ROBOTICS 86
MANAGEMENT 88
AJCL thanks Martin and Iris Kay
and Xerox Palo Alto Research Cen-
ter for their help in preparing
this bibliography.
70
70
71
72
75
76
81
81
81
82
82
83
83
83
84
84
85
86
,
,.
43
GENERAL
Controversy over Computer Power and ,Human Reason - 1
Benjamin !Wipers
MIT Al Laboratory, Cambridge, Massachusetts 02139
John McCarthy
Stanford University AI Laboratory, California 94305
Joseph Weizenbaum
Laboratory for Computer Science, MIT, Cambridge, Massachusetts 02139
SIGART Newsletter 58: 4-12, June 1976
</figure>
<bodyText confidence="0.821078875">
Kuipers: Reactions to Weizenbaum&apos;s book, 4-5. McCarthy: An Unreasonable Book, 5-10.
Weizenbaum: A Response to John McCarthy, 10-12. Kuiper&apos;sfeactions are fairly moderate in
criticis&apos;m of Weizenbaum while McCarthy&apos; s objections, given in some detail, are aimed deeper.
Weizenbaum says: &amp;quot;The distance that separates John McCarthy from Joseph Weizenbaum is
truly measured by the challenges these two hurl at one another: McCarthy defies Weizenbaum
to &apos;Show me a way to knowledge besides science!&apos; And Weizenbaum responds: &apos;Can there be a
way toward an authentic model of man that does not include and ultimately rest on
philosophical and moral thinking?&amp;quot;
</bodyText>
<sectionHeader confidence="0.911733" genericHeader="method">
GENERAL
</sectionHeader>
<reference confidence="0.6220025">
Controversy over Computer Power and Human Reason — 11
Roger C. Schank
Computer Science Department, Yale university, new Haven, Connecticut
Kenneth Mark Colby
Algorithmic Laboratory of Higher Mental Functions, UCLA Department of Psychiatry, Los
Angeles, Califoionia 90024
</reference>
<subsectionHeader confidence="0.771789">
Josephy Weizenbaum
Laboratory of Computer Science, MIT, Cambridge, Massachusetts 02139
</subsectionHeader>
<bodyText confidence="0.913299571428572">
SIGART Newsletter 59: 7-11, August 1976
Schank: Response to*Weizenbaum&apos;s Response to McCarthy, 7-8. Weizenbaum: Reply to Roger
Schank, 8 ). Colby: On the Morality of Computers Providing Psychotherapy, 9-10.
Weizenbaum: Response to Colby, 10-11. Schank and Weizenbaum are concerned with
epistemological status of linguistic theories constructed by linguists (Chomsky in particular)
and NL understanding&apos; programs constructed by AI researchers, (e.g. Schank). Colby and
Weizenbaum discuss the (im)morality of computer administered psychotherapy.
</bodyText>
<note confidence="0.5340646">
GENERAL 44
Computer Science as Empirical Inquiry
Allen Newell, and fierbert A. Simon
Carnegie-Mellon University, Pittsburgh, PA
Communications of the ACM 19: 113-126, March 1976
</note>
<tableCaption confidence="0.9609355">
Two hypotheses concerning the naturepf intelligence and Al are advanced and explicated: .1)
The Physical Symbol System Hypothesis. A physical symbol system has the. necessary and
sufficient means for general intelligent action. 2) Heurivic Search Hypothesis. The
solutions to problems are represented as symbol structures. A physical symbol system
exercises its intelligence in problem solving by search - that is, by generating and
progressively modifying symbol structures until it produces a solution structure.
</tableCaption>
<sectionHeader confidence="0.907359" genericHeader="method">
GENERAL
</sectionHeader>
<subsectionHeader confidence="0.9413575">
Machine Understanding
Dennis de Champeaux
</subsectionHeader>
<subsubsectionHeader confidence="0.502436">
Instituut voor Bedrijfsinformatics, University of Amsterdam, Amsterdam-1006,
Netherlands
April 1975
</subsubsectionHeader>
<bodyText confidence="0.917917888888889">
A brief overview of current NL .programs with most attention on the process of translating
input sentences to a meaning representation.. The impossiblity of delineating independent
sub-problems in this process is discussed. Stal, a rank ordering ts given of simple
representations of the following sub-problems: word recognition, syntax analysis, establishing
existence of references, function of sentence parts, word ambiguity, case ambiguity, reference
ambiguity, sentence type recognition, rOle-theme-scenario-etc.- recognition. Examples are
giyep however of these sub-problems which can be solved only by first (partially) solving
sub-problems which rank higher.
GENERAL 45
</bodyText>
<sectionHeader confidence="0.550889" genericHeader="method">
Dreyfus&apos;s Disproofs
</sectionHeader>
<subsectionHeader confidence="0.516781">
Yorick Wilks
</subsectionHeader>
<subsubsectionHeader confidence="0.515109">
Department &apos;of Computation Logic, University of Edinburgh, EH8 9NW
British Journal for the Philosophy of Science 21: 177:-185. June 1976
</subsubsectionHeader>
<bodyText confidence="0.915564142857143">
Dreyfus&apos; 1972, book What Computers Can&apos;t do is open to 4 major lines of criticism: 1)
Dreyfus doesn&apos;t describe the Al enterprise correctly, 2) Al work has changed considerably
singe his survey and is beginning to incorporate ttiat which he claimed an adequate Al system
must, 3) there is a lack of clarity in his claims, especially in the notion of &amp;quot;digital&amp;quot; (as
opposed to analog) machines, 4) his phenomenological argument shows that machines cannot
understand, but at the old philosophical price of showing that no one but I can, either
(solipcism).
</bodyText>
<sectionHeader confidence="0.857544" genericHeader="method">
GENERAL
</sectionHeader>
<reference confidence="0.880782725">
Uses of Higher Level Knowledge in a Speech Understanding System
William A. Woods, Madeleine Bates, Bertram Bruce, and Bonnie Nash-Webber
Bolt Beranek and Newman, Inc., Cambridge, Massachusetts 02138
Continuous speech understanding, for all but the most restricted languages, requires a
characterization of the sentences of the language, the meanings of the words within the
domain, and the use of the language within the task. Thus, syntax, semantics and pragmatics
become essential components of a total speech understanding system. The control problem in
such a system involves integrating the knowledge available from these components with the
possible interpretations of an utterance suggested by its acoustics. This paper discusses
various strategies for controlling these high-level sources of knowledge, reflecting their
potential for coping with the inevitable acoustic ambiguity and error.
GENERAL 46
The Prague Bulletin of Mathematical Linguistics 25
Universita Karlova, Praha, 1976
CONTENTS
Semantische Naze und selektive Beziehungen,
I. J. Kunze 3
Meaning of Sign, Cognitive Content, and Pragmatics,
P. Sgall 51
GENERAL
The Prague Bulletin of Mathematical Linguistics 26
Universita Karlova, Praha, 1976
CONTENTS
1. 0 znachenii rabot I. I. Revzina v oblasti teoretiko-mnozhestvennoi kontsepsii iazyka,
0. G. Revzina, I. A. Shreider 3
2. Semantische Netze und selektive Beziehungen 11,
J Kunze 17
. .
3. 0 modeli estestvennogo iazyka, osnovannoi na poniatii peremennykh okruzhenii,
J. S. Bleb . • • . • 41
On Some Relationships of Linguistics and Information Retrieval,
P. Sgall 510
PHONETICS-PHONOLOGY 47
Annual Bulletin of the Research Institute of Logopedics and Phoniatrics,
No. 10
Faculty of Medicine, University of Tokyo, March 1976
CONTENTS
An Interactive Display Terminal for Image Measurement Using an X-Ray Microbeam
Scan System 1
S. Kiritani, K. Itoh, E: Takenaka, S. Sekimoto, H. Imagawa, H. Fujisaki, and M.
Sawashima
Simultaneous Recording of EMG with Pellet Tracking by Use of X-Ray Microbeam 13
S. Kiritani, S. Sekimoto, H. Imagawa,-K. Itoh, T. Ushijima, and H. Hirose
Principal Component Analysis of Tongue Pellet Moxement 15
S. Kiritani and H. Imagawa
Tongue Pellet Movement for the Japanese CV Syllables - Observations Using the
X-Ray Microbeam System 19
S. Kiritani, K. Itoh, H. Imagawa, H. Fujisaki, and M. Sawashima
Analysis of Tongue Point Movements by a Linear Second-Order System Model . . . . 29
Y. Sonoda and S. Kiritani
A Preliminary Report on the Electromyographic Study of the Production of the
Japanese Semivowel /j/ 37
K. Kakita, H. Hirose, T. Ushijima and M. Sawashima
Fiberoptic Acoustic Studies of Mandarin Stops and Affricates 47
R. lwata and H. Hirose
Devoiced and Whispered Vowels in Japanese 61
R. S. Weitzman, M. Sawashima, H. Hirose and T. Ushijima
Laryngeal Control in French Stops: A Fiberoptic, Acoustic an Electromyographic Study 81
A-P. Benguerel, H. Hifose, M. Sawashima and T. Ushijima
More on Laryngeal Control for Voicing Distinction in Japanese Consonant Production . 101
H. Hirose and T. Ushijima
Pitch Accent and Vowel Devoicing in Japanese: A Preliminary Study
J. B. Lovins
PHONETICS-PHONOLOGY 48
A Phonetic Descripticin of Tibetan with a review of the literature 127
0. Kjellin
*Analysis, Recognition, and Perception of Voiceless Frictive Consonants in Japanese . . 145
H. Fujisaki and 0 Kunisaki
*Acoustic and Perceptual Analysis of Two-Mora Word&apos;Accent Types in the Osaka
Dialect 1,57
H. Fujisaki and M. Sugito
*Analysis, Synthesis, and Perception of Word Accent Types in Japanese 173
H. Fujisaki, H. Hirose and M. Sugito
*Temmal Organization of Articulatory and Phonatory ControJs in Realization of
Japanese Word Accent 177
H. Fujisaki, H. Morikawa, and M. Sugito
Acoustic Analysis and Subjective Evaluation of Sung Vowels 191
M. Tatsumi, 0. Kunisaki, and H. Fujisaki
On the Development of Perceptual Strategies in Children: A Case Study on the
Japanese Child&apos;s Comprehension of the Relative Clause Constructions 199
S.I. Harada, T. Uyeno, H. Hayashibe, and H. Yamada
A Kinesiological Aspect of Myasthenia Gravis - A Electro-myographic Study of Velar
Movements During Speech
T, Ushijima, M. Sawashima H. Hirose, M. Abe and T. Harada
Construction of a Short Test of Aphasia on the Basis of Factor Analysis 233
,
Y. Fukusako and S. Sasanuma
A Computational Model of the Tongue 243
S. Kiritani, K. Miyawaki, 0. Fujimura, and J. E. Miller ,
Recent Publications 253
*These articles have been abstracted elsewhere on this fiche.
225.
PHONETICS-PHONOLOGY 4 9.
Acoustic-Phonetic Experiment Facility for the Study of Continuous Speech
Richard M. Schwartz
Bolt Beranek and Newman, Inc., Cambridge, Massachusetts 02138,
While gathering acoustic data for the acoustic-phonetic analysis of&apos; speech, it is necessary to
consider many different sounds in varying phonetic environments to assure that the results
are statistically significant. In order to reduce the amount of time requiyed to test
hypotheses, a facility has been developed which provides an interactive environment for
performing a wide variety of acoustic-phonetic experiments on a large data base of
continuous speech. Using this facility, one can formulate an experiment, run it on selected
portions (or all) of the data base, and display or tabulate the results in a meaningful way.
Another experiment may then be run based on the results. CPU time required to run an
experiment on the entire data base is between 5 and 20 seconds, depending on the complexity
of the experiment. Due to the ease of interactio,ns, formulating or revising an experiment,
running it, and displaying the results normally takes less than 5 minutes. This facility has
been used in combination with a data base of 69 hand-labeled sentences to develop
algorithms for acoustic-phonetic segmentation and labeling in a speech understanding system.
Several examples of its use and the results obtained are presented.
PHONETICS-PHONOLOGY: PHONOLOGY
Analysis, Recognition, and Perception of Voiceless Fricative Consonants in
Japanese
Hiroya Fujisaki
Research Institute of Logopedics and Phoniatrics, University of Tokyo
Osamu Kunisaki
Department of Electrical engineering, Faculty of Engineering,
University of Tokyo
Annual Bulletin of the Resedrch Institute of Logopedics and Phoniatrics
10: 145-156, 1976
</reference>
<bodyText confidence="0.98054625">
The model for the spectral characteristics of voiceless fricatiye consonants in Japanese is
based on an equivalent circuit representation of their generation mechanism. The model,
together with its three simplified versions, is then evaluated from the point of view of
automatic recognition hs well as of synthesis of speech. For automatic recognition, spectral
models that contain zeros are found to be particularly effective, and their parameters are
shown to be sufficient for the complete seParation of /s/ - and /sh/ - samples in CV and
VCV utterahces. On the other hand, peiceptual experiments using &apos;synthetic stimuli reveal
considerable smaller differences between Diode&apos;s with spectral zeros and those without zeros.
</bodyText>
<sectionHeader confidence="0.48474" genericHeader="method">
PHONETICS-PHONOLOGY: PHONOLOGY 50
</sectionHeader>
<reference confidence="0.39997075">
Dictionary Expansion via Phonlogical Rules for a Speech Understanding
System
Winiam A. Woods, and Vietor W. Zue
Boit Beranek and Newman, inc., Cambridge, Massachusetts 02138
</reference>
<bodyText confidence="0.956344777777778">
This paper describes some modifications and extensions to the Bobrow-Fraser Phonological
rule testing program (CACM 11, 1968, 766-772), which enable it to expand a dictionary of
words read from a file and systematically try, all possible combinations of optional rules,
compute -a running &amp;quot;probability&amp;quot; of a pronunciation as a function of probabilities of
application and non-application of individual optional rules; associate arbitrary applicability
tests with rules, and generate a summary table of which rules applied to each word and which
words a given rule applied to. The system uses a notation similar to the classical Chomsky-
Halle notation for phonological rules, and has been used to expand a 500-word dictibnary for
the BBN SPEECHUS speech understahding system.
</bodyText>
<sectionHeader confidence="0.7684735" genericHeader="method">
PHONETICS-PHONOLOGY: RECOGNITION
Methods for Nonlinear Spectral Distortion of Speech Signals
</sectionHeader>
<subsectionHeader confidence="0.6946395">
John Makhoul
Bolt Beranek cind Newman, Inc., Cambridge, Massachpseas 02138
</subsectionHeader>
<bodyText confidence="0.976547">
The spectral distortion of speech signals, without, affecting the pitch or the speed of the
signal, has met with some difficulty due to the need for pitch extraction. This paper presents
a general analysis-synthesis scheme for the arbitrary spectral distortion of speech signals
without the need for pitch extraction. Linear predictive warping, cepstral warping, and
autocorrelation warping, are given as examples of the general scheme. Applications include
the unscrambling of helium speech, spectral comprassion for the hard of hearing, bit rate
reduction in speech compression systems, and efficiency of spectral representation for speech
recognition systems.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000006">
<note confidence="0.800411">Journal of Computational Linguistics 64</note>
<title confidence="0.862105333333333">STRING OF THE FOR COMPUTATIONAL LINGUISTICS VOLUME - Report of the ACL 1977 Annual Meeting Panel on Speech Understanding and Computational Linguistics&apos;&apos;: A Critical Examination of the ARPA Project,</title>
<author confidence="0.8210255">by Ernst von_Glasersfeld Constituent Rozencvejg</author>
<author confidence="0.8210255">Pattern in Poetry</author>
<author confidence="0.8210255">Archibald A Hill</author>
<note confidence="0.847466083333333">Reviewed by James Joyce 34 CURRENT BIBLIOGRAPHY 42 AMERNAN JOURNAL OF COMPUTATIONAL LINGUISTICS is published by the Association for Computational Linguistics. SECRETARY-TREASURER: Donald E. Walker, Stanford Research Institute, Menlo Park, California 94025. EDITOR: David G. Hays, 5048 Lake Shore Road, Hamburg, New York, 14075 EDITORIAL ASSISTANT: William Benzon Copyright 0977 Association for Computational Lingbistics Journal of Computational Linguistics 64 : 2</note>
<title confidence="0.983795428571429">REPORT ON ACL PANEL UNDERSTANDING COMPUTATIONAL LINGUISTICS: A CRITICAL EXAMINATION OF THE ARPA PROJECT I CK</title>
<author confidence="0.99941">Jonathan Allen</author>
<author confidence="0.99941">Chairman</author>
<affiliation confidence="0.9062215">Jack Mostow, Carnegie-Mellon University Donald Walker, Stanford Research Institute</affiliation>
<address confidence="0.44449">William Woods, Bolt Beranek and Newman</address>
<abstract confidence="0.983945666666667">Th p present report summarizes the statements of the panelists and paraphrases the questions and answers that followed their talks.</abstract>
<title confidence="0.870221">OF THE ACL 1977 ANNUAL MEETING PANEL ON SPEECH UNDERSTANDING AND COMPUTATIONAL A CRITICAL EXAMINATION OF THE ARPA PROJECT</title>
<author confidence="0.999991">S R Petrick</author>
<affiliation confidence="0.6279435">IBM T J. Watson Research Center This panel discussion was chaired by Jon Allen of MIT. The participants were Jack Mostow of Carnegie-Mellon University (ably substituting for Raj Reddy on short notice), Don Walker of the Stanford Research Institute, and Bill Woods of Bolt, Beranek and Newman</affiliation>
<abstract confidence="0.974459391061452">The panel format featured a half hour talk by each panelist followed by a period of discussion centered around questions from the audience. Jon Allen began by reminding the audience that the ARPA five year speech understanding project had just ended last October, and that this was an appropriate time to step back and ask just what had been learned as a result of that project He then turned the podium over to the panelists to address that issue. Don Walker began by supplying some general background information on all the ARPA-sponsored efforts before he tuened to the specific accomplishments of SRI. They were all concerned with speech understanding rather than speech recognition, and they each focussed on one or more specific tasks SRI, for example, was concerned with information retrieval with respect to a navy ship data base. This concentration on specific tasks limited generality somewhat. There was also a focus on producing an operational system. This was good in some respects, but this emphasis on results did make it impossible to follow many promising research paths. Performance requirements, such as size of vocabulary and processing time, were not all realistic. The approaches followed differed widely from group to group These diverse approaches were perhaps appropriate to the state of knowledge which existed, but they did limit SPEECH PANEL the opportunity to share results. There were competitive aspects to the ARPA effort. In addition, the necessity to attend lots of steering committee meetings hindered local progress. The computing power made available for the ARPA-sponsored efforts was good but still not enough It was not as much as the investigators had hoped for. Walker referred to the money which had been expended and remarked that although it was substantial, it was not enough. Not only would more computing power have been useful but more people and more time would also have contributed to improved results. Turning then to a more detailed examination of the SRI effort, Walker discussed four system issues which had been dealt with: (a) integration of speech, syntax, semantics, etc., 1 e., specifying contributions and interactions of multiple knowledge sources perspicuously and efficiently, (b) cooperation, i.e., sharing information and avoiding duplication of effort, (c) evaluation, i.e., rating alternatives and choosing what to do next, and (d) attention, i e., avoiding dissipation of resources over competing alternatives He also discussed component issues including those required for natural language specification, task modeltrig, and dialog interactions Typical sentences in the SRI information retrieval application were cited and discussed These included WHAT IS THE LENGTH OF THE LAFAYETTE? THE GEORGE WASHINGTON? DID GENERAL DYNAMICS BUILD THE SHARK? WHO OWNS IT? WHICH FRIGATES WERE BUILT BY CAMMELL LAIRD COMPANY? The SRI system organization was discussed briefly Those portions of the system concerned with acoustics, phonetics and phonology were handled by SDC on their own computer while _higher levels such as syntax and semantics were treated by SRI Basically, a flexible control facility provided for such alternatives as either building structure up from acoustically determined words or building it top-down in a syntax-directed fashion. In any PA SPEECH PANEL case, a semantic network was constructed using scores or the degree of phonetic match involved with various alternatives. Turning to the definition of the input language, Walker stated that it is accomplished by means of linguistically motivated (ATN) rules that are general and extensible over a variety of domains. These provide a means for adjusting (&apos;tuning&apos;) the language definition to particular domains without loss of generality. Syntactic, semantic, and discourse information is combined within the rules that define words and phrases. The SRI semantic component makes use of partitional semantic networks. It handles higher-order logical predicates, especially quantifiers It provides deduction routines retrieval and inference that can access a supplementary relational data base in responding to a user&apos;s query It also provides a network substructive that is converted to an English sentence or phrase to answer a user&apos;s question. SRI discourse modeling is based on in-depth studies of domain-oriented dialogs A model of dialog context is encoded through the use of semantic partitions. Dialog context (through semantic closeness and syntactic considerations) is also used to find the meanings of elliptical expressions and the referents of definite noun phrases System integration was briefly discussed. In the SRI system this provides for interaction of information from various sources of knowledge — syntax, semantics, and discourse -as part of the language definition itself The SRI effort avoided commitment to a particular system control strategy, allowing instead for flexible use of various strategies for putting together words and phrases out of incomplete and uncertain fragments. The SRI system control facility was the last topic discussed by Walker. The points made about it included the following: Ratings are assigned to incomplete sentence structures reflecting the highest possible continuation of each such structure. The priorities of available alternative continuations are assigned as a result of those ratings. Data structures are organized for testing hypotheses about utterances in a manner that avoids duplication of effort. Combinations of top-down, bottom-up, and bidirectional strategies are allowed. The flexibility 6 ARPA SPEECH PANEL provided by the SRI system control made possible extensive experimental studies to evaluate design alternatives. Control strategies which were evaluated included the following four paired alternatives: 1) Island driving versus left to right processing of utterances 2) Testing for all words versus testing for just good matches 3) Using sentential context versus ignoring it 4) Focussing., on selected alternatives versus continuing on a single successful path until it succeeds or fails. Experimental findings from these studies of alternatives included a) Context checking is important; it helps priority setting b) Testing for all words improves performance but it is costly. A more efficient mapper is needed. c) Island driving produced wild exponential branching on long sentences. d) Focussing on selected alternatives is less effective than best first following of a single path. Nevertheless, simple best-path-first parse algorithms are not good enough e) There is a strong need for a phrase mapper for word coarticulation f) A high false alarm rate was reported. Hit scores were better than false alarm scores Bill Woods, the next speaker, began with a discussion of a speech spectrogram He described early experiments in which two investigators, Ken Stevens and Dennis Klatt, attempted to &amp;quot;read&amp;quot; spectrograms either on a segment by segment basis or making additional use of their knowledge of English phonology and syntax. They correctly transcribed between 70 per cent and 75 per cent of the individual segments considered in isolation (some segments were only partially identified, i.e., certain of their distinctive features were correctly identibut attained 96 cent word accuracy in an experiment which used a computer program as an aid to human classification together with the human skills and knowledge of linguistic constraints they commanded. A number of observations were made. They included the following: ARPA SPEECH PANEL 7 (1) Small function words are highly unreliable anchors. (2) Acoustically based matching alone is insufficient; accidental word matches outnumber correct ones. (3) Speech understanding is essentially a nondeterministic process. (4) Sequential left-to-right scanning presents problems; the first and last words are often not pronounced carefully; the first stressed word is a more reliable anchor There is a need to recover from any garbled word, especially the first one (5) The space of alternatives must be open-ended You might have to go way down the list of likely alternatives to find the right one (6) There is a strong need for merged representations. many similar hypotheses snare common parts. A discussion of the sources of knowledge in the BBN System followed.Acousticphonetic, phonological, lexical, syntactic, semantic, discourse, and factual sources of knowledge were considered. A program to assign scores to each of a number of overlapping segments for each possible candidate phoneme was described. Results of an experiment were described in which the correct phoneme was identified 53 per cent of the time and the first or second choice was correct 70 per cent of the time. Per cent of phonemes correct within the first N choices for various values of N was reported as follows 1 2 3 4 5 6 7 % correct 53 70 75 79 83 84 86 The BBN verification component was next discussed It takes an analysis-by-synthesis approach to word matching. A synthesis-by-rule procedure is used to generate word templates, and these templates are compared with a parametric version of the signal to determine the closeness of match. The pragmatic ATN-based grammar which served as the .syntactic component was identified as that described by G. Brown in a paper presented earlier that day. SPEECH PANEL diagram of HWIM System organiiation was discussed briefly. The basic operation of the system was described as follows: (1) create initial theories (2) rank them, (3) give them to the parser, (4) have the parser suggest the n best words (5) verify best matches, (6) add results on to islands previously identified to form larger islands, and (7) let these new events trigger new theories and continue this cycle. Woods then presented transparencies citing in parallel columns the ARPA-imposed goals and the degitee of success attained by the BBN HWIM System in meeting goals. I include reproductions of those transparencies. ARPA Goals (from 1971) HWIM (1976) The system should: The system did: Accept continuous Accept connected speech 2 From many From 3 3 Cooperative speakers of Cooperative male general the general American American dialect 4 In a quiet room In a somewhat quiet computer terminal room 5 Over a good quality Over an ordinary close-talking microphone talking microphone 6 Allowing slight tuning With no tuning of the system of the system per speaker for the individual speaker 7 But rewring only And requiring no speaker natural adaptation by adaptation the user Permitting a slightly On 1097 words with no postselected vocabulary 1000 With, a highly artificial syntax combined syntax and semantics (average branching &gt; 100) 10 And a highly constrained And a well defined task. task SPEECH PANEL 11 With a siniple psycho-logical model of the user With no user model 12 Providing graceful. inter And modest interaction action capabilities 13 Tolerating less than 10% With 57% semantic error semantic error 14 In a few times real time In about 1350 times real time 15 On a 100- Mips Machine On a .35 Mips PDP-10 16 With 2561t, 36-bit words 17 With a hierarchical system organization 18 (Cost per utterance) 19 And be demonstrable in And was operational 1976 with a moderate chance of success 12 Octbber 1976 also include information from of results transparency which casts further light on the 57 per cent semantic error figure cited in, item 13. In discussing this summary Woods stressed that the difficulty of recognition is very much affected by the branching factor associated with the syntactic component in question and with the distinctiveness of the paths allowed.</abstract>
<note confidence="0.9102152">ARRA SPEECH PANEL 10 Summary of Results 10/31/76 124 Utterances by 3 Speakers Average Utterance = 6.1 words = 1_8 seconds BIGDICT (1097 words) MIDDICT (409 words) Correct 41% 48% Semantically OK 2% 4% Incorrect Close 23% 20% So-so 10% 6% Bad 5% 6% Gave up (150 theories) 18% 14% System Broke 1% 1 ok Estimated average branching (words) 196 67 Speed (times real time) 1350 1050</note>
<abstract confidence="0.990607074829932">Woods closed with a discussion of those natural language problem areas which had been affected by results obtained through speech understanding research. They included the following: (1) Coping with ambiguity (BBN did not eliminate this by fiat) (2) Error recovery (island driving is good in this respect) (3) Parsing algorithms (4) Interaction of syntax, semantics, and pragmatics Degrees of (BBN was just getting into this. SRI somewhat mo,re so.) ARPA SPEECH PANEL 11 (6) Grammar-writing methodologies (e.g., general categories and specific filters at SRI, categories and a compromise between these as suggested in the paper by Bobrow and Bates) and dialog models (8) Human performance and linguistic theory. The final talk of the afternoon was given by Tom Moslow of Carnegie-Mellon University. The CMU project featured a task domain concerned with information retrieval for a set of AI abstracts. Vocabulary size was 1011 words and a branching factor of 9.53 was cited, indicating a considerably more restricted syntax than that of the other efforts. Two distinct systems, HARPY and HEARSAY-II were implemented, and the results cited for each were: HARPY 97% word accuracy 91% sentence accurac 95% semantic accuracy 27.9 Mips (about 30 times real speech rate) HEARSAY-II 86% word accuracy 73% sentence accuracy 91% semantic accuracy 85.0 Mips (85 times real speech rate) a discussion of how signal variability and imperfect knowledge lead to a combinatorial explosion of alternative transcriptions, making speech understanding very difficult, Mostow gave more detailed descriptions of both HEARSAY-II and HARPY. HEARSAY-II is an asynchronous, data-directed, hypothesize and test model in which cooperating, independent knowledge sources communicate by means of a &amp;quot;blackboard&amp;quot;. In this model it is easy to deactivate a particular knowlpdge source, and this made it relatively SPEECH PANEL to evaluate the worth each such source. Focus of attention mechanisms were studied. The -systern is island-driven and makes use of a task-specific pattern matching grammar HARPY is quite different. It integrates knowledge sources into a uniform state-space netwOrk at the phoneme level. All knowledge sources, high and low, must be integrated into It was claimed to be very efficient for sufficiently restricted languages with a accuracy over 90 per cent on a 1 PDP/10 10 to 30 times real time Proceeding to a new task requires that a similar but new grammar be produced, which Mostow claimed was not a major undertaking. A general observation was made and illustrated by specific experience accrued at CMU. This was that different uses of the same linguistic knowledge require different knowledge representations. Precompiling knowledge into an appropriate data structure for a given use is more efficient than interpreting a single global knowledge structure. The philosophy of Language design adopted by CMU was, given a set of typical sentences, to construct a sequence of increasingly larger languages each of which properly contains its successor. The last member of this sequence must account for the given corpus. In other words it was proposed that full linguistic constraints be relaxed at first to produce a simpler model with a correspondingly more efficient recognition procedure; it was argued that this provides a cheap filter on data, and the output of such a model can, in turn, be constrained by a more elaborate model. In illustration bf this approach with respect to HARPY, it was suggested that a finite state net could serve as the simpler model and an ATN grammar as the more elaborate model. A HEARSAY-II example was also discussed in which a Markov model serves as the simpler model and a pragmatic template grammar as its more elaborate extension. of understanding ungrammatical speech was touched on briefly. The approach suggested was to use a template-based grammar but to accept approximate template realizations. If the normal grammar fails, find grammatical fragments and concatenate them if permitted by general rules. SPEECH PANEL Search techniques for both HEARSAY-II and HARPY were discussed. The locus method of HARPY was as of the main contributions of CMU research along with the HEARSAY-II blackboard model and the results obtained on the interpretation of ungrammatical utterances. Future directions included writChsion of the system(s) to provide for more general (larger and more flexible) input la nguagta and extension of the methods developed to such other At tasks as the development of vision systems. Following the prepared panel talks questions were directed to the panel members from the floor. I cannot identify all the questioners so I will not attempt to supply names. I have paraphrased questions and answers in most cases. Q: What linguistic constraints were imposed other than a 1000 word vocabulary? A: Formal grammars of one type or another were used (for example, an ATN grammar for the BBN effort). Sentences which were grammatical with respect to these formal grammars were read by subjects. Q: Was any use made of intonation? A: (Woods) Some use of intonation constraints was implemented into the HWIM System rather late in the program but was never tested in the final crunch. What results work should help in text processing? A: (Woods) The development of a model for completing unfinished sentences. This has certain psycholinguistic implications. A: (Walker) Language definition involving general categories and pragmatic grammars for one thing. The integration of all knowledge sources at the phrase level is also a significant contribution. A: (Mostow) The determination of how much milage can be squeezed out of cheap methods. We expect to continue this type of speech research but not to do much more on high level constraints. Why do so much better than the others? SPEECH PANEL A: (Combined responses of all three panelists) CMU picked a simpler domain perhaps and definitely used a much cruder (more restrictive) grammar. The BBN LISP Sysfem and operating system were less efficient than CMU&apos;s lower level programming. BBN aimed for a tougher target (as evidenced by their more realistic grammar and correspondingly higher branching factor) and didn&apos;t make it in spite of a ten &amp;quot;0 per improvement over the last The BBN results were intermediate bench marks along the way toward a target which was never reached because of a lack of time whereas CMU was more realistic in budgeting its time. CMU had no real theory. It just threw in a new dictionary entry whenever necessary. CMU emphasis was placed on general Al techniques, not linguistic principles BBN could only process 10 utterances per night whereas CMU could process 10 utterances per hour. Hence, CMU could consider more data SRI did not have a total system avialable to them, being dependent on SDC for their front end, so they were at a disadvantage in this respect. All speakers agreed that once real time speeds are achieved, the user interaction possibilities will be attractive. SRI not only had no opportunities for fine tuning, but their approach was claimed to be even more linguistically motivated than BBN&apos;s. It was similar to BBN&apos;s before the latter&apos;s switch to a pragmatic grammar. SRI reported a 40/1 false alarm-to-hit ratio on the part of the acoustic component supplied by SDC but still achieved close to 90% semantic success Its system was estimated to run in 200-300 times real time on a vocabulary of 320 words and a grammar with a static branching factor of 110. SPEECH PANEL Precomputation gives HARPY its efficiency bin intorporatioti of new knowledge into HARPY makes new precomputatiOn necessary and it is expensive. Q. Does anyone currently have a government contract to pursue speech recognition? A: Not to anyone&apos;s knowledge, but mote general contracts may include some speech work. Speech understanding is not dead but is currently in abeyance. Q: Is the rumor true that NSF may pick up some speech work and therefore some projects that would otherwise be funded will suffer? It true in the sense that people previously ARPA funds are entering the competitive pool for a static amount of research funds There is a &amp;quot;new player&amp;quot; however in the form of the Sloan Foundation, which is supporting less capital intensive research, less systems-oriented, and is to encourage between different groups. Q: What would you do different if you were starting over? (Woods) I would do some things different that woukl be wrong. I would give more reassurance to new people starting work on the project that they were on the right track. ARPA managers need such reassurance too. More intermediate results might have kept up the interest of the sponsors. Too many decisions had to be made on the basis of inadequate experimentaion. A: (Walkei) Groups were brought together and educated and then° disbanded. Q: I can hear the difference between /s/ and /z/, etc. Why don&apos;t you do some better acoustic work so as to be able to make such discriminations reliably? A: You &apos;&apos;hear&amp;quot; the difference in large part by means of higher constraints such as syntax, semantics, and even pragmatics. It is well documented, that the information for identification just isn&apos;t in the segment to be recognized in many cases. Q: Are there any companies now working on speech recognition? A: Yes. IBM is, and other companies such as Threshold Technology are working on word template matching.</abstract>
<note confidence="0.683641">Journal of Computational Linguistics 64 : .16</note>
<title confidence="0.390459">ON LEXICAL VOLUME II</title>
<author confidence="0.2818">BY</author>
<affiliation confidence="0.335717">SPRAKF6RLAGET SITIPTOR</affiliation>
<address confidence="0.51533025">Box S 104 65 Stockholm 1974 282 pages SKr 60 ISBN 91-7282-065-9</address>
<author confidence="0.666143">BY ERNST VON</author>
<affiliation confidence="0.990855">Department of Psychology</affiliation>
<address confidence="0.9346545">of Athens 30602</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Roger</author>
</authors>
<date></date>
<institution>Schank Computer Science Department, Yale university, new Haven, Connecticut Kenneth Mark Colby Algorithmic Laboratory of Higher Mental Functions, UCLA Department of Psychiatry,</institution>
<location>Los Angeles, Califoionia</location>
<marker>Roger, </marker>
<rawString>Roger C. Schank Computer Science Department, Yale university, new Haven, Connecticut Kenneth Mark Colby Algorithmic Laboratory of Higher Mental Functions, UCLA Department of Psychiatry, Los Angeles, Califoionia 90024</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
<author>Madeleine Bates</author>
<author>Bertram Bruce</author>
</authors>
<title>Uses of Higher Level Knowledge in a Speech Understanding System</title>
<date>0213</date>
<institution>and Bonnie Nash-Webber Bolt Beranek and Newman, Inc.,</institution>
<location>Cambridge, Massachusetts</location>
<marker>Woods, Bates, Bruce, 0213</marker>
<rawString>Uses of Higher Level Knowledge in a Speech Understanding System William A. Woods, Madeleine Bates, Bertram Bruce, and Bonnie Nash-Webber Bolt Beranek and Newman, Inc., Cambridge, Massachusetts 02138</rawString>
</citation>
<citation valid="false">
<title>Continuous speech understanding, for all but the most restricted languages, requires a characterization of the sentences of the language, the meanings of the words within the domain, and the use of the language within the task. Thus, syntax, semantics and pragmatics become essential components of a total speech understanding system. The control problem in such a system involves integrating the knowledge available from these components with the possible interpretations of an utterance suggested by its acoustics. This paper discusses various strategies for controlling these high-level sources of knowledge, reflecting their potential for coping with the inevitable acoustic ambiguity and error.</title>
<marker></marker>
<rawString>Continuous speech understanding, for all but the most restricted languages, requires a characterization of the sentences of the language, the meanings of the words within the domain, and the use of the language within the task. Thus, syntax, semantics and pragmatics become essential components of a total speech understanding system. The control problem in such a system involves integrating the knowledge available from these components with the possible interpretations of an utterance suggested by its acoustics. This paper discusses various strategies for controlling these high-level sources of knowledge, reflecting their potential for coping with the inevitable acoustic ambiguity and error.</rawString>
</citation>
<citation valid="true">
<date>1976</date>
<journal>GENERAL 46 The Prague Bulletin of Mathematical Linguistics</journal>
<volume>25</volume>
<institution>Universita Karlova,</institution>
<location>Praha,</location>
<contexts>
<context position="11925" citStr="(1976)" startWordPosition="1833" endWordPosition="1833">sic operation of the system was described as follows: (1) create initial theories (2) rank them, (3) give them to the parser, (4) have the parser suggest the n best words (5) verify best matches, (6) add results on to islands previously identified to form larger islands, and (7) let these new events trigger new theories and continue this cycle. Woods then presented transparencies citing in parallel columns the ARPA-imposed 19:71 goals and the degitee of success attained by the BBN HWIM System in meeting those, goals. I include reproductions of those transparencies. ARPA Goals (from 1971) HWIM (1976) The system should: The system did: 1 Accept continuous speec-h. Accept connected speech 2 From many From 3 3 Cooperative speakers of Cooperative male general the general American American speakers dialect 4 In a quiet room In a somewhat quiet computer terminal room 5 Over a good quality Over an ordinary close-talking microphone talking microphone 6 Allowing slight tuning With no tuning of the system of the system per speaker for the individual speaker 7 But rewring only And requiring no speaker natural adaptation by adaptation the user 8 Permitting a slightly On 1097 words with no postselecte</context>
<context position="51792" citStr="(1976)" startWordPosition="8396" endWordPosition="8396">his reads like research reports prepared for constant collaborators or colleagues who are well acquainted with the work. Finally, since this book was reproduced directly from a type Script, it is a pity that the typewriter used for the corrections was not the same as the original one and that so many errors managed to survive. Lexical Semantics 33 References Fillmore, Charles J. Entailment rules in a semantic theory. Report No. 10. Project on Linguistic Analysis. Columbus, Ohio: Ohio State University, 1965. Frame semantics and the nature of language. Annals of the New York Academy of Sciences (1976) 280, 20-32. Plutchik, Robert The emotions: Facts, theories, and a new model. New York: Random House, 1962. Schank, Roger C. SAM - A story understander. Research Report No. 43. New Haven, Connecticut: Yale University. 1975. American Journal of Computational Linguistics Microfiche 64 : 34 CONSTITUENT AND PATTERN IN POETRY ARCHIBALD A. HILL Prdfessor Emeritus of English and Linguistics University of Texas, Austin THE UNIVERSITY OF TEXAS PRESS Austin 78712 xiv + 157 pages 1976 LC 75-32582 $11.95 ISBN 0-292-72010-6 REVIEWED BY JAMES JOYCE Computer Sciences Division Department of Electrical Enginee</context>
</contexts>
<marker>1976</marker>
<rawString>GENERAL 46 The Prague Bulletin of Mathematical Linguistics 25 Universita Karlova, Praha, 1976</rawString>
</citation>
<citation valid="false">
<title>CONTENTS Semantische Naze und selektive Beziehungen,</title>
<journal>I. J. Kunze</journal>
<volume>3</volume>
<publisher>GENERAL</publisher>
<marker></marker>
<rawString>CONTENTS Semantische Naze und selektive Beziehungen, I. J. Kunze 3 Meaning of Sign, Cognitive Content, and Pragmatics, P. Sgall 51 GENERAL</rawString>
</citation>
<citation valid="false">
<date>1976</date>
<institution>The Prague Bulletin of Mathematical Linguistics 26 Universita Karlova,</institution>
<location>Praha,</location>
<contexts>
<context position="11925" citStr="(1976)" startWordPosition="1833" endWordPosition="1833">sic operation of the system was described as follows: (1) create initial theories (2) rank them, (3) give them to the parser, (4) have the parser suggest the n best words (5) verify best matches, (6) add results on to islands previously identified to form larger islands, and (7) let these new events trigger new theories and continue this cycle. Woods then presented transparencies citing in parallel columns the ARPA-imposed 19:71 goals and the degitee of success attained by the BBN HWIM System in meeting those, goals. I include reproductions of those transparencies. ARPA Goals (from 1971) HWIM (1976) The system should: The system did: 1 Accept continuous speec-h. Accept connected speech 2 From many From 3 3 Cooperative speakers of Cooperative male general the general American American speakers dialect 4 In a quiet room In a somewhat quiet computer terminal room 5 Over a good quality Over an ordinary close-talking microphone talking microphone 6 Allowing slight tuning With no tuning of the system of the system per speaker for the individual speaker 7 But rewring only And requiring no speaker natural adaptation by adaptation the user 8 Permitting a slightly On 1097 words with no postselecte</context>
<context position="51792" citStr="(1976)" startWordPosition="8396" endWordPosition="8396">his reads like research reports prepared for constant collaborators or colleagues who are well acquainted with the work. Finally, since this book was reproduced directly from a type Script, it is a pity that the typewriter used for the corrections was not the same as the original one and that so many errors managed to survive. Lexical Semantics 33 References Fillmore, Charles J. Entailment rules in a semantic theory. Report No. 10. Project on Linguistic Analysis. Columbus, Ohio: Ohio State University, 1965. Frame semantics and the nature of language. Annals of the New York Academy of Sciences (1976) 280, 20-32. Plutchik, Robert The emotions: Facts, theories, and a new model. New York: Random House, 1962. Schank, Roger C. SAM - A story understander. Research Report No. 43. New Haven, Connecticut: Yale University. 1975. American Journal of Computational Linguistics Microfiche 64 : 34 CONSTITUENT AND PATTERN IN POETRY ARCHIBALD A. HILL Prdfessor Emeritus of English and Linguistics University of Texas, Austin THE UNIVERSITY OF TEXAS PRESS Austin 78712 xiv + 157 pages 1976 LC 75-32582 $11.95 ISBN 0-292-72010-6 REVIEWED BY JAMES JOYCE Computer Sciences Division Department of Electrical Enginee</context>
</contexts>
<marker>1976</marker>
<rawString>The Prague Bulletin of Mathematical Linguistics 26 Universita Karlova, Praha, 1976</rawString>
</citation>
<citation valid="false">
<authors>
<author>CONTENTS</author>
</authors>
<title>0 znachenii rabot I. I. Revzina v oblasti teoretiko-mnozhestvennoi kontsepsii iazyka,</title>
<journal>0. G. Revzina, I. A. Shreider</journal>
<booktitle>2. Semantische Netze und selektive Beziehungen 11,</booktitle>
<volume>3</volume>
<marker>CONTENTS, </marker>
<rawString>CONTENTS 1. 0 znachenii rabot I. I. Revzina v oblasti teoretiko-mnozhestvennoi kontsepsii iazyka, 0. G. Revzina, I. A. Shreider 3 2. Semantische Netze und selektive Beziehungen 11,</rawString>
</citation>
<citation valid="false">
<title>0 modeli estestvennogo iazyka, osnovannoi na poniatii peremennykh okruzhenii,</title>
<journal>J. S. Bleb . • • . •</journal>
<volume>41</volume>
<marker></marker>
<rawString>J Kunze 17 . . 3. 0 modeli estestvennogo iazyka, osnovannoi na poniatii peremennykh okruzhenii, J. S. Bleb . • • . • 41</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Retrieval</author>
</authors>
<title>On Some Relationships of Linguistics and Information</title>
<date>1976</date>
<booktitle>Sgall 510 PHONETICS-PHONOLOGY 47 Annual Bulletin of the Research Institute of Logopedics and Phoniatrics, No. 10</booktitle>
<institution>Faculty of Medicine, University of Tokyo,</institution>
<marker>Retrieval, 1976</marker>
<rawString>On Some Relationships of Linguistics and Information Retrieval, P. Sgall 510 PHONETICS-PHONOLOGY 47 Annual Bulletin of the Research Institute of Logopedics and Phoniatrics, No. 10 Faculty of Medicine, University of Tokyo, March 1976 CONTENTS An Interactive Display Terminal for Image Measurement Using an X-Ray Microbeam</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Kiritani</author>
<author>K Itoh</author>
<author>E Takenaka</author>
<author>S Sekimoto</author>
<author>H Imagawa</author>
<author>H Fujisaki</author>
<author>M Sawashima</author>
</authors>
<marker>Kiritani, Itoh, Takenaka, Sekimoto, Imagawa, Fujisaki, Sawashima, </marker>
<rawString>Scan System 1 S. Kiritani, K. Itoh, E: Takenaka, S. Sekimoto, H. Imagawa, H. Fujisaki, and M. Sawashima</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Kiritani</author>
<author>S Sekimoto</author>
<author>H Imagawa</author>
<author>-K Itoh</author>
<author>T Ushijima</author>
<author>H Hirose</author>
</authors>
<title>Simultaneous Recording of EMG with Pellet Tracking by Use of X-Ray</title>
<journal>Microbeam</journal>
<volume>13</volume>
<marker>Kiritani, Sekimoto, Imagawa, Itoh, Ushijima, Hirose, </marker>
<rawString>Simultaneous Recording of EMG with Pellet Tracking by Use of X-Ray Microbeam 13 S. Kiritani, S. Sekimoto, H. Imagawa,-K. Itoh, T. Ushijima, and H. Hirose</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Kiritani</author>
<author>H Imagawa</author>
</authors>
<title>Principal Component Analysis of Tongue Pellet</title>
<journal>Moxement</journal>
<volume>15</volume>
<marker>Kiritani, Imagawa, </marker>
<rawString>Principal Component Analysis of Tongue Pellet Moxement 15 S. Kiritani and H. Imagawa</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tongue Pellet</author>
</authors>
<title>Movement for the Japanese CV Syllables - Observations Using the X-Ray Microbeam System</title>
<date></date>
<booktitle>Kiritani A Preliminary Report on the Electromyographic Study of the Production of the Japanese Semivowel /j/ 37</booktitle>
<marker>Pellet, </marker>
<rawString>Tongue Pellet Movement for the Japanese CV Syllables - Observations Using the X-Ray Microbeam System 19 S. Kiritani, K. Itoh, H. Imagawa, H. Fujisaki, and M. Sawashima Analysis of Tongue Point Movements by a Linear Second-Order System Model . . . . 29 Y. Sonoda and S. Kiritani A Preliminary Report on the Electromyographic Study of the Production of the Japanese Semivowel /j/ 37 K. Kakita, H. Hirose, T. Ushijima and M. Sawashima</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Hirose</author>
</authors>
<booktitle>Fiberoptic Acoustic Studies of Mandarin Stops and Affricates 47 R. lwata</booktitle>
<marker>Hirose, </marker>
<rawString>Fiberoptic Acoustic Studies of Mandarin Stops and Affricates 47 R. lwata and H. Hirose</rawString>
</citation>
<citation valid="false">
<authors>
<author>R S Weitzman</author>
<author>M Sawashima</author>
<author>H Hirose</author>
<author>T Ushijima</author>
</authors>
<title>Laryngeal Control in French Stops: A Fiberoptic,</title>
<journal>Acoustic an Electromyographic Study</journal>
<volume>81</volume>
<marker>Weitzman, Sawashima, Hirose, Ushijima, </marker>
<rawString>Devoiced and Whispered Vowels in Japanese 61 R. S. Weitzman, M. Sawashima, H. Hirose and T. Ushijima Laryngeal Control in French Stops: A Fiberoptic, Acoustic an Electromyographic Study 81 A-P. Benguerel, H. Hifose, M. Sawashima and T. Ushijima</rawString>
</citation>
<citation valid="false">
<authors>
<author>More on</author>
</authors>
<title>Laryngeal Control for Voicing Distinction in Japanese Consonant Production .</title>
<volume>101</volume>
<marker>on, </marker>
<rawString>More on Laryngeal Control for Voicing Distinction in Japanese Consonant Production . 101 H. Hirose and T. Ushijima</rawString>
</citation>
<citation valid="false">
<authors>
<author>Pitch</author>
</authors>
<title>Accent and Vowel Devoicing in Japanese:</title>
<journal>A Preliminary Study J. B. Lovins PHONETICS-PHONOLOGY</journal>
<volume>48</volume>
<marker>Pitch, </marker>
<rawString>Pitch Accent and Vowel Devoicing in Japanese: A Preliminary Study J. B. Lovins PHONETICS-PHONOLOGY 48</rawString>
</citation>
<citation valid="false">
<title>A Phonetic Descripticin of Tibetan with a review of the literature 127 0.</title>
<publisher>Kjellin</publisher>
<marker></marker>
<rawString>A Phonetic Descripticin of Tibetan with a review of the literature 127 0. Kjellin</rawString>
</citation>
<citation valid="false">
<authors>
<author>Recognition Analysis</author>
</authors>
<title>and Perception of Voiceless Frictive Consonants</title>
<booktitle>in Japanese . . 145 H. Fujisaki and 0 Kunisaki *Acoustic and Perceptual Analysis of Two-Mora Word&apos;Accent Types in the Osaka</booktitle>
<marker>Analysis, </marker>
<rawString>*Analysis, Recognition, and Perception of Voiceless Frictive Consonants in Japanese . . 145 H. Fujisaki and 0 Kunisaki *Acoustic and Perceptual Analysis of Two-Mora Word&apos;Accent Types in the Osaka</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Fujisaki</author>
<author>M Sugito</author>
</authors>
<marker>Fujisaki, Sugito, </marker>
<rawString>Dialect 1,57 H. Fujisaki and M. Sugito</rawString>
</citation>
<citation valid="false">
<authors>
<author>Synthesis Analysis</author>
</authors>
<title>and Perception of Word Accent Types in</title>
<journal>Japanese</journal>
<volume>173</volume>
<marker>Analysis, </marker>
<rawString>*Analysis, Synthesis, and Perception of Word Accent Types in Japanese 173 H. Fujisaki, H. Hirose and M. Sugito *Temmal Organization of Articulatory and Phonatory ControJs in Realization of</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Fujisaki</author>
<author>H Morikawa</author>
<author>M Sugito</author>
</authors>
<marker>Fujisaki, Morikawa, Sugito, </marker>
<rawString>Japanese Word Accent 177 H. Fujisaki, H. Morikawa, and M. Sugito</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kunisaki</author>
<author>H</author>
</authors>
<title>Acoustic Analysis and Subjective Evaluation of Sung Vowels</title>
<date></date>
<booktitle>Fujisaki On the Development of Perceptual Strategies in Children: A Case Study on the</booktitle>
<marker>Kunisaki, H, </marker>
<rawString>Acoustic Analysis and Subjective Evaluation of Sung Vowels 191 M. Tatsumi, 0. Kunisaki, and H. Fujisaki On the Development of Perceptual Strategies in Children: A Case Study on the</rawString>
</citation>
<citation valid="false">
<authors>
<author>Japanese Child&apos;s</author>
</authors>
<title>Comprehension of the Relative Clause Constructions</title>
<date></date>
<journal>Sasanuma A Computational Model of the Tongue</journal>
<volume>233</volume>
<marker>Child&apos;s, </marker>
<rawString>Japanese Child&apos;s Comprehension of the Relative Clause Constructions 199 S.I. Harada, T. Uyeno, H. Hayashibe, and H. Yamada A Kinesiological Aspect of Myasthenia Gravis - A Electro-myographic Study of Velar Movements During Speech T, Ushijima, M. Sawashima H. Hirose, M. Abe and T. Harada Construction of a Short Test of Aphasia on the Basis of Factor Analysis 233 , Y. Fukusako and S. Sasanuma A Computational Model of the Tongue 243</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fujimura</author>
<author>J E Miller</author>
</authors>
<title>Recent Publications 253 *These articles have been abstracted elsewhere on this fiche.</title>
<pages>225</pages>
<marker>Fujimura, Miller, </marker>
<rawString>S. Kiritani, K. Miyawaki, 0. Fujimura, and J. E. Miller , Recent Publications 253 *These articles have been abstracted elsewhere on this fiche. 225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richard</author>
</authors>
<title>9. Acoustic-Phonetic Experiment Facility for the Study of Continuous Speech</title>
<date>0213</date>
<volume>4</volume>
<institution>Schwartz Bolt Beranek and Newman, Inc.,</institution>
<location>Cambridge, Massachusetts</location>
<marker>Richard, 0213</marker>
<rawString>PHONETICS-PHONOLOGY 4 9. Acoustic-Phonetic Experiment Facility for the Study of Continuous Speech Richard M. Schwartz Bolt Beranek and Newman, Inc., Cambridge, Massachusetts 02138,</rawString>
</citation>
<citation valid="false">
<title>While gathering acoustic data for the acoustic-phonetic analysis of&apos; speech, it is necessary to consider many different sounds in varying phonetic environments to assure that the results are statistically significant. In order to reduce the amount of time requiyed to test hypotheses, a facility has been developed which provides an interactive environment for performing a wide variety of acoustic-phonetic experiments on a large data base of continuous speech. Using this facility, one can formulate an experiment, run it on selected portions (or all) of the data base, and display or tabulate the results in a meaningful way. Another experiment may then be run based on the results. CPU time required to run an experiment on the entire data base is between 5 and 20 seconds, depending on the complexity of the experiment. Due to the ease of interactio,ns, formulating or revising an experiment, running it, and displaying the results normally takes less than 5 minutes. This facility has been used in combination with a data base of 69 hand-labeled sentences to develop algorithms for acoustic-phonetic segmentation and labeling in a speech understanding system. Several examples of its use and the results obtained are presented.</title>
<marker></marker>
<rawString>While gathering acoustic data for the acoustic-phonetic analysis of&apos; speech, it is necessary to consider many different sounds in varying phonetic environments to assure that the results are statistically significant. In order to reduce the amount of time requiyed to test hypotheses, a facility has been developed which provides an interactive environment for performing a wide variety of acoustic-phonetic experiments on a large data base of continuous speech. Using this facility, one can formulate an experiment, run it on selected portions (or all) of the data base, and display or tabulate the results in a meaningful way. Another experiment may then be run based on the results. CPU time required to run an experiment on the entire data base is between 5 and 20 seconds, depending on the complexity of the experiment. Due to the ease of interactio,ns, formulating or revising an experiment, running it, and displaying the results normally takes less than 5 minutes. This facility has been used in combination with a data base of 69 hand-labeled sentences to develop algorithms for acoustic-phonetic segmentation and labeling in a speech understanding system. Several examples of its use and the results obtained are presented.</rawString>
</citation>
<citation valid="false">
<authors>
<author>PHONETICS-PHONOLOGY PHONOLOGY</author>
</authors>
<title>Analysis, Recognition, and Perception of Voiceless Fricative Consonants</title>
<note>in Japanese</note>
<marker>PHONOLOGY, </marker>
<rawString>PHONETICS-PHONOLOGY: PHONOLOGY Analysis, Recognition, and Perception of Voiceless Fricative Consonants in Japanese</rawString>
</citation>
<citation valid="true">
<title>Dictionary Expansion via Phonlogical Rules for a Speech Understanding System</title>
<date>1976</date>
<journal>Annual Bulletin of the Resedrch Institute of Logopedics and Phoniatrics</journal>
<volume>10</volume>
<pages>145--156</pages>
<institution>Hiroya Fujisaki Research Institute of Logopedics and Phoniatrics, University of Tokyo Osamu Kunisaki Department of Electrical engineering, Faculty of Engineering, University of Tokyo</institution>
<contexts>
<context position="11925" citStr="(1976)" startWordPosition="1833" endWordPosition="1833">sic operation of the system was described as follows: (1) create initial theories (2) rank them, (3) give them to the parser, (4) have the parser suggest the n best words (5) verify best matches, (6) add results on to islands previously identified to form larger islands, and (7) let these new events trigger new theories and continue this cycle. Woods then presented transparencies citing in parallel columns the ARPA-imposed 19:71 goals and the degitee of success attained by the BBN HWIM System in meeting those, goals. I include reproductions of those transparencies. ARPA Goals (from 1971) HWIM (1976) The system should: The system did: 1 Accept continuous speec-h. Accept connected speech 2 From many From 3 3 Cooperative speakers of Cooperative male general the general American American speakers dialect 4 In a quiet room In a somewhat quiet computer terminal room 5 Over a good quality Over an ordinary close-talking microphone talking microphone 6 Allowing slight tuning With no tuning of the system of the system per speaker for the individual speaker 7 But rewring only And requiring no speaker natural adaptation by adaptation the user 8 Permitting a slightly On 1097 words with no postselecte</context>
<context position="51792" citStr="(1976)" startWordPosition="8396" endWordPosition="8396">his reads like research reports prepared for constant collaborators or colleagues who are well acquainted with the work. Finally, since this book was reproduced directly from a type Script, it is a pity that the typewriter used for the corrections was not the same as the original one and that so many errors managed to survive. Lexical Semantics 33 References Fillmore, Charles J. Entailment rules in a semantic theory. Report No. 10. Project on Linguistic Analysis. Columbus, Ohio: Ohio State University, 1965. Frame semantics and the nature of language. Annals of the New York Academy of Sciences (1976) 280, 20-32. Plutchik, Robert The emotions: Facts, theories, and a new model. New York: Random House, 1962. Schank, Roger C. SAM - A story understander. Research Report No. 43. New Haven, Connecticut: Yale University. 1975. American Journal of Computational Linguistics Microfiche 64 : 34 CONSTITUENT AND PATTERN IN POETRY ARCHIBALD A. HILL Prdfessor Emeritus of English and Linguistics University of Texas, Austin THE UNIVERSITY OF TEXAS PRESS Austin 78712 xiv + 157 pages 1976 LC 75-32582 $11.95 ISBN 0-292-72010-6 REVIEWED BY JAMES JOYCE Computer Sciences Division Department of Electrical Enginee</context>
</contexts>
<marker>1976</marker>
<rawString>Hiroya Fujisaki Research Institute of Logopedics and Phoniatrics, University of Tokyo Osamu Kunisaki Department of Electrical engineering, Faculty of Engineering, University of Tokyo Annual Bulletin of the Resedrch Institute of Logopedics and Phoniatrics 10: 145-156, 1976 Dictionary Expansion via Phonlogical Rules for a Speech Understanding System</rawString>
</citation>
<citation valid="false">
<authors>
<author>Winiam A Woods</author>
<author>W Vietor</author>
</authors>
<note>Zue</note>
<marker>Woods, Vietor, </marker>
<rawString>Winiam A. Woods, and Vietor W. Zue</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>