<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000625">
<title confidence="0.99585">
Non-Projective Dependency Parsing in Expected Linear Time
</title>
<author confidence="0.99178">
Joakim Nivre
</author>
<affiliation confidence="0.992789">
Uppsala University, Department of Linguistics and Philology, SE-75126 Uppsala
V¨axj¨o University, School of Mathematics and Systems Engineering, SE-35195 V¨axj¨o
</affiliation>
<email confidence="0.999569">
E-mail: joakim.nivre@lingfil.uu.se
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998672">
We present a novel transition system for
dependency parsing, which constructs arcs
only between adjacent words but can parse
arbitrary non-projective trees by swapping
the order of words in the input. Adding
the swapping operation changes the time
complexity for deterministic parsing from
linear to quadratic in the worst case, but
empirical estimates based on treebank data
show that the expected running time is in
fact linear for the range of data attested in
the corpora. Evaluation on data from five
languages shows state-of-the-art accuracy,
with especially good results for the labeled
exact match score.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999728238095238">
Syntactic parsing using dependency structures has
become a standard technique in natural language
processing with many different parsing models, in
particular data-driven models that can be trained
on syntactically annotated corpora (Yamada and
Matsumoto, 2003; Nivre et al., 2004; McDonald
et al., 2005a; Attardi, 2006; Titov and Henderson,
2007). A hallmark of many of these models is that
they can be implemented very efficiently. Thus,
transition-based parsers normally run in linear or
quadratic time, using greedy deterministic search
or fixed-width beam search (Nivre et al., 2004; At-
tardi, 2006; Johansson and Nugues, 2007; Titov
and Henderson, 2007), and graph-based models
support exact inference in at most cubic time,
which is efficient enough to make global discrim-
inative training practically feasible (McDonald et
al., 2005a; McDonald et al., 2005b).
However, one problem that still has not found
a satisfactory solution in data-driven dependency
parsing is the treatment of discontinuous syntactic
constructions, usually modeled by non-projective
dependency trees, as illustrated in Figure 1. In a
projective dependency tree, the yield of every sub-
tree is a contiguous substring of the sentence. This
is not the case for the tree in Figure 1, where the
subtrees rooted at node 2 (hearing) and node 4
(scheduled) both have discontinuous yields.
Allowing non-projective trees generally makes
parsing computationally harder. Exact inference
for parsing models that allow non-projective trees
is NP hard, except under very restricted indepen-
dence assumptions (Neuhaus and Br¨oker, 1997;
McDonald and Pereira, 2006; McDonald and
Satta, 2007). There is recent work on algorithms
that can cope with important subsets of all non-
projective trees in polynomial time (Kuhlmann
and Satta, 2009; G´omez-Rodr´ıguez et al., 2009),
but the time complexity is at best O(n6), which
can be problematic in practical applications. Even
the best algorithms for deterministic parsing run in
quadratic time, rather than linear (Nivre, 2008a),
unless restricted to a subset of non-projective
structures as in Attardi (2006) and Nivre (2007).
But allowing non-projective dependency trees
also makes parsing empirically harder, because
it requires that we model relations between non-
adjacent structures over potentially unbounded
distances, which often has a negative impact on
parsing accuracy. On the other hand, it is hardly
possible to ignore non-projective structures com-
pletely, given that 25% or more of the sentences
in some languages cannot be given a linguistically
adequate analysis without invoking non-projective
structures (Nivre, 2006; Kuhlmann and Nivre,
2006; Havelka, 2007).
Current approaches to data-driven dependency
parsing typically use one of two strategies to deal
with non-projective trees (unless they ignore them
completely). Either they employ a non-standard
parsing algorithm that can combine non-adjacent
substructures (McDonald et al., 2005b; Attardi,
2006; Nivre, 2007), or they try to recover non-
</bodyText>
<page confidence="0.979124">
351
</page>
<note confidence="0.9996385">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.972705666666667">
P
✞ ☎
ROOT
ADV
✞ ☎
✞ ☎
NMOD PC
DET
✞
❄
✞
SBJ
✞ ☎
❄
VG
✞ ☎
❄ ❄
❄
❄
✞ ☎
DET
☎
❄
✞
❄
❄
ROOT0 A1 hearing2 isg scheduled4 ons the6 issue? today$ .9
</figure>
<figureCaption confidence="0.999968">
Figure 1: Dependency tree for an English sentence (non-projective).
</figureCaption>
<bodyText confidence="0.999891407407407">
projective dependencies by post-processing the
output of a strictly projective parser (Nivre and
Nilsson, 2005; Hall and Nov´ak, 2005; McDonald
and Pereira, 2006). In this paper, we will adopt
a different strategy, suggested in recent work by
Nivre (2008b) and Titov et al. (2009), and pro-
pose an algorithm that only combines adjacent
substructures but derives non-projective trees by
reordering the input words.
The rest of the paper is structured as follows.
In Section 2, we define the formal representations
needed and introduce the framework of transition-
based dependency parsing. In Section 3, we first
define a minimal transition system and explain
how it can be used to perform projective depen-
dency parsing in linear time; we then extend the
system with a single transition for swapping the
order of words in the input and demonstrate that
the extended system can be used to parse unre-
stricted dependency trees with a time complexity
that is quadratic in the worst case but still linear
in the best case. In Section 4, we present experi-
ments indicating that the expected running time of
the new system on naturally occurring data is in
fact linear and that the system achieves state-of-
the-art parsing accuracy. We discuss related work
in Section 5 and conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.996763" genericHeader="introduction">
2 Background Notions
</sectionHeader>
<subsectionHeader confidence="0.999896">
2.1 Dependency Graphs and Trees
</subsectionHeader>
<bodyText confidence="0.973780666666667">
Given a set L of dependency labels, a dependency
graph for a sentence x = w1, ... , wn is a directed
graph G = (Vx, A), where
</bodyText>
<listItem confidence="0.992968">
1. Vx = 10, 1, ... , n} is a set of nodes,
2. A C Vx x L x Vx is a set of labeled arcs.
</listItem>
<bodyText confidence="0.997220125">
The set Vx of nodes is the set of positive integers
up to and including n, each corresponding to the
linear position of a word in the sentence, plus an
extra artificial root node 0. The set A of arcs is a
set of triples (i, l, j), where i and j are nodes and l
is a label. For a dependency graph G = (Vx, A) to
be well-formed, we in addition require that it is a
tree rooted at the node 0, as illustrated in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999554">
2.2 Transition Systems
</subsectionHeader>
<bodyText confidence="0.969907">
Following Nivre (2008a), we define a transition
system for dependency parsing as a quadruple 5 =
(C, T, cs, Ct), where
</bodyText>
<listItem confidence="0.983335428571429">
1. C is a set of configurations,
2. T is a set of transitions, each of which is a
(partial) function t : C —* C,
3. cs is an initialization function, mapping a
sentence x = w1, ... , wn to a configuration
c E C,
4. Ct C C is a set of terminal configurations.
</listItem>
<bodyText confidence="0.848873391304348">
In this paper, we take the set C of configurations
to be the set of all triples c = (E, B, A) such that
E and B are disjoint sublists of the nodes Vx of
some sentence x, and A is a set of dependency arcs
over Vx (and some label set L); we take the initial
configuration for a sentence x = w1, ... , wn to
be cs(x) = ([0], [1, ... , n], 11); and we take the
set Ct of terminal configurations to be the set of
all configurations of the form c = ([0], [ ], A) (for
any arc set A). The set T of transitions will be
discussed in detail in Sections 3.1–3.2.
We will refer to the list E as the stack and the list
B as the buffer, and we will use the variables Q and
0 for arbitrary sublists of E and B, respectively.
For reasons of perspicuity, we will write E with its
head (top) to the right and B with its head to the
left. Thus, c = ([QIi], [j10], A) is a configuration
with the node i on top of the stack E and the node
j as the first node in the buffer B.
Given a transition system 5 = (C, T, cs, Ct), a
transition sequence for a sentence x is a sequence
C0,m = (c0, c1, ... , cm) of configurations, such
that
</bodyText>
<listItem confidence="0.97556025">
1. c0 = cs(x),
2. cm E Ct,
3. for every i (1 G i G m), cz = t(cz−1) for
some t E T.
</listItem>
<page confidence="0.996098">
352
</page>
<subsectionHeader confidence="0.446495">
Transition Condition
</subsectionHeader>
<construct confidence="0.792704">
LEFT-ARCl ([σ|i, j], B, A) ==&gt;. ([σ|j], B, AU{(j,l, i)}) i =� 0
RIGHT-ARCl ([σ|i, j], B, A) ==&gt;. ([σ|i], B, AU{(i,l, j)})
SHIFT (σ, [i|β], A) ==&gt;. ([σ|i], β, A)
SWAP ([σ|i, j], β, A) ==&gt;. ([σ|j], [i|β], A) 0 &lt; i &lt; j
</construct>
<figureCaption confidence="0.996663">
Figure 2: Transitions for dependency parsing; Tp = {LEFT-ARCl, RIGHT-ARCl, SHIFT}; Tu = Tp U {SWAP}.
</figureCaption>
<bodyText confidence="0.999882888888889">
The parse assigned to S by C0,m is the depen-
dency graph Gcm = (Vx, Acm), where Acm is the
set of arcs in cm.
A transition system S is sound for a class G of
dependency graphs iff, for every sentence x and
transition sequence C0,m for x in S, Gcm E G. S
is complete for G iff, for every sentence x and de-
pendency graph G for x in G, there is a transition
sequence C0,m for x in S such that Gcm = G.
</bodyText>
<subsectionHeader confidence="0.999243">
2.3 Deterministic Transition-Based Parsing
</subsectionHeader>
<bodyText confidence="0.999986875">
An oracle for a transition system S is a function
o : C —* T. Ideally, o should always return the
optimal transition t for a given configuration c, but
all we require formally is that it respects the pre-
conditions of transitions in T. That is, if o(c) = t
then t is permissible in c. Given an oracle o, deter-
ministic transition-based parsing can be achieved
by the following simple algorithm:
</bodyText>
<equation confidence="0.7819168">
PARSE(o, x)
1 c +— cs(x)
2 while c E� Ct
3 do t +— o(c); c +— t(c)
4 return Gc
</equation>
<bodyText confidence="0.999973590909091">
Starting in the initial configuration cs(x), the
parser repeatedly calls the oracle function o for the
current configuration c and updates c according to
the oracle transition t. The iteration stops when a
terminal configuration is reached. It is easy to see
that, provided that there is at least one transition
sequence in S for every sentence, the parser con-
structs exactly one transition sequence C0,m for a
sentence x and returns the parse defined by the ter-
minal configuration cm, i.e., Gcm = (Vx, Acm).
Assuming that the calls o(c) and t(c) can both be
performed in constant time, the worst-case time
complexity of a deterministic parser based on a
transition system S is given by an upper bound on
the length of transition sequences in S.
When building practical parsing systems, the
oracle can be approximated by a classifier trained
on treebank data, a technique that has been used
successfully in a number of systems (Yamada and
Matsumoto, 2003; Nivre et al., 2004; Attardi,
2006). This is also the approach we will take in
the experimental evaluation in Section 4.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="method">
3 Transitions for Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999988555555555">
Having defined the set of configurations, including
initial and terminal configurations, we will now
focus on the transition set T required for depen-
dency parsing. The total set of transitions that will
be considered is given in Figure 2, but we will start
in Section 3.1 with the subset Tp (p for projective)
consisting of the first three. In Section 3.2, we
will add the fourth transition (SWAP) to get the full
transition set Tu (u for unrestricted).
</bodyText>
<subsectionHeader confidence="0.999199">
3.1 Projective Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999483">
The minimal transition set Tp for projective depen-
dency parsing contains three transitions:
</bodyText>
<listItem confidence="0.9941116">
1. LEFT-ARCl updates a configuration with i, j
on top of the stack by adding (j,l, i) to A and
replacing i, j on the stack by j alone. It is
permissible as long as i is distinct from 0.
2. RIGHT-ARCl updates a configuration with
i, j on top of the stack by adding (i, l, j) to
A and replacing i, j on the stack by i alone.
3. SHIFT updates a configuration with i as the
first node of the buffer by removing i from
the buffer and pushing it onto the stack.
</listItem>
<bodyText confidence="0.993710333333333">
The system Sp = (C, Tp, cs, Ct) is sound and
complete for the set of projective dependency
trees (over some label set L) and has been used,
in slightly different variants, by a number of
transition-based dependency parsers (Yamada and
Matsumoto, 2003; Nivre, 2004; Attardi, 2006;
</bodyText>
<page confidence="0.996072">
353
</page>
<subsectionHeader confidence="0.322505">
Transition Stack (E) Buffer (B) Added Arc
</subsectionHeader>
<equation confidence="0.993739387096774">
[ROOT0] [A1, ... ,.9]
SHIFT [ROOT0, A1] [hearing2, ... ,.9]
SHIFT [ROOT0, A1, hearing2] [is3, ... , .9]
LADET [ROOT0, hearing2] [is3,... , .9] (2, DET, 1)
SHIFT [ROOT0, hearing2, is3] [scheduled4, ... , .9]
SHIFT [ROOT0,... , is3, scheduled4] [on5,...,.9]
SHIFT [ROOT0, ... , scheduled4, on5] [the6, ... , .9]
SWAP [ROOT0,... , is3, on5] [scheduled4, ... , .9]
SWAP [ROOT0, hearing2, on5] [is3, ... , .9]
SHIFT [ROOT0,... , on5, is3] [scheduled4, ... , .9]
SHIFT [ROOT0,... , is3, scheduled4] [the6, ... , .9]
SHIFT [ROOT0, ... , scheduled4, the6] [issue7,...,.9]
SWAP [ROOT0,... , is3, the6] [scheduled4, ... , .9]
SWAP [ROOT0,... , on5, the6] [is3, ... , .9]
SHIFT [ROOT0,... , the6, is3] [scheduled4, ... , .9]
SHIFT [ROOT0,... , is3, scheduled4] [issue7,...,.9]
SHIFT [ROOT0, ... , scheduled4, issue7] [today$, .9]
SWAP [ROOT0,... , is3, issue7] [scheduled4, ... , .9]
SWAP [ROOT0,... , the6, issue7] [is3, ... , .9]
LADET [ROOT0,... , on5, issue7] [is3,... , .9] (7, DET, 6)
RAPC [ROOT0, hearing2, on5] [is3, ... , .9] (5, PC, 7)
RANMOD [ROOT0, hearing2] [is3, ... , .9] (2, NMOD, 5)
SHIFT [ROOT0, ... , hearing2, is3] [scheduled4, ... , .9]
LASBJ [ROOT0, is3] [scheduled4, ... , .9] (3, SBJ, 2)
SHIFT [ROOT0, is3, scheduled4] [today$, .9]
SHIFT [ROOT0, ... , scheduled4, today$] [.9]
RAADV [ROOT0, is3, scheduled4] [.9] (4, ADV, 8)
RAVG [ROOT0, is3] [.9] (3, VG, 4)
SHIFT [ROOT0, is3, .9] [ ]
RAP [ROOT0, is3] [ ] (3, P, 9)
RAROOT [ROOT0] [] (0, ROOT, 3)
</equation>
<figureCaption confidence="0.999071">
Figure 3: Transition sequence for parsing the sentence in Figure 1 (LA = LEFT-ARC, RA = REFT-ARC).
</figureCaption>
<bodyText confidence="0.999779076923077">
Nivre, 2008a). For proofs of soundness and com-
pleteness, see Nivre (2008a).
As noted in section 2, the worst-case time com-
plexity of a deterministic transition-based parser is
given by an upper bound on the length of transition
sequences. In 5p, the number of transitions for a
sentence x = w1, ... , wn is always exactly 2n,
since a terminal configuration can only be reached
after n SHIFT transitions (moving nodes 1, ... , n
from B to E) and n applications of LEFT-ARCl or
RIGHT-ARCl (removing the same nodes from E).
Hence, the complexity of deterministic parsing is
O(n) in the worst case (as well as in the best case).
</bodyText>
<subsectionHeader confidence="0.996404">
3.2 Unrestricted Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999485789473684">
We now consider what happens when we add the
fourth transition from Figure 2 to get the extended
transition set T,. The SWAP transition updates
a configuration with stack [aJi, j] by moving the
node i back to the buffer. This has the effect that
the order of the nodes i and j in the appended list
E+B is reversed compared to the original word
order in the sentence. It is important to note that
SWAP is only permissible when the two nodes on
top of the stack are in the original word order,
which prevents the same two nodes from being
swapped more than once, and when the leftmost
node i is distinct from the root node 0. Note also
that SWAP moves the node i back to the buffer, so
that LEFT-ARCl, RIGHT-ARCl or SWAP can sub-
sequently apply with the node j on top of the stack.
The fact that we can swap the order of nodes,
implicitly representing subtrees, means that we
can construct non-projective trees by applying
</bodyText>
<page confidence="0.979197">
354
</page>
<table confidence="0.454022">
o(c) = { LEFT-ARCl if c = ([u|i, j], B, A,), (j,l, i)EA and Ai C A,
RIGHT-ARCl if c = ([u|i, j], B, A,), (i,l, j)EA and Ai C A,
SWAP if c = ([u|i, j], B, A,) and j &lt;G i
SHIFT otherwise
</table>
<figureCaption confidence="0.980937">
Figure 4: Oracle function for Su = (C, Tu, cs, Ct) with target tree G = (Vx, A). We use the notation Ai
</figureCaption>
<bodyText confidence="0.992544225">
to denote the subset of A that only contains the outgoing arcs of the node i.
LEFT-ARCl or RIGHT-ARCl to subtrees whose
yields are not adjacent according to the original
word order. This is illustrated in Figure 3, which
shows the transition sequence needed to parse the
example in Figure 1. For readability, we represent
both the stack E and the buffer B as lists of tokens,
indexed by position, rather than abstract nodes.
The last column records the arc that is added to
the arc set A in a given transition (if any).
Given the simplicity of the extension, it is rather
remarkable that the system Su = (C, Tu, cs, Ct)
is sound and complete for the set of all depen-
dency trees (over some label set L), including all
non-projective trees. The soundness part is triv-
ial, since any terminating transition sequence will
have to move all the nodes 1, ... , n from B to E
(using SHIFT) and then remove them from E (us-
ing LEFT-ARCl or RIGHT-ARCl), which will pro-
duce a tree with root 0.
For completeness, we note first that projectiv-
ity is not a property of a dependency tree in itself,
but of the tree in combination with a word order,
and that a tree can always be made projective by
reordering the nodes. For instance, let x be a sen-
tence with dependency tree G = (Vx, A), and let
&lt;G be the total order on Vx defined by an inorder
traversal of G that respects the local ordering of a
node and its children given by the original word
order. Regardless of whether G is projective with
respect to x, it must by necessity be projective with
respect to &lt;G. We call &lt;G the projective order
corresponding to x and G and use it as our canoni-
cal way of finding a node order that makes the tree
projective. By way of illustration, the projective
order for the sentence and tree in Figure 1 is: A1
&lt;G hearing2 &lt;G on5 &lt;G the6 &lt;G issue7 &lt;G is3
&lt;G scheduled4 &lt;G today8 &lt;G .9.
If the words of a sentence x with dependency
tree G are already in projective order, this means
that G is projective with respect to x and that we
can parse the sentence using only transitions in Tp„
because nodes can be pushed onto the stack in pro-
jective order using only the SHIFT transition. If
the words are not in projective order, we can use
a combination of SHIFT and SWAP transitions to
ensure that nodes are still pushed onto the stack in
projective order. More precisely, if the next node
in the projective order is the kth node in the buffer,
we perform k SHIFT transitions, to get this node
onto the stack, followed by k−1 SWAP transitions,
to move the preceding k − 1 nodes back to the
buffer.1 In this way, the parser can effectively sort
the input nodes into projective order on the stack,
repeatedly extracting the minimal element of &lt;G
from the buffer, and build a tree that is projective
with respect to the sorted order. Since any input
can be sorted using SHIFT and SWAP, and any pro-
jective tree can be built using SHIFT, LEFT-ARCl
and RIGHT-ARCl, the system Su is complete for
the set of all dependency trees.
In Figure 4, we define an oracle function o for
the system Su, which implements this “sort and
parse” strategy and predicts the optimal transition
t out of the current configuration c, given the tar-
get dependency tree G = (Vx, A) and the pro-
jective order &lt;G. The oracle predicts LEFT-ARCl
or RIGHT-ARCl if the two top nodes on the stack
should be connected by an arc and if the depen-
dent node of this arc is already connected to all its
dependents; it predicts SWAP if the two top nodes
are not in projective order; and it predicts SHIFT
otherwise. This is the oracle that has been used to
generate training data for classifiers in the experi-
mental evaluation in Section 4.
Let us now consider the time complexity of the
extended system Su = (C, Tu, cs, Ct) and let us
begin by observing that 2n is still a lower bound
on the number of transitions required to reach a
terminal configuration. A sequence of 2n transi-
</bodyText>
<footnote confidence="0.93459175">
1This can be seen in Figure 3, where transitions 4–8, 9–
13, and 14–18 are the transitions needed to make sure that
on5, then and issue7 are processed on the stack before is3 and
scheduled4.
</footnote>
<page confidence="0.997638">
355
</page>
<figureCaption confidence="0.9322455">
Figure 5: Abstract running time during training (black) and parsing (white) for Arabic (1460/146 sen-
tences) and Danish (5190/322 sentences).
</figureCaption>
<bodyText confidence="0.997417361111111">
tions occurs when no SWAP transitions are per-
formed, in which case the behavior of the system
is identical to the simpler system 5p. This is im-
portant, because it means that the best-case com-
plexity of the deterministic parser is still O(n) and
that the we can expect to observe the best case for
all sentences with projective dependency trees.
The exact number of additional transitions
needed to reach a terminal configuration is deter-
mined by the number of SWAP transitions. Since
SWAP moves one node from E to B, there will
be one additional SHIFT for every SWAP, which
means that the total number of transitions is 2n +
2k, where k is the number of SWAP transitions.
Given the condition that SWAP can only apply in a
configuration c = ([a|i, j], B, A) if 0 &lt; i &lt; j, the
number of SWAP transitions is bounded by n(n�1)
2 ,
which means that 2n + n(n − 1) = n + n2 is an
upper bound on the number of transitions in a ter-
minating sequence. Hence, the worst-case com-
plexity of the deterministic parser is O(n2).
The running time of a deterministic transition-
based parser using the system 5,, is O(n) in the
best case and O(n2) in the worst case. But what
about the average case? Empirical studies, based
on data from a wide range of languages, have
shown that dependency trees tend to be projective
and that most non-projective trees only contain
a small number of discontinuities (Nivre, 2006;
Kuhlmann and Nivre, 2006; Havelka, 2007). This
should mean that the expected number of swaps
per sentence is small, and that the running time is
linear on average for the range of inputs that occur
in natural languages. This is a hypothesis that will
be tested experimentally in the next section.
</bodyText>
<sectionHeader confidence="0.99986" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999984888888889">
Our experiments are based on five data sets from
the CoNLL-X shared task: Arabic, Czech, Danish,
Slovene, and Turkish (Buchholz and Marsi, 2006).
These languages have been selected because the
data come from genuine dependency treebanks,
whereas all the other data sets are based on some
kind of conversion from another type of represen-
tation, which could potentially distort the distribu-
tion of different types of structures in the data.
</bodyText>
<subsectionHeader confidence="0.99933">
4.1 Running Time
</subsectionHeader>
<bodyText confidence="0.99968005">
In section 3.2, we hypothesized that the expected
running time of a deterministic parser using the
transition system 5,, would be linear, rather than
quadratic. To test this hypothesis, we examine
how the number of transitions varies as a func-
tion of sentence length. We call this the abstract
running time, since it abstracts over the actual
time needed to compute each oracle prediction and
transition, which is normally constant but depen-
dent on the type of classifier used.
We first measured the abstract running time on
the training sets, using the oracle to derive the
transition sequence for every sentence, to see how
many transitions are required in the ideal case. We
then performed the same measurement on the test
sets, using classifiers trained on the oracle transi-
tion sequences from the training sets (as described
below in Section 4.2), to see whether the trained
parsers deviate from the ideal case.
The result for Arabic and Danish can be seen
</bodyText>
<page confidence="0.995961">
356
</page>
<table confidence="0.99348975">
Arabic Czech Danish Slovene Turkish
System AS EM AS EM AS EM AS EM AS EM
Su 67.1 (9.1) 11.6 82.4 (73.8) 35.3 84.2 (22.5) 26.7 75.2 (23.0) 29.9 64.9 (11.8) 21.5
Sp 67.3 (18.2) 11.6 80.9 (3.7) 31.2 84.6 (0.0) 27.0 74.2 (3.4) 29.9 65.3 (6.6) 21.0
Spp 67.2 (18.2) 11.6 82.1 (60.7) 34.0 84.7 (22.5) 28.9 74.8 (20.7) 26.9 65.5 (11.8) 20.7
Malt-06 66.7 (18.2) 11.0 78.4 (57.9) 27.4 84.8 (27.5) 26.7 70.3 (20.7) 19.7 65.7 (9.2) 19.3
MST-06 66.9 (0.0) 10.3 80.2 (61.7) 29.9 84.8 (62.5) 25.5 73.4 (26.4) 20.9 63.2 (11.8) 20.2
MSTMalt 68.6 (9.4) 11.0 82.3 (69.2) 31.2 86.7 (60.0) 29.8 75.9 (27.6) 26.6 66.3 (9.2) 18.6
</table>
<tableCaption confidence="0.999952">
Table 1: Labeled accuracy; AS = attachment score (non-projective arcs in brackets); EM = exact match.
</tableCaption>
<bodyText confidence="0.999921181818182">
in Figure 5, where black dots represent training
sentences (parsed with the oracle) and white dots
represent test sentences (parsed with a classifier).
For Arabic there is a very clear linear relationship
in both cases with very few outliers. Fitting the
data with a linear function using the least squares
method gives us m = 2.06n (R2 = 0.97) for the
training data and m = 2.02n (R2 = 0.98) for the
test data, where m is the number of transitions in
parsing a sentence of length n. For Danish, there
is clearly more variation, especially for the train-
ing data, but the least-squares approximation still
explains most of the variance, with m = 2.22n
(R2 = 0.85) for the training data and m = 2.07n
(R2 = 0.96) for the test data. For both languages,
we thus see that the classifier-based parsers have
a lower mean number of transitions and less vari-
ance than the oracle parsers. And in both cases, the
expected number of transitions is only marginally
greater than the 2n of the strictly projective transi-
tion system Sp.
We have chosen to display results for Arabic
and Danish because they are the two extremes in
our sample. Arabic has the smallest variance and
the smallest linear coefficients, and Danish has the
largest variance and the largest coefficients. The
remaining three languages all lie somewhere in
the middle, with Czech being closer to Arabic and
Slovene closer to Danish. Together, the evidence
from all five languages strongly corroborates the
hypothesis that the expected running time for the
system Su is linear in sentence length for naturally
occurring data.
</bodyText>
<subsectionHeader confidence="0.999817">
4.2 Parsing Accuracy
</subsectionHeader>
<bodyText confidence="0.999972">
In order to assess the parsing accuracy that can
be achieved with the new transition system, we
trained a deterministic parser using the new tran-
sition system Su for each of the five languages.
For comparison, we also trained two parsers using
Sp, one that is strictly projective and one that uses
the pseudo-projective parsing technique to recover
non-projective dependencies in a post-processing
step (Nivre and Nilsson, 2005). We will refer to
the latter system as Spp. All systems use SVM
classifiers with a polynomial kernel to approxi-
mate the oracle function, with features and para-
meters taken from Nivre et al. (2006), which was
the best performing transition-based system in the
CoNLL-X shared task.2
Table 1 shows the labeled parsing accuracy of
the parsers measured in two ways: attachment
score (AS) is the percentage of tokens with the
correct head and dependency label; exact match
(EM) is the percentage of sentences with a com-
pletely correct labeled dependency tree. The score
in brackets is the attachment score for the (small)
subset of tokens that are connected to their head
by a non-projective arc in the gold standard parse.
For comparison, the table also includes results
for the two best performing systems in the origi-
nal CoNLL-X shared task, Malt-06 (Nivre et al.,
2006) and MST-06 (McDonald et al., 2006), as
well as the integrated system MSTMalt, which is
a graph-based parser guided by the predictions of
a transition-based parser and currently has the best
reported results on the CoNLL-X data sets (Nivre
and McDonald, 2008).
Looking first at the overall attachment score, we
see that Su gives a substantial improvement over
Sp (and outperforms Spp) for Czech and Slovene,
where the scores achieved are rivaled only by the
combo system MSTMalt. For these languages,
there is no statistical difference between Su and
MSTMalt, which are both significantly better than
all the other parsers, except Spp for Czech (Mc-
Nemar’s test, α = .05). This is accompanied
by an improvement on non-projective arcs, where
</bodyText>
<footnote confidence="0.868509">
2Complete information about experimental settings can
be found at http://stp.lingfil.uu.se/∼nivre/exp/.
</footnote>
<page confidence="0.996378">
357
</page>
<bodyText confidence="0.999941538461538">
Su outperforms all other systems for Czech and
is second only to the two MST parsers (MST-06
and MSTMalt) for Slovene. It is worth noting that
the percentage of non-projective arcs is higher for
Czech (1.9%) and Slovene (1.9%) than for any of
the other languages.
For the other three languages, Su has a drop
in overall attachment score compared to Sp, but
none of these differences is statistically signifi-
cant. In fact, the only significant differences in
attachment score here are the positive differences
between MSTMalt and all other systems for Arabic
and Danish, and the negative difference between
MST-06 and all other systems for Turkish. The
attachment scores for non-projective arcs are gen-
erally very low for these languages, except for the
two MST parsers on Danish, but Su performs at
least as well as Spp on Danish and Turkish. (The
results for Arabic are not very meaningful, given
that there are only eleven non-projective arcs in
the entire test set, of which the (pseudo-)projective
parsers found two and Su one, while MSTMalt and
MST-06 found none at all.)
Considering the exact match scores, finally, it is
very interesting to see that Su almost consistently
outperforms all other parsers, including the combo
system MSTMalt, and sometimes by a fairly wide
margin (Czech, Slovene). The difference is statis-
tically significant with respect to all other systems
except MSTMalt for Slovene, all except MSTMalt
and Spp for Czech, and with respect to MSTMalt
for Turkish. For Arabic and Danish, there are no
significant differences in the exact match scores.
We conclude that Su may increase the probabil-
ity of finding a completely correct analysis, which
is sometimes reflected also in the overall attach-
ment score, and we conjecture that the strength of
the positive effect is dependent on the frequency
of non-projective arcs in the language.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999990380952381">
Processing non-projective trees by swapping the
order of words has recently been proposed by both
Nivre (2008b) and Titov et al. (2009), but these
systems cannot handle unrestricted non-projective
trees. It is worth pointing out that, although the
system described in Nivre (2008b) uses four tran-
sitions bearing the same names as the transitions
of Su, the two systems are not equivalent. In par-
ticular, the system of Nivre (2008b) is sound but
not complete for the class of all dependency trees.
There are also affinities to the system of Attardi
(2006), which combines non-adjacent nodes on
the stack instead of swapping nodes and is equiva-
lent to a restricted version of our system, where no
more than two consecutive SWAP transitions are
permitted. This restriction preserves linear worst-
case complexity at the expense of completeness.
Finally, the algorithm first described by Covington
(2001) and used for data-driven parsing by Nivre
(2007), is complete but has quadratic complexity
even in the best case.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998325">
We have presented a novel transition system for
dependency parsing that can handle unrestricted
non-projective trees. The system reuses standard
techniques for building projective trees by com-
bining adjacent nodes (representing subtrees with
adjacent yields), but adds a simple mechanism for
swapping the order of nodes on the stack, which
gives a system that is sound and complete for the
set of all dependency trees over a given label set
but behaves exactly like the standard system for
the subset of projective trees. As a result, the time
complexity of deterministic parsing is O(n2) in
the worst case, which is rare, but O(n) in the best
case, which is common, and experimental results
on data from five languages support the conclusion
that expected running time is linear in the length
of the sentence. Experimental results also show
that parsing accuracy is competitive, especially
for languages like Czech and Slovene where non-
projective dependency structures are common, and
especially with respect to the exact match score,
where it has the best reported results for four out
of five languages. Finally, the simplicity of the
system makes it very easy to implement.
Future research will include an in-depth error
analysis to find out why the system works better
for some languages than others and why the exact
match score improves even when the attachment
score goes down. In addition, we want to explore
alternative oracle functions, which try to minimize
the number of swaps by allowing the stack to be
temporarily “unsorted”.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992965">
Thanks to Johan Hall and Jens Nilsson for help
with implementation and evaluation, and to Marco
Kuhlmann and three anonymous reviewers for
useful comments.
</bodyText>
<page confidence="0.998027">
358
</page>
<sectionHeader confidence="0.99834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999386875">
Giuseppe Attardi. 2006. Experiments with a multi-
language non-projective dependency parser. In Pro-
ceedings of CoNLL, pages 166–170.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149–164.
Michael A. Covington. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
39th Annual ACM Southeast Conference, pages 95–
102.
Carlos G´omez-Rodr´ıguez, David Weir, and John Car-
roll. 2009. Parsing mildly non-projective depen-
dency structures. In Proceedings of EACL, pages
291–299.
Keith Hall and Vaclav Nov´ak. 2005. Corrective mod-
eling for non-projective dependency parsing. In
Proceedings of IWPT, pages 42–52.
Jiri Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 608–615.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
Proceedings of the Shared Task of EMNLP-CoNLL,
pages 1134–1138.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL Main Conference Poster
Sessions, pages 507–514.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of EACL, pages 478–486.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81–88.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proceedings of IWPT, pages 122–131.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings ofACL, pages 91–
98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP, pages 523–530.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL, pages 216–220.
Peter Neuhaus and Norbert Br¨oker. 1997. The com-
plexity of recognition of linguistically adequate de-
pendency grammars. In Proceedings of ACL/EACL,
pages 337–343.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL, pages 950–958.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
ACL, pages 99–106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49–56.
Joakim Nivre, Johan Hall, Jens Nilsson, G¨ulsen
Eryi˘git, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of CoNLL, pages
221–225.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together (ACL), pages 50–57.
Joakim Nivre. 2006. Constraints on non-projective de-
pendency graphs. In Proceedings of EACL, pages
73–80.
Joakim Nivre. 2007. Incremental non-projective de-
pendency parsing. In Proceedings of NAACL HLT,
pages 396–403.
Joakim Nivre. 2008a. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34:513–553.
Joakim Nivre. 2008b. Sorting out dependency pars-
ing. In Proceedings of the 6th International Con-
ference on Natural Language Processing (GoTAL),
pages 16–27.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of IWPT, pages 144–155.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarization
for synchronous parsing of semantic and syntactic
dependencies. In Proceedings of IJCAI.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, pages 195–206.
</reference>
<page confidence="0.999076">
359
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.612374">
<title confidence="0.999465">Non-Projective Dependency Parsing in Expected Linear Time</title>
<author confidence="0.957369">Joakim Nivre</author>
<affiliation confidence="0.802852">Uppsala University, Department of Linguistics and Philology, SE-75126 Uppsala V¨axj¨o University, School of Mathematics and Systems Engineering, SE-35195 V¨axj¨o</affiliation>
<abstract confidence="0.9996629375">We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input. Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Experiments with a multilanguage non-projective dependency parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>166--170</pages>
<contexts>
<context position="1224" citStr="Attardi, 2006" startWordPosition="171" endWordPosition="172">mpirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. 1 Introduction Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007). A hallmark of many of these models is that they can be implemented very efficiently. Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b). However, one problem that still has not found a satis</context>
<context position="3019" citStr="Attardi (2006)" startWordPosition="442" endWordPosition="443">rees is NP hard, except under very restricted independence assumptions (Neuhaus and Br¨oker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). There is recent work on algorithms that can cope with important subsets of all nonprojective trees in polynomial time (Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2009), but the time complexity is at best O(n6), which can be problematic in practical applications. Even the best algorithms for deterministic parsing run in quadratic time, rather than linear (Nivre, 2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). Current approaches to data-drive</context>
<context position="10102" citStr="Attardi, 2006" startWordPosition="1780" endWordPosition="1781">one transition sequence C0,m for a sentence x and returns the parse defined by the terminal configuration cm, i.e., Gcm = (Vx, Acm). Assuming that the calls o(c) and t(c) can both be performed in constant time, the worst-case time complexity of a deterministic parser based on a transition system S is given by an upper bound on the length of transition sequences in S. When building practical parsing systems, the oracle can be approximated by a classifier trained on treebank data, a technique that has been used successfully in a number of systems (Yamada and Matsumoto, 2003; Nivre et al., 2004; Attardi, 2006). This is also the approach we will take in the experimental evaluation in Section 4. 3 Transitions for Dependency Parsing Having defined the set of configurations, including initial and terminal configurations, we will now focus on the transition set T required for dependency parsing. The total set of transitions that will be considered is given in Figure 2, but we will start in Section 3.1 with the subset Tp (p for projective) consisting of the first three. In Section 3.2, we will add the fourth transition (SWAP) to get the full transition set Tu (u for unrestricted). 3.1 Projective Dependen</context>
<context position="11539" citStr="Attardi, 2006" startWordPosition="2038" endWordPosition="2039">k by j alone. It is permissible as long as i is distinct from 0. 2. RIGHT-ARCl updates a configuration with i, j on top of the stack by adding (i, l, j) to A and replacing i, j on the stack by i alone. 3. SHIFT updates a configuration with i as the first node of the buffer by removing i from the buffer and pushing it onto the stack. The system Sp = (C, Tp, cs, Ct) is sound and complete for the set of projective dependency trees (over some label set L) and has been used, in slightly different variants, by a number of transition-based dependency parsers (Yamada and Matsumoto, 2003; Nivre, 2004; Attardi, 2006; 353 Transition Stack (E) Buffer (B) Added Arc [ROOT0] [A1, ... ,.9] SHIFT [ROOT0, A1] [hearing2, ... ,.9] SHIFT [ROOT0, A1, hearing2] [is3, ... , .9] LADET [ROOT0, hearing2] [is3,... , .9] (2, DET, 1) SHIFT [ROOT0, hearing2, is3] [scheduled4, ... , .9] SHIFT [ROOT0,... , is3, scheduled4] [on5,...,.9] SHIFT [ROOT0, ... , scheduled4, on5] [the6, ... , .9] SWAP [ROOT0,... , is3, on5] [scheduled4, ... , .9] SWAP [ROOT0, hearing2, on5] [is3, ... , .9] SHIFT [ROOT0,... , on5, is3] [scheduled4, ... , .9] SHIFT [ROOT0,... , is3, scheduled4] [the6, ... , .9] SHIFT [ROOT0, ... , scheduled4, the6] [iss</context>
<context position="29278" citStr="Attardi (2006)" startWordPosition="5150" endWordPosition="5151">ency of non-projective arcs in the language. 5 Related Work Processing non-projective trees by swapping the order of words has recently been proposed by both Nivre (2008b) and Titov et al. (2009), but these systems cannot handle unrestricted non-projective trees. It is worth pointing out that, although the system described in Nivre (2008b) uses four transitions bearing the same names as the transitions of Su, the two systems are not equivalent. In particular, the system of Nivre (2008b) is sound but not complete for the class of all dependency trees. There are also affinities to the system of Attardi (2006), which combines non-adjacent nodes on the stack instead of swapping nodes and is equivalent to a restricted version of our system, where no more than two consecutive SWAP transitions are permitted. This restriction preserves linear worstcase complexity at the expense of completeness. Finally, the algorithm first described by Covington (2001) and used for data-driven parsing by Nivre (2007), is complete but has quadratic complexity even in the best case. 6 Conclusion We have presented a novel transition system for dependency parsing that can handle unrestricted non-projective trees. The system</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings of CoNLL, pages 166–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="21113" citStr="Buchholz and Marsi, 2006" startWordPosition="3788" endWordPosition="3791">ange of languages, have shown that dependency trees tend to be projective and that most non-projective trees only contain a small number of discontinuities (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). This should mean that the expected number of swaps per sentence is small, and that the running time is linear on average for the range of inputs that occur in natural languages. This is a hypothesis that will be tested experimentally in the next section. 4 Experiments Our experiments are based on five data sets from the CoNLL-X shared task: Arabic, Czech, Danish, Slovene, and Turkish (Buchholz and Marsi, 2006). These languages have been selected because the data come from genuine dependency treebanks, whereas all the other data sets are based on some kind of conversion from another type of representation, which could potentially distort the distribution of different types of structures in the data. 4.1 Running Time In section 3.2, we hypothesized that the expected running time of a deterministic parser using the transition system 5,, would be linear, rather than quadratic. To test this hypothesis, we examine how the number of transitions varies as a function of sentence length. We call this the abs</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A fundamental algorithm for dependency parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual ACM Southeast Conference,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="29622" citStr="Covington (2001)" startWordPosition="5202" endWordPosition="5203">uses four transitions bearing the same names as the transitions of Su, the two systems are not equivalent. In particular, the system of Nivre (2008b) is sound but not complete for the class of all dependency trees. There are also affinities to the system of Attardi (2006), which combines non-adjacent nodes on the stack instead of swapping nodes and is equivalent to a restricted version of our system, where no more than two consecutive SWAP transitions are permitted. This restriction preserves linear worstcase complexity at the expense of completeness. Finally, the algorithm first described by Covington (2001) and used for data-driven parsing by Nivre (2007), is complete but has quadratic complexity even in the best case. 6 Conclusion We have presented a novel transition system for dependency parsing that can handle unrestricted non-projective trees. The system reuses standard techniques for building projective trees by combining adjacent nodes (representing subtrees with adjacent yields), but adds a simple mechanism for swapping the order of nodes on the stack, which gives a system that is sound and complete for the set of all dependency trees over a given label set but behaves exactly like the st</context>
</contexts>
<marker>Covington, 2001</marker>
<rawString>Michael A. Covington. 2001. A fundamental algorithm for dependency parsing. In Proceedings of the 39th Annual ACM Southeast Conference, pages 95– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>David Weir</author>
<author>John Carroll</author>
</authors>
<title>Parsing mildly non-projective dependency structures.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>291--299</pages>
<marker>G´omez-Rodr´ıguez, Weir, Carroll, 2009</marker>
<rawString>Carlos G´omez-Rodr´ıguez, David Weir, and John Carroll. 2009. Parsing mildly non-projective dependency structures. In Proceedings of EACL, pages 291–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Vaclav Nov´ak</author>
</authors>
<title>Corrective modeling for non-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>42--52</pages>
<marker>Hall, Nov´ak, 2005</marker>
<rawString>Keith Hall and Vaclav Nov´ak. 2005. Corrective modeling for non-projective dependency parsing. In Proceedings of IWPT, pages 42–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Havelka</author>
</authors>
<title>Beyond projectivity: Multilingual evaluation of constraints and measures on nonprojective structures.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>608--615</pages>
<contexts>
<context position="3585" citStr="Havelka, 2007" startWordPosition="523" endWordPosition="524">f non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). Current approaches to data-driven dependency parsing typically use one of two strategies to deal with non-projective trees (unless they ignore them completely). Either they employ a non-standard parsing algorithm that can combine non-adjacent substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover non351 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP P ✞ ☎ ROOT ADV ✞ ☎ ✞ ☎ NMOD PC DET ✞ ❄ ✞ SBJ ✞ ☎ ❄ VG ✞ ☎ ❄ ❄ ❄ ❄ ✞ ☎ DET ☎ ❄ ✞ ❄ ❄ ROOT0 A1 hearing2 </context>
<context position="20698" citStr="Havelka, 2007" startWordPosition="3720" endWordPosition="3721">) 2 , which means that 2n + n(n − 1) = n + n2 is an upper bound on the number of transitions in a terminating sequence. Hence, the worst-case complexity of the deterministic parser is O(n2). The running time of a deterministic transitionbased parser using the system 5,, is O(n) in the best case and O(n2) in the worst case. But what about the average case? Empirical studies, based on data from a wide range of languages, have shown that dependency trees tend to be projective and that most non-projective trees only contain a small number of discontinuities (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). This should mean that the expected number of swaps per sentence is small, and that the running time is linear on average for the range of inputs that occur in natural languages. This is a hypothesis that will be tested experimentally in the next section. 4 Experiments Our experiments are based on five data sets from the CoNLL-X shared task: Arabic, Czech, Danish, Slovene, and Turkish (Buchholz and Marsi, 2006). These languages have been selected because the data come from genuine dependency treebanks, whereas all the other data sets are based on some kind of conversion from another type of r</context>
</contexts>
<marker>Havelka, 2007</marker>
<rawString>Jiri Havelka. 2007. Beyond projectivity: Multilingual evaluation of constraints and measures on nonprojective structures. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 608–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Incremental dependency parsing using online learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the Shared Task of EMNLP-CoNLL,</booktitle>
<pages>1134--1138</pages>
<contexts>
<context position="1535" citStr="Johansson and Nugues, 2007" startWordPosition="217" endWordPosition="220">actic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007). A hallmark of many of these models is that they can be implemented very efficiently. Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b). However, one problem that still has not found a satisfactory solution in data-driven dependency parsing is the treatment of discontinuous syntactic constructions, usually modeled by non-projective dependency trees, as illustrated in Figure 1. In a projective dependency tree, the yield of every subtree is a contiguous substring of the sentence. This is not the ca</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Incremental dependency parsing using online learning. In Proceedings of the Shared Task of EMNLP-CoNLL, pages 1134–1138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Mildly non-projective dependency structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL Main Conference Poster Sessions,</booktitle>
<pages>507--514</pages>
<contexts>
<context position="3569" citStr="Kuhlmann and Nivre, 2006" startWordPosition="519" endWordPosition="522">s restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). Current approaches to data-driven dependency parsing typically use one of two strategies to deal with non-projective trees (unless they ignore them completely). Either they employ a non-standard parsing algorithm that can combine non-adjacent substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover non351 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP P ✞ ☎ ROOT ADV ✞ ☎ ✞ ☎ NMOD PC DET ✞ ❄ ✞ SBJ ✞ ☎ ❄ VG ✞ ☎ ❄ ❄ ❄ ❄ ✞ ☎ DET ☎ ❄ ✞ ❄ ❄ RO</context>
<context position="20682" citStr="Kuhlmann and Nivre, 2006" startWordPosition="3716" endWordPosition="3719">itions is bounded by n(n�1) 2 , which means that 2n + n(n − 1) = n + n2 is an upper bound on the number of transitions in a terminating sequence. Hence, the worst-case complexity of the deterministic parser is O(n2). The running time of a deterministic transitionbased parser using the system 5,, is O(n) in the best case and O(n2) in the worst case. But what about the average case? Empirical studies, based on data from a wide range of languages, have shown that dependency trees tend to be projective and that most non-projective trees only contain a small number of discontinuities (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). This should mean that the expected number of swaps per sentence is small, and that the running time is linear on average for the range of inputs that occur in natural languages. This is a hypothesis that will be tested experimentally in the next section. 4 Experiments Our experiments are based on five data sets from the CoNLL-X shared task: Arabic, Czech, Danish, Slovene, and Turkish (Buchholz and Marsi, 2006). These languages have been selected because the data come from genuine dependency treebanks, whereas all the other data sets are based on some kind of conversion from a</context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-projective dependency structures. In Proceedings of the COLING/ACL Main Conference Poster Sessions, pages 507–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
</authors>
<title>Treebank grammar techniques for non-projective dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>478--486</pages>
<contexts>
<context position="2702" citStr="Kuhlmann and Satta, 2009" startWordPosition="393" endWordPosition="396"> contiguous substring of the sentence. This is not the case for the tree in Figure 1, where the subtrees rooted at node 2 (hearing) and node 4 (scheduled) both have discontinuous yields. Allowing non-projective trees generally makes parsing computationally harder. Exact inference for parsing models that allow non-projective trees is NP hard, except under very restricted independence assumptions (Neuhaus and Br¨oker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). There is recent work on algorithms that can cope with important subsets of all nonprojective trees in polynomial time (Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2009), but the time complexity is at best O(n6), which can be problematic in practical applications. Even the best algorithms for deterministic parsing run in quadratic time, rather than linear (Nivre, 2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other</context>
</contexts>
<marker>Kuhlmann, Satta, 2009</marker>
<rawString>Marco Kuhlmann and Giorgio Satta. 2009. Treebank grammar techniques for non-projective dependency parsing. In Proceedings of EACL, pages 478–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="2530" citStr="McDonald and Pereira, 2006" startWordPosition="365" endWordPosition="368">uous syntactic constructions, usually modeled by non-projective dependency trees, as illustrated in Figure 1. In a projective dependency tree, the yield of every subtree is a contiguous substring of the sentence. This is not the case for the tree in Figure 1, where the subtrees rooted at node 2 (hearing) and node 4 (scheduled) both have discontinuous yields. Allowing non-projective trees generally makes parsing computationally harder. Exact inference for parsing models that allow non-projective trees is NP hard, except under very restricted independence assumptions (Neuhaus and Br¨oker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). There is recent work on algorithms that can cope with important subsets of all nonprojective trees in polynomial time (Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2009), but the time complexity is at best O(n6), which can be problematic in practical applications. Even the best algorithms for deterministic parsing run in quadratic time, rather than linear (Nivre, 2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because </context>
<context position="4456" citStr="McDonald and Pereira, 2006" startWordPosition="673" endWordPosition="676"> substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover non351 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP P ✞ ☎ ROOT ADV ✞ ☎ ✞ ☎ NMOD PC DET ✞ ❄ ✞ SBJ ✞ ☎ ❄ VG ✞ ☎ ❄ ❄ ❄ ❄ ✞ ☎ DET ☎ ❄ ✞ ❄ ❄ ROOT0 A1 hearing2 isg scheduled4 ons the6 issue? today$ .9 Figure 1: Dependency tree for an English sentence (non-projective). projective dependencies by post-processing the output of a strictly projective parser (Nivre and Nilsson, 2005; Hall and Nov´ak, 2005; McDonald and Pereira, 2006). In this paper, we will adopt a different strategy, suggested in recent work by Nivre (2008b) and Titov et al. (2009), and propose an algorithm that only combines adjacent substructures but derives non-projective trees by reordering the input words. The rest of the paper is structured as follows. In Section 2, we define the formal representations needed and introduce the framework of transitionbased dependency parsing. In Section 3, we first define a minimal transition system and explain how it can be used to perform projective dependency parsing in linear time; we then extend the system with</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Giorgio Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>122--131</pages>
<contexts>
<context position="2557" citStr="McDonald and Satta, 2007" startWordPosition="369" endWordPosition="372">, usually modeled by non-projective dependency trees, as illustrated in Figure 1. In a projective dependency tree, the yield of every subtree is a contiguous substring of the sentence. This is not the case for the tree in Figure 1, where the subtrees rooted at node 2 (hearing) and node 4 (scheduled) both have discontinuous yields. Allowing non-projective trees generally makes parsing computationally harder. Exact inference for parsing models that allow non-projective trees is NP hard, except under very restricted independence assumptions (Neuhaus and Br¨oker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). There is recent work on algorithms that can cope with important subsets of all nonprojective trees in polynomial time (Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2009), but the time complexity is at best O(n6), which can be problematic in practical applications. Even the best algorithms for deterministic parsing run in quadratic time, rather than linear (Nivre, 2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model r</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>Ryan McDonald and Giorgio Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proceedings of IWPT, pages 122–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1208" citStr="McDonald et al., 2005" startWordPosition="167" endWordPosition="170">in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. 1 Introduction Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007). A hallmark of many of these models is that they can be implemented very efficiently. Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b). However, one problem that still has n</context>
<context position="3866" citStr="McDonald et al., 2005" startWordPosition="559" endWordPosition="562">n has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). Current approaches to data-driven dependency parsing typically use one of two strategies to deal with non-projective trees (unless they ignore them completely). Either they employ a non-standard parsing algorithm that can combine non-adjacent substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover non351 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP P ✞ ☎ ROOT ADV ✞ ☎ ✞ ☎ NMOD PC DET ✞ ❄ ✞ SBJ ✞ ☎ ❄ VG ✞ ☎ ❄ ❄ ❄ ❄ ✞ ☎ DET ☎ ❄ ✞ ❄ ❄ ROOT0 A1 hearing2 isg scheduled4 ons the6 issue? today$ .9 Figure 1: Dependency tree for an English sentence (non-projective). projective dependencies by post-processing the output of a strictly projective parser (Nivre and Nilsson, 2005; Hall and Nov´ak, 2005; McDonald and Pereira, 2006). In this </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings ofACL, pages 91– 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT/EMNLP, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>216--220</pages>
<contexts>
<context position="26034" citStr="McDonald et al., 2006" startWordPosition="4622" endWordPosition="4625"> Table 1 shows the labeled parsing accuracy of the parsers measured in two ways: attachment score (AS) is the percentage of tokens with the correct head and dependency label; exact match (EM) is the percentage of sentences with a completely correct labeled dependency tree. The score in brackets is the attachment score for the (small) subset of tokens that are connected to their head by a non-projective arc in the gold standard parse. For comparison, the table also includes results for the two best performing systems in the original CoNLL-X shared task, Malt-06 (Nivre et al., 2006) and MST-06 (McDonald et al., 2006), as well as the integrated system MSTMalt, which is a graph-based parser guided by the predictions of a transition-based parser and currently has the best reported results on the CoNLL-X data sets (Nivre and McDonald, 2008). Looking first at the overall attachment score, we see that Su gives a substantial improvement over Sp (and outperforms Spp) for Czech and Slovene, where the scores achieved are rivaled only by the combo system MSTMalt. For these languages, there is no statistical difference between Su and MSTMalt, which are both significantly better than all the other parsers, except Spp </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of CoNLL, pages 216–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Neuhaus</author>
<author>Norbert Br¨oker</author>
</authors>
<title>The complexity of recognition of linguistically adequate dependency grammars.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL,</booktitle>
<pages>337--343</pages>
<marker>Neuhaus, Br¨oker, 1997</marker>
<rawString>Peter Neuhaus and Norbert Br¨oker. 1997. The complexity of recognition of linguistically adequate dependency grammars. In Proceedings of ACL/EACL, pages 337–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>950--958</pages>
<contexts>
<context position="26258" citStr="Nivre and McDonald, 2008" startWordPosition="4658" endWordPosition="4661">s with a completely correct labeled dependency tree. The score in brackets is the attachment score for the (small) subset of tokens that are connected to their head by a non-projective arc in the gold standard parse. For comparison, the table also includes results for the two best performing systems in the original CoNLL-X shared task, Malt-06 (Nivre et al., 2006) and MST-06 (McDonald et al., 2006), as well as the integrated system MSTMalt, which is a graph-based parser guided by the predictions of a transition-based parser and currently has the best reported results on the CoNLL-X data sets (Nivre and McDonald, 2008). Looking first at the overall attachment score, we see that Su gives a substantial improvement over Sp (and outperforms Spp) for Czech and Slovene, where the scores achieved are rivaled only by the combo system MSTMalt. For these languages, there is no statistical difference between Su and MSTMalt, which are both significantly better than all the other parsers, except Spp for Czech (McNemar’s test, α = .05). This is accompanied by an improvement on non-projective arcs, where 2Complete information about experimental settings can be found at http://stp.lingfil.uu.se/∼nivre/exp/. 357 Su outperfo</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL, pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>99--106</pages>
<contexts>
<context position="4404" citStr="Nivre and Nilsson, 2005" startWordPosition="665" endWordPosition="668"> parsing algorithm that can combine non-adjacent substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover non351 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP P ✞ ☎ ROOT ADV ✞ ☎ ✞ ☎ NMOD PC DET ✞ ❄ ✞ SBJ ✞ ☎ ❄ VG ✞ ☎ ❄ ❄ ❄ ❄ ✞ ☎ DET ☎ ❄ ✞ ❄ ❄ ROOT0 A1 hearing2 isg scheduled4 ons the6 issue? today$ .9 Figure 1: Dependency tree for an English sentence (non-projective). projective dependencies by post-processing the output of a strictly projective parser (Nivre and Nilsson, 2005; Hall and Nov´ak, 2005; McDonald and Pereira, 2006). In this paper, we will adopt a different strategy, suggested in recent work by Nivre (2008b) and Titov et al. (2009), and propose an algorithm that only combines adjacent substructures but derives non-projective trees by reordering the input words. The rest of the paper is structured as follows. In Section 2, we define the formal representations needed and introduce the framework of transitionbased dependency parsing. In Section 3, we first define a minimal transition system and explain how it can be used to perform projective dependency pa</context>
<context position="25131" citStr="Nivre and Nilsson, 2005" startWordPosition="4470" endWordPosition="4473">dence from all five languages strongly corroborates the hypothesis that the expected running time for the system Su is linear in sentence length for naturally occurring data. 4.2 Parsing Accuracy In order to assess the parsing accuracy that can be achieved with the new transition system, we trained a deterministic parser using the new transition system Su for each of the five languages. For comparison, we also trained two parsers using Sp, one that is strictly projective and one that uses the pseudo-projective parsing technique to recover non-projective dependencies in a post-processing step (Nivre and Nilsson, 2005). We will refer to the latter system as Spp. All systems use SVM classifiers with a polynomial kernel to approximate the oracle function, with features and parameters taken from Nivre et al. (2006), which was the best performing transition-based system in the CoNLL-X shared task.2 Table 1 shows the labeled parsing accuracy of the parsers measured in two ways: attachment score (AS) is the percentage of tokens with the correct head and dependency label; exact match (EM) is the percentage of sentences with a completely correct labeled dependency tree. The score in brackets is the attachment score</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In Proceedings of ACL, pages 99–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="1185" citStr="Nivre et al., 2004" startWordPosition="163" endWordPosition="166">linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. 1 Introduction Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007). A hallmark of many of these models is that they can be implemented very efficiently. Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b). However, one p</context>
<context position="10086" citStr="Nivre et al., 2004" startWordPosition="1776" endWordPosition="1779"> constructs exactly one transition sequence C0,m for a sentence x and returns the parse defined by the terminal configuration cm, i.e., Gcm = (Vx, Acm). Assuming that the calls o(c) and t(c) can both be performed in constant time, the worst-case time complexity of a deterministic parser based on a transition system S is given by an upper bound on the length of transition sequences in S. When building practical parsing systems, the oracle can be approximated by a classifier trained on treebank data, a technique that has been used successfully in a number of systems (Yamada and Matsumoto, 2003; Nivre et al., 2004; Attardi, 2006). This is also the approach we will take in the experimental evaluation in Section 4. 3 Transitions for Dependency Parsing Having defined the set of configurations, including initial and terminal configurations, we will now focus on the transition set T required for dependency parsing. The total set of transitions that will be considered is given in Figure 2, but we will start in Section 3.1 with the subset Tp (p for projective) consisting of the first three. In Section 3.2, we will add the fourth transition (SWAP) to get the full transition set Tu (u for unrestricted). 3.1 Pro</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>G¨ulsen Eryi˘git</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>221--225</pages>
<marker>Nivre, Hall, Nilsson, Eryi˘git, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, G¨ulsen Eryi˘git, and Svetoslav Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of CoNLL, pages 221–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL),</booktitle>
<pages>50--57</pages>
<contexts>
<context position="11524" citStr="Nivre, 2004" startWordPosition="2036" endWordPosition="2037">j on the stack by j alone. It is permissible as long as i is distinct from 0. 2. RIGHT-ARCl updates a configuration with i, j on top of the stack by adding (i, l, j) to A and replacing i, j on the stack by i alone. 3. SHIFT updates a configuration with i as the first node of the buffer by removing i from the buffer and pushing it onto the stack. The system Sp = (C, Tp, cs, Ct) is sound and complete for the set of projective dependency trees (over some label set L) and has been used, in slightly different variants, by a number of transition-based dependency parsers (Yamada and Matsumoto, 2003; Nivre, 2004; Attardi, 2006; 353 Transition Stack (E) Buffer (B) Added Arc [ROOT0] [A1, ... ,.9] SHIFT [ROOT0, A1] [hearing2, ... ,.9] SHIFT [ROOT0, A1, hearing2] [is3, ... , .9] LADET [ROOT0, hearing2] [is3,... , .9] (2, DET, 1) SHIFT [ROOT0, hearing2, is3] [scheduled4, ... , .9] SHIFT [ROOT0,... , is3, scheduled4] [on5,...,.9] SHIFT [ROOT0, ... , scheduled4, on5] [the6, ... , .9] SWAP [ROOT0,... , is3, on5] [scheduled4, ... , .9] SWAP [ROOT0, hearing2, on5] [is3, ... , .9] SHIFT [ROOT0,... , on5, is3] [scheduled4, ... , .9] SHIFT [ROOT0,... , is3, scheduled4] [the6, ... , .9] SHIFT [ROOT0, ... , schedul</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL), pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Constraints on non-projective dependency graphs.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="3543" citStr="Nivre, 2006" startWordPosition="517" endWordPosition="518">2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). Current approaches to data-driven dependency parsing typically use one of two strategies to deal with non-projective trees (unless they ignore them completely). Either they employ a non-standard parsing algorithm that can combine non-adjacent substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover non351 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP P ✞ ☎ ROOT ADV ✞ ☎ ✞ ☎ NMOD PC DET ✞ ❄ ✞ SBJ ✞ ☎ ❄ VG ✞ ☎ ❄ </context>
<context position="20656" citStr="Nivre, 2006" startWordPosition="3714" endWordPosition="3715">of SWAP transitions is bounded by n(n�1) 2 , which means that 2n + n(n − 1) = n + n2 is an upper bound on the number of transitions in a terminating sequence. Hence, the worst-case complexity of the deterministic parser is O(n2). The running time of a deterministic transitionbased parser using the system 5,, is O(n) in the best case and O(n2) in the worst case. But what about the average case? Empirical studies, based on data from a wide range of languages, have shown that dependency trees tend to be projective and that most non-projective trees only contain a small number of discontinuities (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). This should mean that the expected number of swaps per sentence is small, and that the running time is linear on average for the range of inputs that occur in natural languages. This is a hypothesis that will be tested experimentally in the next section. 4 Experiments Our experiments are based on five data sets from the CoNLL-X shared task: Arabic, Czech, Danish, Slovene, and Turkish (Buchholz and Marsi, 2006). These languages have been selected because the data come from genuine dependency treebanks, whereas all the other data sets are based on some</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Constraints on non-projective dependency graphs. In Proceedings of EACL, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incremental non-projective dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT,</booktitle>
<pages>396--403</pages>
<contexts>
<context position="3036" citStr="Nivre (2007)" startWordPosition="445" endWordPosition="446">cept under very restricted independence assumptions (Neuhaus and Br¨oker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). There is recent work on algorithms that can cope with important subsets of all nonprojective trees in polynomial time (Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2009), but the time complexity is at best O(n6), which can be problematic in practical applications. Even the best algorithms for deterministic parsing run in quadratic time, rather than linear (Nivre, 2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). Current approaches to data-driven dependency pars</context>
<context position="29671" citStr="Nivre (2007)" startWordPosition="5210" endWordPosition="5211">ransitions of Su, the two systems are not equivalent. In particular, the system of Nivre (2008b) is sound but not complete for the class of all dependency trees. There are also affinities to the system of Attardi (2006), which combines non-adjacent nodes on the stack instead of swapping nodes and is equivalent to a restricted version of our system, where no more than two consecutive SWAP transitions are permitted. This restriction preserves linear worstcase complexity at the expense of completeness. Finally, the algorithm first described by Covington (2001) and used for data-driven parsing by Nivre (2007), is complete but has quadratic complexity even in the best case. 6 Conclusion We have presented a novel transition system for dependency parsing that can handle unrestricted non-projective trees. The system reuses standard techniques for building projective trees by combining adjacent nodes (representing subtrees with adjacent yields), but adds a simple mechanism for swapping the order of nodes on the stack, which gives a system that is sound and complete for the set of all dependency trees over a given label set but behaves exactly like the standard system for the subset of projective trees.</context>
</contexts>
<marker>Nivre, 2007</marker>
<rawString>Joakim Nivre. 2007. Incremental non-projective dependency parsing. In Proceedings of NAACL HLT, pages 396–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--513</pages>
<contexts>
<context position="2936" citStr="Nivre, 2008" startWordPosition="430" endWordPosition="431">ationally harder. Exact inference for parsing models that allow non-projective trees is NP hard, except under very restricted independence assumptions (Neuhaus and Br¨oker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). There is recent work on algorithms that can cope with important subsets of all nonprojective trees in polynomial time (Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2009), but the time complexity is at best O(n6), which can be problematic in practical applications. Even the best algorithms for deterministic parsing run in quadratic time, rather than linear (Nivre, 2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivr</context>
<context position="4548" citStr="Nivre (2008" startWordPosition="691" endWordPosition="692">gs of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP P ✞ ☎ ROOT ADV ✞ ☎ ✞ ☎ NMOD PC DET ✞ ❄ ✞ SBJ ✞ ☎ ❄ VG ✞ ☎ ❄ ❄ ❄ ❄ ✞ ☎ DET ☎ ❄ ✞ ❄ ❄ ROOT0 A1 hearing2 isg scheduled4 ons the6 issue? today$ .9 Figure 1: Dependency tree for an English sentence (non-projective). projective dependencies by post-processing the output of a strictly projective parser (Nivre and Nilsson, 2005; Hall and Nov´ak, 2005; McDonald and Pereira, 2006). In this paper, we will adopt a different strategy, suggested in recent work by Nivre (2008b) and Titov et al. (2009), and propose an algorithm that only combines adjacent substructures but derives non-projective trees by reordering the input words. The rest of the paper is structured as follows. In Section 2, we define the formal representations needed and introduce the framework of transitionbased dependency parsing. In Section 3, we first define a minimal transition system and explain how it can be used to perform projective dependency parsing in linear time; we then extend the system with a single transition for swapping the order of words in the input and demonstrate that the e</context>
<context position="6307" citStr="Nivre (2008" startWordPosition="1023" endWordPosition="1024">ence x = w1, ... , wn is a directed graph G = (Vx, A), where 1. Vx = 10, 1, ... , n} is a set of nodes, 2. A C Vx x L x Vx is a set of labeled arcs. The set Vx of nodes is the set of positive integers up to and including n, each corresponding to the linear position of a word in the sentence, plus an extra artificial root node 0. The set A of arcs is a set of triples (i, l, j), where i and j are nodes and l is a label. For a dependency graph G = (Vx, A) to be well-formed, we in addition require that it is a tree rooted at the node 0, as illustrated in Figure 1. 2.2 Transition Systems Following Nivre (2008a), we define a transition system for dependency parsing as a quadruple 5 = (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C —* C, 3. cs is an initialization function, mapping a sentence x = w1, ... , wn to a configuration c E C, 4. Ct C C is a set of terminal configurations. In this paper, we take the set C of configurations to be the set of all triples c = (E, B, A) such that E and B are disjoint sublists of the nodes Vx of some sentence x, and A is a set of dependency arcs over Vx (and some label set L); we take</context>
<context position="13158" citStr="Nivre, 2008" startWordPosition="2317" endWordPosition="2318">. , .9] (7, DET, 6) RAPC [ROOT0, hearing2, on5] [is3, ... , .9] (5, PC, 7) RANMOD [ROOT0, hearing2] [is3, ... , .9] (2, NMOD, 5) SHIFT [ROOT0, ... , hearing2, is3] [scheduled4, ... , .9] LASBJ [ROOT0, is3] [scheduled4, ... , .9] (3, SBJ, 2) SHIFT [ROOT0, is3, scheduled4] [today$, .9] SHIFT [ROOT0, ... , scheduled4, today$] [.9] RAADV [ROOT0, is3, scheduled4] [.9] (4, ADV, 8) RAVG [ROOT0, is3] [.9] (3, VG, 4) SHIFT [ROOT0, is3, .9] [ ] RAP [ROOT0, is3] [ ] (3, P, 9) RAROOT [ROOT0] [] (0, ROOT, 3) Figure 3: Transition sequence for parsing the sentence in Figure 1 (LA = LEFT-ARC, RA = REFT-ARC). Nivre, 2008a). For proofs of soundness and completeness, see Nivre (2008a). As noted in section 2, the worst-case time complexity of a deterministic transition-based parser is given by an upper bound on the length of transition sequences. In 5p, the number of transitions for a sentence x = w1, ... , wn is always exactly 2n, since a terminal configuration can only be reached after n SHIFT transitions (moving nodes 1, ... , n from B to E) and n applications of LEFT-ARCl or RIGHT-ARCl (removing the same nodes from E). Hence, the complexity of deterministic parsing is O(n) in the worst case (as well as in th</context>
<context position="28833" citStr="Nivre (2008" startWordPosition="5075" endWordPosition="5076">stems except MSTMalt for Slovene, all except MSTMalt and Spp for Czech, and with respect to MSTMalt for Turkish. For Arabic and Danish, there are no significant differences in the exact match scores. We conclude that Su may increase the probability of finding a completely correct analysis, which is sometimes reflected also in the overall attachment score, and we conjecture that the strength of the positive effect is dependent on the frequency of non-projective arcs in the language. 5 Related Work Processing non-projective trees by swapping the order of words has recently been proposed by both Nivre (2008b) and Titov et al. (2009), but these systems cannot handle unrestricted non-projective trees. It is worth pointing out that, although the system described in Nivre (2008b) uses four transitions bearing the same names as the transitions of Su, the two systems are not equivalent. In particular, the system of Nivre (2008b) is sound but not complete for the class of all dependency trees. There are also affinities to the system of Attardi (2006), which combines non-adjacent nodes on the stack instead of swapping nodes and is equivalent to a restricted version of our system, where no more than two </context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008a. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34:513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Sorting out dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Natural Language Processing (GoTAL),</booktitle>
<pages>16--27</pages>
<contexts>
<context position="2936" citStr="Nivre, 2008" startWordPosition="430" endWordPosition="431">ationally harder. Exact inference for parsing models that allow non-projective trees is NP hard, except under very restricted independence assumptions (Neuhaus and Br¨oker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). There is recent work on algorithms that can cope with important subsets of all nonprojective trees in polynomial time (Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2009), but the time complexity is at best O(n6), which can be problematic in practical applications. Even the best algorithms for deterministic parsing run in quadratic time, rather than linear (Nivre, 2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivr</context>
<context position="4548" citStr="Nivre (2008" startWordPosition="691" endWordPosition="692">gs of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP P ✞ ☎ ROOT ADV ✞ ☎ ✞ ☎ NMOD PC DET ✞ ❄ ✞ SBJ ✞ ☎ ❄ VG ✞ ☎ ❄ ❄ ❄ ❄ ✞ ☎ DET ☎ ❄ ✞ ❄ ❄ ROOT0 A1 hearing2 isg scheduled4 ons the6 issue? today$ .9 Figure 1: Dependency tree for an English sentence (non-projective). projective dependencies by post-processing the output of a strictly projective parser (Nivre and Nilsson, 2005; Hall and Nov´ak, 2005; McDonald and Pereira, 2006). In this paper, we will adopt a different strategy, suggested in recent work by Nivre (2008b) and Titov et al. (2009), and propose an algorithm that only combines adjacent substructures but derives non-projective trees by reordering the input words. The rest of the paper is structured as follows. In Section 2, we define the formal representations needed and introduce the framework of transitionbased dependency parsing. In Section 3, we first define a minimal transition system and explain how it can be used to perform projective dependency parsing in linear time; we then extend the system with a single transition for swapping the order of words in the input and demonstrate that the e</context>
<context position="6307" citStr="Nivre (2008" startWordPosition="1023" endWordPosition="1024">ence x = w1, ... , wn is a directed graph G = (Vx, A), where 1. Vx = 10, 1, ... , n} is a set of nodes, 2. A C Vx x L x Vx is a set of labeled arcs. The set Vx of nodes is the set of positive integers up to and including n, each corresponding to the linear position of a word in the sentence, plus an extra artificial root node 0. The set A of arcs is a set of triples (i, l, j), where i and j are nodes and l is a label. For a dependency graph G = (Vx, A) to be well-formed, we in addition require that it is a tree rooted at the node 0, as illustrated in Figure 1. 2.2 Transition Systems Following Nivre (2008a), we define a transition system for dependency parsing as a quadruple 5 = (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C —* C, 3. cs is an initialization function, mapping a sentence x = w1, ... , wn to a configuration c E C, 4. Ct C C is a set of terminal configurations. In this paper, we take the set C of configurations to be the set of all triples c = (E, B, A) such that E and B are disjoint sublists of the nodes Vx of some sentence x, and A is a set of dependency arcs over Vx (and some label set L); we take</context>
<context position="13158" citStr="Nivre, 2008" startWordPosition="2317" endWordPosition="2318">. , .9] (7, DET, 6) RAPC [ROOT0, hearing2, on5] [is3, ... , .9] (5, PC, 7) RANMOD [ROOT0, hearing2] [is3, ... , .9] (2, NMOD, 5) SHIFT [ROOT0, ... , hearing2, is3] [scheduled4, ... , .9] LASBJ [ROOT0, is3] [scheduled4, ... , .9] (3, SBJ, 2) SHIFT [ROOT0, is3, scheduled4] [today$, .9] SHIFT [ROOT0, ... , scheduled4, today$] [.9] RAADV [ROOT0, is3, scheduled4] [.9] (4, ADV, 8) RAVG [ROOT0, is3] [.9] (3, VG, 4) SHIFT [ROOT0, is3, .9] [ ] RAP [ROOT0, is3] [ ] (3, P, 9) RAROOT [ROOT0] [] (0, ROOT, 3) Figure 3: Transition sequence for parsing the sentence in Figure 1 (LA = LEFT-ARC, RA = REFT-ARC). Nivre, 2008a). For proofs of soundness and completeness, see Nivre (2008a). As noted in section 2, the worst-case time complexity of a deterministic transition-based parser is given by an upper bound on the length of transition sequences. In 5p, the number of transitions for a sentence x = w1, ... , wn is always exactly 2n, since a terminal configuration can only be reached after n SHIFT transitions (moving nodes 1, ... , n from B to E) and n applications of LEFT-ARCl or RIGHT-ARCl (removing the same nodes from E). Hence, the complexity of deterministic parsing is O(n) in the worst case (as well as in th</context>
<context position="28833" citStr="Nivre (2008" startWordPosition="5075" endWordPosition="5076">stems except MSTMalt for Slovene, all except MSTMalt and Spp for Czech, and with respect to MSTMalt for Turkish. For Arabic and Danish, there are no significant differences in the exact match scores. We conclude that Su may increase the probability of finding a completely correct analysis, which is sometimes reflected also in the overall attachment score, and we conjecture that the strength of the positive effect is dependent on the frequency of non-projective arcs in the language. 5 Related Work Processing non-projective trees by swapping the order of words has recently been proposed by both Nivre (2008b) and Titov et al. (2009), but these systems cannot handle unrestricted non-projective trees. It is worth pointing out that, although the system described in Nivre (2008b) uses four transitions bearing the same names as the transitions of Su, the two systems are not equivalent. In particular, the system of Nivre (2008b) is sound but not complete for the class of all dependency trees. There are also affinities to the system of Attardi (2006), which combines non-adjacent nodes on the stack instead of swapping nodes and is equivalent to a restricted version of our system, where no more than two </context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008b. Sorting out dependency parsing. In Proceedings of the 6th International Conference on Natural Language Processing (GoTAL), pages 16–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>A latent variable model for generative dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>144--155</pages>
<contexts>
<context position="1252" citStr="Titov and Henderson, 2007" startWordPosition="173" endWordPosition="176">tes based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. 1 Introduction Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007). A hallmark of many of these models is that they can be implemented very efficiently. Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b). However, one problem that still has not found a satisfactory solution in data-dri</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007. A latent variable model for generative dependency parsing. In Proceedings of IWPT, pages 144–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Online graph planarization for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="4574" citStr="Titov et al. (2009)" startWordPosition="694" endWordPosition="697">ual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP P ✞ ☎ ROOT ADV ✞ ☎ ✞ ☎ NMOD PC DET ✞ ❄ ✞ SBJ ✞ ☎ ❄ VG ✞ ☎ ❄ ❄ ❄ ❄ ✞ ☎ DET ☎ ❄ ✞ ❄ ❄ ROOT0 A1 hearing2 isg scheduled4 ons the6 issue? today$ .9 Figure 1: Dependency tree for an English sentence (non-projective). projective dependencies by post-processing the output of a strictly projective parser (Nivre and Nilsson, 2005; Hall and Nov´ak, 2005; McDonald and Pereira, 2006). In this paper, we will adopt a different strategy, suggested in recent work by Nivre (2008b) and Titov et al. (2009), and propose an algorithm that only combines adjacent substructures but derives non-projective trees by reordering the input words. The rest of the paper is structured as follows. In Section 2, we define the formal representations needed and introduce the framework of transitionbased dependency parsing. In Section 3, we first define a minimal transition system and explain how it can be used to perform projective dependency parsing in linear time; we then extend the system with a single transition for swapping the order of words in the input and demonstrate that the extended system can be used</context>
<context position="28859" citStr="Titov et al. (2009)" startWordPosition="5078" endWordPosition="5081">lt for Slovene, all except MSTMalt and Spp for Czech, and with respect to MSTMalt for Turkish. For Arabic and Danish, there are no significant differences in the exact match scores. We conclude that Su may increase the probability of finding a completely correct analysis, which is sometimes reflected also in the overall attachment score, and we conjecture that the strength of the positive effect is dependent on the frequency of non-projective arcs in the language. 5 Related Work Processing non-projective trees by swapping the order of words has recently been proposed by both Nivre (2008b) and Titov et al. (2009), but these systems cannot handle unrestricted non-projective trees. It is worth pointing out that, although the system described in Nivre (2008b) uses four transitions bearing the same names as the transitions of Su, the two systems are not equivalent. In particular, the system of Nivre (2008b) is sound but not complete for the class of all dependency trees. There are also affinities to the system of Attardi (2006), which combines non-adjacent nodes on the stack instead of swapping nodes and is equivalent to a restricted version of our system, where no more than two consecutive SWAP transitio</context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Ivan Titov, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online graph planarization for synchronous parsing of semantic and syntactic dependencies. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="1165" citStr="Yamada and Matsumoto, 2003" startWordPosition="159" endWordPosition="162"> deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. 1 Introduction Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007). A hallmark of many of these models is that they can be implemented very efficiently. Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 20</context>
<context position="10066" citStr="Yamada and Matsumoto, 2003" startWordPosition="1772" endWordPosition="1775">r every sentence, the parser constructs exactly one transition sequence C0,m for a sentence x and returns the parse defined by the terminal configuration cm, i.e., Gcm = (Vx, Acm). Assuming that the calls o(c) and t(c) can both be performed in constant time, the worst-case time complexity of a deterministic parser based on a transition system S is given by an upper bound on the length of transition sequences in S. When building practical parsing systems, the oracle can be approximated by a classifier trained on treebank data, a technique that has been used successfully in a number of systems (Yamada and Matsumoto, 2003; Nivre et al., 2004; Attardi, 2006). This is also the approach we will take in the experimental evaluation in Section 4. 3 Transitions for Dependency Parsing Having defined the set of configurations, including initial and terminal configurations, we will now focus on the transition set T required for dependency parsing. The total set of transitions that will be considered is given in Figure 2, but we will start in Section 3.1 with the subset Tp (p for projective) consisting of the first three. In Section 3.2, we will add the fourth transition (SWAP) to get the full transition set Tu (u for un</context>
<context position="11511" citStr="Yamada and Matsumoto, 2003" startWordPosition="2032" endWordPosition="2035">l, i) to A and replacing i, j on the stack by j alone. It is permissible as long as i is distinct from 0. 2. RIGHT-ARCl updates a configuration with i, j on top of the stack by adding (i, l, j) to A and replacing i, j on the stack by i alone. 3. SHIFT updates a configuration with i as the first node of the buffer by removing i from the buffer and pushing it onto the stack. The system Sp = (C, Tp, cs, Ct) is sound and complete for the set of projective dependency trees (over some label set L) and has been used, in slightly different variants, by a number of transition-based dependency parsers (Yamada and Matsumoto, 2003; Nivre, 2004; Attardi, 2006; 353 Transition Stack (E) Buffer (B) Added Arc [ROOT0] [A1, ... ,.9] SHIFT [ROOT0, A1] [hearing2, ... ,.9] SHIFT [ROOT0, A1, hearing2] [is3, ... , .9] LADET [ROOT0, hearing2] [is3,... , .9] (2, DET, 1) SHIFT [ROOT0, hearing2, is3] [scheduled4, ... , .9] SHIFT [ROOT0,... , is3, scheduled4] [on5,...,.9] SHIFT [ROOT0, ... , scheduled4, on5] [the6, ... , .9] SWAP [ROOT0,... , is3, on5] [scheduled4, ... , .9] SWAP [ROOT0, hearing2, on5] [is3, ... , .9] SHIFT [ROOT0,... , on5, is3] [scheduled4, ... , .9] SHIFT [ROOT0,... , is3, scheduled4] [the6, ... , .9] SHIFT [ROOT0, </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT, pages 195–206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>