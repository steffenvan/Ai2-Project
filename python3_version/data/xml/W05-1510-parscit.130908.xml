<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.951181">
Probabilistic models for disambiguation of an HPSG-based chart generator
</title>
<note confidence="0.76748725">
Yusuke Miyao
CREST, JST
Honcho 4-1-8, Kawaguchi-shi
Saitama 332-0012, Japan
</note>
<author confidence="0.98628">
Hiroko Nakanishi
</author>
<affiliation confidence="0.9958345">
Department of Computer Science
University of Tokyo
</affiliation>
<address confidence="0.475397">
Hongo 7-3-1, Bunkyo-ku
Tokyo 113-0033, Japan
</address>
<author confidence="0.96674">
Jun’ichi Tsujii
</author>
<affiliation confidence="0.99698">
School of Informatics
University of Manchester
</affiliation>
<address confidence="0.9559965">
POBox 88, Sackville St
MANCHESTER M60 1QD, UK
</address>
<email confidence="0.994489">
n165, yusuke, tsujii@is.s.u-tokyo.ac.jp
</email>
<bodyText confidence="0.99963465">
Abstract The complicated language in the huge new law
has muddied the fight.
We describe probabilistic models for a
chart generator based on HPSG. Within
the research field of parsing with lex-
icalized grammars such as HPSG, re-
cent developments have achieved efficient
estimation of probabilistic models and
high-speed parsing guided by probabilis-
tic models. The focus of this paper is
to show that two essential techniques –
model estimation on packed parse forests
and beam search during parsing – are suc-
cessfully exported to the task of natural
language generation. Additionally, we re-
port empirical evaluation of the perfor-
mance of several disambiguation models
and how the performance changes accord-
ing to the feature set used in the models
and the size of training data.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999900142857143">
Surface realization is the final stage of natural lan-
guage generation which receives a semantic rep-
resentation and outputs a corresponding sentence
where all words are properly inflected and ordered.
This paper presents log-linear models to address the
ambiguity which arises when HPSG (Head-driven
Phrase Structure Grammar (Pollard and Sag, 1994))
is applied to sentence generation. Usually a single
semantic representation can be realized as several
sentences. For example, consider the following two
sentences generated from the same input.
The language in the huge new law complicated
has muddied the fight.
The latter is not an appropriate realization because
“complicated” tends to be wrongly interpreted to
modify “law”. Therefore the generator needs to se-
lect a candidate sentence which is more fluent and
easier to understand than others.
In principle, we need to enumerate all alternative
realizations in order to estimate a log-linear model
for generation. It therefore requires high compu-
tational cost to estimate a probabilistic model for a
wide-coverage grammar because there are consider-
able ambiguities and the alternative realizations are
hard to enumerate explicitly. Moreover, even after
the model has been estimated, to explore all possible
candidates in runtime is also expensive. The same
problems also arise with HPSG parsing, and recent
studies (Tsuruoka et al., 2004; Miyao and Tsujii,
2005; Ninomiya et al., 2005) proposed a number of
solutions including the methods of estimating log-
linear models using packed forests of parse trees and
pruning improbable candidates during parsing.
The aim of this paper is to apply these techniques
to generation. Since parsing and generation both
output the best probable tree under some constraints,
we expect that techniques that work effectively in
parsing are also beneficial for generation. First, we
enabled estimation of log-linear models with less
cost by representing a set of generation trees in a
packed forest. The forest representation was ob-
tained by adopting chart generation (Kay, 1996; Car-
</bodyText>
<page confidence="0.984173">
93
</page>
<note confidence="0.6848175">
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93–102,
Vancouver, October 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999687708333334">
roll et al., 1999) where ambiguous candidates are
packed into an equivalence class and mapping a
chart into a forest in the same way as parsing. Sec-
ond, we reduced the search space in runtime by
adopting iterative beam search (Tsuruoka and Tsu-
jii, 2004) that efficiently pruned improbable candi-
dates. We evaluated the generator on the Penn Tree-
bank (Marcus et al., 1993), which is highly reliable
corpus consisting of real-world texts.
Through a series of experiments, we compared
the performance of several disambiguation mod-
els following an existing study (Velldal and Oepen,
2005) and examined how the performance changed
according to the size of training data, the feature set,
and the beam width. Comparing the latter half of the
experimental results with those on parsing (Miyao
and Tsujii, 2005), we investigated similarities and
differences between probabilistic models for parsing
and generation. The results indicated that the tech-
niques exported from parsing to generation worked
well while the effects were slightly different in de-
tail.
The Nitrogen system (Langkilde and Knight,
1998; Langkilde, 2000) maps semantic relations to a
packed forest containing all realizations and selects
the best one with a bigram model. Our method ex-
tends their approach in that we can utilize syntactic
features in the disambiguation model in addition to
the bigram.
From the perspective of using a lexicalized gram-
mar developed for parsing and importing pars-
ing techniques, our method is similar to the fol-
lowing approaches. The Fergus system (Banga-
lore and Rambow, 2000) uses LTAG (Lexicalized
Tree Adjoining Grammar (Schabes et al., 1988))
for generating a word lattice containing realizations
and selects the best one using a trigram model.
White and Baldridge (2003) developed a chart gen-
erator for CCG (Combinatory Categorial Gram-
mar (Steedman, 2000)) and proposed several tech-
niques for efficient generation such as best-first
search, beam thresholding and chunking the input
logical forms (White, 2004). Although some of the
techniques look effective, the models to rank can-
didates are still limited to simple language mod-
els. Carroll et al. (1999) developed a chart gen-
erator using HPSG. After the generator outputs all
the sentences the grammar allows, the ranking mod-
</bodyText>
<figureCaption confidence="0.98665">
Figure 1: PASs for “He bought the book.”
</figureCaption>
<bodyText confidence="0.999771076923077">
ule (Velldal and Oepen, 2005) selects the best one
using alog-linear model. Their model is trained us-
ing only 864 sentences where all realizations can be
explicitly enumerated.
As a grammar is extended to support more lin-
guistic phenomena and to achieve higher cover-
age, the number of alternative realizations increases
and the enumeration requires much higher compu-
tational cost. Moreover, using a variety of syntactic
features also increases the cost. By representing a
set of realizations compactly with a packed forest,
we trained the models with rich features on a large
corpus using awide-coverage grammar.
</bodyText>
<sectionHeader confidence="0.971231" genericHeader="keywords">
2 Background
</sectionHeader>
<bodyText confidence="0.999597">
This section describes background of this work in-
cluding the representation of the input to our gener-
ator, the algorithm of chart generation, and proba-
bilistic models for HPSG.
</bodyText>
<subsectionHeader confidence="0.991225">
2.1 Predicate-argument structures
</subsectionHeader>
<bodyText confidence="0.999982526315789">
The grammar we adopted is the Enju grammar,
which is an English HPSG grammar extracted from
the Penn Treebank by Miyao et al. (2004). In
parsing a sentence with the Enju grammar, seman-
tic relations of words is output included in a parse
tree. The semantic relations are represented by a
set of predicate-argument structures (PASs), which
in turn becomes the input to our generator. Figure
1 shows an example input to our generator which
corresponds to the sentence “He bought the book.”,
which consists of four predicates. REL expresses
the base form of the word corresponding to the pred-
icate. INDEX expresses a semantic variable to iden-
tify each word in the set of relations. ARG1 and
ARG2 express relationships between the predicate
and its arguments, e.g., the circled part in Figure 1
shows “he” is the subject of “buy” in this example.
The other constraints in the parse tree are omitted
in the input for the generator. Since PASs abstract
</bodyText>
<equation confidence="0.95591345">
REL buy
TENSE past
INDEX e
ARG2 y
 R EL th e 
 
INDEX z
 
y 
ARG1
 R EL h e 
 INDEX x 
 R EL book

y 
INDEX

ARG1
x

</equation>
<page confidence="0.994099">
94
</page>
<bodyText confidence="0.996837375">
away superficial differences, generation from a set
of PASs contains ambiguities in the order of modi-
fiers like the example in Section 1 or the syntactic
categories of phrases. For example, the PASs in Fig-
ure 1 can generate the NP, “the book he bought.”
When processing the input PASs, we split a single
PAS into a set of relations like (1) representing the
first PAS in Figure 1.
</bodyText>
<equation confidence="0.616057">
(1)
</equation>
<bodyText confidence="0.999165869565217">
This representation is very similar to the notion of
HLDS (Hybrid Logic Dependency Semantics) em-
ployed by White and Baldridge (2003), which is
a related notion to MRS (Minimal Recursion Se-
mantics) employed by Carroll et al. (1999). The
most significant difference between our current in-
put representation (not PAS itself) and the other rep-
resentations is that each word corresponds to exactly
one PAS while words like infinitival “to” have no
semantic relations in HLDS. This means that “The
book was bought by him.” is not generated from the
same PASs as Figure 1 because there must be the
PASs for “was” and “by” to generate the sentence.
We currently adopt this constraint for simple im-
plementation, but it is possible to use the input where
PASs for words like “to” are removed. As proposed
and implemented in the previous studies (Carroll
et al., 1999; White and Baldridge, 2003), handling
such inputs is feasible by modification in chart gen-
eration described in the following section. The algo-
rithms proposed in this paper can be integrated with
their algorithms although the implementation is left
for future research.
</bodyText>
<subsectionHeader confidence="0.999171">
2.2 Chart generation
</subsectionHeader>
<bodyText confidence="0.999839040816327">
Chart generation is similar to chart parsing, but what
an edge covers is the semantic relations associated
with it. We developed a CKY-like generator which
deals with binarized grammars including the Enju.
Figure 2 shows a chart for generating “He bought
the book.” First, lexical edges are assigned to each
PAS. Then the following loop is repeated from
to the cardinality of the input.
Apply binary rules to existing edges to generate
new edges holding PASs.
Apply unary rules to the new edges generated
in the previous process.
Store the edges generated in the current loop
into the chart&apos;.
In Figure 2, boxes in the chart represent , which
contain edges covering the same PASs, and solid
arrows represent rule applications. Each edge is
packed into an equivalence class and stored in a cell.
Equivalence classes are identified with their signs
and the semantic relations they cover. Edges with
different strings (e.g., NPs associated with “a big
white dog” and “a white big dog”) can be packed
into the same equivalence class if they have the same
feature structure.
In parsing, each edge must be combined with its
adjacent edges. Since there is no such constraint
in generation, the combinations of edges easily ex-
plodes. We followed two partial solutions to this
problem by Kay (1996).
The one is indexing edges with the semantic vari-
ables (e.g., circled in Figure 2). For example, since
the SUBCAT feature of the edge for “bought the
book” specifies that it requires an NP with an in-
dex , we can find the required edges efficiently by
checking the edges indexed with .
The other is prohibiting proliferation of gram-
matically correct, but unusable sub-phrases. Dur-
ing generating the sentence “Newspaper reports said
that the tall young Polish athlete ran fast”, sub-
phrases with incomplete modifiers such as “the tall
young athlete” or “the young Polish athlete” do not
construct the final output, but slow down the gener-
ation because they can be combined with the rest of
the input to construct grammatically correct phrases
or sentences. Carroll et al. (1999) and White (2004)
proposed different algorithms to address the same
problem. We adopted Kay’s simple solution in the
current ongoing work, but logical form chunking
proposed by White is also applicable to our system.
</bodyText>
<subsectionHeader confidence="0.665501">
2.3 Probabilistic models for generation with
HPSG
</subsectionHeader>
<bodyText confidence="0.999339142857143">
Some existing studies on probabilistic models for
HPSG parsing (Malouf and van Noord, 2004; Miyao
and Tsujii, 2005) adopted log-linear models (Berger
et al., 1996). Since log-linear models allow us to
&apos;To introduce an edge with no semantic relations as men-
tioned in the previous section, we need to combine the edges
with edges having no relations.
</bodyText>
<page confidence="0.997113">
95
</page>
<figureCaption confidence="0.999081">
Figure 2: The chart for “He bought the book.”
</figureCaption>
<figure confidence="0.999489758241758">
e
y
 HEAD verb:e 
 SUBCAT 
 H EAD noun :y
 SUBCAT 
Index
he bought the book
the book he bought
Equivalent class
e
 HEAD
 SUBCAT
verb
NP
:x
:e


bought the book
e
 H EAD
 N O N L OC|SLASH
he bought
y
verb
NP
:
:e
y


H EAD verbe
:
ISUBCAT
NP:x NP
:Y]
H EAD det:z
&apos;yJ  SPEC noun:y 
 H E AD v er b e
: 
 
SUBCAT N P x
:
 
 N O N L O C  |SL ASH  
 N P y:
e
x
 HEAD noun :x 
 SUBCAT 
H EAD noun :y
y
 S U BCAT det


bought
the
he
book
REL
INDEX
ARG1
ARG2
TENSE
R EL book
y 
INDEX
 HEAD noun :y
SUBCAT
the book
buy
e
x
y
the �
�
�� �
z
REL
INDEX
iARG1
y
past
 REL
 INDEX
he


x
Lexical edges
</figure>
<bodyText confidence="0.999737291666667">
use multiple overlapping features without assuming
independence among features, the models are suit-
able for HPSG parsing where feature structures with
complicated constraints are involved and dividing
such constraints into independent features is diffi-
cult. Log-linear models have also been used for
HPSG generation by Velldal and Oepen (2005). In
their method, the probability of a realization given
a semantic representation is formulated as
where is a feature function observed in ,is
the weight of , and represents the set of all
possible realizations of . To estimate , pairs of
are needed, where is the most preferred
realization for . Their method first automatically
generates a paraphrase treebank, where
are enumerated. Then, a log-linear model is trained
with this treebank, i.e., each is estimated so as to
maximize the likelihood of training data. As well
as features used in their previous work on statistical
parsing (Toutanova and Manning, 2002), an addi-
tional feature that represents sentence probabilities
of 4-gram model is incorporated. They showed that
the combined model outperforms the model without
the 4-gram feature.
</bodyText>
<sectionHeader confidence="0.997659" genericHeader="introduction">
3 Disambiguation models for chart
generation
</sectionHeader>
<subsectionHeader confidence="0.998337">
3.1 Packed representation of a chart
</subsectionHeader>
<bodyText confidence="0.9996005">
As mentioned in Section 2.3, to estimate log-linear
models for HPSG generation, we need all alterna-
tive derivation trees generated from the input .
However, the size of is exponential to the cardi-
nality of and they cannot be enumerated explicitly.
This problem is especially serious in wide-coverage
grammars because such grammars are designed to
cover a wide variety of linguistic phenomena, and
thus produce many realizations. In this section, we
present a method of making the estimation tractable
which is similar to a technique developed for HPSG
parsing.
When estimating log-linear models, we map
in the chart into a packed representation called a fea-
ture forest, intuitively an “AND-OR” graph. Miyao
and Tsujii (2005) represented a set of HPSG parse
</bodyText>
<page confidence="0.992733">
96
</page>
<figureCaption confidence="0.997489">
Figure 3: Feature forest for “He bought the book.”
</figureCaption>
<figure confidence="0.934869899082569">
[
S
]
verb:e
NP
:
y
� HEAD noun: y �
�� SU B CAT ��
HEAD noun: y]
U B CAT
� HEAD
�� N O N LO C|SLASH
�HEAD verb:e �
�� SUBCAT ��
� HEAD noun:x �
�� SUBCAT ��
HEAD
verb e
:
:
SUBCAT
NP
x
]
[
[
]
HEAD
verb e
:
SUBCAT
NP
:
x
[
]
H EAD
verb:e
HEAD noun :y
SU B CAT
NP
;.
NP
SUBCAT
�
��
verb:e
N
C|
N O
L O
SLASH
NP
:
y
� H EAD
�� SU B CAT
�
��
[
�
��
[
verb:e
HEAD det:z
SPEC
SUBCAT
det
NP
NP
:
x
:
y
noun:
y
book
the
bought
H EAD no un, H EAD verb:e
ISU B CAT SU B CAT NP x NP y
: : ��
IH EA D no un :x
SU B CAT ��
he
[ Y]
� HEAD det:z �
�� SPEC noun : y ��
IH EAD noun :y
SU B CAT ��
�H EAD noun :y
S UB CAT det �
�� ��
HEAD noun: y]
H EAD
SU B CAT
�N O N LO C|SLASH
:
verb
e
:
NP
:
x
i
NP
y
bought
</figure>
<bodyText confidence="0.9381978">
trees using a feature forest and succeeded in esti-
mating given a sentence and a parse tree
using dynamic programming without unpacking the
chart. If is represented in a feature forest in
generation, can also be estimated in the same
way.
,
and are respectively the mother edge, the left
daughter, and the right daughter in a single rule ap-
plication. Nodes connected by dotted lines repre-
sent OR-nodes, i.e., equivalence classes in the same
cell. Feature functions are assigned to OR-nodes.
By doing so, we can capture important features for
disambiguation in HPSG, i.e., combinations of a
mother and its daughter(s). Nodes connected by
solid arrows represent AND-nodes corresponding to
the daughters of the parent node. By using feature
forests, we can efficiently pack the node generated
more than once in the set of trees. For example, the
nodes corresponding to “the book” in “He bought
the book.” and “the book he bought” are identical
and described only once in the forest. The merits
of using forest representations in generation instead
of lattices or simple enumeration are discussed thor-
oughly by Langkilde (2000).
</bodyText>
<subsectionHeader confidence="0.999459">
3.2 Model variation
</subsectionHeader>
<bodyText confidence="0.999979142857143">
We implemented and compared four different dis-
ambiguation models as Velldal and Oepen (2005)
did. Throughout the models, we assigned a score
called figure-of-merit (FOM) on each edge and cal-
culated the FOM of a mother edge by dynamic pro-
gramming. FOM represents the log probability of an
edge which is not normalized.
</bodyText>
<sectionHeader confidence="0.554717" genericHeader="method">
H EA D
</sectionHeader>
<bodyText confidence="0.959213076923077">
Baseline model We started with a simple baseline
model, , where is a PAS
in the input semantic representation and is a lexi-
cal entry assigned to . The FOM of the mother edge
is computed simply as
. All the other models use this model as a ref-
erence distribution (Miyao and Tsujii, 2005), i.e.,
is estimated to maximize the likelihood of the train-
ing data , which is calculated with the following
equation.
Figure 3 shows a feature forest representing the
chart in Figure 2. Each node corresponds to either
a lexical entry or a tuple of where
</bodyText>
<page confidence="0.994642">
97
</page>
<bodyText confidence="0.992903189189189">
Bigram model The second model is a log-linear
model with only one feature that corresponds to
bigram probabilities for adjacent word-pairs in the
sentence. We estimated a bigram language model
using a part of the British National Corpus as train-
ing data2. In the chart each edge is identified with
the first and last word in the phrase as well as
its feature structure and covered relations. When
two edges are combined, is computed as
, where
is the weight of the bigram feature, is the
last word of the left daughter, is the first word
of the right daughter, and represents a log
probability of a bigram. Contrary to the method of
Velldal and Oepen (2005) where the input is a set
of sentences and is computed on a whole
sentence, we computed on each phrase as
Langkilde (2000) did The language model can be
extended to -gram if each edge holds last
words although the number of edges increase.
Syntax model The third model incorporates a
variety of syntactic features and lexical features
where is computed as
. The feature set consists of com-
binations of atomic features shown in Table 1. The
atomic features and their combinations are imported
from the previous work on HPSG parsing (Miyao
and Tsujii, 2005). We defined three types of fea-
ture combinations to capture the characteristics of
binary and unary rule applications and root edges as
described below.
An example of extracted features is shown in Fig-
ure 4 where “bought the book” is combined with
its subject “he”. Since the mother edge is a root
edge, two features ( and ) are extracted
from this node. In the feature, the phrasal cat-
egory SYM becomes S (sentence), the head word
</bodyText>
<footnote confidence="0.969963">
2The model estimation was done using the CMU-Cambridge
Statistical Language Modeling toolkit (Clarkson and Rosenfeld,
1997).
</footnote>
<figureCaption confidence="0.990782">
Figure 4: Example of features
</figureCaption>
<tableCaption confidence="0.653812">
Table 1: Atomic features
</tableCaption>
<table confidence="0.2423794">
RULE the name of the applied schema
DIST the distance between the head words of
the daughters
COMMA whether a comma exists between daughters
and/or inside of daughter phrases
SPAN the number of words dominated by the phrase
SYM the symbol of the phrasal category
(e.g., NP, VP)
WORD the surface form of the head word
LE the lexical entry assigned to the head word
</table>
<bodyText confidence="0.974005875">
WORD becomes “bought”, and its lexical entry LE
becomes that of transitive verbs. In the fea-
ture, properties of the left and right daughters are
instantiated in addition to those of the mother edge.
Combined model The fourth and final model is
the combination of the syntax model and the bigram
model. This model is obtained by simply adding the
bigram feature to the syntax model.
</bodyText>
<sectionHeader confidence="0.990614" genericHeader="method">
4 Iterative beam search
</sectionHeader>
<bodyText confidence="0.995765076923077">
For efficient statistical generation with a wide-
coverage grammar, we reduce the search space by
pruning edges during generation. We use beam
search where edges with low FOMs are pruned dur-
ing generation. We use two parameters, and:
in each cell, the generator prunes except for top
edges, and edges whose FOMs are lower than that
of the top edgeare also pruned.
Another technique for achieving efficiency is it-
erative generation which is adopted from iterative
CKY parsing (Tsuruoka and Tsujii, 2004). When
beam width is too narrow, correct edges to consti-
tute a correct sentence may be discarded during gen-
</bodyText>
<figure confidence="0.97236925">
S,bought, transitive
� HEAD verb:e �
�� SUBCAT ��
� HEAD noun:x �
�� SUBCAT ��
� HEAD v er b e
: �
�� SUBCAT NP:x ��
he bought the book
subject
-head
,
1 no
,
,
1, NP, he,p roper
-noun
,
3 VP
, , bought, transitive
</figure>
<page confidence="0.998039">
98
</page>
<tableCaption confidence="0.997192">
Table 2: Averaged generation time and accuracy by four models
</tableCaption>
<table confidence="0.999521625">
Model Baseline Bigram Syntax Combined
Coverage (%) 91.15 90.15 90.75 90.56
Time (ms) 3512 4085 3821 4315
BLEU (89 sentences) 0.7776 0.7503 0.8195 0.7359
(179 sentences) 0.5544 0.6323 0.7339 0.7305
(326 sentences) 0.5809 0.6415 0.7735 0.7384
(412 sentences) 0.5863 0.6542 0.7835 0.7533
Total (1,006 sentences) 0.5959 0.6544 0.7733 0.7420
</table>
<bodyText confidence="0.993463625">
eration and it causes degradation in coverage, i.e.,
the ratio the generator successfully outputs a sen-
tence. The appropriate beam width depends on in-
puts and cannot be predefined. In iterative genera-
tion, the process of chart generation is repeated with
increasing beam width until a complete sentence is
generated or the beam width exceeds the predefined
maximum.
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999941949152543">
In this section, we present five experiments: com-
parison among four models described in Section 3.2,
syntax models with different features, different cor-
pus sizes, different beam widths, and the distribution
of generation time. The bigram model was trained
using 100,000 sentences in the BNC. The unigram
and syntax model was trained using Section 02-21 of
the WSJ portion of the Penn Treebank (39,832 sen-
tences). Section 22 (1,700 sentences) and 23 (2,416
sentences) were used as the development and test
data, respectively.
Because the generator is still slow to gener-
ate long sentences, sentences with more than 20
words were not used. We converted the treebank
into HPSG-style derivation trees by the method of
Miyao et al. (2004) and extracted the semantic rela-
tions, which are used as the inputs to the generator.
The sentences where this conversion failed were also
eliminated although such sentences were few (about
0.3% of the eliminated data). The resulting training
data consisted of 18,052 sentences and the test data
consisted of 1,006 sentences. During training, un-
covered sentences – where the lexicon does not in-
clude the lexical entry to construct correct derivation
– were also ignored, while such sentences remained
in the test data. The final training data we can utilize
consisted of 15,444 sentences. The average sentence
length of the test data was 12.4, which happens to be
close to that of Velldal and Oepen (2005) though the
test data is different.
The accuracy of the generator outputs was eval-
uated by the BLEU score (Papineni et al., 2001),
which is commonly used for the evaluation of ma-
chine translation and recently used for the evalua-
tion of generation (Langkilde-Geary, 2002; Velldal
and Oepen, 2005). BLEU is the weighted average of
n-gram precision against the reference sentence. We
used the sentences in the Penn Treebank as the refer-
ence sentences. The beam width was increased from
to in two steps. The pa-
rameters were empirically determined using the de-
velopment set. All the experiments were conducted
on AMD Opteron servers with a 2.0-GHz CPU and
12-GB memory.
Table 2 shows the average generation time and the
accuracy of the models presented in Section 3. The
generation time includes time for the input for which
the generator could not output a sentence, while the
accuracy was calculated only in the case of success-
ful generation. All models succeeded in generation
for over 90% of the test data.
Contrary to the result of the Velldal and Oepen
(2005), the syntax model outperformed the com-
bined model. We observed the same result when we
varied the parameters for beam thresholding. This
is possibly just because the language model was not
trained enough as that of the previous research (Vell-
dal and Oepen, 2005) where the model was 4-gram
and trained with the entire BNC3.
</bodyText>
<footnote confidence="0.7837555">
3We could not use the entire corpus for training because of
a problem in implementation. This problem will be fixed in the
</footnote>
<page confidence="0.991885">
99
</page>
<figure confidence="0.999387769230769">
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0 2000 4000 6000 8000 10000 12000 14000 16000
Size of training data
</figure>
<figureCaption confidence="0.997646">
Figure 5: Size of training data vs. performance
</figureCaption>
<tableCaption confidence="0.913146">
Table 3: Feature set vs. performance
</tableCaption>
<table confidence="0.9990443">
Feature BLEU diff.
All 0.7734
-COMMA 0.7492 -0.0242
-DIST 0.7702 -0.0032
-LE 0.7423 -0.0311
-RULE 0.7709 -0.0025
-SPAN 0.7640 -0.0094
-SYM 0.7400 -0.0334
-WORD 0.7610 -0.0124
None 0.5959 -0.1775
</table>
<bodyText confidence="0.992281285714286">
Although the accuracy shown in Table 2 was
lower than that of Velldal and Oepen, there is lit-
tle point in direct comparison between the accuracy
of the two systems because the settings are consider-
ably different in terms of the grammar, the input rep-
resentation, and the training and test set. The algo-
rithm we proposed does not depend on our specific
setting and can be integrated and evaluated within
their setting. We used larger training data (15,444
sentences) and test data (1,006 sentences), compared
to their treebank of 864 sentences where the log-
linear models were evaluated by cross validation.
This is the advantage of adopting feature forests to
efficiently estimate the log-linear models.
Figure 5 shows the relationship between the size
of training data and the accuracy. All the following
experiments were conducted on the syntax model.
The accuracy seems to saturate around 4000 sen-
tences, which indicates that a small training set is
enough to train the current syntax model and that
future development.
</bodyText>
<tableCaption confidence="0.99744">
Table 4: vs. performance
</tableCaption>
<table confidence="0.999876666666667">
Coverage (%) Time (ms) BLEU
4 66.10 768 0.7685
8 82.91 3359 0.7654
12 87.89 7191 0.7735
16 89.46 11051 0.7738
20 90.56 15530 0.7723
</table>
<tableCaption confidence="0.96656">
Table 5:vs. performance
</tableCaption>
<table confidence="0.9945434">
Coverage (%) Time (ms) BLEU
4.0 78.23 2450 0.7765
6.0 89.56 9083 0.7693
8.0 91.15 19320 0.7697
10.0 89.86 35897 0.7689
</table>
<bodyText confidence="0.99970275862069">
we could use an additional feature set to improve
the accuracy. Similar results are reported in pars-
ing (Miyao and Tsujii, 2005) while the accuracy sat-
urated around 16,000 sentences. When we use more
complicated features or train the model with longer
sentences, possibly the size of necessary training
data will increase.
Table 3 shows the performance of syntax mod-
els with different feature sets. Each row represents
a model where one of the atomic features in Table
1 was removed. The “None” row is the baseline
model. The rightmost column represents the differ-
ence of the accuracy from the model trained with
all features. SYM, LE, and COMMA features had a
significant influence on the performance. These re-
sults are different from those in parsing reported by
Miyao and Tsujii (2005) where COMMA and SPAN
especially contributed to the accuracy. This observa-
tion implies that there is still room for improvement
by tuning the combination of features for generation.
We compared the performance of the generator
with different beam widths to investigate the effect
of iterative beam search. Table 4 shows the results
when we varied , which is the number of edges,
while thresholding by FOM differences is disabled,
and Table 5 shows the results when we varied only
, which is the FOM difference.
Intuitively, beam search may decrease the accu-
racy because it cannot explore all possible candi-
</bodyText>
<figure confidence="0.987450888888889">
BLEU
100
300 50
250 45
200 40
150 35
100 30
50 25
0 20
15
10
5
0
# of sentences # of sentences
0 100 200 300 400 500 600 700
generation time (sec)
0 100 200 300 400 500 600 700
generation time (sec)
</figure>
<figureCaption confidence="0.999996">
Figure 6: Distribution of generation time
</figureCaption>
<bodyText confidence="0.999951448275862">
dates during generation. Iterative beam search is
more likely to decrease the accuracy than ordinary
beam search. However, the results show that the ac-
curacy did not drastically decrease at small widths.
Moreover, the accuracy of iterative beam search was
almost the same as that of . On the other
hand, generation time significantly increased as or
increased, indicating that iterative beam search ef-
ficiently discarded unnecessary edges without loos-
ing the accuracy. Although the coverage increases as
the beam width increases, the coverage at or
is lower than that of iterative beam search
(Table 2)4.
Finally, we examined the distribution of genera-
tion time without the limitation of sentence length
in order to investigate the strategy to improve the ef-
ficiency of the generator. Figure 6 is a histogram of
generation time for 500 sentences randomly selected
from the development set, where 418 sentences were
successfully generated and the average BLEU score
was 0.705. The average sentence length was 22.1
and the maximum length was 60, and the aver-
age generation time was 27.9 sec, which was much
longer than that for short sentences. It shows that a
few sentences require extremely long time for gen-
eration although about 70% of the sentences were
generated within 5 sec. Hence, the average time pos-
sibly decreases if we investigate what kind of sen-
tences require especially long time and improve the
</bodyText>
<footnote confidence="0.54170275">
4This is because the generator fails when the number of
edges exceeds 10,000. Since the number of edges significantly
increases when or is large, generation fails even if the cor-
rect edges are in the chart.
</footnote>
<bodyText confidence="0.999710545454546">
algorithm to remove such time-consuming fractions.
The investigation is left for future research.
The closest empirical evaluations on the same task
is that of Langkilde-Geary (2002) which reported
the performance of the HALogen system while the
approach is rather different. Hand-written mapping
rules are used to make a forest containing all can-
didates and the best candidate is selected using the
bigram model. The performance of the generator
was evaluated on Section 23 of the Penn Treebank
in terms of the number of ambiguities, generation
time, coverage, and accuracy. Several types of in-
put specifications were examined in order to mea-
sure how specific the input should be for generat-
ing valid sentences. One of the specifications named
“permute, no dir” is similar to our input in that the
order of modifiers is not determined at all. The gen-
erator produced outputs for 82.7% of the inputs with
average generation time 30.0 sec and BLEU score
0.757. The results of our last experiment are com-
parable to these results though the used section is
different.
</bodyText>
<sectionHeader confidence="0.999549" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999967333333333">
We presented a chart generator using HPSG and de-
veloped log-linear models which we believe was es-
sential to develop a sentence generator. Several tech-
niques developed for parsing also worked in genera-
tion. The introduced techniques were an estimation
method for log-linear models using a packed for-
est representation of HPSG trees and iterative beam
search. The system was evaluated through applica-
tion to real-world texts. The experimental results
</bodyText>
<page confidence="0.99617">
101
</page>
<bodyText confidence="0.999996625">
showed that the generator was able to output a sen-
tence for over 90% of the test data when the data was
limited to short sentences. The accuracy was signif-
icantly improved by incorporating syntactic features
into the log-linear model. As future work we intend
to tune the feature set for generation. We also plan
to further increase the efficiency of the generator so
as to generate longer sentences.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999730231884058">
S. Bangalore and O. Rambow. 2000. Exploiting a proba-
bilistic hierarchical model for generation. In Proceed-
ings of the COLING’00.
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39–71.
J. Carroll, A. Copestake, D. Flickinger, and V. Poznanski.
1999. An efficient chart generator for (semi-)lexicalist
grammars. In Proceedings of the EWNLG’99.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the CMU-Cambridge toolkit. In Pro-
ceedings of ESCA Eurospeech.
M. Kay. 1996. Chart generation. In Proceedings of the
ACL’96, pages 200–204.
I. Langkilde and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proceed-
ings of the COLING-ACL’98, pages 704–710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In Proceedings of the INLG’02.
I. Langkilde. 2000. Forest-based statistical sentence gen-
eration. In Proceedings of the NAACL’00.
R. Malouf and G. van Noord. 2004. Wide coverage pars-
ing with stochastic attribute value grammars. In Pro-
ceedings of the IJCNLP-04 Workshop on Beyond Shal-
low Analyses.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics.
Y. Miyao and J. Tsujii. 2005. Probabilistic disambigua-
tion models for wide-coverage HPSG parsing. In Pro-
ceedings of the ACL’05.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2004. Corpus-
oriented grammar development for acquiring a Head-
Driven Phrase Structure Grammar from the Penn Tree-
bank. In Proceedings of the IJCNLP-04.
T. Ninomiya, Y. Tsuruoka, Y. Miyao, and J. Tsujii. 2005.
Efficacy of beam thresholding, unification filtering and
hybrid parsing in probabilistic HPSG parsing. In Pro-
ceedings of the IWPT’05.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the ACL’01.
C. Pollard and I. A. Sag. 1994. Head-driven Phrase
Structure Grammar. University of Chicago Press.
Y. Schabes, A. Abeille, and A. K. Joshi. 1988. Parsing
strategies with ’lexicalized’ grammars: Application to
tree adjoining grammar. In Proceedings of the COL-
ING 1988, pages 578–583.
M. Steedman. 2000. The Syntactic Process. MIT Press.
K. Toutanova and C. D. Manning. 2002. Feature selec-
tion for a rich HPSG grammar using decision trees. In
Proceedings of the CoNLL’02.
Y. Tsuruoka and J. Tsujii. 2004. Iterative CKY parsing
for Probabilistic Context-Free Grammars. In Proceed-
ings of the IJCNLP’04.
Y. Tsuruoka, Y. Miyao, and J. Tsujii. 2004. Towards ef-
ficient probabilistic HPSG parsing: integrating seman-
tic and syntactic preference to guide the parsing. In
Proceedings of the IJCNLP-04 Workshop on Beyond
Shallow Analyses.
E. Velldal and S. Oepen. 2005. Maximum entropy mod-
els for realization ranking. In Proceedings of the MT-
Summit’05.
M. White and J. Baldridge. 2003. Adapting chart real-
ization to CCG. In Proceedings of the EWNNLG’03.
M. White. 2004. Reining in CCG chart realization. In
Proceedings of the INLG’04.
</reference>
<page confidence="0.998619">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.246169">
<title confidence="0.998485">Probabilistic models for disambiguation of an HPSG-based chart generator</title>
<author confidence="0.996613">Yusuke Miyao</author>
<affiliation confidence="0.977881">CREST, JST</affiliation>
<address confidence="0.8625605">Honcho 4-1-8, Saitama 332-0012, Japan</address>
<email confidence="0.696664">Hiroko</email>
<affiliation confidence="0.9987795">Department of Computer University of</affiliation>
<address confidence="0.6541195">Hongo 7-3-1, Tokyo 113-0033, Japan</address>
<author confidence="0.985558">Jun’ichi Tsujii</author>
<affiliation confidence="0.999954">School of Informatics University of Manchester</affiliation>
<address confidence="0.9846505">POBox 88, Sackville St MANCHESTER M60 1QD, UK</address>
<email confidence="0.91105">n165,yusuke,tsujii@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.999828894736842">in the huge new law has muddied the fight. We describe probabilistic models for a chart generator based on HPSG. Within the research field of parsing with lexicalized grammars such as HPSG, recent developments have achieved efficient estimation of probabilistic models and high-speed parsing guided by probabilistic models. The focus of this paper is to show that two essential techniques – model estimation on packed parse forests and beam search during parsing – are successfully exported to the task of natural language generation. Additionally, we report empirical evaluation of the performance of several disambiguation models and how the performance changes according to the feature set used in the models and the size of training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING’00.</booktitle>
<contexts>
<context position="5011" citStr="Bangalore and Rambow, 2000" startWordPosition="763" endWordPosition="767">niques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a cha</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>S. Bangalore and O. Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proceedings of the COLING’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11710" citStr="Berger et al., 1996" startWordPosition="1893" endWordPosition="1896">onstruct the final output, but slow down the generation because they can be combined with the rest of the input to construct grammatically correct phrases or sentences. Carroll et al. (1999) and White (2004) proposed different algorithms to address the same problem. We adopted Kay’s simple solution in the current ongoing work, but logical form chunking proposed by White is also applicable to our system. 2.3 Probabilistic models for generation with HPSG Some existing studies on probabilistic models for HPSG parsing (Malouf and van Noord, 2004; Miyao and Tsujii, 2005) adopted log-linear models (Berger et al., 1996). Since log-linear models allow us to &apos;To introduce an edge with no semantic relations as mentioned in the previous section, we need to combine the edges with edges having no relations. 95 Figure 2: The chart for “He bought the book.” e y  HEAD verb:e   SUBCAT   H EAD noun :y  SUBCAT  Index he bought the book the book he bought Equivalent class e  HEAD  SUBCAT verb NP :x :e   bought the book e  H EAD  N O N L OC|SLASH he bought y verb NP : :e y   H EAD verbe : ISUBCAT NP:x NP :Y] H EAD det:z &apos;yJ  SPEC noun:y   H E AD v er b e :    SUBCAT N P x :    N O N L O</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>V Poznanski</author>
</authors>
<title>An efficient chart generator for (semi-)lexicalist grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the EWNLG’99.</booktitle>
<contexts>
<context position="5595" citStr="Carroll et al. (1999)" startWordPosition="854" endWordPosition="857">s system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentences the grammar allows, the ranking modFigure 1: PASs for “He bought the book.” ule (Velldal and Oepen, 2005) selects the best one using alog-linear model. Their model is trained using only 864 sentences where all realizations can be explicitly enumerated. As a grammar is extended to support more linguistic phenomena and to achieve higher coverage, the number of alternative realizations increases and the enumeration requires much higher computational cost. Moreover, using a variety of syntactic features also inc</context>
<context position="8298" citStr="Carroll et al. (1999)" startWordPosition="1328" endWordPosition="1331">  94 away superficial differences, generation from a set of PASs contains ambiguities in the order of modifiers like the example in Section 1 or the syntactic categories of phrases. For example, the PASs in Figure 1 can generate the NP, “the book he bought.” When processing the input PASs, we split a single PAS into a set of relations like (1) representing the first PAS in Figure 1. (1) This representation is very similar to the notion of HLDS (Hybrid Logic Dependency Semantics) employed by White and Baldridge (2003), which is a related notion to MRS (Minimal Recursion Semantics) employed by Carroll et al. (1999). The most significant difference between our current input representation (not PAS itself) and the other representations is that each word corresponds to exactly one PAS while words like infinitival “to” have no semantic relations in HLDS. This means that “The book was bought by him.” is not generated from the same PASs as Figure 1 because there must be the PASs for “was” and “by” to generate the sentence. We currently adopt this constraint for simple implementation, but it is possible to use the input where PASs for words like “to” are removed. As proposed and implemented in the previous stu</context>
<context position="11280" citStr="Carroll et al. (1999)" startWordPosition="1827" endWordPosition="1830">” specifies that it requires an NP with an index , we can find the required edges efficiently by checking the edges indexed with . The other is prohibiting proliferation of grammatically correct, but unusable sub-phrases. During generating the sentence “Newspaper reports said that the tall young Polish athlete ran fast”, subphrases with incomplete modifiers such as “the tall young athlete” or “the young Polish athlete” do not construct the final output, but slow down the generation because they can be combined with the rest of the input to construct grammatically correct phrases or sentences. Carroll et al. (1999) and White (2004) proposed different algorithms to address the same problem. We adopted Kay’s simple solution in the current ongoing work, but logical form chunking proposed by White is also applicable to our system. 2.3 Probabilistic models for generation with HPSG Some existing studies on probabilistic models for HPSG parsing (Malouf and van Noord, 2004; Miyao and Tsujii, 2005) adopted log-linear models (Berger et al., 1996). Since log-linear models allow us to &apos;To introduce an edge with no semantic relations as mentioned in the previous section, we need to combine the edges with edges havin</context>
</contexts>
<marker>Carroll, Copestake, Flickinger, Poznanski, 1999</marker>
<rawString>J. Carroll, A. Copestake, D. Flickinger, and V. Poznanski. 1999. An efficient chart generator for (semi-)lexicalist grammars. In Proceedings of the EWNLG’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of ESCA Eurospeech.</booktitle>
<contexts>
<context position="19112" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="3293" endWordPosition="3296">e imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). We defined three types of feature combinations to capture the characteristics of binary and unary rule applications and root edges as described below. An example of extracted features is shown in Figure 4 where “bought the book” is combined with its subject “he”. Since the mother edge is a root edge, two features ( and ) are extracted from this node. In the feature, the phrasal category SYM becomes S (sentence), the head word 2The model estimation was done using the CMU-Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld, 1997). Figure 4: Example of features Table 1: Atomic features RULE the name of the applied schema DIST the distance between the head words of the daughters COMMA whether a comma exists between daughters and/or inside of daughter phrases SPAN the number of words dominated by the phrase SYM the symbol of the phrasal category (e.g., NP, VP) WORD the surface form of the head word LE the lexical entry assigned to the head word WORD becomes “bought”, and its lexical entry LE becomes that of transitive verbs. In the feature, properties of the left and right daughters are instantiated in addition to those </context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>P. Clarkson and R. Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. In Proceedings of ESCA Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Chart generation.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL’96,</booktitle>
<pages>200--204</pages>
<contexts>
<context position="3267" citStr="Kay, 1996" startWordPosition="494" endWordPosition="495">er of solutions including the methods of estimating loglinear models using packed forests of parse trees and pruning improbable candidates during parsing. The aim of this paper is to apply these techniques to generation. Since parsing and generation both output the best probable tree under some constraints, we expect that techniques that work effectively in parsing are also beneficial for generation. First, we enabled estimation of log-linear models with less cost by representing a set of generation trees in a packed forest. The forest representation was obtained by adopting chart generation (Kay, 1996; Car93 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93–102, Vancouver, October 2005. c�2005 Association for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-w</context>
<context position="10504" citStr="Kay (1996)" startWordPosition="1698" endWordPosition="1699">, and solid arrows represent rule applications. Each edge is packed into an equivalence class and stored in a cell. Equivalence classes are identified with their signs and the semantic relations they cover. Edges with different strings (e.g., NPs associated with “a big white dog” and “a white big dog”) can be packed into the same equivalence class if they have the same feature structure. In parsing, each edge must be combined with its adjacent edges. Since there is no such constraint in generation, the combinations of edges easily explodes. We followed two partial solutions to this problem by Kay (1996). The one is indexing edges with the semantic variables (e.g., circled in Figure 2). For example, since the SUBCAT feature of the edge for “bought the book” specifies that it requires an NP with an index , we can find the required edges efficiently by checking the edges indexed with . The other is prohibiting proliferation of grammatically correct, but unusable sub-phrases. During generating the sentence “Newspaper reports said that the tall young Polish athlete ran fast”, subphrases with incomplete modifiers such as “the tall young athlete” or “the young Polish athlete” do not construct the f</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>M. Kay. 1996. Chart generation. In Proceedings of the ACL’96, pages 200–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL’98,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="4539" citStr="Langkilde and Knight, 1998" startWordPosition="687" endWordPosition="690">e compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the COLING-ACL’98, pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<booktitle>In Proceedings of the INLG’02.</booktitle>
<contexts>
<context position="23251" citStr="Langkilde-Geary, 2002" startWordPosition="3991" endWordPosition="3992">sentences – where the lexicon does not include the lexical entry to construct correct derivation – were also ignored, while such sentences remained in the test data. The final training data we can utilize consisted of 15,444 sentences. The average sentence length of the test data was 12.4, which happens to be close to that of Velldal and Oepen (2005) though the test data is different. The accuracy of the generator outputs was evaluated by the BLEU score (Papineni et al., 2001), which is commonly used for the evaluation of machine translation and recently used for the evaluation of generation (Langkilde-Geary, 2002; Velldal and Oepen, 2005). BLEU is the weighted average of n-gram precision against the reference sentence. We used the sentences in the Penn Treebank as the reference sentences. The beam width was increased from to in two steps. The parameters were empirically determined using the development set. All the experiments were conducted on AMD Opteron servers with a 2.0-GHz CPU and 12-GB memory. Table 2 shows the average generation time and the accuracy of the models presented in Section 3. The generation time includes time for the input for which the generator could not output a sentence, while </context>
<context position="29644" citStr="Langkilde-Geary (2002)" startWordPosition="5071" endWordPosition="5072">ire extremely long time for generation although about 70% of the sentences were generated within 5 sec. Hence, the average time possibly decreases if we investigate what kind of sentences require especially long time and improve the 4This is because the generator fails when the number of edges exceeds 10,000. Since the number of edges significantly increases when or is large, generation fails even if the correct edges are in the chart. algorithm to remove such time-consuming fractions. The investigation is left for future research. The closest empirical evaluations on the same task is that of Langkilde-Geary (2002) which reported the performance of the HALogen system while the approach is rather different. Hand-written mapping rules are used to make a forest containing all candidates and the best candidate is selected using the bigram model. The performance of the generator was evaluated on Section 23 of the Penn Treebank in terms of the number of ambiguities, generation time, coverage, and accuracy. Several types of input specifications were examined in order to measure how specific the input should be for generating valid sentences. One of the specifications named “permute, no dir” is similar to our i</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>I. Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Proceedings of the INLG’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
</authors>
<title>Forest-based statistical sentence generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACL’00.</booktitle>
<contexts>
<context position="4557" citStr="Langkilde, 2000" startWordPosition="691" endWordPosition="692">f several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best </context>
<context position="16453" citStr="Langkilde (2000)" startWordPosition="2829" endWordPosition="2830">can capture important features for disambiguation in HPSG, i.e., combinations of a mother and its daughter(s). Nodes connected by solid arrows represent AND-nodes corresponding to the daughters of the parent node. By using feature forests, we can efficiently pack the node generated more than once in the set of trees. For example, the nodes corresponding to “the book” in “He bought the book.” and “the book he bought” are identical and described only once in the forest. The merits of using forest representations in generation instead of lattices or simple enumeration are discussed thoroughly by Langkilde (2000). 3.2 Model variation We implemented and compared four different disambiguation models as Velldal and Oepen (2005) did. Throughout the models, we assigned a score called figure-of-merit (FOM) on each edge and calculated the FOM of a mother edge by dynamic programming. FOM represents the log probability of an edge which is not normalized. H EA D Baseline model We started with a simple baseline model, , where is a PAS in the input semantic representation and is a lexical entry assigned to . The FOM of the mother edge is computed simply as . All the other models use this model as a reference dist</context>
<context position="18124" citStr="Langkilde (2000)" startWordPosition="3129" endWordPosition="3130">mated a bigram language model using a part of the British National Corpus as training data2. In the chart each edge is identified with the first and last word in the phrase as well as its feature structure and covered relations. When two edges are combined, is computed as , where is the weight of the bigram feature, is the last word of the left daughter, is the first word of the right daughter, and represents a log probability of a bigram. Contrary to the method of Velldal and Oepen (2005) where the input is a set of sentences and is computed on a whole sentence, we computed on each phrase as Langkilde (2000) did The language model can be extended to -gram if each edge holds last words although the number of edges increase. Syntax model The third model incorporates a variety of syntactic features and lexical features where is computed as . The feature set consists of combinations of atomic features shown in Table 1. The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). We defined three types of feature combinations to capture the characteristics of binary and unary rule applications and root edges as described below. An example of </context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>I. Langkilde. 2000. Forest-based statistical sentence generation. In Proceedings of the NAACL’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
<author>G van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04 Workshop on Beyond Shallow Analyses.</booktitle>
<marker>Malouf, van Noord, 2004</marker>
<rawString>R. Malouf and G. van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proceedings of the IJCNLP-04 Workshop on Beyond Shallow Analyses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="3813" citStr="Marcus et al., 1993" startWordPosition="578" endWordPosition="581">forest representation was obtained by adopting chart generation (Kay, 1996; Car93 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93–102, Vancouver, October 2005. c�2005 Association for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-world texts. Through a series of experiments, we compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL’05.</booktitle>
<contexts>
<context position="2618" citStr="Miyao and Tsujii, 2005" startWordPosition="391" endWordPosition="394">h is more fluent and easier to understand than others. In principle, we need to enumerate all alternative realizations in order to estimate a log-linear model for generation. It therefore requires high computational cost to estimate a probabilistic model for a wide-coverage grammar because there are considerable ambiguities and the alternative realizations are hard to enumerate explicitly. Moreover, even after the model has been estimated, to explore all possible candidates in runtime is also expensive. The same problems also arise with HPSG parsing, and recent studies (Tsuruoka et al., 2004; Miyao and Tsujii, 2005; Ninomiya et al., 2005) proposed a number of solutions including the methods of estimating loglinear models using packed forests of parse trees and pruning improbable candidates during parsing. The aim of this paper is to apply these techniques to generation. Since parsing and generation both output the best probable tree under some constraints, we expect that techniques that work effectively in parsing are also beneficial for generation. First, we enabled estimation of log-linear models with less cost by representing a set of generation trees in a packed forest. The forest representation was</context>
<context position="4245" citStr="Miyao and Tsujii, 2005" startWordPosition="645" endWordPosition="648">e in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-world texts. Through a series of experiments, we compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexical</context>
<context position="11662" citStr="Miyao and Tsujii, 2005" startWordPosition="1886" endWordPosition="1889">ung athlete” or “the young Polish athlete” do not construct the final output, but slow down the generation because they can be combined with the rest of the input to construct grammatically correct phrases or sentences. Carroll et al. (1999) and White (2004) proposed different algorithms to address the same problem. We adopted Kay’s simple solution in the current ongoing work, but logical form chunking proposed by White is also applicable to our system. 2.3 Probabilistic models for generation with HPSG Some existing studies on probabilistic models for HPSG parsing (Malouf and van Noord, 2004; Miyao and Tsujii, 2005) adopted log-linear models (Berger et al., 1996). Since log-linear models allow us to &apos;To introduce an edge with no semantic relations as mentioned in the previous section, we need to combine the edges with edges having no relations. 95 Figure 2: The chart for “He bought the book.” e y  HEAD verb:e   SUBCAT   H EAD noun :y  SUBCAT  Index he bought the book the book he bought Equivalent class e  HEAD  SUBCAT verb NP :x :e   bought the book e  H EAD  N O N L OC|SLASH he bought y verb NP : :e y   H EAD verbe : ISUBCAT NP:x NP :Y] H EAD det:z &apos;yJ  SPEC noun:y   H E AD</context>
<context position="14547" citStr="Miyao and Tsujii (2005)" startWordPosition="2419" endWordPosition="2422">ve derivation trees generated from the input . However, the size of is exponential to the cardinality of and they cannot be enumerated explicitly. This problem is especially serious in wide-coverage grammars because such grammars are designed to cover a wide variety of linguistic phenomena, and thus produce many realizations. In this section, we present a method of making the estimation tractable which is similar to a technique developed for HPSG parsing. When estimating log-linear models, we map in the chart into a packed representation called a feature forest, intuitively an “AND-OR” graph. Miyao and Tsujii (2005) represented a set of HPSG parse 96 Figure 3: Feature forest for “He bought the book.” [ S ] verb:e NP : y � HEAD noun: y � �� SU B CAT �� HEAD noun: y] U B CAT � HEAD �� N O N LO C|SLASH �HEAD verb:e � �� SUBCAT �� � HEAD noun:x � �� SUBCAT �� HEAD verb e : : SUBCAT NP x ] [ [ ] HEAD verb e : SUBCAT NP : x [ ] H EAD verb:e HEAD noun :y SU B CAT NP ;. NP SUBCAT � �� verb:e N C| N O L O SLASH NP : y � H EAD �� SU B CAT � �� [ � �� [ verb:e HEAD det:z SPEC SUBCAT det NP NP : x : y noun: y book the bought H EAD no un, H EAD verb:e ISU B CAT SU B CAT NP x NP y : : �� IH EA D no un :x SU B CAT �� h</context>
<context position="17086" citStr="Miyao and Tsujii, 2005" startWordPosition="2941" endWordPosition="2944">l variation We implemented and compared four different disambiguation models as Velldal and Oepen (2005) did. Throughout the models, we assigned a score called figure-of-merit (FOM) on each edge and calculated the FOM of a mother edge by dynamic programming. FOM represents the log probability of an edge which is not normalized. H EA D Baseline model We started with a simple baseline model, , where is a PAS in the input semantic representation and is a lexical entry assigned to . The FOM of the mother edge is computed simply as . All the other models use this model as a reference distribution (Miyao and Tsujii, 2005), i.e., is estimated to maximize the likelihood of the training data , which is calculated with the following equation. Figure 3 shows a feature forest representing the chart in Figure 2. Each node corresponds to either a lexical entry or a tuple of where 97 Bigram model The second model is a log-linear model with only one feature that corresponds to bigram probabilities for adjacent word-pairs in the sentence. We estimated a bigram language model using a part of the British National Corpus as training data2. In the chart each edge is identified with the first and last word in the phrase as we</context>
<context position="18557" citStr="Miyao and Tsujii, 2005" startWordPosition="3200" endWordPosition="3203">lity of a bigram. Contrary to the method of Velldal and Oepen (2005) where the input is a set of sentences and is computed on a whole sentence, we computed on each phrase as Langkilde (2000) did The language model can be extended to -gram if each edge holds last words although the number of edges increase. Syntax model The third model incorporates a variety of syntactic features and lexical features where is computed as . The feature set consists of combinations of atomic features shown in Table 1. The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). We defined three types of feature combinations to capture the characteristics of binary and unary rule applications and root edges as described below. An example of extracted features is shown in Figure 4 where “bought the book” is combined with its subject “he”. Since the mother edge is a root edge, two features ( and ) are extracted from this node. In the feature, the phrasal category SYM becomes S (sentence), the head word 2The model estimation was done using the CMU-Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld, 1997). Figure 4: Example of features Table 1: Atom</context>
<context position="26338" citStr="Miyao and Tsujii, 2005" startWordPosition="4512" endWordPosition="4515">cted on the syntax model. The accuracy seems to saturate around 4000 sentences, which indicates that a small training set is enough to train the current syntax model and that future development. Table 4: vs. performance Coverage (%) Time (ms) BLEU 4 66.10 768 0.7685 8 82.91 3359 0.7654 12 87.89 7191 0.7735 16 89.46 11051 0.7738 20 90.56 15530 0.7723 Table 5:vs. performance Coverage (%) Time (ms) BLEU 4.0 78.23 2450 0.7765 6.0 89.56 9083 0.7693 8.0 91.15 19320 0.7697 10.0 89.86 35897 0.7689 we could use an additional feature set to improve the accuracy. Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. When we use more complicated features or train the model with longer sentences, possibly the size of necessary training data will increase. Table 3 shows the performance of syntax models with different feature sets. Each row represents a model where one of the atomic features in Table 1 was removed. The “None” row is the baseline model. The rightmost column represents the difference of the accuracy from the model trained with all features. SYM, LE, and COMMA features had a significant influence on the performance. These results are differe</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Y. Miyao and J. Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>T Ninomiya</author>
<author>J Tsujii</author>
</authors>
<title>Corpusoriented grammar development for acquiring a HeadDriven Phrase Structure Grammar from the Penn Treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04.</booktitle>
<contexts>
<context position="6727" citStr="Miyao et al. (2004)" startWordPosition="1036" endWordPosition="1039">uch higher computational cost. Moreover, using a variety of syntactic features also increases the cost. By representing a set of realizations compactly with a packed forest, we trained the models with rich features on a large corpus using awide-coverage grammar. 2 Background This section describes background of this work including the representation of the input to our generator, the algorithm of chart generation, and probabilistic models for HPSG. 2.1 Predicate-argument structures The grammar we adopted is the Enju grammar, which is an English HPSG grammar extracted from the Penn Treebank by Miyao et al. (2004). In parsing a sentence with the Enju grammar, semantic relations of words is output included in a parse tree. The semantic relations are represented by a set of predicate-argument structures (PASs), which in turn becomes the input to our generator. Figure 1 shows an example input to our generator which corresponds to the sentence “He bought the book.”, which consists of four predicates. REL expresses the base form of the word corresponding to the predicate. INDEX expresses a semantic variable to identify each word in the set of relations. ARG1 and ARG2 express relationships between the predic</context>
<context position="22277" citStr="Miyao et al. (2004)" startWordPosition="3828" endWordPosition="3831">with different features, different corpus sizes, different beam widths, and the distribution of generation time. The bigram model was trained using 100,000 sentences in the BNC. The unigram and syntax model was trained using Section 02-21 of the WSJ portion of the Penn Treebank (39,832 sentences). Section 22 (1,700 sentences) and 23 (2,416 sentences) were used as the development and test data, respectively. Because the generator is still slow to generate long sentences, sentences with more than 20 words were not used. We converted the treebank into HPSG-style derivation trees by the method of Miyao et al. (2004) and extracted the semantic relations, which are used as the inputs to the generator. The sentences where this conversion failed were also eliminated although such sentences were few (about 0.3% of the eliminated data). The resulting training data consisted of 18,052 sentences and the test data consisted of 1,006 sentences. During training, uncovered sentences – where the lexicon does not include the lexical entry to construct correct derivation – were also ignored, while such sentences remained in the test data. The final training data we can utilize consisted of 15,444 sentences. The average</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Y. Miyao, T. Ninomiya, and J. Tsujii. 2004. Corpusoriented grammar development for acquiring a HeadDriven Phrase Structure Grammar from the Penn Treebank. In Proceedings of the IJCNLP-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ninomiya</author>
<author>Y Tsuruoka</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the IWPT’05.</booktitle>
<contexts>
<context position="2642" citStr="Ninomiya et al., 2005" startWordPosition="395" endWordPosition="398">ier to understand than others. In principle, we need to enumerate all alternative realizations in order to estimate a log-linear model for generation. It therefore requires high computational cost to estimate a probabilistic model for a wide-coverage grammar because there are considerable ambiguities and the alternative realizations are hard to enumerate explicitly. Moreover, even after the model has been estimated, to explore all possible candidates in runtime is also expensive. The same problems also arise with HPSG parsing, and recent studies (Tsuruoka et al., 2004; Miyao and Tsujii, 2005; Ninomiya et al., 2005) proposed a number of solutions including the methods of estimating loglinear models using packed forests of parse trees and pruning improbable candidates during parsing. The aim of this paper is to apply these techniques to generation. Since parsing and generation both output the best probable tree under some constraints, we expect that techniques that work effectively in parsing are also beneficial for generation. First, we enabled estimation of log-linear models with less cost by representing a set of generation trees in a packed forest. The forest representation was obtained by adopting ch</context>
</contexts>
<marker>Ninomiya, Tsuruoka, Miyao, Tsujii, 2005</marker>
<rawString>T. Ninomiya, Y. Tsuruoka, Y. Miyao, and J. Tsujii. 2005. Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic HPSG parsing. In Proceedings of the IWPT’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL’01.</booktitle>
<contexts>
<context position="23111" citStr="Papineni et al., 2001" startWordPosition="3967" endWordPosition="3970"> data). The resulting training data consisted of 18,052 sentences and the test data consisted of 1,006 sentences. During training, uncovered sentences – where the lexicon does not include the lexical entry to construct correct derivation – were also ignored, while such sentences remained in the test data. The final training data we can utilize consisted of 15,444 sentences. The average sentence length of the test data was 12.4, which happens to be close to that of Velldal and Oepen (2005) though the test data is different. The accuracy of the generator outputs was evaluated by the BLEU score (Papineni et al., 2001), which is commonly used for the evaluation of machine translation and recently used for the evaluation of generation (Langkilde-Geary, 2002; Velldal and Oepen, 2005). BLEU is the weighted average of n-gram precision against the reference sentence. We used the sentences in the Penn Treebank as the reference sentences. The beam width was increased from to in two steps. The parameters were empirically determined using the development set. All the experiments were conducted on AMD Opteron servers with a 2.0-GHz CPU and 12-GB memory. Table 2 shows the average generation time and the accuracy of th</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the ACL’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1550" citStr="Pollard and Sag, 1994" startWordPosition="228" endWordPosition="231">xported to the task of natural language generation. Additionally, we report empirical evaluation of the performance of several disambiguation models and how the performance changes according to the feature set used in the models and the size of training data. 1 Introduction Surface realization is the final stage of natural language generation which receives a semantic representation and outputs a corresponding sentence where all words are properly inflected and ordered. This paper presents log-linear models to address the ambiguity which arises when HPSG (Head-driven Phrase Structure Grammar (Pollard and Sag, 1994)) is applied to sentence generation. Usually a single semantic representation can be realized as several sentences. For example, consider the following two sentences generated from the same input. The language in the huge new law complicated has muddied the fight. The latter is not an appropriate realization because “complicated” tends to be wrongly interpreted to modify “law”. Therefore the generator needs to select a candidate sentence which is more fluent and easier to understand than others. In principle, we need to enumerate all alternative realizations in order to estimate a log-linear m</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>A Abeille</author>
<author>A K Joshi</author>
</authors>
<title>Parsing strategies with ’lexicalized’ grammars: Application to tree adjoining grammar.</title>
<date>1988</date>
<booktitle>In Proceedings of the COLING 1988,</booktitle>
<pages>578--583</pages>
<contexts>
<context position="5080" citStr="Schabes et al., 1988" startWordPosition="774" endWordPosition="777">re slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentence</context>
</contexts>
<marker>Schabes, Abeille, Joshi, 1988</marker>
<rawString>Y. Schabes, A. Abeille, and A. K. Joshi. 1988. Parsing strategies with ’lexicalized’ grammars: Application to tree adjoining grammar. In Proceedings of the COLING 1988, pages 578–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<booktitle>In Proceedings of the CoNLL’02.</booktitle>
<publisher>MIT</publisher>
<contexts>
<context position="5295" citStr="Steedman, 2000" startWordPosition="809" endWordPosition="810">ethod extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentences the grammar allows, the ranking modFigure 1: PASs for “He bought the book.” ule (Velldal and Oepen, 2005) selects the best one using alog-linear model. Their model is trained using only 864 sentences where all rea</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The Syntactic Process. MIT Press. K. Toutanova and C. D. Manning. 2002. Feature selection for a rich HPSG grammar using decision trees. In Proceedings of the CoNLL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
</authors>
<title>Iterative CKY parsing for Probabilistic Context-Free Grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP’04.</booktitle>
<contexts>
<context position="3696" citStr="Tsuruoka and Tsujii, 2004" startWordPosition="558" endWordPosition="562">e enabled estimation of log-linear models with less cost by representing a set of generation trees in a packed forest. The forest representation was obtained by adopting chart generation (Kay, 1996; Car93 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93–102, Vancouver, October 2005. c�2005 Association for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-world texts. Through a series of experiments, we compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences betw</context>
<context position="20439" citStr="Tsuruoka and Tsujii, 2004" startWordPosition="3519" endWordPosition="3522">and the bigram model. This model is obtained by simply adding the bigram feature to the syntax model. 4 Iterative beam search For efficient statistical generation with a widecoverage grammar, we reduce the search space by pruning edges during generation. We use beam search where edges with low FOMs are pruned during generation. We use two parameters, and: in each cell, the generator prunes except for top edges, and edges whose FOMs are lower than that of the top edgeare also pruned. Another technique for achieving efficiency is iterative generation which is adopted from iterative CKY parsing (Tsuruoka and Tsujii, 2004). When beam width is too narrow, correct edges to constitute a correct sentence may be discarded during genS,bought, transitive � HEAD verb:e � �� SUBCAT �� � HEAD noun:x � �� SUBCAT �� � HEAD v er b e : � �� SUBCAT NP:x �� he bought the book subject -head , 1 no , , 1, NP, he,p roper -noun , 3 VP , , bought, transitive 98 Table 2: Averaged generation time and accuracy by four models Model Baseline Bigram Syntax Combined Coverage (%) 91.15 90.15 90.75 90.56 Time (ms) 3512 4085 3821 4315 BLEU (89 sentences) 0.7776 0.7503 0.8195 0.7359 (179 sentences) 0.5544 0.6323 0.7339 0.7305 (326 sentences) </context>
</contexts>
<marker>Tsuruoka, Tsujii, 2004</marker>
<rawString>Y. Tsuruoka and J. Tsujii. 2004. Iterative CKY parsing for Probabilistic Context-Free Grammars. In Proceedings of the IJCNLP’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Towards efficient probabilistic HPSG parsing: integrating semantic and syntactic preference to guide the parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04 Workshop on Beyond Shallow Analyses.</booktitle>
<contexts>
<context position="2594" citStr="Tsuruoka et al., 2004" startWordPosition="387" endWordPosition="390">candidate sentence which is more fluent and easier to understand than others. In principle, we need to enumerate all alternative realizations in order to estimate a log-linear model for generation. It therefore requires high computational cost to estimate a probabilistic model for a wide-coverage grammar because there are considerable ambiguities and the alternative realizations are hard to enumerate explicitly. Moreover, even after the model has been estimated, to explore all possible candidates in runtime is also expensive. The same problems also arise with HPSG parsing, and recent studies (Tsuruoka et al., 2004; Miyao and Tsujii, 2005; Ninomiya et al., 2005) proposed a number of solutions including the methods of estimating loglinear models using packed forests of parse trees and pruning improbable candidates during parsing. The aim of this paper is to apply these techniques to generation. Since parsing and generation both output the best probable tree under some constraints, we expect that techniques that work effectively in parsing are also beneficial for generation. First, we enabled estimation of log-linear models with less cost by representing a set of generation trees in a packed forest. The f</context>
</contexts>
<marker>Tsuruoka, Miyao, Tsujii, 2004</marker>
<rawString>Y. Tsuruoka, Y. Miyao, and J. Tsujii. 2004. Towards efficient probabilistic HPSG parsing: integrating semantic and syntactic preference to guide the parsing. In Proceedings of the IJCNLP-04 Workshop on Beyond Shallow Analyses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Velldal</author>
<author>S Oepen</author>
</authors>
<title>Maximum entropy models for realization ranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the MTSummit’05.</booktitle>
<contexts>
<context position="4026" citStr="Velldal and Oepen, 2005" startWordPosition="609" endWordPosition="612">ssociation for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-world texts. Through a series of experiments, we compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizatio</context>
<context position="5787" citStr="Velldal and Oepen, 2005" startWordPosition="887" endWordPosition="890">ing a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentences the grammar allows, the ranking modFigure 1: PASs for “He bought the book.” ule (Velldal and Oepen, 2005) selects the best one using alog-linear model. Their model is trained using only 864 sentences where all realizations can be explicitly enumerated. As a grammar is extended to support more linguistic phenomena and to achieve higher coverage, the number of alternative realizations increases and the enumeration requires much higher computational cost. Moreover, using a variety of syntactic features also increases the cost. By representing a set of realizations compactly with a packed forest, we trained the models with rich features on a large corpus using awide-coverage grammar. 2 Background Thi</context>
<context position="12941" citStr="Velldal and Oepen (2005)" startWordPosition="2165" endWordPosition="2168">ASH    N P y: e x  HEAD noun :x   SUBCAT  H EAD noun :y y  S U BCAT det   bought the he book REL INDEX ARG1 ARG2 TENSE R EL book y  INDEX  HEAD noun :y SUBCAT the book buy e x y the � � �� � z REL INDEX iARG1 y past  REL  INDEX he   x Lexical edges use multiple overlapping features without assuming independence among features, the models are suitable for HPSG parsing where feature structures with complicated constraints are involved and dividing such constraints into independent features is difficult. Log-linear models have also been used for HPSG generation by Velldal and Oepen (2005). In their method, the probability of a realization given a semantic representation is formulated as where is a feature function observed in ,is the weight of , and represents the set of all possible realizations of . To estimate , pairs of are needed, where is the most preferred realization for . Their method first automatically generates a paraphrase treebank, where are enumerated. Then, a log-linear model is trained with this treebank, i.e., each is estimated so as to maximize the likelihood of training data. As well as features used in their previous work on statistical parsing (Toutanova </context>
<context position="16567" citStr="Velldal and Oepen (2005)" startWordPosition="2844" endWordPosition="2847">(s). Nodes connected by solid arrows represent AND-nodes corresponding to the daughters of the parent node. By using feature forests, we can efficiently pack the node generated more than once in the set of trees. For example, the nodes corresponding to “the book” in “He bought the book.” and “the book he bought” are identical and described only once in the forest. The merits of using forest representations in generation instead of lattices or simple enumeration are discussed thoroughly by Langkilde (2000). 3.2 Model variation We implemented and compared four different disambiguation models as Velldal and Oepen (2005) did. Throughout the models, we assigned a score called figure-of-merit (FOM) on each edge and calculated the FOM of a mother edge by dynamic programming. FOM represents the log probability of an edge which is not normalized. H EA D Baseline model We started with a simple baseline model, , where is a PAS in the input semantic representation and is a lexical entry assigned to . The FOM of the mother edge is computed simply as . All the other models use this model as a reference distribution (Miyao and Tsujii, 2005), i.e., is estimated to maximize the likelihood of the training data , which is c</context>
<context position="18002" citStr="Velldal and Oepen (2005)" startWordPosition="3104" endWordPosition="3107">a log-linear model with only one feature that corresponds to bigram probabilities for adjacent word-pairs in the sentence. We estimated a bigram language model using a part of the British National Corpus as training data2. In the chart each edge is identified with the first and last word in the phrase as well as its feature structure and covered relations. When two edges are combined, is computed as , where is the weight of the bigram feature, is the last word of the left daughter, is the first word of the right daughter, and represents a log probability of a bigram. Contrary to the method of Velldal and Oepen (2005) where the input is a set of sentences and is computed on a whole sentence, we computed on each phrase as Langkilde (2000) did The language model can be extended to -gram if each edge holds last words although the number of edges increase. Syntax model The third model incorporates a variety of syntactic features and lexical features where is computed as . The feature set consists of combinations of atomic features shown in Table 1. The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). We defined three types of feature combinati</context>
<context position="22982" citStr="Velldal and Oepen (2005)" startWordPosition="3944" endWordPosition="3947">tor. The sentences where this conversion failed were also eliminated although such sentences were few (about 0.3% of the eliminated data). The resulting training data consisted of 18,052 sentences and the test data consisted of 1,006 sentences. During training, uncovered sentences – where the lexicon does not include the lexical entry to construct correct derivation – were also ignored, while such sentences remained in the test data. The final training data we can utilize consisted of 15,444 sentences. The average sentence length of the test data was 12.4, which happens to be close to that of Velldal and Oepen (2005) though the test data is different. The accuracy of the generator outputs was evaluated by the BLEU score (Papineni et al., 2001), which is commonly used for the evaluation of machine translation and recently used for the evaluation of generation (Langkilde-Geary, 2002; Velldal and Oepen, 2005). BLEU is the weighted average of n-gram precision against the reference sentence. We used the sentences in the Penn Treebank as the reference sentences. The beam width was increased from to in two steps. The parameters were empirically determined using the development set. All the experiments were condu</context>
<context position="24305" citStr="Velldal and Oepen, 2005" startWordPosition="4167" endWordPosition="4171">on time and the accuracy of the models presented in Section 3. The generation time includes time for the input for which the generator could not output a sentence, while the accuracy was calculated only in the case of successful generation. All models succeeded in generation for over 90% of the test data. Contrary to the result of the Velldal and Oepen (2005), the syntax model outperformed the combined model. We observed the same result when we varied the parameters for beam thresholding. This is possibly just because the language model was not trained enough as that of the previous research (Velldal and Oepen, 2005) where the model was 4-gram and trained with the entire BNC3. 3We could not use the entire corpus for training because of a problem in implementation. This problem will be fixed in the 99 1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 0 2000 4000 6000 8000 10000 12000 14000 16000 Size of training data Figure 5: Size of training data vs. performance Table 3: Feature set vs. performance Feature BLEU diff. All 0.7734 -COMMA 0.7492 -0.0242 -DIST 0.7702 -0.0032 -LE 0.7423 -0.0311 -RULE 0.7709 -0.0025 -SPAN 0.7640 -0.0094 -SYM 0.7400 -0.0334 -WORD 0.7610 -0.0124 None 0.5959 -0.1775 Although the accu</context>
</contexts>
<marker>Velldal, Oepen, 2005</marker>
<rawString>E. Velldal and S. Oepen. 2005. Maximum entropy models for realization ranking. In Proceedings of the MTSummit’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>J Baldridge</author>
</authors>
<title>Adapting chart realization to CCG.</title>
<date>2003</date>
<booktitle>In Proceedings of the EWNNLG’03.</booktitle>
<contexts>
<context position="5210" citStr="White and Baldridge (2003)" startWordPosition="794" endWordPosition="797"> a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentences the grammar allows, the ranking modFigure 1: PASs for “He bought the book.” ule (Velldal and Oepen, 2005) selects the best one u</context>
<context position="8200" citStr="White and Baldridge (2003)" startWordPosition="1311" endWordPosition="1314">  R EL th e    INDEX z   y  ARG1  R EL h e   INDEX x   R EL book  y  INDEX  ARG1 x  94 away superficial differences, generation from a set of PASs contains ambiguities in the order of modifiers like the example in Section 1 or the syntactic categories of phrases. For example, the PASs in Figure 1 can generate the NP, “the book he bought.” When processing the input PASs, we split a single PAS into a set of relations like (1) representing the first PAS in Figure 1. (1) This representation is very similar to the notion of HLDS (Hybrid Logic Dependency Semantics) employed by White and Baldridge (2003), which is a related notion to MRS (Minimal Recursion Semantics) employed by Carroll et al. (1999). The most significant difference between our current input representation (not PAS itself) and the other representations is that each word corresponds to exactly one PAS while words like infinitival “to” have no semantic relations in HLDS. This means that “The book was bought by him.” is not generated from the same PASs as Figure 1 because there must be the PASs for “was” and “by” to generate the sentence. We currently adopt this constraint for simple implementation, but it is possible to use the</context>
</contexts>
<marker>White, Baldridge, 2003</marker>
<rawString>M. White and J. Baldridge. 2003. Adapting chart realization to CCG. In Proceedings of the EWNNLG’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
</authors>
<title>Reining in CCG chart realization.</title>
<date>2004</date>
<booktitle>In Proceedings of the INLG’04.</booktitle>
<contexts>
<context position="5449" citStr="White, 2004" startWordPosition="831" endWordPosition="832"> lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentences the grammar allows, the ranking modFigure 1: PASs for “He bought the book.” ule (Velldal and Oepen, 2005) selects the best one using alog-linear model. Their model is trained using only 864 sentences where all realizations can be explicitly enumerated. As a grammar is extended to support more linguistic phenomena and to achieve higher coverage, the number of altern</context>
<context position="11297" citStr="White (2004)" startWordPosition="1832" endWordPosition="1833">es an NP with an index , we can find the required edges efficiently by checking the edges indexed with . The other is prohibiting proliferation of grammatically correct, but unusable sub-phrases. During generating the sentence “Newspaper reports said that the tall young Polish athlete ran fast”, subphrases with incomplete modifiers such as “the tall young athlete” or “the young Polish athlete” do not construct the final output, but slow down the generation because they can be combined with the rest of the input to construct grammatically correct phrases or sentences. Carroll et al. (1999) and White (2004) proposed different algorithms to address the same problem. We adopted Kay’s simple solution in the current ongoing work, but logical form chunking proposed by White is also applicable to our system. 2.3 Probabilistic models for generation with HPSG Some existing studies on probabilistic models for HPSG parsing (Malouf and van Noord, 2004; Miyao and Tsujii, 2005) adopted log-linear models (Berger et al., 1996). Since log-linear models allow us to &apos;To introduce an edge with no semantic relations as mentioned in the previous section, we need to combine the edges with edges having no relations. 9</context>
</contexts>
<marker>White, 2004</marker>
<rawString>M. White. 2004. Reining in CCG chart realization. In Proceedings of the INLG’04.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>