<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<title confidence="0.994251">
Fast Statistical Parsing with Parallel Multiple Context-Free Grammars
</title>
<author confidence="0.988316">
Krasimir Angelov and Peter Ljungl¨of
</author>
<affiliation confidence="0.995382">
University of Gothenburg and Chalmers University of Technology
</affiliation>
<address confidence="0.495007">
G¨oteborg, Sweden
</address>
<email confidence="0.961244">
krasimir@chalmers.se
peter.ljunglof@cse.gu.se
</email>
<sectionHeader confidence="0.997198" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999564">
We present an algorithm for incremental
statistical parsing with Parallel Multiple
Context-Free Grammars (PMCFG). This
is an extension of the algorithm by An-
gelov (2009) to which we added statisti-
cal ranking. We show that the new al-
gorithm is several times faster than other
statistical PMCFG parsing algorithms on
real-sized grammars. At the same time the
algorithm is more general since it supports
non-binarized and non-linear grammars.
We also show that if we make the
search heuristics non-admissible, the pars-
ing speed improves even further, at the risk
of returning sub-optimal solutions.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999856607843138">
In this paper we present an algorithm for incre-
mental parsing using Parallel Multiple Context-
Free Grammars (PMCFG) (Seki et al., 1991). This
is a non context-free formalism allowing disconti-
nuity and crossing dependencies, while remaining
with polynomial parsing complexity.
The algorithm is an extension of the algorithm
by Angelov (2009; 2011) which adds statistical
ranking. This is a top-down algorithm, shown by
Ljungl¨of (2012) to be similar to other top-down al-
gorithms (Burden and Ljungl¨of, 2005; Kanazawa,
2008; Kallmeyer and Maier, 2009). None of the
other top-down algorithms are statistical.
The only statistical PMCFG parsing algorithms
(Kato et al., 2006; Kallmeyer and Maier, 2013;
Maier et al., 2012) all use bottom-up parsing
strategies. Furthermore, they require the gram-
mar to be binarized and linear, which means that
they only support linear context-free rewriting sys-
tems (LCFRS). In contrast, our algorithm natu-
rally supports the full power of PMCFG. By lift-
ing these restrictions, we make it possible to ex-
periment with novel grammar induction methods
(Maier, 2013) and to use statistical disambiguation
for hand-crafted grammars (Angelov, 2011).
By extending the algorithm with a statistical
model, we allow the parser to explore only parts
of the search space, when only the most proba-
ble parse tree is needed. Our cost estimation is
similar to the estimation for the Viterbi probabil-
ity as in Stolcke (1995), except that we have to
take into account that our grammar is not context-
free. The estimation is both admissible and mono-
tonic (Klein and Manning, 2003) which guaran-
tees that we always find a tree whose probability
is the global maximum.
We also describe a variant with a non-
admissible estimation, which further improves the
efficiency of the parser at the risk of returning a
suboptimal parse tree.
We start with a formal definition of a weighted
PMCFG in Section 2, and we continue with a pre-
sentation of our algorithm by means of a weighted
deduction system in Section 3. In Section 4,
we prove that our estimations are admissible and
monotonic. In Section 5 we calculate an esti-
mate for the minimal inside probability for every
category, and in Section 6 we discuss the non-
admissible heuristics. Sections 7 and 8 describe
the implementation and our evaluation, and the fi-
nal Section 9 concludes the paper.
</bodyText>
<sectionHeader confidence="0.996735" genericHeader="introduction">
2 PMCFG definition
</sectionHeader>
<bodyText confidence="0.9999797">
Our definition of weighted PMCFG (Definition 1)
is the same as the one used by Angelov (2009;
2011), except that we extend it with weights for
the productions. This definition is also similar to
Kato et al (2006), with the small difference that we
allow non-linear functions.
As an illustration for PMCFG parsing, we use
a simple grammar (Figure 1) which can generate
phrases like “both black and white” and “either
red or white” but rejects the incorrect combina-
</bodyText>
<page confidence="0.976179">
368
</page>
<note confidence="0.997933">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 368–376,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.377905333333333">
Definition 1
A parallel multiple context-free grammar is a tuple
G = (N, T, F, P, 5, d, di, r, a) where:
</figure>
<listItem confidence="0.996963818181818">
• N is a finite set of categories and a positive in-
teger d(A) called dimension is given for each
AEN.
• T is a finite set of terminal symbols which is dis-
joint with N.
• F is a finite set offunctions where the arity a(f)
and the dimensions r(f) and di(f) (1 &lt; i &lt;
a(f)) are given for every f E F. For every
positive integer d, (T*)d denote the set of all d-
tuples of strings over T. Each function f E F
is a total mapping from (T*)d1(f) x(T*)d2(f) x
</listItem>
<equation confidence="0.794422666666667">
� � �x (T*)da(f)(f) to (T*)r(f), defined as:
f := (α1, α2, ... , αr(f))
Here αi is a sequence of terminals and (k; l)
</equation>
<construct confidence="0.729450727272727">
pairs, where 1 &lt; k &lt; a(f) is called argument
index and 1 &lt; l &lt; dk(f) is called constituent
index.
• P is a finite set ofproductions of the form:
A -w—* f[A1, A2,..., Aa(f)]
where A E N is called result category,
A1, A2,..., Aa(f) E N are called argument
categories, f E F is the function symbol and
w &gt; 0 is a weight. For the production to be
well formed the conditions di(f) = d(Ai) (1 &lt;
i &lt; a(f)) and r(f) = d(A) must hold.
</construct>
<listItem confidence="0.68629">
• 5 is the start category and d(5) = 1.
</listItem>
<bodyText confidence="0.969893214285714">
tions both-or and either-and. We avoid these com-
binations by coupling the right pairs of words in a
single function, i.e. we have the abstract conjunc-
tions both and and either or which are linearized
as discontinuous phrases. The phrase insertion it-
self is done in the definition of conjA. It takes the
conjunction as its first argument, and it uses (1; 1)
and (1; 2) to insert the first and the second con-
stituent of the argument at the right places in the
complete phrase.
A tree of function applications that yelds a com-
plete phrase is the parse tree for the phrase. For
instance, the phrase “both red and either black or
white” is represented by the tree:
</bodyText>
<equation confidence="0.935723882352941">
(conjA both and red
(conjA either or black white))
A w1 ��conjA[Conj, A, A]
A w2 ��black[]
A w3
�� white[]
A w4 ��red[]
ConjConjw5
�� both and[]
w6
�� either or[]
conjA := ((1;1)(2; 1)(1; 2)(3; 1))
black := (”black”)
white := (”white”)
red := (”red”)
both and := (”both”, ”and”)
either or := (”either”, ”or”)
</equation>
<figureCaption confidence="0.999156">
Figure 1: Example Grammar
</figureCaption>
<bodyText confidence="0.943295666666667">
The weight of a tree is the sum of the weights for
all functions that are used in it. In this case the
weight for the example is w1+w5+w4+w1+w6+
w2 + w3. If there are ambiguities in the sentence,
the algorithm described in Section 3 always finds
a tree which minimizes the weight.
Usually the weights for the productions are log-
arithmic probabilities, i.e. the weight of the pro-
duction A —* f[B] is:
</bodyText>
<equation confidence="0.988508">
w = — log P(A — f[�B]  |A)
</equation>
<bodyText confidence="0.999932">
where P(A f[B]  |A) is the probability to
choose this production when the result category is
fixed. In this case the probabilities for all produc-
tions with the same result category sum to one:
</bodyText>
<equation confidence="0.861515">
� e−w = 1
Aw��f[~B] EP
</equation>
<bodyText confidence="0.99934">
However, the parsing algorithm does not depend
on the probabilistic interpretation of the weights,
so the same algorithm can be used with any other
kind of weights.
</bodyText>
<sectionHeader confidence="0.977494" genericHeader="method">
3 Deduction System
</sectionHeader>
<bodyText confidence="0.999024875">
We define the algorithm as weighted deduction
system (Nederhof, 2003) which generalizes An-
gelov’s system.
A key feature in his algorithm is that the ex-
pressive PMCFG is reduced to a simple context-
free grammar which is extended dynamically at
parsing time in order to account for context de-
pendent features in the original grammar. This
</bodyText>
<page confidence="0.998494">
369
</page>
<bodyText confidence="0.999067777777778">
can be exemplified with the grammar in Fig-
ure 1, where there are two productions for cat-
egory Conj. Given the phrase “both black and
white”, after accepting the token both, only the
production Conj �5
−→ both and[] can be applied
for parsing the second part of the conjunction.
This is achieved by generating a new category
Conj2 which has just a single production:
</bodyText>
<equation confidence="0.997349">
Conj2 −→ both and[]
�5 (1)
</equation>
<bodyText confidence="0.9951912">
The parsing algorithm is basically an extension of
Earley’s (1970) algorithm, except that the parse
items in the chart also keep track of the categories
for the arguments. In the particular case, the cor-
responding chart item will be updated to point to
Conj2 instead of Conj. This guarantees that only
and will be accepted as a second constituent after
seeing that the first constituent is both.
Now since the set of productions is dynamic, the
parser must keep three kinds of items in the chart,
instead of two as in the Earley algorithm:
Productions The parser maintains a dynamic set
with all productions that are derived during the
parsing. The initial state is populated with the pro-
ductions from the set P in the grammar.
Active Items The active items play the same
role as the active items in the Earley algorithm.
They have the form:
[k� A T−→ f[B]; l : α • Q; wz; wo]
and represent the fact that a constituent l of a cat-
egory A has been partially recognized from posi-
tion j to k in the sentence. Here A T−→ f[B] is
the production and the concatenation αQ is the se-
quence of terminals and hk; ri pairs which defines
the l-th constituent of function f. The dot • be-
tween α and Q separates the part of the constituent
that is already recognized from the part which is
still pending. Finally wz and wo are the inside and
outside weights for the item.
Passive Items The passive items are of the form:
</bodyText>
<equation confidence="0.977668">
[k� A; l; ˆA]
</equation>
<bodyText confidence="0.999902166666667">
and state that a constituent with index l from cate-
gory A was recognized from position j to position
k in the sentence. As a consequence the parser has
created a new category ˆA. The set of productions
derived for Aˆ compactly records all possible ways
to parse the j − k fragment.
</bodyText>
<subsectionHeader confidence="0.9954">
3.1 Inside and outside weights
</subsectionHeader>
<bodyText confidence="0.999986357142857">
The inside weight wz and the outside weight wo in
the active items deserve more attention since this
is the only difference compared to Angelov (2009;
2011). When the item is complete, it will yield the
forest of all trees that derive the sub-string cov-
ered by the item. For example, when the first con-
stituent for category Conj is completely parsed,
the forest will contain the single production in (1).
The inside weight for the active item is the cur-
rently best known estimation for the lowest weight
of a tree in the forest. The trees yielded by the item
do not cover the whole sentence however. Instead,
they will become part of larger trees that cover the
whole sentence. The outside weight is the esti-
mation for the lowest weight for an extension of a
tree to a full tree. The sum wz + wo estimates the
weight of the full tree.
Before turning to the deduction rules we also
need a notation for the lowest possible weight for
a tree of a given category. If A ∈ N is a category
then wA will denote the lowest weight that a tree of
category A can have. For convenience, we also use
wB as a notation for the sum Ez wBi of the weight
of all categories in the vector B. If the category
A is defined in the grammar then we assume that
the weight is precomputed as described in Section
5. When the parser creates the category, it will
compute the weight dynamically.
</bodyText>
<subsectionHeader confidence="0.999592">
3.2 Deduction rules
</subsectionHeader>
<bodyText confidence="0.99995155">
The deduction rules are shown in Figure 2. Here
the assumption is that the active items are pro-
cessed in the order of increasing wz + wo weight.
In the actual implementation we put all active
items in a priority queue and we always take first
the item with the lowest weight. We never throw
away items but the processing of items with very
high weight might be delayed indefinitely or they
may never be processed if the best tree is found
before that. Furthermore, we think of the deduc-
tion system as a way do derive a set of items, but in
our case we ignore the weights when we consider
whether two active items are the same. In this way,
every item is derived only once and the weights for
the active items are computed from the weights of
the first antecedents that led to its derivation.
Finally, we use two more notations in the rules:
rhs(g, r) denotes constituent with index r in func-
tion g; and ωk denotes the k-th token in the sen-
tence.
</bodyText>
<page confidence="0.829408">
370
</page>
<equation confidence="0.950196">
INITIAL PREDICT
S w −→ f[~B] S = start category, γ = rhs(f,1)
[0S w−→ f[~B];1 : • γ; w + w~B; 0]
PREDICT
Bd −→ g[ ~C] [k
w&apos; j A w2
−→ f[~B]; l : α • hd; ri β; wi; wo]
w&apos; f γ = rhs(g, r)
[kBd−→ g[C]; r : • γ; w1 + w~C; wi − wBd + wo]
SCAN
[kjAw−→ f[~B];l:α•sβ;wi;wo]
s = ωk+1
[k+1
j A w −→ f[~B]; l : α s • β; wi; wo]
COMPLETE
[kjAw−→ f[~B];l:α•;wi;wo]
Aˆ w−→ f[~B] [kj A;l; ˆA]
COMBINE
[ujA w−→ f[~B]; l : α • hd; ri β; wi; wo] [kuBd;r; ˆBd]
[kj A w−→ f[~B{d := ˆBd}]; l : α hd; ri • β; wi + wBd − wBd; wo]
</equation>
<figureCaption confidence="0.94507">
Figure 2: Deduction Rules
</figureCaption>
<bodyText confidence="0.994201886792453">
Aˆ = (A, l, j, k), w A� = wi
The first rule on Figure 2 is INITIAL PREDICT and
here we predict the initial active items from the
productions for the start category S. Since this
is the start category, we set the outside weight to
zero. The inside weight is equal to the sum of the
weight w for the production and the lowest pos-
sible weight w~B for the vector of arguments ~B.
The reason is that despite that we do not know the
weight for the final tree yet, it cannot be lower than
w +w~B since w~B is the lowest possible weight for
the arguments of function f.
The interaction between inside and outside
weights is more interesting in the PREDICT rule.
Here we have an item where the dot is before hd; ri
and from this we must predict one item for each
production Bd w&apos;→ g[~C] of category Bd. The in-
side weight for the new item is w1 + w~C for the
same reasons as for the INITIAL PREDICT rule. The
outside weight however is not zero because the
new item is predicted from another item. The in-
side weight for the active item in the antecedents
is now part of the outside weight of the new item.
We just have to subtract wBd from wi because the
new item is going to produce a new tree which will
replace the d-th argument of f. For this reason the
estimation for the outside weight is wi−wBd +wo,
where we also added the outside weight for the an-
tecedent item.
In the SCAN rule, we just move the dot past a
token, if it matches the current token ωk+1. Both
the inside and the outside weights are passed un-
touched from the antecedent to the consequent.
In the COMPLETE rule, we have an item where the
dot has reached the end of the constituent. Here we
generate a new category Aˆ which is unique for the
combination (A, l, j, k), and we derive the produc-
tion Aˆ w−→ f[~B] for it. We set the weight wA� for Aˆ
to be equal to wi and in Section 4, we will prove
that this is indeed the lowest weight for a tree of
category ˆA.
In the last rule COMBINE, we combine an active
item with a passive item. The outside weight wo
for the new active item remains the same. How-
ever, we must update the inside weight since we
have replaced the d-th argument in B~ with the
newly generated category ˆBd. The new weight is
wi + w Bd − wBd, i.e. we add the weight for the
new category and we subtract the weight for the
previous category Bd.
Now for the correctness of the weights we must
prove that the estimations are both admissible and
monotonic.
</bodyText>
<sectionHeader confidence="0.995079" genericHeader="method">
4 Admissibility and Monotonicity
</sectionHeader>
<bodyText confidence="0.99974025">
We will first prove that the weights grow mono-
tonically, i.e. if we derive one active item from
another then the sum wi + wo for the new item is
always greater or equal to the sum for the previous
</bodyText>
<page confidence="0.994117">
371
</page>
<bodyText confidence="0.999383">
item. PREDICT and COMBINE are the only two rules
with an active item both in the antecedents and in
the consequents.
Note that in PREDICT we choose one particular
production for category Bd. We know that the
lowest possible weight of a tree of this category
is wBd. If we restrict the set of trees to those
that not only have the same category Bd but also
use the same production Bd −→ g[~C] on the top
</bodyText>
<equation confidence="0.483484">
w&apos;
</equation>
<bodyText confidence="0.998729">
level, then the best weight for such a tree will be
w1 + we. According to the definition of wBd, it
must follow that:
</bodyText>
<equation confidence="0.832536">
w1 + we ≥ wBd
From this we can trivially derive that:
(w1 + we) + (wz − wBd + wo) ≥ wz + wo
</equation>
<bodyText confidence="0.879071">
which is the monotonicity condition for rule
PREDICT. Similarly in rule COMBINE, the condition:
</bodyText>
<equation confidence="0.993861">
w ˆ &gt;w
Bd — Bd
</equation>
<bodyText confidence="0.997615566666667">
must hold because the forest of trees for ˆBd is in-
cluded in the forest for Bd. From this we conclude
the monotonicity condition:
(wz + w ˆBd − wBd) + wo ≥ wz + wo
The last two inequalities are valid only if we can
correctly compute w ˆBd for a dynamically gener-
ated category ˆBd. This happens in rule COMPLETE,
where we have a complete active item with a cor-
rectly computed inside weight wz. Since we pro-
cess the active items in the order of increasing
wz + wo weight and since we create Aˆ when we
find the first complete item for category A, it is
guaranteed that at this point we have an item with
minimal wz + wo value. Furthermore, all items
with the same result category A and the same start
position j must have the same outside weight. It
follows that when we create Aˆ we actually do it
from an active item with minimal inside weight
wz. This means that it is safe to assign that w Aˆ =
wz.
It is also easy to see that the estimation is ad-
missible. The only places where we use estima-
tions for the unseen parts of the sentence is in the
rules INITIAL PREDICT and PREDICT where we use
the weights wB and we which may include com-
ponents corresponding to function argument that
are not seen yet. However by definition it is not
possible to build a tree with weight lower than the
weight for the category. This means that the esti-
mation is always admissible.
</bodyText>
<sectionHeader confidence="0.998622" genericHeader="method">
5 Initial Estimation
</sectionHeader>
<bodyText confidence="0.999913368421053">
The minimal weight for a dynamically created cat-
egory is computed by the parser, but we must ini-
tialize the weights for the categories that are de-
fined in the grammar. The easiest way is to just
set all weights to zero, and this is safe since the
weights for the predefined categories are used only
as estimations for the yet unseen parts of the sen-
tence. Essentially this gives us a statistical parser
which performs Dijkstra search in the space of all
parse trees. Any other reasonable weight assign-
ment will give us an A∗ algorithm (Hart et al.,
1968).
In general it is possible to devise different
heuristics which will give us different improve-
ments in the parsing time. In our current im-
plementation of the parser we use a weight as-
signment which considers only the already known
probabilities for the productions in the grammar.
The weight for a category A is computed as:
</bodyText>
<equation confidence="0.999029">
wA = min (w + wB)
Aw−→f[B] ∈ P
</equation>
<bodyText confidence="0.998974428571429">
Here the sum w + wB is the minimal weight for
a tree constructed with the production A w−→ f[~B]
at the root. By taking the minimum over all pro-
ductions for A, we get the corresponding weight
wA. This is a recursive equation since its right-
hand side contains the value wB which depends on
the weights for the categories in ~B. It might hap-
pen that there are mutually dependent categories
which will lead to a recursion in the equation.
The solution is found with iterative assignments
until a fixed point is reached. In the beginning we
assign wA = 0 for all categories. After that we re-
compute the new weights with the equation above
until we reach a fixed point.
</bodyText>
<sectionHeader confidence="0.998804" genericHeader="method">
6 Non-admissible heuristics
</sectionHeader>
<bodyText confidence="0.99988825">
The set of active items is kept in a priority queue
and at each step we process the item with the low-
est weight. However, when we experimented with
the algorithm we noticed that most of the time the
item that is selected would eventually contribute
with an alternative reading of the sentence but not
to the best parse. What happens is that despite that
there are already items ending at position k in the
sentence, the current best item might have a span
i − j where j &lt; k. The parser then picks the
best item only to discover later that the item be-
came much heavier until it reached the span i − k.
</bodyText>
<page confidence="0.99662">
372
</page>
<bodyText confidence="0.999913263157895">
This suggests that when we compare the weights
of items with different end positions, then we must
take into account the weight that will be accumu-
lated by the item that ends earlier until the two
items align at the same end position.
We use the following heuristic to estimate the
difference. The first time when we extend an
item from position i to position i + 1, we record
the weight increment wΔ(i + 1) for that position.
The increment wΔ is the difference between the
weights for the best active item reaching position
i + 1 and the best active item reaching position i.
From now on when we compare the weights for
two items xj and xk, with end positions j and k
respectively (j &lt; k), then we always add to the
score wxj of the first item a fraction of the sum of
the increments for the positions between j and k.
In other words, instead of using wxj when com-
paring with wxk, we use
</bodyText>
<equation confidence="0.9901095">
�wxj + h · wΔ(i)
j&lt;i≤k
</equation>
<bodyText confidence="0.999989785714286">
We call the constant h ∈ [0, 1] the “heuristics fac-
tor”. If h = 0, we obtain the basic algorithm that
we described earlier which is admissible and al-
ways returns the best parse. However, the evalua-
tion in Section 8.3 shows that a significant speed-
up can be obtained by using larger values of h.
Unfortunately, if h &gt; 0, we loose some accuracy
and cannot guarantee that the best parse is always
returned first.
Note that the heuristics does not change the
completeness of the algorithm – it will succeed
for all grammatical sentences and fail for all non-
grammatical. But it does not guarantee that the
first parse tree will be the optimal.
</bodyText>
<sectionHeader confidence="0.997081" genericHeader="method">
7 Implementation
</sectionHeader>
<bodyText confidence="0.999978090909091">
The parser is implemented in C and is distributed
as a part of the runtime system for the open-source
Grammatical Framework (GF) programming lan-
guage (Ranta, 2011).1 Although the primary ap-
plication of the runtime system is to run GF appli-
cations, it is not specific to one formalism, and it
can serve as an execution platform for other frame-
works where natural language parsing and gener-
ation is needed.
The GF system is distributed with a library
of manually authored resource grammars (Ranta,
</bodyText>
<footnote confidence="0.929191">
1http://www.grammaticalframework.org/
</footnote>
<bodyText confidence="0.999684888888889">
2009) for over 25 languages, which are used as a
resource for deriving domain specific grammars.
Adding a big lexicon to the resource grammar re-
sults in a highly ambiguous grammar, which can
give rise to millions of trees even for moderately
complex sentences. Previously, the GF system has
not been able to parse with such ambiguous gram-
mars, but with our statistical algorithm it is now
feasible.
</bodyText>
<sectionHeader confidence="0.997505" genericHeader="evaluation">
8 Evaluation
</sectionHeader>
<bodyText confidence="0.999960548387097">
We did an initial evaluation on the GF English re-
source grammar augmented with a large-coverage
lexicon of 40 000 lemmas taken from the Oxford
Advanced Learner’s Dictionary (Mitton, 1986). In
total the grammar has 44 000 productions. The
rule weights were trained from a version of the
Penn Treebank (Marcus et al., 1993) which was
converted to trees compatible with the grammar.
The trained grammar was tested on Penn Tree-
bank sentences of length up to 35 tokens, and the
parsing times were at most 7 seconds per sentence.
This initial test was run on a computer with a 2.4
GHz Intel Core i5 processor with 8 GB RAM. This
result was very encouraging, given the complexity
of the grammar, so we decided to do a larger test
and compare with an existing state-of-the-art sta-
tistical PMCFG parser.
Rparse (Kallmeyer and Maier, 2013) is a an-
other state-of-the-art training and parsing system
for PMCFG.2 It is written in Java and developed at
the Universities of T¨ubingen and D¨usseldorf, Ger-
many. Rparse can be used for training probabilis-
tic PMCFGs from discontinuous treebanks. It can
also be used for parsing new sentences with the
trained grammars.
In our evaluation we used Rparse to extract PM-
CFG grammars from the discontinuous German
Tiger Treebank (Brants et al., 2002). The rea-
son for using this treebank is that the extracted
grammars are non-context-free, and our parser is
specifically made for such grammars.
</bodyText>
<subsectionHeader confidence="0.998989">
8.1 Evaluation data
</subsectionHeader>
<bodyText confidence="0.999001166666667">
In our evaluations we got the same general results
regardless of the size of the grammar, so we only
report the results from one of these runs.
In this particular example, we trained the gram-
mar on 40 000 sentences from the Tiger Treebank
with lengths up to 160 tokens. We evaluated on
</bodyText>
<footnote confidence="0.982203">
2https://github.com/wmaier/rparse
</footnote>
<page confidence="0.994808">
373
</page>
<table confidence="0.9992852">
Count
Training sentences 40 000
Test sentences 4 607
Non-binarized grammar rules 30 863
Binarized grammar rules 26111
</table>
<tableCaption confidence="0.999899">
Table 1: Training and testing data.
</tableCaption>
<bodyText confidence="0.992869125">
4 600 Tiger sentences, with a length of 5–60 to-
kens. The exact numbers are shown in Table 1.
All tests were run on a computer with a 2.3 GHz
Intel Core i7 processor with 16GB RAM.
As a comparison, Maier et al (2012) train on
approximately 15 000 sentences from the Negra
Treebank, and only evaluate on sentences of at
most 40 tokens.
</bodyText>
<subsectionHeader confidence="0.999597">
8.2 Comparison with Rparse
</subsectionHeader>
<bodyText confidence="0.999482727272727">
We evaluated our parser by comparing it with
Rparse’s built-in parser. Note that we are only in-
terested in the efficiency of our implementation,
not the coverage and accuracy of the trained gram-
mar. In the comparison we used only the ad-
missible heuristics, and we did confirm that the
parsers produce optimal trees with exactly the
same weight for the same input.
Rparse extracts grammars in two steps. First
it converts the treebank into a PMCFG, and then
it binarizes that grammar. The binarization pro-
cess uses markovization to improve the precision
and recall of the final grammar (Kallmeyer and
Maier, 2013). We tested both Rparse’s standard
(Kallmeyer and Maier, 2013) and its new im-
proved parsing alogorithm (Maier et al., 2012).
The new algorithm unfortunately works only with
LCFRS grammars with a fan-out &lt; 2 (Maier et al.,
2012).
In this test we used the optimal binarization
method described in Kallmeyer (2010, chapter
7.2). This was the only binarization algorithm in
Rparse that produced a grammar with fan-out &lt; 2.
As can be seen in Figure 3, our parser outper-
forms Rparse for all sentence lengths. For sen-
tences longer than 15 tokens, the standard Rparse
parser needs on average 100 times longer time
than our parser. This difference increases with
sentence length, suggesting that our algorithm has
a better parsing complexity than Rparse.
The PGF parser also outperforms the improved
Rparse parser, but the relative difference seems to
stabilize on a speedup of 10–15 times.
</bodyText>
<figureCaption confidence="0.9986085">
Figure 3: Parsing time (seconds) compared with
Rparse.
Figure 4: Parsing time (seconds) with different
heuristics factors.
</figureCaption>
<subsectionHeader confidence="0.99336">
8.3 Comparing different heuristics
</subsectionHeader>
<bodyText confidence="0.99940955">
In another test we compared the effect of the
heuristic factor h described in Section 6. We used
the same training and testing data as before, and
we tried four different heuristic factors: h = 0,
0.50, 0.75 and 0.95. As mentioned in Section 6,
a factor of 0 gives an admissible heuristics, which
means that the parser is guaranteed to return the
tree with the best weight.
The parsing times are shown in Figure 4. As
can be seen, a higher heuristics factor h gives a
considerable speed-up. For 40 token sentences,
h = 0.50 gives an average speedup of 5 times,
while h = 0.75 is 30 times faster, and h = 0.95 is
almost 500 times faster than using the admissible
heuristics h = 0. This is more clearly seen in Fig-
ure 5, where the parsing times are shown relative
to the admissible heuristics.
Note that all charts have a logarithmic y-axis,
which means that a straight line is equivalent to
exponential growth. If we examine the graph lines
</bodyText>
<figure confidence="0.994722894736842">
0,01
100
0,1
10
1
5 10 15 20 25 30 35 40
Rparse, standard
Rparse, fanout ≤ 2
PGF, admissible
0,01
100
0,1
10
1
5 10 15 20 25 30 35 40 45 50 55 60
PGF, admissible
PGF, h=0.50
PGF, h=0.75
PGF, h=0.95
</figure>
<page confidence="0.869457">
374
</page>
<figureCaption confidence="0.9906025">
Figure 5: Relative parsing time for different values
of h, compared to admissible heuristic.
</figureCaption>
<bodyText confidence="0.99851825">
more closely, we can see that they are not straight.
The closest curves are in fact polynomial, with
a degree of 4–6 depending on the parser and the
value of h.3
</bodyText>
<subsectionHeader confidence="0.976385">
8.4 Non-admissibility and parsing quality
</subsectionHeader>
<bodyText confidence="0.999991538461539">
What about the loss of parsing quality when we
use a non-admissible heuristics? Firstly, as men-
tioned in Section 6, the parser still recognizes ex-
actly the same language as defined by the gram-
mar. The difference is that it is not guaranteed to
return the tree with the best weight.
In our evaluation we saw that for a factor h =
0.50, 80% of the trees are optimal, and only 3%
of the trees have a weight more than 5% from the
optimal weight. The performance gradually gets
worse for higher h, and with h = 0.95 almost 10%
of the trees have a weight more than 20% from the
optimum.
These numbers only show how the parsing qual-
ity degrades relative to the grammar. But since
the grammar is trained from a treebank it is more
interesting to evaluate how the parsing quality on
the treebank sentences is affected when we use a
non-admissible heuristics. Table 2 shows how the
labelled precision and recall are changed with dif-
ferent values for h. The evaluation was done us-
ing the EVALB measure which is implemented in
Rparse (Maier, 2010). As can be seen, a factor of
h = 0.50 only results in a f-score loss of 3 points,
which is arguably not very much. On the other
extreme, for h = 0.95 the f-score drops 14 points.
</bodyText>
<footnote confidence="0.972601">
3The exception is the standard Rparse parser, which has a
polynomial degree of 8.
</footnote>
<table confidence="0.9999322">
Precision Recall F-score
admissible 71.1 67.7 69.3
h = 0.50 68.0 64.9 66.4
h = 0.75 63.0 60.8 61.9
h = 0.95 55.1 55.6 55.3
</table>
<tableCaption confidence="0.998811">
Table 2: Parsing quality for different values of h.
</tableCaption>
<sectionHeader confidence="0.995141" genericHeader="conclusions">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999980875">
The presented algorithm is an important general-
ization of the classical algorithms of Earley (1970)
and Stolcke (1995) for parsing with probabilistic
context-free grammars to the more general formal-
ism of parallel multiple context-free grammars.
The algorithm has been implemented as part of the
runtime for the Grammatical Framework (Ranta,
2011), but it is not limited to GF alone.
</bodyText>
<subsectionHeader confidence="0.920819">
9.1 Performance
</subsectionHeader>
<bodyText confidence="0.999965">
To show the universality of the algorithm, we eval-
uated it on large LCFRS grammars trained from
the Tiger Treebank.
Our parser is around 10–15 times faster than the
latest, optimized version of the Rparse state-of-
the-art parser. This improvement seems to be con-
stant, which means that it can be a consequence
of low-level optimizations. More important is that
our algorithm does not impose any restrictions at
all on the underlying PMCFG grammar. Rparse on
the other hand requires that the grammar is both
binarized and has a fan-out of at most 2.
By using a non-admissible heuristics, the speed
improves by orders of magnitude, at the expense
of parsing quality. This makes it possible to
parse long sentences (more than 50 tokens) in just
around a second on a standard desktop computer.
</bodyText>
<subsectionHeader confidence="0.771789">
9.2 Future work
</subsectionHeader>
<bodyText confidence="0.9966885">
We would like to extend the algorithm to be able to
use lexicalized statistical models (Collins, 2003).
Furthermore, it would be interesting to develop
better heuristics for A∗ search, and to investigate
how to incorporate beam search pruning into the
algorithm.
</bodyText>
<figure confidence="0.985913444444445">
0,001
0,01
0,1
1
5 10 15 20 25 30 35 40
PGF, admissible
PGF, h=0.50
PGF, h=0.75
PGF, h=0.95
</figure>
<page confidence="0.99477">
375
</page>
<sectionHeader confidence="0.998075" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915030612245">
Krasimir Angelov. 2009. Incremental parsing with
parallel multiple context-free grammars. In Pro-
ceedings of EACL 2009, the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Athens, Greece.
Krasimir Angelov. 2011. The Mechanics of the Gram-
matical Framework. Ph.D. thesis, Chalmers Univer-
sity of Technology, Gothenburg, Sweden.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of TLT 2002, the 1st Work-
shop on Treebanks and Linguistic Theories, So-
zopol, Bulgaria.
H˚akan Burden and Peter Ljungl¨of. 2005. Parsing lin-
ear context-free rewriting systems. In Proceedings
of IWPT 2005, the 9th International Workshop on
Parsing Technologies, Vancouver, Canada.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589–637.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Communications of the ACM, 13(2):94–
102.
Peter Hart, Nils Nilsson, and Bertram Raphael. 1968.
A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions of Systems
Science and Cybernetics, 4(2):100–107.
Laura Kallmeyer and Wolfgang Maier. 2009. An in-
cremental Earley parser for simple range concatena-
tion grammar. In Proceedings of IWPT 2009, the
11th International Conference on Parsing Technolo-
gies, Paris, France.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-
driven parsing using probabilistic linear context-
free rewriting systems. Computational Linguistics,
39(1):87–119.
Laura Kallmeyer. 2010. Parsing Beyond Context-Free
Grammars. Springer.
Makoto Kanazawa. 2008. A prefix-correct Earley
recognizer for multiple context-free grammars. In
Proceedings of TAG+9, the 9th International Work-
shop on Tree Adjoining Grammar and Related For-
malisms, T¨ubingen, Germany.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic multiple context-free grammar for RNA
pseudoknot modeling. In Proceedings of TAGRF
2006, the 8th International Workshop on Tree Ad-
joining Grammar and Related Formalisms, Sydney,
Australia.
Dan Klein and Christopher D. Manning. 2003. A∗
parsing: fast exact Viterbi parse selection. In Pro-
ceedings of HLT-NAACL 2003, the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, Edmonton, Canada.
Peter Ljungl¨of. 2012. Practical parsing of parallel
multiple context-free grammars. In Proceedings of
TAG+11, the 11th International Workshop on Tree
Adjoining Grammar and Related Formalisms, Paris,
France.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. PLCFRS parsing revisited: Re-
stricting the fan-out to two. In Proceedings of
TAG+11, the 11th International Workshop on Tree
Adjoining Grammar and Related Formalisms, Paris,
France.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in German. In Proceedings of
SPRML 2010, the 1st Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, Los Ange-
les, California.
Wolfgang Maier. 2013. LCFRS binarization and de-
binarization for directional parsing. In Proceedings
of IWPT 2013, the 13th International Conference on
Parsing Technologies, Nara, Japan.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19:313–330.
Roger Mitton. 1986. A partial dictionary of English in
computer-usable form. Literary &amp; Linguistic Com-
puting, 1(4):214–215.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth’s algorithm. Computational Linguis-
tics, 29(1):135–143.
Aarne Ranta. 2009. The GF resource grammar library.
Linguistic Issues in Language Technology, 2(2).
Aarne Ranta. 2011. Grammatical Framework: Pro-
gramming with Multilingual Grammars. CSLI Pub-
lications, Stanford.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88(2):191–229.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165–201.
</reference>
<page confidence="0.999109">
376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.489376">
<title confidence="0.999903">Fast Statistical Parsing with Parallel Multiple Context-Free Grammars</title>
<author confidence="0.906064">Angelov</author>
<affiliation confidence="0.7761125">University of Gothenburg and Chalmers University of G¨oteborg,</affiliation>
<email confidence="0.952293">peter.ljunglof@cse.gu.se</email>
<abstract confidence="0.999613375">We present an algorithm for incremental statistical parsing with Parallel Multiple Context-Free Grammars (PMCFG). This is an extension of the algorithm by Angelov (2009) to which we added statistical ranking. We show that the new algorithm is several times faster than other statistical PMCFG parsing algorithms on real-sized grammars. At the same time the algorithm is more general since it supports non-binarized and non-linear grammars. We also show that if we make the search heuristics non-admissible, the parsing speed improves even further, at the risk of returning sub-optimal solutions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Krasimir Angelov</author>
</authors>
<title>Incremental parsing with parallel multiple context-free grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL 2009, the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="1192" citStr="Angelov (2009" startWordPosition="170" endWordPosition="171">At the same time the algorithm is more general since it supports non-binarized and non-linear grammars. We also show that if we make the search heuristics non-admissible, the parsing speed improves even further, at the risk of returning sub-optimal solutions. 1 Introduction In this paper we present an algorithm for incremental parsing using Parallel Multiple ContextFree Grammars (PMCFG) (Seki et al., 1991). This is a non context-free formalism allowing discontinuity and crossing dependencies, while remaining with polynomial parsing complexity. The algorithm is an extension of the algorithm by Angelov (2009; 2011) which adds statistical ranking. This is a top-down algorithm, shown by Ljungl¨of (2012) to be similar to other top-down algorithms (Burden and Ljungl¨of, 2005; Kanazawa, 2008; Kallmeyer and Maier, 2009). None of the other top-down algorithms are statistical. The only statistical PMCFG parsing algorithms (Kato et al., 2006; Kallmeyer and Maier, 2013; Maier et al., 2012) all use bottom-up parsing strategies. Furthermore, they require the grammar to be binarized and linear, which means that they only support linear context-free rewriting systems (LCFRS). In contrast, our algorithm natural</context>
<context position="3311" citStr="Angelov (2009" startWordPosition="523" endWordPosition="524"> with a formal definition of a weighted PMCFG in Section 2, and we continue with a presentation of our algorithm by means of a weighted deduction system in Section 3. In Section 4, we prove that our estimations are admissible and monotonic. In Section 5 we calculate an estimate for the minimal inside probability for every category, and in Section 6 we discuss the nonadmissible heuristics. Sections 7 and 8 describe the implementation and our evaluation, and the final Section 9 concludes the paper. 2 PMCFG definition Our definition of weighted PMCFG (Definition 1) is the same as the one used by Angelov (2009; 2011), except that we extend it with weights for the productions. This definition is also similar to Kato et al (2006), with the small difference that we allow non-linear functions. As an illustration for PMCFG parsing, we use a simple grammar (Figure 1) which can generate phrases like “both black and white” and “either red or white” but rejects the incorrect combina368 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 368–376, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Definition 1 A</context>
<context position="9469" citStr="Angelov (2009" startWordPosition="1664" endWordPosition="1665">ch is still pending. Finally wz and wo are the inside and outside weights for the item. Passive Items The passive items are of the form: [k� A; l; ˆA] and state that a constituent with index l from category A was recognized from position j to position k in the sentence. As a consequence the parser has created a new category ˆA. The set of productions derived for Aˆ compactly records all possible ways to parse the j − k fragment. 3.1 Inside and outside weights The inside weight wz and the outside weight wo in the active items deserve more attention since this is the only difference compared to Angelov (2009; 2011). When the item is complete, it will yield the forest of all trees that derive the sub-string covered by the item. For example, when the first constituent for category Conj is completely parsed, the forest will contain the single production in (1). The inside weight for the active item is the currently best known estimation for the lowest weight of a tree in the forest. The trees yielded by the item do not cover the whole sentence however. Instead, they will become part of larger trees that cover the whole sentence. The outside weight is the estimation for the lowest weight for an exten</context>
</contexts>
<marker>Angelov, 2009</marker>
<rawString>Krasimir Angelov. 2009. Incremental parsing with parallel multiple context-free grammars. In Proceedings of EACL 2009, the 12th Conference of the European Chapter of the Association for Computational Linguistics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krasimir Angelov</author>
</authors>
<title>The Mechanics of the Grammatical Framework.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Chalmers University of Technology, Gothenburg, Sweden.</institution>
<contexts>
<context position="2024" citStr="Angelov, 2011" startWordPosition="297" endWordPosition="298">f the other top-down algorithms are statistical. The only statistical PMCFG parsing algorithms (Kato et al., 2006; Kallmeyer and Maier, 2013; Maier et al., 2012) all use bottom-up parsing strategies. Furthermore, they require the grammar to be binarized and linear, which means that they only support linear context-free rewriting systems (LCFRS). In contrast, our algorithm naturally supports the full power of PMCFG. By lifting these restrictions, we make it possible to experiment with novel grammar induction methods (Maier, 2013) and to use statistical disambiguation for hand-crafted grammars (Angelov, 2011). By extending the algorithm with a statistical model, we allow the parser to explore only parts of the search space, when only the most probable parse tree is needed. Our cost estimation is similar to the estimation for the Viterbi probability as in Stolcke (1995), except that we have to take into account that our grammar is not contextfree. The estimation is both admissible and monotonic (Klein and Manning, 2003) which guarantees that we always find a tree whose probability is the global maximum. We also describe a variant with a nonadmissible estimation, which further improves the efficienc</context>
</contexts>
<marker>Angelov, 2011</marker>
<rawString>Krasimir Angelov. 2011. The Mechanics of the Grammatical Framework. Ph.D. thesis, Chalmers University of Technology, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of TLT 2002, the 1st Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="22967" citStr="Brants et al., 2002" startWordPosition="4223" endWordPosition="4226">n the complexity of the grammar, so we decided to do a larger test and compare with an existing state-of-the-art statistical PMCFG parser. Rparse (Kallmeyer and Maier, 2013) is a another state-of-the-art training and parsing system for PMCFG.2 It is written in Java and developed at the Universities of T¨ubingen and D¨usseldorf, Germany. Rparse can be used for training probabilistic PMCFGs from discontinuous treebanks. It can also be used for parsing new sentences with the trained grammars. In our evaluation we used Rparse to extract PMCFG grammars from the discontinuous German Tiger Treebank (Brants et al., 2002). The reason for using this treebank is that the extracted grammars are non-context-free, and our parser is specifically made for such grammars. 8.1 Evaluation data In our evaluations we got the same general results regardless of the size of the grammar, so we only report the results from one of these runs. In this particular example, we trained the grammar on 40 000 sentences from the Tiger Treebank with lengths up to 160 tokens. We evaluated on 2https://github.com/wmaier/rparse 373 Count Training sentences 40 000 Test sentences 4 607 Non-binarized grammar rules 30 863 Binarized grammar rules</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Proceedings of TLT 2002, the 1st Workshop on Treebanks and Linguistic Theories, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H˚akan Burden</author>
<author>Peter Ljungl¨of</author>
</authors>
<title>Parsing linear context-free rewriting systems.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT 2005, the 9th International Workshop on Parsing Technologies,</booktitle>
<location>Vancouver, Canada.</location>
<marker>Burden, Ljungl¨of, 2005</marker>
<rawString>H˚akan Burden and Peter Ljungl¨of. 2005. Parsing linear context-free rewriting systems. In Proceedings of IWPT 2005, the 9th International Workshop on Parsing Technologies, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>102</pages>
<contexts>
<context position="28646" citStr="Earley (1970)" startWordPosition="5230" endWordPosition="5231">g the EVALB measure which is implemented in Rparse (Maier, 2010). As can be seen, a factor of h = 0.50 only results in a f-score loss of 3 points, which is arguably not very much. On the other extreme, for h = 0.95 the f-score drops 14 points. 3The exception is the standard Rparse parser, which has a polynomial degree of 8. Precision Recall F-score admissible 71.1 67.7 69.3 h = 0.50 68.0 64.9 66.4 h = 0.75 63.0 60.8 61.9 h = 0.95 55.1 55.6 55.3 Table 2: Parsing quality for different values of h. 9 Discussion The presented algorithm is an important generalization of the classical algorithms of Earley (1970) and Stolcke (1995) for parsing with probabilistic context-free grammars to the more general formalism of parallel multiple context-free grammars. The algorithm has been implemented as part of the runtime for the Grammatical Framework (Ranta, 2011), but it is not limited to GF alone. 9.1 Performance To show the universality of the algorithm, we evaluated it on large LCFRS grammars trained from the Tiger Treebank. Our parser is around 10–15 times faster than the latest, optimized version of the Rparse state-ofthe-art parser. This improvement seems to be constant, which means that it can be a co</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hart</author>
<author>Nils Nilsson</author>
<author>Bertram Raphael</author>
</authors>
<title>A formal basis for the heuristic determination of minimum cost paths.</title>
<date>1968</date>
<journal>IEEE Transactions of Systems Science and Cybernetics,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="17522" citStr="Hart et al., 1968" startWordPosition="3245" endWordPosition="3248"> that the estimation is always admissible. 5 Initial Estimation The minimal weight for a dynamically created category is computed by the parser, but we must initialize the weights for the categories that are defined in the grammar. The easiest way is to just set all weights to zero, and this is safe since the weights for the predefined categories are used only as estimations for the yet unseen parts of the sentence. Essentially this gives us a statistical parser which performs Dijkstra search in the space of all parse trees. Any other reasonable weight assignment will give us an A∗ algorithm (Hart et al., 1968). In general it is possible to devise different heuristics which will give us different improvements in the parsing time. In our current implementation of the parser we use a weight assignment which considers only the already known probabilities for the productions in the grammar. The weight for a category A is computed as: wA = min (w + wB) Aw−→f[B] ∈ P Here the sum w + wB is the minimal weight for a tree constructed with the production A w−→ f[~B] at the root. By taking the minimum over all productions for A, we get the corresponding weight wA. This is a recursive equation since its righthan</context>
</contexts>
<marker>Hart, Nilsson, Raphael, 1968</marker>
<rawString>Peter Hart, Nils Nilsson, and Bertram Raphael. 1968. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions of Systems Science and Cybernetics, 4(2):100–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
<author>Wolfgang Maier</author>
</authors>
<title>An incremental Earley parser for simple range concatenation grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of IWPT 2009, the 11th International Conference on Parsing Technologies,</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="1402" citStr="Kallmeyer and Maier, 2009" startWordPosition="200" endWordPosition="203"> even further, at the risk of returning sub-optimal solutions. 1 Introduction In this paper we present an algorithm for incremental parsing using Parallel Multiple ContextFree Grammars (PMCFG) (Seki et al., 1991). This is a non context-free formalism allowing discontinuity and crossing dependencies, while remaining with polynomial parsing complexity. The algorithm is an extension of the algorithm by Angelov (2009; 2011) which adds statistical ranking. This is a top-down algorithm, shown by Ljungl¨of (2012) to be similar to other top-down algorithms (Burden and Ljungl¨of, 2005; Kanazawa, 2008; Kallmeyer and Maier, 2009). None of the other top-down algorithms are statistical. The only statistical PMCFG parsing algorithms (Kato et al., 2006; Kallmeyer and Maier, 2013; Maier et al., 2012) all use bottom-up parsing strategies. Furthermore, they require the grammar to be binarized and linear, which means that they only support linear context-free rewriting systems (LCFRS). In contrast, our algorithm naturally supports the full power of PMCFG. By lifting these restrictions, we make it possible to experiment with novel grammar induction methods (Maier, 2013) and to use statistical disambiguation for hand-crafted gr</context>
</contexts>
<marker>Kallmeyer, Maier, 2009</marker>
<rawString>Laura Kallmeyer and Wolfgang Maier. 2009. An incremental Earley parser for simple range concatenation grammar. In Proceedings of IWPT 2009, the 11th International Conference on Parsing Technologies, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
<author>Wolfgang Maier</author>
</authors>
<title>Datadriven parsing using probabilistic linear contextfree rewriting systems.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="1550" citStr="Kallmeyer and Maier, 2013" startWordPosition="222" endWordPosition="225">allel Multiple ContextFree Grammars (PMCFG) (Seki et al., 1991). This is a non context-free formalism allowing discontinuity and crossing dependencies, while remaining with polynomial parsing complexity. The algorithm is an extension of the algorithm by Angelov (2009; 2011) which adds statistical ranking. This is a top-down algorithm, shown by Ljungl¨of (2012) to be similar to other top-down algorithms (Burden and Ljungl¨of, 2005; Kanazawa, 2008; Kallmeyer and Maier, 2009). None of the other top-down algorithms are statistical. The only statistical PMCFG parsing algorithms (Kato et al., 2006; Kallmeyer and Maier, 2013; Maier et al., 2012) all use bottom-up parsing strategies. Furthermore, they require the grammar to be binarized and linear, which means that they only support linear context-free rewriting systems (LCFRS). In contrast, our algorithm naturally supports the full power of PMCFG. By lifting these restrictions, we make it possible to experiment with novel grammar induction methods (Maier, 2013) and to use statistical disambiguation for hand-crafted grammars (Angelov, 2011). By extending the algorithm with a statistical model, we allow the parser to explore only parts of the search space, when onl</context>
<context position="22520" citStr="Kallmeyer and Maier, 2013" startWordPosition="4150" endWordPosition="4153">rammar has 44 000 productions. The rule weights were trained from a version of the Penn Treebank (Marcus et al., 1993) which was converted to trees compatible with the grammar. The trained grammar was tested on Penn Treebank sentences of length up to 35 tokens, and the parsing times were at most 7 seconds per sentence. This initial test was run on a computer with a 2.4 GHz Intel Core i5 processor with 8 GB RAM. This result was very encouraging, given the complexity of the grammar, so we decided to do a larger test and compare with an existing state-of-the-art statistical PMCFG parser. Rparse (Kallmeyer and Maier, 2013) is a another state-of-the-art training and parsing system for PMCFG.2 It is written in Java and developed at the Universities of T¨ubingen and D¨usseldorf, Germany. Rparse can be used for training probabilistic PMCFGs from discontinuous treebanks. It can also be used for parsing new sentences with the trained grammars. In our evaluation we used Rparse to extract PMCFG grammars from the discontinuous German Tiger Treebank (Brants et al., 2002). The reason for using this treebank is that the extracted grammars are non-context-free, and our parser is specifically made for such grammars. 8.1 Eval</context>
<context position="24583" citStr="Kallmeyer and Maier, 2013" startWordPosition="4499" endWordPosition="4502">n with Rparse We evaluated our parser by comparing it with Rparse’s built-in parser. Note that we are only interested in the efficiency of our implementation, not the coverage and accuracy of the trained grammar. In the comparison we used only the admissible heuristics, and we did confirm that the parsers produce optimal trees with exactly the same weight for the same input. Rparse extracts grammars in two steps. First it converts the treebank into a PMCFG, and then it binarizes that grammar. The binarization process uses markovization to improve the precision and recall of the final grammar (Kallmeyer and Maier, 2013). We tested both Rparse’s standard (Kallmeyer and Maier, 2013) and its new improved parsing alogorithm (Maier et al., 2012). The new algorithm unfortunately works only with LCFRS grammars with a fan-out &lt; 2 (Maier et al., 2012). In this test we used the optimal binarization method described in Kallmeyer (2010, chapter 7.2). This was the only binarization algorithm in Rparse that produced a grammar with fan-out &lt; 2. As can be seen in Figure 3, our parser outperforms Rparse for all sentence lengths. For sentences longer than 15 tokens, the standard Rparse parser needs on average 100 times longer</context>
</contexts>
<marker>Kallmeyer, Maier, 2013</marker>
<rawString>Laura Kallmeyer and Wolfgang Maier. 2013. Datadriven parsing using probabilistic linear contextfree rewriting systems. Computational Linguistics, 39(1):87–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
</authors>
<title>Parsing Beyond Context-Free Grammars.</title>
<date>2010</date>
<publisher>Springer.</publisher>
<contexts>
<context position="24893" citStr="Kallmeyer (2010" startWordPosition="4552" endWordPosition="4553">l trees with exactly the same weight for the same input. Rparse extracts grammars in two steps. First it converts the treebank into a PMCFG, and then it binarizes that grammar. The binarization process uses markovization to improve the precision and recall of the final grammar (Kallmeyer and Maier, 2013). We tested both Rparse’s standard (Kallmeyer and Maier, 2013) and its new improved parsing alogorithm (Maier et al., 2012). The new algorithm unfortunately works only with LCFRS grammars with a fan-out &lt; 2 (Maier et al., 2012). In this test we used the optimal binarization method described in Kallmeyer (2010, chapter 7.2). This was the only binarization algorithm in Rparse that produced a grammar with fan-out &lt; 2. As can be seen in Figure 3, our parser outperforms Rparse for all sentence lengths. For sentences longer than 15 tokens, the standard Rparse parser needs on average 100 times longer time than our parser. This difference increases with sentence length, suggesting that our algorithm has a better parsing complexity than Rparse. The PGF parser also outperforms the improved Rparse parser, but the relative difference seems to stabilize on a speedup of 10–15 times. Figure 3: Parsing time (seco</context>
</contexts>
<marker>Kallmeyer, 2010</marker>
<rawString>Laura Kallmeyer. 2010. Parsing Beyond Context-Free Grammars. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Kanazawa</author>
</authors>
<title>A prefix-correct Earley recognizer for multiple context-free grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of TAG+9, the 9th International Workshop on Tree Adjoining Grammar and Related Formalisms,</booktitle>
<location>T¨ubingen, Germany.</location>
<contexts>
<context position="1374" citStr="Kanazawa, 2008" startWordPosition="198" endWordPosition="199">g speed improves even further, at the risk of returning sub-optimal solutions. 1 Introduction In this paper we present an algorithm for incremental parsing using Parallel Multiple ContextFree Grammars (PMCFG) (Seki et al., 1991). This is a non context-free formalism allowing discontinuity and crossing dependencies, while remaining with polynomial parsing complexity. The algorithm is an extension of the algorithm by Angelov (2009; 2011) which adds statistical ranking. This is a top-down algorithm, shown by Ljungl¨of (2012) to be similar to other top-down algorithms (Burden and Ljungl¨of, 2005; Kanazawa, 2008; Kallmeyer and Maier, 2009). None of the other top-down algorithms are statistical. The only statistical PMCFG parsing algorithms (Kato et al., 2006; Kallmeyer and Maier, 2013; Maier et al., 2012) all use bottom-up parsing strategies. Furthermore, they require the grammar to be binarized and linear, which means that they only support linear context-free rewriting systems (LCFRS). In contrast, our algorithm naturally supports the full power of PMCFG. By lifting these restrictions, we make it possible to experiment with novel grammar induction methods (Maier, 2013) and to use statistical disamb</context>
</contexts>
<marker>Kanazawa, 2008</marker>
<rawString>Makoto Kanazawa. 2008. A prefix-correct Earley recognizer for multiple context-free grammars. In Proceedings of TAG+9, the 9th International Workshop on Tree Adjoining Grammar and Related Formalisms, T¨ubingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuki Kato</author>
<author>Hiroyuki Seki</author>
<author>Tadao Kasami</author>
</authors>
<title>Stochastic multiple context-free grammar for RNA pseudoknot modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of TAGRF 2006, the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="1523" citStr="Kato et al., 2006" startWordPosition="218" endWordPosition="221">l parsing using Parallel Multiple ContextFree Grammars (PMCFG) (Seki et al., 1991). This is a non context-free formalism allowing discontinuity and crossing dependencies, while remaining with polynomial parsing complexity. The algorithm is an extension of the algorithm by Angelov (2009; 2011) which adds statistical ranking. This is a top-down algorithm, shown by Ljungl¨of (2012) to be similar to other top-down algorithms (Burden and Ljungl¨of, 2005; Kanazawa, 2008; Kallmeyer and Maier, 2009). None of the other top-down algorithms are statistical. The only statistical PMCFG parsing algorithms (Kato et al., 2006; Kallmeyer and Maier, 2013; Maier et al., 2012) all use bottom-up parsing strategies. Furthermore, they require the grammar to be binarized and linear, which means that they only support linear context-free rewriting systems (LCFRS). In contrast, our algorithm naturally supports the full power of PMCFG. By lifting these restrictions, we make it possible to experiment with novel grammar induction methods (Maier, 2013) and to use statistical disambiguation for hand-crafted grammars (Angelov, 2011). By extending the algorithm with a statistical model, we allow the parser to explore only parts of</context>
<context position="3431" citStr="Kato et al (2006)" startWordPosition="542" endWordPosition="545">means of a weighted deduction system in Section 3. In Section 4, we prove that our estimations are admissible and monotonic. In Section 5 we calculate an estimate for the minimal inside probability for every category, and in Section 6 we discuss the nonadmissible heuristics. Sections 7 and 8 describe the implementation and our evaluation, and the final Section 9 concludes the paper. 2 PMCFG definition Our definition of weighted PMCFG (Definition 1) is the same as the one used by Angelov (2009; 2011), except that we extend it with weights for the productions. This definition is also similar to Kato et al (2006), with the small difference that we allow non-linear functions. As an illustration for PMCFG parsing, we use a simple grammar (Figure 1) which can generate phrases like “both black and white” and “either red or white” but rejects the incorrect combina368 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 368–376, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Definition 1 A parallel multiple context-free grammar is a tuple G = (N, T, F, P, 5, d, di, r, a) where: • N is a finite set of catego</context>
</contexts>
<marker>Kato, Seki, Kasami, 2006</marker>
<rawString>Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006. Stochastic multiple context-free grammar for RNA pseudoknot modeling. In Proceedings of TAGRF 2006, the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A∗ parsing: fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003, the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2442" citStr="Klein and Manning, 2003" startWordPosition="369" endWordPosition="372"> of PMCFG. By lifting these restrictions, we make it possible to experiment with novel grammar induction methods (Maier, 2013) and to use statistical disambiguation for hand-crafted grammars (Angelov, 2011). By extending the algorithm with a statistical model, we allow the parser to explore only parts of the search space, when only the most probable parse tree is needed. Our cost estimation is similar to the estimation for the Viterbi probability as in Stolcke (1995), except that we have to take into account that our grammar is not contextfree. The estimation is both admissible and monotonic (Klein and Manning, 2003) which guarantees that we always find a tree whose probability is the global maximum. We also describe a variant with a nonadmissible estimation, which further improves the efficiency of the parser at the risk of returning a suboptimal parse tree. We start with a formal definition of a weighted PMCFG in Section 2, and we continue with a presentation of our algorithm by means of a weighted deduction system in Section 3. In Section 4, we prove that our estimations are admissible and monotonic. In Section 5 we calculate an estimate for the minimal inside probability for every category, and in Sec</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. A∗ parsing: fast exact Viterbi parse selection. In Proceedings of HLT-NAACL 2003, the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Ljungl¨of</author>
</authors>
<title>Practical parsing of parallel multiple context-free grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of TAG+11, the 11th International Workshop on Tree Adjoining Grammar and Related Formalisms,</booktitle>
<location>Paris, France.</location>
<marker>Ljungl¨of, 2012</marker>
<rawString>Peter Ljungl¨of. 2012. Practical parsing of parallel multiple context-free grammars. In Proceedings of TAG+11, the 11th International Workshop on Tree Adjoining Grammar and Related Formalisms, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Miriam Kaeshammer</author>
<author>Laura Kallmeyer</author>
</authors>
<title>PLCFRS parsing revisited: Restricting the fan-out to two.</title>
<date>2012</date>
<booktitle>In Proceedings of TAG+11, the 11th International Workshop on Tree Adjoining Grammar and Related Formalisms,</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="1571" citStr="Maier et al., 2012" startWordPosition="226" endWordPosition="229">Grammars (PMCFG) (Seki et al., 1991). This is a non context-free formalism allowing discontinuity and crossing dependencies, while remaining with polynomial parsing complexity. The algorithm is an extension of the algorithm by Angelov (2009; 2011) which adds statistical ranking. This is a top-down algorithm, shown by Ljungl¨of (2012) to be similar to other top-down algorithms (Burden and Ljungl¨of, 2005; Kanazawa, 2008; Kallmeyer and Maier, 2009). None of the other top-down algorithms are statistical. The only statistical PMCFG parsing algorithms (Kato et al., 2006; Kallmeyer and Maier, 2013; Maier et al., 2012) all use bottom-up parsing strategies. Furthermore, they require the grammar to be binarized and linear, which means that they only support linear context-free rewriting systems (LCFRS). In contrast, our algorithm naturally supports the full power of PMCFG. By lifting these restrictions, we make it possible to experiment with novel grammar induction methods (Maier, 2013) and to use statistical disambiguation for hand-crafted grammars (Angelov, 2011). By extending the algorithm with a statistical model, we allow the parser to explore only parts of the search space, when only the most probable p</context>
<context position="23825" citStr="Maier et al (2012)" startWordPosition="4372" endWordPosition="4375">f the grammar, so we only report the results from one of these runs. In this particular example, we trained the grammar on 40 000 sentences from the Tiger Treebank with lengths up to 160 tokens. We evaluated on 2https://github.com/wmaier/rparse 373 Count Training sentences 40 000 Test sentences 4 607 Non-binarized grammar rules 30 863 Binarized grammar rules 26111 Table 1: Training and testing data. 4 600 Tiger sentences, with a length of 5–60 tokens. The exact numbers are shown in Table 1. All tests were run on a computer with a 2.3 GHz Intel Core i7 processor with 16GB RAM. As a comparison, Maier et al (2012) train on approximately 15 000 sentences from the Negra Treebank, and only evaluate on sentences of at most 40 tokens. 8.2 Comparison with Rparse We evaluated our parser by comparing it with Rparse’s built-in parser. Note that we are only interested in the efficiency of our implementation, not the coverage and accuracy of the trained grammar. In the comparison we used only the admissible heuristics, and we did confirm that the parsers produce optimal trees with exactly the same weight for the same input. Rparse extracts grammars in two steps. First it converts the treebank into a PMCFG, and th</context>
</contexts>
<marker>Maier, Kaeshammer, Kallmeyer, 2012</marker>
<rawString>Wolfgang Maier, Miriam Kaeshammer, and Laura Kallmeyer. 2012. PLCFRS parsing revisited: Restricting the fan-out to two. In Proceedings of TAG+11, the 11th International Workshop on Tree Adjoining Grammar and Related Formalisms, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
</authors>
<title>Direct parsing of discontinuous constituents in German.</title>
<date>2010</date>
<booktitle>In Proceedings of SPRML 2010, the 1st Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<location>Los Angeles, California.</location>
<contexts>
<context position="28097" citStr="Maier, 2010" startWordPosition="5127" endWordPosition="5128">he optimal weight. The performance gradually gets worse for higher h, and with h = 0.95 almost 10% of the trees have a weight more than 20% from the optimum. These numbers only show how the parsing quality degrades relative to the grammar. But since the grammar is trained from a treebank it is more interesting to evaluate how the parsing quality on the treebank sentences is affected when we use a non-admissible heuristics. Table 2 shows how the labelled precision and recall are changed with different values for h. The evaluation was done using the EVALB measure which is implemented in Rparse (Maier, 2010). As can be seen, a factor of h = 0.50 only results in a f-score loss of 3 points, which is arguably not very much. On the other extreme, for h = 0.95 the f-score drops 14 points. 3The exception is the standard Rparse parser, which has a polynomial degree of 8. Precision Recall F-score admissible 71.1 67.7 69.3 h = 0.50 68.0 64.9 66.4 h = 0.75 63.0 60.8 61.9 h = 0.95 55.1 55.6 55.3 Table 2: Parsing quality for different values of h. 9 Discussion The presented algorithm is an important generalization of the classical algorithms of Earley (1970) and Stolcke (1995) for parsing with probabilistic </context>
</contexts>
<marker>Maier, 2010</marker>
<rawString>Wolfgang Maier. 2010. Direct parsing of discontinuous constituents in German. In Proceedings of SPRML 2010, the 1st Workshop on Statistical Parsing of Morphologically-Rich Languages, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
</authors>
<title>LCFRS binarization and debinarization for directional parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of IWPT 2013, the 13th International Conference on Parsing Technologies,</booktitle>
<location>Nara, Japan.</location>
<contexts>
<context position="1550" citStr="Maier, 2013" startWordPosition="224" endWordPosition="225"> ContextFree Grammars (PMCFG) (Seki et al., 1991). This is a non context-free formalism allowing discontinuity and crossing dependencies, while remaining with polynomial parsing complexity. The algorithm is an extension of the algorithm by Angelov (2009; 2011) which adds statistical ranking. This is a top-down algorithm, shown by Ljungl¨of (2012) to be similar to other top-down algorithms (Burden and Ljungl¨of, 2005; Kanazawa, 2008; Kallmeyer and Maier, 2009). None of the other top-down algorithms are statistical. The only statistical PMCFG parsing algorithms (Kato et al., 2006; Kallmeyer and Maier, 2013; Maier et al., 2012) all use bottom-up parsing strategies. Furthermore, they require the grammar to be binarized and linear, which means that they only support linear context-free rewriting systems (LCFRS). In contrast, our algorithm naturally supports the full power of PMCFG. By lifting these restrictions, we make it possible to experiment with novel grammar induction methods (Maier, 2013) and to use statistical disambiguation for hand-crafted grammars (Angelov, 2011). By extending the algorithm with a statistical model, we allow the parser to explore only parts of the search space, when onl</context>
<context position="22520" citStr="Maier, 2013" startWordPosition="4152" endWordPosition="4153">000 productions. The rule weights were trained from a version of the Penn Treebank (Marcus et al., 1993) which was converted to trees compatible with the grammar. The trained grammar was tested on Penn Treebank sentences of length up to 35 tokens, and the parsing times were at most 7 seconds per sentence. This initial test was run on a computer with a 2.4 GHz Intel Core i5 processor with 8 GB RAM. This result was very encouraging, given the complexity of the grammar, so we decided to do a larger test and compare with an existing state-of-the-art statistical PMCFG parser. Rparse (Kallmeyer and Maier, 2013) is a another state-of-the-art training and parsing system for PMCFG.2 It is written in Java and developed at the Universities of T¨ubingen and D¨usseldorf, Germany. Rparse can be used for training probabilistic PMCFGs from discontinuous treebanks. It can also be used for parsing new sentences with the trained grammars. In our evaluation we used Rparse to extract PMCFG grammars from the discontinuous German Tiger Treebank (Brants et al., 2002). The reason for using this treebank is that the extracted grammars are non-context-free, and our parser is specifically made for such grammars. 8.1 Eval</context>
<context position="24583" citStr="Maier, 2013" startWordPosition="4501" endWordPosition="4502">We evaluated our parser by comparing it with Rparse’s built-in parser. Note that we are only interested in the efficiency of our implementation, not the coverage and accuracy of the trained grammar. In the comparison we used only the admissible heuristics, and we did confirm that the parsers produce optimal trees with exactly the same weight for the same input. Rparse extracts grammars in two steps. First it converts the treebank into a PMCFG, and then it binarizes that grammar. The binarization process uses markovization to improve the precision and recall of the final grammar (Kallmeyer and Maier, 2013). We tested both Rparse’s standard (Kallmeyer and Maier, 2013) and its new improved parsing alogorithm (Maier et al., 2012). The new algorithm unfortunately works only with LCFRS grammars with a fan-out &lt; 2 (Maier et al., 2012). In this test we used the optimal binarization method described in Kallmeyer (2010, chapter 7.2). This was the only binarization algorithm in Rparse that produced a grammar with fan-out &lt; 2. As can be seen in Figure 3, our parser outperforms Rparse for all sentence lengths. For sentences longer than 15 tokens, the standard Rparse parser needs on average 100 times longer</context>
</contexts>
<marker>Maier, 2013</marker>
<rawString>Wolfgang Maier. 2013. LCFRS binarization and debinarization for directional parsing. In Proceedings of IWPT 2013, the 13th International Conference on Parsing Technologies, Nara, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="22012" citStr="Marcus et al., 1993" startWordPosition="4060" endWordPosition="4063">icon to the resource grammar results in a highly ambiguous grammar, which can give rise to millions of trees even for moderately complex sentences. Previously, the GF system has not been able to parse with such ambiguous grammars, but with our statistical algorithm it is now feasible. 8 Evaluation We did an initial evaluation on the GF English resource grammar augmented with a large-coverage lexicon of 40 000 lemmas taken from the Oxford Advanced Learner’s Dictionary (Mitton, 1986). In total the grammar has 44 000 productions. The rule weights were trained from a version of the Penn Treebank (Marcus et al., 1993) which was converted to trees compatible with the grammar. The trained grammar was tested on Penn Treebank sentences of length up to 35 tokens, and the parsing times were at most 7 seconds per sentence. This initial test was run on a computer with a 2.4 GHz Intel Core i5 processor with 8 GB RAM. This result was very encouraging, given the complexity of the grammar, so we decided to do a larger test and compare with an existing state-of-the-art statistical PMCFG parser. Rparse (Kallmeyer and Maier, 2013) is a another state-of-the-art training and parsing system for PMCFG.2 It is written in Java</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Mitton</author>
</authors>
<title>A partial dictionary of English in computer-usable form.</title>
<date>1986</date>
<journal>Literary &amp; Linguistic Computing,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="21878" citStr="Mitton, 1986" startWordPosition="4038" endWordPosition="4039">ramework.org/ 2009) for over 25 languages, which are used as a resource for deriving domain specific grammars. Adding a big lexicon to the resource grammar results in a highly ambiguous grammar, which can give rise to millions of trees even for moderately complex sentences. Previously, the GF system has not been able to parse with such ambiguous grammars, but with our statistical algorithm it is now feasible. 8 Evaluation We did an initial evaluation on the GF English resource grammar augmented with a large-coverage lexicon of 40 000 lemmas taken from the Oxford Advanced Learner’s Dictionary (Mitton, 1986). In total the grammar has 44 000 productions. The rule weights were trained from a version of the Penn Treebank (Marcus et al., 1993) which was converted to trees compatible with the grammar. The trained grammar was tested on Penn Treebank sentences of length up to 35 tokens, and the parsing times were at most 7 seconds per sentence. This initial test was run on a computer with a 2.4 GHz Intel Core i5 processor with 8 GB RAM. This result was very encouraging, given the complexity of the grammar, so we decided to do a larger test and compare with an existing state-of-the-art statistical PMCFG </context>
</contexts>
<marker>Mitton, 1986</marker>
<rawString>Roger Mitton. 1986. A partial dictionary of English in computer-usable form. Literary &amp; Linguistic Computing, 1(4):214–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6924" citStr="Nederhof, 2003" startWordPosition="1198" endWordPosition="1199">. Usually the weights for the productions are logarithmic probabilities, i.e. the weight of the production A —* f[B] is: w = — log P(A — f[�B] |A) where P(A f[B] |A) is the probability to choose this production when the result category is fixed. In this case the probabilities for all productions with the same result category sum to one: � e−w = 1 Aw��f[~B] EP However, the parsing algorithm does not depend on the probabilistic interpretation of the weights, so the same algorithm can be used with any other kind of weights. 3 Deduction System We define the algorithm as weighted deduction system (Nederhof, 2003) which generalizes Angelov’s system. A key feature in his algorithm is that the expressive PMCFG is reduced to a simple contextfree grammar which is extended dynamically at parsing time in order to account for context dependent features in the original grammar. This 369 can be exemplified with the grammar in Figure 1, where there are two productions for category Conj. Given the phrase “both black and white”, after accepting the token both, only the production Conj �5 −→ both and[] can be applied for parsing the second part of the conjunction. This is achieved by generating a new category Conj2</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Mark-Jan Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. Computational Linguistics, 29(1):135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<title>The GF resource grammar library.</title>
<date>2009</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>2</volume>
<issue>2</issue>
<marker>Ranta, 2009</marker>
<rawString>Aarne Ranta. 2009. The GF resource grammar library. Linguistic Issues in Language Technology, 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<title>Grammatical Framework: Programming with Multilingual Grammars.</title>
<date>2011</date>
<publisher>CSLI Publications, Stanford.</publisher>
<contexts>
<context position="20908" citStr="Ranta, 2011" startWordPosition="3880" endWordPosition="3881">ction 8.3 shows that a significant speedup can be obtained by using larger values of h. Unfortunately, if h &gt; 0, we loose some accuracy and cannot guarantee that the best parse is always returned first. Note that the heuristics does not change the completeness of the algorithm – it will succeed for all grammatical sentences and fail for all nongrammatical. But it does not guarantee that the first parse tree will be the optimal. 7 Implementation The parser is implemented in C and is distributed as a part of the runtime system for the open-source Grammatical Framework (GF) programming language (Ranta, 2011).1 Although the primary application of the runtime system is to run GF applications, it is not specific to one formalism, and it can serve as an execution platform for other frameworks where natural language parsing and generation is needed. The GF system is distributed with a library of manually authored resource grammars (Ranta, 1http://www.grammaticalframework.org/ 2009) for over 25 languages, which are used as a resource for deriving domain specific grammars. Adding a big lexicon to the resource grammar results in a highly ambiguous grammar, which can give rise to millions of trees even fo</context>
<context position="28894" citStr="Ranta, 2011" startWordPosition="5266" endWordPosition="5267">exception is the standard Rparse parser, which has a polynomial degree of 8. Precision Recall F-score admissible 71.1 67.7 69.3 h = 0.50 68.0 64.9 66.4 h = 0.75 63.0 60.8 61.9 h = 0.95 55.1 55.6 55.3 Table 2: Parsing quality for different values of h. 9 Discussion The presented algorithm is an important generalization of the classical algorithms of Earley (1970) and Stolcke (1995) for parsing with probabilistic context-free grammars to the more general formalism of parallel multiple context-free grammars. The algorithm has been implemented as part of the runtime for the Grammatical Framework (Ranta, 2011), but it is not limited to GF alone. 9.1 Performance To show the universality of the algorithm, we evaluated it on large LCFRS grammars trained from the Tiger Treebank. Our parser is around 10–15 times faster than the latest, optimized version of the Rparse state-ofthe-art parser. This improvement seems to be constant, which means that it can be a consequence of low-level optimizations. More important is that our algorithm does not impose any restrictions at all on the underlying PMCFG grammar. Rparse on the other hand requires that the grammar is both binarized and has a fan-out of at most 2.</context>
</contexts>
<marker>Ranta, 2011</marker>
<rawString>Aarne Ranta. 2011. Grammatical Framework: Programming with Multilingual Grammars. CSLI Publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On multiple contextfree grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<volume>88</volume>
<issue>2</issue>
<contexts>
<context position="988" citStr="Seki et al., 1991" startWordPosition="139" endWordPosition="142">tension of the algorithm by Angelov (2009) to which we added statistical ranking. We show that the new algorithm is several times faster than other statistical PMCFG parsing algorithms on real-sized grammars. At the same time the algorithm is more general since it supports non-binarized and non-linear grammars. We also show that if we make the search heuristics non-admissible, the parsing speed improves even further, at the risk of returning sub-optimal solutions. 1 Introduction In this paper we present an algorithm for incremental parsing using Parallel Multiple ContextFree Grammars (PMCFG) (Seki et al., 1991). This is a non context-free formalism allowing discontinuity and crossing dependencies, while remaining with polynomial parsing complexity. The algorithm is an extension of the algorithm by Angelov (2009; 2011) which adds statistical ranking. This is a top-down algorithm, shown by Ljungl¨of (2012) to be similar to other top-down algorithms (Burden and Ljungl¨of, 2005; Kanazawa, 2008; Kallmeyer and Maier, 2009). None of the other top-down algorithms are statistical. The only statistical PMCFG parsing algorithms (Kato et al., 2006; Kallmeyer and Maier, 2013; Maier et al., 2012) all use bottom-u</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On multiple contextfree grammars. Theoretical Computer Science, 88(2):191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2289" citStr="Stolcke (1995)" startWordPosition="344" endWordPosition="345"> which means that they only support linear context-free rewriting systems (LCFRS). In contrast, our algorithm naturally supports the full power of PMCFG. By lifting these restrictions, we make it possible to experiment with novel grammar induction methods (Maier, 2013) and to use statistical disambiguation for hand-crafted grammars (Angelov, 2011). By extending the algorithm with a statistical model, we allow the parser to explore only parts of the search space, when only the most probable parse tree is needed. Our cost estimation is similar to the estimation for the Viterbi probability as in Stolcke (1995), except that we have to take into account that our grammar is not contextfree. The estimation is both admissible and monotonic (Klein and Manning, 2003) which guarantees that we always find a tree whose probability is the global maximum. We also describe a variant with a nonadmissible estimation, which further improves the efficiency of the parser at the risk of returning a suboptimal parse tree. We start with a formal definition of a weighted PMCFG in Section 2, and we continue with a presentation of our algorithm by means of a weighted deduction system in Section 3. In Section 4, we prove t</context>
<context position="28665" citStr="Stolcke (1995)" startWordPosition="5233" endWordPosition="5234">e which is implemented in Rparse (Maier, 2010). As can be seen, a factor of h = 0.50 only results in a f-score loss of 3 points, which is arguably not very much. On the other extreme, for h = 0.95 the f-score drops 14 points. 3The exception is the standard Rparse parser, which has a polynomial degree of 8. Precision Recall F-score admissible 71.1 67.7 69.3 h = 0.50 68.0 64.9 66.4 h = 0.75 63.0 60.8 61.9 h = 0.95 55.1 55.6 55.3 Table 2: Parsing quality for different values of h. 9 Discussion The presented algorithm is an important generalization of the classical algorithms of Earley (1970) and Stolcke (1995) for parsing with probabilistic context-free grammars to the more general formalism of parallel multiple context-free grammars. The algorithm has been implemented as part of the runtime for the Grammatical Framework (Ranta, 2011), but it is not limited to GF alone. 9.1 Performance To show the universality of the algorithm, we evaluated it on large LCFRS grammars trained from the Tiger Treebank. Our parser is around 10–15 times faster than the latest, optimized version of the Rparse state-ofthe-art parser. This improvement seems to be constant, which means that it can be a consequence of low-le</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>