<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.167750">
<title confidence="0.997967">
uOttawa: System description for SemEval 2013 Task 2 Sentiment
Analysis in Twitter
</title>
<author confidence="0.99949">
Hamid Poursepanj, Josh Weissbock, and Diana Inkpen
</author>
<affiliation confidence="0.9986305">
School of Electrical Engineering and Computer Science
University of Ottawa
</affiliation>
<address confidence="0.581029">
Ottawa, K1N6N5, Canada
</address>
<email confidence="0.997401">
{hpour099, jweis035, Diana.Inkpen}@uottawa.ca
</email>
<sectionHeader confidence="0.995621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999663416666667">
We present two systems developed at the Uni-
versity of Ottawa for the SemEval 2013 Task 2.
The first system (for Task A) classifies the po-
larity / sentiment orientation of one target word
in a Twitter message. The second system (for
Task B) classifies the polarity of whole Twitter
messages. Our two systems are very simple,
based on supervised classifiers with bag-of-
words feature representation, enriched with in-
formation from several sources. We present a
few additional results, besides results of the
submitted runs.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930875">
The Semeval 2013 Task 2 focused on classifying
Twitter messages (“tweets”) as expressing a posi-
tive opinion, a negative opinion, a neutral opinion,
or no opinion (objective). In fact, the neutral and
objective were joined in one class for the require-
ments of the shared task. Task A contained target
words whose sense had to be classified in the con-
text, while Task B was to classify each text into
one of the three classes: positive, negative, and
neutral/objective. The training data that was made
available for each task consisted in annotated
Twitter message. There were two test sets for each
task, one composed of Twitter messages and one
of SMS message (even if there was no specific
training data for SMS messages). See more details
about the datasets in (Wilson et al., 2013).
</bodyText>
<sectionHeader confidence="0.991495" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9998513125">
We used supervised learning classifiers from We-
ka (Witten and Frank, 2005). Initially we extracted
simple bag-of-word features (BOW). For the sub-
mitted systems, we also used features calculated
based on SentiWordNet information (Baccianella
et al., 2010). SentiWordNet contains positivity,
negativity, and objectivity scores for each sense of
a word. We explain below how this information
was used for each task.
As classifiers, we used Support Vector Ma-
chines (SVM) (SMO and libSVM from Weka with
default values for parameters), because SVM is
known to perform well on many tasks, and Multi-
nomial Naive Bayes (MNB), because MNB is
known to perform well on text data and it is faster
than SVM.
</bodyText>
<subsectionHeader confidence="0.996427">
2.1 Task A
</subsectionHeader>
<bodyText confidence="0.999829153846154">
Our system for Task A involved two parts: the
expansion of our training data and the classifica-
tion. The expansion was done with information
from SentiWordNet. Stop words and words that
appeared only once in the training data were fil-
tered out. Then the classification was completed
with algorithms from Weka.
As mentioned, the first task was to expand all of
the tweets that were provided as training data. This
was doing using Python and the Python NLTK
library, as well as SentiWordNet. SentiWordNet
provides a score of the sentient state for each word
(for each sense, in case the word has more than
</bodyText>
<page confidence="0.955411">
380
</page>
<bodyText confidence="0.996715897959184">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 380–383, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
one sense). As an example, the word “want” can
mean “a state of extreme poverty” with the Senti-
WordNet score of (Positive: 0 Objective:
0.75 Negative: 0.25). The same word could also
mean “a specific feeling of desire” with a score of
(Positive: 0.5 Objective: 0.5 Negative: 0). We also
used for expansion the definitions and synonyms
of each word sense, from WordNet.
The tweets in the training data are labeled with
their sentiment type (Positive, Negative, Objective
and Neutral). Neutral and Objective are treated the
same. The provided training data has the target
word marked, and also the sentiment orientation of
the word in the context of the tweeter message.
These target words were the ones expanded by our
method. When the target was a multi-word expres-
sion, if the expression was found in WordNet, then
the expansion was done directly; if not, each word
was expanded in a similar fashion and concatenat-
ed to the original tweet. These target words were
looked up in SentiWordNet and matched with the
definition that had the highest score that also
matched their sentiment label in the training data.
Original Tweet The great Noel Gallagher is about to
hit the stage in St. Paul. Plenty of
room here so we&apos;re 4th row center.
Plenty of room. Pretty fired up
Key Words Great
Sentiment Positive
Definition very good; &amp;quot;he did a bully job&amp;quot;; &amp;quot;a
neat sports car&amp;quot;; &amp;quot;had a great time at
the party&amp;quot;; &amp;quot;you look simply smash-
ing&amp;quot;
Synonyms Swell, smashing, slap-up, peachy,
not_bad, nifty, neat, keen, groovy,
dandy, cracking, corking, bully,
bang-up
Expanded The great Noel Gallagher is about to
Tweet hit the stage in St. Paul. Plenty of
room here so were 4th row center.
Plenty of room. Pretty fired up swell
smashing slap-up peachy not_bad
nifty neat keen groovy dandy crack-
ing corking bully bang-up very good
he did a bully job a neat sports car
had a great time at the party you look
simply smashing
</bodyText>
<tableCaption confidence="0.991076">
Table 1: Example of tweet expansion for Task A
</tableCaption>
<bodyText confidence="0.99988375">
The target word’s definition and synonyms were then
concatenated to the original tweet. No additional
changes were made to either the original tweet or the
features that were added from SentiWordNet. An ex-
ample follows in Table 1. The test data (Twitter and
SMS) was not expanded, because there are no labels in
the test data to be able to choose the sense with corre-
sponding sentiment.
</bodyText>
<subsectionHeader confidence="0.997365">
2.2 Task B
</subsectionHeader>
<bodyText confidence="0.999991230769231">
For this task, we used the following resources:
SentiwordNet (Baccianella et al, 2010), the Polari-
ty Lexicon (Wilson et al., 2005), the General In-
quirer (Stone et al., 1966), and the Stanford NLP
tools (Toutanova et al., 2003) for preprocessing
and feature selection. The preprocessing of Twitter
messages is implemented in three steps namely,
stop-word removal, stemming, and removal of
words with occurrence frequency of one. Several
extra features will be used: the number of positive
words and negative words identified by three lexi-
cal resources mentioned above, the number of
emoticons, the number of elongated words, and the
number of punctuation tokens (single or repeated
exclamation marks, etc.). As for SentiWordNet,
for each word a score is calculated that shows the
positive or negative weight of that word. No sense
disambiguation is done (the first sense is used), but
the scores are used for the right part-of-speech (in
case a word has more than one possible part-of-
speech). Part-of-Speech tagging was done with the
Stanford NLP Tools. As for General Inquirer and
Polarity Lexicon, we simply used the list positive
and negative words from these resources in order
to count how many positive and how many nega-
tive terms appear in a message.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="evaluation">
3 Results
</sectionHeader>
<subsectionHeader confidence="0.996925">
3.1 Task A
</subsectionHeader>
<bodyText confidence="0.999983444444445">
For classification, we first trained on our expanded
training data using 10-fold cross-validation and
using the SVM (libSVM) and Multinomial Na-
iveBayes classifiers from Weka, using their default
settings. The training data was represented as a
bag of words (BOW). These classifiers were cho-
sen as they have given us good results in the past
for text classification. The classifiers were run
with 10-fold cross-validation. See Table 2 for the
</bodyText>
<page confidence="0.993972">
381
</page>
<bodyText confidence="0.999906235294118">
results. Without expanding the tweets, the accura-
cy of the SVM classifier was equal to the baseline
of classifying everything into the most frequent
class, which was “positive“ in the training data.
For MNB, the results were lower than the baseline.
After expanding the tweets, the accuracy increased
to 73% for SVM and to 80.36% for MNB. We
concluded that MNB works better for Task A. This
is why the submitted runs used the MNB model
that was created from the expanded training data.
Then we used this to classify the Twitter and SMS
test data. The average F-score for the positive and
the negative class for our submitted runs can be
seen in Table 3, compared to the other systems
that participated in the task. We report this meas-
ure because it was the official evaluation measure
used in the task.
</bodyText>
<table confidence="0.997687">
System SVM MIB
Baseline 66.32% 66.32%
BOW features 66.32% 33.23%
BOW+ text expansion 73.00% 80.36%
</table>
<tableCaption confidence="0.9924115">
Table 2: Accuracy results for task A by 10-fold cross-
validation on the training data
</tableCaption>
<table confidence="0.99953025">
System Tweets SMS
uOttawa system 0.6020 0.5589
Median system 0.7489 0.7283
Best system 0.8893 0.8837
</table>
<tableCaption confidence="0.98728">
Table 3: Results for Task A for the submitted runs
(Average F-score for positive/negative class)
</tableCaption>
<bodyText confidence="0.9997582">
The precision, recall and F-score on the Twitter
and SMS test data for our submitted runs can be
seen in Tables 4 and 5, respectively. All our sub-
mitted runs were for the “constrained” task; no
additional training data was used.
</bodyText>
<table confidence="0.9943295">
Class Precision Recall F-Score
Positive 0.6934 0.7659 0.7278
Negative 0.5371 0.4276 0.4762
Neutral 0.0585 0.0688 0.0632
</table>
<tableCaption confidence="0.979096">
Table 4: Results for Tweet test data for Task A, for
each class.
</tableCaption>
<table confidence="0.9999365">
Class Precision Recall F-Score
Positive 0.5606 0.5705 0.5655
Negative 0.5998 0.5118 0.5523
Neutral 0.1159 0.2201 0.1518
</table>
<tableCaption confidence="0.986246">
Table 5: Results for SMS test data for Task A, for each
class.
</tableCaption>
<subsectionHeader confidence="0.999374">
3.2 Task B
</subsectionHeader>
<bodyText confidence="0.999974461538461">
First we present results on the training data (10-
fold cross-validation), then we present the results
for the submitted runs (also without any additional
training data).
Table 6 shows the overall accuracy for BOW
features for two classifiers, evaluated based on 10-
fold cross validation on the training data, for two
classifiers: SVM (SMO in Weka) and Multidimen-
sional Naïve Bays (MNB in Weka). The BOW
plus SentiWordNet features also include the num-
ber of positive and negative words identified from
SentiWordNet. The BOW plus extra features rep-
resentation includes the number of positive and
negative words identified from SentiWordNet,
General Inquirer, and Polarity Lexicon (six extra
features). The last row of the table shows the over-
all accuracy for BOW features plus all the extra
features mentioned in Section 2.2, including in-
formation extracted from SentiWordNet, Polarity
Lexicon, and General Inquirer. We can see that the
SentiWordNet features help, and that when includ-
ing all the extra features, the results improve even
more. We noticed that the features from the Polari-
ty Lexicon contributed the most. When we re-
moved GI, the accuracy did not change much; we
believe this is because GI has too small coverage.
</bodyText>
<table confidence="0.998461">
System SVM MIB
Baseline 48.50% 48.50%
BOW features 58.75% 59.56%
BOW+ SentiWordNet 69.43% 63.30%
BOW+ extra features 82.42% 73.09%
</table>
<tableCaption confidence="0.9931555">
Table 6: Accuracy results for task B by 10-fold cross-
validation on the training data
</tableCaption>
<bodyText confidence="0.99828">
The baseline in Table 6 is the accuracy of a triv-
ial classifier that puts everything in the most fre-
quent class, which is neutral/objective for the
training data (ZeroR classifier in Weka).
</bodyText>
<page confidence="0.995804">
382
</page>
<bodyText confidence="0.999894769230769">
The results of the submitted runs are in Table 7
for the two data sets. The features representation
was BOW plus SentiWordNet information. The
official evaluation measure is reported (average F-
score for the positive and negative class). The de-
tailed results for each class are presented in Tables
8 and 9.
In Table 7, we added an extra row for a new
uOttawa system (SVM with BOW plus extra fea-
tures) that uses the best classifier that we designed
(as chosen based on the experiments on the train-
ing data, see Table 6). This classifier uses SVM
with BOW and all the extra features.
</bodyText>
<table confidence="0.996192">
System Tweets SMS
uOttawa submitted 0.4251 0.4051
system
uOttawa new system 0.8684 0.9140
Median system 0.5150 0.4523
Best system 0.6902 0.6846
</table>
<tableCaption confidence="0.850527">
Table 7: Results for Task B for the submitted runs
(Average F-score for positive/negative).
</tableCaption>
<table confidence="0.99989325">
Class Precision Recall F-score
Positive 0.6206 0.5089 0.5592
Negative 0.4845 0.2080 0.2910
Neutral 0.5357 0.7402 0.6216
</table>
<tableCaption confidence="0.998957">
Table 8: Results for each class for task B, for the sub-
mitted system (SVM with BOW plus SentiWordNet
features) for the Twitter test data.
</tableCaption>
<table confidence="0.9998585">
Class Precision Recall F-score
Positive 0.4822 0.5508 0.5142
Negative 0.5643 0.2005 0.2959
Neutral 0.6932 0.7988 0.7423
</table>
<tableCaption confidence="0.996037">
Table 9: Results for each class for task B, for the sub-
mitted system (SVM with BOW plus SentiWordNet
features) for the SMS test data.
</tableCaption>
<sectionHeader confidence="0.997311" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999925944444444">
In Task A, we expanded upon the Twitter messag-
es from the training data using their keyword’s
definition and synonyms from SentiWordNet. We
showed that the expansion helped improve the
classification performance. In future work, we
would like to try an SVM using asymmetric soft-
boundaries to try and penalize the classifier for
missing items in the neutral class, the class with
the least items in the Task A training data.
The overall accuracy of the classifiers for Task
B increased a lot when we introduced the extra
features discussed in section 2.2. The overall accu-
racy of SVM increased from 58.75% to 82.42%
(as measures by cross-validation on the training
data). When applying this classifier on the two test
data sets, the results were very surprisingly good
(even higher that the best system submitted by the
SemEval participants for Task B1).
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999460592592593">
Stefano Baccianella, Andrea Esuli and Fabrizio Sebas-
tiani. SentiWordNet 3.0: An Enhanced Lexical Re-
source for Sentiment Analysis and Opinion Mining.
In Proceedings of the Seventh International Confer-
ence on Language Resources and Evaluation
(LREC&apos;10), Valletta, Malta, May 2010.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. The General Inquirer: A
computer approach to content analysis. MIT Press,
1966.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. Feature-Rich Part-of-Speech
Tagging with a Cyclic Dependency Network. In Pro-
ceedings of HLT-NAACL 2003, pp. 252-259, 2003.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov and Alan Ritter.
SemEval-2013 Task 2: Sentiment Analysis in Twit-
ter. In Proceedings of the International Workshop on
Semantic Evaluation SemEval &apos;13, Atlanta, Georgia,
June 2013.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
Recognizing contextual polarity in phrase- level sen-
timent analysis. In Proceedings of HLT/ EMNLP
2005.
Ian H. Witten and Eibe Frank. Data Mining: Practical
Machine Learning Tools and Techniques, 2nd edi-
tion, Morgan Kaufmann, San Francisco, 2005.
</reference>
<footnote confidence="0.525103">
1 Computed with the provided scoring script.
</footnote>
<page confidence="0.99834">
383
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936650">
<title confidence="0.992125">uOttawa: System description for SemEval 2013 Task 2 Sentiment Analysis in Twitter</title>
<author confidence="0.993291">Hamid Poursepanj</author>
<author confidence="0.993291">Josh Weissbock</author>
<author confidence="0.993291">Diana</author>
<affiliation confidence="0.9997765">School of Electrical Engineering and Computer University of</affiliation>
<address confidence="0.998162">Ottawa, K1N6N5, Canada</address>
<email confidence="0.991145">jweis035,Diana.Inkpen}@uottawa.ca</email>
<abstract confidence="0.997360384615385">We present two systems developed at the University of Ottawa for the SemEval 2013 Task 2. The first system (for Task A) classifies the polarity / sentiment orientation of one target word in a Twitter message. The second system (for Task B) classifies the polarity of whole Twitter messages. Our two systems are very simple, based on supervised classifiers with bag-ofwords feature representation, enriched with information from several sources. We present a few additional results, besides results of the submitted runs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="1883" citStr="Baccianella et al., 2010" startWordPosition="293" endWordPosition="296">egative, and neutral/objective. The training data that was made available for each task consisted in annotated Twitter message. There were two test sets for each task, one composed of Twitter messages and one of SMS message (even if there was no specific training data for SMS messages). See more details about the datasets in (Wilson et al., 2013). 2 System Description We used supervised learning classifiers from Weka (Witten and Frank, 2005). Initially we extracted simple bag-of-word features (BOW). For the submitted systems, we also used features calculated based on SentiWordNet information (Baccianella et al., 2010). SentiWordNet contains positivity, negativity, and objectivity scores for each sense of a word. We explain below how this information was used for each task. As classifiers, we used Support Vector Machines (SVM) (SMO and libSVM from Weka with default values for parameters), because SVM is known to perform well on many tasks, and Multinomial Naive Bayes (MNB), because MNB is known to perform well on text data and it is faster than SVM. 2.1 Task A Our system for Task A involved two parts: the expansion of our training data and the classification. The expansion was done with information from Sen</context>
<context position="5627" citStr="Baccianella et al, 2010" startWordPosition="927" endWordPosition="930"> he did a bully job a neat sports car had a great time at the party you look simply smashing Table 1: Example of tweet expansion for Task A The target word’s definition and synonyms were then concatenated to the original tweet. No additional changes were made to either the original tweet or the features that were added from SentiWordNet. An example follows in Table 1. The test data (Twitter and SMS) was not expanded, because there are no labels in the test data to be able to choose the sense with corresponding sentiment. 2.2 Task B For this task, we used the following resources: SentiwordNet (Baccianella et al, 2010), the Polarity Lexicon (Wilson et al., 2005), the General Inquirer (Stone et al., 1966), and the Stanford NLP tools (Toutanova et al., 2003) for preprocessing and feature selection. The preprocessing of Twitter messages is implemented in three steps namely, stop-word removal, stemming, and removal of words with occurrence frequency of one. Several extra features will be used: the number of positive words and negative words identified by three lexical resources mentioned above, the number of emoticons, the number of elongated words, and the number of punctuation tokens (single or repeated excla</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli and Fabrizio Sebastiani. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10), Valletta, Malta, May 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A computer approach to content analysis.</title>
<date>1966</date>
<publisher>MIT Press,</publisher>
<contexts>
<context position="5714" citStr="Stone et al., 1966" startWordPosition="943" endWordPosition="946"> Table 1: Example of tweet expansion for Task A The target word’s definition and synonyms were then concatenated to the original tweet. No additional changes were made to either the original tweet or the features that were added from SentiWordNet. An example follows in Table 1. The test data (Twitter and SMS) was not expanded, because there are no labels in the test data to be able to choose the sense with corresponding sentiment. 2.2 Task B For this task, we used the following resources: SentiwordNet (Baccianella et al, 2010), the Polarity Lexicon (Wilson et al., 2005), the General Inquirer (Stone et al., 1966), and the Stanford NLP tools (Toutanova et al., 2003) for preprocessing and feature selection. The preprocessing of Twitter messages is implemented in three steps namely, stop-word removal, stemming, and removal of words with occurrence frequency of one. Several extra features will be used: the number of positive words and negative words identified by three lexical resources mentioned above, the number of emoticons, the number of elongated words, and the number of punctuation tokens (single or repeated exclamation marks, etc.). As for SentiWordNet, for each word a score is calculated that show</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. The General Inquirer: A computer approach to content analysis. MIT Press, 1966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="5767" citStr="Toutanova et al., 2003" startWordPosition="952" endWordPosition="955">he target word’s definition and synonyms were then concatenated to the original tweet. No additional changes were made to either the original tweet or the features that were added from SentiWordNet. An example follows in Table 1. The test data (Twitter and SMS) was not expanded, because there are no labels in the test data to be able to choose the sense with corresponding sentiment. 2.2 Task B For this task, we used the following resources: SentiwordNet (Baccianella et al, 2010), the Polarity Lexicon (Wilson et al., 2005), the General Inquirer (Stone et al., 1966), and the Stanford NLP tools (Toutanova et al., 2003) for preprocessing and feature selection. The preprocessing of Twitter messages is implemented in three steps namely, stop-word removal, stemming, and removal of words with occurrence frequency of one. Several extra features will be used: the number of positive words and negative words identified by three lexical resources mentioned above, the number of emoticons, the number of elongated words, and the number of punctuation tokens (single or repeated exclamation marks, etc.). As for SentiWordNet, for each word a score is calculated that shows the positive or negative weight of that word. No se</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<title>SemEval-2013 Task 2: Sentiment Analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation SemEval &apos;13,</booktitle>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1606" citStr="Wilson et al., 2013" startWordPosition="253" endWordPosition="256">bjective). In fact, the neutral and objective were joined in one class for the requirements of the shared task. Task A contained target words whose sense had to be classified in the context, while Task B was to classify each text into one of the three classes: positive, negative, and neutral/objective. The training data that was made available for each task consisted in annotated Twitter message. There were two test sets for each task, one composed of Twitter messages and one of SMS message (even if there was no specific training data for SMS messages). See more details about the datasets in (Wilson et al., 2013). 2 System Description We used supervised learning classifiers from Weka (Witten and Frank, 2005). Initially we extracted simple bag-of-word features (BOW). For the submitted systems, we also used features calculated based on SentiWordNet information (Baccianella et al., 2010). SentiWordNet contains positivity, negativity, and objectivity scores for each sense of a word. We explain below how this information was used for each task. As classifiers, we used Support Vector Machines (SVM) (SMO and libSVM from Weka with default values for parameters), because SVM is known to perform well on many ta</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov and Alan Ritter. SemEval-2013 Task 2: Sentiment Analysis in Twitter. In Proceedings of the International Workshop on Semantic Evaluation SemEval &apos;13, Atlanta, Georgia, June 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase- level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/ EMNLP</booktitle>
<contexts>
<context position="5671" citStr="Wilson et al., 2005" startWordPosition="935" endWordPosition="938">t time at the party you look simply smashing Table 1: Example of tweet expansion for Task A The target word’s definition and synonyms were then concatenated to the original tweet. No additional changes were made to either the original tweet or the features that were added from SentiWordNet. An example follows in Table 1. The test data (Twitter and SMS) was not expanded, because there are no labels in the test data to be able to choose the sense with corresponding sentiment. 2.2 Task B For this task, we used the following resources: SentiwordNet (Baccianella et al, 2010), the Polarity Lexicon (Wilson et al., 2005), the General Inquirer (Stone et al., 1966), and the Stanford NLP tools (Toutanova et al., 2003) for preprocessing and feature selection. The preprocessing of Twitter messages is implemented in three steps namely, stop-word removal, stemming, and removal of words with occurrence frequency of one. Several extra features will be used: the number of positive words and negative words identified by three lexical resources mentioned above, the number of emoticons, the number of elongated words, and the number of punctuation tokens (single or repeated exclamation marks, etc.). As for SentiWordNet, fo</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe and Paul Hoffmann. Recognizing contextual polarity in phrase- level sentiment analysis. In Proceedings of HLT/ EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques, 2nd edition,</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="1703" citStr="Witten and Frank, 2005" startWordPosition="268" endWordPosition="271"> the shared task. Task A contained target words whose sense had to be classified in the context, while Task B was to classify each text into one of the three classes: positive, negative, and neutral/objective. The training data that was made available for each task consisted in annotated Twitter message. There were two test sets for each task, one composed of Twitter messages and one of SMS message (even if there was no specific training data for SMS messages). See more details about the datasets in (Wilson et al., 2013). 2 System Description We used supervised learning classifiers from Weka (Witten and Frank, 2005). Initially we extracted simple bag-of-word features (BOW). For the submitted systems, we also used features calculated based on SentiWordNet information (Baccianella et al., 2010). SentiWordNet contains positivity, negativity, and objectivity scores for each sense of a word. We explain below how this information was used for each task. As classifiers, we used Support Vector Machines (SVM) (SMO and libSVM from Weka with default values for parameters), because SVM is known to perform well on many tasks, and Multinomial Naive Bayes (MNB), because MNB is known to perform well on text data and it </context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. Data Mining: Practical Machine Learning Tools and Techniques, 2nd edition, Morgan Kaufmann, San Francisco, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>