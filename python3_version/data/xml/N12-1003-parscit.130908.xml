<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004382">
<title confidence="0.997942">
Identifying High-Level Organizational Elements
in Argumentative Discourse
</title>
<author confidence="0.980066">
Nitin Madnani Michael Heilman Joel Tetreault
</author>
<affiliation confidence="0.900849">
Educational Testing Service
</affiliation>
<address confidence="0.907381">
Princeton, NJ, USA
</address>
<email confidence="0.997777">
{nmadnani,mheilman,jtetreault}@ets.org
</email>
<author confidence="0.987868">
Martin Chodorow
</author>
<affiliation confidence="0.987205">
Hunter College of CUNY
</affiliation>
<address confidence="0.806365">
New York, NY, USA
</address>
<email confidence="0.997277">
martin.chodorow@hunter.cuny.edu
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999746235294118">
Argumentative discourse contains not only
language expressing claims and evidence, but
also language used to organize these claims
and pieces of evidence. Differentiating be-
tween the two may be useful for many appli-
cations, such as those that focus on the content
(e.g., relation extraction) of arguments and
those that focus on the structure of arguments
(e.g., automated essay scoring). We propose
an automated approach to detecting high-level
organizational elements in argumentative dis-
course that combines a rule-based system and
a probabilistic sequence model in a principled
manner. We present quantitative results on a
dataset of human-annotated persuasive essays,
and qualitative analyses of performance on es-
says and on political debates.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916636363636">
When presenting an argument, a writer or speaker
usually cannot simply state a list of claims and
pieces of evidence. Instead, the arguer must explic-
itly structure those claims and pieces of evidence, as
well as explain how they relate to an opponent’s ar-
gument. Consider example 1 below, adapted from
an essay rebutting an opponent’s argument that griz-
zly bears lived in a specific region of Canada.
The argument states that based on the
result of the recent research, there proba-
bly were grizzly bears in Labrador. It may
</bodyText>
<page confidence="0.917489">
20
</page>
<bodyText confidence="0.999885192307692">
seem reasonable at first glance, but ac-
tually, there are some logical mistakes
in it. ... There is a possibility that they
were a third kind of bear apart from black
and grizzly bears. Also, the explorer ac-
counts were recorded in the nineteenth
century, which was more than 100 years
ago. ... In sum, the conclusion of this
argument is not reasonable since the ac-
count and the research are not convinc-
ing enough... .
The argument begins by explicitly restating the
opponent’s claim, prefacing the claim with the
phrase “The argument states that.” Then, the sec-
ond sentence explicitly marks the opponent’s argu-
ment as flawed. Later on, the phrase “There is a
possibility that” indicates the subsequent clause in-
troduces evidence contrary to the opponent’s claim.
Finally, the sentence “In sum, ...” sums up the ar-
guer’s stance in relation to the opponent’s claim.1
As illustrated in the above example, argumenta-
tive discourse can be viewed as consisting of lan-
guage used to express claims and evidence, and
language used to organize them. We believe that
differentiating organizational elements from content
would be useful for analyzing persuasive discourse.
</bodyText>
<footnote confidence="0.98936625">
1The word Also signals that additional evidence is about to
be presented and should also be marked as shell. However, it
was not marked in this specific case by our human annotator
(§3.2).
</footnote>
<note confidence="0.603215">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20–28,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998061">
We refer to such organizational elements as shell, in-
dicating that they differ from the specific claims and
evidence, or “meat,” of an argument. In this work,
we develop techniques for detecting shell in texts.
We envision potential applications in political sci-
ence (e.g., to better understand political debates), in-
formation extraction or retrieval (e.g., to help a sys-
tem focus on content rather than organization), and
automated essay scoring (e.g., to analyze the quality
of a test-taker’s argument), though additional work
is needed to determine exactly how to integrate our
approach into such applications.
Detecting organizational elements could also be a
first step in parsing an argument to infer its structure.
We focus on this initial step, leaving the other steps
of categorization of spans (as to whether they evalu-
ate the opponent’s claims, connect one’s own claims,
etc.), and the inference of argumentation structure to
future work.
Before describing our approach to identifying
shell, we begin by defining it. Shell refers to se-
quences of words used to refer to claims and evi-
dence in persuasive writing or speaking, providing
an organizational framework for an argument. It
may be used by the writer or the speaker in the fol-
lowing ways:
</bodyText>
<listItem confidence="0.9885964">
• to declare one’s own claims (e.g., “There is the
possibility that”)
• to restate an opponent’s claims (e.g., “The argu-
ment states that”)
• to evaluate an opponent’s claims (e.g., “It may
seem reasonable at first glance, but actually, there
are some logical mistakes in it”)
• to present evidence and relate it to specific claims
(e.g., “To illustrate my point, I will now give the
example of”)
</listItem>
<bodyText confidence="0.999808692307692">
There are many ways of analyzing discourse. The
most relevant is perhaps rhetorical structure theory
(RST) (Mann and Thompson, 1988). To our knowl-
edge, the RST parser from Marcu (2000) is the only
RST parser readily available for experimentation.
The parser is trained to model the RST corpus (Carl-
son et al., 2001), which treats complete clauses (i.e.,
clauses with their obligatory complements) as the el-
ementary units of analysis. Thus, the parser treats
the first sentence in example 1 as a single unit and
does not differentiate between the main and subordi-
nate clauses. In contrast, our approach distinguishes
the sequence “The argument states that ...” as shell
(which is used here to restate the external claim).
Furthermore, we identify the entire second sentence
as shell (here, used to evaluate the external claim),
whereas the RST parser splits the sentence into two
clauses, “It may seem ...” and “but actually ...”,
linked by a “contrast” relationship.2 Finally, our
approach focuses on explicit markers of organiza-
tional structure in arguments, whereas RST covers a
broader range of discourse connections (e.g., elabo-
ration, background information, etc.), including im-
plicit ones. (Note that additional related work is de-
scribed in §6.)
This work makes the following contributions:
</bodyText>
<listItem confidence="0.98419275">
• We describe a principled approach to the task
of detecting high-level organizational elements in
argumentative discourse, combining rules and a
probabilistic sequence model (§2).
• We conduct experiments to validate the approach
on an annotated sample of essays (§3, §4).
• We qualitatively explore how the approach per-
forms in a new domain: political debate (§5).
</listItem>
<sectionHeader confidence="0.998647" genericHeader="method">
2 Detection Methods
</sectionHeader>
<bodyText confidence="0.9999035">
In this section, we describe three approaches to the
problem of shell detection: a rule-based system
(§2.1), a supervised probabilistic sequence model
(§2.2), and a simple lexical baseline (§2.3).
</bodyText>
<subsectionHeader confidence="0.986663">
2.1 Rule-based system
</subsectionHeader>
<bodyText confidence="0.9999562">
We begin by describing a knowledge-based ap-
proach to detecting organizational elements in argu-
mentative discourse. This approach uses a set of 25
hand-written regular expression patterns.3
In order to develop these patterns, we created a
sample of 170 annotated essays across 57 distinct
prompts.4 The essays were written by test-takers of
a standardized test for graduate admissions. This
sample of essays was similar in nature to but did
not overlap with those discussed in other sections
</bodyText>
<footnote confidence="0.997140857142857">
2We used the RST parser of Marcu (2000) to analyze the
original essay from which the example was adapted.
3We use the PyParsing toolkit to parse sentences with the
grammar for the rule system.
4Prompts are short texts that present an argument or issue
and ask test takers to respond to it, either by analyzing the given
argument or taking a stance on the given issue.
</footnote>
<page confidence="0.993698">
21
</page>
<equation confidence="0.9578816">
MODAL —* do  |don’t  |can  |cannot  |will  |would  |...
ADVERB —* strongly  |totally  |fundamentally  |vehemently  |.. .
AGREEVERB —* disagree  |agree  |concur  |...
AUTHORNOUN —* writer  |author  |speaker  |...
SHELL —* I [MODAL] [ADVERB] AGREEVERB with the AUTHORNOUN
</equation>
<figureCaption confidence="0.9672125">
Figure 1: An example pattern that recognizes shell language describing the author’s position with respect to an oppo-
nent’s, e.g., I totally agree with the author or I will strongly disagree with the speaker.
</figureCaption>
<bodyText confidence="0.999880470588235">
of the paper (§2.2, §3.2). The annotations were car- L(θ|w, y) = N ( )
ried out by individuals experienced in scoring per- = z=1 log p y(z)  |w(z), θ (1)
suasive writing. No formal annotation guidelines N (θTf(w(z), y(z)) − log Z(z))
were provided. Besides shell language, there were z=1
other annotations relevant to essay scoring. How-
ever, we ignored them for this study because they
are not directly relevant to the task of shell language
detection.
From this sample, we computed lists of n-grams
(n = 1, 2, ... , 9) that occurred more than once in
essays from at least half of the 57 distinct essay
prompts. We then wrote rules to recognize the shell
language present in the n-gram lists. Additional
rules were added to cover instances of shell that we
observed in the annotated essays but that were not
frequent enough to appear in the n-gram analysis.
We use “Rules” to refer to this method.
</bodyText>
<subsectionHeader confidence="0.99667">
2.2 Supervised Sequence Model
</subsectionHeader>
<bodyText confidence="0.942038230769231">
The next approach we describe is a supervised, prob-
abilistic sequence model based on conditional ran-
dom fields (CRFs) (Lafferty et al., 2001), using a
small number of general features based on lexical
frequencies. We assume access to a labeled dataset
of N examples (w, y) indexed by i, containing se-
quences of words w(z) and sequences of labels y(z),
with individual words and labels indexed by j (§3
describes our development and testing sets). y(z) is a
sequence of binary values, indicating whether each
word w(z)
� in the sequence is shell (y(z)
� = 1) or not
</bodyText>
<page confidence="0.849222">
7
</page>
<bodyText confidence="0.980092692307692">
((z) = 0). Following Lafferty et al. (2001), we find
y
a parameter vector θ that maximizes the following
log-likelihood objective function:
The normalization constant Zz is a sum over all
possible label sequences for the ith example, and f
is a feature function that takes pairs of word and la-
bel sequences and returns a vector of feature values,
equal in dimensions to the number of parameters in
θ.5
The feature values for the jth word and label pair
are as follows (these are summed over all elements
to compute the values of f for the entire sequence):
</bodyText>
<listItem confidence="0.995020117647059">
• The relative frequency of w(z)
� in the British Na-
tional Corpus.
• The relative frequency of w(z)
� in a set of 100,000
essays (see below).
• Eight binary features for whether the above fre-
quencies meet or exceed the following thresholds:
10{−6,−5,−4,−3}.
• The proportion of prompts for which w(z)
� ap-
peared in at least one essay about that prompt in
the set of 100,000.
• Three binary features for whether the above pro-
portion of prompts meets or exceeds the following
thresholds: 10.25, 0.50, 0.75}.
• A binary feature with value 1 if w(z)
</listItem>
<bodyText confidence="0.798843">
� consists only
of letters a-z, and 0 otherwise. This feature dis-
tinguishes punctuation and numbers from other to-
kens.
</bodyText>
<footnote confidence="0.9796495">
5We used CRFsuite 0.12 (Okazaki, 2007) to implement the
CRF model.
</footnote>
<page confidence="0.995546">
22
</page>
<listItem confidence="0.987751769230769">
• A binary feature with value 1 if the rule-based sys-
tem predicts that w(i)
j is shell, and 0 otherwise.
• A binary feature with value 1 if the rule-based sys-
tem predicts thatw(i)
j�1 is shell, and 0 otherwise.
• Two binary features for whether or not the current
token was the first or last in the sentence, respec-
tively.
• Four binary features for the possible transitions
between previous and current labels (y(i)
j and y(i)
j�1,
</listItem>
<bodyText confidence="0.976878421052632">
respectively).
To define the features related to essay prompts
and lexical frequencies in essays, we created a set
of 100,000 essays from a larger set of essays written
by test-takers of a standardized test for graduate ad-
missions (the same domain as in §2.1). The essays
were written in response to 228 different prompts
that asked students to analyze various issues or ar-
guments. We use additional essays sampled from
this source later to acquire annotated training and
test data (§3.2).
We developed the above feature set using cross-
validation on our development set (§3). The intu-
ition behind developing the word frequency features
is that shell language generally consists of chunks of
words that occur frequently in persuasive language
(e.g., “claims,” “conclude”) but not necessarily as
frequently in general text (e.g., the BNC). The se-
quence model can also learn to disprefer changes of
state, such that multi-word subsequences are labeled
as shell even though some of the individual words in
the subsequence are stop words, punctuation, etc.
Note there are a relatively small number of pa-
rameters in the model,6 which allows us to estimate
parameters on a relatively small set of labeled data.
We briefly experimented with adding an E2 penalty
on the magnitude of 0 in Equation 2, but this did not
seem to improve performance.
When making predictions y(i) about the label se-
quence for a new sentence, the most common ap-
proach is to find the most likely sequence of labels y
given the words w(i), found with Viterbi decoding:
6There were 42 parameters in our implementation of the full
CRF model. Excluding the four transition features, each of the
19 features had two parameters, one for the positive class and
one for the negative class. Having two parameters for each is
unnecessary, but we are not aware of how to have the crfsuite
toolkit avoid these extra features.
</bodyText>
<equation confidence="0.9700155">
y(i) = argmax My I w(i)) (2)
y
</equation>
<bodyText confidence="0.996596210526316">
We use “CRFv” to refer to this approach. We use
the suffix “+R” to denote models that include the
two rule-based system prediction features, and we
use “-R” to denote models that exclude these two
features.
In development, we observed that this decoding
approach seemed to very strongly prefer labeling an
entire sentence as shell or not, which is often not
desirable since shell often appears at just the begin-
nings of sentences (e.g., “The argument states that”).
We therefore test an alternative prediction rule
that works at the word-level, rather than sequence-
level. This approach labels each word as shell if
the sum of the probabilities of all paths in which
the word was labeled as shell—that is, the marginal
probability—exceeds some threshold A. Words are
labeled as non-shell otherwise. Specifically, an indi-
vidual word w(i)
j is labeled as shell (i.e., �y(i)
</bodyText>
<equation confidence="0.545605">
j = 1)
</equation>
<bodyText confidence="0.997229666666667">
according to the following equation, where 1(q) is
an indicator function that returns 1 if its argument q
is true, and 0 otherwise.
</bodyText>
<equation confidence="0.690251">
y�i) = 1 ((E
y � My 1 w(i)) yj &gt; A (3)
</equation>
<bodyText confidence="0.996688666666667">
We tune A using the development set, as discussed
in §3.
We use “CRFm” to refer to this approach.
</bodyText>
<subsectionHeader confidence="0.999222">
2.3 Lexical Baseline
</subsectionHeader>
<bodyText confidence="0.999961363636363">
As a simple baseline, we also evaluated a method
that labels words as shell if they appear frequently
in persuasive writing—specifically, in the set of
100,000 unannotated essays described in §2.2. In
this approach, word tokens are marked as shell
if they belonged to the set of k most frequent
words from the essays. Using the development
set discussed in §3.2, we tested values of k in
1100, 200, ... ,1000}. Setting k = 700 led to the
highest F1.
We use “TopWords” to refer to this method.
</bodyText>
<page confidence="0.996773">
23
</page>
<sectionHeader confidence="0.999682" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999895">
In this section, we discuss the design of our exper-
imental evaluation and present results on our devel-
opment set, which we used to select the final meth-
ods to evaluate on the held-out test set.
</bodyText>
<subsectionHeader confidence="0.996521">
3.1 Metrics
</subsectionHeader>
<bodyText confidence="0.9999806">
In our experiments, we evaluated the performance
of the shell detection methods by comparing token-
level system predictions to human labels. Shell lan-
guage typically occurs as fairly long sequences of
words, but identifying the exact span of a sequence
of shell seems less important than in related tag-
ging tasks, such as named entity recognition. There-
fore, rather than evaluating based on spans (either
with exact or a partial credit system), we measured
performance at the word token-level using standard
metrics: precision, recall, and the F1 measure. For
example, for precision, we computed the propor-
tion of tokens predicted as shell by a system that
were also labeled as shell in our human-annotated
datasets.
</bodyText>
<subsectionHeader confidence="0.999242">
3.2 Annotated Data
</subsectionHeader>
<bodyText confidence="0.999717045454545">
To evaluate the methods described in §2, we gath-
ered annotations for 200 essays that were not in the
larger, unannotated set discussed in §2.2. We split
this set of essays into a development set of 150 es-
says (68,601 word tokens) and a held-out test set of
50 essays (21,277 word tokens). An individual with
extensive experience at scoring persuasive writing
and familiarity with shell language annotated all to-
kens in the essays with judgments of whether they
were shell or not (in contrast to §2.1, this annotation
only involved labeling shell language).
From the first annotator’s judgments on the devel-
opment set, we created a set of annotation guidelines
and trained a second annotator. The second anno-
tator marked the held-out test set so that we could
measure human agreement. Comparing the two an-
notators’ test set annotations, we observed agree-
ment of F1 = 0.736 and Cohen’s r. = 0.699 (we
do not use r. in our experiments but report it here
since it is a common measure of human agreement).
Except for measuring agreement, we did not use the
second annotator’s judgments in our experiments.7
</bodyText>
<footnote confidence="0.944941">
7In the version of this paper submitted for review, we mea-
</footnote>
<figureCaption confidence="0.9976118">
Figure 2: Precision and recall of the detection methods at
various thresholds, computed through cross-validation on
the development set. Points indicate performance for the
rule-based and baseline system as well as points where
Fl is highest.
</figureCaption>
<subsectionHeader confidence="0.998199">
3.3 Cross-validation Results
</subsectionHeader>
<bodyText confidence="0.997474428571429">
To develop the CRF’s feature set, to tune hyperpa-
rameters, and to select the most promising systems
to evaluate on the test set, we randomly split the sen-
tences from the development set into two halves and
conducted tests with two-fold cross-validation.
We tested thresholds for the CRF at A =
10.01, 0.02, ... ,1.00}.
Figure 2 shows the results on the development set.
For the rule-based system, which did not require la-
beled data, performance is computed on the entire
development set. For the CRF approaches, the pre-
cision and recall were computed after concatenating
predictions on each of the cross-validation folds.
The TopWords baseline performed quite poorly,
with F1 = 0.205. The rule-based system performed
much better, with F1 = 0.382, but still not as well
as the CRF systems. The CRF systems that pre-
dict maximum sequences had F1 = 0.382 without
the rule-based system features (CRF„_R), and F1 =
0.467 with the rule-based features (CRF„+R). The
CRF systems that made predictions from marginal
scores performed best, with F1 = 0.516 without
the rule-based features, and F1 = 0.551 with the
rule-based features. Thus, both the rule-based sys-
sured test set agreement with judgments from a third individ-
ual, who was informally trained by the first, without the formal
guidelines. Agreement was somewhat lower: Fl = 0.668 and
r, = 0.613.
</bodyText>
<figure confidence="0.999482476190476">
0.0 0.2 0.4 0.6 0.8 1.0
recall
points
CRFm−R
CRFm+R
CRFv−R
CRFv+R
Rules
TopWords
●
precision
0.8
0.6
0.4
0.2
0.0
1.0
●
lines
CRFm−R
CRFm+R
</figure>
<page confidence="0.994785">
24
</page>
<table confidence="0.9998505">
Method P R F1 Len
TopWords 0.125 0.759 0.214 * 2.80
Rules 0.561 0.360 0.439 * 4.99
CRF„−R 0.729 0.268 0.392 * 15.67
CRF„+R 0.763 0.369 0.498 * 13.30
CRFm−R 0.586 0.574 0.580 9.00
CRFm+R 0.556 0.670 0.607 9.96
Human 0.685 0.796 0.736 * 7.91
</table>
<tableCaption confidence="0.9427102">
Table 1: Performance on the held-out test set, in terms of
precision (P), recall (R), F1 measure, and average length
in tokens of sequences of one or more words labeled as
shell (Len). * indicates F1 scores that are statistically
reliably different from CRF,,,,,+R at the p &lt; 0.01 level.
</tableCaption>
<bodyText confidence="0.999866866666667">
tem features and the marginal prediction approach
led to gains in performance.
From an examination of the predictions from the
CRFm+R and CRFm−R systems, it appears that a
major contribution of the features derived from the
rule-based system is to help the hybrid CRFm+R
system avoid tagging entire sentences as shell when
only parts of them are actually shell. For exam-
ple, consider the sentence “According to this state-
ment, the speaker asserts that technology can not
only influence but also determine social customs and
ethics” (typographical errors included). CRFm−R
tags everything up to “determine” as shell, whereas
the rule-based system and CRFm+R correctly stop
after “asserts that.”
</bodyText>
<sectionHeader confidence="0.948613" genericHeader="method">
4 Test Set Results
</sectionHeader>
<bodyText confidence="0.99999564">
Next, we present results on the held-out test set.
For the CRFm systems, we used the thresholds that
led to the highest F1 scores on the development
set (A = 0.26 for CRFm+R and A = 0.32 for
CRFm−R). Table 1 presents the results for all sys-
tems, along with results comparing the second anno-
tator’s labels (“Human”) to the gold standard labels
from the first annotator.
The same pattern emerged as on the development
set, with CRFm+R performing the best. The F1
score of 0.607 for the CRFm+R system was rel-
atively close to the F1 score of 0.736 for agree-
ment between human annotators. To test whether
CRFm+R’s relatively high performance was due to
chance, we computed 99% confidence intervals for
the differences in F1 score between CRFm+R and
each of the other methods. We used the bias-
corrected and accelerated (BC,,,) Bootstrap (Efron
and Tibshirani, 1993) with 10,000 rounds of resam-
pling at the sentence level for each comparison. A
difference is statistically reliable at the α level (i.e.,
p &lt; α) if the (1 − α)% confidence interval for the
difference does not contain zero, which corresponds
to the null hypothesis. Statistically reliable differ-
ences are indicated in Table 1. The only system that
did not have a reliably lower F1 score than CRFm+R
was CRFm−R, though due to the relatively small
size of our test set, we do not take this as strong ev-
idence against using the rule-based system features
in the CRF.
We note that while the CRFm+R system had lower
precision (0.556) than the CRF„+R system (0.763),
its threshold A could be tuned to prefer high preci-
sion rather than the best development set F1. Such
tuning could be very important depending on the rel-
ative costs of false positives and false negatives for
a particular application.
We also computed the mean length of sequences
of one or more contiguous words labeled as shell.
Here also, we observed that the CRFm+R approach
provided a close match to human performance. The
mean lengths of shell for the first and second anno-
tators were 8.49 and 7.91 tokens, respectively. For
the CRFm+R approach, the mean length was slightly
higher at 9.96 tokens, but this was much closer to the
means of the human annotators than the mean for
the CRF„+R system, which was 13.30 tokens. For
the rule-based system, the mean length was 4.99 to-
kens, indicating that it captures short sequences such
as “In addition,” more often than the other systems.
</bodyText>
<sectionHeader confidence="0.891146" genericHeader="method">
5 Observations about a New Domain
</sectionHeader>
<bodyText confidence="0.999774142857143">
In this section, we apply our system to a corpus of
transcripts of political debates8 in order to under-
stand whether the system can generalize to a new
domain with a somewhat different style of argu-
mentation. Our analyses are primarily qualitative
in nature due to the lack of gold-standard annota-
tions. We chose two historically well-known debates
</bodyText>
<footnote confidence="0.999946">
8The Lincoln–Douglas debates were downloaded from
http://www.bartleby.com/251/. The other debates
were downloaded from http://debates.org/.
</footnote>
<page confidence="0.998742">
25
</page>
<bodyText confidence="0.999743">
(Lincoln–Douglas from 1858 and Kennedy–Nixon
from 1960) and two debates that occurred more re-
cently (Gore–Bush from 2000 and Obama–McCain
from 2008). These debates range in length from
38,000 word tokens to 65,000 word tokens.
Political debates are similar to the persuasive es-
says we used above in that debate participants state
their own claims and evidence as well as evaluate
their opponents’ claims. They are different from es-
says in that they are spoken rather than written—
meaning that they contain more disfluencies, collo-
quial language, etc.—and that they cover different
social and economic issues. Also, the debates are in
some sense a dialogue between two people.
We tagged all the debates using the CRF,,t+R sys-
tem, using the same parameters as for the test set
experiments (§4).
First, we observed that a smaller percentage of
tokens were tagged as shell in the debates than in
the essays. For the annotated essay test set (§3.2),
the percentage of tokens tagged as shell was 14.0%
(11.6% were labeled as shell by the first annota-
tor). In contrast, the percentage of tokens tagged
as shell was 4.2% for Lincoln–Douglas, 5.4% for
Kennedy–Nixon, 4.6% for Gore–Bush, and 4.8% for
Obama–McCain. It is not completely clear whether
the smaller percentages tagged as shell are due to a
lack of coverage by the shell detector or more sub-
stantial differences in the domain.
However, it seems that these debates genuinely in-
clude less shell. One potential reason is that many of
the essay prompts asked test-takers to respond to a
particular argument, leading to responses containing
many phrases such as “The speaker claims that” and
“However, the argument lacks specificity ... ”.
We analyzed the system’s predictions and ex-
tracted a set of examples, some of which appear in
Table 2, showing true positives, where most of the
tokens appear to be labeled correctly as shell; false
positives, where tokens were incorrectly labeled as
shell; and false negatives, where the system missed
tokens that should have been marked.
Table 2 also provides some examples from our de-
velopment set, for comparison.
We observed many instances of correctly marked
shell, including many that appeared very different
in style than the language used in essays. For ex-
ample, Lincoln demonstrates an aggressive style in
the following: “Now, I say that there is no charitable
way to look at that statement, except to conclude that
he is actually crazy.” Also, Bush employs a some-
what atypical sentence structure here: “It’s not what
I think and its not my intentions and not my plan.”
However, the system also incorrectly tagged se-
quences as shell, particularly in short sentences (e.g.,
“Are we as strong as we should be?”). It also missed
shell, partially or entirely, such as in the following
example: “But let’s get back to the core issue here.”
These results suggest that although there is poten-
tial for improvement in adapting to new domains,
our approach to shell detection at least partially gen-
eralizes beyond our initial domain of persuasive es-
say writing.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999579466666667">
There has been much previous work on analyzing
discourse. In this section, we describe similarities
and differences between that work and ours.
Rhetorical structure theory (Mann and Thomp-
son, 1988) is perhaps the most relevant area of work.
See §1 for a discussion.
In research on intentional structure, Grosz and
Sidner (1986) propose that any discourse is com-
posed of three interacting components: the linguistic
structure defined by the actual utterances, the inten-
tional structure defined by the purposes underlying
the discourse, and an attentional structure defined by
the discourse participants’ focus of attention. De-
tecting shell may also be seen as trying to identify
explicit cues of intentional structure in a discourse.
Additionally, the categorization of shell spans as to
whether they evaluate the opponents claims, connect
ones own claims, etc., may be seen as determining
what Grosz and Sidener call “discourse segment pur-
poses” (i.e., the intentions underlying the segments
containing the shell spans).
We can also view shell detection as the task of
identifying phrases that indicate certain types of
speech acts (Searle, 1975). In particular, we aim to
identify markers of assertive speech acts, which de-
clare that the speaker believes a certain proposition,
and expressive speech acts, which express attitudes
toward propositions.
Shell also overlaps with the concept of discourse
markers (Hutchinson, 2004), such as “however” or
</bodyText>
<page confidence="0.988539">
26
</page>
<table confidence="0.991030142857143">
LINCOLN (L) — DOUGLAS (D) DEBATES
TP L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is
actually crazy.
L: The first thing I see fit to notice is the fact that ...
FP D: He became noted as the author of the scheme to ...
D: ... such amendments were to be made to it as would render it useless and inefficient ...
FN D: I wish to impress it upon you, that every man who voted for those resolutions ...
L: That statement he makes, too, in the teeth of the knowledge that I had made the stipulation to
come down here ...
KENNEDY (K) — NIXON (N) DEBATES
TP N: I favor that because I believe that’s the best way to aid our schools ...
N: And in our case, I do believe that our programs will stimulate the creative energies of ...
FP N: We are for programs, in addition, which will see that our medical care for the aged ...
K: Are we as strong as we should be?
FN K: I should make it clear that I do not think we’re doing enough ...
N: Why did Senator Kennedy take that position then? Why do I take it now?
BUSH (B) — GORE (G) DEBATES
TP B: It’s not what I think and its not my intentions and not my plan.
G: And FEMA has been a major flagship project of our reinventing government efforts. And I agree, it
works extremely well now.
FP B: First of all, most of this is at the state level.
G: And it focuses not only on increasing the supply, which I agree we have to do, but also on ...
FN B: My opponent thinks the government—the surplus is the government’s money. That’s not what I
think
G: I strongly support local control, so does Governor Bush.
OBAMA (O) — MCCAIN (M) DEBATES
TP M: But the point is—the point is, we have finally seen Republicans and Democrats sitting down and
negotiating together ...
O: And one of the things I think we have to do is make sure that college is affordable ...
FP O: ... but in the short term there’s an outlay and we may not see that money for a while.
O: We have to do that now, because it will actually make our businesses and our families better off.
FN O: So I think the lesson to be drawn is that we should never hesitate to use military force ... to keep the
American people safe.
O: But let’s get back to the core issue here.
PERSUASIVE ESSAYS (DEVELOPMENT SET, SPELLING ERRORS INCLUDED)
TP However, the argument lacks specificity and relies on too many questionable assumptions to make a
strong case for adopting an expensive and logistically complicated program.
I believe that both of these claims have been made in hase and other factors need to be considered.
FP Since they are all far from now, the prove is not strong enough to support the conclusion.
As we know that one mind can not think as the other does.
FN History has proven that ...
The given issue which states that in any field of inquiry ... is a controversional one.
</table>
<tableCaption confidence="0.93484225">
Table 2: Examples of CRF,,,,,+R performance. Underlining marks tokens predicted to be shell, and bold font indicates
shell according to human judgments (our judgments for the debate transcripts, and the annotator’s judgments for the
development set). Examples include true positives (TP), false positives (FP), and false negatives (FN). Note that some
FP and FN examples include partially accurate predictions.
</tableCaption>
<page confidence="0.996845">
27
</page>
<bodyText confidence="0.99997765625">
“therefore.” Discourse markers, however, are typ-
ically only single words or short phrases that ex-
press a limited number of relationships. On the other
hand, shell can capture longer sequences that ex-
press more complex relationships between the com-
ponents of an argumentative discourse (e.g., “But
let’s get back to the core issue here” signals that the
following point is more important than the previous
one).
There are also various other approaches to ana-
lyzing arguments. Notably, much recent theoreti-
cal research on argumentation has focused on ar-
gumentation schemes (Walton et al., 2008), which
are high-level strategies for constructing arguments
(e.g., argument from consequences). Recently, Feng
and Hirst (2011) developed automated methods for
classifying texts by argumentation scheme. In sim-
ilar work, Anand et al. (2011) use argumentation
schemes to identify tactics in blog posts (e.g., moral
appeal, social generalization, appeals to external au-
thorities etc.). Although shell language can certainly
be found in persuasive writing, it is used to orga-
nize the persuader’s tactics and claims rather than
to express them. For example, consider the follow-
ing sentence: “It must be the case that this diet
works since it was recommended by someone who
lost 20 pounds on it.” In shell detection, we focus
on the lexico-syntactic level, aiming to identify the
bold words as shell. In contrast, work on argumenta-
tion schemes focuses at a higher level of abstraction,
aiming to classify the sentence as an attempt to per-
suade by appealing to an external authority.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999977583333333">
In this paper, we described our approach to detect-
ing language used to explicitly structure an arguer’s
claims and pieces of evidence as well as explain
how they relate to an opponent’s argument. We im-
plemented a rule-based system, a supervised proba-
bilistic sequence model, and a principled hybrid ver-
sion of the two. We presented evaluations of these
systems using human-annotated essays, and we ob-
served that the hybrid sequence model system per-
formed the best. We also applied our system to po-
litical debates and found evidence of the potential to
generalize to new domains.
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999915">
We would like to thank the annotators for helping
us create the essay data sets. We would also like
to thank James Carlson, Paul Deane, Yoko Futagi,
Beata Beigman Klebanov, Melissa Lopez, and the
anonymous reviewers for their useful comments on
the paper and annotation scheme.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99985096875">
P. Anand, J. King, J. Boyd-Graber, E. Wagner, C. Martell,
D. Oard, and P. Resnik. 2011. Believe me–we can
do this! annotating persuasive acts in blog text. In
Proc. of AAAI Workshop on Computational Models of
Natural Argument.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proc. of the Second
SIGdial Workshop on Discourse and Dialogue.
B. Efron and R. Tibshirani. 1993. An Introduction to the
Bootstrap. Chapman and Hall/CRC.
V. W. Feng and G. Hirst. 2011. Classifying arguments
by scheme. In Proc. of ACL.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse. Com-
put. Linguist., 12(3):175–204.
B. Hutchinson. 2004. Acquiring the meaning of dis-
course markers. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ofICML.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3).
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
N. Okazaki. 2007. CRFsuite: a fast implementation of
conditional random fields (CRFs).
J. R. Searle. 1975. A classification of illocutionary acts.
Language in Society, 5(1).
D. Walton, C. Reed, and F. Macagno. 2008. Argumenta-
tion Schemes. Cambridge University Press.
</reference>
<page confidence="0.999071">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.192875">
<title confidence="0.998563">Identifying High-Level Organizational Elements in Argumentative Discourse</title>
<author confidence="0.982292">Nitin Madnani Michael Heilman Joel</author>
<degree confidence="0.5202585">Educational Testing Princeton, NJ, Martin Hunter College of</degree>
<author confidence="0.794871">New York</author>
<author confidence="0.794871">NY</author>
<email confidence="0.998273">martin.chodorow@hunter.cuny.edu</email>
<abstract confidence="0.997600555555555">Argumentative discourse contains not only language expressing claims and evidence, but also language used to organize these claims and pieces of evidence. Differentiating between the two may be useful for many applications, such as those that focus on the content (e.g., relation extraction) of arguments and those that focus on the structure of arguments (e.g., automated essay scoring). We propose an automated approach to detecting high-level organizational elements in argumentative discourse that combines a rule-based system and a probabilistic sequence model in a principled manner. We present quantitative results on a dataset of human-annotated persuasive essays, and qualitative analyses of performance on essays and on political debates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Anand</author>
<author>J King</author>
<author>J Boyd-Graber</author>
<author>E Wagner</author>
<author>C Martell</author>
<author>D Oard</author>
<author>P Resnik</author>
</authors>
<title>Believe me–we can do this! annotating persuasive acts in blog text.</title>
<date>2011</date>
<booktitle>In Proc. of AAAI Workshop on Computational Models of Natural Argument.</booktitle>
<contexts>
<context position="31481" citStr="Anand et al. (2011)" startWordPosition="5281" endWordPosition="5284">plex relationships between the components of an argumentative discourse (e.g., “But let’s get back to the core issue here” signals that the following point is more important than the previous one). There are also various other approaches to analyzing arguments. Notably, much recent theoretical research on argumentation has focused on argumentation schemes (Walton et al., 2008), which are high-level strategies for constructing arguments (e.g., argument from consequences). Recently, Feng and Hirst (2011) developed automated methods for classifying texts by argumentation scheme. In similar work, Anand et al. (2011) use argumentation schemes to identify tactics in blog posts (e.g., moral appeal, social generalization, appeals to external authorities etc.). Although shell language can certainly be found in persuasive writing, it is used to organize the persuader’s tactics and claims rather than to express them. For example, consider the following sentence: “It must be the case that this diet works since it was recommended by someone who lost 20 pounds on it.” In shell detection, we focus on the lexico-syntactic level, aiming to identify the bold words as shell. In contrast, work on argumentation schemes f</context>
</contexts>
<marker>Anand, King, Boyd-Graber, Wagner, Martell, Oard, Resnik, 2011</marker>
<rawString>P. Anand, J. King, J. Boyd-Graber, E. Wagner, C. Martell, D. Oard, and P. Resnik. 2011. Believe me–we can do this! annotating persuasive acts in blog text. In Proc. of AAAI Workshop on Computational Models of Natural Argument.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proc. of the Second SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="5132" citStr="Carlson et al., 2001" startWordPosition="808" endWordPosition="812">nent’s claims (e.g., “The argument states that”) • to evaluate an opponent’s claims (e.g., “It may seem reasonable at first glance, but actually, there are some logical mistakes in it”) • to present evidence and relate it to specific claims (e.g., “To illustrate my point, I will now give the example of”) There are many ways of analyzing discourse. The most relevant is perhaps rhetorical structure theory (RST) (Mann and Thompson, 1988). To our knowledge, the RST parser from Marcu (2000) is the only RST parser readily available for experimentation. The parser is trained to model the RST corpus (Carlson et al., 2001), which treats complete clauses (i.e., clauses with their obligatory complements) as the elementary units of analysis. Thus, the parser treats the first sentence in example 1 as a single unit and does not differentiate between the main and subordinate clauses. In contrast, our approach distinguishes the sequence “The argument states that ...” as shell (which is used here to restate the external claim). Furthermore, we identify the entire second sentence as shell (here, used to evaluate the external claim), whereas the RST parser splits the sentence into two clauses, “It may seem ...” and “but </context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>L. Carlson, D. Marcu, and M. E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proc. of the Second SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman and Hall/CRC.</title>
<date>1993</date>
<contexts>
<context position="20814" citStr="Efron and Tibshirani, 1993" startWordPosition="3464" endWordPosition="3467"> all systems, along with results comparing the second annotator’s labels (“Human”) to the gold standard labels from the first annotator. The same pattern emerged as on the development set, with CRFm+R performing the best. The F1 score of 0.607 for the CRFm+R system was relatively close to the F1 score of 0.736 for agreement between human annotators. To test whether CRFm+R’s relatively high performance was due to chance, we computed 99% confidence intervals for the differences in F1 score between CRFm+R and each of the other methods. We used the biascorrected and accelerated (BC,,,) Bootstrap (Efron and Tibshirani, 1993) with 10,000 rounds of resampling at the sentence level for each comparison. A difference is statistically reliable at the α level (i.e., p &lt; α) if the (1 − α)% confidence interval for the difference does not contain zero, which corresponds to the null hypothesis. Statistically reliable differences are indicated in Table 1. The only system that did not have a reliably lower F1 score than CRFm+R was CRFm−R, though due to the relatively small size of our test set, we do not take this as strong evidence against using the rule-based system features in the CRF. We note that while the CRFm+R system </context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V W Feng</author>
<author>G Hirst</author>
</authors>
<title>Classifying arguments by scheme.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="31369" citStr="Feng and Hirst (2011)" startWordPosition="5264" endWordPosition="5267">ess a limited number of relationships. On the other hand, shell can capture longer sequences that express more complex relationships between the components of an argumentative discourse (e.g., “But let’s get back to the core issue here” signals that the following point is more important than the previous one). There are also various other approaches to analyzing arguments. Notably, much recent theoretical research on argumentation has focused on argumentation schemes (Walton et al., 2008), which are high-level strategies for constructing arguments (e.g., argument from consequences). Recently, Feng and Hirst (2011) developed automated methods for classifying texts by argumentation scheme. In similar work, Anand et al. (2011) use argumentation schemes to identify tactics in blog posts (e.g., moral appeal, social generalization, appeals to external authorities etc.). Although shell language can certainly be found in persuasive writing, it is used to organize the persuader’s tactics and claims rather than to express them. For example, consider the following sentence: “It must be the case that this diet works since it was recommended by someone who lost 20 pounds on it.” In shell detection, we focus on the </context>
</contexts>
<marker>Feng, Hirst, 2011</marker>
<rawString>V. W. Feng and G. Hirst. 2011. Classifying arguments by scheme. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<date>1986</date>
<journal>Attention, Intentions, and the Structure of Discourse. Comput. Linguist.,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="26280" citStr="Grosz and Sidner (1986)" startWordPosition="4373" endWordPosition="4376">following example: “But let’s get back to the core issue here.” These results suggest that although there is potential for improvement in adapting to new domains, our approach to shell detection at least partially generalizes beyond our initial domain of persuasive essay writing. 6 Related Work There has been much previous work on analyzing discourse. In this section, we describe similarities and differences between that work and ours. Rhetorical structure theory (Mann and Thompson, 1988) is perhaps the most relevant area of work. See §1 for a discussion. In research on intentional structure, Grosz and Sidner (1986) propose that any discourse is composed of three interacting components: the linguistic structure defined by the actual utterances, the intentional structure defined by the purposes underlying the discourse, and an attentional structure defined by the discourse participants’ focus of attention. Detecting shell may also be seen as trying to identify explicit cues of intentional structure in a discourse. Additionally, the categorization of shell spans as to whether they evaluate the opponents claims, connect ones own claims, etc., may be seen as determining what Grosz and Sidener call “discourse</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, Intentions, and the Structure of Discourse. Comput. Linguist., 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hutchinson</author>
</authors>
<title>Acquiring the meaning of discourse markers.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="27381" citStr="Hutchinson, 2004" startWordPosition="4541" endWordPosition="4542">e opponents claims, connect ones own claims, etc., may be seen as determining what Grosz and Sidener call “discourse segment purposes” (i.e., the intentions underlying the segments containing the shell spans). We can also view shell detection as the task of identifying phrases that indicate certain types of speech acts (Searle, 1975). In particular, we aim to identify markers of assertive speech acts, which declare that the speaker believes a certain proposition, and expressive speech acts, which express attitudes toward propositions. Shell also overlaps with the concept of discourse markers (Hutchinson, 2004), such as “however” or 26 LINCOLN (L) — DOUGLAS (D) DEBATES TP L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is actually crazy. L: The first thing I see fit to notice is the fact that ... FP D: He became noted as the author of the scheme to ... D: ... such amendments were to be made to it as would render it useless and inefficient ... FN D: I wish to impress it upon you, that every man who voted for those resolutions ... L: That statement he makes, too, in the teeth of the knowledge that I had made the stipulation to come down here ... KENN</context>
</contexts>
<marker>Hutchinson, 2004</marker>
<rawString>B. Hutchinson. 2004. Acquiring the meaning of discourse markers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ofICML.</booktitle>
<contexts>
<context position="9112" citStr="Lafferty et al., 2001" startWordPosition="1456" endWordPosition="1459">tion. From this sample, we computed lists of n-grams (n = 1, 2, ... , 9) that occurred more than once in essays from at least half of the 57 distinct essay prompts. We then wrote rules to recognize the shell language present in the n-gram lists. Additional rules were added to cover instances of shell that we observed in the annotated essays but that were not frequent enough to appear in the n-gram analysis. We use “Rules” to refer to this method. 2.2 Supervised Sequence Model The next approach we describe is a supervised, probabilistic sequence model based on conditional random fields (CRFs) (Lafferty et al., 2001), using a small number of general features based on lexical frequencies. We assume access to a labeled dataset of N examples (w, y) indexed by i, containing sequences of words w(z) and sequences of labels y(z), with individual words and labels indexed by j (§3 describes our development and testing sets). y(z) is a sequence of binary values, indicating whether each word w(z) � in the sequence is shell (y(z) � = 1) or not 7 ((z) = 0). Following Lafferty et al. (2001), we find y a parameter vector θ that maximizes the following log-likelihood objective function: The normalization constant Zz is a</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3).</tech>
<contexts>
<context position="4949" citStr="Mann and Thompson, 1988" startWordPosition="776" endWordPosition="779">l framework for an argument. It may be used by the writer or the speaker in the following ways: • to declare one’s own claims (e.g., “There is the possibility that”) • to restate an opponent’s claims (e.g., “The argument states that”) • to evaluate an opponent’s claims (e.g., “It may seem reasonable at first glance, but actually, there are some logical mistakes in it”) • to present evidence and relate it to specific claims (e.g., “To illustrate my point, I will now give the example of”) There are many ways of analyzing discourse. The most relevant is perhaps rhetorical structure theory (RST) (Mann and Thompson, 1988). To our knowledge, the RST parser from Marcu (2000) is the only RST parser readily available for experimentation. The parser is trained to model the RST corpus (Carlson et al., 2001), which treats complete clauses (i.e., clauses with their obligatory complements) as the elementary units of analysis. Thus, the parser treats the first sentence in example 1 as a single unit and does not differentiate between the main and subordinate clauses. In contrast, our approach distinguishes the sequence “The argument states that ...” as shell (which is used here to restate the external claim). Furthermore</context>
<context position="26150" citStr="Mann and Thompson, 1988" startWordPosition="4350" endWordPosition="4354">ularly in short sentences (e.g., “Are we as strong as we should be?”). It also missed shell, partially or entirely, such as in the following example: “But let’s get back to the core issue here.” These results suggest that although there is potential for improvement in adapting to new domains, our approach to shell detection at least partially generalizes beyond our initial domain of persuasive essay writing. 6 Related Work There has been much previous work on analyzing discourse. In this section, we describe similarities and differences between that work and ours. Rhetorical structure theory (Mann and Thompson, 1988) is perhaps the most relevant area of work. See §1 for a discussion. In research on intentional structure, Grosz and Sidner (1986) propose that any discourse is composed of three interacting components: the linguistic structure defined by the actual utterances, the intentional structure defined by the purposes underlying the discourse, and an attentional structure defined by the discourse participants’ focus of attention. Detecting shell may also be seen as trying to identify explicit cues of intentional structure in a discourse. Additionally, the categorization of shell spans as to whether th</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. C. Mann and S. A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5001" citStr="Marcu (2000)" startWordPosition="788" endWordPosition="789"> speaker in the following ways: • to declare one’s own claims (e.g., “There is the possibility that”) • to restate an opponent’s claims (e.g., “The argument states that”) • to evaluate an opponent’s claims (e.g., “It may seem reasonable at first glance, but actually, there are some logical mistakes in it”) • to present evidence and relate it to specific claims (e.g., “To illustrate my point, I will now give the example of”) There are many ways of analyzing discourse. The most relevant is perhaps rhetorical structure theory (RST) (Mann and Thompson, 1988). To our knowledge, the RST parser from Marcu (2000) is the only RST parser readily available for experimentation. The parser is trained to model the RST corpus (Carlson et al., 2001), which treats complete clauses (i.e., clauses with their obligatory complements) as the elementary units of analysis. Thus, the parser treats the first sentence in example 1 as a single unit and does not differentiate between the main and subordinate clauses. In contrast, our approach distinguishes the sequence “The argument states that ...” as shell (which is used here to restate the external claim). Furthermore, we identify the entire second sentence as shell (h</context>
<context position="7250" citStr="Marcu (2000)" startWordPosition="1140" endWordPosition="1141">equence model (§2.2), and a simple lexical baseline (§2.3). 2.1 Rule-based system We begin by describing a knowledge-based approach to detecting organizational elements in argumentative discourse. This approach uses a set of 25 hand-written regular expression patterns.3 In order to develop these patterns, we created a sample of 170 annotated essays across 57 distinct prompts.4 The essays were written by test-takers of a standardized test for graduate admissions. This sample of essays was similar in nature to but did not overlap with those discussed in other sections 2We used the RST parser of Marcu (2000) to analyze the original essay from which the example was adapted. 3We use the PyParsing toolkit to parse sentences with the grammar for the rule system. 4Prompts are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue. 21 MODAL —* do |don’t |can |cannot |will |would |... ADVERB —* strongly |totally |fundamentally |vehemently |.. . AGREEVERB —* disagree |agree |concur |... AUTHORNOUN —* writer |author |speaker |... SHELL —* I [MODAL] [ADVERB] AGREEVERB with the AUTHORNOUN Figure 1: An e</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of conditional random fields (CRFs).</title>
<date>2007</date>
<contexts>
<context position="10798" citStr="Okazaki, 2007" startWordPosition="1759" endWordPosition="1760">cy of w(z) � in a set of 100,000 essays (see below). • Eight binary features for whether the above frequencies meet or exceed the following thresholds: 10{−6,−5,−4,−3}. • The proportion of prompts for which w(z) � appeared in at least one essay about that prompt in the set of 100,000. • Three binary features for whether the above proportion of prompts meets or exceeds the following thresholds: 10.25, 0.50, 0.75}. • A binary feature with value 1 if w(z) � consists only of letters a-z, and 0 otherwise. This feature distinguishes punctuation and numbers from other tokens. 5We used CRFsuite 0.12 (Okazaki, 2007) to implement the CRF model. 22 • A binary feature with value 1 if the rule-based system predicts that w(i) j is shell, and 0 otherwise. • A binary feature with value 1 if the rule-based system predicts thatw(i) j�1 is shell, and 0 otherwise. • Two binary features for whether or not the current token was the first or last in the sentence, respectively. • Four binary features for the possible transitions between previous and current labels (y(i) j and y(i) j�1, respectively). To define the features related to essay prompts and lexical frequencies in essays, we created a set of 100,000 essays fr</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>N. Okazaki. 2007. CRFsuite: a fast implementation of conditional random fields (CRFs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>A classification of illocutionary acts.</title>
<date>1975</date>
<journal>Language in Society,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="27099" citStr="Searle, 1975" startWordPosition="4500" endWordPosition="4501">, and an attentional structure defined by the discourse participants’ focus of attention. Detecting shell may also be seen as trying to identify explicit cues of intentional structure in a discourse. Additionally, the categorization of shell spans as to whether they evaluate the opponents claims, connect ones own claims, etc., may be seen as determining what Grosz and Sidener call “discourse segment purposes” (i.e., the intentions underlying the segments containing the shell spans). We can also view shell detection as the task of identifying phrases that indicate certain types of speech acts (Searle, 1975). In particular, we aim to identify markers of assertive speech acts, which declare that the speaker believes a certain proposition, and expressive speech acts, which express attitudes toward propositions. Shell also overlaps with the concept of discourse markers (Hutchinson, 2004), such as “however” or 26 LINCOLN (L) — DOUGLAS (D) DEBATES TP L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is actually crazy. L: The first thing I see fit to notice is the fact that ... FP D: He became noted as the author of the scheme to ... D: ... such amendme</context>
</contexts>
<marker>Searle, 1975</marker>
<rawString>J. R. Searle. 1975. A classification of illocutionary acts. Language in Society, 5(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Walton</author>
<author>C Reed</author>
<author>F Macagno</author>
</authors>
<title>Argumentation Schemes.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="31241" citStr="Walton et al., 2008" startWordPosition="5248" endWordPosition="5251">ly accurate predictions. 27 “therefore.” Discourse markers, however, are typically only single words or short phrases that express a limited number of relationships. On the other hand, shell can capture longer sequences that express more complex relationships between the components of an argumentative discourse (e.g., “But let’s get back to the core issue here” signals that the following point is more important than the previous one). There are also various other approaches to analyzing arguments. Notably, much recent theoretical research on argumentation has focused on argumentation schemes (Walton et al., 2008), which are high-level strategies for constructing arguments (e.g., argument from consequences). Recently, Feng and Hirst (2011) developed automated methods for classifying texts by argumentation scheme. In similar work, Anand et al. (2011) use argumentation schemes to identify tactics in blog posts (e.g., moral appeal, social generalization, appeals to external authorities etc.). Although shell language can certainly be found in persuasive writing, it is used to organize the persuader’s tactics and claims rather than to express them. For example, consider the following sentence: “It must be t</context>
</contexts>
<marker>Walton, Reed, Macagno, 2008</marker>
<rawString>D. Walton, C. Reed, and F. Macagno. 2008. Argumentation Schemes. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>