<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.945885">
Weighted Alignment Matrices for Statistical Machine Translation
</title>
<author confidence="0.973306">
Yang Liu, Tian Xia , Xinyan Xiao and Qun Liu
</author>
<affiliation confidence="0.957691">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.865587">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.998896">
{yliu,xiatian,xiaoxinyan,liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.994798" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923705882353">
Current statistical machine translation sys-
tems usually extract rules from bilingual
corpora annotated with 1-best alignments.
They are prone to learn noisy rules due
to alignment mistakes. We propose a new
structure called weighted alignment matrix
to encode all possible alignments for a par-
allel text compactly. The key idea is to as-
sign a probability to each word pair to in-
dicate how well they are aligned. We de-
sign new algorithms for extracting phrase
pairs from weighted alignment matrices
and estimating their probabilities. Our ex-
periments on multiple language pairs show
that using weighted matrices achieves con-
sistent improvements over using n-best
lists in significant less extraction time.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942377049181">
Statistical machine translation (SMT) relies heav-
ily on annotated bilingual corpora. Word align-
ment, which indicates the correspondence be-
tween the words in a parallel text, is one of the
most important annotations in SMT. Word-aligned
corpora have been found to be an excellent source
for translation-related knowledge, not only for
phrase-based models (Och and Ney, 2004; Koehn
et al., 2003), but also for syntax-based models
(e.g., (Chiang, 2007; Galley et al., 2006; Shen
et al., 2008; Liu et al., 2006)). Och and Ney
(2003) indicate that the quality of machine transla-
tion output depends directly on the quality of ini-
tial word alignment.
Modern alignment methods can be divided into
two major categories: generative methods and dis-
criminative methods. Generative methods (Brown
et al., 1993; Vogel and Ney, 1996) treat word
alignment as a hidden process and maximize the
likelihood of bilingual training corpus using the
expectation maximization (EM) algorithm. In
contrast, discriminative methods (e.g., (Moore et
al., 2006; Taskar et al., 2005; Liu et al., 2005;
Blunsom and Cohn, 2006)) have the freedom to
define arbitrary feature functions that describe var-
ious characteristics of an alignment. They usu-
ally optimize feature weights on manually-aligned
data. While discriminative methods show supe-
rior alignment accuracy in benchmarks, genera-
tive methods are still widely used to produce word
alignments for large sentence-aligned corpora.
However, neither generative nor discriminative
alignment methods are reliable enough to yield
high quality alignments for SMT, especially for
distantly-related language pairs such as Chinese-
English and Arabic-English. The F-measures for
Chinese-English and Arabic-English are usually
around 80% (Liu et al., 2005) and 70% (Fraser
and Marcu, 2007), respectively. As most current
SMT systems only use 1-best alignments for ex-
tracting rules, alignment errors might impair trans-
lation quality.
Recently, several studies have shown that offer-
ing more alternatives of annotations to SMT sys-
tems will result in significant improvements, such
as replacing 1-best trees with packed forests (Mi
et al., 2008) and replacing 1-best word segmenta-
tions with word lattices (Dyer et al., 2008). Sim-
ilarly, Venugopal et al. (2008) use n-best align-
ments instead of 1-best alignments for translation
rule extraction. While they achieve significant im-
provements on the IWSLT data, extracting rules
from n-best alignments might be computationally
expensive.
In this paper, we propose a new structure named
weighted alignment matrix to represent the align-
ment distribution for a sentence pair compactly. In
a weighted matrix, each element that corresponds
to a word pair is assigned a probability to measure
the confidence of aligning the two words. There-
fore, a weighted matrix is capable of using a lin-
</bodyText>
<page confidence="0.964283">
1017
</page>
<note confidence="0.783094">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1017–1026,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
economy
’s
China
of
development
the
</note>
<figureCaption confidence="0.9869655">
Figure 1: An example of word alignment between
a pair of Chinese and English sentences.
</figureCaption>
<bodyText confidence="0.998273222222222">
ear space to encode the probabilities of exponen-
tially many alignments. We develop a new algo-
rithm for extracting phrase pairs from weighted
matrices and show how to estimate their relative
frequencies and lexical weights. Experimental re-
sults show that using weighted matrices achieves
consistent improvements in translation quality and
significant reduction in extraction time over using
n-best lists.
</bodyText>
<sectionHeader confidence="0.987531" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99885875">
Figure 1 shows an example of word alignment be-
tween a pair of Chinese and English sentences.
The Chinese and English words are listed horizon-
tally and vertically, respectively. The dark points
indicate the correspondence between the words in
two languages. For example, the first Chinese
word “zhongguo” is aligned to the fourth English
word “China”.
Formally, given a source sentence f = fJ1 =
f1, ... , fj, ... , fJ and a target sentence e = eI1 =
e1, ... , ei, ... , eI, we define a link l = (j, i) to
exist if fj and ei are translation (or part of trans-
lation) of one another. Then, an alignment a is a
subset of the Cartesian product of word positions:
a ⊆ {(j,i) : j = 1,... , J;i = 1,... ,I} (1)
Usually, SMT systems only use the 1-best align-
ments for extracting translation rules. For exam-
ple, given a source phrase f˜ and a target phrase
˜e, the phrase pair ( ˜f, ˜e) is said to be consistent
(Och and Ney, 2004) with the alignment if and
only if: (1) there must be at least one word in-
side one phrase aligned to a word inside the other
phrase and (2) no words inside one phrase can be
aligned to a word outside the other phrase.
After all phrase pairs are extracted from the
training corpus, their translation probabilities can
be estimated as relative frequencies (Och and Ney,
2004):
</bodyText>
<equation confidence="0.9592485">
f, ˜e)
E˜e′ count(
</equation>
<bodyText confidence="0.999155666666667">
where count(˜f, ˜e) indicates how often the phrase
pair (˜f, ˜e) occurs in the training corpus.
Besides relative frequencies, lexical weights
(Koehn et al., 2003) are widely used to estimate
how well the words in f˜ translate the words in
˜e. To do this, one needs first to estimate a lexi-
cal translation probability distribution w(e|f) by
relative frequency from the same word alignments
in the training corpus:
</bodyText>
<equation confidence="0.996958">
count(f,e)
w(e|f) = (3)
Ee′ count(f, e′)
</equation>
<bodyText confidence="0.998919833333333">
Note that a special source NULL token is added
to each source sentence and aligned to each un-
aligned target word.
As the alignment a˜ between a phrase pair ( f, ˜e)
is retained during extraction, the lexical weight
can be calculated as
</bodyText>
<equation confidence="0.992359666666667">
1
{j|(j,i) �w(ei|fj) (4)
Ea}I
</equation>
<bodyText confidence="0.999454">
If there are multiple alignments a˜ for a phrase
pair (˜f, ˜e), Koehn et al. (2003) choose the one
with the highest lexical weight:
</bodyText>
<equation confidence="0.9979695">
{ }
pw(˜e |˜f, ˜a) (5)
</equation>
<bodyText confidence="0.999958333333333">
Simple and effective, relative frequencies and
lexical weights have become the standard features
in modern discriminative SMT systems.
</bodyText>
<sectionHeader confidence="0.99326" genericHeader="method">
3 Weighted Alignment Matrix
</sectionHeader>
<bodyText confidence="0.999959777777778">
We believe that offering more candidate align-
ments to extracting translation rules might help
improve translation quality. Instead of using n-
best lists (Venugopal et al., 2008), we propose a
new structure called weighted alignment matrix.
We use an example to illustrate our idea. Fig-
ure 2(a) and Figure 2(b) show two alignments of
a Chinese-English sentence pair. We observe that
some links (e.g., (1,4) corresponding to the word
</bodyText>
<equation confidence="0.981184357142857">
zhongguo
de
jingji
fazhan
φ(˜e|
count(
f) =
(2)
˜f, ˜e′)
pw(˜e |˜f, ˜a) = � |˜e|
i=1
˜f) = max
a˜
pw(˜e|
</equation>
<page confidence="0.879913">
1018
</page>
<bodyText confidence="0.858507833333333">
economy 0 0 1.0 0
’s 0 0.4 0.4 0
China 1.0 0 0 0
of 0 0.6 0 0.4
development 0 0 0 1.0
the 0 0 0 0
</bodyText>
<figure confidence="0.98998016">
economy
’s
China
of
development
the
economy
’s
China
of
development
the
zhongguo
de
jingji
fazhan
zhongguo
de
jingji
fazhan
zhongguo
de
jingji
fazhan
(a) (b) (c)
</figure>
<figureCaption confidence="0.990820666666667">
Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c)
the resulting weighted alignment matrix that takes the two alignments as samples, of which the initial
probabilities are 0.6 and 0.4, respectively.
</figureCaption>
<bodyText confidence="0.992869">
where
</bodyText>
<equation confidence="0.98773025">
�
1 (j, i) E a
δ(a, j, i) = (8)
0 otherwise
</equation>
<bodyText confidence="0.996727777777778">
Note that N is an n-best list, p(a) is the probabil-
ity of an alignment a in the n-best list, δ(a, j, i)
indicates whether a link (j, i) occurs in the align-
ment a or not. We assign 0 to any unseen
alignment. As p(a) is usually normalized (i.e.,
EaENp(a) - 1), we remove the denominator in
Eq. (6).
Accordingly, the probability that the two words
fj and ei are not aligned is
</bodyText>
<equation confidence="0.891961">
�pm(j, i) = 1.0 − pm(j, i) (9)
</equation>
<bodyText confidence="0.9996786">
For example, as shown in Figure 2(c), the prob-
ability for the two words “de” and “of” being
aligned is 0.6 and the probability that they are not
aligned is 0.4.
Intuitively, the probability of an alignment a is
the product of link probabilities. If a link (j, i)
occurs in a, we use pm(j, i); otherwise we use
pm(j, i). Formally, given a weighted alignment
matrix m, the probability of an alignment a can
be calculated as
</bodyText>
<equation confidence="0.9988105">
(pm(j, i) x δ(a, j, i) +
Pm(j, i) x (1 − δ(a, j, i))) (10)
</equation>
<bodyText confidence="0.997642310344828">
It proves that the sum of all alignment proba-
bilities is always 1: EaEA pm(a) - 1, where A
pair (“zhongguo”, “China”)) occur in both align-
ments, some links (e.g., (2,3) corresponding to the
word pair (“de”,“of”)) occur only in one align-
ment, and some links (e.g., (1,1) corresponding
to the word pair (“zhongguo”, “the”)) do not oc-
cur. Intuitively, we can estimate how well two
words are aligned by calculating its relative fre-
quency, which is the probability sum of align-
ments in which the link occurs divided by the
probability sum of all possible alignments. Sup-
pose that the probabilities of the two alignments in
Figures 2(a) and 2(b) are 0.6 and 0.4, respectively.
We can estimate the relative frequencies for every
word pair and obtain a weighted matrix shown in
Figure 2(c). Therefore, each word pair is associ-
ated with a probability to indicate how well they
are aligned. For example, in Figure 2(c), we say
that the word pair (“zhongguo”, “China”) is def-
initely aligned, (“zhongguo”, “the”) is definitely
unaligned, and (“de”, “of”) has a 60% chance to
get aligned.
Formally, a weighted alignment matrix m is a
J x I matrix, in which each element stores a link
probability pm(j, i) to indicate how well fj and
ei are aligned. Currently, we estimate link proba-
bilities from an n-best list by calculating relative
frequencies:
</bodyText>
<equation confidence="0.9956639">
EaEN p(a) x δ(a, j, i)
pm(j, i) = (6)
EaEN p(a)
11 = p(a) x δ(a,j,i) (7)
aEN
I
i=1
pm(a) =
J
j=1
</equation>
<page confidence="0.95898">
1019
</page>
<listItem confidence="0.948856">
1: procedure PHRASEEXTRACT(fi , el, m, l)
2: R ← ∅
3: for j1 ← 1... J do
4: j2 ← j1
5: while j2 &lt; J ∧ j2 − j1 &lt; l do
6: T ← {i|∃j : j1 ≤ j ≤ j2 ∧ pm(j, i) &gt; 0}
7: il ← MIN(T)
8: iu ← MAX(T)
9: for n ← 1 ... l do
10: for i1 ← il − n + 1... iu do
11: i2 ← i1 + n − 1
</listItem>
<figureCaption confidence="0.776971">
Figure 3: Algorithm for extracting phrase pairs
from a sentence pair (fJ1 , eI1) annotated with a
weighted alignment matrix m.
</figureCaption>
<bodyText confidence="0.984625875">
is the set of all possible alignments. Therefore, a
weighted alignment matrix is capable of encoding
the probabilities of 2J×I alignments using only a
J x I space.
Note that pm(a) is not necessarily equal to p(a)
because the encoding of a weighted alignment ma-
trix changes the alignment probability distribu-
tion. For example, while the initial probability of
the alignment in Figure 2(a) (i.e., p(a)) is 0.6, the
probability of the same alignment encoded in the
matrix shown in Figure 2(c) (i.e., pm(a)) becomes
0.1296 according to Eq. (10). It should be em-
phasized that a weighted matrix encodes all pos-
sible alignments rather than the input n-best list,
although the link probabilities are estimated from
the n-best list.
</bodyText>
<sectionHeader confidence="0.981591" genericHeader="method">
4 Phrase Pair Extraction
</sectionHeader>
<bodyText confidence="0.999349">
In this section, we describe how to extract phrase
pairs from the training corpus annotated with
weighted alignment matrices (Section 4.1) and
how to estimate their relative frequencies (Section
4.2) and lexical weights (Section 4.3).
</bodyText>
<subsectionHeader confidence="0.995291">
4.1 Extraction Algorithm
</subsectionHeader>
<bodyText confidence="0.99970552173913">
Och and Ney (2004) describe a “phrase-extract”
algorithm for extracting phrase pairs from a sen-
tence pair annotated with a 1-best alignment.
Given a source phrase, they first identify the target
phrase that is consistent with the alignment. Then,
they expand the boundaries of the target phrase if
the boundary words are unaligned.
Unfortunately, this algorithm cannot be directly
used to manipulate a weighted alignment matrix,
which is a compact representation of all pos-
sible alignments. The major difference is that
the “tight” phrase that has both boundary words
aligned is not necessarily the smallest candidate
in a weighted matrix. For example, in Figure
2(a), the “tight” target phrase corresponding to
the source phrase “zhongguo de” is “of China”.
According to Och’s algorithm, the target phrase
“China” breaks the alignment consistency and
therefore is not valid candidate. However, this is
not true for using the weighted matrix shown in
Figure 2(c). The target phrase “China” is treated
as a “potential” candidate 1, although it might be
assigned only a small fractional count (see Table
1).
Therefore, we enumerate all potential phrase
pairs and calculate their fractional counts for
eliminating less promising candidates. Figure 3
shows the algorithm for extracting phrases from
a weighted matrix. The input of the algorithm
is a source sentence fJ1 , a target sentence eI1, a
weighted alignment matrix m, and a phrase length
limit l (line 1). After initializing R that stores col-
lected phrase pairs (line 2), we identify the cor-
responding target phrases for all possible source
phrases (lines 3-5). Given a source phrase fj2
j1 , we
find the lower and upper bounds of target positions
(i.e., il and iu) that have positive link probabili-
ties (lines 6-8). For example, the lower bound is
3 and the upper bound is 5 for the source phrase
“zhongguo de” in Figure 2(c). Finally, we enu-
merate all target phrases that allow for unaligned
boundary words with varying phrase lengths (lines
9-14). Note that we need to ensure that 1 &lt; i1 &lt; I
and 1 &lt; i2 &lt; I in lines 10-11, which are omitted
for simplicity.
</bodyText>
<subsectionHeader confidence="0.9996">
4.2 Calculating Relative Frequencies
</subsectionHeader>
<bodyText confidence="0.999995222222222">
To estimate the relative frequency of a phrase pair,
we need to estimate how often it occurs in the
training corpus. Given an n-best list, the fractional
count of a phrase pair is the probability sum of
the alignments with which the phrase pair is con-
sistent. Obviously, it is unrealistic for a weighted
alignment matrix to enumerate all possible align-
ments explicitly to calculate fractional counts. In-
stead, we resort to link probabilities to calculate
</bodyText>
<footnote confidence="0.979921666666667">
1By potential, we mean that the fractional count of a
phrase pair is positive. Section 4.2 describes how to calcu-
late fractional counts.
</footnote>
<table confidence="0.810718304347826">
R ← R ∪ {(f�2
�1 , e�2
�1)}
13: end for
14: end for
15: j2 ← j2 + 1
16: end while
17: end for
18: return R
19: end procedure
1020
target phrase α β count
of China 1.0 0.36 0.36
of China ’s 1.0 0.36 0.36
China ’s 1.0 0.24 0.24
China 1.0 0.24 0.24
’s economy 0.4 0 0
economy 0 0 1.0 0
’s 0 0.4 0.4 0
China 1.0 0 0 0
of 0 0.6 0 0.4
development 0 0 0 1.0
the 0 0 0 0
</table>
<tableCaption confidence="0.893427">
Table 1: Some candidate target phrases of the
</tableCaption>
<figure confidence="0.734820285714286">
source phrase “zhongguo de” in Figure 4, where α
is inside probability, β is outside probability, and
count is fractional count.
zhongguo
de
jingji
fazhan
</figure>
<figureCaption confidence="0.998406">
Figure 4: An example of calculating fractional
</figureCaption>
<bodyText confidence="0.8786679">
count. Given the phrase pair (“zhongguo de”, “of
China”), we divide the matrix into three areas: in-
side (heavy shading), outside (light shading), and
irrelevant (no shading).
counts efficiently. Equivalent to explicit enumera-
tion, we interpret the fractional count of a phrase
pair as the probability that it satisfies the two align-
ment consistency conditions (see Section 2).
Given a phrase pair, we divide the elements of
a weighted alignment matrix into three categories:
</bodyText>
<listItem confidence="0.855915">
(1) inside elements that fall inside the phrase pair,
</listItem>
<bodyText confidence="0.94325216">
(2) outside elements that fall outside the phrase
pair while fall in the same row or the same col-
umn, and (3) irrelevant elements that fall outside
the phrase pair while fall in neither the same row
nor the same column. Figure 4 shows an exam-
ple. Given the phrase pair (“zhongguo de”, “of
China”), we divide the matrix into three areas: in-
side (heavy shading), outside (light shading), and
irrelevant (no shading).
To what extent a phrase pair satisfies the align-
ment consistency is measured by calculating in-
side and outside probabilities. Although there are
the same terms in the parsing literature, they have
different meanings here. The inside probability in-
dicates the chance that there is at least one word
inside one phrase aligned to a word inside the
other phrase. The outside probability indicates the
chance that no words inside one phrase are aligned
to a word outside the other phrase.
Given a phrase pair (fj2
j1 , ei2 ), we denote the in-
i1
side area as in(j1, j2, i1, i2) and the outside area
as out(j1, j2, i1, i2). Therefore, the inside proba-
bility of a phrase pair is calculated as
</bodyText>
<equation confidence="0.9885685">
α(j1, j2, i1, i2) = 1 − 11 Pm(j, i) (11)
(j,i)∈in(j1,j2,i1,i2)
</equation>
<bodyText confidence="0.999451">
For example, the inside probability for (“zhong-
guo de”, “of China”) in Figure 4 is 1.0, which
means that there always exists at least one aligned
word pair inside.
Accordingly, the outside probability of a phrase
pair is calculated as
</bodyText>
<equation confidence="0.99743">
β(j1,j2, i1, i2) = 11 pm(j, i) (12)
(j,i)∈out(j1,j2,i1,i2)
</equation>
<bodyText confidence="0.999487571428571">
For example, the outside probability for
(“zhongguo de”, “of China”) in Figure 4 is 0.36,
which means the probability that there are no
aligned word pairs outside is 0.36.
Finally, we use the product of inside and outside
probabilities as the fractional count of a phrase
pair:
</bodyText>
<equation confidence="0.99589375">
count(fj2
j1 ,ei2
i1) = α(j1,j2,i1,i2) ×
β(j1,j2,i1,i2) (13)
</equation>
<bodyText confidence="0.99751755">
Table 1 lists some candidate target phrases of
the source phrase “zhongguo de” in Figure 4. We
also give their inside probabilities, outside proba-
bilities, and fractional counts.
After collecting the fractional counts from the
training corpus, we then use Eq. (2) to calculate
relative frequencies in two translation directions.
Often, our approach extracts a large amount of
phrase pairs from training corpus as we soften
the alignment consistency constraint. To main-
tain a reasonable phrase table size, we discard any
phrase pair that has a fractional count lower than
a threshold t. During extraction, we first obtain
a list of candidate target phrases for each source
phrase, as shown in Table 1. Then, we prune the
list according to the threshold t. For example, we
only retain the top two candidates in Table 1 if
t = 0.3. Note that we perform the pruning locally.
Although it is more reasonable to prune a phrase
table after accumulating all fractional counts from
</bodyText>
<page confidence="0.67224">
1021
</page>
<equation confidence="0.960435">
pw(˜e|˜f,m) = H |˜e |((1
i=1 {j|pmu, i) &gt; 01 x
� �p(ei|fj) x pm(j, i) +
dj:pm(j,i)&gt;0
p(ei|f0) x  |˜f |�¯pm(j, i) (16)
H
j=1
</equation>
<bodyText confidence="0.997891">
training corpus, such global pruning strategy usu-
ally leads to very large disk and memory require-
ments.
</bodyText>
<subsectionHeader confidence="0.999603">
4.3 Calculating Lexical Weights
</subsectionHeader>
<bodyText confidence="0.997434833333333">
Recall that we need to obtain two translation prob-
ability tables w(e|f) and w(f|e) before calculat-
ing lexical weights (see Section 2). Following
Koehn et al. (2003), we estimate the two distribu-
tions by relative frequencies from the training cor-
pus annotated with weighted alignment matrices.
In other words, we still use Eq. (3) but the way of
calculating fractional counts is different now.
Given a source word fj, a target word ei, and
a weighted alignment matrix, the fractional count
count(fj, ei) is pm(j, i). For NULL words, the
fractional counts can be calculated as
</bodyText>
<equation confidence="0.9987046">
count(fj, e0) = HI ¯pm(j, i) (14)
i=1
J
count(f0, ei) = H ¯pm(j, i) (15)
j=1
</equation>
<bodyText confidence="0.99647175">
For example, in Figure 4, count(de, of) is 0.6,
count(de,NULL) is 0.24, and count(NULL,of) is
0.24.
Then, we adapt Eq. (4) to calculate lexical
weight:
For example, for the target word “of” in Figure
4, the sum of aligned and unaligned probabilities
is
</bodyText>
<equation confidence="0.993653">
1
2
p(of|NULL) x 0.24
</equation>
<bodyText confidence="0.999686">
Note that we take link probabilities into account
and calculate the probability that a target word
translates a source NULL token explicitly.
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.992834">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999975825">
We evaluated our approach on Chinese-to-English
translation. We used the FBIS corpus (6.9M
+ 8.9M words) as the training data. For lan-
guage model, we used the SRI Language Mod-
eling Toolkit (Stolcke, 2002) to train a 4-gram
model on the Xinhua portion of GIGAWORD cor-
pus. We used the NIST 2002 MT evaluation test
set as our development set, and used the NIST
2005 test set as our test set. We evaluated the trans-
lation quality using case-insensitive BLEU metric
(Papineni et al., 2002).
To obtain weighted alignment matrices, we fol-
lowed Venugopal et al. (2008) to produce n-
best lists via GIZA++. We first ran GIZA++
to produce 50-best lists in two translation direc-
tions. Then, we used the refinement technique
“grow-diag-final-and” (Koehn et al., 2003) to all
50 x 50 bidirectional alignment pairs. Suppose
that ps2t and pt2s are the probabilities of an align-
ment pair assigned by GIZA++, respectively. We
used ps2t x pt2s as the probability of the result-
ing symmetric alignment. As different alignment
pairs might produce the same symmetric align-
ments, we followed Venugopal et al. (2008) to
remove duplicate alignments and retain only the
alignment with the highest probability. Therefore,
there were 550 candidate alignments on average
for each sentence pair in the training data. We
obtained n-best lists by selecting the top n align-
ments from the 550-best lists. The probability of
each alignment in the n-best list was re-estimated
by re-normalization (Venugopal et al., 2008). Fi-
nally, these n-best alignments served as samples
for constructing weighted alignment matrices.
After extracting phrase pairs from n-best lists
and weighted alignment matrices, we ran Moses
(Koehn et al., 2007) to translate the development
and test sets. We used the simple distance-based
reordering model to remove the dependency of
lexicalization on word alignments for Moses.
</bodyText>
<subsectionHeader confidence="0.999868">
5.2 Effect of Pruning Threshold
</subsectionHeader>
<bodyText confidence="0.999991181818182">
Our first experiment investigated the effect of
pruning threshold on translation quality (BLEU
scores on the test set) and the phrase table size (fil-
tered for the test set), as shown in Figure 5. To
save time, we extracted phrase pairs just from the
first 10K sentence pairs of the FBIS corpus. We
used 12 different thresholds: 0.0001, 0.001, 0.01,
0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9. Obvi-
ously, the lower the threshold is, the more phrase
pairs are extracted. When t = 0.0001, the number
of phrase pairs used on the test set was 460,284
</bodyText>
<equation confidence="0.881442">
x (p(of|de) x 0.6 + p(of|fazhan) x 0.4) +
</equation>
<page confidence="0.962871">
1022
</page>
<figure confidence="0.999650617021277">
BLEU score
BLEU score
0.293
0.292
0.291
0.290
0.289
0.288
0.287
0.286
0.285
0.284
0.283
0.282
0.281
0.280
n=1
n=10
n=50
n=5
n=5
n=10
n-best
m(n)
n=50
150 200 250 300 350 400 450 500
phrase table size (103)
0 10 20 30 40 50 60 70 80 90
average extracting time (milliseconds/sentence pair)
0.208
0.207
0.206
0.205
0.204
0.203
0.202
0.201
0.200
0.199
0.198
0.197
0.196
0.195
t=0.9...0.1
t=10-2
t=10-3
t=10-4
</figure>
<figureCaption confidence="0.915133">
Figure 5: Effect of pruning threshold on transla-
tion quality and phrase table size.
</figureCaption>
<bodyText confidence="0.998507">
and the BLEU score was 20.55. Generally, both
the number of phrase pairs and the BLEU score
went down with the increase of t. However, this
trend did not hold within the range [0.1, 0.9]. To
achieve a good tradeoff between translation qual-
ity and phrase table size, we set t = 0.01 for the
following experiments.
</bodyText>
<subsectionHeader confidence="0.99616">
5.3 N-best lists Vs. Weighted Matrices
</subsectionHeader>
<bodyText confidence="0.995790653846154">
Figure 6 shows the BLEU scores and aver-
age extraction time using n-best alignments and
weighted matrices, respectively. We used the en-
tire training data for phrase extraction. When us-
ing 1-best alignments, Moses achieved a BLEU
score of 0.2826 and the average extraction time
was 4.19 milliseconds per sentence pair (see point
n = 1). The BLEU scores rose with the in-
crease of n for using n-best alignments. How-
ever, the score went down slightly when n = 50.
This suggests that including more noisy align-
ments might be harmful. These improvements
over 1-best alignments are not statistically signif-
icant. This finding failed to echo the promising
results reported by Venogopal et al. (2008). We
think that there are two possible reasons. First,
they evaluated their approach on the IWSLT data
while we used the NIST data. It might be easier
to obtain significant improvements on the IWSLT
data in which the sentences are shorter. Sec-
ond, they used the hierarchical phrase-based sys-
tem while we used the phrase-based system, which
might be less sensitive to word alignments because
the alignments inside the phrase pairs hardly have
an effect.
When using weighted alignment matrices, we
</bodyText>
<figureCaption confidence="0.813116333333333">
Figure 6: Comparison of n-best alignments and
weighted alignment matrices. We use m(n) to de-
note the matrices that take n-best lists as samples.
</figureCaption>
<bodyText confidence="0.999010916666667">
obtained higher BLEU scores than using n-best
lists with much less extraction time. We achieved
a BLEU score of 0.2901 when using the weighted
matrices estimated from 10-best lists. The abso-
lute improvement of 0.75 over using 1-best align-
ments (from 0.2826 to 0.2901) is statistically sig-
nificant at p &lt; 0.05 by using sign-test (Collins
et al., 2005). Although the improvements over n-
best lists are not always statistically significant,
weighted alignment matrices maintain consistent
superiority in both translation quality and extrac-
tion speed.
</bodyText>
<subsectionHeader confidence="0.999595">
5.4 Comparison of Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.99988935">
In theory, the set of phrase pairs extracted from n-
best alignments is the subset of the set extracted
from the corresponding weighted matrices. In
practice, however, this is not true because we use
the pruning threshold t to maintain a reasonable
table size. Even so, the phrase tables produced by
n-best lists and weighted matrices still share many
phrase pairs.
Table 2 gives some statistics. We use m(10)
to represent the weighted matrices estimated from
10-best lists. “all” denotes the full phrase table,
“shared” denotes the intersection of two tables,
and “non-shared” denotes the complement. Note
that the probabilities of “shared” phrase pairs are
different for the two approaches. We obtained
6.13M and 6.34M phrase pairs for the test set by
using 10-best lists and the corresponding matrices,
respectively. There were 4.58M phrase pairs in-
cluded by both tables. Note that the relative fre-
quencies and lexical weights for the same phrase
</bodyText>
<page confidence="0.864928">
1023
</page>
<table confidence="0.9977235">
method shared non-shared all
phrases BLEU phrases BLEU phrases BLEU
10-best 4.58M 28.35 1.55M 12.32 6.13M 28.47
m(10) 4.58M 28.90 1.76M 13.21 6.34M 29.01
</table>
<tableCaption confidence="0.998135">
Table 2: Comparison of phrase tables learned from n-best lists and weighted matrices. We use m(10)
</tableCaption>
<bodyText confidence="0.954078333333333">
to represent the weighted matrices estimated from 10-best lists. “all” denotes the full phrase table,
“shared” denotes the intersection of two tables, and “non-shared” denotes the complement. Note that the
probabilities of “shared” phrase pairs are different for the two approaches.
</bodyText>
<figure confidence="0.7585215">
0 50 100 150 200 250
training corpus size (103)
</figure>
<figureCaption confidence="0.976500333333333">
Figure 7: Comparison of n-best alignments and
weighted alignment matrices with varying training
corpus sizes.
</figureCaption>
<bodyText confidence="0.999767285714286">
pairs might be different in two tables. We found
that using matrices outperformed using n-best lists
even with the same phrase pairs. This suggests that
our methods for parameter estimation make better
use of noisy data. Another interesting finding was
that using the shared phrase pairs achieved almost
the same results with using full phrase tables.
</bodyText>
<subsectionHeader confidence="0.99905">
5.5 Effect of Training Corpus Size
</subsectionHeader>
<bodyText confidence="0.999980444444444">
To investigate the effect of training corpus size on
our approach, we extracted phrase pairs from n-
best lists and weighted matrices trained on five
training corpora with varying sizes: 10K, 50K,
100K, 150K, and 239K sentence pairs. As shown
in Figure 7, our approach outperformed both 1-
best and n-best lists consistently. More impor-
tantly, the gains seem increase when more training
data are used.
</bodyText>
<subsectionHeader confidence="0.957587">
5.6 Results on Other Language Pairs
</subsectionHeader>
<bodyText confidence="0.99965175">
To further examine the efficacy of the proposed ap-
proach, we scaled our experiments to large data
with multiple language pairs. We used the Eu-
roparl training corpus from the WMT07 shared
</bodyText>
<table confidence="0.99873275">
S↔E F↔E G↔E
Sentences 1.26M 1.29M 1.26M
Foreign words 33.16M 33.18M 29.58M
English words 31.81M 32.62M 31.93M
</table>
<tableCaption confidence="0.852458">
Table 3: Statistics of the Europarl training data.
“S” denotes Spanish, “E” denotes English, “F” de-
notes French, “G” denotes German.
</tableCaption>
<table confidence="0.999765571428571">
1-best 10-best m(10)
S→E 30.90 30.97 31.03
E→S 31.16 31.25 31.34
F→E 30.69 30.76 30.82
E→F 26.42 26.65 26.54
G→E 24.46 24.58 24.66
E→G 18.03 18.30 18.20
</table>
<tableCaption confidence="0.980002">
Table 4: BLEU scores (case-insensitive) on the
</tableCaption>
<bodyText confidence="0.993205761904762">
Europarl data. “S” denotes Spanish, “E” denotes
English, “F” denotes French, “G” denotes Ger-
man.
task. 2 Table 3 shows the statistics of the train-
ing data. There are four languages (Spanish,
French, German, and English) and six transla-
tion directions (Foreign-to-English and English-
to-Foreign). We used the “dev2006” data in the
“dev” directory as the development set and the
“test2006” data in the “devtest” directory as the
test set. Both the development and test sets contain
2,000 sentences with single reference translations.
We tokenized and lowercased all the training,
development, and test data. We trained a 4-gram
language model using SRI Language Modeling
Toolkit on the target side of the training corpus for
each task. We ran GIZA++ on the entire train-
ing data to obtain n-best alignments and weighted
matrices. To save time, we just used the first 100K
sentences of each aligned training corpus to ex-
tract phrase pairs.
</bodyText>
<footnote confidence="0.579911">
2http://www.statmt.org/wmt07/shared-task.html
</footnote>
<figure confidence="0.998581357142857">
BLEU score
0.290
0.280
0.270
0.260
0.250
0.240
0.230
0.220
0.210
0.200
1-best
10-best
m(10)
</figure>
<page confidence="0.993925">
1024
</page>
<bodyText confidence="0.9989578">
Table 4 lists the case-insensitive BLEU scores
of 1-best, 10-best, and m(10) on the Europarl
data. Using weighted packed matrices continued
to show advantage overusing 1-best alignments on
multiple language pairs. However, these improve-
ments were very small and not significant. We at-
tribute this to the fact that GIZA++ usually pro-
duces high quality 1-best alignments for closely-
related European language pairs, especially when
trained on millions of sentences.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999976710526316">
Recent studies has shown that SMT systems
can benefit from making the annotation pipeline
wider: using packed forests instead of 1-best trees
(Mi et al., 2008), word lattices instead of 1-best
segmentations (Dyer et al., 2008), and n-best
alignments instead of 1-best alignments (Venu-
gopal et al., 2008). We propose a compact repre-
sentation of multiple word alignments that enables
SMT systems to make a better use of noisy align-
ments.
Matusov et al. (2004) propose “cost matrices”
for producing symmetric alignments. Kumar et al.
(2007) describe how to use “posterior probabil-
ity matrices” to improve alignment accuracy via
a bridge language. Although not using the term
”weighted matrices” directly, they both assign a
probability to each word pair.
We follow Och and Ney (2004) to develop
a new phrase extraction algorithm for weighted
alignment matrices. The methods for calculating
relative frequencies (Och and Ney, 2004) and lex-
ical weights (Koehn et al., 2003) are also adapted
for the weighted matrix case.
Many researchers (e.g., (Venugopal et al., 2003;
Deng et al., 2008)) observe that softening the
alignment consistency constraint help improve
translation quality. For example, Deng et al.
(2008) define a feature named “within phrase pair
consistency ratio” to measure the degree of consis-
tency. As each link is associated with a probability
in a weighted matrix, we use these probabilities to
evaluate the validity of a phrase pair.
We estimate the link probabilities by calculating
relative frequencies over n-best lists. Niehues and
Vogel (2008) propose a discriminative approach to
modeling the alignment matrix directly. The dif-
ference is that they assign a boolean value instead
of a probability to each word pair.
</bodyText>
<sectionHeader confidence="0.964255" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999971857142857">
We have presented a new structure called weighted
alignment matrix that encodes the alignment dis-
tribution for a sentence pair. Accordingly, we de-
velop new methods for extracting phrase pairs and
estimating their probabilities. Our experiments
show that the proposed approach achieves better
translation quality over using n-best lists in less
extraction time. An interesting finding is that our
approach performs better than the baseline even
they use the same phrase pairs.
Although our approach consistently outper-
forms using 1-best alignments for varying lan-
guage pairs, the improvements are comparatively
small. One possible reason is that taking n-best
lists as samples sometimes might change align-
ment probability distributions inappropriately. A
more principled solution is to directly model the
weighted alignment matrices, either in a genera-
tive or a discriminative way. We believe that better
estimation of alignment distributions will result in
more significant improvements.
Another interesting direction is applying our ap-
proach to extracting translation rules with hierar-
chical structures such as hierarchical phrases (Chi-
ang, 2007) and tree-to-string rules (Galley et al.,
2006; Liu et al., 2006). We expect that these
syntax-based systems could benefit more from our
approach.
</bodyText>
<sectionHeader confidence="0.954621" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999023076923077">
The authors were supported by Microsoft Re-
search Asia Natural Language Processing Theme
Program grant (2009-2010), High-Technology
R&amp;D Program (863) Project No. 2006AA010108,
and National Natural Science Foundation of China
Contract 60736014. Part of this work was done
while Yang Liu was visiting the SMT group led by
Stephan Vogel at CMU. We thank the anonymous
reviewers for their insightful comments. We are
also grateful to Stephan Vogel, Alon Lavie, Fran-
cisco Guzman, Nguyen Bach, Andreas Zollmann,
Vamshi Ambati, and Kevin Gimpel for their help-
ful feedback.
</bodyText>
<sectionHeader confidence="0.999152" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99843625">
Phil Blunsom and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of COLING/ACL 2006, pages 65–72,
Sydney, Australia, July.
</reference>
<page confidence="0.799342">
1025
</page>
<reference confidence="0.999963719626169">
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Michael Collins, Philipp Koehn, and Ivona Ku˘cerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531–540, Ann Arbor, USA, June.
Yonggang Deng, Jia Xu, and Yuqing Gao. 2008.
Phrase table training for precision and recall: What
makes a good phrase and a good phrase pair?
In Proceedings of ACL/HLT 2008, pages 81–88,
Columbus, Ohio, USA, June.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of ACL/HLT 2008, pages
1012–1020, Columbus, Ohio, June.
Alexander Fraser and Daniel Marcu. 2007. Measur-
ing word alignment quality for statistical machine
translation. Computational Linguistics, Squibs and
Discussions, 33(3):293–303.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING/ACL 2006, pages 961–968,
Sydney, Australia, July.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT/NAACL 2003, pages 127–133, Edmonton,
Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL 2007 (poster), pages
77–80, Prague, Czech Republic, June.
Shankar Kumar, Franz J. Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In Proceedings of EMNLP 2007,
pages 42–50, Prague, Czech Republic, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
ofACL 2005, pages 459–466, Ann Arbor, Michigan,
June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL 2006,
pages 609–616, Sydney, Australia, July.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In Proceedings of COLING
2004, pages 219–225, Geneva, Switzerland, August.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings ofACL/HLT 2008,
pages 192–199, Columbus, Ohio, June.
Robert C. Moore, Wen-tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In Proceedings of COLING/ACL 2006,
pages 513–520, Sydney, Australia, July.
Jan Niehues and Stephan Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of WMT-3, pages 18–25, Columbus,
Ohio, USA, June.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417–449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL 2002, pages 311–318, Philadelphia, Penn-
sylvania, USA, July.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL/HLT 2008, pages 577–585,
Columbus, Ohio, June.
Andreas Stolcke. 2002. Srilm - an extension language
model modeling toolkit. In Proceedings of ICSLP
2002, pages 901–904, Denver, Colorado, Septem-
ber.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT/EMNLP 2005,
pages 73–80, Vancouver, British Columbia, Canada,
October.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL 2003,
pages 319–326, Sapporo, Japan, July.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: n-
best alignments and parses in mt training. In Pro-
ceedings of AMTA 2008, pages 192–201, Waikiki,
Hawaii, October.
Stephan Vogel and Hermann Ney. 1996. Hmm-based
word alignment in statistical translation. In Pro-
ceedings of COLING 1996, pages 836–841, Copen-
hagen, Danmark, August.
</reference>
<page confidence="0.993474">
1026
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851309">
<title confidence="0.998864">Weighted Alignment Matrices for Statistical Machine Translation</title>
<author confidence="0.985614">Tian Xia Liu</author>
<affiliation confidence="0.9840785">Key Laboratory of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.9363055">Chinese Academy of P.O. Box 2704, Beijing 100190,</address>
<abstract confidence="0.999737111111111">Current statistical machine translation systems usually extract rules from bilingual corpora annotated with 1-best alignments. They are prone to learn noisy rules due to alignment mistakes. We propose a new called alignment matrix to encode all possible alignments for a parallel text compactly. The key idea is to assign a probability to each word pair to indicate how well they are aligned. We design new algorithms for extracting phrase pairs from weighted alignment matrices and estimating their probabilities. Our experiments on multiple language pairs show that using weighted matrices achieves conimprovements over using lists in significant less extraction time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative word alignment with conditional random fields.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL 2006,</booktitle>
<pages>65--72</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2127" citStr="Blunsom and Cohn, 2006" startWordPosition="319" endWordPosition="322">Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the freedom to define arbitrary feature functions that describe various characteristics of an alignment. They usually optimize feature weights on manually-aligned data. While discriminative methods show superior alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large sentence-aligned corpora. However, neither generative nor discriminative alignment methods are reliable enough to yield high quality alignments for SMT, especially for distantly-related language pairs such as ChineseEnglish and Arabic-English. The F-measures for Chines</context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative word alignment with conditional random fields. In Proceedings of COLING/ACL 2006, pages 65–72, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1829" citStr="Brown et al., 1993" startWordPosition="273" endWordPosition="276">ne of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the freedom to define arbitrary feature functions that describe various characteristics of an alignment. They usually optimize feature weights on manually-aligned data. While discriminative methods show superior alignment accuracy in benchmarks, generative methods are still widely used to produ</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1481" citStr="Chiang, 2007" startWordPosition="218" endWordPosition="219">tiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2</context>
<context position="32583" citStr="Chiang, 2007" startWordPosition="5458" endWordPosition="5460">t alignments for varying language pairs, the improvements are comparatively small. One possible reason is that taking n-best lists as samples sometimes might change alignment probability distributions inappropriately. A more principled solution is to directly model the weighted alignment matrices, either in a generative or a discriminative way. We believe that better estimation of alignment distributions will result in more significant improvements. Another interesting direction is applying our approach to extracting translation rules with hierarchical structures such as hierarchical phrases (Chiang, 2007) and tree-to-string rules (Galley et al., 2006; Liu et al., 2006). We expect that these syntax-based systems could benefit more from our approach. Acknowledgement The authors were supported by Microsoft Research Asia Natural Language Processing Theme Program grant (2009-2010), High-Technology R&amp;D Program (863) Project No. 2006AA010108, and National Natural Science Foundation of China Contract 60736014. Part of this work was done while Yang Liu was visiting the SMT group led by Stephan Vogel at CMU. We thank the anonymous reviewers for their insightful comments. We are also grateful to Stephan </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Ku˘cerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, USA,</location>
<marker>Collins, Koehn, Ku˘cerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Ku˘cerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL 2005, pages 531–540, Ann Arbor, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>Jia Xu</author>
<author>Yuqing Gao</author>
</authors>
<title>Phrase table training for precision and recall: What makes a good phrase and a good phrase pair?</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT</booktitle>
<pages>81--88</pages>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="30754" citStr="Deng et al., 2008" startWordPosition="5184" endWordPosition="5187"> “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named “within phrase pair consistency ratio” to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair. We estimate the link probabilities by calculating relative frequencies over n-best lists. Niehues and Vogel (2008) propose a discriminative approach to modeling the alignment matrix directly. The difference is that they assign a boole</context>
</contexts>
<marker>Deng, Xu, Gao, 2008</marker>
<rawString>Yonggang Deng, Jia Xu, and Yuqing Gao. 2008. Phrase table training for precision and recall: What makes a good phrase and a good phrase pair? In Proceedings of ACL/HLT 2008, pages 81–88, Columbus, Ohio, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT 2008,</booktitle>
<pages>1012--1020</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3261" citStr="Dyer et al., 2008" startWordPosition="484" endWordPosition="487"> language pairs such as ChineseEnglish and Arabic-English. The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al., 2005) and 70% (Fraser and Marcu, 2007), respectively. As most current SMT systems only use 1-best alignments for extracting rules, alignment errors might impair translation quality. Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Mi et al., 2008) and replacing 1-best word segmentations with word lattices (Dyer et al., 2008). Similarly, Venugopal et al. (2008) use n-best alignments instead of 1-best alignments for translation rule extraction. While they achieve significant improvements on the IWSLT data, extracting rules from n-best alignments might be computationally expensive. In this paper, we propose a new structure named weighted alignment matrix to represent the alignment distribution for a sentence pair compactly. In a weighted matrix, each element that corresponds to a word pair is assigned a probability to measure the confidence of aligning the two words. Therefore, a weighted matrix is capable of using </context>
<context position="29897" citStr="Dyer et al., 2008" startWordPosition="5047" endWordPosition="5050">roparl data. Using weighted packed matrices continued to show advantage overusing 1-best alignments on multiple language pairs. However, these improvements were very small and not significant. We attribute this to the fact that GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm </context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of ACL/HLT 2008, pages 1012–1020, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<booktitle>Computational Linguistics, Squibs and Discussions,</booktitle>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="2830" citStr="Fraser and Marcu, 2007" startWordPosition="417" endWordPosition="420">haracteristics of an alignment. They usually optimize feature weights on manually-aligned data. While discriminative methods show superior alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large sentence-aligned corpora. However, neither generative nor discriminative alignment methods are reliable enough to yield high quality alignments for SMT, especially for distantly-related language pairs such as ChineseEnglish and Arabic-English. The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al., 2005) and 70% (Fraser and Marcu, 2007), respectively. As most current SMT systems only use 1-best alignments for extracting rules, alignment errors might impair translation quality. Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Mi et al., 2008) and replacing 1-best word segmentations with word lattices (Dyer et al., 2008). Similarly, Venugopal et al. (2008) use n-best alignments instead of 1-best alignments for translation rule extraction. While they achieve significant improvements on t</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Computational Linguistics, Squibs and Discussions, 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1502" citStr="Galley et al., 2006" startWordPosition="220" endWordPosition="223"> pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005</context>
<context position="32629" citStr="Galley et al., 2006" startWordPosition="5464" endWordPosition="5467"> the improvements are comparatively small. One possible reason is that taking n-best lists as samples sometimes might change alignment probability distributions inappropriately. A more principled solution is to directly model the weighted alignment matrices, either in a generative or a discriminative way. We believe that better estimation of alignment distributions will result in more significant improvements. Another interesting direction is applying our approach to extracting translation rules with hierarchical structures such as hierarchical phrases (Chiang, 2007) and tree-to-string rules (Galley et al., 2006; Liu et al., 2006). We expect that these syntax-based systems could benefit more from our approach. Acknowledgement The authors were supported by Microsoft Research Asia Natural Language Processing Theme Program grant (2009-2010), High-Technology R&amp;D Program (863) Project No. 2006AA010108, and National Natural Science Foundation of China Contract 60736014. Part of this work was done while Yang Liu was visiting the SMT group led by Stephan Vogel at CMU. We thank the anonymous reviewers for their insightful comments. We are also grateful to Stephan Vogel, Alon Lavie, Francisco Guzman, Nguyen Ba</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING/ACL 2006, pages 961–968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL 2003,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1426" citStr="Koehn et al., 2003" startWordPosition="208" endWordPosition="211">ces and estimating their probabilities. Our experiments on multiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminati</context>
<context position="6043" citStr="Koehn et al., 2003" startWordPosition="960" endWordPosition="963">ase pair ( ˜f, ˜e) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word inside one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase. After all phrase pairs are extracted from the training corpus, their translation probabilities can be estimated as relative frequencies (Och and Ney, 2004): f, ˜e) E˜e′ count( where count(˜f, ˜e) indicates how often the phrase pair (˜f, ˜e) occurs in the training corpus. Besides relative frequencies, lexical weights (Koehn et al., 2003) are widely used to estimate how well the words in f˜ translate the words in ˜e. To do this, one needs first to estimate a lexical translation probability distribution w(e|f) by relative frequency from the same word alignments in the training corpus: count(f,e) w(e|f) = (3) Ee′ count(f, e′) Note that a special source NULL token is added to each source sentence and aligned to each unaligned target word. As the alignment a˜ between a phrase pair ( f, ˜e) is retained during extraction, the lexical weight can be calculated as 1 {j|(j,i) �w(ei|fj) (4) Ea}I If there are multiple alignments a˜ for a </context>
<context position="18822" citStr="Koehn et al. (2003)" startWordPosition="3235" endWordPosition="3238">n the top two candidates in Table 1 if t = 0.3. Note that we perform the pruning locally. Although it is more reasonable to prune a phrase table after accumulating all fractional counts from 1021 pw(˜e|˜f,m) = H |˜e |((1 i=1 {j|pmu, i) &gt; 01 x � �p(ei|fj) x pm(j, i) + dj:pm(j,i)&gt;0 p(ei|f0) x |˜f |�¯pm(j, i) (16) H j=1 training corpus, such global pruning strategy usually leads to very large disk and memory requirements. 4.3 Calculating Lexical Weights Recall that we need to obtain two translation probability tables w(e|f) and w(f|e) before calculating lexical weights (see Section 2). Following Koehn et al. (2003), we estimate the two distributions by relative frequencies from the training corpus annotated with weighted alignment matrices. In other words, we still use Eq. (3) but the way of calculating fractional counts is different now. Given a source word fj, a target word ei, and a weighted alignment matrix, the fractional count count(fj, ei) is pm(j, i). For NULL words, the fractional counts can be calculated as count(fj, e0) = HI ¯pm(j, i) (14) i=1 J count(f0, ei) = H ¯pm(j, i) (15) j=1 For example, in Figure 4, count(de, of) is 0.6, count(de,NULL) is 0.24, and count(NULL,of) is 0.24. Then, we ada</context>
<context position="20515" citStr="Koehn et al., 2003" startWordPosition="3524" endWordPosition="3527">guage model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT evaluation test set as our development set, and used the NIST 2005 test set as our test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). To obtain weighted alignment matrices, we followed Venugopal et al. (2008) to produce nbest lists via GIZA++. We first ran GIZA++ to produce 50-best lists in two translation directions. Then, we used the refinement technique “grow-diag-final-and” (Koehn et al., 2003) to all 50 x 50 bidirectional alignment pairs. Suppose that ps2t and pt2s are the probabilities of an alignment pair assigned by GIZA++, respectively. We used ps2t x pt2s as the probability of the resulting symmetric alignment. As different alignment pairs might produce the same symmetric alignments, we followed Venugopal et al. (2008) to remove duplicate alignments and retain only the alignment with the highest probability. Therefore, there were 550 candidate alignments on average for each sentence pair in the training data. We obtained n-best lists by selecting the top n alignments from the </context>
<context position="30639" citStr="Koehn et al., 2003" startWordPosition="5165" endWordPosition="5168">ple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named “within phrase pair consistency ratio” to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair. We estimate the link probabilities by calculating relative frequencies over n-best lists. Niehues and Vogel (2008) prop</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL 2003, pages 127–133, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>77--80</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="21458" citStr="Koehn et al., 2007" startWordPosition="3672" endWordPosition="3675">et al. (2008) to remove duplicate alignments and retain only the alignment with the highest probability. Therefore, there were 550 candidate alignments on average for each sentence pair in the training data. We obtained n-best lists by selecting the top n alignments from the 550-best lists. The probability of each alignment in the n-best list was re-estimated by re-normalization (Venugopal et al., 2008). Finally, these n-best alignments served as samples for constructing weighted alignment matrices. After extracting phrase pairs from n-best lists and weighted alignment matrices, we ran Moses (Koehn et al., 2007) to translate the development and test sets. We used the simple distance-based reordering model to remove the dependency of lexicalization on word alignments for Moses. 5.2 Effect of Pruning Threshold Our first experiment investigated the effect of pruning threshold on translation quality (BLEU scores on the test set) and the phrase table size (filtered for the test set), as shown in Figure 5. To save time, we extracted phrase pairs just from the first 10K sentence pairs of the FBIS corpus. We used 12 different thresholds: 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9. O</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL 2007 (poster), pages 77–80, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Franz J Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Improving word alignment with bridge languages.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP 2007,</booktitle>
<pages>42--50</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="30208" citStr="Kumar et al. (2007)" startWordPosition="5097" endWordPosition="5100"> language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency con</context>
</contexts>
<marker>Kumar, Och, Macherey, 2007</marker>
<rawString>Shankar Kumar, Franz J. Och, and Wolfgang Macherey. 2007. Improving word alignment with bridge languages. In Proceedings of EMNLP 2007, pages 42–50, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Loglinear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL 2005,</booktitle>
<pages>459--466</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2102" citStr="Liu et al., 2005" startWordPosition="315" endWordPosition="318">ley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the freedom to define arbitrary feature functions that describe various characteristics of an alignment. They usually optimize feature weights on manually-aligned data. While discriminative methods show superior alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large sentence-aligned corpora. However, neither generative nor discriminative alignment methods are reliable enough to yield high quality alignments for SMT, especially for distantly-related language pairs such as ChineseEnglish and Arabic-English. </context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear models for word alignment. In Proceedings ofACL 2005, pages 459–466, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL 2006,</booktitle>
<pages>609--616</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1540" citStr="Liu et al., 2006" startWordPosition="228" endWordPosition="231"> achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the fr</context>
<context position="32648" citStr="Liu et al., 2006" startWordPosition="5468" endWordPosition="5471"> comparatively small. One possible reason is that taking n-best lists as samples sometimes might change alignment probability distributions inappropriately. A more principled solution is to directly model the weighted alignment matrices, either in a generative or a discriminative way. We believe that better estimation of alignment distributions will result in more significant improvements. Another interesting direction is applying our approach to extracting translation rules with hierarchical structures such as hierarchical phrases (Chiang, 2007) and tree-to-string rules (Galley et al., 2006; Liu et al., 2006). We expect that these syntax-based systems could benefit more from our approach. Acknowledgement The authors were supported by Microsoft Research Asia Natural Language Processing Theme Program grant (2009-2010), High-Technology R&amp;D Program (863) Project No. 2006AA010108, and National Natural Science Foundation of China Contract 60736014. Part of this work was done while Yang Liu was visiting the SMT group led by Stephan Vogel at CMU. We thank the anonymous reviewers for their insightful comments. We are also grateful to Stephan Vogel, Alon Lavie, Francisco Guzman, Nguyen Bach, Andreas Zollman</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of COLING/ACL 2006, pages 609–616, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Symmetric word alignments for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>219--225</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="30128" citStr="Matusov et al. (2004)" startWordPosition="5086" endWordPosition="5089">GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al.</context>
</contexts>
<marker>Matusov, Zens, Ney, 2004</marker>
<rawString>Evgeny Matusov, Richard Zens, and Hermann Ney. 2004. Symmetric word alignments for statistical machine translation. In Proceedings of COLING 2004, pages 219–225, Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL/HLT 2008,</booktitle>
<pages>192--199</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3182" citStr="Mi et al., 2008" startWordPosition="471" endWordPosition="474">gh to yield high quality alignments for SMT, especially for distantly-related language pairs such as ChineseEnglish and Arabic-English. The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al., 2005) and 70% (Fraser and Marcu, 2007), respectively. As most current SMT systems only use 1-best alignments for extracting rules, alignment errors might impair translation quality. Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Mi et al., 2008) and replacing 1-best word segmentations with word lattices (Dyer et al., 2008). Similarly, Venugopal et al. (2008) use n-best alignments instead of 1-best alignments for translation rule extraction. While they achieve significant improvements on the IWSLT data, extracting rules from n-best alignments might be computationally expensive. In this paper, we propose a new structure named weighted alignment matrix to represent the alignment distribution for a sentence pair compactly. In a weighted matrix, each element that corresponds to a word pair is assigned a probability to measure the confiden</context>
<context position="29830" citStr="Mi et al., 2008" startWordPosition="5037" endWordPosition="5040">e-insensitive BLEU scores of 1-best, 10-best, and m(10) on the Europarl data. Using weighted packed matrices continued to show advantage overusing 1-best alignments on multiple language pairs. However, these improvements were very small and not significant. We attribute this to the fact that GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We foll</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings ofACL/HLT 2008, pages 192–199, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Wen-tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved discriminative bilingual word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL 2006,</booktitle>
<pages>513--520</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2063" citStr="Moore et al., 2006" startWordPosition="307" endWordPosition="310">ax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the freedom to define arbitrary feature functions that describe various characteristics of an alignment. They usually optimize feature weights on manually-aligned data. While discriminative methods show superior alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large sentence-aligned corpora. However, neither generative nor discriminative alignment methods are reliable enough to yield high quality alignments for SMT, especially for distantly-related language pairs such</context>
</contexts>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006. Improved discriminative bilingual word alignment. In Proceedings of COLING/ACL 2006, pages 513–520, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Stephan Vogel</author>
</authors>
<title>Discriminative word alignment via alignment matrix modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of WMT-3,</booktitle>
<pages>18--25</pages>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="31234" citStr="Niehues and Vogel (2008)" startWordPosition="5257" endWordPosition="5260"> weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named “within phrase pair consistency ratio” to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair. We estimate the link probabilities by calculating relative frequencies over n-best lists. Niehues and Vogel (2008) propose a discriminative approach to modeling the alignment matrix directly. The difference is that they assign a boolean value instead of a probability to each word pair. 7 Conclusion and Future Work We have presented a new structure called weighted alignment matrix that encodes the alignment distribution for a sentence pair. Accordingly, we develop new methods for extracting phrase pairs and estimating their probabilities. Our experiments show that the proposed approach achieves better translation quality over using n-best lists in less extraction time. An interesting finding is that our ap</context>
</contexts>
<marker>Niehues, Vogel, 2008</marker>
<rawString>Jan Niehues and Stephan Vogel. 2008. Discriminative word alignment via alignment matrix modeling. In Proceedings of WMT-3, pages 18–25, Columbus, Ohio, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1561" citStr="Och and Ney (2003)" startWordPosition="232" endWordPosition="235"> improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the freedom to define arbit</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1405" citStr="Och and Ney, 2004" startWordPosition="204" endWordPosition="207">ted alignment matrices and estimating their probabilities. Our experiments on multiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In c</context>
<context position="5487" citStr="Och and Ney, 2004" startWordPosition="864" endWordPosition="867"> is aligned to the fourth English word “China”. Formally, given a source sentence f = fJ1 = f1, ... , fj, ... , fJ and a target sentence e = eI1 = e1, ... , ei, ... , eI, we define a link l = (j, i) to exist if fj and ei are translation (or part of translation) of one another. Then, an alignment a is a subset of the Cartesian product of word positions: a ⊆ {(j,i) : j = 1,... , J;i = 1,... ,I} (1) Usually, SMT systems only use the 1-best alignments for extracting translation rules. For example, given a source phrase f˜ and a target phrase ˜e, the phrase pair ( ˜f, ˜e) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word inside one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase. After all phrase pairs are extracted from the training corpus, their translation probabilities can be estimated as relative frequencies (Och and Ney, 2004): f, ˜e) E˜e′ count( where count(˜f, ˜e) indicates how often the phrase pair (˜f, ˜e) occurs in the training corpus. Besides relative frequencies, lexical weights (Koehn et al., 2003) are widely used to estimate how well the wo</context>
<context position="11761" citStr="Och and Ney (2004)" startWordPosition="2016" endWordPosition="2019">the probability of the same alignment encoded in the matrix shown in Figure 2(c) (i.e., pm(a)) becomes 0.1296 according to Eq. (10). It should be emphasized that a weighted matrix encodes all possible alignments rather than the input n-best list, although the link probabilities are estimated from the n-best list. 4 Phrase Pair Extraction In this section, we describe how to extract phrase pairs from the training corpus annotated with weighted alignment matrices (Section 4.1) and how to estimate their relative frequencies (Section 4.2) and lexical weights (Section 4.3). 4.1 Extraction Algorithm Och and Ney (2004) describe a “phrase-extract” algorithm for extracting phrase pairs from a sentence pair annotated with a 1-best alignment. Given a source phrase, they first identify the target phrase that is consistent with the alignment. Then, they expand the boundaries of the target phrase if the boundary words are unaligned. Unfortunately, this algorithm cannot be directly used to manipulate a weighted alignment matrix, which is a compact representation of all possible alignments. The major difference is that the “tight” phrase that has both boundary words aligned is not necessarily the smallest candidate </context>
<context position="30451" citStr="Och and Ney (2004)" startWordPosition="5136" endWordPosition="5139">ord lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named “within phrase pair consistency ratio” to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, w</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="20246" citStr="Papineni et al., 2002" startWordPosition="3481" endWordPosition="3484">to account and calculate the probability that a target word translates a source NULL token explicitly. 5 Experiments 5.1 Data Preparation We evaluated our approach on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training data. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT evaluation test set as our development set, and used the NIST 2005 test set as our test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). To obtain weighted alignment matrices, we followed Venugopal et al. (2008) to produce nbest lists via GIZA++. We first ran GIZA++ to produce 50-best lists in two translation directions. Then, we used the refinement technique “grow-diag-final-and” (Koehn et al., 2003) to all 50 x 50 bidirectional alignment pairs. Suppose that ps2t and pt2s are the probabilities of an alignment pair assigned by GIZA++, respectively. We used ps2t x pt2s as the probability of the resulting symmetric alignment. As different alignment pairs might produce the same symmetric alignments, we followed Venugopal et al. </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL 2002, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT</booktitle>
<pages>577--585</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1521" citStr="Shen et al., 2008" startWordPosition="224" endWordPosition="227">g weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn,</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL/HLT 2008, pages 577–585, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extension language model modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP 2002,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado,</location>
<contexts>
<context position="19966" citStr="Stolcke, 2002" startWordPosition="3432" endWordPosition="3433"> 0.6, count(de,NULL) is 0.24, and count(NULL,of) is 0.24. Then, we adapt Eq. (4) to calculate lexical weight: For example, for the target word “of” in Figure 4, the sum of aligned and unaligned probabilities is 1 2 p(of|NULL) x 0.24 Note that we take link probabilities into account and calculate the probability that a target word translates a source NULL token explicitly. 5 Experiments 5.1 Data Preparation We evaluated our approach on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training data. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT evaluation test set as our development set, and used the NIST 2005 test set as our test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). To obtain weighted alignment matrices, we followed Venugopal et al. (2008) to produce nbest lists via GIZA++. We first ran GIZA++ to produce 50-best lists in two translation directions. Then, we used the refinement technique “grow-diag-final-and” (Koehn et al., 2003) to all 50 x 50 bidirectional alignment pairs. Supp</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extension language model modeling toolkit. In Proceedings of ICSLP 2002, pages 901–904, Denver, Colorado, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP 2005,</booktitle>
<pages>73--80</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="2084" citStr="Taskar et al., 2005" startWordPosition="311" endWordPosition="314">., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the freedom to define arbitrary feature functions that describe various characteristics of an alignment. They usually optimize feature weights on manually-aligned data. While discriminative methods show superior alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large sentence-aligned corpora. However, neither generative nor discriminative alignment methods are reliable enough to yield high quality alignments for SMT, especially for distantly-related language pairs such as ChineseEnglish an</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In Proceedings of HLT/EMNLP 2005, pages 73–80, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Effective phrase translation extraction from alignment models.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL 2003,</booktitle>
<pages>319--326</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="30734" citStr="Venugopal et al., 2003" startWordPosition="5180" endWordPosition="5183">ov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named “within phrase pair consistency ratio” to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair. We estimate the link probabilities by calculating relative frequencies over n-best lists. Niehues and Vogel (2008) propose a discriminative approach to modeling the alignment matrix directly. The difference is that</context>
</contexts>
<marker>Venugopal, Vogel, Waibel, 2003</marker>
<rawString>Ashish Venugopal, Stephan Vogel, and Alex Waibel. 2003. Effective phrase translation extraction from alignment models. In Proceedings of ACL 2003, pages 319–326, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Wider pipelines: nbest alignments and parses in mt training.</title>
<date>2008</date>
<booktitle>In Proceedings of AMTA</booktitle>
<pages>192--201</pages>
<location>Waikiki, Hawaii,</location>
<contexts>
<context position="3297" citStr="Venugopal et al. (2008)" startWordPosition="490" endWordPosition="493">English and Arabic-English. The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al., 2005) and 70% (Fraser and Marcu, 2007), respectively. As most current SMT systems only use 1-best alignments for extracting rules, alignment errors might impair translation quality. Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Mi et al., 2008) and replacing 1-best word segmentations with word lattices (Dyer et al., 2008). Similarly, Venugopal et al. (2008) use n-best alignments instead of 1-best alignments for translation rule extraction. While they achieve significant improvements on the IWSLT data, extracting rules from n-best alignments might be computationally expensive. In this paper, we propose a new structure named weighted alignment matrix to represent the alignment distribution for a sentence pair compactly. In a weighted matrix, each element that corresponds to a word pair is assigned a probability to measure the confidence of aligning the two words. Therefore, a weighted matrix is capable of using a lin1017 Proceedings of the 2009 Co</context>
<context position="7095" citStr="Venugopal et al., 2008" startWordPosition="1137" endWordPosition="1140">ween a phrase pair ( f, ˜e) is retained during extraction, the lexical weight can be calculated as 1 {j|(j,i) �w(ei|fj) (4) Ea}I If there are multiple alignments a˜ for a phrase pair (˜f, ˜e), Koehn et al. (2003) choose the one with the highest lexical weight: { } pw(˜e |˜f, ˜a) (5) Simple and effective, relative frequencies and lexical weights have become the standard features in modern discriminative SMT systems. 3 Weighted Alignment Matrix We believe that offering more candidate alignments to extracting translation rules might help improve translation quality. Instead of using nbest lists (Venugopal et al., 2008), we propose a new structure called weighted alignment matrix. We use an example to illustrate our idea. Figure 2(a) and Figure 2(b) show two alignments of a Chinese-English sentence pair. We observe that some links (e.g., (1,4) corresponding to the word zhongguo de jingji fazhan φ(˜e| count( f) = (2) ˜f, ˜e′) pw(˜e |˜f, ˜a) = � |˜e| i=1 ˜f) = max a˜ pw(˜e| 1018 economy 0 0 1.0 0 ’s 0 0.4 0.4 0 China 1.0 0 0 0 of 0 0.6 0 0.4 development 0 0 0 1.0 the 0 0 0 0 economy ’s China of development the economy ’s China of development the zhongguo de jingji fazhan zhongguo de jingji fazhan zhongguo de j</context>
<context position="20322" citStr="Venugopal et al. (2008)" startWordPosition="3493" endWordPosition="3496">urce NULL token explicitly. 5 Experiments 5.1 Data Preparation We evaluated our approach on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training data. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT evaluation test set as our development set, and used the NIST 2005 test set as our test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). To obtain weighted alignment matrices, we followed Venugopal et al. (2008) to produce nbest lists via GIZA++. We first ran GIZA++ to produce 50-best lists in two translation directions. Then, we used the refinement technique “grow-diag-final-and” (Koehn et al., 2003) to all 50 x 50 bidirectional alignment pairs. Suppose that ps2t and pt2s are the probabilities of an alignment pair assigned by GIZA++, respectively. We used ps2t x pt2s as the probability of the resulting symmetric alignment. As different alignment pairs might produce the same symmetric alignments, we followed Venugopal et al. (2008) to remove duplicate alignments and retain only the alignment with the</context>
<context position="29974" citStr="Venugopal et al., 2008" startWordPosition="5058" endWordPosition="5062">overusing 1-best alignments on multiple language pairs. However, these improvements were very small and not significant. We attribute this to the fact that GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequen</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2008</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2008. Wider pipelines: nbest alignments and parses in mt training. In Proceedings of AMTA 2008, pages 192–201, Waikiki, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Danmark,</location>
<contexts>
<context position="1851" citStr="Vogel and Ney, 1996" startWordPosition="277" endWordPosition="280">tant annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the freedom to define arbitrary feature functions that describe various characteristics of an alignment. They usually optimize feature weights on manually-aligned data. While discriminative methods show superior alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for</context>
</contexts>
<marker>Vogel, Ney, 1996</marker>
<rawString>Stephan Vogel and Hermann Ney. 1996. Hmm-based word alignment in statistical translation. In Proceedings of COLING 1996, pages 836–841, Copenhagen, Danmark, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>