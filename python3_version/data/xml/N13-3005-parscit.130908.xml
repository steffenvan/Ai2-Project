<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013278">
<title confidence="0.999196">
A Web Application for the Diagnostic Evaluation of Machine Translation
over Specific Linguistic Phenomena
</title>
<author confidence="0.99697">
Antonio Toral Sudip Kumar Naskar Joris Vreeke Federico Gaspari Declan Groves
</author>
<affiliation confidence="0.817167333333333">
School of Computing
Dublin City University
Ireland
</affiliation>
<email confidence="0.974823">
{atoral, snaskar, fgaspari, dgroves}@computing.dcu.ie joris.vreeke@dcu.ie
</email>
<sectionHeader confidence="0.995556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999163833333333">
This paper presents a web application and a
web service for the diagnostic evaluation of
Machine Translation (MT). These web-based
tools are built on top of DELiC4MT, an open-
source software package that assesses the per-
formance of MT systems over user-defined
linguistic phenomena (lexical, morphological,
syntactic and semantic). The advantage of the
web-based scenario is clear; compared to the
standalone tool, the user does not need to carry
out any installation, configuration or mainte-
nance of the tool.
</bodyText>
<sectionHeader confidence="0.9620605" genericHeader="method">
1 Automatic Evaluation of Machine
Translation beyond Overall Scores
</sectionHeader>
<bodyText confidence="0.999741277777778">
Machine translation (MT) output can be evaluated
using different approaches, which can essentially be
divided into human and automatic, both of which,
however, present a number of shortcomings. Hu-
man evaluation tends to be more reliable in a num-
ber of ways and can be tailored to a variety of situ-
ations, but is rather expensive (both in terms of re-
sources and time) and is difficult to replicate. On
the other hand, standard automatic MT evaluation
metrics such as BLEU (Papineni et al., 2002) and
METEOR (Banerjee and Lavie, 2005) are consid-
erably cheaper and provide faster results, but return
rather crude scores that are difficult to interpret for
MT users and developers alike. Crucially, current
standard automatic MT evaluation metrics also lack
any diagnostic value, i.e. they cannot identify spe-
cific weaknesses in the MT output. Diagnostic in-
formation can be extremely valuable for MT devel-
</bodyText>
<page confidence="0.91584">
20
</page>
<bodyText confidence="0.9997798125">
opers and users, e.g. to improve the performance of
the system or to decide which output is more suited
for particular scenarios.
An interesting alternative to the traditional MT
evaluation metrics is to evaluate the performance
of MT systems over specific linguistic phenomena.
While retaining the main advantage of automatic
metrics (low cost), this approach provides more fine-
grained linguistically-motivated evaluation. The lin-
guistic phenomena, also referred to as linguistic
checkpoints, can be defined in terms of linguistic in-
formation at different levels (lexical, morphological,
syntactic, semantic, etc.) that appear in the source
language. Examples of such linguistic checkpoints,
what translation information they can represent, and
their relevance for MT are provided in Table 1.
</bodyText>
<sectionHeader confidence="0.637222" genericHeader="method">
Checkpoint Relevance for MT
</sectionHeader>
<subsectionHeader confidence="0.945218">
Lexical Words that can have multiple translations in
</subsectionHeader>
<bodyText confidence="0.99969">
the target. For example, the preposition “de”
in Spanish can be translated into English as
“of” or “from” depending on the context.
</bodyText>
<subsectionHeader confidence="0.936716">
Syntactic Syntactic constructs that are difficult to trans-
</subsectionHeader>
<bodyText confidence="0.999608857142857">
late. E.g., a checkpoint containing the se-
quence a noun (noun]) followed by the
preposition “de”, followed by another noun
(noun2) when translating from Spanish to
English. The equivalent English construct
would be noun2’s noun], the translation thus
involving some reordering.
</bodyText>
<subsectionHeader confidence="0.968084">
Semantic Words with multiple meanings, which possi-
</subsectionHeader>
<bodyText confidence="0.98499125">
bly correspond to different translations in the
target language. Polysemous words can be
collected from electronic dictionaries such as
WordNet (Miller, 1995).
</bodyText>
<tableCaption confidence="0.9813935">
Table 1: Linguistic Checkpoints
Checkpoints can also be built by combining el-
</tableCaption>
<subsubsectionHeader confidence="0.388711">
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 20–23,
</subsubsectionHeader>
<bodyText confidence="0.979894348837209">
Atlanta, Georgia, 10-12 June 2013. c�2013 Association for Computational Linguistics
ements from different categories. For example, by
combining lexical and syntantic elements, we could
define a checkpoint for prepositional phrases (syn-
tactic element) which start with the preposition “de”
(lexical element).
Woodpecker (Zhou et al., 2008) is a tool that per-
forms diagnostic evaluation of MT systems over lin-
guistic checkpoints for English–Chinese. Probably
due to its limitation to one language pair, its pro-
prietary nature as well as rather restrictive licensing
conditions, Woodpecker does not seem to have been
widely used in the community, in spite of its ability
to support diagnostic evaluation.
DELiC4MT1 is an open-source software that fol-
lows the same approach as Woodpecker. However,
DELiC4MT is easily portable to any language pair2
and provides additional functionality such as filter-
ing of noisy checkpoint instances and support for
statistical significance tests. This paper focuses on
the usage of this tool through a web application and
a web service from the user’s perspective. Details
regarding its implementation, evaluation, etc. can
be found in (Toral et al., 2012; Naskar et al., 2011).
2 Web Services for Language Technology
Tools
There exist many freely available language pro-
cessing tools, some of which are distributed under
open-source licenses. In order to use these tools,
they need to be downloaded, installed, configured
and maintained, which results in high cost both in
terms of manual effort and computing resources.
The requirement for in-depth technical knowledge
severely limits the usability of these tools amongst
non-technical users, particularly in our case amongst
translators and post-editors.
Web services introduce a new paradigm in the
way we use software tools where only providers
of the tools are required to have knowledge re-
garding their installation, configuration and mainte-
nance. This enables wider adoption of the tools and
reduces the learning curve for users as the only infor-
mation needed is basic knowledge of the functional-
</bodyText>
<footnote confidence="0.9590178">
1http://www.computing.dcu.ie/˜atoral/
delic4mt/
2It has already been tested on language pairs involving
the following languages: Arabic, Bulgarian, Dutch, English,
French, German, Hindi, Italian, Turkish and Welsh.
</footnote>
<bodyText confidence="0.997235454545454">
ity and input/output parameters (which can be easily
included, e.g. as part of an online tutorial). While
this paradigm is rather new in the field of compu-
tational linguistics, it is quite mature and successful
in other fields such as bioinformatics (Oinn et al.,
2004; Labarga et al., 2007).
Related work includes two web applications in the
area of MT evaluation. iBLEU (Madnani, 2011) or-
ganises BLEU scoring information in a visual man-
ner. Berka et al. (2012) perform automatic error de-
tection and classification of MT output.
</bodyText>
<figureCaption confidence="0.997222">
Figure 1: Web interface for the web service.
</figureCaption>
<sectionHeader confidence="0.997859" genericHeader="method">
3 Demo
</sectionHeader>
<bodyText confidence="0.998562">
The demo presented in this paper consists of a
web service and a web application built on top of
DELiC4MT that allow to assess the performance of
MT systems on different linguistic phenomena de-
</bodyText>
<page confidence="0.998714">
21
</page>
<figureCaption confidence="0.999822">
Figure 2: Screenshot of the web application (visualisation of results).
</figureCaption>
<bodyText confidence="0.974629">
fined by the user. The following subsections detail
both parts of the demo.
</bodyText>
<subsectionHeader confidence="0.999294">
3.1 Web Service
</subsectionHeader>
<bodyText confidence="0.997312">
A SOAP-compliant web service3 has been built on
top of DELiC4MT. It receives the following input
parameters (see Figure 1):
</bodyText>
<listItem confidence="0.9802254">
1. Word alignment between the source and target
sides of the testset, in the GIZA++ (Och and
Ney, 2003) output format.
2. Linguistic checkpoint defined as a Ky-
bot4 (Vossen et al., 2010) profile.
3. Output of the MT system to be evaluated, in
plain text, tokenised and one sentence per line.
4. Source and target sides of the testset (or
gold standard), in KAF format (Bosma et al.,
2009).5
</listItem>
<bodyText confidence="0.995664666666667">
The tool then evaluates the performance of the
MT system (input parameter 3) on the linguistic phe-
nomenon (parameter 2) by following this procedure:
</bodyText>
<footnote confidence="0.994017666666667">
3http://registry.elda.org/services/301
4Kybot profiles can be understood as regular expressions
over KAF documents, http://kyoto.let.vu.nl/svn/
kyoto/trunk/modules/mining_module/
5An XML format for text analysis based on representation
standards from ISO TC37/SC4.
</footnote>
<listItem confidence="0.876209727272727">
• Occurrences of the linguistic phenomenon (pa-
rameter 2) are identified in the source side of
the testset (parameter 4).
• The equivalent tokens of these occurrences in
the target side (parameter 5) are found by using
word alignment information (parameter 1).
• For each checkpoint instance, the tool checks
how many of the n-grams present in the refer-
ence of the checkpoint instance are contained
in the output produced by the MT system (pa-
rameter 3).
</listItem>
<subsectionHeader confidence="0.99908">
3.2 Web Application
</subsectionHeader>
<bodyText confidence="0.999928066666667">
The web application builds a graphical interface on
top of the web service. It allows the user to visualise
the results in a fine-grained manner, the user can see
the performance of the MT system for each single
occurrence of the linguistic phenomenon.
Sample MT output for the “noun” checkpoint for
the English to French language direction is shown
in Figure 2. Two occurrences of the checkpoint are
shown. The first one regards the source noun “mr.”
and its translation in the reference “monsieur”, iden-
tified through word alignments. The alignment (4-
4) indicates that both the source and target tokens
appear at the fifth position (0-based index) in the
sentence. The reference token (“monsieur”) is not
found in the MT output and thus a score of 0/1
</bodyText>
<page confidence="0.991331">
22
</page>
<bodyText confidence="0.999675454545455">
(0 n-gram matches out of a total of 1 possible n-
gram) is assigned to the MT system for this noun in-
stance. Conversely, the score for the second occur-
rence (“speaker”) is 1/1 since the MT output con-
tains the 1-gram of the reference translation (“ora-
teur”).
The recall-based overall score is shown at the bot-
tom of the figure (0.5025). This is calculated by
summing up the scores (matching n-grams) for all
the occurrences (803) and dividing the result by the
total number of possible n-grams (1598).
</bodyText>
<sectionHeader confidence="0.997007" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.998578666666667">
In this paper we have presented a web applica-
tion and a web service for the diagnostic evalua-
tion of MT output over linguistic phenomena using
DELiC4MT. The tool allows users and developers
of MT systems to easily receive fine-grained feed-
back on the performance of their MT systems over
linguistic checkpoints of their interest. The applica-
tion is open-source, freely available and adaptable to
any language pair.
</bodyText>
<sectionHeader confidence="0.985629" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999975">
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme FP7/2007-2013 under
grant agreements FP7-ICT-4-248531 and PIAP-GA-
2012-324414 and through Science Foundation Ire-
land as part of the CNGL (grant 07/CE/I1142)
</bodyText>
<sectionHeader confidence="0.998411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998185171428571">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Intrin-
sic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, Proceedings of the
ACL-05 Workshop, pages 65–72, University of Michi-
gan, Ann Arbor, Michigan, USA.
Jan Berka, Ondej Bojar, Mark Fishel, Maja Popovi, and
Daniel Zeman. 2012. Automatic MT Error Anal-
ysis: Hjerson Helping Addicter. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation (LREC’12), Istanbul, Turkey.
European Language Resources Association (ELRA).
W. E. Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic
semantic annotation format. In Proceedings of the
GL2009 Workshop on Semantic Annotation, Septem-
ber.
Alberto Labarga, Franck Valentin, Mikael Andersson,
and Rodrigo Lopez. 2007. Web services at the euro-
pean bioinformatics institute. Nucleic Acids Research,
35(Web-Server-Issue):6–11.
Nitin Madnani. 2011. iBLEU: Interactively Debugging
and Scoring Statistical Machine Translation Systems.
In Proceedings of the 2011 IEEE Fifth International
Conference on Semantic Computing, ICSC ’11, pages
213–214, Washington, DC, USA. IEEE Computer So-
ciety.
George A. Miller. 1995. WordNet: a lexical database for
English. Commun. ACM, 38(11):39–41, November.
Sudip Kumar Naskar, Antonio Toral, Federico Gaspari,
and Andy Way. 2011. A Framework for Diagnostic
Evaluation of MT based on Linguistic Checkpoints. In
Proceedings of the 13th Machine Translation Summit,
pages 529–536, Xiamen, China, September.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19–51, March.
Tom Oinn, Matthew Addis, Justin Ferris, Darren Marvin,
Martin Senger, Mark Greenwood, Tim Carver, Kevin
Glover, Matthew R. Pocock, Anil Wipat, and Peter Li.
2004. Taverna: a tool for the composition and en-
actment of bioinformatics workflows. Bioinformatics,
20(17):3045–3054, November.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Antonio Toral, Sudip Kumar Naskar, Federico Gaspari,
and Declan Groves. 2012. DELiC4MT: A Tool for
Diagnostic MT Evaluation over User-defined Linguis-
tic Phenomena. The Prague Bulletin of Mathematical
Linguistics, pages 121–132.
Piek Vossen, German Rigau, Eneko Agirre, Aitor Soroa,
Monica Monachini, and Roberto Bartolini. 2010. KY-
OTO: an open platform for mining facts. In Proceed-
ings of the 6th Workshop on Ontologies and Lexical
Resources, pages 1–10, Beijing, China.
Ming Zhou, Bo Wang, Shujie Liu, Mu Li, Dongdong
Zhang, and Tiejun Zhao. 2008. Diagnostic evalu-
ation of machine translation systems using automati-
cally constructed linguistic check-points. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics - Volume 1, COLING ’08, pages
1121–1128, Stroudsburg, PA, USA. Association for
Computational Linguistics.
</reference>
<page confidence="0.998937">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.980452">A Web Application for the Diagnostic Evaluation of Machine over Specific Linguistic Phenomena</title>
<author confidence="0.999717">Antonio Toral Sudip Kumar Naskar Joris Vreeke Federico Gaspari Declan</author>
<affiliation confidence="0.9955295">School of Dublin City</affiliation>
<address confidence="0.575258">Ireland</address>
<email confidence="0.993983">snaskar,fgaspari,joris.vreeke@dcu.ie</email>
<abstract confidence="0.993707444976076">This paper presents a web application and a web service for the diagnostic evaluation of Machine Translation (MT). These web-based tools are built on top of DELiC4MT, an opensource software package that assesses the performance of MT systems over user-defined linguistic phenomena (lexical, morphological, syntactic and semantic). The advantage of the web-based scenario is clear; compared to the standalone tool, the user does not need to carry out any installation, configuration or maintenance of the tool. 1 Automatic Evaluation of Machine Translation beyond Overall Scores Machine translation (MT) output can be evaluated using different approaches, which can essentially be divided into human and automatic, both of which, however, present a number of shortcomings. Human evaluation tends to be more reliable in a number of ways and can be tailored to a variety of situations, but is rather expensive (both in terms of resources and time) and is difficult to replicate. On the other hand, standard automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are considerably cheaper and provide faster results, but return rather crude scores that are difficult to interpret for MT users and developers alike. Crucially, current standard automatic MT evaluation metrics also lack any diagnostic value, i.e. they cannot identify specific weaknesses in the MT output. Diagnostic incan be extremely valuable for MT devel- 20 opers and users, e.g. to improve the performance of the system or to decide which output is more suited for particular scenarios. An interesting alternative to the traditional MT evaluation metrics is to evaluate the performance of MT systems over specific linguistic phenomena. While retaining the main advantage of automatic metrics (low cost), this approach provides more finegrained linguistically-motivated evaluation. The linguistic phenomena, also referred to as linguistic checkpoints, can be defined in terms of linguistic information at different levels (lexical, morphological, syntactic, semantic, etc.) that appear in the source language. Examples of such linguistic checkpoints, what translation information they can represent, and their relevance for MT are provided in Table 1. Checkpoint Relevance for MT Lexical Words that can have multiple translations in the target. For example, the preposition “de” in Spanish can be translated into English as “of” or “from” depending on the context. Syntactic constructs that are difficult to translate. E.g., a checkpoint containing the sea noun followed by the preposition “de”, followed by another noun when translating from Spanish to English. The equivalent English construct be the translation thus involving some reordering. Words with multiple meanings, which possibly correspond to different translations in the target language. Polysemous words can be collected from electronic dictionaries such as WordNet (Miller, 1995). Table 1: Linguistic Checkpoints can also be built by combining elof the NAACL HLT 2013 Demonstration pages Georgia, 10-12 June 2013. Association for Computational Linguistics ements from different categories. For example, by combining lexical and syntantic elements, we could define a checkpoint for prepositional phrases (syntactic element) which start with the preposition “de” (lexical element). Woodpecker (Zhou et al., 2008) is a tool that performs diagnostic evaluation of MT systems over linguistic checkpoints for English–Chinese. Probably due to its limitation to one language pair, its proprietary nature as well as rather restrictive licensing conditions, Woodpecker does not seem to have been widely used in the community, in spite of its ability to support diagnostic evaluation. is an open-source software that follows the same approach as Woodpecker. However, is easily portable to any language and provides additional functionality such as filtering of noisy checkpoint instances and support for statistical significance tests. This paper focuses on the usage of this tool through a web application and a web service from the user’s perspective. Details regarding its implementation, evaluation, etc. can be found in (Toral et al., 2012; Naskar et al., 2011). 2 Web Services for Language Technology Tools There exist many freely available language processing tools, some of which are distributed under open-source licenses. In order to use these tools, they need to be downloaded, installed, configured and maintained, which results in high cost both in terms of manual effort and computing resources. The requirement for in-depth technical knowledge severely limits the usability of these tools amongst non-technical users, particularly in our case amongst translators and post-editors. Web services introduce a new paradigm in the way we use software tools where only providers of the tools are required to have knowledge regarding their installation, configuration and maintenance. This enables wider adoption of the tools and reduces the learning curve for users as the only inforneeded is basic knowledge of the functionaldelic4mt/ has already been tested on language pairs involving the following languages: Arabic, Bulgarian, Dutch, English, French, German, Hindi, Italian, Turkish and Welsh. ity and input/output parameters (which can be easily included, e.g. as part of an online tutorial). While this paradigm is rather new in the field of computational linguistics, it is quite mature and successful in other fields such as bioinformatics (Oinn et al., 2004; Labarga et al., 2007). Related work includes two web applications in the area of MT evaluation. iBLEU (Madnani, 2011) organises BLEU scoring information in a visual manner. Berka et al. (2012) perform automatic error detection and classification of MT output. Figure 1: Web interface for the web service. 3 Demo The demo presented in this paper consists of a web service and a web application built on top of DELiC4MT that allow to assess the performance of systems on different linguistic phenomena de- 21 Figure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service SOAP-compliant web has been built on top of DELiC4MT. It receives the following input parameters (see Figure 1): 1. Word alignment between the source and target sides of the testset, in the GIZA++ (Och and Ney, 2003) output format. 2. Linguistic checkpoint defined as a Ky- (Vossen et al., 2010) profile. 3. Output of the MT system to be evaluated, in plain text, tokenised and one sentence per line. 4. Source and target sides of the testset (or gold standard), in KAF format (Bosma et al., The tool then evaluates the performance of the MT system (input parameter 3) on the linguistic phenomenon (parameter 2) by following this procedure: profiles can be understood as regular expressions KAF documents, kyoto/trunk/modules/mining_module/ XML format for text analysis based on representation standards from ISO TC37/SC4. • Occurrences of the linguistic phenomenon (parameter 2) are identified in the source side of the testset (parameter 4). • The equivalent tokens of these occurrences in the target side (parameter 5) are found by using word alignment information (parameter 1). • For each checkpoint instance, the tool checks many of the present in the reference of the checkpoint instance are contained in the output produced by the MT system (parameter 3). 3.2 Web Application The web application builds a graphical interface on top of the web service. It allows the user to visualise the results in a fine-grained manner, the user can see the performance of the MT system for each single occurrence of the linguistic phenomenon. Sample MT output for the “noun” checkpoint for the English to French language direction is shown in Figure 2. Two occurrences of the checkpoint are shown. The first one regards the source noun “mr.” and its translation in the reference “monsieur”, identified through word alignments. The alignment (4- 4) indicates that both the source and target tokens appear at the fifth position (0-based index) in the sentence. The reference token (“monsieur”) is not in the MT output and thus a score of 22 matches out of a total of 1 possible gram) is assigned to the MT system for this noun instance. Conversely, the score for the second occur- (“speaker”) is since the MT output contains the 1-gram of the reference translation (“orateur”). The recall-based overall score is shown at the botof the figure This is calculated by up the scores (matching for all the occurrences (803) and dividing the result by the number of possible (1598). 4 Conclusions In this paper we have presented a web application and a web service for the diagnostic evaluation of MT output over linguistic phenomena using DELiC4MT. The tool allows users and developers of MT systems to easily receive fine-grained feedback on the performance of their MT systems over linguistic checkpoints of their interest. The application is open-source, freely available and adaptable to any language pair.</abstract>
<note confidence="0.735430642857143">Acknowledgments The research leading to these results has received funding from the European Union Seventh Framework Programme FP7/2007-2013 under grant agreements FP7-ICT-4-248531 and PIAP-GA- 2012-324414 and through Science Foundation Ireland as part of the CNGL (grant 07/CE/I1142) References Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Im- Correlation with Human Judgments. In Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Proceedings of the pages 65–72, University of Michi-</note>
<address confidence="0.951866">gan, Ann Arbor, Michigan, USA.</address>
<author confidence="0.922113">Automatic MT Error Anal-</author>
<title confidence="0.671467">Hjerson Helping Addicter. In of the Eight International Conference on Language Reand Evaluation Istanbul, Turkey. European Language Resources Association (ELRA).</title>
<author confidence="0.9770455">W E Bosma</author>
<author confidence="0.9770455">Piek Vossen</author>
<author confidence="0.9770455">Aitor Soroa</author>
<author confidence="0.9770455">German Rigau</author>
<author confidence="0.9770455">Maurizio Tesconi</author>
<author confidence="0.9770455">Andrea Marchetti</author>
<author confidence="0.9770455">Monica Mona-</author>
<abstract confidence="0.517263260869565">chini, and Carlo Aliprandi. 2009. KAF: a generic annotation format. In of the Workshop on Semantic September. Alberto Labarga, Franck Valentin, Mikael Andersson, and Rodrigo Lopez. 2007. Web services at the eurobioinformatics institute. Acids 35(Web-Server-Issue):6–11. Nitin Madnani. 2011. iBLEU: Interactively Debugging and Scoring Statistical Machine Translation Systems. of the 2011 IEEE Fifth International on Semantic ICSC ’11, pages 213–214, Washington, DC, USA. IEEE Computer Society. George A. Miller. 1995. WordNet: a lexical database for 38(11):39–41, November. Sudip Kumar Naskar, Antonio Toral, Federico Gaspari, and Andy Way. 2011. A Framework for Diagnostic Evaluation of MT based on Linguistic Checkpoints. In of the 13th Machine Translation pages 529–536, Xiamen, China, September. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment mod-</abstract>
<address confidence="0.627243">29:19–51, March.</address>
<author confidence="0.8722715">Tom Oinn</author>
<author confidence="0.8722715">Matthew Addis</author>
<author confidence="0.8722715">Justin Ferris</author>
<author confidence="0.8722715">Darren Marvin</author>
<author confidence="0.8722715">Martin Senger</author>
<author confidence="0.8722715">Mark Greenwood</author>
<author confidence="0.8722715">Tim Carver</author>
<author confidence="0.8722715">Kevin</author>
<note confidence="0.8538649">Glover, Matthew R. Pocock, Anil Wipat, and Peter Li. 2004. Taverna: a tool for the composition and enof bioinformatics workflows. 20(17):3045–3054, November. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalof machine translation. In of the 40th Annual Meeting on Association for Computa- ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Lin-</note>
<email confidence="0.772885">guistics.</email>
<author confidence="0.947529">Antonio Toral</author>
<author confidence="0.947529">Sudip Kumar Naskar</author>
<author confidence="0.947529">Federico Gaspari</author>
<title confidence="0.3590605">and Declan Groves. 2012. DELiC4MT: A Tool for Diagnostic MT Evaluation over User-defined Linguis-</title>
<note confidence="0.889250625">Phenomena. Prague Bulletin of Mathematical pages 121–132. Piek Vossen, German Rigau, Eneko Agirre, Aitor Soroa, Monica Monachini, and Roberto Bartolini. 2010. KYan open platform for mining facts. In Proceedings of the 6th Workshop on Ontologies and Lexical pages 1–10, Beijing, China. Ming Zhou, Bo Wang, Shujie Liu, Mu Li, Dongdong Zhang, and Tiejun Zhao. 2008. Diagnostic evaluation of machine translation systems using automaticonstructed linguistic check-points. In Proceedings of the 22nd International Conference on Compu- Linguistics - Volume COLING ’08, pages 1121–1128, Stroudsburg, PA, USA. Association for Computational Linguistics. 23</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Proceedings of the ACL-05 Workshop,</booktitle>
<pages>65--72</pages>
<institution>University of Michigan,</institution>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="1427" citStr="Banerjee and Lavie, 2005" startWordPosition="213" endWordPosition="216">maintenance of the tool. 1 Automatic Evaluation of Machine Translation beyond Overall Scores Machine translation (MT) output can be evaluated using different approaches, which can essentially be divided into human and automatic, both of which, however, present a number of shortcomings. Human evaluation tends to be more reliable in a number of ways and can be tailored to a variety of situations, but is rather expensive (both in terms of resources and time) and is difficult to replicate. On the other hand, standard automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are considerably cheaper and provide faster results, but return rather crude scores that are difficult to interpret for MT users and developers alike. Crucially, current standard automatic MT evaluation metrics also lack any diagnostic value, i.e. they cannot identify specific weaknesses in the MT output. Diagnostic information can be extremely valuable for MT devel20 opers and users, e.g. to improve the performance of the system or to decide which output is more suited for particular scenarios. An interesting alternative to the traditional MT evaluation metrics is to evaluate the performance</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Proceedings of the ACL-05 Workshop, pages 65–72, University of Michigan, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Berka</author>
<author>Ondej Bojar</author>
<author>Mark Fishel</author>
<author>Maja Popovi</author>
<author>Daniel Zeman</author>
</authors>
<title>Automatic MT Error Analysis: Hjerson Helping Addicter.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul,</location>
<contexts>
<context position="6260" citStr="Berka et al. (2012)" startWordPosition="941" endWordPosition="944">ready been tested on language pairs involving the following languages: Arabic, Bulgarian, Dutch, English, French, German, Hindi, Italian, Turkish and Welsh. ity and input/output parameters (which can be easily included, e.g. as part of an online tutorial). While this paradigm is rather new in the field of computational linguistics, it is quite mature and successful in other fields such as bioinformatics (Oinn et al., 2004; Labarga et al., 2007). Related work includes two web applications in the area of MT evaluation. iBLEU (Madnani, 2011) organises BLEU scoring information in a visual manner. Berka et al. (2012) perform automatic error detection and classification of MT output. Figure 1: Web interface for the web service. 3 Demo The demo presented in this paper consists of a web service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena de21 Figure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service A SOAP-compliant web service3 has been built on top of DELiC4MT. It receives the following input parameters (see Figure </context>
</contexts>
<marker>Berka, Bojar, Fishel, Popovi, Zeman, 2012</marker>
<rawString>Jan Berka, Ondej Bojar, Mark Fishel, Maja Popovi, and Daniel Zeman. 2012. Automatic MT Error Analysis: Hjerson Helping Addicter. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W E Bosma</author>
<author>Piek Vossen</author>
<author>Aitor Soroa</author>
<author>German Rigau</author>
<author>Maurizio Tesconi</author>
<author>Andrea Marchetti</author>
<author>Monica Monachini</author>
<author>Carlo Aliprandi</author>
</authors>
<title>KAF: a generic semantic annotation format.</title>
<date>2009</date>
<booktitle>In Proceedings of the GL2009 Workshop on Semantic Annotation,</booktitle>
<contexts>
<context position="7251" citStr="Bosma et al., 2009" startWordPosition="1112" endWordPosition="1115">on of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service A SOAP-compliant web service3 has been built on top of DELiC4MT. It receives the following input parameters (see Figure 1): 1. Word alignment between the source and target sides of the testset, in the GIZA++ (Och and Ney, 2003) output format. 2. Linguistic checkpoint defined as a Kybot4 (Vossen et al., 2010) profile. 3. Output of the MT system to be evaluated, in plain text, tokenised and one sentence per line. 4. Source and target sides of the testset (or gold standard), in KAF format (Bosma et al., 2009).5 The tool then evaluates the performance of the MT system (input parameter 3) on the linguistic phenomenon (parameter 2) by following this procedure: 3http://registry.elda.org/services/301 4Kybot profiles can be understood as regular expressions over KAF documents, http://kyoto.let.vu.nl/svn/ kyoto/trunk/modules/mining_module/ 5An XML format for text analysis based on representation standards from ISO TC37/SC4. • Occurrences of the linguistic phenomenon (parameter 2) are identified in the source side of the testset (parameter 4). • The equivalent tokens of these occurrences in the target sid</context>
</contexts>
<marker>Bosma, Vossen, Soroa, Rigau, Tesconi, Marchetti, Monachini, Aliprandi, 2009</marker>
<rawString>W. E. Bosma, Piek Vossen, Aitor Soroa, German Rigau, Maurizio Tesconi, Andrea Marchetti, Monica Monachini, and Carlo Aliprandi. 2009. KAF: a generic semantic annotation format. In Proceedings of the GL2009 Workshop on Semantic Annotation, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Labarga</author>
<author>Franck Valentin</author>
<author>Mikael Andersson</author>
<author>Rodrigo Lopez</author>
</authors>
<title>Web services at the european bioinformatics institute.</title>
<date>2007</date>
<journal>Nucleic Acids Research,</journal>
<pages>35--6</pages>
<contexts>
<context position="6089" citStr="Labarga et al., 2007" startWordPosition="912" endWordPosition="915"> tools and reduces the learning curve for users as the only information needed is basic knowledge of the functional1http://www.computing.dcu.ie/˜atoral/ delic4mt/ 2It has already been tested on language pairs involving the following languages: Arabic, Bulgarian, Dutch, English, French, German, Hindi, Italian, Turkish and Welsh. ity and input/output parameters (which can be easily included, e.g. as part of an online tutorial). While this paradigm is rather new in the field of computational linguistics, it is quite mature and successful in other fields such as bioinformatics (Oinn et al., 2004; Labarga et al., 2007). Related work includes two web applications in the area of MT evaluation. iBLEU (Madnani, 2011) organises BLEU scoring information in a visual manner. Berka et al. (2012) perform automatic error detection and classification of MT output. Figure 1: Web interface for the web service. 3 Demo The demo presented in this paper consists of a web service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena de21 Figure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsecti</context>
</contexts>
<marker>Labarga, Valentin, Andersson, Lopez, 2007</marker>
<rawString>Alberto Labarga, Franck Valentin, Mikael Andersson, and Rodrigo Lopez. 2007. Web services at the european bioinformatics institute. Nucleic Acids Research, 35(Web-Server-Issue):6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
</authors>
<title>iBLEU: Interactively Debugging and Scoring Statistical Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE Fifth International Conference on Semantic Computing, ICSC ’11,</booktitle>
<pages>213--214</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="6185" citStr="Madnani, 2011" startWordPosition="929" endWordPosition="930">e functional1http://www.computing.dcu.ie/˜atoral/ delic4mt/ 2It has already been tested on language pairs involving the following languages: Arabic, Bulgarian, Dutch, English, French, German, Hindi, Italian, Turkish and Welsh. ity and input/output parameters (which can be easily included, e.g. as part of an online tutorial). While this paradigm is rather new in the field of computational linguistics, it is quite mature and successful in other fields such as bioinformatics (Oinn et al., 2004; Labarga et al., 2007). Related work includes two web applications in the area of MT evaluation. iBLEU (Madnani, 2011) organises BLEU scoring information in a visual manner. Berka et al. (2012) perform automatic error detection and classification of MT output. Figure 1: Web interface for the web service. 3 Demo The demo presented in this paper consists of a web service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena de21 Figure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service A SOAP-compliant web service3 has been built </context>
</contexts>
<marker>Madnani, 2011</marker>
<rawString>Nitin Madnani. 2011. iBLEU: Interactively Debugging and Scoring Statistical Machine Translation Systems. In Proceedings of the 2011 IEEE Fifth International Conference on Semantic Computing, ICSC ’11, pages 213–214, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="3350" citStr="Miller, 1995" startWordPosition="500" endWordPosition="501"> translated into English as “of” or “from” depending on the context. Syntactic Syntactic constructs that are difficult to translate. E.g., a checkpoint containing the sequence a noun (noun]) followed by the preposition “de”, followed by another noun (noun2) when translating from Spanish to English. The equivalent English construct would be noun2’s noun], the translation thus involving some reordering. Semantic Words with multiple meanings, which possibly correspond to different translations in the target language. Polysemous words can be collected from electronic dictionaries such as WordNet (Miller, 1995). Table 1: Linguistic Checkpoints Checkpoints can also be built by combining elProceedings of the NAACL HLT 2013 Demonstration Session, pages 20–23, Atlanta, Georgia, 10-12 June 2013. c�2013 Association for Computational Linguistics ements from different categories. For example, by combining lexical and syntantic elements, we could define a checkpoint for prepositional phrases (syntactic element) which start with the preposition “de” (lexical element). Woodpecker (Zhou et al., 2008) is a tool that performs diagnostic evaluation of MT systems over linguistic checkpoints for English–Chinese. Pro</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Commun. ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudip Kumar Naskar</author>
<author>Antonio Toral</author>
<author>Federico Gaspari</author>
<author>Andy Way</author>
</authors>
<title>A Framework for Diagnostic Evaluation of MT based on Linguistic Checkpoints.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th Machine Translation Summit,</booktitle>
<pages>529--536</pages>
<location>Xiamen, China,</location>
<contexts>
<context position="4707" citStr="Naskar et al., 2011" startWordPosition="703" endWordPosition="706">oes not seem to have been widely used in the community, in spite of its ability to support diagnostic evaluation. DELiC4MT1 is an open-source software that follows the same approach as Woodpecker. However, DELiC4MT is easily portable to any language pair2 and provides additional functionality such as filtering of noisy checkpoint instances and support for statistical significance tests. This paper focuses on the usage of this tool through a web application and a web service from the user’s perspective. Details regarding its implementation, evaluation, etc. can be found in (Toral et al., 2012; Naskar et al., 2011). 2 Web Services for Language Technology Tools There exist many freely available language processing tools, some of which are distributed under open-source licenses. In order to use these tools, they need to be downloaded, installed, configured and maintained, which results in high cost both in terms of manual effort and computing resources. The requirement for in-depth technical knowledge severely limits the usability of these tools amongst non-technical users, particularly in our case amongst translators and post-editors. Web services introduce a new paradigm in the way we use software tools</context>
</contexts>
<marker>Naskar, Toral, Gaspari, Way, 2011</marker>
<rawString>Sudip Kumar Naskar, Antonio Toral, Federico Gaspari, and Andy Way. 2011. A Framework for Diagnostic Evaluation of MT based on Linguistic Checkpoints. In Proceedings of the 13th Machine Translation Summit, pages 529–536, Xiamen, China, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="6967" citStr="Och and Ney, 2003" startWordPosition="1061" endWordPosition="1064">ce for the web service. 3 Demo The demo presented in this paper consists of a web service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena de21 Figure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service A SOAP-compliant web service3 has been built on top of DELiC4MT. It receives the following input parameters (see Figure 1): 1. Word alignment between the source and target sides of the testset, in the GIZA++ (Och and Ney, 2003) output format. 2. Linguistic checkpoint defined as a Kybot4 (Vossen et al., 2010) profile. 3. Output of the MT system to be evaluated, in plain text, tokenised and one sentence per line. 4. Source and target sides of the testset (or gold standard), in KAF format (Bosma et al., 2009).5 The tool then evaluates the performance of the MT system (input parameter 3) on the linguistic phenomenon (parameter 2) by following this procedure: 3http://registry.elda.org/services/301 4Kybot profiles can be understood as regular expressions over KAF documents, http://kyoto.let.vu.nl/svn/ kyoto/trunk/modules/</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Oinn</author>
<author>Matthew Addis</author>
<author>Justin Ferris</author>
<author>Darren Marvin</author>
<author>Martin Senger</author>
<author>Mark Greenwood</author>
<author>Tim Carver</author>
<author>Kevin Glover</author>
<author>Matthew R Pocock</author>
<author>Anil Wipat</author>
<author>Peter Li</author>
</authors>
<title>Taverna: a tool for the composition and enactment of bioinformatics workflows.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>17</issue>
<contexts>
<context position="6066" citStr="Oinn et al., 2004" startWordPosition="908" endWordPosition="911">der adoption of the tools and reduces the learning curve for users as the only information needed is basic knowledge of the functional1http://www.computing.dcu.ie/˜atoral/ delic4mt/ 2It has already been tested on language pairs involving the following languages: Arabic, Bulgarian, Dutch, English, French, German, Hindi, Italian, Turkish and Welsh. ity and input/output parameters (which can be easily included, e.g. as part of an online tutorial). While this paradigm is rather new in the field of computational linguistics, it is quite mature and successful in other fields such as bioinformatics (Oinn et al., 2004; Labarga et al., 2007). Related work includes two web applications in the area of MT evaluation. iBLEU (Madnani, 2011) organises BLEU scoring information in a visual manner. Berka et al. (2012) perform automatic error detection and classification of MT output. Figure 1: Web interface for the web service. 3 Demo The demo presented in this paper consists of a web service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena de21 Figure 2: Screenshot of the web application (visualisation of results). fined by the user.</context>
</contexts>
<marker>Oinn, Addis, Ferris, Marvin, Senger, Greenwood, Carver, Glover, Pocock, Wipat, Li, 2004</marker>
<rawString>Tom Oinn, Matthew Addis, Justin Ferris, Darren Marvin, Martin Senger, Mark Greenwood, Tim Carver, Kevin Glover, Matthew R. Pocock, Anil Wipat, and Peter Li. 2004. Taverna: a tool for the composition and enactment of bioinformatics workflows. Bioinformatics, 20(17):3045–3054, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1389" citStr="Papineni et al., 2002" startWordPosition="207" endWordPosition="210">any installation, configuration or maintenance of the tool. 1 Automatic Evaluation of Machine Translation beyond Overall Scores Machine translation (MT) output can be evaluated using different approaches, which can essentially be divided into human and automatic, both of which, however, present a number of shortcomings. Human evaluation tends to be more reliable in a number of ways and can be tailored to a variety of situations, but is rather expensive (both in terms of resources and time) and is difficult to replicate. On the other hand, standard automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are considerably cheaper and provide faster results, but return rather crude scores that are difficult to interpret for MT users and developers alike. Crucially, current standard automatic MT evaluation metrics also lack any diagnostic value, i.e. they cannot identify specific weaknesses in the MT output. Diagnostic information can be extremely valuable for MT devel20 opers and users, e.g. to improve the performance of the system or to decide which output is more suited for particular scenarios. An interesting alternative to the traditional MT evaluation </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Toral</author>
<author>Sudip Kumar Naskar</author>
<author>Federico Gaspari</author>
<author>Declan Groves</author>
</authors>
<title>DELiC4MT: A Tool for Diagnostic MT Evaluation over User-defined Linguistic Phenomena. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2012</date>
<pages>121--132</pages>
<contexts>
<context position="4685" citStr="Toral et al., 2012" startWordPosition="699" endWordPosition="702">itions, Woodpecker does not seem to have been widely used in the community, in spite of its ability to support diagnostic evaluation. DELiC4MT1 is an open-source software that follows the same approach as Woodpecker. However, DELiC4MT is easily portable to any language pair2 and provides additional functionality such as filtering of noisy checkpoint instances and support for statistical significance tests. This paper focuses on the usage of this tool through a web application and a web service from the user’s perspective. Details regarding its implementation, evaluation, etc. can be found in (Toral et al., 2012; Naskar et al., 2011). 2 Web Services for Language Technology Tools There exist many freely available language processing tools, some of which are distributed under open-source licenses. In order to use these tools, they need to be downloaded, installed, configured and maintained, which results in high cost both in terms of manual effort and computing resources. The requirement for in-depth technical knowledge severely limits the usability of these tools amongst non-technical users, particularly in our case amongst translators and post-editors. Web services introduce a new paradigm in the way</context>
</contexts>
<marker>Toral, Naskar, Gaspari, Groves, 2012</marker>
<rawString>Antonio Toral, Sudip Kumar Naskar, Federico Gaspari, and Declan Groves. 2012. DELiC4MT: A Tool for Diagnostic MT Evaluation over User-defined Linguistic Phenomena. The Prague Bulletin of Mathematical Linguistics, pages 121–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piek Vossen</author>
<author>German Rigau</author>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
<author>Monica Monachini</author>
<author>Roberto Bartolini</author>
</authors>
<title>KYOTO: an open platform for mining facts.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th Workshop on Ontologies and Lexical Resources,</booktitle>
<pages>1--10</pages>
<location>Beijing, China.</location>
<contexts>
<context position="7049" citStr="Vossen et al., 2010" startWordPosition="1075" endWordPosition="1078">b service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena de21 Figure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service A SOAP-compliant web service3 has been built on top of DELiC4MT. It receives the following input parameters (see Figure 1): 1. Word alignment between the source and target sides of the testset, in the GIZA++ (Och and Ney, 2003) output format. 2. Linguistic checkpoint defined as a Kybot4 (Vossen et al., 2010) profile. 3. Output of the MT system to be evaluated, in plain text, tokenised and one sentence per line. 4. Source and target sides of the testset (or gold standard), in KAF format (Bosma et al., 2009).5 The tool then evaluates the performance of the MT system (input parameter 3) on the linguistic phenomenon (parameter 2) by following this procedure: 3http://registry.elda.org/services/301 4Kybot profiles can be understood as regular expressions over KAF documents, http://kyoto.let.vu.nl/svn/ kyoto/trunk/modules/mining_module/ 5An XML format for text analysis based on representation standards </context>
</contexts>
<marker>Vossen, Rigau, Agirre, Soroa, Monachini, Bartolini, 2010</marker>
<rawString>Piek Vossen, German Rigau, Eneko Agirre, Aitor Soroa, Monica Monachini, and Roberto Bartolini. 2010. KYOTO: an open platform for mining facts. In Proceedings of the 6th Workshop on Ontologies and Lexical Resources, pages 1–10, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Zhou</author>
<author>Bo Wang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Dongdong Zhang</author>
<author>Tiejun Zhao</author>
</authors>
<title>Diagnostic evaluation of machine translation systems using automatically constructed linguistic check-points.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>1121--1128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3837" citStr="Zhou et al., 2008" startWordPosition="566" endWordPosition="569"> translations in the target language. Polysemous words can be collected from electronic dictionaries such as WordNet (Miller, 1995). Table 1: Linguistic Checkpoints Checkpoints can also be built by combining elProceedings of the NAACL HLT 2013 Demonstration Session, pages 20–23, Atlanta, Georgia, 10-12 June 2013. c�2013 Association for Computational Linguistics ements from different categories. For example, by combining lexical and syntantic elements, we could define a checkpoint for prepositional phrases (syntactic element) which start with the preposition “de” (lexical element). Woodpecker (Zhou et al., 2008) is a tool that performs diagnostic evaluation of MT systems over linguistic checkpoints for English–Chinese. Probably due to its limitation to one language pair, its proprietary nature as well as rather restrictive licensing conditions, Woodpecker does not seem to have been widely used in the community, in spite of its ability to support diagnostic evaluation. DELiC4MT1 is an open-source software that follows the same approach as Woodpecker. However, DELiC4MT is easily portable to any language pair2 and provides additional functionality such as filtering of noisy checkpoint instances and supp</context>
</contexts>
<marker>Zhou, Wang, Liu, Li, Zhang, Zhao, 2008</marker>
<rawString>Ming Zhou, Bo Wang, Shujie Liu, Mu Li, Dongdong Zhang, and Tiejun Zhao. 2008. Diagnostic evaluation of machine translation systems using automatically constructed linguistic check-points. In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 1121–1128, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>