<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000550">
<title confidence="0.9992515">
Knowledge-Based Weak Supervision for Information Extraction
of Overlapping Relations
</title>
<author confidence="0.999424">
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S. Weld
</author>
<affiliation confidence="0.997003">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.757834">
Seattle, WA 98195, USA
</address>
<email confidence="0.999367">
{raphaelh,clzhang,xiaoling,lsz,weld}@cs.washington.edu
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999755461538461">
Information extraction (IE) holds the promise
of generating a large-scale knowledge
base from the Web‚Äôs natural language text.
Knowledge-based weak supervision, using
structured data to heuristically label a training
corpus, works towards this goal by enabling
the automated learning of a potentially
unbounded number of relation extractors.
Recently, researchers have developed multi-
instance learning algorithms to combat the
noisy training data that can come from
heuristic labeling, but their models assume
relations are disjoint ‚Äî for example they
cannot extract the pair Founded(Jobs,
Apple) and CEO-of(Jobs, Apple).
This paper presents a novel approach for
multi-instance learning with overlapping re-
lations that combines a sentence-level extrac-
tion model with a simple, corpus-level compo-
nent for aggregating the individual facts. We
apply our model to learn extractors for NY
Times text using weak supervision from Free-
base. Experiments show that the approach
runs quickly and yields surprising gains in
accuracy, at both the aggregate and sentence
level.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974906976744">
Information-extraction (IE), the process of generat-
ing relational data from natural-language text, con-
tinues to gain attention. Many researchers dream of
creating a large repository of high-quality extracted
tuples, arguing that such a knowledge base could
benefit many important tasks such as question an-
swering and summarization. Most approaches to IE
use supervised learning of relation-specific exam-
ples, which can achieve high precision and recall.
Unfortunately, however, fully supervised methods
are limited by the availability of training data and are
unlikely to scale to the thousands of relations found
on the Web.
A more promising approach, often called ‚Äúweak‚Äù
or ‚Äúdistant‚Äù supervision, creates its own training
data by heuristically matching the contents of a
database to corresponding text (Craven and Kum-
lien, 1999). For example, suppose that r(e1, e2) =
Founded(Jobs,Apple) is a ground tuple in the
database and s =‚ÄúSteve Jobs founded Apple, Inc.‚Äù
is a sentence containing synonyms for both e1 =
Jobs and e2 = Apple, then s may be a natural
language expression of the fact that r(e1, e2) holds
and could be a useful training example.
While weak supervision works well when the tex-
tual corpus is tightly aligned to the database con-
tents (e.g., matching Wikipedia infoboxes to as-
sociated articles (Hoffmann et al., 2010)), Riedel
et al. (2010) observe that the heuristic leads to
noisy data and poor extraction performance when
the method is applied more broadly (e.g., matching
Freebase records to NY Times articles). To fix
this problem they cast weak supervision as a form of
multi-instance learning, assuming only that at least
one of the sentences containing e1 and e2 are ex-
pressing r(e1, e2), and their method yields a sub-
stantial improvement in extraction performance.
However, Riedel et al.‚Äôs model (like that of
previous systems (Mintz et al., 2009)) assumes
that relations do not overlap ‚Äî there cannot
exist two facts r(e1, e2) and q(e1, e2) that are
both true for any pair of entities, e1 and e2.
Unfortunately, this assumption is often violated;
</bodyText>
<page confidence="0.967536">
541
</page>
<note confidence="0.9795125">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 541‚Äì550,
Portland, Oregon, June 19-24, 2011. cÔøΩ2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.997246">
for example both Founded(Jobs, Apple) and
CEO-of(Jobs, Apple) are clearly true. In-
deed, 18.3% of the weak supervision facts in Free-
base that match sentences in the NY Times 2007 cor-
pus have overlapping relations.
This paper presents MULTIR, a novel model of
weak supervision that makes the following contri-
butions:
</bodyText>
<listItem confidence="0.97967065">
‚Ä¢ MULTIR introduces a probabilistic, graphical
model of multi-instance learning which handles
overlapping relations.
‚Ä¢ MULTIR also produces accurate sentence-level
predictions, decoding individual sentences as
well as making corpus-level extractions.
‚Ä¢ MULTIR is computationally tractable. Inference
reduces to weighted set cover, for which it uses
a greedy approximation with worst case running
time O(|R |¬∑ |S|) where R is the set of possi-
ble relations and S is largest set of sentences for
any entity pair. In practice, MULTIR runs very
quickly.
‚Ä¢ We present experiments showing that MULTIR
outperforms a reimplementation of Riedel
et al. (2010)‚Äôs approach on both aggregate (cor-
pus as a whole) and sentential extractions.
Additional experiments characterize aspects of
MULTIR‚Äôs performance.
2 Weak Supervision from a Database
</listItem>
<bodyText confidence="0.999890159090909">
Given a corpus of text, we seek to extract facts about
entities, such as the company Apple or the city
Boston. A ground fact (or relation instance), is
an expression r(e) where r is a relation name, for
example Founded or CEO-of, and e = e1, ... , e,,,
is a list of entities.
An entity mention is a contiguous sequence of tex-
tual tokens denoting an entity. In this paper we as-
sume that there is an oracle which can identify all
entity mentions in a corpus, but the oracle doesn‚Äôt
normalize or disambiguate these mentions. We use
eZ E E to denote both an entity and its name (i.e.,
the tokens in its mention).
A relation mention is a sequence of text (in-
cluding one or more entity mentions) which states
that some ground fact r(e) is true. For example,
‚ÄúSteve Ballmer, CEO of Microsoft, spoke recently
at CES.‚Äù contains three entity mentions as well as a
relation mention for CEO-of(Steve Ballmer,
Microsoft). In this paper we restrict our atten-
tion to binary relations. Furthermore, we assume
that both entity mentions appear as noun phrases in
a single sentence.
The task of aggregate extraction takes two inputs,
E, a set of sentences comprising the corpus, and an
extraction model; as output it should produce a set
of ground facts, I, such that each fact r(e) E I is
expressed somewhere in the corpus.
Sentential extraction takes the same input and
likewise produces I, but in addition it also produces
a function, F : I ‚Äî* P(E), which identifies, for
each r(e) E I, the set of sentences in E that contain
a mention describing r(e). In general, the corpus-
level extraction problem is easier, since it need only
make aggregate predictions, perhaps using corpus-
wide statistics. In contrast, sentence-level extrac-
tion must justify each extraction with every sentence
which expresses the fact.
The knowledge-based weakly supervised learning
problem takes as input (1) E, a training corpus, (2)
E, a set of entities mentioned in that corpus, (3) R,
a set of relation names, and (4), A, a set of ground
facts of relations in R. As output the learner pro-
duces an extraction model.
</bodyText>
<sectionHeader confidence="0.955402" genericHeader="method">
3 Modeling Overlapping Relations
</sectionHeader>
<bodyText confidence="0.99997575">
We define an undirected graphical model that al-
lows joint reasoning about aggregate (corpus-level)
and sentence-level extraction decisions. Figure 1(a)
shows the model in plate form.
</bodyText>
<subsectionHeader confidence="0.999054">
3.1 Random Variables
</subsectionHeader>
<bodyText confidence="0.987297916666667">
There exists a connected component for each pair of
entities e = (e1, e2) E E x E that models all of
the extraction decisions for this pair. There is one
Boolean output variable Y &apos; for each relation name
r E R, which represents whether the ground fact
r(e) is true. Including this set of binary random
variables enables our model to extract overlapping
relations.
Let S(,,,,,) C E be the set of sentences which
contain mentions of both of the entities. For each
sentence xZ E S(,,,,,) there exists a latent variable
ZZ which ranges over the relation names r E R and,
</bodyText>
<page confidence="0.991661">
542
</page>
<figure confidence="0.99916164">
Steve Jobs was founder
of Apple.
Steve Jobs is CEO of
Apple.
Steve Jobs, Steve Wozniak and
Ronald Wayne founded Apple.
...
YbornIn
0
YfounderOf YlocatedIn YcapitalOf
...
...
founder
founder
none
Z1
Z2 Z3
(b)
1 0 0
E √ó E
S
R
ùëçùëñ
(a)
ùëå
</figure>
<figureCaption confidence="0.988719">
Figure 1: (a) Network structure depicted as plate model and (b) an example network instantiation for the pair of entities
Steve Jobs,Apple.
</figureCaption>
<bodyText confidence="0.976753857142857">
importantly, also the distinct value none. Zi should tence level assignments Zi = zi signals a mention
be assigned a value r E R only when xi expresses of r(e).
the ground fact r(e), thereby modeling sentence- The extraction factors 4jextract are given by
level extraction. ÔøΩ
Figure 1(b) shows an example instantiation of the ÔøΩÔøΩÔøΩextract(zi, xi) def = exp Œ∏jœÜj(zi, xi)
model with four relation names and three sentences.
j
</bodyText>
<subsectionHeader confidence="0.999352">
3.2 A Joint, Conditional Extraction Model
</subsectionHeader>
<bodyText confidence="0.998626043478261">
We use a conditional probability model that defines
a joint distribution over all of the extraction random
variables defined above. The model is undirected
and includes repeated factors for making sentence
level predictions as well as globals factors for ag-
gregating these choices.
For each entity pair e = (e1, e2), define x to
be a vector concatenating the individual sentences
xi E S(e1,e2), Y to be vector of binary Yr random
variables, one for each r E R, and Z to be the vec-
tor of Zi variables, one for each sentence xi. Our
conditional extraction model is defined as follows:
where the parameter vector Œ∏ is used, below, to de-
fine the factor cfiextract.
The factors Voin are deterministic OR operators
which are included to ensure that the ground fact
r(e) is predicted at the aggregate level for the as-
signment Yr = yr only if at least one of the sen-
where the features œÜj are sensitive to the relation
name assigned to extraction variable zi, if any, and
cues from the sentence xi. We will make use of the
Mintz et al. (2009) sentence-level features in the ex-
peiments, as described in Section 7.
</bodyText>
<subsectionHeader confidence="0.992004">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99997545">
This model was designed to provide a joint approach
where extraction decisions are almost entirely driven
by sentence-level reasoning. However, defining the
Yr random variables and tying them to the sentence-
level variables, Zi, provides a direct method for
modeling weak supervision. We can simply train the
model so that the Y variables match the facts in the
database, treating the Zi as hidden variables that can
take any value, as long as they produce the correct
aggregate predictions.
This approach is related to the multi-instance
learning approach of Riedel et al. (2010), in that
both models include sentence-level and aggregate
random variables. However, their sentence level
variables are binary and they only have a single ag-
gregate variable that takes values r E R U {none},
thereby ruling out overlapping relations. Addition-
ally, their aggregate decisions make use of Mintz-
style aggregate features (Mintz et al., 2009), that col-
lect evidence from multiple sentences, while we use
</bodyText>
<equation confidence="0.991460888888889">
p(Y = y, Z = z|x; Œ∏) def =
1 ÔøΩ
Zx 11 r
-jextract(zi, xi)
ÔøΩ4join(yr, z)
i
oin r def f 1 if yr = true n ]i : zi = r
q&apos;j (y,z)=
0 otherwise
</equation>
<page confidence="0.980901">
543
</page>
<listItem confidence="0.9847">
Inputs: Finally, we can now define the training set to be
(1) E, a set of sentences, pairs {(xi, yi)|i = 1... n}, where i is an index
(2) E, a set of entities mentioned in the sentences, corresponding to a particular entity pair (ej, ek), xi
(3) R, a set of relation names, and contains all of the sentences with mentions of this
(4) A, a database of atomic facts of the form pair, and yi = relVector(ej, ek).
</listItem>
<bodyText confidence="0.863768095238095">
r(e1, e2) for r E R and ei E E. Given this form of supervision, we would like to
Definitions: find the setting for 0 with the highest likelihood:
We define the training set {(xi, yi)|i = 1... n}, O(0) = rl p(yi|xi; 0) = rl E p(yi, z|xi; 0)
where i is an index corresponding to a particu- i i z
lar entity pair (ej, ek) in A, xi contains all of However, this objective would be difficult to op-
the sentences in E with mentions of this pair, and timize exactly, and algorithms for doing so would
yi = relVector(ej, ek). be unlikely to scale to data sets of the size we con-
Computation: sider. Instead, we make two approximations, de-
initialize parameter vector O &lt;-- 0 scribed below, leading to a Perceptron-style addi-
fort = 1...T do tive (Collins, 2002) parameter update scheme which
for i = 1...n do has been modified to reason about hidden variables,
(y&apos;, z&apos;) &lt;-- arg maxy,z p(y, z|xi; 0) similar in style to the approaches of (Liang et al.,
if y&apos; =ÔøΩ yi then 2006; Zettlemoyer and Collins, 2007), but adapted
z* &lt;-- arg maxz for our specific model. This approximate algorithm
p(z|xi, yi; 0) is computationally efficient and, as we will see,
O &lt;-- O + O(xi, z*) ‚Äî O(xi, z&apos;) works well in practice.
end if Our first modification is to do online learning
end for instead of optimizing the full objective. Define the
end for feature sums O(x, z) = Ej O(xj, zj) which range
Return O over the sentences, as indexed by j. Now, we can
Figure 2: The MULTIR Learning Algorithm define an update based on the gradient of the local
only the deterministic OR nodes. Perhaps surpris- log likelihood for example i:
ing, we are still able to improve performance at both ‚àÇ log Oi(Œ∏)
the sentential and aggregate extraction tasks. = Ep(z|xi,yi;Œ∏)[Oj(xi,z)]
4 Learning ‚àÇŒ∏ÔøΩ
We now present a multi-instance learning algo- ‚ÄîEp(y,z|xi;Œ∏)[Oj(xi, z)]
rithm for our weak-supervision model that treats the where the deterministic OR 4oin factors ensure that
sentence-level extraction random variables Zi as la- the first expectation assigns positive probability only
tent, and uses facts from a database (e.g., Freebase) to assignments that produce the labeled facts yi but
as supervision for the aggregate-level variables Y&apos;. that the second considers all valid sets of extractions.
As input we have (1) E, a set of sentences, (2) Of course, these expectations themselves, espe-
E, a set of entities mentioned in the sentences, (3) cially the second one, would be difficult to com-
R, a set of relation names, and (4) A, a database pute exactly. Our second modification is to do
of atomic facts of the form r(e1, e2) for r E R and a Viterbi approximation, by replacing the expecta-
ei E E. Since we are using weak learning, the Y&apos; tions with maximizations. Specifically, we compute
variables in Y are not directly observed, but can be the most likely sentence extractions for the label
approximated from the database A. We use a proce- facts arg maxz p(z|xi, yi; 0) and the most likely ex-
dure, relVector(e1, e2) to return a bit vector whose traction for the input, without regard to the labels,
jth bit is one if rj(e1, e2) E A. The vector does not arg maxy,z p(y, z|xi; 0). We then compute the fea-
have a bit for the special none relation; if there is no tures for these assignments and do a simple additive
relation between the two entities, all bits are zero. update. The final algorithm is detailed in Figure 2.
544
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="method">
5 Inference
</sectionHeader>
<bodyText confidence="0.972622347826087">
To support learning, as described above, we need
to compute assignments arg maxz p(z|x, y; Œ∏) and
arg maxy,z p(y, z|x; Œ∏). In this section, we describe
algorithms for both cases that use the deterministic
OR nodes to simplify the required computations.
Predicting the most likely joint extraction
arg maxy,z p(y, z|x; Œ∏) can be done efficiently
given the structure of our model. In particular, we
note that the factors Œ¶join represent deterministic de-
pendencies between Z and Y, which when satisfied
do not affect the probability of the solution. It is thus
sufficient to independently compute an assignment
for each sentence-level extraction variable Zi, ignor-
ing the deterministic dependencies. The optimal set-
ting for the aggregate variables Y is then simply the
assignment that is consistent with these extractions.
The time complexity is O(|R |- |S|).
Predicting sentence level extractions given weak
supervision facts, arg maxz p(z|x, y; Œ∏), is more
challenging. We start by computing extraction
scores Œ¶extract(xi, zi) for each possible extraction as-
signment Zi = zi at each sentence xi E S, and
storing the values in a dynamic programming table.
Next, we must find the most likely assignment z that
respects our output variables y. It turns out that
this problem is a variant of the weighted, edge-cover
problem, for which there exist polynomial time op-
timal solutions.
Let G = (¬£, V = VS U Vy) be a complete
weighted bipartite graph with one node vSi E VS for
each sentence xi E S and one node vyr E Vy for each
relation r E R where yr = 1. The edge weights are
given by c((vSi , vyr )) def= Œ¶extract(xi, zi). Our goal is
to select a subset of the edges which maximizes the
sum of their weights, subject to each node vSi E VS
being incident to exactly one edge, and each node
vr E Vy being incident to at least one edge.
y
Exact Solution An exact solution can be obtained
by first computing the maximum weighted bipartite
matching, and adding edges to nodes which are not
incident to an edge. This can be computed in time
O(|V|(|¬£ |+ |V |log |V|)), which we can rewrite as
O((|R |+ |S|)(|R||S |+ (|R |+ |S|) log(|R |+ |S|))).
Approximate Solution An approximate solution
can be obtained by iterating over the nodes in Vy,
</bodyText>
<equation confidence="0.660874833333333">
y y
VbornIn VlocatedIn
p(Z1 = bornIn|x) p(Z3 = locatedIn|x)
p(Z1 ...
S
V1S V2V3S
</equation>
<figureCaption confidence="0.9938265">
Figure 3: Inference of arg max, p(Z = z|x, y) requires
solving a weighted, edge-cover problem.
</figureCaption>
<bodyText confidence="0.999657875">
and each time adding the highest weight incident
edge whose addition doesn‚Äôt violate a constraint.
The running time is O(|R||S|). This greedy search
guarantees each fact is extracted at least once and
allows any additional extractions that increase the
overall probability of the assignment. Given the
computational advantage, we use it in all of the ex-
perimental evaluations.
</bodyText>
<sectionHeader confidence="0.999546" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999932333333333">
We follow the approach of Riedel et al. (2010) for
generating weak supervision data, computing fea-
tures, and evaluating aggregate extraction. We also
introduce new metrics for measuring sentential ex-
traction performance, both relation-independent and
relation-specific.
</bodyText>
<subsectionHeader confidence="0.996379">
6.1 Data Generation
</subsectionHeader>
<bodyText confidence="0.999949375">
We used the same data sets as Riedel et al. (2010)
for weak supervision. The data was first tagged with
the Stanford NER system (Finkel et al., 2005) and
then entity mentions were found by collecting each
continuous phrase where words were tagged iden-
tically (i.e., as a person, location, or organization).
Finally, these phrases were matched to the names of
Freebase entities.
Given the set of matches, define Œ£ to be set of NY
Times sentences with two matched phrases, E to be
the set of Freebase entities which were mentioned in
one or more sentences, Œî to be the set of Freebase
facts whose arguments, e1 and e2 were mentioned in
a sentence in Œ£, and R to be set of relations names
used in the facts of Œî. These sets define the weak
supervision data.
</bodyText>
<subsectionHeader confidence="0.978856">
6.2 Features and Initialization
</subsectionHeader>
<bodyText confidence="0.9922665">
We use the set of sentence-level features described
by Riedel et al. (2010), which were originally de-
</bodyText>
<page confidence="0.977314">
545
</page>
<table confidence="0.265399">
Precision
</table>
<bodyText confidence="0.997461230769231">
veloped by Mintz et al. (2009). These include in-
dicators for various lexical, part of speech, named
entity, and dependency tree path properties of entity
mentions in specific sentences, as computed with the
Malt dependency parser (Nivre and Nilsson, 2004)
and OpenNLP POS tagger1. However, unlike the
previous work, we did not make use of any features
that explicitly aggregate these properties across mul-
tiple mention instances.
The MULTIR algorithm has a single parameter T,
the number of training iterations, that must be spec-
ified manually. We used T = 50 iterations, which
performed best in development experiments.
</bodyText>
<subsectionHeader confidence="0.986275">
6.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.99997352">
Evaluation is challenging, since only a small per-
centage (approximately 3%) of sentences match
facts in Freebase, and the number of matches is
highly unbalanced across relations, as we will see
in more detail later. We use the following metrics.
Aggregate Extraction Let De be the set of ex-
tracted relations for any of the systems; we com-
pute aggregate precision and recall by comparing
De with D. This metric is easily computed but un-
derestimates extraction accuracy because Freebase
is incomplete and some true relations in De will be
marked wrong.
Sentential Extraction Let 5e be the sentences
where some system extracted a relation and 5F be
the sentences that match the arguments of a fact in
D. We manually compute sentential extraction ac-
curacy by sampling a set of 1000 sentences from
5e U 5F and manually labeling the correct extrac-
tion decision, either a relation r E R or none. We
then report precision and recall for each system on
this set of sampled sentences. These results provide
a good approximation to the true precision but can
overestimate the actual recall, since we did not man-
ually check the much larger set of sentences where
no approach predicted extractions.
</bodyText>
<subsectionHeader confidence="0.984511">
6.4 Precision / Recall Curves
</subsectionHeader>
<bodyText confidence="0.999926666666667">
To compute precision / recall curves for the tasks,
we ranked the MULTIR extractions as follows. For
sentence-level evaluations, we ordered according to
</bodyText>
<footnote confidence="0.748108">
1http://opennlp.sourceforge.net/
</footnote>
<figure confidence="0.9223325">
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Recall
</figure>
<figureCaption confidence="0.997302666666667">
Figure 4: Aggregate extraction precision / recall curves
for Riedel et al. (2010), a reimplementation of that ap-
proach (SOLOR), and our algorithm (MULTIR).
</figureCaption>
<bodyText confidence="0.9999015">
the extraction factor score 4bextract(zi7 xi). For aggre-
gate comparisons, we set the score for an extraction
Y&apos; = true to be the max of the extraction factor
scores for the sentences where r was extracted.
</bodyText>
<sectionHeader confidence="0.99945" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999993">
To evaluate our algorithm, we first compare it to an
existing approach for using multi-instance learning
with weak supervision (Riedel et al., 2010), using
the same data and features. We report both aggregate
extraction and sentential extraction results. We then
investigate relation-specific performance of our sys-
tem. Finally, we report running time comparisons.
</bodyText>
<subsectionHeader confidence="0.997589">
7.1 Aggregate Extraction
</subsectionHeader>
<bodyText confidence="0.9999290625">
Figure 4 shows approximate precision / recall curves
for three systems computed with aggregate metrics
(Section 6.3) that test how closely the extractions
match the facts in Freebase. The systems include the
original results reported by Riedel et al. (2010) as
well as our new model (MULTIR). We also compare
with SOLOR, a reimplementation of their algorithm,
which we built in Factorie (McCallum et al., 2009),
and will use later to evaluate sentential extraction.
MULTIR achieves competitive or higher preci-
sion over all ranges of recall, with the exception
of the very low recall range of approximately 0-
1%. It also significantly extends the highest recall
achieved, from 20% to 25%, with little loss in preci-
sion. To investigate the low precision in the 0-1% re-
call range, we manually checked the ten highest con-
</bodyText>
<figure confidence="0.972143230769231">
1.0
MULTiR
SOLOR
Riedel et al., 2010
0.8
0.6
0.4
0.2
0.0
546
Precision
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Recall
</figure>
<figureCaption confidence="0.9988995">
Figure 5: Sentential extraction precision / recall curves
for MULTIR and SOLOR.
</figureCaption>
<bodyText confidence="0.9999368">
fidence extractions produced by MULTIR that were
marked wrong. We found that all ten were true facts
that were simply missing from Freebase. A manual
evaluation, as we perform next for sentential extrac-
tion, would remove this dip.
</bodyText>
<subsectionHeader confidence="0.998389">
7.2 Sentential Extraction
</subsectionHeader>
<bodyText confidence="0.999896461538462">
Although their model includes variables to model
sentential extraction, Riedel et al. (2010) did not re-
port sentence level performance. To generate the
precision / recall curve we used the joint model as-
signment score for each of the sentences that con-
tributed to the aggregate extraction decision.
Figure 4 shows approximate precision / recall
curves for MULTIR and SOLOR computed against
manually generated sentence labels, as defined in
Section 6.3. MULTIR achieves significantly higher
recall with a consistently high level of precision. At
the highest recall point, MULTIR reaches 72.4% pre-
cision and 51.9% recall, for an F1 score of 60.5%.
</bodyText>
<subsectionHeader confidence="0.993695">
7.3 Relation-Specific Performance
</subsectionHeader>
<bodyText confidence="0.9999868">
Since the data contains an unbalanced number of in-
stances of each relation, we also report precision and
recall for each of the ten most frequent relations. Let
SM be the sentences where MULTIR extracted an
instance of relation r E R, and let Sr be the sen-
tences that match the arguments of a fact about re-
lation r in A. For each r, we sample 100 sentences
from both SM and Sr and manually check accu-
racy. To estimate precision Pr we compute the ratio
of true relation mentions in SM , and to estimate re-
call Rr we take the ratio of true relation mentions in
SÔøΩr which are returned by our system.
Table 1 presents this approximate precision and
recall for MULTIR on each of the relations, along
with statistics we computed to measure the qual-
ity of the weak supervision. Precision is high for
the majority of relations but recall is consistently
lower. We also see that the Freebase matches are
highly skewed in quantity and can be low quality for
some relations, with very few of them actually cor-
responding to true extractions. The approach gener-
ally performs best on the relations with a sufficiently
large number of true matches, in many cases even
achieving precision that outperforms the accuracy of
the heuristic matches, at reasonable recall levels.
</bodyText>
<subsectionHeader confidence="0.999225">
7.4 Overlapping Relations
</subsectionHeader>
<bodyText confidence="0.999819214285714">
Table 1 also highlights some of the effects of learn-
ing with overlapping relations. For example, in the
data, almost all of the matches for the administra-
tive divisions relation overlap with the contains re-
lation, because they both model relationships for a
pair of locations. Since, in general, sentences are
much more likely to describe a contains relation, this
overlap leads to a situation were almost none of the
administrate division matches are true ones, and we
cannot accurately learn an extractor. However, we
can still learn to accurately extract the contains rela-
tion, despite the distracting matches. Similarly, the
place of birth and place of death relations tend to
overlap, since it is often the case that people are born
and die in the same city. In both cases, the precision
outperforms the labeling accuracy and the recall is
relatively high.
To measure the impact of modeling overlapping
relations, we also evaluated a simple, restricted
baseline. Instead of labeling each entity pair with
the set of all true Freebase facts, we created a dataset
where each true relation was used to create a dif-
ferent training example. Training MULTIR on this
data simulates effects of conflicting supervision that
can come from not modeling overlaps. On average
across relations, precision increases 12 points but re-
call drops 26 points, for an overall reduction in F1
score from 60.5% to 40.3%.
</bodyText>
<subsectionHeader confidence="0.999429">
7.5 Running Time
</subsectionHeader>
<bodyText confidence="0.9996175">
One final advantage of our model is the mod-
est running time. Our implementation of the
</bodyText>
<figure confidence="0.99695">
MULTIR
SOLOR
1.0
0.8
0.6
0.4
0.2
0.0
</figure>
<page confidence="0.98628">
547
</page>
<table confidence="0.999915666666667">
Relation Freebase Matches MULTIR R
#sents % true P
/business/person/company 302 89.0 100.0 25.8
/people/person/place lived 450 60.0 80.0 6.7
/location/location/contains 2793 51.0 100.0 56.0
/business/company/founders 95 48.4 71.4 10.9
/people/person/nationality 723 41.0 85.7 15.0
/location/neighborhood/neighborhood of 68 39.7 100.0 11.1
/people/person/children 30 80.0 100.0 8.3
/people/deceased person/place of death 68 22.1 100.0 20.0
/people/person/place of birth 162 12.0 100.0 33.0
/location/country/administrative divisions 424 0.2 N/A 0.0
</table>
<tableCaption confidence="0.990628">
Table 1: Estimated precision and recall by relation, as well as the number of matched sentences (#sents) and accuracy
(% true) of matches between sentences and facts in Freebase.
</tableCaption>
<bodyText confidence="0.9997906">
Riedel et al. (2010) approach required approxi-
mately 6 hours to train on NY Times 05-06 and 4
hours to test on the NY Times 07, each without pre-
processing. Although they do sampling for infer-
ence, the global aggregation variables require rea-
soning about an exponentially large (in the number
of sentences) sample space.
In contrast, our approach required approximately
one minute to train and less than one second to test,
on the same data. This advantage comes from the
decomposition that is possible with the determinis-
tic OR aggregation variables. For test, we simply
consider each sentence in isolation and during train-
ing our approximation to the weighted assignment
problem is linear in the number of sentences.
</bodyText>
<subsectionHeader confidence="0.942284">
7.6 Discussion
</subsectionHeader>
<bodyText confidence="0.999957444444444">
The sentential extraction results demonstrates the
advantages of learning a model that is primarily
driven by sentence-level features. Although previ-
ous approaches have used more sophisticated fea-
tures for aggregating the evidence from individual
sentences, we demonstrate that aggregating strong
sentence-level evidence with a simple deterministic
OR that models overlapping relations is more effec-
tive, and also enables training of a sentence extractor
that runs with no aggregate information.
While the Riedel et al. approach does include a
model of which sentences express relations, it makes
significant use of aggregate features that are primar-
ily designed to do entity-level relation predictions
and has a less detailed model of extractions at the
individual sentence level. Perhaps surprisingly, our
model is able to do better at both the sentential and
aggregate levels.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="conclusions">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999950166666667">
Supervised-learning approaches to IE were intro-
duced in (Soderland et al., 1995) and are too nu-
merous to summarize here. While they offer high
precision and recall, these methods are unlikely to
scale to the thousands of relations found in text on
the Web. Open IE systems, which perform self-
supervised learning of relation-independent extrac-
tors (e.g., Preemptive IE (Shinyama and Sekine,
2006), TEXTRUNNER (Banko et al., 2007; Banko
and Etzioni, 2008) and WOE (Wu and Weld, 2010))
can scale to millions of documents, but don‚Äôt output
canonicalized relations.
</bodyText>
<subsectionHeader confidence="0.979831">
8.1 Weak Supervision
</subsectionHeader>
<bodyText confidence="0.999954333333333">
Weak supervision (also known as distant- or self su-
pervision) refers to a broad class of methods, but
we focus on the increasingly-popular idea of using
a store of structured data to heuristicaly label a tex-
tual corpus. Craven and Kumlien (1999) introduced
the idea by matching the Yeast Protein Database
(YPD) to the abstracts of papers in PubMed and
training a naive-Bayes extractor. Bellare and Mc-
Callum (2007) used a database of BibTex records
to train a CRF extractor on 12 bibliographic rela-
tions. The KYLIN system aplied weak supervision
to learn relations from Wikipedia, treating infoboxes
as the associated database (Wu and Weld, 2007);
Wu et al. (2008) extended the system to use smooth-
ing over an automatically generated infobox taxon-
</bodyText>
<page confidence="0.970051">
548
</page>
<note confidence="0.510282">
omy. Mintz et al. (2009) used Freebase facts to train automatically learn a nearly unbounded number of
100 relational extractors on Wikipedia. Hoffmann relational extractors. Since the processs of match-
</note>
<bodyText confidence="0.998274068181818">
et al. (2010) describe a system similar to KYLIN, ing database tuples to sentences is inherently heuris-
but which dynamically generates lexicons in order tic, researchers have proposed multi-instance learn-
to handle sparse data, learning over 5000 Infobox ing algorithms as a means for coping with the result-
relations with an average F1 score of 61%. Yao ing noisy data. Unfortunately, previous approaches
et al. (2010) perform weak supervision, while using assume that all relations are disjoint ‚Äî for exam-
selectional preference constraints to a jointly reason ple they cannot extract the pair Founded(Jobs,
about entity types. Apple) and CEO-of(Jobs, Apple), because
The NELL system (Carlson et al., 2010) can also two relations are not allowed to have the same argu-
be viewed as performing weak supervision. Its ini- ments.
tial knowledge consists of a selectional preference This paper presents a novel approach for multi-
constraint and 20 ground fact seeds. NELL then instance learning with overlapping relations that
matches entity pairs from the seeds to a Web cor- combines a sentence-level extraction model with a
pus, but instead of learning a probabilistic model, simple, corpus-level component for aggregating the
it bootstraps a set of extraction patterns using semi- individual facts. We apply our model to learn extrac-
supervised methods for multitask learning. tors for NY Times text using weak supervision from
8.2 Multi-Instance Learning Freebase. Experiments show improvements for both
Multi-instance learning was introduced in order to sentential and aggregate (corpus level) extraction,
combat the problem of ambiguously-labeled train- and demonstrate that the approach is computation-
ing data when predicting the activity of differ- ally efficient.
ent drugs (Dietterich et al., 1997). Bunescu and Our early progress suggests many interesting di-
Mooney (2007) connect weak supervision with rections. By joining two or more Freebase tables,
multi-instance learning and extend their relational we can generate many more matches and learn more
extraction kernel to this context. relations. We also wish to refine our model in order
Riedel et al. (2010), combine weak supervision to improve precision. For example, we would like
and multi-instance learning in a more sophisticated to add type reasoning about entities and selectional
manner, training a graphical model, which assumes preference constraints for relations. Finally, we are
only that at least one of the matches between the also interested in applying the overall learning ap-
arguments of a Freebase fact and sentences in the proaches to other tasks that could be modeled with
corpus is a true relational mention. Our model may weak supervision, such as coreference and named
be seen as an extension of theirs, since both models entity classification.
include sentence-level and aggregate random vari- The source code of our system, its out-
ables. However, Riedel et al. have only a single ag- put, and all data annotations are available at
gregate variable that takes values r E R U {none}, http://cs.uw.edu/homes/raphaelh/mr.
thereby ruling out overlapping relations. We have Acknowledgments
discussed the comparison in more detail throughout We thank Sebastian Riedel and Limin Yao for shar-
the paper, including in the model formulation sec- ing their data and providing valuable advice. This
tion and experiments. material is based upon work supported by a WRF /
9 Conclusion TJ Cable Professorship, a gift from Google and by
We argue that weak supervision is promising method the Air Force Research Laboratory (AFRL) under
for scaling information extraction to the level where prime contract no. FA8750-09-C-0181. Any opin-
it can handle the myriad, different relations on the ions, findings, and conclusion or recommendations
Web. By using the contents of a database to heuris- expressed in this material are those of the author(s)
tically label a training corpus, we may be able to and do not necessarily reflect the view of the Air
549 Force Research Laboratory (AFRL).
</bodyText>
<sectionHeader confidence="0.995669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999744245283019">
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-08), pages
28‚Äì36.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI-07), pages 2670‚Äì2676.
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Sixth International Workshop on Infor-
mation Integration on the Web.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI-10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002).
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology, pages 77‚Äì86.
Thomas G. Dietterich, Richard H. Lathrop, and Tom¬¥as
Lozano-P¬¥erez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artificial Intel-
ligence, 89:31‚Äì71, January.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL-
05), pages 363‚Äì370.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-10), pages
286‚Äì295.
Percy Liang, A. Bouchard-CÀÜot¬¥e, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach
to machine translation. In International Conference on
Computational Linguistics and Association for Com-
putational Linguistics (COLING/ACL).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via imper-
atively defined factor graphs. In Neural Information
Processing Systems Conference (NIPS).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-2009), pages 1003‚Äì1011.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of the Conference
on Natural Language Learning (CoNLL-04), pages
49‚Äì56.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteenth Euro-
pean Conference on Machine Learning (ECML-2010),
pages 148‚Äì163.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computation Linguistics (HLT-
NAACL-06).
Stephen Soderland, David Fisher, Jonathan Aseltine, and
Wendy G. Lehnert. 1995. Crystal: Inducing a concep-
tual dictionary. In Proceedings of the Fourteenth In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-1995), pages 1314‚Äì1321.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM-2007), pages 41‚Äì50.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings of
the 17th International Conference on World Wide Web
(WWW-2008), pages 635‚Äì644.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In The Annual Meeting of
the Association for Computational Linguistics (ACL-
2010), pages 118‚Äì127.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2010), pages 1013‚Äì1023.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to log-
ical form. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL-2007).
</reference>
<page confidence="0.997246">
550
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.946677">
<title confidence="0.999404">Knowledge-Based Weak Supervision for Information of Overlapping Relations</title>
<author confidence="0.999726">Raphael Hoffmann</author>
<author confidence="0.999726">Congle Zhang</author>
<author confidence="0.999726">Xiao Ling</author>
<author confidence="0.999726">Luke Zettlemoyer</author>
<author confidence="0.999726">S Daniel</author>
<affiliation confidence="0.9998255">Computer Science &amp; University of</affiliation>
<address confidence="0.999692">Seattle, WA 98195, USA</address>
<abstract confidence="0.997912192307692">Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web‚Äôs natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume are for example they extract the pair This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of 46th Annual Meeting of the Association for Computational Linguistics (ACL-08),</booktitle>
<pages>28--36</pages>
<contexts>
<context position="29009" citStr="Banko and Etzioni, 2008" startWordPosition="4818" endWordPosition="4821">ed model of extractions at the individual sentence level. Perhaps surprisingly, our model is able to do better at both the sentential and aggregate levels. 8 Related Work Supervised-learning approaches to IE were introduced in (Soderland et al., 1995) and are too numerous to summarize here. While they offer high precision and recall, these methods are unlikely to scale to the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), TEXTRUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and WOE (Wu and Weld, 2010)) can scale to millions of documents, but don‚Äôt output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007) used a database of BibTex records to train a CRF extracto</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In Proceedings of 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 28‚Äì36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07),</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="28983" citStr="Banko et al., 2007" startWordPosition="4814" endWordPosition="4817">nd has a less detailed model of extractions at the individual sentence level. Perhaps surprisingly, our model is able to do better at both the sentential and aggregate levels. 8 Related Work Supervised-learning approaches to IE were introduced in (Soderland et al., 1995) and are too numerous to summarize here. While they offer high precision and recall, these methods are unlikely to scale to the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), TEXTRUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and WOE (Wu and Weld, 2010)) can scale to millions of documents, but don‚Äôt output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007) used a database of BibTex recor</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2670‚Äì2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning extractors from unlabeled text using relevant databases.</title>
<date>2007</date>
<booktitle>In Sixth International Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="29551" citStr="Bellare and McCallum (2007)" startWordPosition="4906" endWordPosition="4910">(Shinyama and Sekine, 2006), TEXTRUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and WOE (Wu and Weld, 2010)) can scale to millions of documents, but don‚Äôt output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007) used a database of BibTex records to train a CRF extractor on 12 bibliographic relations. The KYLIN system aplied weak supervision to learn relations from Wikipedia, treating infoboxes as the associated database (Wu and Weld, 2007); Wu et al. (2008) extended the system to use smoothing over an automatically generated infobox taxon548 omy. Mintz et al. (2009) used Freebase facts to train automatically learn a nearly unbounded number of 100 relational extractors on Wikipedia. Hoffmann relational extractors. Since the processs of matchet al. (2010) describe a system similar to KYLIN, ing databas</context>
</contexts>
<marker>Bellare, McCallum, 2007</marker>
<rawString>Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In Sixth International Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL07).</booktitle>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI-10).</booktitle>
<contexts>
<context position="30795" citStr="Carlson et al., 2010" startWordPosition="5101" endWordPosition="5104">s inherently heurisbut which dynamically generates lexicons in order tic, researchers have proposed multi-instance learnto handle sparse data, learning over 5000 Infobox ing algorithms as a means for coping with the resultrelations with an average F1 score of 61%. Yao ing noisy data. Unfortunately, previous approaches et al. (2010) perform weak supervision, while using assume that all relations are disjoint ‚Äî for examselectional preference constraints to a jointly reason ple they cannot extract the pair Founded(Jobs, about entity types. Apple) and CEO-of(Jobs, Apple), because The NELL system (Carlson et al., 2010) can also two relations are not allowed to have the same argube viewed as performing weak supervision. Its ini- ments. tial knowledge consists of a selectional preference This paper presents a novel approach for multiconstraint and 20 ground fact seeds. NELL then instance learning with overlapping relations that matches entity pairs from the seeds to a Web cor- combines a sentence-level extraction model with a pus, but instead of learning a probabilistic model, simple, corpus-level component for aggregating the it bootstraps a set of extraction patterns using semi- individual facts. We apply o</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002).</booktitle>
<contexts>
<context position="11931" citStr="Collins, 2002" startWordPosition="1991" endWordPosition="1992">ikelihood: We define the training set {(xi, yi)|i = 1... n}, O(0) = rl p(yi|xi; 0) = rl E p(yi, z|xi; 0) where i is an index corresponding to a particu- i i z lar entity pair (ej, ek) in A, xi contains all of However, this objective would be difficult to opthe sentences in E with mentions of this pair, and timize exactly, and algorithms for doing so would yi = relVector(ej, ek). be unlikely to scale to data sets of the size we conComputation: sider. Instead, we make two approximations, deinitialize parameter vector O &lt;-- 0 scribed below, leading to a Perceptron-style addifort = 1...T do tive (Collins, 2002) parameter update scheme which for i = 1...n do has been modified to reason about hidden variables, (y&apos;, z&apos;) &lt;-- arg maxy,z p(y, z|xi; 0) similar in style to the approaches of (Liang et al., if y&apos; =ÔøΩ yi then 2006; Zettlemoyer and Collins, 2007), but adapted z* &lt;-- arg maxz for our specific model. This approximate algorithm p(z|xi, yi; 0) is computationally efficient and, as we will see, O &lt;-- O + O(xi, z*) ‚Äî O(xi, z&apos;) works well in practice. end if Our first modification is to do online learning end for instead of optimizing the full objective. Define the end for feature sums O(x, z) = Ej O(xj</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,</booktitle>
<pages>77--86</pages>
<contexts>
<context position="2213" citStr="Craven and Kumlien, 1999" startWordPosition="308" endWordPosition="312">ed tuples, arguing that such a knowledge base could benefit many important tasks such as question answering and summarization. Most approaches to IE use supervised learning of relation-specific examples, which can achieve high precision and recall. Unfortunately, however, fully supervised methods are limited by the availability of training data and are unlikely to scale to the thousands of relations found on the Web. A more promising approach, often called ‚Äúweak‚Äù or ‚Äúdistant‚Äù supervision, creates its own training data by heuristically matching the contents of a database to corresponding text (Craven and Kumlien, 1999). For example, suppose that r(e1, e2) = Founded(Jobs,Apple) is a ground tuple in the database and s =‚ÄúSteve Jobs founded Apple, Inc.‚Äù is a sentence containing synonyms for both e1 = Jobs and e2 = Apple, then s may be a natural language expression of the fact that r(e1, e2) holds and could be a useful training example. While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles (Hoffmann et al., 2010)), Riedel et al. (2010) observe that the heuristic leads to noisy data and poor extraction perfo</context>
<context position="29383" citStr="Craven and Kumlien (1999)" startWordPosition="4880" endWordPosition="4883">o the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), TEXTRUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and WOE (Wu and Weld, 2010)) can scale to millions of documents, but don‚Äôt output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007) used a database of BibTex records to train a CRF extractor on 12 bibliographic relations. The KYLIN system aplied weak supervision to learn relations from Wikipedia, treating infoboxes as the associated database (Wu and Weld, 2007); Wu et al. (2008) extended the system to use smoothing over an automatically generated infobox taxon548 omy. Mintz et al. (2009) used Freebase facts to train automatically learn a nearly unbounded nu</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology, pages 77‚Äì86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
<author>Richard H Lathrop</author>
<author>Tom¬¥as Lozano-P¬¥erez</author>
</authors>
<title>Solving the multiple instance problem with axis-parallel rectangles.</title>
<date>1997</date>
<journal>Artificial Intelligence,</journal>
<volume>89</volume>
<marker>Dietterich, Lathrop, Lozano-P¬¥erez, 1997</marker>
<rawString>Thomas G. Dietterich, Richard H. Lathrop, and Tom¬¥as Lozano-P¬¥erez. 1997. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence, 89:31‚Äì71, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05),</booktitle>
<pages>363--370</pages>
<contexts>
<context position="17822" citStr="Finkel et al., 2005" startWordPosition="3004" endWordPosition="3007">ional extractions that increase the overall probability of the assignment. Given the computational advantage, we use it in all of the experimental evaluations. 6 Experimental Setup We follow the approach of Riedel et al. (2010) for generating weak supervision data, computing features, and evaluating aggregate extraction. We also introduce new metrics for measuring sentential extraction performance, both relation-independent and relation-specific. 6.1 Data Generation We used the same data sets as Riedel et al. (2010) for weak supervision. The data was first tagged with the Stanford NER system (Finkel et al., 2005) and then entity mentions were found by collecting each continuous phrase where words were tagged identically (i.e., as a person, location, or organization). Finally, these phrases were matched to the names of Freebase entities. Given the set of matches, define Œ£ to be set of NY Times sentences with two matched phrases, E to be the set of Freebase entities which were mentioned in one or more sentences, Œî to be the set of Freebase facts whose arguments, e1 and e2 were mentioned in a sentence in Œ£, and R to be set of relations names used in the facts of Œî. These sets define the weak supervision </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pages 363‚Äì370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>286--295</pages>
<contexts>
<context position="2717" citStr="Hoffmann et al., 2010" startWordPosition="397" endWordPosition="400">wn training data by heuristically matching the contents of a database to corresponding text (Craven and Kumlien, 1999). For example, suppose that r(e1, e2) = Founded(Jobs,Apple) is a ground tuple in the database and s =‚ÄúSteve Jobs founded Apple, Inc.‚Äù is a sentence containing synonyms for both e1 = Jobs and e2 = Apple, then s may be a natural language expression of the fact that r(e1, e2) holds and could be a useful training example. While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles (Hoffmann et al., 2010)), Riedel et al. (2010) observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles). To fix this problem they cast weak supervision as a form of multi-instance learning, assuming only that at least one of the sentences containing e1 and e2 are expressing r(e1, e2), and their method yields a substantial improvement in extraction performance. However, Riedel et al.‚Äôs model (like that of previous systems (Mintz et al., 2009)) assumes that relations do not overlap ‚Äî there cannot exist </context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 286‚Äì295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>A Bouchard-CÀÜot¬¥e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL).</booktitle>
<marker>Liang, Bouchard-CÀÜot¬¥e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, A. Bouchard-CÀÜot¬¥e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Karl Schultz</author>
<author>Sameer Singh</author>
</authors>
<title>Factorie: Probabilistic programming via imperatively defined factor graphs.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems Conference (NIPS).</booktitle>
<contexts>
<context position="21839" citStr="McCallum et al., 2009" startWordPosition="3661" endWordPosition="3664">ures. We report both aggregate extraction and sentential extraction results. We then investigate relation-specific performance of our system. Finally, we report running time comparisons. 7.1 Aggregate Extraction Figure 4 shows approximate precision / recall curves for three systems computed with aggregate metrics (Section 6.3) that test how closely the extractions match the facts in Freebase. The systems include the original results reported by Riedel et al. (2010) as well as our new model (MULTIR). We also compare with SOLOR, a reimplementation of their algorithm, which we built in Factorie (McCallum et al., 2009), and will use later to evaluate sentential extraction. MULTIR achieves competitive or higher precision over all ranges of recall, with the exception of the very low recall range of approximately 0- 1%. It also significantly extends the highest recall achieved, from 20% to 25%, with little loss in precision. To investigate the low precision in the 0-1% recall range, we manually checked the ten highest con1.0 MULTiR SOLOR Riedel et al., 2010 0.8 0.6 0.4 0.2 0.0 546 Precision 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Recall Figure 5: Sentential extraction precision / recall curves for MULTIR and SOLOR. fidenc</context>
</contexts>
<marker>McCallum, Schultz, Singh, 2009</marker>
<rawString>Andrew McCallum, Karl Schultz, and Sameer Singh. 2009. Factorie: Probabilistic programming via imperatively defined factor graphs. In Neural Information Processing Systems Conference (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-2009),</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="3256" citStr="Mintz et al., 2009" startWordPosition="485" endWordPosition="488">.g., matching Wikipedia infoboxes to associated articles (Hoffmann et al., 2010)), Riedel et al. (2010) observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles). To fix this problem they cast weak supervision as a form of multi-instance learning, assuming only that at least one of the sentences containing e1 and e2 are expressing r(e1, e2), and their method yields a substantial improvement in extraction performance. However, Riedel et al.‚Äôs model (like that of previous systems (Mintz et al., 2009)) assumes that relations do not overlap ‚Äî there cannot exist two facts r(e1, e2) and q(e1, e2) that are both true for any pair of entities, e1 and e2. Unfortunately, this assumption is often violated; 541 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 541‚Äì550, Portland, Oregon, June 19-24, 2011. cÔøΩ2011 Association for Computational Linguistics for example both Founded(Jobs, Apple) and CEO-of(Jobs, Apple) are clearly true. Indeed, 18.3% of the weak supervision facts in Freebase that match sentences in the NY Times 2007 corpus have overlapping rela</context>
<context position="9551" citStr="Mintz et al. (2009)" startWordPosition="1556" endWordPosition="1559">inary Yr random variables, one for each r E R, and Z to be the vector of Zi variables, one for each sentence xi. Our conditional extraction model is defined as follows: where the parameter vector Œ∏ is used, below, to define the factor cfiextract. The factors Voin are deterministic OR operators which are included to ensure that the ground fact r(e) is predicted at the aggregate level for the assignment Yr = yr only if at least one of the senwhere the features œÜj are sensitive to the relation name assigned to extraction variable zi, if any, and cues from the sentence xi. We will make use of the Mintz et al. (2009) sentence-level features in the expeiments, as described in Section 7. 3.3 Discussion This model was designed to provide a joint approach where extraction decisions are almost entirely driven by sentence-level reasoning. However, defining the Yr random variables and tying them to the sentencelevel variables, Zi, provides a direct method for modeling weak supervision. We can simply train the model so that the Y variables match the facts in the database, treating the Zi as hidden variables that can take any value, as long as they produce the correct aggregate predictions. This approach is relate</context>
<context position="18605" citStr="Mintz et al. (2009)" startWordPosition="3143" endWordPosition="3146">y, these phrases were matched to the names of Freebase entities. Given the set of matches, define Œ£ to be set of NY Times sentences with two matched phrases, E to be the set of Freebase entities which were mentioned in one or more sentences, Œî to be the set of Freebase facts whose arguments, e1 and e2 were mentioned in a sentence in Œ£, and R to be set of relations names used in the facts of Œî. These sets define the weak supervision data. 6.2 Features and Initialization We use the set of sentence-level features described by Riedel et al. (2010), which were originally de545 Precision veloped by Mintz et al. (2009). These include indicators for various lexical, part of speech, named entity, and dependency tree path properties of entity mentions in specific sentences, as computed with the Malt dependency parser (Nivre and Nilsson, 2004) and OpenNLP POS tagger1. However, unlike the previous work, we did not make use of any features that explicitly aggregate these properties across multiple mention instances. The MULTIR algorithm has a single parameter T, the number of training iterations, that must be specified manually. We used T = 50 iterations, which performed best in development experiments. 6.3 Evalu</context>
<context position="29912" citStr="Mintz et al. (2009)" startWordPosition="4967" endWordPosition="4970">store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007) used a database of BibTex records to train a CRF extractor on 12 bibliographic relations. The KYLIN system aplied weak supervision to learn relations from Wikipedia, treating infoboxes as the associated database (Wu and Weld, 2007); Wu et al. (2008) extended the system to use smoothing over an automatically generated infobox taxon548 omy. Mintz et al. (2009) used Freebase facts to train automatically learn a nearly unbounded number of 100 relational extractors on Wikipedia. Hoffmann relational extractors. Since the processs of matchet al. (2010) describe a system similar to KYLIN, ing database tuples to sentences is inherently heurisbut which dynamically generates lexicons in order tic, researchers have proposed multi-instance learnto handle sparse data, learning over 5000 Infobox ing algorithms as a means for coping with the resultrelations with an average F1 score of 61%. Yao ing noisy data. Unfortunately, previous approaches et al. (2010) perf</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-2009), pages 1003‚Äì1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL-04),</booktitle>
<pages>49--56</pages>
<contexts>
<context position="18830" citStr="Nivre and Nilsson, 2004" startWordPosition="3177" endWordPosition="3180">ne or more sentences, Œî to be the set of Freebase facts whose arguments, e1 and e2 were mentioned in a sentence in Œ£, and R to be set of relations names used in the facts of Œî. These sets define the weak supervision data. 6.2 Features and Initialization We use the set of sentence-level features described by Riedel et al. (2010), which were originally de545 Precision veloped by Mintz et al. (2009). These include indicators for various lexical, part of speech, named entity, and dependency tree path properties of entity mentions in specific sentences, as computed with the Malt dependency parser (Nivre and Nilsson, 2004) and OpenNLP POS tagger1. However, unlike the previous work, we did not make use of any features that explicitly aggregate these properties across multiple mention instances. The MULTIR algorithm has a single parameter T, the number of training iterations, that must be specified manually. We used T = 50 iterations, which performed best in development experiments. 6.3 Evaluation Metrics Evaluation is challenging, since only a small percentage (approximately 3%) of sentences match facts in Freebase, and the number of matches is highly unbalanced across relations, as we will see in more detail la</context>
</contexts>
<marker>Nivre, Nilsson, 2004</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of the Conference on Natural Language Learning (CoNLL-04), pages 49‚Äì56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Sixteenth European Conference on Machine Learning (ECML-2010),</booktitle>
<pages>148--163</pages>
<contexts>
<context position="2740" citStr="Riedel et al. (2010)" startWordPosition="401" endWordPosition="404">stically matching the contents of a database to corresponding text (Craven and Kumlien, 1999). For example, suppose that r(e1, e2) = Founded(Jobs,Apple) is a ground tuple in the database and s =‚ÄúSteve Jobs founded Apple, Inc.‚Äù is a sentence containing synonyms for both e1 = Jobs and e2 = Apple, then s may be a natural language expression of the fact that r(e1, e2) holds and could be a useful training example. While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles (Hoffmann et al., 2010)), Riedel et al. (2010) observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles). To fix this problem they cast weak supervision as a form of multi-instance learning, assuming only that at least one of the sentences containing e1 and e2 are expressing r(e1, e2), and their method yields a substantial improvement in extraction performance. However, Riedel et al.‚Äôs model (like that of previous systems (Mintz et al., 2009)) assumes that relations do not overlap ‚Äî there cannot exist two facts r(e1, e2) and</context>
<context position="4613" citStr="Riedel et al. (2010)" startWordPosition="693" endWordPosition="696">ilistic, graphical model of multi-instance learning which handles overlapping relations. ‚Ä¢ MULTIR also produces accurate sentence-level predictions, decoding individual sentences as well as making corpus-level extractions. ‚Ä¢ MULTIR is computationally tractable. Inference reduces to weighted set cover, for which it uses a greedy approximation with worst case running time O(|R |¬∑ |S|) where R is the set of possible relations and S is largest set of sentences for any entity pair. In practice, MULTIR runs very quickly. ‚Ä¢ We present experiments showing that MULTIR outperforms a reimplementation of Riedel et al. (2010)‚Äôs approach on both aggregate (corpus as a whole) and sentential extractions. Additional experiments characterize aspects of MULTIR‚Äôs performance. 2 Weak Supervision from a Database Given a corpus of text, we seek to extract facts about entities, such as the company Apple or the city Boston. A ground fact (or relation instance), is an expression r(e) where r is a relation name, for example Founded or CEO-of, and e = e1, ... , e,,, is a list of entities. An entity mention is a contiguous sequence of textual tokens denoting an entity. In this paper we assume that there is an oracle which can ide</context>
<context position="10216" citStr="Riedel et al. (2010)" startWordPosition="1662" endWordPosition="1665">s described in Section 7. 3.3 Discussion This model was designed to provide a joint approach where extraction decisions are almost entirely driven by sentence-level reasoning. However, defining the Yr random variables and tying them to the sentencelevel variables, Zi, provides a direct method for modeling weak supervision. We can simply train the model so that the Y variables match the facts in the database, treating the Zi as hidden variables that can take any value, as long as they produce the correct aggregate predictions. This approach is related to the multi-instance learning approach of Riedel et al. (2010), in that both models include sentence-level and aggregate random variables. However, their sentence level variables are binary and they only have a single aggregate variable that takes values r E R U {none}, thereby ruling out overlapping relations. Additionally, their aggregate decisions make use of Mintzstyle aggregate features (Mintz et al., 2009), that collect evidence from multiple sentences, while we use p(Y = y, Z = z|x; Œ∏) def = 1 ÔøΩ Zx 11 r -jextract(zi, xi) ÔøΩ4join(yr, z) i oin r def f 1 if yr = true n ]i : zi = r q&apos;j (y,z)= 0 otherwise 543 Inputs: Finally, we can now define the train</context>
<context position="17429" citStr="Riedel et al. (2010)" startWordPosition="2946" endWordPosition="2949">y y VbornIn VlocatedIn p(Z1 = bornIn|x) p(Z3 = locatedIn|x) p(Z1 ... S V1S V2V3S Figure 3: Inference of arg max, p(Z = z|x, y) requires solving a weighted, edge-cover problem. and each time adding the highest weight incident edge whose addition doesn‚Äôt violate a constraint. The running time is O(|R||S|). This greedy search guarantees each fact is extracted at least once and allows any additional extractions that increase the overall probability of the assignment. Given the computational advantage, we use it in all of the experimental evaluations. 6 Experimental Setup We follow the approach of Riedel et al. (2010) for generating weak supervision data, computing features, and evaluating aggregate extraction. We also introduce new metrics for measuring sentential extraction performance, both relation-independent and relation-specific. 6.1 Data Generation We used the same data sets as Riedel et al. (2010) for weak supervision. The data was first tagged with the Stanford NER system (Finkel et al., 2005) and then entity mentions were found by collecting each continuous phrase where words were tagged identically (i.e., as a person, location, or organization). Finally, these phrases were matched to the names </context>
<context position="20744" citStr="Riedel et al. (2010)" startWordPosition="3490" endWordPosition="3493">recision and recall for each system on this set of sampled sentences. These results provide a good approximation to the true precision but can overestimate the actual recall, since we did not manually check the much larger set of sentences where no approach predicted extractions. 6.4 Precision / Recall Curves To compute precision / recall curves for the tasks, we ranked the MULTIR extractions as follows. For sentence-level evaluations, we ordered according to 1http://opennlp.sourceforge.net/ 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Recall Figure 4: Aggregate extraction precision / recall curves for Riedel et al. (2010), a reimplementation of that approach (SOLOR), and our algorithm (MULTIR). the extraction factor score 4bextract(zi7 xi). For aggregate comparisons, we set the score for an extraction Y&apos; = true to be the max of the extraction factor scores for the sentences where r was extracted. 7 Experiments To evaluate our algorithm, we first compare it to an existing approach for using multi-instance learning with weak supervision (Riedel et al., 2010), using the same data and features. We report both aggregate extraction and sentential extraction results. We then investigate relation-specific performance </context>
<context position="22283" citStr="Riedel et al., 2010" startWordPosition="3737" endWordPosition="3740">y Riedel et al. (2010) as well as our new model (MULTIR). We also compare with SOLOR, a reimplementation of their algorithm, which we built in Factorie (McCallum et al., 2009), and will use later to evaluate sentential extraction. MULTIR achieves competitive or higher precision over all ranges of recall, with the exception of the very low recall range of approximately 0- 1%. It also significantly extends the highest recall achieved, from 20% to 25%, with little loss in precision. To investigate the low precision in the 0-1% recall range, we manually checked the ten highest con1.0 MULTiR SOLOR Riedel et al., 2010 0.8 0.6 0.4 0.2 0.0 546 Precision 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Recall Figure 5: Sentential extraction precision / recall curves for MULTIR and SOLOR. fidence extractions produced by MULTIR that were marked wrong. We found that all ten were true facts that were simply missing from Freebase. A manual evaluation, as we perform next for sentential extraction, would remove this dip. 7.2 Sentential Extraction Although their model includes variables to model sentential extraction, Riedel et al. (2010) did not report sentence level performance. To generate the precision / recall curve we used the join</context>
<context position="26948" citStr="Riedel et al. (2010)" startWordPosition="4496" endWordPosition="4499">60.0 80.0 6.7 /location/location/contains 2793 51.0 100.0 56.0 /business/company/founders 95 48.4 71.4 10.9 /people/person/nationality 723 41.0 85.7 15.0 /location/neighborhood/neighborhood of 68 39.7 100.0 11.1 /people/person/children 30 80.0 100.0 8.3 /people/deceased person/place of death 68 22.1 100.0 20.0 /people/person/place of birth 162 12.0 100.0 33.0 /location/country/administrative divisions 424 0.2 N/A 0.0 Table 1: Estimated precision and recall by relation, as well as the number of matched sentences (#sents) and accuracy (% true) of matches between sentences and facts in Freebase. Riedel et al. (2010) approach required approximately 6 hours to train on NY Times 05-06 and 4 hours to test on the NY Times 07, each without preprocessing. Although they do sampling for inference, the global aggregation variables require reasoning about an exponentially large (in the number of sentences) sample space. In contrast, our approach required approximately one minute to train and less than one second to test, on the same data. This advantage comes from the decomposition that is possible with the deterministic OR aggregation variables. For test, we simply consider each sentence in isolation and during tr</context>
<context position="32254" citStr="Riedel et al. (2010)" startWordPosition="5322" endWordPosition="5325">der to sentential and aggregate (corpus level) extraction, combat the problem of ambiguously-labeled train- and demonstrate that the approach is computationing data when predicting the activity of differ- ally efficient. ent drugs (Dietterich et al., 1997). Bunescu and Our early progress suggests many interesting diMooney (2007) connect weak supervision with rections. By joining two or more Freebase tables, multi-instance learning and extend their relational we can generate many more matches and learn more extraction kernel to this context. relations. We also wish to refine our model in order Riedel et al. (2010), combine weak supervision to improve precision. For example, we would like and multi-instance learning in a more sophisticated to add type reasoning about entities and selectional manner, training a graphical model, which assumes preference constraints for relations. Finally, we are only that at least one of the matches between the also interested in applying the overall learning aparguments of a Freebase fact and sentences in the proaches to other tasks that could be modeled with corpus is a true relational mention. Our model may weak supervision, such as coreference and named be seen as an </context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the Sixteenth European Conference on Machine Learning (ECML-2010), pages 148‚Äì163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computation Linguistics (HLTNAACL-06).</booktitle>
<contexts>
<context position="28951" citStr="Shinyama and Sekine, 2006" startWordPosition="4809" endWordPosition="4812">o do entity-level relation predictions and has a less detailed model of extractions at the individual sentence level. Perhaps surprisingly, our model is able to do better at both the sentential and aggregate levels. 8 Related Work Supervised-learning approaches to IE were introduced in (Soderland et al., 1995) and are too numerous to summarize here. While they offer high precision and recall, these methods are unlikely to scale to the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), TEXTRUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and WOE (Wu and Weld, 2010)) can scale to millions of documents, but don‚Äôt output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007)</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computation Linguistics (HLTNAACL-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>David Fisher</author>
<author>Jonathan Aseltine</author>
<author>Wendy G Lehnert</author>
</authors>
<title>Crystal: Inducing a conceptual dictionary.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-1995),</booktitle>
<pages>1314--1321</pages>
<contexts>
<context position="28636" citStr="Soderland et al., 1995" startWordPosition="4758" endWordPosition="4761">that models overlapping relations is more effective, and also enables training of a sentence extractor that runs with no aggregate information. While the Riedel et al. approach does include a model of which sentences express relations, it makes significant use of aggregate features that are primarily designed to do entity-level relation predictions and has a less detailed model of extractions at the individual sentence level. Perhaps surprisingly, our model is able to do better at both the sentential and aggregate levels. 8 Related Work Supervised-learning approaches to IE were introduced in (Soderland et al., 1995) and are too numerous to summarize here. While they offer high precision and recall, these methods are unlikely to scale to the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), TEXTRUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and WOE (Wu and Weld, 2010)) can scale to millions of documents, but don‚Äôt output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, </context>
</contexts>
<marker>Soderland, Fisher, Aseltine, Lehnert, 1995</marker>
<rawString>Stephen Soderland, David Fisher, Jonathan Aseltine, and Wendy G. Lehnert. 1995. Crystal: Inducing a conceptual dictionary. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-1995), pages 1314‚Äì1321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Information and Knowledge Management (CIKM-2007),</booktitle>
<pages>41--50</pages>
<contexts>
<context position="29783" citStr="Wu and Weld, 2007" startWordPosition="4944" endWordPosition="4947">n as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007) used a database of BibTex records to train a CRF extractor on 12 bibliographic relations. The KYLIN system aplied weak supervision to learn relations from Wikipedia, treating infoboxes as the associated database (Wu and Weld, 2007); Wu et al. (2008) extended the system to use smoothing over an automatically generated infobox taxon548 omy. Mintz et al. (2009) used Freebase facts to train automatically learn a nearly unbounded number of 100 relational extractors on Wikipedia. Hoffmann relational extractors. Since the processs of matchet al. (2010) describe a system similar to KYLIN, ing database tuples to sentences is inherently heurisbut which dynamically generates lexicons in order tic, researchers have proposed multi-instance learnto handle sparse data, learning over 5000 Infobox ing algorithms as a means for coping wi</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the International Conference on Information and Knowledge Management (CIKM-2007), pages 41‚Äì50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Automatically refining the wikipedia infobox ontology.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web (WWW-2008),</booktitle>
<pages>635--644</pages>
<marker>Wu, Weld, 2008</marker>
<rawString>Fei Wu and Daniel S. Weld. 2008. Automatically refining the wikipedia infobox ontology. In Proceedings of the 17th International Conference on World Wide Web (WWW-2008), pages 635‚Äì644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In The Annual Meeting of the Association for Computational Linguistics (ACL2010),</booktitle>
<pages>118--127</pages>
<contexts>
<context position="29037" citStr="Wu and Weld, 2010" startWordPosition="4824" endWordPosition="4827">ividual sentence level. Perhaps surprisingly, our model is able to do better at both the sentential and aggregate levels. 8 Related Work Supervised-learning approaches to IE were introduced in (Soderland et al., 1995) and are too numerous to summarize here. While they offer high precision and recall, these methods are unlikely to scale to the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), TEXTRUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and WOE (Wu and Weld, 2010)) can scale to millions of documents, but don‚Äôt output canonicalized relations. 8.1 Weak Supervision Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. Bellare and McCallum (2007) used a database of BibTex records to train a CRF extractor on 12 bibliographic relati</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using wikipedia. In The Annual Meeting of the Association for Computational Linguistics (ACL2010), pages 118‚Äì127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective cross-document relation extraction without labelled data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010),</booktitle>
<pages>1013--1023</pages>
<marker>Yao, Riedel, McCallum, 2010</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction without labelled data. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010), pages 1013‚Äì1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007).</booktitle>
<contexts>
<context position="12175" citStr="Zettlemoyer and Collins, 2007" startWordPosition="2034" endWordPosition="2037">tive would be difficult to opthe sentences in E with mentions of this pair, and timize exactly, and algorithms for doing so would yi = relVector(ej, ek). be unlikely to scale to data sets of the size we conComputation: sider. Instead, we make two approximations, deinitialize parameter vector O &lt;-- 0 scribed below, leading to a Perceptron-style addifort = 1...T do tive (Collins, 2002) parameter update scheme which for i = 1...n do has been modified to reason about hidden variables, (y&apos;, z&apos;) &lt;-- arg maxy,z p(y, z|xi; 0) similar in style to the approaches of (Liang et al., if y&apos; =ÔøΩ yi then 2006; Zettlemoyer and Collins, 2007), but adapted z* &lt;-- arg maxz for our specific model. This approximate algorithm p(z|xi, yi; 0) is computationally efficient and, as we will see, O &lt;-- O + O(xi, z*) ‚Äî O(xi, z&apos;) works well in practice. end if Our first modification is to do online learning end for instead of optimizing the full objective. Define the end for feature sums O(x, z) = Ej O(xj, zj) which range Return O over the sentences, as indexed by j. Now, we can Figure 2: The MULTIR Learning Algorithm define an update based on the gradient of the local only the deterministic OR nodes. Perhaps surpris- log likelihood for example</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>