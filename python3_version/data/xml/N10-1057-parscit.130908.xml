<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.077210">
<title confidence="0.9985265">
Predicting Human-Targeted Translation Edit Rate
via Untrained Human Annotators
</title>
<author confidence="0.932334">
Omar F. Zaidan and Chris Callison-Burch
</author>
<affiliation confidence="0.983432">
Dept. of Computer Science, Johns Hopkins University
</affiliation>
<address confidence="0.744496">
Baltimore, MD 21218, USA
</address>
<email confidence="0.999269">
{ozaidan,ccb}@cs.jhu.edu
</email>
<sectionHeader confidence="0.993905" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999775227272727">
In the field of machine translation, automatic
metrics have proven quite valuable in system
development for tracking progress and meas-
uring the impact of incremental changes.
However, human judgment still plays a large
role in the context of evaluating MT systems.
For example, the GALE project uses human-
targeted translation edit rate (HTER), wherein
the MT output is scored against a post-edited
version of itself (as opposed to being scored
against an existing human reference). This
poses a problem for MT researchers, since
HTER is not an easy metric to calculate, and
would require hiring and training human an-
notators to perform the editing task. In this
work, we explore soliciting those edits from
untrained human annotators, via the online
service Amazon Mechanical Turk. We show
that the collected data allows us to predict
HTER-ranking of documents at a significantly
higher level than the ranking obtained using
automatic metrics.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984295454546">
In the early days of machine translation (MT), it
was typical to evaluate MT output by soliciting
judgments from human subjects, such as evaluat-
ing the fluency and adequacy of MT output (LDC,
2005). While this approach was appropriate (in-
deed desired) for evaluating a system, it was not a
practical means of tracking the progress of a sys-
tem during its development, since collecting hu-
man judgments is both costly and time-consuming.
The introduction of automatic metrics like BLEU
contributed greatly to MT research, for instance
allowing researchers to measure and evaluate the
impact of small modifications to an MT system.
However, manual evaluation remains a core
component of system evaluation. Teams on the
GALE project, a DARPA-sponsored MT research
program, are evaluated using the HTER metric,
which is a version of TER whereby the output is
scored against a post-edited version of itself, in-
stead of a preexisting reference. Moreover, empha-
sis is placed on performing well across all
documents and across all genres. Therefore, it is
important for a research team to be able to evaluate
their system using HTER, or at least determine the
ranking of the documents according to HTER, for
purposes of error analysis. Instead of hiring a
human translator and training them, we propose
moving the task to the virtual world of Amazon’s
Mechanical Turk (AMT), hiring workers to edit the
MT output and predict HTER from those edits. We
show that edits collected this way are better at
predicting document ranking than automatic
metrics, and furthermore that it can be done at a
low cost, both in terms of time and money.
The paper is organized as follows. We first
discuss options available to predict HTER, such as
automatic metrics. We then discuss the possibility
of relying on human annotators, and the inherent
difficulty in training them, before discussing the
concept of soliciting edits over AMT. We detail
the task given to the workers and summarize the
data that we collected, then show how we can
combine their data to obtain significanly better
rank predictions of documents.
</bodyText>
<sectionHeader confidence="0.996968" genericHeader="method">
2 Human-Targeted TER
</sectionHeader>
<bodyText confidence="0.999407428571429">
Translation edit rate (TER) measures the number
of edits required to transform a hypothesis into an
appropriate sentence in terms of grammaticality
and meaning (Snover et al., 2006). While TER
usually scores a hypothesis against an existing ref-
erence sentence, human-targeted TER scores a
hypothesis against a post-edited version of itself.
</bodyText>
<page confidence="0.960424">
369
</page>
<subsubsectionHeader confidence="0.759052">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 369–372,
</subsubsectionHeader>
<bodyText confidence="0.912548625">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
While HTER has been shown to correlate quite
well with human judgment of MT quality, it is
quite challenging to obtain HTER scores for MT
output, since this would require hiring and training
human subjects to perform the editing task. There-
fore, other metrics such as BLEU or TER are used
as proxies for HTER.
</bodyText>
<subsectionHeader confidence="0.997825">
2.1 Amazon&apos;s Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.99997604">
The high cost associated with hiring and training a
human editor makes it difficult to imagine an alter-
native to automatic metrics. However, we propose
soliciting edits from workers on Amazon’s Me-
chanical Turk (AMT). AMT is a virtual market-
place where “requesters” can post tasks to be
completed by “workers” (aka Turkers) around the
world. Two main advantages of AMT are the pre-
existing infrastructure, and the low cost of com-
pleting tasks, both in terms of time and money.
Data collected over AMT has already been used in
several papers such as Snow et al. (2008) and Cal-
lison-Burch (2009).
When a requester creates a task to be completed
over AMT, it is typical to have completed by more
than one worker. The reason is that the use of
AMT for data collection has an inherent problem
with data quality. A requester has fewer tools at
their disposal to ensure workers are doing the task
properly (via training, feedback, etc) when com-
pared to hiring annotators in the ‘real’ world.
Those redundant annotations are therefore col-
lected to increase the likelihood of at least one
submission from a faithful (and competent)
worker.
</bodyText>
<subsectionHeader confidence="0.965197">
2.2 AMT for HTER
</subsectionHeader>
<bodyText confidence="0.999997307692308">
The main idea it to mimic the real-world HTER
setup by supplying workers with the original MT
output that needs to be edited. The worker is also
given a human reference, produced independently
from the MT output. The instructions ask the
worker to modify the MT output, using as few ed-
its as possible, to match the human reference in
meaning and grammaticality.
The submitted edited hypothesis can then be
used as the reference for calculating HTER. The
idea is that, with this setup, a competent worker
would be able to closely match the editing behav-
ior of the professionally trained editor.
</bodyText>
<sectionHeader confidence="0.994189" genericHeader="method">
3 The Datasets
</sectionHeader>
<bodyText confidence="0.9990858">
We solicited edits of the output from one of
GALE’s teams on the Arabic-to-English task. This
MT output was submitted by this team and HTER-
scored by LDC-hired human translators. Therefore,
we already had the edits produced by a
professional translator. These edits were used as
the “gold-standard” to evaluate the edits solicited
from AMT and to evaluate our methods of
combining Turkers’ submissions.
The MT output is a translation of more than
2,153 Arabic segments spread across 195 docu-
ments in 4 different genres: broadcast conversa-
tions (BC), broadcast news (BN), newswire (NW),
and blogs (WB). Table 1 gives a summary of each
genre’s dataset.
</bodyText>
<table confidence="0.9998058">
Genre # docs Segs/doc Words/seg
BC 40 15.8 28.3
BN 48 9.6 36.1
NW 54 8.7 39.5
WB 53 11.1 31.6
</table>
<tableCaption confidence="0.99997">
Table 1: The 4 genres of the dataset.
</tableCaption>
<bodyText confidence="0.999910555555556">
For each of the 2,153 MT output segments, we
collected edits from 5 distinct workers on AMT,
for a total of 10,765 post-edited segments by a total
of about 500 distinct workers.1 The segments were
presented in 1,210 groups of up to 15 segments
each, with a reward of $0.25 per group. Hence the
total rewards to workers was around $300, at a rate
of 36 post-edited segments per dollar (or 2.8 pen-
nies per segment).
</bodyText>
<sectionHeader confidence="0.941245" genericHeader="method">
4 What are we measuring?
</sectionHeader>
<bodyText confidence="0.999981444444444">
We are interested in predicting the ranking the
documents according to HTER, not necessarily
predicting the HTER itself (though of course at-
tempting to predict the latter accurately is the cor-
nerstone of our approach to predict the former). To
measure the quality of a predicted ranking, we use
Spearman’s rank correlation coefficient, ρ, where
we first convert the raw scores into ranks and then
use the following formula to measure correlation:
</bodyText>
<equation confidence="0.970153">
(rank (x,) − rank(y, )) 2
n(n
1 Data available at http://cs.jhu.edu/-ozaidan/hter.
n
6
E
P (X, Y) =1− ,=1
2 − 1 )
</equation>
<page confidence="0.963673">
370
</page>
<bodyText confidence="0.999344">
where n is the number of documents, and each of X
and Y is a vector of n HTER scores.
Notice that values for ρ range from –1 to 1, with
+1 indicating perfect rank correlation, –1 perfect
inverse correlation, and 0 no correlation. That is,
for a fixed X, the best-correlated Y is that for which
ρ(X,Y) is highest.
</bodyText>
<sectionHeader confidence="0.968724" genericHeader="method">
5 Combining Tukers&apos; Edits
</sectionHeader>
<bodyText confidence="0.999987148148148">
Once we have collected edits from the human
workers, how should we attempt to predict HTER
from them? If we could assume that all Turkers are
doing the task faithfully (and doing it adequately),
we should use the annotations of the worker per-
forming the least amount of editing, since that
would mirror the real-life scenario.
However, data collected from AMT should be
treated with caution, since a non-trivial portion of
the collected data is of poor quality. Note that this
does not necessarily indicate a ‘cheating’ worker,
for even if a worker is acting in good faith, they
might not be able to perform the task adequately,
due to misunderstanding the task, or neglecting to
attempt to use a small number of edits.
And so we need to combine the redundant edits in
an intelligent manner. Recall that, given a segment,
we collected edits from multiple workers. Some
baseline methods include taking the minimum over
the edits, taking the median, and taking the aver-
age.
Once we start thinking of averages, we should
consider taking a weighted average of the edits for
a segment. The weight associated with a worker
should reflect our confidence in the quality of that
worker’s edits. But how can we evaluate a worker
in the first place?
</bodyText>
<subsectionHeader confidence="0.998772">
5.1 Self Verification of Turkers
</subsectionHeader>
<bodyText confidence="0.999970952380952">
We have available “gold-standard” editing behav-
ior for the segments, and we treat a small portion
of the segments edited by a Turker as a verification
dataset. On that portion, we evaluate how closely
the Turker matches the LDC editor, and weight
them accordingly when predicting the number of
edits of the rest of that group’s segments. Specifi-
cally, the Turker’s weight is the absolute difference
between the Turker’s edit count and the profes-
sional editor’s edit count.
Notice that we are not simply interested in a
worker whose edited submission closely matches
the edited submission of the professional transla-
tor. Rather, we are interested in mirroring the pro-
fessional translator’s edit rate. That is, the closer a
Turker’s edit rate is to the LDC editor’s, the more
we should prefer the worker. This is a subtle point,
but it is indeed possible for a Turker to have simi-
lar edit rate as the LDC editor but still require a
large number of edits to get the LDC editor’s sub-
mission itself.
</bodyText>
<sectionHeader confidence="0.998672" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999992125">
We examine the effectiveness of any of the above
methods by comparing the resulting document
ranking versus the desired ranking by HTER. In
addition to the above methods, we use a baseline a
ranking predicted by TER to a human reference.
(For clarity, we omit discussion with other metrics
such as BLEU and (TER–BLEU)/2, since those
baselines are not as strong as the TER baseline.
</bodyText>
<subsectionHeader confidence="0.981598">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999145818181818">
We examine each genre individually, since genres
vary quite a bit in difficulty, and, more impor-
tantly, we care about the internal ranking within
each genre, to mirror the GALE evaluation proce-
dure.
We examine the effect of varying the amount of
data by which we judge a Turker’s data quality.
The amount of this “verification” data is varied as
a percentage of the total available segments. Those
segments are chosen at random, and we perform
100 trials for each point.
</bodyText>
<subsectionHeader confidence="0.997399">
6.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.998565692307692">
Figure 1 shows the rank correlations for various
methods across different sizes of verification sub-
sets. Notice that some methods, such as the TER
baseline, have horizontal lines, since these do not
rate a Turker based on a verification subset.
It is worth noting that the oracle performs very
well. This is an indication that predicting HTER
accurately is mostly a matter of identifying the best
worker. While oracle scenarios usually represent
unachievable upper bounds, keep in mind that
there are only a very small number of editors per
segment (five, as opposed to oracle scenarios deal-
ing with 100-best lists, etc).
</bodyText>
<page confidence="0.995683">
371
</page>
<bodyText confidence="0.999924444444445">
Other than that, in general, it is possible to
achieve very high rank correlation using Turkers’
data, significantly outperforming the TER ranking,
even with a small verification subset. The genres
do vary quite a bit in difficulty for Turkers, with
BC and especially NW being quite difficult,
though in the case of NW for instance, this is due
to the human reference doing quite well to begin
with, rather than Turkers performing poorly.
</bodyText>
<sectionHeader confidence="0.996965" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999991176470588">
We proposed soliciting edits of MT output via
Amazon’s Mechanical Turk and showed we can
predict ranking significantly better than an auto-
matic metric. The next step is to explicitly identify
undesired worker behavior, such as not editing the
MT output at all, or using the human reference as
is instead of editing the MT output. This can be
detected by not limiting our verification to compar-
ing behavior to the professional editor’s, but also
by comparing submitted edits to the MT output
itself and to the human reference. In other words, a
worker’s submission could be characterized in
terms of its distance to the MT output and to the
human reference, thus building a complete ‘pro-
file’ of the worker, and adding another component
to guard against poor data quality and to reward
the desired behavior.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999458">
This work was supported by the EuroMatrixPlus
Project (funded by the European Commission), and
by the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors&apos; alone.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996976230769231">
Chris Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon&apos;s Me-
chanical Turk. In Proceedings of EMNLP.
LDC. 2005. Linguistic data annotation specification:
Assessment offluency and adequacy in translations.
Revision 1.5.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz.
2006. A Study of Translation Edit Rate with Targeted
Human Annotation. Proceedings of AMTA.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast — but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP.
</reference>
<figureCaption confidence="0.81051525">
Figure 1: Rank correlation between predicted rank-
ing and HTER ranking for different prediction
schemes, across the four genres, and across various
sizes of the worker verification set.
</figureCaption>
<page confidence="0.995155">
372
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933360">
<title confidence="0.975914">Predicting Human-Targeted Translation Edit via Untrained Human Annotators</title>
<author confidence="0.999382">F Zaidan</author>
<affiliation confidence="0.999251">Dept. of Computer Science, Johns Hopkins</affiliation>
<address confidence="0.991568">Baltimore, MD 21218,</address>
<email confidence="0.999819">ozaidan@cs.jhu.edu</email>
<email confidence="0.999819">ccb@cs.jhu.edu</email>
<abstract confidence="0.999544260869565">In the field of machine translation, automatic metrics have proven quite valuable in system development for tracking progress and measuring the impact of incremental changes. However, human judgment still plays a large in the context of systems. For example, the GALE project uses humantargeted translation edit rate (HTER), wherein the MT output is scored against a post-edited version of itself (as opposed to being scored against an existing human reference). This poses a problem for MT researchers, since HTER is not an easy metric to calculate, and would require hiring and training human annotators to perform the editing task. In this work, we explore soliciting those edits from annotators, via the online service Amazon Mechanical Turk. We show that the collected data allows us to predict HTER-ranking of documents at a significantly higher level than the ranking obtained using automatic metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon&apos;s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4775" citStr="Callison-Burch (2009)" startWordPosition="764" endWordPosition="766"> 2.1 Amazon&apos;s Mechanical Turk The high cost associated with hiring and training a human editor makes it difficult to imagine an alternative to automatic metrics. However, we propose soliciting edits from workers on Amazon’s Mechanical Turk (AMT). AMT is a virtual marketplace where “requesters” can post tasks to be completed by “workers” (aka Turkers) around the world. Two main advantages of AMT are the preexisting infrastructure, and the low cost of completing tasks, both in terms of time and money. Data collected over AMT has already been used in several papers such as Snow et al. (2008) and Callison-Burch (2009). When a requester creates a task to be completed over AMT, it is typical to have completed by more than one worker. The reason is that the use of AMT for data collection has an inherent problem with data quality. A requester has fewer tools at their disposal to ensure workers are doing the task properly (via training, feedback, etc) when compared to hiring annotators in the ‘real’ world. Those redundant annotations are therefore collected to increase the likelihood of at least one submission from a faithful (and competent) worker. 2.2 AMT for HTER The main idea it to mimic the real-world HTER</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon&apos;s Mechanical Turk. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>Linguistic data annotation specification: Assessment offluency and adequacy in translations.</title>
<date>2005</date>
<journal>Revision</journal>
<volume>1</volume>
<contexts>
<context position="1386" citStr="LDC, 2005" startWordPosition="211" endWordPosition="212">sy metric to calculate, and would require hiring and training human annotators to perform the editing task. In this work, we explore soliciting those edits from untrained human annotators, via the online service Amazon Mechanical Turk. We show that the collected data allows us to predict HTER-ranking of documents at a significantly higher level than the ranking obtained using automatic metrics. 1 Introduction In the early days of machine translation (MT), it was typical to evaluate MT output by soliciting judgments from human subjects, such as evaluating the fluency and adequacy of MT output (LDC, 2005). While this approach was appropriate (indeed desired) for evaluating a system, it was not a practical means of tracking the progress of a system during its development, since collecting human judgments is both costly and time-consuming. The introduction of automatic metrics like BLEU contributed greatly to MT research, for instance allowing researchers to measure and evaluate the impact of small modifications to an MT system. However, manual evaluation remains a core component of system evaluation. Teams on the GALE project, a DARPA-sponsored MT research program, are evaluated using the HTER </context>
</contexts>
<marker>LDC, 2005</marker>
<rawString>LDC. 2005. Linguistic data annotation specification: Assessment offluency and adequacy in translations. Revision 1.5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>Proceedings of AMTA.</booktitle>
<contexts>
<context position="3483" citStr="Snover et al., 2006" startWordPosition="551" endWordPosition="554">ss options available to predict HTER, such as automatic metrics. We then discuss the possibility of relying on human annotators, and the inherent difficulty in training them, before discussing the concept of soliciting edits over AMT. We detail the task given to the workers and summarize the data that we collected, then show how we can combine their data to obtain significanly better rank predictions of documents. 2 Human-Targeted TER Translation edit rate (TER) measures the number of edits required to transform a hypothesis into an appropriate sentence in terms of grammaticality and meaning (Snover et al., 2006). While TER usually scores a hypothesis against an existing reference sentence, human-targeted TER scores a hypothesis against a post-edited version of itself. 369 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 369–372, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics While HTER has been shown to correlate quite well with human judgment of MT quality, it is quite challenging to obtain HTER scores for MT output, since this would require hiring and training human subjects to perform the editing task. The</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2006</marker>
<rawString>Matthew Snover, Bonnie J. Dorr, Richard Schwartz. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast — but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast — but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>