<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003535">
<title confidence="0.997767">
Where&apos;s the Verb?
Correcting Machine Translation During Question Answering
</title>
<author confidence="0.999834">
Wei-Yun Ma, Kathleen McKeown
</author>
<affiliation confidence="0.99724">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.973243">
New York, NY 10027, USA
</address>
<email confidence="0.998198">
{ma,kathy}@cs.columbia.edu
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993046875">
When a multi-lingual question-answering (QA)
system provides an answer that has been
incorrectly translated, it is very likely to be
regarded as irrelevant. In this paper, we
propose a novel method for correcting a
deletion error that affects overall
understanding of the sentence. Our post-editing
technique uses information available at query
time: examples drawn from related documents
determined to be relevant to the query. Our
results show that 4%-7% of MT sentences are
missing the main verb and on average, 79% of
the modified sentences are judged to be more
comprehensible. The QA performance also
benefits from the improved MT: 7% of
irrelevant response sentences become relevant.
</bodyText>
<sectionHeader confidence="0.998218" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999961295454545">
We are developing a multi-lingual question-
answering (QA) system that must provide
relevant English answers for a given query,
drawing pieces of the answer from translated
foreign source. Relevance and translation quality
are usually inseparable: an incorrectly translated
sentence in the answer is very likely to be
regarded as irrelevant even when the
corresponding source language sentence is
actually relevant. We use a phrase-based
statistical machine translation system for the MT
component and thus, for us, MT serves as a
black box that produces the translated
documents in our corpus; we cannot change the
MT system itself. As MT is used in more and
more multi-lingual applications, this situation
will become quite common.
We propose a novel method which uses
redundant information available at question-
answering time to correct errors. We present a
post-editing mechanism to both detect and
correct errors in translated documents
determined to be relevant for the response. In
this paper, we focus on cases where the main
verb of a Chinese sentence has not been
translated. The main verb usually plays a crucial
role in conveying the meaning of a sentence. In
cases where only the main verb is missing, an
MT score relying on edit distance (e.g., TER or
Bleu) may be high, but the sentence may
nonetheless be incomprehensible.
Handling this problem at query time rather
than during SMT gives us valuable information
which was not available during SMT, namely, a
set of related sentences and their translations
which may contain the missing verb. By using
translation examples of verb phrases and
alignment information in the related documents,
we are able to find an appropriate English verb
and embed it in the right position as the main
verb in order to improve MT quality.
A missing main verb can result in an incom-
prehensible sentence as seen here where the
Chinese verb “被捕” was not translated at all.
</bodyText>
<equation confidence="0.846712666666667">
MT: On December 13 Saddam .
REF : On December 13 Saddam was arrested.
Chinese: 12月13日萨达姆被捕。
</equation>
<bodyText confidence="0.998912375">
In other cases, a deleted main verb can result
in miscommunication; below the Chinese verb
“减退” should have been translated as
“reduced”. An English native speaker could
easily misunderstand the meaning to be “People
love classical music every year.” which happens
to be the opposite of the original intended
meaning.
</bodyText>
<listItem confidence="0.333981">
MT: People of classical music loving every year.
REF : People’s love for classical music reduced every year.
Chinese: 民众对古典音乐的热爱逐年减退。
</listItem>
<sectionHeader confidence="0.999563" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.9976325">
Post-editing has been used in full MT systems
for tasks such as article selection (a, an, the) for
</bodyText>
<page confidence="0.992187">
333
</page>
<note confidence="0.925979">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 333–336,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999781916666667">
English noun phrases (Knight and Chander
1994). Simard et al in 2007 even developed a
statistical phrase based MT system in a post-
editing task, which takes the output of a rule-
based MT system and produces post-edited
target-language text. Zwarts et al. (2008) target
selecting the best of a set of outputs from
different MT systems through their
classification-based approach. Others have also
proposed using the question-answering context
to detect errors in MT, showing how to correct
names (Parton et. al 2008, Ji et. al 2008).
</bodyText>
<sectionHeader confidence="0.89722" genericHeader="method">
3. System Overview
</sectionHeader>
<bodyText confidence="0.999783976190476">
The architecture of our QA system is shown in
Figure 1. Our MT post-editing system (the bold
block in Figure 1) runs after document retrieval
has retrieved all potentially relevant documents
and before the response generator selects
sentences for the answer. It modifies any MT
documents retrieved by the embedded
information retrieval system that are missing a
main verb. All MT results are provided by a
phrase-based SMT system.
Post-editing includes three steps: detect a
clause with a missing main verb, determine
which Chinese verb should have been translated,
and find an example sentence in the related
documents with an appropriate sentence which
can be used to modify the sentence in question.
To detect clauses, we first tag the corpus using a
Conditional Random Fields (CRF) POS tagger
and then use manually designed regular
expressions to identify main clauses of the
sentence, subordinate clauses (i.e., clauses which
are arguments to a verb) and conjunct clauses in
a sentence with conjunction. We do not handle
adjunct clauses. Hereafter, we simply refer to all
of these as “clause”. If a clause does not have
any POS tag that can serve as a main verb (VB,
VBD, VBP, VBZ), it is marked as missing a
main verb.
MT alignment information is used to further
ensure that these marked clauses are really
missing main verbs. We segment and tag the
Chinese source sentence using the Stanford
Chinese segmenter and the CRF Chinese POS
tagger developed by Purdue University. If we
find a verb phrase in the Chinese source
sentence that was not aligned with any English
words in the SMT alignment tables, then we
label it as a verb translation gap (VTG) and
confirm that the marking was correct.
In the following sections, we describe how we
determine which Chinese verb should have been
translated and how that occurs.
</bodyText>
<figure confidence="0.687653333333333">
Modified English docs
Response Generator
Response in English
</figure>
<figureCaption confidence="0.999889">
Figure 1. The System Pipeline
</figureCaption>
<sectionHeader confidence="0.684191" genericHeader="method">
4. Finding the Main Verb Position
</sectionHeader>
<bodyText confidence="0.999965590909091">
Chinese ordering differs from English mainly
in clause ordering (Wang et al., 2007) and
within the noun phrase. But within a clause
centered by a verb, Chinese mostly uses a SVO
or SV structure, like English (Yamada and
Knight 2001), and we can assume the local
alignment centered by a verb between Chinese
and English is a linear mapping relation. Under
this assumption, the translation of “被捕” in the
above example should be placed in the position
between “Saddam” and “.”. Thus, once we find a
VTG, its translation can be inserted into the
corresponding position of the target sentence
using the alignment.
This assumes, however, that there is only one
VTG found within a clause. In practice, more
than one VTG may be found in a clause. If we
choose one of them, we risk making the wrong
choice. Instead, we insert the translations of both
VTGs simultaneously. This strategy could result
in more than one main verb in a clause, but it is
more helpful than having no verb at all.
</bodyText>
<sectionHeader confidence="0.833959" genericHeader="method">
5. Obtaining a VTG Translation
</sectionHeader>
<bodyText confidence="0.999606888888889">
We translate VTGs by using verb redundancy
in related documents: if the VTG was translated
in other places in related documents, the existing
translations can be reused. Related documents
are likely to use a good translation for a specific
VTG as it is used in a similar context. A verb’s
aspect and tense can be directly determined by
referencing the corresponding MT examples and
their contexts. If, unfortunately, a given VTG
</bodyText>
<figure confidence="0.568445117647059">
Query in English
Corpus of translated
English documents with
Chinese-English word
alignment
Static Verb
Phrase Table
Detecting Possible Clauses
with no Main Verb
Obtain Translation of the Main
Verb and embed it to the
translated sentence
Finding the Main Verb Position
Document Retrieval
Retrieved English docs
Dynamic Verb
Phrase Table
</figure>
<page confidence="0.995876">
334
</page>
<bodyText confidence="0.9998589">
did not have any other translation record, then
the VTG will not be processed.
To do this, our system first builds verb phrase
tables from relevant documents and then uses
the tables to translate the VTG. We use two verb
phrase tables: one is built from a collection of
MT documents before any query and is called
the “Static Verb Phrase Table”, and the other
one is dynamically built from the retrieved
relevant MT documents for each query and is
called the “Dynamic Verb Phrase Table”.
The construction procedure is the same for
both. Given a set of related MT documents and
their MT alignments, we collect all Chinese verb
phrases and their translations along with their
frequencies and contexts.
One key issue is to decide appropriate
contextual features of a verb. A number of
researchers (Cabezas and Resnik 2005, Carpuat
and Wu 2007) provide abundant evidence that
rich context features are useful in MT tasks.
Carpuat and Wu (2007) tried to integrate a
Phrase Sense Disambiguation (PSD) model into
their Chinese-English SMT system and they
found that the POS tag preceding a given phrase,
the POS tag following the phrase and bag-of-
words are the three most useful features.
Following their approach, we use the word
preceding and the word following a verb as the
context features.
The Static and Dynamic Verb Phrase Tables
provide us with MT examples to translate a
VTG. The system first references the Dynamic
Verb Phrase Table as it is more likely to yield a
good translation. If the record is not found, the
Static one is referenced. If it is not found in
either, the given VTG will not be processed. No
matter which table is referenced, the following
Naive Bayes equation is applied to obtain the
translation of a given VTG.
</bodyText>
<equation confidence="0.994495">
Pt( ) log (  |) log ( |
+ P pw t + P fw t
k k k
tk
</equation>
<bodyText confidence="0.998796">
pw, fw and tk respectively represent the
preceding source word, the following source
word and a translation candidate of a VTG.
</bodyText>
<sectionHeader confidence="0.999352" genericHeader="evaluation">
6. Experiments
</sectionHeader>
<bodyText confidence="0.999968">
Our test data is drawn from Chinese-English MT
results generated by Aachen’s 2007 RWTH sys-
tem (Mauser et al., 2007), a phrase-based SMT
system with 38.5% BLEU score on IWSLT
2007 evaluation data.
Newswires and blog articles are retrieved for
five queries which served as our experimental
test bed. The queries are open-ended and on av-
erage, answers were 30 sentences in length.
</bodyText>
<listItem confidence="0.935778285714286">
Q1: Who/What is involved in Saddam Hussein&apos;s trial
Q2: Produce a biography of Jacques Rene Chirac
Q3: Describe arrests of person from Salafist Group for
Preaching and Combat
Q4: Provide information on Chen Sui Bian
Q5: What connections are there between World Cup games and
stock markets?
</listItem>
<bodyText confidence="0.999741285714286">
We used MT documents retrieved by IR for
each query to build the Dynamic Verb Phrase
Table. We tested the system on 18,886 MT
sentences from the retrieved MT documents for
all of the five queries. Among these MT
sentences, 1,142 sentences were detected and
modified (6 % of all retrieved MT sentences).
</bodyText>
<subsectionHeader confidence="0.995867">
6.1 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999992967741935">
For evaluation, we used human judgments of the
modified and original MT. We did not have
reference translations for the data used by our
question-answering system and thus, could not
use metrics such as TER or Bleu. Moreover, at
best, TER or Bleu score would increase by a
small amount and that is only if we select the
same main verb in the same position as the
reference. Critically, we also know that a
missing main verb can cause major problems
with comprehension. Thus, readers could better
determine if the modified sentence better
captured the meaning of the source sentence. We
also evaluated relevance of a sentence to a query
before and after modification.
We recruited 13 Chinese native speakers who
are also proficient in English to judge MT
quality. Native English speakers cannot tell
which translation is better since they do not
understand the meaning of the original Chinese.
To judge relevance to the query, we used native
English speakers.
Each modified sentence was evaluated by
three people. They were shown the Chinese
sentence and two translations, the original MT
and the modified one. Evaluators did not know
which MT sentence was modified. They were
asked to decide which sentence is a better
translation, after reading the Chinese sentence.
An evaluator also had the option of answering
“no difference”.
</bodyText>
<subsectionHeader confidence="0.981335">
6.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999594333333333">
We used majority voting (two out of three) to
decide the final evaluation of a sentence judged
by three people. On average, 900 (79%) of the
</bodyText>
<equation confidence="0.987516714285714">
tk
t = argmaxP(tk
 |, )
pw fw
&apos;
= argmax(log
))
</equation>
<page confidence="0.991537">
335
</page>
<bodyText confidence="0.969009176470588">
1142 modified sentences, which comprise 5% of
all 18,886 retrieved MT sentences, are better
than the original sentences based on majority
voting. And for 629 (70%) of these 900 better
modified sentences all three evaluators agreed
that the modified sentence is better.
Furthermore, we found that for every
individual query, the evaluators preferred more
of the modified sentences than the original MT.
And among these improved sentences, 81%
sentences reference the Dynamic Verb Phrase
Table, while only 19% sentences had to draw
from the Static Verb Phrase Table, thus
demonstrating that the question answering
context is quite helpful in improving MT.
We also evaluated the impact of post-editing
on the 234 sentences returned by our response
generator. In our QA task, response sentences
were judged as “Relevant(R)”, “Partially
Relevant(PR)”, “Irrelevant(I)” and “Too little
information to judge(T)” sentences. With our
post-editing technique, 7% of 141 I/T responses
become R/PR responses and none of the R/PR
responses become I/T responses. This means
that R/PR response percentage has an increase of
4%, thus demonstrating that our correction of
MT truly improves QA performance. An
example of a change from T to PR is:
Question: What connections are there between World Cup games
and stock markets?
Original QA answer: But if winning the ball, not necessarily in
the stock market.
Modified QA answer: But if winning the ball, not necessarily in
the stock market increased.
</bodyText>
<subsectionHeader confidence="0.999621">
6.3 Analysis of Different MT Systems
</subsectionHeader>
<bodyText confidence="0.9999476">
In order to examine how often missing verbs
occur in different recent MT systems, in addition
to using Aachen’s up-to-date system – “RWTH-
PBT”of 2008, we also ran the detection process
for another state-of-the-art MT system – “SRI-
HPBT” (Hierarchical Phrase-Based System) of
2008 provided by SRI, which uses a grammar on
the target side as well as reordering, and focuses
on improving grammaticality of the target
language. Based on a government 2008 MT
evaluation, the systems achieve 30.3% and
30.9% BLEU scores respectively. We used the
same test set, which includes 94 written articles
(953 sentences).
Overall, 7% of sentences translated by
RWTH-PBT are detected with missing verbs
while 4% of sentences translated by SRI-HPBT
are detected with missing verb. This shows that
while MT systems improve every year, missing
verbs remain a problem.
</bodyText>
<sectionHeader confidence="0.998729" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999969416666667">
In this paper, we have presented a technique for
detecting and correcting deletion errors in trans-
lated Chinese answers as part of a multi-lingual
QA system. Our approach uses a regular gram-
mar and alignment information to detect missing
verbs and draws from examples in documents
determined to be relevant to the query to insert a
new verb translation. Our evaluation demon-
strates that MT quality and QA performance are
both improved. In the future, we plan to extend
our approach to tackle other MT error types by
using information available at query time.
</bodyText>
<sectionHeader confidence="0.998273" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.895519333333333">
This material is based upon work supported
by the Defense Advanced Research Projects
Agency under Contract No. HR0011-06-C-0023
</bodyText>
<sectionHeader confidence="0.991262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997565764705882">
Clara Cabezas and Philip Resnik. 2005. Using WSD
Techniques for Lexical Selection in Statistical
Machine, Translation Technical report CS-TR-
4736
Marine Carpuat and Dekai Wu. 2007. Context-
Dependent Phrasal Translation Lexicons for
Statistical Machine Translation, Machine
Translation Summit XI, Copenhagen
Heng Ji, Ralph Grishman and Wen Wang. 2008.
Phonetic Name Matching For Cross-lingual
Spoken Sentence Retrieval, IEEE-ACL SLT08.
Goa, India
K. Knight and I. Chander. 1994. Automated
Postediting of Documents, AAAI
Kristen Parton, Kathleen R. McKeown, James Allan,
and Enrique Henestroza. 2008. Simultaneous
multilingual search for translingual information
retrieval, ACM 17th CIKM
Arne Mauser, David Vilar, Gregor Leusch, Yuqi
Zhang, and Hermann Ney. 2007. The RWTH
Machine Translation System for IWSLT 2007,
IWSLT
Michel Simard, Cyril Goutte and Pierre Isabelle.
2007. Statistical Phrase-based Post-Editing,
NAACL-HLT
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese Syntactic Reordering for
Statistical Machine Translation, EMNLP-
CoNLL.
Kenji Yamada , Kevin Knight. 2001. A syntax-based
statistical translation model, ACL
S. Zwarts and M. Dras. 2008. Choosing the Right
Translation: A Syntactically Informed
Approach, COLING
</reference>
<page confidence="0.999089">
336
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.991567">
<title confidence="0.998885">Where&apos;s the Verb? Correcting Machine Translation During Question Answering</title>
<author confidence="0.999945">Wei-Yun Ma</author>
<author confidence="0.999945">Kathleen McKeown</author>
<affiliation confidence="0.999902">Department of Computer Science Columbia University</affiliation>
<address confidence="0.99984">New York, NY 10027, USA</address>
<email confidence="0.999789">ma@cs.columbia.edu</email>
<email confidence="0.999789">kathy@cs.columbia.edu</email>
<abstract confidence="0.999588470588235">When a multi-lingual question-answering (QA) system provides an answer that has been incorrectly translated, it is very likely to be regarded as irrelevant. In this paper, we propose a novel method for correcting a deletion error that affects overall understanding of the sentence. Our post-editing technique uses information available at query time: examples drawn from related documents determined to be relevant to the query. Our results show that 4%-7% of MT sentences are missing the main verb and on average, 79% of the modified sentences are judged to be more comprehensible. The QA performance also benefits from the improved MT: 7% of irrelevant response sentences become relevant.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Clara Cabezas</author>
<author>Philip Resnik</author>
</authors>
<title>Using WSD Techniques for Lexical Selection in Statistical Machine, Translation</title>
<date>2005</date>
<tech>Technical report CS-TR4736</tech>
<contexts>
<context position="8732" citStr="Cabezas and Resnik 2005" startWordPosition="1426" endWordPosition="1429"> the VTG. We use two verb phrase tables: one is built from a collection of MT documents before any query and is called the “Static Verb Phrase Table”, and the other one is dynamically built from the retrieved relevant MT documents for each query and is called the “Dynamic Verb Phrase Table”. The construction procedure is the same for both. Given a set of related MT documents and their MT alignments, we collect all Chinese verb phrases and their translations along with their frequencies and contexts. One key issue is to decide appropriate contextual features of a verb. A number of researchers (Cabezas and Resnik 2005, Carpuat and Wu 2007) provide abundant evidence that rich context features are useful in MT tasks. Carpuat and Wu (2007) tried to integrate a Phrase Sense Disambiguation (PSD) model into their Chinese-English SMT system and they found that the POS tag preceding a given phrase, the POS tag following the phrase and bag-ofwords are the three most useful features. Following their approach, we use the word preceding and the word following a verb as the context features. The Static and Dynamic Verb Phrase Tables provide us with MT examples to translate a VTG. The system first references the Dynamic</context>
</contexts>
<marker>Cabezas, Resnik, 2005</marker>
<rawString>Clara Cabezas and Philip Resnik. 2005. Using WSD Techniques for Lexical Selection in Statistical Machine, Translation Technical report CS-TR4736</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>ContextDependent Phrasal Translation Lexicons for Statistical Machine Translation, Machine Translation Summit XI,</title>
<date>2007</date>
<location>Copenhagen</location>
<contexts>
<context position="8754" citStr="Carpuat and Wu 2007" startWordPosition="1430" endWordPosition="1433"> phrase tables: one is built from a collection of MT documents before any query and is called the “Static Verb Phrase Table”, and the other one is dynamically built from the retrieved relevant MT documents for each query and is called the “Dynamic Verb Phrase Table”. The construction procedure is the same for both. Given a set of related MT documents and their MT alignments, we collect all Chinese verb phrases and their translations along with their frequencies and contexts. One key issue is to decide appropriate contextual features of a verb. A number of researchers (Cabezas and Resnik 2005, Carpuat and Wu 2007) provide abundant evidence that rich context features are useful in MT tasks. Carpuat and Wu (2007) tried to integrate a Phrase Sense Disambiguation (PSD) model into their Chinese-English SMT system and they found that the POS tag preceding a given phrase, the POS tag following the phrase and bag-ofwords are the three most useful features. Following their approach, we use the word preceding and the word following a verb as the context features. The Static and Dynamic Verb Phrase Tables provide us with MT examples to translate a VTG. The system first references the Dynamic Verb Phrase Table as </context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. ContextDependent Phrasal Translation Lexicons for Statistical Machine Translation, Machine Translation Summit XI, Copenhagen</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Wen Wang</author>
</authors>
<date>2008</date>
<booktitle>Phonetic Name Matching For Cross-lingual Spoken Sentence Retrieval, IEEE-ACL SLT08.</booktitle>
<location>Goa, India</location>
<marker>Ji, Grishman, Wang, 2008</marker>
<rawString>Heng Ji, Ralph Grishman and Wen Wang. 2008. Phonetic Name Matching For Cross-lingual Spoken Sentence Retrieval, IEEE-ACL SLT08. Goa, India</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automated Postediting of Documents,</title>
<date>1994</date>
<publisher>AAAI</publisher>
<contexts>
<context position="3678" citStr="Knight and Chander 1994" startWordPosition="579" endWordPosition="582"> as “reduced”. An English native speaker could easily misunderstand the meaning to be “People love classical music every year.” which happens to be the opposite of the original intended meaning. MT: People of classical music loving every year. REF : People’s love for classical music reduced every year. Chinese: 民众对古典音乐的热爱逐年减退。 2. Related Work Post-editing has been used in full MT systems for tasks such as article selection (a, an, the) for 333 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 333–336, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP English noun phrases (Knight and Chander 1994). Simard et al in 2007 even developed a statistical phrase based MT system in a postediting task, which takes the output of a rulebased MT system and produces post-edited target-language text. Zwarts et al. (2008) target selecting the best of a set of outputs from different MT systems through their classification-based approach. Others have also proposed using the question-answering context to detect errors in MT, showing how to correct names (Parton et. al 2008, Ji et. al 2008). 3. System Overview The architecture of our QA system is shown in Figure 1. Our MT post-editing system (the bold blo</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>K. Knight and I. Chander. 1994. Automated Postediting of Documents, AAAI</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Parton</author>
<author>Kathleen R McKeown</author>
<author>James Allan</author>
<author>Enrique Henestroza</author>
</authors>
<title>Simultaneous multilingual search for translingual information retrieval,</title>
<date>2008</date>
<journal>ACM 17th CIKM</journal>
<marker>Parton, McKeown, Allan, Henestroza, 2008</marker>
<rawString>Kristen Parton, Kathleen R. McKeown, James Allan, and Enrique Henestroza. 2008. Simultaneous multilingual search for translingual information retrieval, ACM 17th CIKM</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>David Vilar</author>
<author>Gregor Leusch</author>
<author>Yuqi Zhang</author>
<author>Hermann Ney</author>
</authors>
<date>2007</date>
<booktitle>The RWTH Machine Translation System for IWSLT 2007, IWSLT</booktitle>
<contexts>
<context position="9959" citStr="Mauser et al., 2007" startWordPosition="1648" endWordPosition="1651">hrase Table as it is more likely to yield a good translation. If the record is not found, the Static one is referenced. If it is not found in either, the given VTG will not be processed. No matter which table is referenced, the following Naive Bayes equation is applied to obtain the translation of a given VTG. Pt( ) log ( |) log ( | + P pw t + P fw t k k k tk pw, fw and tk respectively represent the preceding source word, the following source word and a translation candidate of a VTG. 6. Experiments Our test data is drawn from Chinese-English MT results generated by Aachen’s 2007 RWTH system (Mauser et al., 2007), a phrase-based SMT system with 38.5% BLEU score on IWSLT 2007 evaluation data. Newswires and blog articles are retrieved for five queries which served as our experimental test bed. The queries are open-ended and on average, answers were 30 sentences in length. Q1: Who/What is involved in Saddam Hussein&apos;s trial Q2: Produce a biography of Jacques Rene Chirac Q3: Describe arrests of person from Salafist Group for Preaching and Combat Q4: Provide information on Chen Sui Bian Q5: What connections are there between World Cup games and stock markets? We used MT documents retrieved by IR for each qu</context>
</contexts>
<marker>Mauser, Vilar, Leusch, Zhang, Ney, 2007</marker>
<rawString>Arne Mauser, David Vilar, Gregor Leusch, Yuqi Zhang, and Hermann Ney. 2007. The RWTH Machine Translation System for IWSLT 2007, IWSLT</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Cyril Goutte</author>
<author>Pierre Isabelle</author>
</authors>
<date>2007</date>
<note>Statistical Phrase-based Post-Editing, NAACL-HLT</note>
<marker>Simard, Goutte, Isabelle, 2007</marker>
<rawString>Michel Simard, Cyril Goutte and Pierre Isabelle. 2007. Statistical Phrase-based Post-Editing, NAACL-HLT</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese Syntactic Reordering for Statistical Machine Translation,</title>
<date>2007</date>
<location>EMNLPCoNLL.</location>
<contexts>
<context position="6214" citStr="Wang et al., 2007" startWordPosition="999" endWordPosition="1002">er and the CRF Chinese POS tagger developed by Purdue University. If we find a verb phrase in the Chinese source sentence that was not aligned with any English words in the SMT alignment tables, then we label it as a verb translation gap (VTG) and confirm that the marking was correct. In the following sections, we describe how we determine which Chinese verb should have been translated and how that occurs. Modified English docs Response Generator Response in English Figure 1. The System Pipeline 4. Finding the Main Verb Position Chinese ordering differs from English mainly in clause ordering (Wang et al., 2007) and within the noun phrase. But within a clause centered by a verb, Chinese mostly uses a SVO or SV structure, like English (Yamada and Knight 2001), and we can assume the local alignment centered by a verb between Chinese and English is a linear mapping relation. Under this assumption, the translation of “被捕” in the above example should be placed in the position between “Saddam” and “.”. Thus, once we find a VTG, its translation can be inserted into the corresponding position of the target sentence using the alignment. This assumes, however, that there is only one VTG found within a clause. </context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese Syntactic Reordering for Statistical Machine Translation, EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
</authors>
<title>A syntax-based statistical translation model,</title>
<date>2001</date>
<publisher>ACL</publisher>
<marker>Yamada, 2001</marker>
<rawString>Kenji Yamada , Kevin Knight. 2001. A syntax-based statistical translation model, ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zwarts</author>
<author>M Dras</author>
</authors>
<title>Choosing the Right Translation: A Syntactically Informed Approach,</title>
<date>2008</date>
<location>COLING</location>
<marker>Zwarts, Dras, 2008</marker>
<rawString>S. Zwarts and M. Dras. 2008. Choosing the Right Translation: A Syntactically Informed Approach, COLING</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>