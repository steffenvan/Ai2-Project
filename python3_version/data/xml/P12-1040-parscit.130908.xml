<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002338">
<title confidence="0.996693">
A Discriminative Hierarchical Model for Fast Coreference at Large Scale
</title>
<author confidence="0.995533">
Michael Wick
</author>
<affiliation confidence="0.997736">
University of Massachsetts
</affiliation>
<address confidence="0.933867">
140 Governor’s Drive
Amherst, MA
</address>
<email confidence="0.99942">
mwick@cs.umass.edu
</email>
<author confidence="0.964986">
Sameer Singh
</author>
<affiliation confidence="0.989699">
University of Massachusetts
</affiliation>
<address confidence="0.937384">
140 Governor’s Drive
Amherst, MA
</address>
<email confidence="0.999456">
sameer@cs.umass.edu
</email>
<author confidence="0.993798">
Andrew McCallum
</author>
<affiliation confidence="0.997418">
University of Massachusetts
</affiliation>
<address confidence="0.9396675">
140 Governor’s Drive
Amherst, MA
</address>
<email confidence="0.999635">
mccallum@cs.umass.edu
</email>
<sectionHeader confidence="0.996659" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999652904761905">
Methods that measure compatibility between
mention pairs are currently the dominant ap-
proach to coreference. However, they suffer
from a number of drawbacks including diffi-
culties scaling to large numbers of mentions
and limited representational power. As these
drawbacks become increasingly restrictive,
the need to replace the pairwise approaches
with a more expressive, highly scalable al-
ternative is becoming urgent. In this paper
we propose a novel discriminative hierarchical
model that recursively partitions entities into
trees of latent sub-entities. These trees suc-
cinctly summarize the mentions providing a
highly compact, information-rich structure for
reasoning about entities and coreference un-
certainty at massive scales. We demonstrate
that the hierarchical model is several orders
of magnitude faster than pairwise, allowing us
to perform coreference on six million author
mentions in under four hours on a single CPU.
</bodyText>
<sectionHeader confidence="0.99888" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890244444445">
Coreference resolution, the task of clustering men-
tions into partitions representing their underlying
real-world entities, is fundamental for high-level in-
formation extraction and data integration, including
semantic search, question answering, and knowl-
edge base construction. For example, coreference
is vital for determining author publication lists in
bibliographic knowledge bases such as CiteSeer and
Google Scholar, where the repository must know
if the “R. Hamming” who authored “Error detect-
ing and error correcting codes” is the same” “R.
Hamming” who authored “The unreasonable effec-
tiveness of mathematics.” Features of the mentions
(e.g., bags-of-words in titles, contextual snippets
and co-author lists) provide evidence for resolving
such entities.
Over the years, various machine learning tech-
niques have been applied to different variations of
the coreference problem. A commonality in many
of these approaches is that they model the prob-
lem of entity coreference as a collection of deci-
sions between mention pairs (Bagga and Baldwin,
1999; Soon et al., 2001; McCallum and Wellner,
2004; Singla and Domingos, 2005; Bengston and
Roth, 2008). That is, coreference is solved by an-
swering a quadratic number of questions of the form
“does mention A refer to the same entity as mention
B?” with a compatibility function that indicates how
likely A and B are coreferent. While these models
have been successful in some domains, they also ex-
hibit several undesirable characteristics. The first is
that pairwise models lack the expressivity required
to represent aggregate properties of the entities. Re-
cent work has shown that these entity-level prop-
erties allow systems to correct coreference errors
made from myopic pairwise decisions (Ng, 2005;
Culotta et al., 2007; Yang et al., 2008; Rahman and
Ng, 2009; Wick et al., 2009), and can even provide
a strong signal for unsupervised coreference (Bhat-
tacharya and Getoor, 2006; Haghighi and Klein,
2007; Haghighi and Klein, 2010).
A second problem, that has received significantly
less attention in the literature, is that the pair-
wise coreference models scale poorly to large col-
lections of mentions especially when the expected
</bodyText>
<page confidence="0.986091">
379
</page>
<note confidence="0.992523">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.997698">
Figure 1: Discriminative hierarchical factor graph for coreference: Latent entity nodes (white boxes)
</figureCaption>
<bodyText confidence="0.85115625">
summarize subtrees. Pairwise factors (black squares) measure compatibilities between child and parent
nodes, avoiding quadratic blow-up. Corresponding decision variables (open circles) indicate whether one
node is the child of another. Mentions (gray boxes) are leaves. Deciding whether to merge these two entities
requires evaluating just a single factor (red square), corresponding to the new child-parent relationship.
</bodyText>
<note confidence="0.866928285714286">
Name: Jamie Callan
Ins(tu(ons: CMU,LTI.
Topics:{WWW, IR, SIGIR}
Coref?
Name: James Callan
Ins(tu(ons: CMU
Topics:{WWW, IR, large-scale}
</note>
<figure confidence="0.997314482758621">
J. Callan
Inst: CMU
J. Callan
Inst: LTI
J. Callan
Topic: WWW
James Callan
Topics: WWW
Inst:CMU
Jamie Callan
Topics: IR
Jamie Callan
Topics: IR
J. Callan
Inst: CMU
Name: J. Callan
Ins(tu(ons: LTI
Topics: WWW
J. Callan
Topics: IR
Inst: CMU
J. Callan
Topics: L-S
Name:Jamie Callan
Ins(tu(ons:
Topics: IR
Name: J. Callan
Ins(tu(ons: CMU,LTI
Topics: WWW
</figure>
<bodyText confidence="0.999857862068966">
number of mentions in each entity cluster is also
large. Current systems cope with this by either
dividing the data into blocks to reduce the search
space (Hern´andez and Stolfo, 1995; McCallum et
al., 2000; Bilenko et al., 2006), using fixed heuris-
tics to greedily compress the mentions (Ravin and
Kazi, 1999; Rao et al., 2010), employing special-
ized Markov chain Monte Carlo procedures (Milch
et al., 2006; Richardson and Domingos, 2006; Singh
et al., 2010), or introducing shallow hierarchies of
sub-entities for MCMC block moves and super-
entities for adaptive distributed inference (Singh et
al., 2011). However, while these methods help man-
age the search space for medium-scale data, eval-
uating each coreference decision in many of these
systems still scales linearly with the number of men-
tions in an entity, resulting in prohibitive computa-
tional costs associated with large datasets. This scal-
ing with the number of mentions per entity seems
particularly wasteful because although it is common
for an entity to be referenced by a large number
of mentions, many of these coreferent mentions are
highly similar to each other. For example, in author
coreference the two most common strings that refer
to Richard Hamming might have the form “R. Ham-
ming” and “Richard Hamming.” In newswire coref-
erence, a prominent entity like Barack Obama may
have millions of “Obama” mentions (many occur-
ring in similar semantic contexts). Deciding whether
a mention belongs to this entity need not involve
comparisons to all contextually similar “Obama”
mentions; rather we prefer a more compact repre-
sentation in order to efficiently reason about them.
In this paper we propose a novel hierarchical dis-
criminative factor graph for coreference resolution
that recursively structures each entity as a tree of la-
tent sub-entities with mentions at the leaves. Our
hierarchical model avoids the aforementioned prob-
lems of the pairwise approach: not only can it jointly
reason about attributes of entire entities (using the
power of discriminative conditional random fields),
but it is also able to scale to datasets with enor-
mous numbers of mentions because scoring enti-
ties does not require computing a quadratic number
of compatibility functions. The key insight is that
each node in the tree functions as a highly compact
information-rich summary of its children. Thus, a
small handful of upper-level nodes may summarize
millions of mentions (for example, a single node
may summarize all contextually similar “R. Ham-
ming” mentions). Although inferring the structure
of the entities requires reasoning over a larger state-
space, the latent trees are actually beneficial to in-
ference (as shown for shallow trees in Singh et
al. (2011)), resulting in rapid progress toward high
probability regions, and mirroring known benefits
of auxiliary variable methods in statistical physics
(such as Swendsen and Wang (1987)). Moreover,
</bodyText>
<page confidence="0.995172">
380
</page>
<bodyText confidence="0.99998347826087">
each step of inference is computationally efficient
because evaluating the cost of attaching (or detach-
ing) sub-trees requires computing just a single com-
patibility function (as seen in Figure 1). Further,
our hierarchical approach provides a number of ad-
ditional advantages. First, the recursive nature of the
tree (arbitrary depth and width) allows the model to
adapt to different types of data and effectively com-
press entities of different scales (e.g., entities with
more mentions may require a deeper hierarchy to
compress). Second, the model contains compatibil-
ity functions at all levels of the tree enabling it to si-
multaneously reason at multiple granularities of en-
tity compression. Third, the trees can provide split
points for finer-grained entities by placing contex-
tually similar mentions under the same subtree. Fi-
nally, if memory is limited, redundant mentions can
be pruned by replacing subtrees with their roots.
Empirically, we demonstrate that our model is
several orders of magnitude faster than a pairwise
model, allowing us to perform efficient coreference
on nearly six million author mentions in under four
hours using a single CPU.
</bodyText>
<sectionHeader confidence="0.925509" genericHeader="method">
2 Background: Pairwise Coreference
</sectionHeader>
<bodyText confidence="0.999972476190476">
Coreference is the problem of clustering mentions
such that mentions in the same set refer to the same
real-world entity; it is also known as entity disam-
biguation, record linkage, and de-duplication. For
example, in author coreference, each mention might
be represented as a record extracted from the author
field of a textual citation or BibTeX record. The
mention record may contain attributes for the first,
middle, and last name of the author, as well as con-
textual information occurring in the citation string,
co-authors, titles, topics, and institutions. The goal
is to cluster these mention records into sets, each
containing all the mentions of the author to which
they refer; we use this task as a running pedagogical
example.
Let M be the space of observed mention records;
then the traditional pairwise coreference approach
scores candidate coreference solutions with a com-
patibility function 0 : M x M —* R that mea-
sures how likely it is that the two mentions re-
fer to the same entity.1 In discriminative log-
</bodyText>
<footnote confidence="0.700948">
1We can also include an incompatibility function for when
</footnote>
<bodyText confidence="0.999660304347826">
linear models, the function 0 takes the form of
weights 0 on features 0(mi, mj), i.e., 0(mi, mj) =
exp (0 · 0(mi, mj)). For example, in author coref-
erence, the feature functions 0 might test whether
the name fields for two author mentions are string
identical, or compute cosine similarity between the
two mentions’ bags-of-words, each representing a
mention’s context. The corresponding real-valued
weights 0 determine the impact of these features on
the overall pairwise score.
Coreference can be solved by introducing a set of
binary coreference decision variables for each men-
tion pair and predicting a setting to their values that
maximizes the sum of pairwise compatibility func-
tions. While it is possible to independently make
pairwise decisions and enforce transitivity post hoc,
this can lead to poor accuracy because the decisions
are tightly coupled. For higher accuracy, a graphi-
cal model such as a conditional random field (CRF)
is constructed from the compatibility functions to
jointly reason about the pairwise decisions (McCal-
lum and Wellner, 2004). We now describe the pair-
wise CRF for coreference as a factor graph.
</bodyText>
<subsectionHeader confidence="0.993315">
2.1 Pairwise Conditional Random Field
</subsectionHeader>
<bodyText confidence="0.999868214285714">
Each mention mi E M is an observed variable, and
for each mention pair (mi, mj) we have a binary
coreference decision variable yij whose value de-
termines whether mi and mj refer to the same en-
tity (i.e., 1 means they are coreferent and 0 means
they are not coreferent). The pairwise compatibility
functions become the factors in the graphical model.
Each factor examines the properties of its mention
pair as well as the setting to the coreference decision
variable and outputs a score indicating how likely
the setting of that coreference variable is. The joint
probability distribution over all possible settings to
the coreference decision variables (y) is given as a
product of all the pairwise compatibility factors:
</bodyText>
<equation confidence="0.925806">
0(mi, mj, yij) (1)
</equation>
<bodyText confidence="0.9642932">
Given the pairwise CRF, the problem of coreference
is then solved by searching for the setting of the
coreference decision variables that has the highest
probability according to Equation 1 subject to the
the mentions are not coreferent, e.g., 7P : M x M x {0, 11 → R
</bodyText>
<equation confidence="0.944565833333333">
n
i=1
Pr(y|m) a
n
H
j=1
</equation>
<page confidence="0.952072">
381
</page>
<figure confidence="0.989997666666667">
Jamie Callan Jamie Callan
J. Callan J. Callan
J. Callan
</figure>
<figureCaption confidence="0.982534">
Figure 2: Pairwise model on six mentions: Open
</figureCaption>
<bodyText confidence="0.942373272727273">
circles are the binary coreference decision variables,
shaded circles are the observed mentions, and the
black boxes are the factors of the graphical model
that encode the pairwise compatibility functions.
constraint that the setting to the coreference vari-
ables obey transitivity;2 this is the maximum proba-
bility estimate (MPE) setting. However, the solution
to this problem is intractable, and even approximate
inference methods such as loopy belief propagation
can be difficult due to the cubic number of determin-
istic transitivity constraints.
</bodyText>
<subsectionHeader confidence="0.998522">
2.2 Approximate Inference
</subsectionHeader>
<bodyText confidence="0.999953066666667">
An approximate inference framework that has suc-
cessfully been used for coreference models is
Metropolis-Hastings (MH) (Milch et al. (2006), Cu-
lotta and McCallum (2006), Poon and Domingos
(2007), amongst others), a Markov chain Monte
Carlo algorithm traditionally used for marginal in-
ference, but which can also be tuned for MPE in-
ference. MH is a flexible framework for specify-
ing customized local-search transition functions and
provides a principled way of deciding which local
search moves to accept. A proposal function q takes
the current coreference hypothesis and proposes a
new hypothesis by modifying a subset of the de-
cision variables. The proposed change is accepted
with probability α:
</bodyText>
<equation confidence="0.988479">
α = min1, Pr(y&apos;) q(y |y&apos;) l (2)
( Pr(y) q(y&apos; |y) /
</equation>
<footnote confidence="0.5533045">
2We say that a full assignment to the coreference variables
y obeys transitivity if V ijk yzj = 1 n yjk = 1=#&apos; yzk = 1
</footnote>
<bodyText confidence="0.9992964">
When using MH for MPE inference, the second term
q(y|y&apos;)/q(y&apos;|y) is optional, and usually omitted.
Moves that reduce model score may be accepted and
Jamie Callan
an optional temperature can be used for annealing.
The primary advantages of MH for coreference are
(1) only the compatibility functions of the changed
decision variables need to be evaluated to accept a
Jamie Calan J. Calan v
move, and (2) the proposal function can enforce the
transitivity constraint by exploring only variable set-
tings that result in valid coreference partitionings.
distribution for coref-
J. Callan J. Callan J
erence is the following: (1) randomly select two
mentions (mi, mj), (2) if the mentions (mi, mj) are
in the same entity cluster according to y then move
one mention into a singleton cluster (by setting the
necessary decision variables to 0), otherwise, move
mention mi so it is in the same cluster as mj (by
setting the necessary decision variables). Typically,
MH is employed by first initializing to a singleton
configuration (all entities have one mention), and
then executing the MH for a certain number of steps
(or until the predicted coreference hypothesis stops
changing).
This proposal distribution always moves a sin-
gle mention m from some entity ei to another en-
tity ej and thus the configuration y and y&apos; only dif-
fer by the setting of decision variables governing to
which entity m refers. In order to guarantee transi-
tivity and a valid coreference equivalence relation,
we must properly remove m from ei by untethering
m from each mention in ei (this requires computing
|ei |− 1 pairwise factors). Similarly—again, for the
sake of transitivity—in order to complete the move
into ej we must coref m to each mention in ej (this
requires computing |ej |pairwise factors). Clearly,
all the other coreference decision variables are in-
dependent and so their corresponding factors can-
cel because they yield the same scores under y and
y&apos;. Thus, evaluating each proposal for the pairwise
model scales linearly with the number of mentions
assigned to the entities, requiring the evaluation of
2(|ei |+ |ej |− 1) compatibility functions (factors).
</bodyText>
<sectionHeader confidence="0.997322" genericHeader="method">
3 Hierarchical Coreference
</sectionHeader>
<bodyText confidence="0.915071666666667">
Instead of only capturing a single coreference clus-
tering between mention pairs, we can imagine mul-
tiple levels of coreference decisions over different
</bodyText>
<figure confidence="0.824322">
J. Callan A commonly used proposal
ie Callan Jamie Callan
</figure>
<page confidence="0.988235">
382
</page>
<bodyText confidence="0.9999797">
granularities. For example, mentions of an author
may be further partitioned into semantically similar
sets, such that mentions from each set have topically
similar papers. This partitioning can be recursive,
i.e., each of these sets can be further partitioned, cap-
turing candidate splits for an entity that can facilitate
inference. In this section, we describe a model that
captures arbitrarily deep hierarchies over such lay-
ers of coreference decisions, enabling efficient in-
ference and rich entity representations.
</bodyText>
<subsectionHeader confidence="0.976073">
3.1 Discriminative Hierarchical Model
</subsectionHeader>
<bodyText confidence="0.999681368421053">
In contrast to the pairwise model, where each en-
tity is a flat cluster of mentions, our proposed model
structures each entity recursively as a tree. The
leaves of the tree are the observed mentions with
a set of attribute values. Each internal node of the
tree is latent and contains a set of unobserved at-
tributes; recursively, these node records summarize
the attributes of their child nodes (see Figure 1), for
example, they may aggregate the bags of context
words of the children. The root of each tree repre-
sents the entire entity, with the leaves containing its
mentions. Formally, the coreference decision vari-
ables in the hierarchical model no longer represent
pairwise decisions directly. Instead, a decision vari-
able yrz,r, = 1 indicates that node-record rj is the
parent of node-record ri. We say a node-record ex-
ists if either it is a mention, has a parent, or has at
least one child. Let R be the set of all existing node
records, let rp denote the parent for node r, that is
yr,rr&apos; = 1, and dry =� rp, yr,r, = 0. As we describe
in more detail later, the structure of the tree and the
values of the unobserved attributes are determined
during inference.
In order to represent our recursive model of coref-
erence, we include two types of factors: pairwise
factors opw that measure compatibility between a
child node-record and its parent, and unit-wise fac-
tors 0rw that measure compatibilities of the node-
records themselves. For efficiency we enforce that
parent-child factors only produce a non-zero score
when the corresponding decision variable is 1. The
unit-wise factors can examine compatibility of set-
tings to the attribute variables for a particular node
(for example, the set of topics may be too diverse
to represent just a single entity), as well as enforce
priors over the tree’s breadth and depth. Our recur-
sive hierarchical model defines the probability of a
configuration as:
</bodyText>
<equation confidence="0.566722">
Pr(y, R|m) a � 4&apos;rw(r)4&apos;pw(r, rp) (3)
rER
</equation>
<subsectionHeader confidence="0.962232">
3.2 MCMC Inference for Hierarchical models
</subsectionHeader>
<bodyText confidence="0.999991658536585">
The state space of our hierarchical model is substan-
tially larger (theoretically infinite) than the pairwise
model due to the arbitrarily deep (and wide) latent
structure of the cluster trees. Inference must simul-
taneously determine the structure of the tree, the la-
tent node-record values, as well as the coreference
decisions themselves.
While this may seem daunting, the structures be-
ing inferred are actually beneficial to inference. In-
deed, despite the enlarged state space, inference
in the hierarchical model is substantially faster
than a pairwise model with a smaller state space.
One explanatory intuition comes from the statisti-
cal physics community: we can view the latent tree
as auxiliary variables in a data-augmentation sam-
pling scheme that guide MCMC through the state
space more efficiently. There is a large body of lit-
erature in the statistics community describing how
these auxiliary variables can lead to faster conver-
gence despite the enlarged state space (classic exam-
ples include Swendsen and Wang (1987) and slice
samplers (Neal, 2000)).
Further, evaluating each proposal during infer-
ence in the hierarchical model is substantially faster
than in the pairwise model. Indeed, we can replace
the linear number of factor evaluations (as in the
pairwise model) with a constant number of factor
evaluations for most proposals (for example, adding
a subtree requires re-evaluating only a single parent-
child factor between the subtree and the attachment
point, and a single node-wise factor).
Since inference must determine the structure of
the entity trees in addition to coreference, it is ad-
vantageous to consider multiple MH proposals per
sample. Therefore, we employ a modified variant
of MH that is similar to multi-try Metropolis (Liu
et al., 2000). Our modified MH algorithm makes k
proposals and samples one according to its model
ratio score (the first term in Equation 2) normalized
across all k. More specificaly, for each MH step, we
first randomly select two subtrees headed by node-
</bodyText>
<page confidence="0.998404">
383
</page>
<bodyText confidence="0.9773775">
records ri and rj from the current coreference hy-
pothesis. If ri and rj are in different clusters, we
propose several alternate merge operations: (also in
Figure 3):
</bodyText>
<listItem confidence="0.965611888888889">
• Merge Left - merges the entire subtree of rj into
node ri by making rj a child of ri
• Merge Entity Left - merges rj with ri’s root
• Merge Left and Collapse - merges rj into ri then
performs a collapse on rj (see below).
• Merge Up - merges node ri with node rj by cre-
ating a new parent node-record variable rp with ri
and rj as the children. The attribute fields of rp are
selected from ri and rj.
</listItem>
<bodyText confidence="0.8227585">
Otherwise ri and rj are subtrees in the same entity
tree, then the following proposals are used instead:
</bodyText>
<listItem confidence="0.998676333333333">
• Split Right - Make the subtree rj the root of a new
entity by detaching it from its parent
• Collapse - If ri has a parent, then move ri’s chil-
dren to ri’s parent and then delete ri.
• Sample attribute - Pick a new value for an at-
tribute of ri from its children.
</listItem>
<bodyText confidence="0.999811357142857">
Computing the model ratio for all of coreference
proposals requires only a constant number of com-
patibility functions. On the other hand, evaluating
proposals in the pairwise model requires evaluat-
ing a number of compatibility functions equal to the
number of mentions in the clusters being modified.
Note that changes to the attribute values of the
node-record and collapsing still require evaluating
a linear number of factors, but this is only linear in
the number of child nodes, not linear in the number
of mentions referring to the entity. Further, attribute
values rarely change once the entities stabilize. Fi-
nally, we incrementally update bags during corefer-
ence to reflect the aggregates of their children.
</bodyText>
<sectionHeader confidence="0.994525" genericHeader="method">
4 Experiments: Author Coreference
</sectionHeader>
<bodyText confidence="0.998957666666667">
Author coreference is a tremendously important
task, enabling improved search and mining of sci-
entific papers by researchers, funding agencies, and
governments. The problem is extremely difficult due
to the wide variations of names, limited contextual
evidence, misspellings, people with common names,
lack of standard citation formats, and large numbers
of mentions.
For this task we use a publicly available collec-
tion of 4,394 BibTeX files containing 817,193 en-
tries.3 We extract 1,322,985 author mentions, each
containing first, middle, last names, bags-of-words
of paper titles, topics in paper titles (by running la-
tent Dirichlet allocation (Blei et al., 2003)), and last
names of co-authors. In addition we include 2,833
mentions from the REXA dataset4 labeled for coref-
erence, in order to assess accuracy. We also include
∼5 million mentions from DBLP.
</bodyText>
<subsectionHeader confidence="0.998717">
4.1 Models and Inference
</subsectionHeader>
<bodyText confidence="0.999799228571429">
Due to the paucity of labeled training data, we did
not estimate parameters from data, but rather set
the compatibility functions manually by specifying
their log scores. The pairwise compatibility func-
tions punish a string difference in first, middle, and
last name, (−8); reward a match (+2); and reward
matching initials (+1). Additionally, we use the co-
sine similarity (shifted and scaled between −4 and
4) between the bags-of-words containing title to-
kens, topics, and co-author last names. These com-
patibility functions define the scores of the factors
in the pairwise model and the parent-child factors
in the hierarchical model. Additionally, we include
priors over the model structure. We encourage each
node to have eight children using a per node factor
having score 1/(number of children−8|+1), manage
tree depth by placing a cost on the creation of inter-
mediate tree nodes −8 and encourage clustering by
placing a cost on the creation of root-level entities
−7. These weights were determined by just a few
hours of tuning on a development set.
We initialize the MCMC procedures to the single-
ton configuration (each entity consists of one men-
tion) for each model, and run the MH algorithm de-
scribed in Section 2.2 for the pairwise model and
multi-try MH (described in Section 3.2) for the hi-
erarchical model. We augment these samplers us-
ing canopies constructed by concatenating the first
initial and last name: that is, mentions are only
selected from within the same canopy (or block)
to reduce the search space (Bilenko et al., 2006).
During the course of MCMC inference, we record
the pairwise F1 scores of the labeled subset. The
source code for our model is available as part of the
FACTORIE package (McCallum et al., 2009, http:
</bodyText>
<footnote confidence="0.993735666666667">
3http://www.iesl.cs.umass.edu/data/bibtex
4http://www2.selu.edu/Academics/Faculty/
aculotta/data/rexa.html
</footnote>
<page confidence="0.996665">
384
</page>
<figure confidence="0.9556033">
+*,-* .*/ +*,-* 0&amp;quot;$)1 .*/ +*,-* 23 +*,-* .*/ %&amp;quot;4 56&amp;&amp;%3(*
!$
&amp;&amp;quot;
!&amp;quot;
!&amp;quot;#$%&amp; ()%)*
!$ !&amp;quot; !$ !&amp;quot; !$ !&amp;quot; !$
&amp;&amp;quot;
&amp;&amp;quot;
!%
!&amp;quot;
</figure>
<figureCaption confidence="0.998048">
Figure 3: Example coreference proposals for the case where ri and rj are initially in different clusters.
</figureCaption>
<bodyText confidence="0.644744">
//factorie.cs.umass.edu/).
</bodyText>
<subsectionHeader confidence="0.971209">
4.2 Comparison to Pairwise Model
</subsectionHeader>
<bodyText confidence="0.983631866666667">
&amp;
In Figure 4a we plot the number of samples com-
&amp;quot;$
pleted over time for a 145k subset of the data. Re-
call that we initialized to the singleton configuration
and that as the size of the entities grows, the cost of
% * 73&amp;#)8#-9
evaluating the entities in MCMC becomes more ex-3
pensive. The pairwise model struggles with the large
cluster sizes while the hierarchical model is hardly
affected. Even though the hierarchical model is eval-
uating up to four proposals for each sample, it is still
able to sample much faster than the pairwise model;
this is expected because the cost of evaluating a pro-
posal requires evaluating fewer factors. Next, we
plot coreference F1 accuracy over time and show in
Figure 5a that the prolific sampling rate of the hierar-
chical model results in faster coreference. Using the
plot, we can compare running times for any desired
level of accuracy. For example, on the 145k men-
tion dataset, at a 60% accuracy level the hierarchical
model is 19 times faster and at 90% accuracy it is
31 times faster. These performance improvements
are even more profound on larger datasets: the hi-
erarchical model achieves a 60% level of accuracy
72 times faster than the pairwise model on the 1.3
million mention dataset, reaching 90% in just 2,350
seconds. Note, however, that the hierarchical model
requires more samples to reach a similar level of ac-
curacy due to the larger state space (Figure 4b).
</bodyText>
<subsectionHeader confidence="0.999607">
4.3 Large Scale Experiments
</subsectionHeader>
<bodyText confidence="0.999968">
In order to demonstrate the scalability of the hierar-
chical model, we run it on nearly 5 million author
mentions from DBLP. In under two hours (6,700
seconds), we achieve an accuracy of 80%, and in
under three hours (10,600 seconds), we achieve an
accuracy of over 90%. Finally, we combine DBLP
with BibTeX data to produce a dataset with almost 6
million mentions (5,803,811). Our performance on
&apos;&amp;quot;this dataset is similar to DBLP, taking just 13,500
seconds to reach a 90% accuracy.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999376633333333">
Singh et al. (2011) introduce a hierarchical model
for coreference that treats entities as a two-tiered
structure, by introducing the concept of sub-entities
and super-entities. Super-entities reduce the search
space in order to propose fruitful jumps. Sub-
entities provide a tighter granularity of coreference
and can be used to perform larger block moves dur-
ing MCMC. However, the hierarchy is fixed and
shallow. In contrast, our model can be arbitrarily
deep and wide. Even more importantly, their model
has pairwise factors and suffers from the quadratic
curse, which they address by distributing inference.
The work of Rao et al. (2010) uses streaming
clustering for large-scale coreference. However, the
greedy nature of the approach does not allow errors
to be revisited. Further, they compress entities by
averaging their mentions’ features. We are able to
provide richer entity compression, the ability to re-
visit errors, and scale to larger data.
Our hierarchical model provides the advantages
of recently proposed entity-based coreference sys-
tems that are known to provide higher accuracy
(Haghighi and Klein, 2007; Culotta et al., 2007;
Yang et al., 2008; Wick et al., 2009; Haghighi and
Klein, 2010). However, these systems reason over a
single layer of entities and do not scale well.
Techniques such as lifted inference (Singla and
Domingos, 2008) for graphical models exploit re-
dundancy in the data, but typically do not achieve
any significant compression on coreference data be-
</bodyText>
<page confidence="0.994588">
385
</page>
<figure confidence="0.9969249375">
400,000
Number of Samples
350,000
300,000
250,000
200,000
150,000
100,000
50,000
0
Samples versus Time
0 500 1,000 1,500 2,000
Running time (s)
(a) Sampling Performance
Accuracy versus Samples
0 50,000 100,000 150,000 200,000
Number of Samples
(b) Accuracy vs. samples (convergence accuracy as dashes)
Hierar Pairwise
Hierar Pairwise
1.0
0.9
0.8
F1 Accuracy
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
</figure>
<figureCaption confidence="0.994372">
Figure 4: Sampling Performance Plots for 145k mentions
</figureCaption>
<figure confidence="0.999751696969697">
1.0
0.9
0.8
F1 Accuracy
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.9
0.8
F1 Accuracy
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Accuracy versus Time
0 250 500 750 1,000 1,250 1,500 1,750 2,000
Running time (s)
(a) Accuracy vs. time (145k mentions)
Accuracy versus Time
Hierar Pairwise
0 10,000 20,000 30,000 40,000 50,000 60,000
Running time (s)
(b) Accuracy vs. time (1.3 million mentions)
Hierar Pairwise
</figure>
<figureCaption confidence="0.99997">
Figure 5: Runtime performance on two datasets
</figureCaption>
<bodyText confidence="0.9995918">
cause the observations usually violate any symmetry
assumptions. On the other hand, our model is able
to compress similar (but potentially different) obser-
vations together in order to make inference fast even
in the presence of asymmetric observed data.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.996919777777778">
In this paper we present a new hierarchical model
for large scale coreference and demonstrate it on
the problem of author disambiguation. Our model
recursively defines an entity as a summary of its
children nodes, allowing succinct representations of
millions of mentions. Indeed, inference in the hier-
archy is orders of magnitude faster than a pairwise
CRF, allowing us to infer accurate coreference on
six million mentions on one CPU in just 4 hours.
</bodyText>
<sectionHeader confidence="0.998688" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999093545454545">
We would like to thank Veselin Stoyanov for his feed-
back. This work was supported in part by the CIIR, in
part by ARFL under prime contract #FA8650-10-C-7059,
in part by DARPA under AFRL prime contract #FA8750-
09-C-0181, and in part by IARPA via DoI/NBC contract
#D11PC20152. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation thereon.
Any opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
</bodyText>
<page confidence="0.997981">
386
</page>
<sectionHeader confidence="0.983583" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999671180952381">
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: annotations, experiments, and ob-
servations. In Proceedings of the Workshop on Coref-
erence and its Applications, CorefApp ’99, pages 1–8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
Dirichlet model for unsupervised entity resolution. In
SDM.
Mikhail Bilenko, Beena Kamath, and Raymond J.
Mooney. 2006. Adaptive blocking: Learning to scale
up record linkage. In Proceedings of the Sixth Interna-
tional Conference on Data Mining, ICDM ’06, pages
87–96, Washington, DC, USA. IEEE Computer Soci-
ety.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal on Machine
Learning Research, 3:993–1022.
Aron Culotta and Andrew McCallum. 2006. Prac-
tical Markov logic containing first-order quantifiers
with application to identity uncertainty. In Human
Language Technology Workshop on Computationally
Hard Problems and Joint Inference in Speech and Lan-
guage Processing (HLT/NAACL), June.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL HLT).
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 848–855.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385–393.
Mauricio A. Hern´andez and Salvatore J. Stolfo. 1995.
The merge/purge problem for large databases. In Pro-
ceedings of the 1995 ACM SIGMOD international
conference on Management of data, SIGMOD ’95,
pages 127–138, New York, NY, USA. ACM.
Jun S. Liu, Faming Liang, and Wing Hung Wong. 2000.
The multiple-try method and local optimization in
metropolis sampling. Journal of the American Statis-
tical Association, 96(449):121–134.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems (NIPS).
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 169–178.
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In Neural Informa-
tion Processing Systems (NIPS).
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2006.
BLOG: Relational Modeling with Unknown Objects.
Ph.D. thesis, University of California, Berkeley.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31:705–767.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Annual Meeting of the Association for Computational
Linguistics (ACL).
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In AAAI Conference
on Artificial Intelligence, pages 913–918.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2 - Volume 2, EMNLP
’09, pages 968–977, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In International Conference on Computa-
tional Linguistics (COLING), pages 1050–1058, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Yael Ravin and Zunaid Kazi. 1999. Is Hillary Rodham
Clinton the president? disambiguating names across
documents. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 9–16.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107–136.
Sameer Singh, Michael L. Wick, and Andrew McCallum.
2010. Distantly labeling data for large scale cross-
document coreference. Computing Research Reposi-
tory (CoRR), abs/1005.4298.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
</reference>
<page confidence="0.979492">
387
</page>
<reference confidence="0.99957047826087">
Parag Singla and Pedro Domingos. 2005. Discrimina-
tive training of Markov logic networks. In AAAI, Pitts-
burgh, PA.
Parag Singla and Pedro Domingos. 2008. Lifted first-
order belief propagation. In Proceedings of the 23rd
national conference on Artificial intelligence - Volume
2, AAAI’08, pages 1094–1099. AAAI Press.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. Comput. Linguist.,
27(4):521–544.
R.H. Swendsen and J.S. Wang. 1987. Nonuniversal crit-
ical dynamics in MC simulations. Phys. Rev. Lett.,
58(2):68–88.
Michael Wick, Aron Culotta, Khashayar Rohanimanesh,
and Andrew McCallum. 2009. An entity-based model
for coreference resolution. In SIAM International
Conference on Data Mining (SDM).
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In Association for Computational Linguistics,
pages 843–851.
</reference>
<page confidence="0.998243">
388
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.281967">
<title confidence="0.999768">A Discriminative Hierarchical Model for Fast Coreference at Large Scale</title>
<author confidence="0.993159">Michael</author>
<affiliation confidence="0.999887">University of Massachsetts</affiliation>
<address confidence="0.8279935">140 Governor’s Drive Amherst,</address>
<email confidence="0.999901">mwick@cs.umass.edu</email>
<author confidence="0.728041">Sameer</author>
<affiliation confidence="0.99101">University of</affiliation>
<address confidence="0.9150185">140 Governor’s Amherst,</address>
<email confidence="0.999936">sameer@cs.umass.edu</email>
<author confidence="0.96629">Andrew</author>
<affiliation confidence="0.998984">University of</affiliation>
<address confidence="0.827215">140 Governor’s Amherst,</address>
<email confidence="0.999941">mccallum@cs.umass.edu</email>
<abstract confidence="0.998410272727273">Methods that measure compatibility between mention pairs are currently the dominant approach to coreference. However, they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Cross-document event coreference: annotations, experiments, and observations.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop on Coreference and its Applications, CorefApp ’99,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2373" citStr="Bagga and Baldwin, 1999" startWordPosition="330" endWordPosition="333">repository must know if the “R. Hamming” who authored “Error detecting and error correcting codes” is the same” “R. Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions (e.g., bags-of-words in titles, contextual snippets and co-author lists) provide evidence for resolving such entities. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level </context>
</contexts>
<marker>Bagga, Baldwin, 1999</marker>
<rawString>Amit Bagga and Breck Baldwin. 1999. Cross-document event coreference: annotations, experiments, and observations. In Proceedings of the Workshop on Coreference and its Applications, CorefApp ’99, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengston</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2473" citStr="Bengston and Roth, 2008" startWordPosition="346" endWordPosition="349">is the same” “R. Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions (e.g., bags-of-words in titles, contextual snippets and co-author lists) provide evidence for resolving such entities. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005</context>
</contexts>
<marker>Bengston, Roth, 2008</marker>
<rawString>Eric Bengston and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Indrajit Bhattacharya</author>
<author>Lise Getoor</author>
</authors>
<title>A latent Dirichlet model for unsupervised entity resolution.</title>
<date>2006</date>
<booktitle>In SDM.</booktitle>
<contexts>
<context position="3253" citStr="Bhattacharya and Getoor, 2006" startWordPosition="472" endWordPosition="476">a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Discriminative hierarchical factor graph for coreference: Latent entity nodes (white boxes) summarize subtrees. Pairwise factors (black sq</context>
</contexts>
<marker>Bhattacharya, Getoor, 2006</marker>
<rawString>Indrajit Bhattacharya and Lise Getoor. 2006. A latent Dirichlet model for unsupervised entity resolution. In SDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bilenko</author>
<author>Beena Kamath</author>
<author>Raymond J Mooney</author>
</authors>
<title>Adaptive blocking: Learning to scale up record linkage.</title>
<date>2006</date>
<booktitle>In Proceedings of the Sixth International Conference on Data Mining, ICDM ’06,</booktitle>
<pages>87--96</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="4943" citStr="Bilenko et al., 2006" startWordPosition="722" endWordPosition="725">(tu(ons: CMU Topics:{WWW, IR, large-scale} J. Callan Inst: CMU J. Callan Inst: LTI J. Callan Topic: WWW James Callan Topics: WWW Inst:CMU Jamie Callan Topics: IR Jamie Callan Topics: IR J. Callan Inst: CMU Name: J. Callan Ins(tu(ons: LTI Topics: WWW J. Callan Topics: IR Inst: CMU J. Callan Topics: L-S Name:Jamie Callan Ins(tu(ons: Topics: IR Name: J. Callan Ins(tu(ons: CMU,LTI Topics: WWW number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in p</context>
<context position="24730" citStr="Bilenko et al., 2006" startWordPosition="3954" endWordPosition="3957">cost on the creation of root-level entities −7. These weights were determined by just a few hours of tuning on a development set. We initialize the MCMC procedures to the singleton configuration (each entity consists of one mention) for each model, and run the MH algorithm described in Section 2.2 for the pairwise model and multi-try MH (described in Section 3.2) for the hierarchical model. We augment these samplers using canopies constructed by concatenating the first initial and last name: that is, mentions are only selected from within the same canopy (or block) to reduce the search space (Bilenko et al., 2006). During the course of MCMC inference, we record the pairwise F1 scores of the labeled subset. The source code for our model is available as part of the FACTORIE package (McCallum et al., 2009, http: 3http://www.iesl.cs.umass.edu/data/bibtex 4http://www2.selu.edu/Academics/Faculty/ aculotta/data/rexa.html 384 +*,-* .*/ +*,-* 0&amp;quot;$)1 .*/ +*,-* 23 +*,-* .*/ %&amp;quot;4 56&amp;&amp;%3(* !$ &amp;&amp;quot; !&amp;quot; !&amp;quot;#$%&amp; ()%)* !$ !&amp;quot; !$ !&amp;quot; !$ !&amp;quot; !$ &amp;&amp;quot; &amp;&amp;quot; !% !&amp;quot; Figure 3: Example coreference proposals for the case where ri and rj are initially in different clusters. //factorie.cs.umass.edu/). 4.2 Comparison to Pairwise Model &amp; In Figur</context>
</contexts>
<marker>Bilenko, Kamath, Mooney, 2006</marker>
<rawString>Mikhail Bilenko, Beena Kamath, and Raymond J. Mooney. 2006. Adaptive blocking: Learning to scale up record linkage. In Proceedings of the Sixth International Conference on Data Mining, ICDM ’06, pages 87–96, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal on Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="22962" citStr="Blei et al., 2003" startWordPosition="3663" endWordPosition="3666">ask, enabling improved search and mining of scientific papers by researchers, funding agencies, and governments. The problem is extremely difficult due to the wide variations of names, limited contextual evidence, misspellings, people with common names, lack of standard citation formats, and large numbers of mentions. For this task we use a publicly available collection of 4,394 BibTeX files containing 817,193 entries.3 We extract 1,322,985 author mentions, each containing first, middle, last names, bags-of-words of paper titles, topics in paper titles (by running latent Dirichlet allocation (Blei et al., 2003)), and last names of co-authors. In addition we include 2,833 mentions from the REXA dataset4 labeled for coreference, in order to assess accuracy. We also include ∼5 million mentions from DBLP. 4.1 Models and Inference Due to the paucity of labeled training data, we did not estimate parameters from data, but rather set the compatibility functions manually by specifying their log scores. The pairwise compatibility functions punish a string difference in first, middle, and last name, (−8); reward a match (+2); and reward matching initials (+1). Additionally, we use the cosine similarity (shifte</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal on Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Practical Markov logic containing first-order quantifiers with application to identity uncertainty.</title>
<date>2006</date>
<booktitle>In Human Language Technology Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing (HLT/NAACL),</booktitle>
<contexts>
<context position="12958" citStr="Culotta and McCallum (2006)" startWordPosition="2004" endWordPosition="2008">s are the factors of the graphical model that encode the pairwise compatibility functions. constraint that the setting to the coreference variables obey transitivity;2 this is the maximum probability estimate (MPE) setting. However, the solution to this problem is intractable, and even approximate inference methods such as loopy belief propagation can be difficult due to the cubic number of deterministic transitivity constraints. 2.2 Approximate Inference An approximate inference framework that has successfully been used for coreference models is Metropolis-Hastings (MH) (Milch et al. (2006), Culotta and McCallum (2006), Poon and Domingos (2007), amongst others), a Markov chain Monte Carlo algorithm traditionally used for marginal inference, but which can also be tuned for MPE inference. MH is a flexible framework for specifying customized local-search transition functions and provides a principled way of deciding which local search moves to accept. A proposal function q takes the current coreference hypothesis and proposes a new hypothesis by modifying a subset of the decision variables. The proposed change is accepted with probability α: α = min1, Pr(y&apos;) q(y |y&apos;) l (2) ( Pr(y) q(y&apos; |y) / 2We say that a ful</context>
</contexts>
<marker>Culotta, McCallum, 2006</marker>
<rawString>Aron Culotta and Andrew McCallum. 2006. Practical Markov logic containing first-order quantifiers with application to identity uncertainty. In Human Language Technology Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing (HLT/NAACL), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Andrew McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution.</title>
<date>2007</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT).</booktitle>
<contexts>
<context position="3095" citStr="Culotta et al., 2007" startWordPosition="446" endWordPosition="449">That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Li</context>
<context position="28407" citStr="Culotta et al., 2007" startWordPosition="4556" endWordPosition="4559"> and suffers from the quadratic curse, which they address by distributing inference. The work of Rao et al. (2010) uses streaming clustering for large-scale coreference. However, the greedy nature of the approach does not allow errors to be revisited. Further, they compress entities by averaging their mentions’ features. We are able to provide richer entity compression, the ability to revisit errors, and scale to larger data. Our hierarchical model provides the advantages of recently proposed entity-based coreference systems that are known to provide higher accuracy (Haghighi and Klein, 2007; Culotta et al., 2007; Yang et al., 2008; Wick et al., 2009; Haghighi and Klein, 2010). However, these systems reason over a single layer of entities and do not scale well. Techniques such as lifted inference (Singla and Domingos, 2008) for graphical models exploit redundancy in the data, but typically do not achieve any significant compression on coreference data be385 400,000 Number of Samples 350,000 300,000 250,000 200,000 150,000 100,000 50,000 0 Samples versus Time 0 500 1,000 1,500 2,000 Running time (s) (a) Sampling Performance Accuracy versus Samples 0 50,000 100,000 150,000 200,000 Number of Samples (b) </context>
</contexts>
<marker>Culotta, Wick, McCallum, 2007</marker>
<rawString>Aron Culotta, Michael Wick, and Andrew McCallum. 2007. First-order probabilistic models for coreference resolution. In North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric bayesian model.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>848--855</pages>
<contexts>
<context position="3279" citStr="Haghighi and Klein, 2007" startWordPosition="477" endWordPosition="480">ndicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Discriminative hierarchical factor graph for coreference: Latent entity nodes (white boxes) summarize subtrees. Pairwise factors (black squares) measure compatibili</context>
<context position="28385" citStr="Haghighi and Klein, 2007" startWordPosition="4552" endWordPosition="4555">model has pairwise factors and suffers from the quadratic curse, which they address by distributing inference. The work of Rao et al. (2010) uses streaming clustering for large-scale coreference. However, the greedy nature of the approach does not allow errors to be revisited. Further, they compress entities by averaging their mentions’ features. We are able to provide richer entity compression, the ability to revisit errors, and scale to larger data. Our hierarchical model provides the advantages of recently proposed entity-based coreference systems that are known to provide higher accuracy (Haghighi and Klein, 2007; Culotta et al., 2007; Yang et al., 2008; Wick et al., 2009; Haghighi and Klein, 2010). However, these systems reason over a single layer of entities and do not scale well. Techniques such as lifted inference (Singla and Domingos, 2008) for graphical models exploit redundancy in the data, but typically do not achieve any significant compression on coreference data be385 400,000 Number of Samples 350,000 300,000 250,000 200,000 150,000 100,000 50,000 0 Samples versus Time 0 500 1,000 1,500 2,000 Running time (s) (a) Sampling Performance Accuracy versus Samples 0 50,000 100,000 150,000 200,000 </context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 848–855.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT),</booktitle>
<pages>385--393</pages>
<contexts>
<context position="3306" citStr="Haghighi and Klein, 2010" startWordPosition="481" endWordPosition="484">B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Discriminative hierarchical factor graph for coreference: Latent entity nodes (white boxes) summarize subtrees. Pairwise factors (black squares) measure compatibilities between child and pare</context>
<context position="28472" citStr="Haghighi and Klein, 2010" startWordPosition="4568" endWordPosition="4571"> distributing inference. The work of Rao et al. (2010) uses streaming clustering for large-scale coreference. However, the greedy nature of the approach does not allow errors to be revisited. Further, they compress entities by averaging their mentions’ features. We are able to provide richer entity compression, the ability to revisit errors, and scale to larger data. Our hierarchical model provides the advantages of recently proposed entity-based coreference systems that are known to provide higher accuracy (Haghighi and Klein, 2007; Culotta et al., 2007; Yang et al., 2008; Wick et al., 2009; Haghighi and Klein, 2010). However, these systems reason over a single layer of entities and do not scale well. Techniques such as lifted inference (Singla and Domingos, 2008) for graphical models exploit redundancy in the data, but typically do not achieve any significant compression on coreference data be385 400,000 Number of Samples 350,000 300,000 250,000 200,000 150,000 100,000 50,000 0 Samples versus Time 0 500 1,000 1,500 2,000 Running time (s) (a) Sampling Performance Accuracy versus Samples 0 50,000 100,000 150,000 200,000 Number of Samples (b) Accuracy vs. samples (convergence accuracy as dashes) Hierar Pair</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT), pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauricio A Hern´andez</author>
<author>Salvatore J Stolfo</author>
</authors>
<title>The merge/purge problem for large databases.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 ACM SIGMOD international conference on Management of data, SIGMOD ’95,</booktitle>
<pages>127--138</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Hern´andez, Stolfo, 1995</marker>
<rawString>Mauricio A. Hern´andez and Salvatore J. Stolfo. 1995. The merge/purge problem for large databases. In Proceedings of the 1995 ACM SIGMOD international conference on Management of data, SIGMOD ’95, pages 127–138, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun S Liu</author>
<author>Faming Liang</author>
<author>Wing Hung Wong</author>
</authors>
<title>The multiple-try method and local optimization in metropolis sampling.</title>
<date>2000</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>96</volume>
<issue>449</issue>
<contexts>
<context position="20362" citStr="Liu et al., 2000" startWordPosition="3216" endWordPosition="3219"> faster than in the pairwise model. Indeed, we can replace the linear number of factor evaluations (as in the pairwise model) with a constant number of factor evaluations for most proposals (for example, adding a subtree requires re-evaluating only a single parentchild factor between the subtree and the attachment point, and a single node-wise factor). Since inference must determine the structure of the entity trees in addition to coreference, it is advantageous to consider multiple MH proposals per sample. Therefore, we employ a modified variant of MH that is similar to multi-try Metropolis (Liu et al., 2000). Our modified MH algorithm makes k proposals and samples one according to its model ratio score (the first term in Equation 2) normalized across all k. More specificaly, for each MH step, we first randomly select two subtrees headed by node383 records ri and rj from the current coreference hypothesis. If ri and rj are in different clusters, we propose several alternate merge operations: (also in Figure 3): • Merge Left - merges the entire subtree of rj into node ri by making rj a child of ri • Merge Entity Left - merges rj with ri’s root • Merge Left and Collapse - merges rj into ri then perf</context>
</contexts>
<marker>Liu, Liang, Wong, 2000</marker>
<rawString>Jun S. Liu, Faming Liang, and Wing Hung Wong. 2000. The multiple-try method and local optimization in metropolis sampling. Journal of the American Statistical Association, 96(449):121–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="2420" citStr="McCallum and Wellner, 2004" startWordPosition="338" endWordPosition="341"> authored “Error detecting and error correcting codes” is the same” “R. Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions (e.g., bags-of-words in titles, contextual snippets and co-author lists) provide evidence for resolving such entities. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference</context>
<context position="10968" citStr="McCallum and Wellner, 2004" startWordPosition="1678" endWordPosition="1682">e features on the overall pairwise score. Coreference can be solved by introducing a set of binary coreference decision variables for each mention pair and predicting a setting to their values that maximizes the sum of pairwise compatibility functions. While it is possible to independently make pairwise decisions and enforce transitivity post hoc, this can lead to poor accuracy because the decisions are tightly coupled. For higher accuracy, a graphical model such as a conditional random field (CRF) is constructed from the compatibility functions to jointly reason about the pairwise decisions (McCallum and Wellner, 2004). We now describe the pairwise CRF for coreference as a factor graph. 2.1 Pairwise Conditional Random Field Each mention mi E M is an observed variable, and for each mention pair (mi, mj) we have a binary coreference decision variable yij whose value determines whether mi and mj refer to the same entity (i.e., 1 means they are coreferent and 0 means they are not coreferent). The pairwise compatibility functions become the factors in the graphical model. Each factor examines the properties of its mention pair as well as the setting to the coreference decision variable and outputs a score indica</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>Andrew McCallum and Ben Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
<author>Lyle Ungar</author>
</authors>
<title>Efficient clustering of high-dimensional data sets with application to reference matching.</title>
<date>2000</date>
<booktitle>In International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>169--178</pages>
<contexts>
<context position="4920" citStr="McCallum et al., 2000" startWordPosition="718" endWordPosition="721"> Name: James Callan Ins(tu(ons: CMU Topics:{WWW, IR, large-scale} J. Callan Inst: CMU J. Callan Inst: LTI J. Callan Topic: WWW James Callan Topics: WWW Inst:CMU Jamie Callan Topics: IR Jamie Callan Topics: IR J. Callan Inst: CMU Name: J. Callan Ins(tu(ons: LTI Topics: WWW J. Callan Topics: IR Inst: CMU J. Callan Topics: L-S Name:Jamie Callan Ins(tu(ons: Topics: IR Name: J. Callan Ins(tu(ons: CMU,LTI Topics: WWW number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an</context>
</contexts>
<marker>McCallum, Nigam, Ungar, 2000</marker>
<rawString>Andrew McCallum, Kamal Nigam, and Lyle Ungar. 2000. Efficient clustering of high-dimensional data sets with application to reference matching. In International Conference on Knowledge Discovery and Data Mining (KDD), pages 169–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Karl Schultz</author>
<author>Sameer Singh</author>
</authors>
<title>FACTORIE: Probabilistic programming via imperatively defined factor graphs.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="24922" citStr="McCallum et al., 2009" startWordPosition="3988" endWordPosition="3991">on (each entity consists of one mention) for each model, and run the MH algorithm described in Section 2.2 for the pairwise model and multi-try MH (described in Section 3.2) for the hierarchical model. We augment these samplers using canopies constructed by concatenating the first initial and last name: that is, mentions are only selected from within the same canopy (or block) to reduce the search space (Bilenko et al., 2006). During the course of MCMC inference, we record the pairwise F1 scores of the labeled subset. The source code for our model is available as part of the FACTORIE package (McCallum et al., 2009, http: 3http://www.iesl.cs.umass.edu/data/bibtex 4http://www2.selu.edu/Academics/Faculty/ aculotta/data/rexa.html 384 +*,-* .*/ +*,-* 0&amp;quot;$)1 .*/ +*,-* 23 +*,-* .*/ %&amp;quot;4 56&amp;&amp;%3(* !$ &amp;&amp;quot; !&amp;quot; !&amp;quot;#$%&amp; ()%)* !$ !&amp;quot; !$ !&amp;quot; !$ !&amp;quot; !$ &amp;&amp;quot; &amp;&amp;quot; !% !&amp;quot; Figure 3: Example coreference proposals for the case where ri and rj are initially in different clusters. //factorie.cs.umass.edu/). 4.2 Comparison to Pairwise Model &amp; In Figure 4a we plot the number of samples com&amp;quot;$ pleted over time for a 145k subset of the data. Recall that we initialized to the singleton configuration and that as the size of the entities grows, t</context>
</contexts>
<marker>McCallum, Schultz, Singh, 2009</marker>
<rawString>Andrew McCallum, Karl Schultz, and Sameer Singh. 2009. FACTORIE: Probabilistic programming via imperatively defined factor graphs. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Milch</author>
<author>Bhaskara Marthi</author>
<author>Stuart Russell</author>
</authors>
<title>BLOG: Relational Modeling with Unknown Objects.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="5121" citStr="Milch et al., 2006" startWordPosition="751" endWordPosition="754">: IR J. Callan Inst: CMU Name: J. Callan Ins(tu(ons: LTI Topics: WWW J. Callan Topics: IR Inst: CMU J. Callan Topics: L-S Name:Jamie Callan Ins(tu(ons: Topics: IR Name: J. Callan Ins(tu(ons: CMU,LTI Topics: WWW number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with the number of mentions per entity seems particularly wasteful because although it is common for a</context>
<context position="12929" citStr="Milch et al. (2006)" startWordPosition="2000" endWordPosition="2003">s, and the black boxes are the factors of the graphical model that encode the pairwise compatibility functions. constraint that the setting to the coreference variables obey transitivity;2 this is the maximum probability estimate (MPE) setting. However, the solution to this problem is intractable, and even approximate inference methods such as loopy belief propagation can be difficult due to the cubic number of deterministic transitivity constraints. 2.2 Approximate Inference An approximate inference framework that has successfully been used for coreference models is Metropolis-Hastings (MH) (Milch et al. (2006), Culotta and McCallum (2006), Poon and Domingos (2007), amongst others), a Markov chain Monte Carlo algorithm traditionally used for marginal inference, but which can also be tuned for MPE inference. MH is a flexible framework for specifying customized local-search transition functions and provides a principled way of deciding which local search moves to accept. A proposal function q takes the current coreference hypothesis and proposes a new hypothesis by modifying a subset of the decision variables. The proposed change is accepted with probability α: α = min1, Pr(y&apos;) q(y |y&apos;) l (2) ( Pr(y) </context>
</contexts>
<marker>Milch, Marthi, Russell, 2006</marker>
<rawString>Brian Milch, Bhaskara Marthi, and Stuart Russell. 2006. BLOG: Relational Modeling with Unknown Objects. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2000</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="19649" citStr="Neal, 2000" startWordPosition="3106" endWordPosition="3107">ite the enlarged state space, inference in the hierarchical model is substantially faster than a pairwise model with a smaller state space. One explanatory intuition comes from the statistical physics community: we can view the latent tree as auxiliary variables in a data-augmentation sampling scheme that guide MCMC through the state space more efficiently. There is a large body of literature in the statistics community describing how these auxiliary variables can lead to faster convergence despite the enlarged state space (classic examples include Swendsen and Wang (1987) and slice samplers (Neal, 2000)). Further, evaluating each proposal during inference in the hierarchical model is substantially faster than in the pairwise model. Indeed, we can replace the linear number of factor evaluations (as in the pairwise model) with a constant number of factor evaluations for most proposals (for example, adding a subtree requires re-evaluating only a single parentchild factor between the subtree and the attachment point, and a single node-wise factor). Since inference must determine the structure of the entity trees in addition to coreference, it is advantageous to consider multiple MH proposals per</context>
</contexts>
<marker>Neal, 2000</marker>
<rawString>Radford Neal. 2000. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Machine learning for coreference resolution: From local classification to global ranking.</title>
<date>2005</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3073" citStr="Ng, 2005" startWordPosition="444" endWordPosition="445">h, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Associatio</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>Vincent Ng. 2005. Machine learning for coreference resolution: From local classification to global ranking. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint inference in information extraction.</title>
<date>2007</date>
<booktitle>In AAAI Conference on Artificial Intelligence,</booktitle>
<pages>913--918</pages>
<contexts>
<context position="12984" citStr="Poon and Domingos (2007)" startWordPosition="2009" endWordPosition="2012">hical model that encode the pairwise compatibility functions. constraint that the setting to the coreference variables obey transitivity;2 this is the maximum probability estimate (MPE) setting. However, the solution to this problem is intractable, and even approximate inference methods such as loopy belief propagation can be difficult due to the cubic number of deterministic transitivity constraints. 2.2 Approximate Inference An approximate inference framework that has successfully been used for coreference models is Metropolis-Hastings (MH) (Milch et al. (2006), Culotta and McCallum (2006), Poon and Domingos (2007), amongst others), a Markov chain Monte Carlo algorithm traditionally used for marginal inference, but which can also be tuned for MPE inference. MH is a flexible framework for specifying customized local-search transition functions and provides a principled way of deciding which local search moves to accept. A proposal function q takes the current coreference hypothesis and proposes a new hypothesis by modifying a subset of the decision variables. The proposed change is accepted with probability α: α = min1, Pr(y&apos;) q(y |y&apos;) l (2) ( Pr(y) q(y&apos; |y) / 2We say that a full assignment to the corefe</context>
</contexts>
<marker>Poon, Domingos, 2007</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2007. Joint inference in information extraction. In AAAI Conference on Artificial Intelligence, pages 913–918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>968--977</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3135" citStr="Rahman and Ng, 2009" startWordPosition="454" endWordPosition="457">ng a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Discriminative hiera</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Altaf Rahman and Vincent Ng. 2009. Supervised models for coreference resolution. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 968–977, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Paul McNamee</author>
<author>Mark Dredze</author>
</authors>
<title>Streaming cross document entity coreference resolution.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1050--1058</pages>
<location>Beijing, China,</location>
<contexts>
<context position="5042" citStr="Rao et al., 2010" startWordPosition="739" endWordPosition="742"> James Callan Topics: WWW Inst:CMU Jamie Callan Topics: IR Jamie Callan Topics: IR J. Callan Inst: CMU Name: J. Callan Ins(tu(ons: LTI Topics: WWW J. Callan Topics: IR Inst: CMU J. Callan Topics: L-S Name:Jamie Callan Ins(tu(ons: Topics: IR Name: J. Callan Ins(tu(ons: CMU,LTI Topics: WWW number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with the number of ment</context>
<context position="27901" citStr="Rao et al. (2010)" startWordPosition="4480" endWordPosition="4483">roduce a hierarchical model for coreference that treats entities as a two-tiered structure, by introducing the concept of sub-entities and super-entities. Super-entities reduce the search space in order to propose fruitful jumps. Subentities provide a tighter granularity of coreference and can be used to perform larger block moves during MCMC. However, the hierarchy is fixed and shallow. In contrast, our model can be arbitrarily deep and wide. Even more importantly, their model has pairwise factors and suffers from the quadratic curse, which they address by distributing inference. The work of Rao et al. (2010) uses streaming clustering for large-scale coreference. However, the greedy nature of the approach does not allow errors to be revisited. Further, they compress entities by averaging their mentions’ features. We are able to provide richer entity compression, the ability to revisit errors, and scale to larger data. Our hierarchical model provides the advantages of recently proposed entity-based coreference systems that are known to provide higher accuracy (Haghighi and Klein, 2007; Culotta et al., 2007; Yang et al., 2008; Wick et al., 2009; Haghighi and Klein, 2010). However, these systems reas</context>
</contexts>
<marker>Rao, McNamee, Dredze, 2010</marker>
<rawString>Delip Rao, Paul McNamee, and Mark Dredze. 2010. Streaming cross document entity coreference resolution. In International Conference on Computational Linguistics (COLING), pages 1050–1058, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yael Ravin</author>
<author>Zunaid Kazi</author>
</authors>
<title>Is Hillary Rodham Clinton the president? disambiguating names across documents.</title>
<date>1999</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="5023" citStr="Ravin and Kazi, 1999" startWordPosition="735" endWordPosition="738">I J. Callan Topic: WWW James Callan Topics: WWW Inst:CMU Jamie Callan Topics: IR Jamie Callan Topics: IR J. Callan Inst: CMU Name: J. Callan Ins(tu(ons: LTI Topics: WWW J. Callan Topics: IR Inst: CMU J. Callan Topics: L-S Name:Jamie Callan Ins(tu(ons: Topics: IR Name: J. Callan Ins(tu(ons: CMU,LTI Topics: WWW number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with</context>
</contexts>
<marker>Ravin, Kazi, 1999</marker>
<rawString>Yael Ravin and Zunaid Kazi. 1999. Is Hillary Rodham Clinton the president? disambiguating names across documents. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="5152" citStr="Richardson and Domingos, 2006" startWordPosition="755" endWordPosition="758"> CMU Name: J. Callan Ins(tu(ons: LTI Topics: WWW J. Callan Topics: IR Inst: CMU J. Callan Topics: L-S Name:Jamie Callan Ins(tu(ons: Topics: IR Name: J. Callan Ins(tu(ons: CMU,LTI Topics: WWW number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with the number of mentions per entity seems particularly wasteful because although it is common for an entity to be referenced by a </context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62(1-2):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Michael L Wick</author>
<author>Andrew McCallum</author>
</authors>
<title>Distantly labeling data for large scale crossdocument coreference. Computing Research Repository (CoRR),</title>
<date>2010</date>
<contexts>
<context position="5173" citStr="Singh et al., 2010" startWordPosition="759" endWordPosition="762">: LTI Topics: WWW J. Callan Topics: IR Inst: CMU J. Callan Topics: L-S Name:Jamie Callan Ins(tu(ons: Topics: IR Name: J. Callan Ins(tu(ons: CMU,LTI Topics: WWW number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with the number of mentions per entity seems particularly wasteful because although it is common for an entity to be referenced by a large number of menti</context>
</contexts>
<marker>Singh, Wick, McCallum, 2010</marker>
<rawString>Sameer Singh, Michael L. Wick, and Andrew McCallum. 2010. Distantly labeling data for large scale crossdocument coreference. Computing Research Repository (CoRR), abs/1005.4298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>Andrew McCallum</author>
</authors>
<title>Large-scale crossdocument coreference using distributed inference and hierarchical models.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics: Human Language Technologies (ACL HLT).</booktitle>
<contexts>
<context position="5320" citStr="Singh et al., 2011" startWordPosition="780" endWordPosition="783"> Topics: WWW number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with the number of mentions per entity seems particularly wasteful because although it is common for an entity to be referenced by a large number of mentions, many of these coreferent mentions are highly similar to each other. For example, in author coreference the two most common strings that refer </context>
<context position="7433" citStr="Singh et al. (2011)" startWordPosition="1118" endWordPosition="1121"> with enormous numbers of mentions because scoring entities does not require computing a quadratic number of compatibility functions. The key insight is that each node in the tree functions as a highly compact information-rich summary of its children. Thus, a small handful of upper-level nodes may summarize millions of mentions (for example, a single node may summarize all contextually similar “R. Hamming” mentions). Although inferring the structure of the entities requires reasoning over a larger statespace, the latent trees are actually beneficial to inference (as shown for shallow trees in Singh et al. (2011)), resulting in rapid progress toward high probability regions, and mirroring known benefits of auxiliary variable methods in statistical physics (such as Swendsen and Wang (1987)). Moreover, 380 each step of inference is computationally efficient because evaluating the cost of attaching (or detaching) sub-trees requires computing just a single compatibility function (as seen in Figure 1). Further, our hierarchical approach provides a number of additional advantages. First, the recursive nature of the tree (arbitrary depth and width) allows the model to adapt to different types of data and eff</context>
<context position="27280" citStr="Singh et al. (2011)" startWordPosition="4384" endWordPosition="4387">ch a similar level of accuracy due to the larger state space (Figure 4b). 4.3 Large Scale Experiments In order to demonstrate the scalability of the hierarchical model, we run it on nearly 5 million author mentions from DBLP. In under two hours (6,700 seconds), we achieve an accuracy of 80%, and in under three hours (10,600 seconds), we achieve an accuracy of over 90%. Finally, we combine DBLP with BibTeX data to produce a dataset with almost 6 million mentions (5,803,811). Our performance on &apos;&amp;quot;this dataset is similar to DBLP, taking just 13,500 seconds to reach a 90% accuracy. 5 Related Work Singh et al. (2011) introduce a hierarchical model for coreference that treats entities as a two-tiered structure, by introducing the concept of sub-entities and super-entities. Super-entities reduce the search space in order to propose fruitful jumps. Subentities provide a tighter granularity of coreference and can be used to perform larger block moves during MCMC. However, the hierarchy is fixed and shallow. In contrast, our model can be arbitrarily deep and wide. Even more importantly, their model has pairwise factors and suffers from the quadratic curse, which they address by distributing inference. The work</context>
</contexts>
<marker>Singh, Subramanya, Pereira, McCallum, 2011</marker>
<rawString>Sameer Singh, Amarnag Subramanya, Fernando Pereira, and Andrew McCallum. 2011. Large-scale crossdocument coreference using distributed inference and hierarchical models. In Association for Computational Linguistics: Human Language Technologies (ACL HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parag Singla</author>
<author>Pedro Domingos</author>
</authors>
<title>Discriminative training of Markov logic networks.</title>
<date>2005</date>
<booktitle>In AAAI,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2447" citStr="Singla and Domingos, 2005" startWordPosition="342" endWordPosition="345">nd error correcting codes” is the same” “R. Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions (e.g., bags-of-words in titles, contextual snippets and co-author lists) provide evidence for resolving such entities. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pa</context>
</contexts>
<marker>Singla, Domingos, 2005</marker>
<rawString>Parag Singla and Pedro Domingos. 2005. Discriminative training of Markov logic networks. In AAAI, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parag Singla</author>
<author>Pedro Domingos</author>
</authors>
<title>Lifted firstorder belief propagation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd national conference on Artificial intelligence - Volume 2, AAAI’08,</booktitle>
<pages>1094--1099</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="28622" citStr="Singla and Domingos, 2008" startWordPosition="4592" endWordPosition="4595">ach does not allow errors to be revisited. Further, they compress entities by averaging their mentions’ features. We are able to provide richer entity compression, the ability to revisit errors, and scale to larger data. Our hierarchical model provides the advantages of recently proposed entity-based coreference systems that are known to provide higher accuracy (Haghighi and Klein, 2007; Culotta et al., 2007; Yang et al., 2008; Wick et al., 2009; Haghighi and Klein, 2010). However, these systems reason over a single layer of entities and do not scale well. Techniques such as lifted inference (Singla and Domingos, 2008) for graphical models exploit redundancy in the data, but typically do not achieve any significant compression on coreference data be385 400,000 Number of Samples 350,000 300,000 250,000 200,000 150,000 100,000 50,000 0 Samples versus Time 0 500 1,000 1,500 2,000 Running time (s) (a) Sampling Performance Accuracy versus Samples 0 50,000 100,000 150,000 200,000 Number of Samples (b) Accuracy vs. samples (convergence accuracy as dashes) Hierar Pairwise Hierar Pairwise 1.0 0.9 0.8 F1 Accuracy 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Figure 4: Sampling Performance Plots for 145k mentions 1.0 0.9 0.8 F1 Acc</context>
</contexts>
<marker>Singla, Domingos, 2008</marker>
<rawString>Parag Singla and Pedro Domingos. 2008. Lifted firstorder belief propagation. In Proceedings of the 23rd national conference on Artificial intelligence - Volume 2, AAAI’08, pages 1094–1099. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Comput. Linguist.,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="2392" citStr="Soon et al., 2001" startWordPosition="334" endWordPosition="337">he “R. Hamming” who authored “Error detecting and error correcting codes” is the same” “R. Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions (e.g., bags-of-words in titles, contextual snippets and co-author lists) provide evidence for resolving such entities. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow sy</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Comput. Linguist., 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Swendsen</author>
<author>J S Wang</author>
</authors>
<title>Nonuniversal critical dynamics in MC simulations.</title>
<date>1987</date>
<journal>Phys. Rev. Lett.,</journal>
<volume>58</volume>
<issue>2</issue>
<contexts>
<context position="7612" citStr="Swendsen and Wang (1987)" startWordPosition="1143" endWordPosition="1146"> tree functions as a highly compact information-rich summary of its children. Thus, a small handful of upper-level nodes may summarize millions of mentions (for example, a single node may summarize all contextually similar “R. Hamming” mentions). Although inferring the structure of the entities requires reasoning over a larger statespace, the latent trees are actually beneficial to inference (as shown for shallow trees in Singh et al. (2011)), resulting in rapid progress toward high probability regions, and mirroring known benefits of auxiliary variable methods in statistical physics (such as Swendsen and Wang (1987)). Moreover, 380 each step of inference is computationally efficient because evaluating the cost of attaching (or detaching) sub-trees requires computing just a single compatibility function (as seen in Figure 1). Further, our hierarchical approach provides a number of additional advantages. First, the recursive nature of the tree (arbitrary depth and width) allows the model to adapt to different types of data and effectively compress entities of different scales (e.g., entities with more mentions may require a deeper hierarchy to compress). Second, the model contains compatibility functions a</context>
<context position="19617" citStr="Swendsen and Wang (1987)" startWordPosition="3099" endWordPosition="3102">ctually beneficial to inference. Indeed, despite the enlarged state space, inference in the hierarchical model is substantially faster than a pairwise model with a smaller state space. One explanatory intuition comes from the statistical physics community: we can view the latent tree as auxiliary variables in a data-augmentation sampling scheme that guide MCMC through the state space more efficiently. There is a large body of literature in the statistics community describing how these auxiliary variables can lead to faster convergence despite the enlarged state space (classic examples include Swendsen and Wang (1987) and slice samplers (Neal, 2000)). Further, evaluating each proposal during inference in the hierarchical model is substantially faster than in the pairwise model. Indeed, we can replace the linear number of factor evaluations (as in the pairwise model) with a constant number of factor evaluations for most proposals (for example, adding a subtree requires re-evaluating only a single parentchild factor between the subtree and the attachment point, and a single node-wise factor). Since inference must determine the structure of the entity trees in addition to coreference, it is advantageous to co</context>
</contexts>
<marker>Swendsen, Wang, 1987</marker>
<rawString>R.H. Swendsen and J.S. Wang. 1987. Nonuniversal critical dynamics in MC simulations. Phys. Rev. Lett., 58(2):68–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wick</author>
<author>Aron Culotta</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>An entity-based model for coreference resolution.</title>
<date>2009</date>
<booktitle>In SIAM International Conference on Data Mining (SDM).</booktitle>
<contexts>
<context position="3155" citStr="Wick et al., 2009" startWordPosition="458" endWordPosition="461"> of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Discriminative hierarchical factor graph</context>
<context position="28445" citStr="Wick et al., 2009" startWordPosition="4564" endWordPosition="4567">ich they address by distributing inference. The work of Rao et al. (2010) uses streaming clustering for large-scale coreference. However, the greedy nature of the approach does not allow errors to be revisited. Further, they compress entities by averaging their mentions’ features. We are able to provide richer entity compression, the ability to revisit errors, and scale to larger data. Our hierarchical model provides the advantages of recently proposed entity-based coreference systems that are known to provide higher accuracy (Haghighi and Klein, 2007; Culotta et al., 2007; Yang et al., 2008; Wick et al., 2009; Haghighi and Klein, 2010). However, these systems reason over a single layer of entities and do not scale well. Techniques such as lifted inference (Singla and Domingos, 2008) for graphical models exploit redundancy in the data, but typically do not achieve any significant compression on coreference data be385 400,000 Number of Samples 350,000 300,000 250,000 200,000 150,000 100,000 50,000 0 Samples versus Time 0 500 1,000 1,500 2,000 Running time (s) (a) Sampling Performance Accuracy versus Samples 0 50,000 100,000 150,000 200,000 Number of Samples (b) Accuracy vs. samples (convergence accu</context>
</contexts>
<marker>Wick, Culotta, Rohanimanesh, McCallum, 2009</marker>
<rawString>Michael Wick, Aron Culotta, Khashayar Rohanimanesh, and Andrew McCallum. 2009. An entity-based model for coreference resolution. In SIAM International Conference on Data Mining (SDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Jun Lang</author>
<author>Chew Lim Tan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>An entity-mention model for coreference resolution with inductive logic programming.</title>
<date>2008</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<pages>843--851</pages>
<contexts>
<context position="3114" citStr="Yang et al., 2008" startWordPosition="450" endWordPosition="453">s solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1:</context>
<context position="28426" citStr="Yang et al., 2008" startWordPosition="4560" endWordPosition="4563">quadratic curse, which they address by distributing inference. The work of Rao et al. (2010) uses streaming clustering for large-scale coreference. However, the greedy nature of the approach does not allow errors to be revisited. Further, they compress entities by averaging their mentions’ features. We are able to provide richer entity compression, the ability to revisit errors, and scale to larger data. Our hierarchical model provides the advantages of recently proposed entity-based coreference systems that are known to provide higher accuracy (Haghighi and Klein, 2007; Culotta et al., 2007; Yang et al., 2008; Wick et al., 2009; Haghighi and Klein, 2010). However, these systems reason over a single layer of entities and do not scale well. Techniques such as lifted inference (Singla and Domingos, 2008) for graphical models exploit redundancy in the data, but typically do not achieve any significant compression on coreference data be385 400,000 Number of Samples 350,000 300,000 250,000 200,000 150,000 100,000 50,000 0 Samples versus Time 0 500 1,000 1,500 2,000 Running time (s) (a) Sampling Performance Accuracy versus Samples 0 50,000 100,000 150,000 200,000 Number of Samples (b) Accuracy vs. sample</context>
</contexts>
<marker>Yang, Su, Lang, Tan, Liu, Li, 2008</marker>
<rawString>Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting Liu, and Sheng Li. 2008. An entity-mention model for coreference resolution with inductive logic programming. In Association for Computational Linguistics, pages 843–851.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>