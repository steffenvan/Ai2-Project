<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012701">
<title confidence="0.996743">
Blocked Inference in Bayesian Tree Substitution Grammars
</title>
<author confidence="0.99887">
Trevor Cohn Phil Blunsom
</author>
<affiliation confidence="0.9998515">
Department of Computer Science Computing Laboratory
University of Sheffield University of Oxford
</affiliation>
<email confidence="0.983742">
T.Cohn@dcs.shef.ac.uk Phil.Blunsom@comlab.ox.ac.uk
</email>
<sectionHeader confidence="0.99716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807363636364">
Learning a tree substitution grammar is
very challenging due to derivational am-
biguity. Our recent approach used a
Bayesian non-parametric model to induce
good derivations from treebanked input
(Cohn et al., 2009), biasing towards small
grammars composed of small generalis-
able productions. In this paper we present
a novel training method for the model us-
ing a blocked Metropolis-Hastings sam-
pler in place of the previous method’s lo-
cal Gibbs sampler. The blocked sam-
pler makes considerably larger moves than
the local sampler and consequently con-
verges in less time. A core component
of the algorithm is a grammar transforma-
tion which represents an infinite tree sub-
stitution grammar in a finite context free
grammar. This enables efficient blocked
inference for training and also improves
the parsing algorithm. Both algorithms are
shown to improve parsing accuracy.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845327272728">
Tree Substitution Grammar (TSG) is a compelling
grammar formalism which allows nonterminal
rewrites in the form of trees, thereby enabling
the modelling of complex linguistic phenomena
such as argument frames, lexical agreement and
idiomatic phrases. A fundamental problem with
TSGs is that they are difficult to estimate, even in
the supervised scenario where treebanked data is
available. This is because treebanks are typically
not annotated with their TSG derivations (how to
decompose a tree into elementary tree fragments);
instead the derivation needs to be inferred.
In recent work we proposed a TSG model which
infers an optimal decomposition under a non-
parametric Bayesian prior (Cohn et al., 2009).
This used a Gibbs sampler for training, which re-
peatedly samples for every node in every training
tree a binary value indicating whether the node is
or is not a substitution point in the tree’s deriva-
tion. Aggregated over the whole corpus, these val-
ues and the underlying trees specify the weighted
grammar. Local Gibbs samplers, although con-
ceptually simple, suffer from slow convergence
(a.k.a. poor mixing). The sampler can get easily
stuck because many locally improbable decisions
are required to escape from a locally optimal solu-
tion. This problem manifests itself both locally to
a sentence and globally over the training sample.
The net result is a sampler that is non-convergent,
overly dependent on its initialisation and cannot be
said to be sampling from the posterior.
In this paper we present a blocked Metropolis-
Hasting sampler for learning a TSG, similar to
Johnson et al. (2007). The sampler jointly updates
all the substitution variables in a tree, making
much larger moves than the local single-variable
sampler. A critical issue when developing a
Metroplis-Hastings sampler is choosing a suitable
proposal distribution, which must have the same
support as the true distribution. For our model the
natural proposal distribution is a MAP point esti-
mate, however this cannot be represented directly
as it is infinitely large. To solve this problem we
develop a grammar transformation which can suc-
cinctly represent an infinite TSG in an equivalent
finite Context Free Grammar (CFG). The trans-
formed grammar can be used as a proposal dis-
tribution, from which samples can be drawn in
polynomial time. Empirically, the blocked sam-
pler converges in fewer iterations and in less time
than the local Gibbs sampler. In addition, we also
show how the transformed grammar can be used
for parsing, which yields theoretical and empiri-
cal improvements over our previous method which
truncated the grammar.
</bodyText>
<page confidence="0.981852">
225
</page>
<note confidence="0.727199">
Proceedings of the ACL 2010 Conference Short Papers, pages 225–230,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.986895" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999669838709677">
A Tree Substitution Grammar (TSG; Bod et
al. (2003)) is a 4-tuple, G = (T, N, S, R), where
T is a set of terminal symbols, N is a set of non-
terminal symbols, S E N is the distinguished root
nonterminal and R is a set of productions (rules).
The productions take the form of tree fragments,
called elementary trees (ETs), in which each in-
ternal node is labelled with a nonterminal and each
leaf is labelled with either a terminal or a nonter-
minal. The frontier nonterminal nodes in each ET
form the sites into which other ETs can be substi-
tuted. A derivation creates a tree by recursive sub-
stitution starting with the root symbol and finish-
ing when there are no remaining frontier nonter-
minals. Figure 1 (left) shows an example deriva-
tion where the arrows denote substitution. A Prob-
abilistic Tree Substitution Grammar (PTSG) as-
signs a probability to each rule in the grammar,
where each production is assumed to be condi-
tionally independent given its root nonterminal. A
derivation’s probability is the product of the prob-
abilities of the rules therein.
In this work we employ the same non-
parametric TSG model as Cohn et al. (2009),
which we now summarise. The inference prob-
lem within this model is to identify the posterior
distribution of the elementary trees e given whole
trees t. The model is characterised by the use of
a Dirichlet Process (DP) prior over the grammar.
We define the distribution over elementary trees e
with root nonterminal symbol c as
</bodyText>
<equation confidence="0.979622">
Gc|αc, P0 — DP(αc, P0(&apos;|c))
e|c — Gc
</equation>
<bodyText confidence="0.999854818181818">
where P0(&apos;|c) (the base distribution) is a distribu-
tion over the infinite space of trees rooted with c,
and αc (the concentration parameter) controls the
model’s tendency towards either reusing elemen-
tary trees or creating novel ones as each training
instance is encountered.
Rather than representing the distribution Gc ex-
plicitly, we integrate over all possible values of
Gc. The key result required for inference is that
the conditional distribution of ei, given e−i, =
e1 ... en\ei and the root category c is:
</bodyText>
<equation confidence="0.9321914">
−i
nei,c
p(ei|e−i, c, αc, P0)=
n−i
·,c + αc
</equation>
<bodyText confidence="0.802895333333333">
where n−i
ei,c is the number number of times ei has
been used to rewrite c in e−i, and n−i
</bodyText>
<page confidence="0.97988">
·,c = Ee n−i
e,c
</page>
<figure confidence="0.971759333333333">
S
VP,0
broccoli
</figure>
<figureCaption confidence="0.992084666666667">
Figure 1: TSG derivation and its corresponding Gibbs state
for the local sampler, where each node is marked with a bi-
nary variable denoting whether it is a substitution site.
</figureCaption>
<bodyText confidence="0.998895444444444">
is the total count of rewriting c. Henceforth we
omit the —i sub-/super-script for brevity.
A primary consideration is the definition of P0.
Each ei can be generated in one of two ways:
by drawing from the base distribution, where the
probability of any particular tree is proportional to
αcP0(ei|c), or by drawing from a cache of previ-
ous expansions of c, where the probability of any
particular expansion is proportional to the number
of times that expansion has been used before. In
Cohn et al. (2009) we presented base distributions
that favour small elementary trees which we ex-
pect will generalise well to unseen data. In this
work we show that if P0 is chosen such that it
decomposes with the CFG rules contained within
each elementary tree,1 then we can use a novel dy-
namic programming algorithm to sample deriva-
tions without ever enumerating all the elementary
trees in the grammar.
The model was trained using a local Gibbs sam-
pler (Geman and Geman, 1984), a Markov chain
Monte Carlo (MCMC) method in which random
variables are repeatedly sampled conditioned on
the values of all other random variables in the
model. To formulate the local sampler, we asso-
ciate a binary variable with each non-root inter-
nal node of each tree in the training set, indicat-
ing whether that node is a substitution point or
not (illustrated in Figure 1). The sampler then vis-
its each node in a random schedule and resamples
that node’s substitution variable, where the proba-
bility of the two different configurations are given
by (1). Parsing was performed using a Metropolis-
Hastings sampler to draw derivation samples for
a string, from which the best tree was recovered.
However the sampler used for parsing was biased
</bodyText>
<footnote confidence="0.95821325">
1Both choices of base distribution in Cohn et al. (2009)
decompose into CFG rules. In this paper we focus on the
better performing one, Po , which combines a PCFG applied
recursively with a stopping probability, s, at each node.
</footnote>
<figure confidence="0.99258915">
αcP0(ei|c)
n−i
·,c + αc
+
(1)
NP
NP
S
V
NP
VP
George
hates
NP
NP,1
George
V,0
hates
NP,1
broccoli
</figure>
<page confidence="0.995502">
226
</page>
<bodyText confidence="0.9998032">
because it used as its proposal distribution a trun-
cated grammar which excluded all but a handful
of the unseen elementary trees. Consequently the
proposal had smaller support than the true model,
voiding the MCMC convergence proofs.
</bodyText>
<sectionHeader confidence="0.986004" genericHeader="method">
3 Grammar Transformation
</sectionHeader>
<bodyText confidence="0.999418153846154">
We now present a blocked sampler using the
Metropolis-Hastings (MH) algorithm to perform
sentence-level inference, based on the work of
Johnson et al. (2007) who presented a MH sampler
for a Bayesian PCFG. This approach repeats the
following steps for each sentence in the training
set: 1) run the inside algorithm (Lari and Young,
1990) to calculate marginal expansion probabil-
ities under a MAP approximation, 2) sample an
analysis top-down and 3) accept or reject using a
Metropolis-Hastings (MH) test to correct for dif-
ferences between the MAP proposal and the true
model. Though our model is similar to John-
son et al. (2007)’s, we have an added complica-
tion: the MAP grammar cannot be estimated di-
rectly. This is a consequence of the base distri-
bution having infinite support (assigning non-zero
probability to infinitely many unseen tree frag-
ments), which means the MAP has an infinite rule
set. For example, if our base distribution licences
the CFG production NP —* NP PP then our TSG
grammar will contain the infinite set of elemen-
tary trees NP —* NP PP, NP —* (NP NP PP) PP,
NP —* (NP (NP NP PP) PP) PP, ... with decreas-
ing but non-zero probability.
However, we can represent the infinite MAP us-
ing a grammar transformation inspired by Good-
man (2003), which represents the MAP TSG in an
equivalent finite PCFG.2 Under the transformed
PCFG inference is efficient, allowing its use as
the proposal distribution in a blocked MH sam-
pler. We represent the MAP using the grammar
transformation in Table 1 which separates the ne,e
and Po terms in (1) into two separate CFGs, A and
B. Grammar A has productions for every ET with
ne,e &gt; 1 which are assigned unsmoothed proba-
bilities: omitting the Po term from (1).3 Grammar
B has productions for every CFG production li-
censed under Po; its productions are denoted using
</bodyText>
<footnote confidence="0.950049333333333">
2Backoff DOP uses a similar packed representation to en-
code the set of smaller subtrees for a given elementary tree
(Sima’an and Buratto, 2003), which are used to smooth its
probability estimate.
3The transform assumes inside inference. For Viterbi re-
place the probability for c → sign(e) with n
</footnote>
<table confidence="0.999589315789474">
For every ET, e, rewriting c with non-zero count:
n− e,c
→
c sign(e) n−·,c+αc
For every internal node ei in e with children ei,1, ... ,ei,n
sign(ei) → sign(ei,1) ... sign(ei,n) 1
For every nonterminal, c:
0αc
c → cn− ·,c+αc
For every pre-terminal CFG production, c → t:
c0 → t PCFG(c → t)
For every unary CFG production, c → a:
c0 → a PCFG(c → a)sa
c0 → a0 PCFG(c → a)(1 − sa)
For every binary CFG production, c → ab:
c0 → ab PCFG(c → ab)sasb
c0 → ab0 PCFG(c → ab)sa(1 − sb)
c0 → a0b PCFG(c → ab)(1 − sa)sb
c0 → a0b0 PCFG(c → ab)(1 − sa)(1 − sb)
</table>
<tableCaption confidence="0.980047666666667">
Table 1: Grammar transformation rules to map a MAP TSG
into a CFG. Production probabilities are shown to the right of
each rule. The sign(e) function creates a unique string sig-
nature for an ET e (where the signature of a frontier node is
itself) and sc is the Bernoulli probability of c being a substi-
tution variable (and stopping the P0 recursion).
</tableCaption>
<bodyText confidence="0.99933684375">
primed (’) nonterminals. The rule c —* c&apos; bridges
from A to B, weighted by the smoothing term
excluding Po, which is computed recursively via
child productions. The remaining rules in gram-
mar B correspond to every CFG production in the
underlying PCFG base distribution, coupled with
the binary decision whether or not nonterminal
children should be substitution sites (frontier non-
terminals). This choice affects the rule probability
by including a s or 1 − s factor, and child sub-
stitution sites also function as a bridge back from
grammar B to A. In this way there are often two
equivalent paths to reach the same chart cell using
the same elementary tree – via grammar A using
observed TSG productions and via grammar B us-
ing Po backoff; summing these yields the desired
net probability.
Figure 2 shows an example of the transforma-
tion of an elementary tree with non-zero count,
ne,e &gt; 1, into the two types of CFG rules. Both
parts are capable of parsing the string NP, saw, NP
into a S, as illustrated in Figure 3; summing the
probability of both analyses gives the model prob-
ability from (1). Note that although the probabili-
ties exactly match the true model for a single ele-
mentary tree, the probability of derivations com-
posed of many elementary trees may not match
because the model’s caching behaviour has been
suppressed, i.e., the counts, n, are not incremented
during the course of a derivation.
For training we define the MH sampler as fol-
lows. First we estimate the MAP grammar over
</bodyText>
<equation confidence="0.93861675">
e,c+αcP0(e |c)
−
.
n− ·,c+αc
227
�
e,S
�-,S+αS
VP{V{saw},NP} --+ V{saw} NP 1
V{saw} --+ saw 1
αS
S � S’
�-,S+αS
S’ --+ NP VP’ PCFG(S --+ NP VP)sNP(1 − sV P)
VP’ --+ V’ NP PCFG(VP --+ V NP)(1 − sV )sNP
V’ --+ saw PCFG(V saw)
</equation>
<figureCaption confidence="0.9998346">
Figure 2: Example of the transformed grammar for the ET
(S NP (VP (V saw) NP)). Taking the product of the rule
scores above the line yields the left term in (1), and the prod-
uct of the scores below the line yields the right term.
Figure 3: Example trees under the grammar transform, which
</figureCaption>
<bodyText confidence="0.980579927272728">
both encode the same TSG derivation from Figure 1. The left
tree encodes that the S --+ NP (VP (V hates) NP elementary
tree was drawn from the cache, while for the right tree this
same elementary tree was drawn from the base distribution
(the left and right terms in (1), respectively).
the derivations of training corpus excluding the
current tree, which we represent using the PCFG
transformation. The next step is to sample deriva-
tions for a given tree, for which we use a con-
strained variant of the inside algorithm (Lari and
Young, 1990). We must ensure that the TSG
derivation produces the given tree, and therefore
during inside inference we only consider spans
that are constituents in the tree and are labelled
with the correct nonterminal. Nonterminals are
said to match their primed and signed counter-
parts, e.g., NP&apos; and NP{DT,NN{car}} both match
NP. Under the tree constraints the time complex-
ity of inside inference is linear in the length of the
sentence. A derivation is then sampled from the
inside chart using a top-down traversal (Johnson
et al., 2007), and converted back into its equiva-
lent TSG derivation. The derivation is scored with
the true model and accepted or rejected using the
MH test; accepted samples then replace the cur-
rent derivation for the tree, and rejected samples
leave the previous derivation unchanged. These
steps are then repeated for another tree in the train-
ing set, and the process is then repeated over the
full training set many times.
Parsing The grammar transform is not only use-
ful for training, but also for parsing. To parse a
sentence we sample a number of TSG derivations
from the MAP which are then accepted or rejected
into the full model using a MH step. The samples
are obtained from the same transformed grammar
but adapting the algorithm for an unsupervised set-
ting where parse trees are not available. For this
we use the standard inside algorithm applied to
the sentence, omitting the tree constraints, which
has time complexity cubic in the length of the sen-
tence. We then sample a derivation from the in-
side chart and perform the MH acceptance test.
This setup is theoretically more appealing than our
previous approach in which we truncated the ap-
proximation grammar to exclude most of the zero
count rules (Cohn et al., 2009). We found that
both the maximum probability derivation and tree
were considerably worse than a tree constructed
to maximise the expected number of correct CFG
rules (MER), based on Goodman’s (2003) algo-
rithm for maximising labelled recall. For this rea-
son we the MER parsing algorithm using sampled
Monte Carlo estimates for the marginals over CFG
rules at each sentence span.
</bodyText>
<sectionHeader confidence="0.999239" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999702818181818">
We tested our model on the Penn treebank using
the same data setup as Cohn et al. (2009). Specifi-
cally, we used only section 2 for training and sec-
tion 22 (devel) for reporting results. Our models
were all sampled for 5k iterations with hyperpa-
rameter inference for α, and s, b c E N, but in
contrast to our previous approach we did not use
annealing which we did not find to help general-
isation accuracy. The MH acceptance rates were
in excess of 99% across both training and parsing.
All results are averages over three runs.
For training the blocked MH sampler exhibits
faster convergence than the local Gibbs sam-
pler, as shown in Figure 4. Irrespective of the
initialisation the blocked sampler finds higher
likelihood states in many fewer iterations (the
same trend continues until iteration 5k). To be
fair, the blocked sampler is slower per iteration
(roughly 50% worse) due to the higher overheads
of the grammar transform and performing dy-
namic programming (despite nominal optimisa-
tion).4 Even after accounting for the time differ-
</bodyText>
<footnote confidence="0.9886665">
4The speed difference diminishes with corpus size: on
sections 2–22 the blocked sampler is only 19% slower per
</footnote>
<figure confidence="0.982849869565217">
NP
VP’
NP
VP{V{hates}},NP
S
S{NP,{VP{V{hates}},NP}}
George
V{hates}
hates
NP
broccoli
George
NP
V’
hates
broccoli
S
S’
S --+ NP VP{V{saw},NP}
�
228
log likelihood
iteration
</figure>
<figureCaption confidence="0.998571">
Figure 4: Training likelihood vs. iteration. Each sampling
method was initialised with both minimal and maximal ele-
mentary trees.
</figureCaption>
<table confidence="0.9994724">
Training truncated transform
Local minimal init 77.63 77.98
Local maximal init 77.19 77.71
Blocked minimal init 77.98 78.40
Blocked maximal init 77.67 78.24
</table>
<tableCaption confidence="0.901922666666667">
Table 2: Development F1 scores using the truncated pars-
ing algorithm and the novel grammar transform algorithm for
four different training configurations.
</tableCaption>
<bodyText confidence="0.988983136363637">
ence the blocked sampler is more effective than the
local Gibbs sampler. Training likelihood is highly
correlated with generalisation F1 (Pearson’s cor-
relation efficient of 0.95), and therefore improving
the sampler convergence will have immediate ef-
fects on performance.
Parsing results are shown in Table 2.5 The
blocked sampler results in better generalisation F1
scores than the local Gibbs sampler, irrespective of
the initialisation condition or parsing method used.
The use of the grammar transform in parsing also
yields better scores irrespective of the underlying
model. Together these results strongly advocate
the use of the grammar transform for inference in
infinite TSGs.
We also trained the model on the standard Penn
treebank training set (sections 2–21). We ini-
tialised the model with the final sample from a
run on the small training set, and used the blocked
sampler for 6500 iterations. Averaged over three
runs, the test F1 (section 23) was 85.3 an improve-
iteration than the local sampler.
</bodyText>
<footnote confidence="0.8123265">
5Our baseline ‘Local maximal init’ slightly exceeds pre-
viously reported score of 76.89% (Cohn et al., 2009).
</footnote>
<bodyText confidence="0.9999041875">
ment over our earlier 84.0 (Cohn et al., 2009)
although still well below state-of-the-art parsers.
We conjecture that the performance gap is due to
the model using an overly simplistic treatment of
unknown words, and also a further mixing prob-
lems with the sampler. For the full data set the
counts are much larger in magnitude which leads
to stronger modes. The sampler has difficulty es-
caping such modes and therefore is slower to mix.
One way to solve the mixing problem is for the
sampler to make more global moves, e.g., with
table label resampling (Johnson and Goldwater,
2009) or split-merge (Jain and Neal, 2000). An-
other way is to use a variational approximation in-
stead of MCMC sampling (Wainwright and Jor-
dan, 2008).
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9999514375">
We have demonstrated how our grammar trans-
formation can implicitly represent an exponential
space of tree fragments efficiently, allowing us
to build a sampler with considerably better mix-
ing properties than a local Gibbs sampler. The
same technique was also shown to improve the
parsing algorithm. These improvements are in
no way limited to our particular choice of a TSG
parsing model, many hierarchical Bayesian mod-
els have been proposed which would also permit
similar optimised samplers. In particular mod-
els which induce segmentations of complex struc-
tures stand to benefit from this work; Examples
include the word segmentation model of Goldwa-
ter et al. (2006) for which it would be trivial to
adapt our technique to develop a blocked sampler.
Hierarchical Bayesian segmentation models have
also become popular in statistical machine transla-
tion where there is a need to learn phrasal transla-
tion structures that can be decomposed at the word
level (DeNero et al., 2008; Blunsom et al., 2009;
Cohn and Blunsom, 2009). We envisage similar
representations being applied to these models to
improve their mixing properties.
A particularly interesting avenue for further re-
search is to employ our blocked sampler for un-
supervised grammar induction. While it is diffi-
cult to extend the local Gibbs sampler to the case
where the tree is not observed, the dynamic pro-
gram for our blocked sampler can be easily used
for unsupervised inference by omitting the tree
matching constraints.
</bodyText>
<figure confidence="0.7837875">
0 100 200 300 400 500
−330000 −325000 −320000 −315000 −310000 −305000
Block maximal init
Block minimal init
Local minimal init
Local maximal init
</figure>
<page confidence="0.995166">
229
</page>
<sectionHeader confidence="0.996101" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999877129870129">
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP (ACL-
IJCNLP), pages 782–790, Suntec, Singapore, Au-
gust.
Rens Bod, Remko Scha, and Khalil Sima’an, editors.
2003. Data-oriented parsing. Center for the Study
of Language and Information - Studies in Computa-
tional Linguistics. University of Chicago Press.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
model of syntax-directed tree to string grammar in-
duction. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 352–361, Singapore, August.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 548–556, Boulder, Colorado, June.
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 314–323, Honolulu,
Hawaii, October.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6:721–741.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 673–680,
Sydney, Australia, July.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al. (Bod et al., 2003),
chapter 8.
Sonia Jain and Radford M. Neal. 2000. A split-merge
Markov chain Monte Carlo procedure for the Dirich-
let process mixture model. Journal of Computa-
tional and Graphical Statistics, 13:158–182.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317–325,
Boulder, Colorado, June.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 139–146,
Rochester, NY, April.
Karim Lari and Steve J. Young. 1990. The esti-
mation of stochastic context-free grammars using
the inside-outside algorithm. Computer Speech and
Language, 4:35–56.
Khalil Sima’an and Luciano Buratto. 2003. Backoff
parameter estimation for the dop model. In Nada
Lavrac, Dragan Gamberger, Ljupco Todorovski, and
Hendrik Blockeel, editors, ECML, volume 2837 of
Lecture Notes in Computer Science, pages 373–384.
Springer.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Vari-
ational Inference. Now Publishers Inc., Hanover,
MA, USA.
</reference>
<page confidence="0.997085">
230
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904396">
<title confidence="0.999569">Blocked Inference in Bayesian Tree Substitution Grammars</title>
<author confidence="0.999633">Trevor Cohn Phil Blunsom</author>
<affiliation confidence="0.999933">Department of Computer Science Computing Laboratory University of Sheffield University of Oxford</affiliation>
<email confidence="0.915951">T.Cohn@dcs.shef.ac.ukPhil.Blunsom@comlab.ox.ac.uk</email>
<abstract confidence="0.999453652173913">Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently converges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACLIJCNLP),</booktitle>
<pages>782--790</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="20907" citStr="Blunsom et al., 2009" startWordPosition="3534" endWordPosition="3537">ng model, many hierarchical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al. (2006) for which it would be trivial to adapt our technique to develop a blocked sampler. Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level (DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009). We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is difficult to extend the local Gibbs sampler to the case where the tree is not observed, the dynamic program for our blocked sampler can be easily used for unsupervised inference by omitting the tree matching constraints. 0 100 200 300 400 500 −330000 −325000 −320000 −315000 −310000 −305000 Block maximal init Block minimal init Local </context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACLIJCNLP), pages 782–790, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<date>2003</date>
<booktitle>Data-oriented parsing. Center for the Study of Language and Information - Studies in Computational Linguistics.</booktitle>
<editor>Rens Bod, Remko Scha, and Khalil Sima’an, editors.</editor>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="3965" citStr="(2003)" startWordPosition="611" endWordPosition="611">can be used as a proposal distribution, from which samples can be drawn in polynomial time. Empirically, the blocked sampler converges in fewer iterations and in less time than the local Gibbs sampler. In addition, we also show how the transformed grammar can be used for parsing, which yields theoretical and empirical improvements over our previous method which truncated the grammar. 225 Proceedings of the ACL 2010 Conference Short Papers, pages 225–230, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2 Background A Tree Substitution Grammar (TSG; Bod et al. (2003)) is a 4-tuple, G = (T, N, S, R), where T is a set of terminal symbols, N is a set of nonterminal symbols, S E N is the distinguished root nonterminal and R is a set of productions (rules). The productions take the form of tree fragments, called elementary trees (ETs), in which each internal node is labelled with a nonterminal and each leaf is labelled with either a terminal or a nonterminal. The frontier nonterminal nodes in each ET form the sites into which other ETs can be substituted. A derivation creates a tree by recursive substitution starting with the root symbol and finishing when the</context>
<context position="9828" citStr="(2003)" startWordPosition="1634" endWordPosition="1634">lication: the MAP grammar cannot be estimated directly. This is a consequence of the base distribution having infinite support (assigning non-zero probability to infinitely many unseen tree fragments), which means the MAP has an infinite rule set. For example, if our base distribution licences the CFG production NP —* NP PP then our TSG grammar will contain the infinite set of elementary trees NP —* NP PP, NP —* (NP NP PP) PP, NP —* (NP (NP NP PP) PP) PP, ... with decreasing but non-zero probability. However, we can represent the infinite MAP using a grammar transformation inspired by Goodman (2003), which represents the MAP TSG in an equivalent finite PCFG.2 Under the transformed PCFG inference is efficient, allowing its use as the proposal distribution in a blocked MH sampler. We represent the MAP using the grammar transformation in Table 1 which separates the ne,e and Po terms in (1) into two separate CFGs, A and B. Grammar A has productions for every ET with ne,e &gt; 1 which are assigned unsmoothed probabilities: omitting the Po term from (1).3 Grammar B has productions for every CFG production licensed under Po; its productions are denoted using 2Backoff DOP uses a similar packed repr</context>
<context position="16086" citStr="(2003)" startWordPosition="2754" endWordPosition="2754">standard inside algorithm applied to the sentence, omitting the tree constraints, which has time complexity cubic in the length of the sentence. We then sample a derivation from the inside chart and perform the MH acceptance test. This setup is theoretically more appealing than our previous approach in which we truncated the approximation grammar to exclude most of the zero count rules (Cohn et al., 2009). We found that both the maximum probability derivation and tree were considerably worse than a tree constructed to maximise the expected number of correct CFG rules (MER), based on Goodman’s (2003) algorithm for maximising labelled recall. For this reason we the MER parsing algorithm using sampled Monte Carlo estimates for the marginals over CFG rules at each sentence span. 4 Experiments We tested our model on the Penn treebank using the same data setup as Cohn et al. (2009). Specifically, we used only section 2 for training and section 22 (devel) for reporting results. Our models were all sampled for 5k iterations with hyperparameter inference for α, and s, b c E N, but in contrast to our previous approach we did not use annealing which we did not find to help generalisation accuracy. </context>
</contexts>
<marker>2003</marker>
<rawString>Rens Bod, Remko Scha, and Khalil Sima’an, editors. 2003. Data-oriented parsing. Center for the Study of Language and Information - Studies in Computational Linguistics. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>352--361</pages>
<location>Singapore,</location>
<contexts>
<context position="20932" citStr="Cohn and Blunsom, 2009" startWordPosition="3538" endWordPosition="3541">hical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al. (2006) for which it would be trivial to adapt our technique to develop a blocked sampler. Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level (DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009). We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is difficult to extend the local Gibbs sampler to the case where the tree is not observed, the dynamic program for our blocked sampler can be easily used for unsupervised inference by omitting the tree matching constraints. 0 100 200 300 400 500 −330000 −325000 −320000 −315000 −310000 −305000 Block maximal init Block minimal init Local minimal init Local maxima</context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 352–361, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate treesubstitution grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>548--556</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1832" citStr="Cohn et al., 2009" startWordPosition="268" endWordPosition="271"> rewrites in the form of trees, thereby enabling the modelling of complex linguistic phenomena such as argument frames, lexical agreement and idiomatic phrases. A fundamental problem with TSGs is that they are difficult to estimate, even in the supervised scenario where treebanked data is available. This is because treebanks are typically not annotated with their TSG derivations (how to decompose a tree into elementary tree fragments); instead the derivation needs to be inferred. In recent work we proposed a TSG model which infers an optimal decomposition under a nonparametric Bayesian prior (Cohn et al., 2009). This used a Gibbs sampler for training, which repeatedly samples for every node in every training tree a binary value indicating whether the node is or is not a substitution point in the tree’s derivation. Aggregated over the whole corpus, these values and the underlying trees specify the weighted grammar. Local Gibbs samplers, although conceptually simple, suffer from slow convergence (a.k.a. poor mixing). The sampler can get easily stuck because many locally improbable decisions are required to escape from a locally optimal solution. This problem manifests itself both locally to a sentence</context>
<context position="5045" citStr="Cohn et al. (2009)" startWordPosition="802" endWordPosition="805">h other ETs can be substituted. A derivation creates a tree by recursive substitution starting with the root symbol and finishing when there are no remaining frontier nonterminals. Figure 1 (left) shows an example derivation where the arrows denote substitution. A Probabilistic Tree Substitution Grammar (PTSG) assigns a probability to each rule in the grammar, where each production is assumed to be conditionally independent given its root nonterminal. A derivation’s probability is the product of the probabilities of the rules therein. In this work we employ the same nonparametric TSG model as Cohn et al. (2009), which we now summarise. The inference problem within this model is to identify the posterior distribution of the elementary trees e given whole trees t. The model is characterised by the use of a Dirichlet Process (DP) prior over the grammar. We define the distribution over elementary trees e with root nonterminal symbol c as Gc|αc, P0 — DP(αc, P0(&apos;|c)) e|c — Gc where P0(&apos;|c) (the base distribution) is a distribution over the infinite space of trees rooted with c, and αc (the concentration parameter) controls the model’s tendency towards either reusing elementary trees or creating novel ones</context>
<context position="6773" citStr="Cohn et al. (2009)" startWordPosition="1105" endWordPosition="1108">r the local sampler, where each node is marked with a binary variable denoting whether it is a substitution site. is the total count of rewriting c. Henceforth we omit the —i sub-/super-script for brevity. A primary consideration is the definition of P0. Each ei can be generated in one of two ways: by drawing from the base distribution, where the probability of any particular tree is proportional to αcP0(ei|c), or by drawing from a cache of previous expansions of c, where the probability of any particular expansion is proportional to the number of times that expansion has been used before. In Cohn et al. (2009) we presented base distributions that favour small elementary trees which we expect will generalise well to unseen data. In this work we show that if P0 is chosen such that it decomposes with the CFG rules contained within each elementary tree,1 then we can use a novel dynamic programming algorithm to sample derivations without ever enumerating all the elementary trees in the grammar. The model was trained using a local Gibbs sampler (Geman and Geman, 1984), a Markov chain Monte Carlo (MCMC) method in which random variables are repeatedly sampled conditioned on the values of all other random v</context>
<context position="8035" citStr="Cohn et al. (2009)" startWordPosition="1320" endWordPosition="1323">l sampler, we associate a binary variable with each non-root internal node of each tree in the training set, indicating whether that node is a substitution point or not (illustrated in Figure 1). The sampler then visits each node in a random schedule and resamples that node’s substitution variable, where the probability of the two different configurations are given by (1). Parsing was performed using a MetropolisHastings sampler to draw derivation samples for a string, from which the best tree was recovered. However the sampler used for parsing was biased 1Both choices of base distribution in Cohn et al. (2009) decompose into CFG rules. In this paper we focus on the better performing one, Po , which combines a PCFG applied recursively with a stopping probability, s, at each node. αcP0(ei|c) n−i ·,c + αc + (1) NP NP S V NP VP George hates NP NP,1 George V,0 hates NP,1 broccoli 226 because it used as its proposal distribution a truncated grammar which excluded all but a handful of the unseen elementary trees. Consequently the proposal had smaller support than the true model, voiding the MCMC convergence proofs. 3 Grammar Transformation We now present a blocked sampler using the Metropolis-Hastings (MH</context>
<context position="15888" citStr="Cohn et al., 2009" startWordPosition="2720" endWordPosition="2723">d into the full model using a MH step. The samples are obtained from the same transformed grammar but adapting the algorithm for an unsupervised setting where parse trees are not available. For this we use the standard inside algorithm applied to the sentence, omitting the tree constraints, which has time complexity cubic in the length of the sentence. We then sample a derivation from the inside chart and perform the MH acceptance test. This setup is theoretically more appealing than our previous approach in which we truncated the approximation grammar to exclude most of the zero count rules (Cohn et al., 2009). We found that both the maximum probability derivation and tree were considerably worse than a tree constructed to maximise the expected number of correct CFG rules (MER), based on Goodman’s (2003) algorithm for maximising labelled recall. For this reason we the MER parsing algorithm using sampled Monte Carlo estimates for the marginals over CFG rules at each sentence span. 4 Experiments We tested our model on the Penn treebank using the same data setup as Cohn et al. (2009). Specifically, we used only section 2 for training and section 22 (devel) for reporting results. Our models were all sa</context>
<context position="19165" citStr="Cohn et al., 2009" startWordPosition="3246" endWordPosition="3249">form in parsing also yields better scores irrespective of the underlying model. Together these results strongly advocate the use of the grammar transform for inference in infinite TSGs. We also trained the model on the standard Penn treebank training set (sections 2–21). We initialised the model with the final sample from a run on the small training set, and used the blocked sampler for 6500 iterations. Averaged over three runs, the test F1 (section 23) was 85.3 an improveiteration than the local sampler. 5Our baseline ‘Local maximal init’ slightly exceeds previously reported score of 76.89% (Cohn et al., 2009). ment over our earlier 84.0 (Cohn et al., 2009) although still well below state-of-the-art parsers. We conjecture that the performance gap is due to the model using an overly simplistic treatment of unknown words, and also a further mixing problems with the sampler. For the full data set the counts are much larger in magnitude which leads to stronger modes. The sampler has difficulty escaping such modes and therefore is slower to mix. One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate treesubstitution grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 548–556, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>314--323</pages>
<location>Honolulu, Hawaii,</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314–323, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="7234" citStr="Geman and Geman, 1984" startWordPosition="1185" endWordPosition="1188">ansions of c, where the probability of any particular expansion is proportional to the number of times that expansion has been used before. In Cohn et al. (2009) we presented base distributions that favour small elementary trees which we expect will generalise well to unseen data. In this work we show that if P0 is chosen such that it decomposes with the CFG rules contained within each elementary tree,1 then we can use a novel dynamic programming algorithm to sample derivations without ever enumerating all the elementary trees in the grammar. The model was trained using a local Gibbs sampler (Geman and Geman, 1984), a Markov chain Monte Carlo (MCMC) method in which random variables are repeatedly sampled conditioned on the values of all other random variables in the model. To formulate the local sampler, we associate a binary variable with each non-root internal node of each tree in the training set, indicating whether that node is a substitution point or not (illustrated in Figure 1). The sampler then visits each node in a random schedule and resamples that node’s substitution variable, where the probability of the two different configurations are given by (1). Parsing was performed using a MetropolisH</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="20576" citStr="Goldwater et al. (2006)" startWordPosition="3478" endWordPosition="3482">mation can implicitly represent an exponential space of tree fragments efficiently, allowing us to build a sampler with considerably better mixing properties than a local Gibbs sampler. The same technique was also shown to improve the parsing algorithm. These improvements are in no way limited to our particular choice of a TSG parsing model, many hierarchical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al. (2006) for which it would be trivial to adapt our technique to develop a blocked sampler. Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level (DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009). We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is diff</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 673–680, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions.</title>
<date>2003</date>
<booktitle>In Bod et</booktitle>
<note>chapter 8.</note>
<contexts>
<context position="9828" citStr="Goodman (2003)" startWordPosition="1632" endWordPosition="1634">ded complication: the MAP grammar cannot be estimated directly. This is a consequence of the base distribution having infinite support (assigning non-zero probability to infinitely many unseen tree fragments), which means the MAP has an infinite rule set. For example, if our base distribution licences the CFG production NP —* NP PP then our TSG grammar will contain the infinite set of elementary trees NP —* NP PP, NP —* (NP NP PP) PP, NP —* (NP (NP NP PP) PP) PP, ... with decreasing but non-zero probability. However, we can represent the infinite MAP using a grammar transformation inspired by Goodman (2003), which represents the MAP TSG in an equivalent finite PCFG.2 Under the transformed PCFG inference is efficient, allowing its use as the proposal distribution in a blocked MH sampler. We represent the MAP using the grammar transformation in Table 1 which separates the ne,e and Po terms in (1) into two separate CFGs, A and B. Grammar A has productions for every ET with ne,e &gt; 1 which are assigned unsmoothed probabilities: omitting the Po term from (1).3 Grammar B has productions for every CFG production licensed under Po; its productions are denoted using 2Backoff DOP uses a similar packed repr</context>
</contexts>
<marker>Goodman, 2003</marker>
<rawString>Joshua Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. In Bod et al. (Bod et al., 2003), chapter 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonia Jain</author>
<author>Radford M Neal</author>
</authors>
<title>A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model.</title>
<date>2000</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<pages>13--158</pages>
<contexts>
<context position="19787" citStr="Jain and Neal, 2000" startWordPosition="3352" endWordPosition="3355">ment over our earlier 84.0 (Cohn et al., 2009) although still well below state-of-the-art parsers. We conjecture that the performance gap is due to the model using an overly simplistic treatment of unknown words, and also a further mixing problems with the sampler. For the full data set the counts are much larger in magnitude which leads to stronger modes. The sampler has difficulty escaping such modes and therefore is slower to mix. One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge (Jain and Neal, 2000). Another way is to use a variational approximation instead of MCMC sampling (Wainwright and Jordan, 2008). 5 Discussion We have demonstrated how our grammar transformation can implicitly represent an exponential space of tree fragments efficiently, allowing us to build a sampler with considerably better mixing properties than a local Gibbs sampler. The same technique was also shown to improve the parsing algorithm. These improvements are in no way limited to our particular choice of a TSG parsing model, many hierarchical Bayesian models have been proposed which would also permit similar optim</context>
</contexts>
<marker>Jain, Neal, 2000</marker>
<rawString>Sonia Jain and Radford M. Neal. 2000. A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model. Journal of Computational and Graphical Statistics, 13:158–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>317--325</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="19750" citStr="Johnson and Goldwater, 2009" startWordPosition="3346" endWordPosition="3349">eported score of 76.89% (Cohn et al., 2009). ment over our earlier 84.0 (Cohn et al., 2009) although still well below state-of-the-art parsers. We conjecture that the performance gap is due to the model using an overly simplistic treatment of unknown words, and also a further mixing problems with the sampler. For the full data set the counts are much larger in magnitude which leads to stronger modes. The sampler has difficulty escaping such modes and therefore is slower to mix. One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge (Jain and Neal, 2000). Another way is to use a variational approximation instead of MCMC sampling (Wainwright and Jordan, 2008). 5 Discussion We have demonstrated how our grammar transformation can implicitly represent an exponential space of tree fragments efficiently, allowing us to build a sampler with considerably better mixing properties than a local Gibbs sampler. The same technique was also shown to improve the parsing algorithm. These improvements are in no way limited to our particular choice of a TSG parsing model, many hierarchical Bayesian models have been proposed </context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 317–325, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<location>Rochester, NY,</location>
<contexts>
<context position="2730" citStr="Johnson et al. (2007)" startWordPosition="415" endWordPosition="418">es specify the weighted grammar. Local Gibbs samplers, although conceptually simple, suffer from slow convergence (a.k.a. poor mixing). The sampler can get easily stuck because many locally improbable decisions are required to escape from a locally optimal solution. This problem manifests itself both locally to a sentence and globally over the training sample. The net result is a sampler that is non-convergent, overly dependent on its initialisation and cannot be said to be sampling from the posterior. In this paper we present a blocked MetropolisHasting sampler for learning a TSG, similar to Johnson et al. (2007). The sampler jointly updates all the substitution variables in a tree, making much larger moves than the local single-variable sampler. A critical issue when developing a Metroplis-Hastings sampler is choosing a suitable proposal distribution, which must have the same support as the true distribution. For our model the natural proposal distribution is a MAP point estimate, however this cannot be represented directly as it is infinitely large. To solve this problem we develop a grammar transformation which can succinctly represent an infinite TSG in an equivalent finite Context Free Grammar (C</context>
<context position="8726" citStr="Johnson et al. (2007)" startWordPosition="1437" endWordPosition="1440">ming one, Po , which combines a PCFG applied recursively with a stopping probability, s, at each node. αcP0(ei|c) n−i ·,c + αc + (1) NP NP S V NP VP George hates NP NP,1 George V,0 hates NP,1 broccoli 226 because it used as its proposal distribution a truncated grammar which excluded all but a handful of the unseen elementary trees. Consequently the proposal had smaller support than the true model, voiding the MCMC convergence proofs. 3 Grammar Transformation We now present a blocked sampler using the Metropolis-Hastings (MH) algorithm to perform sentence-level inference, based on the work of Johnson et al. (2007) who presented a MH sampler for a Bayesian PCFG. This approach repeats the following steps for each sentence in the training set: 1) run the inside algorithm (Lari and Young, 1990) to calculate marginal expansion probabilities under a MAP approximation, 2) sample an analysis top-down and 3) accept or reject using a Metropolis-Hastings (MH) test to correct for differences between the MAP proposal and the true model. Though our model is similar to Johnson et al. (2007)’s, we have an added complication: the MAP grammar cannot be estimated directly. This is a consequence of the base distribution h</context>
<context position="14665" citStr="Johnson et al., 2007" startWordPosition="2509" endWordPosition="2512"> tree, for which we use a constrained variant of the inside algorithm (Lari and Young, 1990). We must ensure that the TSG derivation produces the given tree, and therefore during inside inference we only consider spans that are constituents in the tree and are labelled with the correct nonterminal. Nonterminals are said to match their primed and signed counterparts, e.g., NP&apos; and NP{DT,NN{car}} both match NP. Under the tree constraints the time complexity of inside inference is linear in the length of the sentence. A derivation is then sampled from the inside chart using a top-down traversal (Johnson et al., 2007), and converted back into its equivalent TSG derivation. The derivation is scored with the true model and accepted or rejected using the MH test; accepted samples then replace the current derivation for the tree, and rejected samples leave the previous derivation unchanged. These steps are then repeated for another tree in the training set, and the process is then repeated over the full training set many times. Parsing The grammar transform is not only useful for training, but also for parsing. To parse a sentence we sample a number of TSG derivations from the MAP which are then accepted or re</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 139–146, Rochester, NY, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Lari</author>
<author>Steve J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="8906" citStr="Lari and Young, 1990" startWordPosition="1468" endWordPosition="1471"> hates NP,1 broccoli 226 because it used as its proposal distribution a truncated grammar which excluded all but a handful of the unseen elementary trees. Consequently the proposal had smaller support than the true model, voiding the MCMC convergence proofs. 3 Grammar Transformation We now present a blocked sampler using the Metropolis-Hastings (MH) algorithm to perform sentence-level inference, based on the work of Johnson et al. (2007) who presented a MH sampler for a Bayesian PCFG. This approach repeats the following steps for each sentence in the training set: 1) run the inside algorithm (Lari and Young, 1990) to calculate marginal expansion probabilities under a MAP approximation, 2) sample an analysis top-down and 3) accept or reject using a Metropolis-Hastings (MH) test to correct for differences between the MAP proposal and the true model. Though our model is similar to Johnson et al. (2007)’s, we have an added complication: the MAP grammar cannot be estimated directly. This is a consequence of the base distribution having infinite support (assigning non-zero probability to infinitely many unseen tree fragments), which means the MAP has an infinite rule set. For example, if our base distributio</context>
<context position="14136" citStr="Lari and Young, 1990" startWordPosition="2422" endWordPosition="2425">lds the right term. Figure 3: Example trees under the grammar transform, which both encode the same TSG derivation from Figure 1. The left tree encodes that the S --+ NP (VP (V hates) NP elementary tree was drawn from the cache, while for the right tree this same elementary tree was drawn from the base distribution (the left and right terms in (1), respectively). the derivations of training corpus excluding the current tree, which we represent using the PCFG transformation. The next step is to sample derivations for a given tree, for which we use a constrained variant of the inside algorithm (Lari and Young, 1990). We must ensure that the TSG derivation produces the given tree, and therefore during inside inference we only consider spans that are constituents in the tree and are labelled with the correct nonterminal. Nonterminals are said to match their primed and signed counterparts, e.g., NP&apos; and NP{DT,NN{car}} both match NP. Under the tree constraints the time complexity of inside inference is linear in the length of the sentence. A derivation is then sampled from the inside chart using a top-down traversal (Johnson et al., 2007), and converted back into its equivalent TSG derivation. The derivation</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>Karim Lari and Steve J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
<author>Luciano Buratto</author>
</authors>
<title>Backoff parameter estimation for the dop model.</title>
<date>2003</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>2837</volume>
<pages>373--384</pages>
<editor>In Nada Lavrac, Dragan Gamberger, Ljupco Todorovski, and Hendrik Blockeel, editors, ECML,</editor>
<publisher>Springer.</publisher>
<marker>Sima’an, Buratto, 2003</marker>
<rawString>Khalil Sima’an and Luciano Buratto. 2003. Backoff parameter estimation for the dop model. In Nada Lavrac, Dragan Gamberger, Ljupco Todorovski, and Hendrik Blockeel, editors, ECML, volume 2837 of Lecture Notes in Computer Science, pages 373–384. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Wainwright</author>
<author>Michael I Jordan</author>
</authors>
<title>Graphical Models, Exponential Families, and Variational Inference.</title>
<date>2008</date>
<publisher>Now Publishers Inc.,</publisher>
<location>Hanover, MA, USA.</location>
<contexts>
<context position="19893" citStr="Wainwright and Jordan, 2008" startWordPosition="3370" endWordPosition="3374"> We conjecture that the performance gap is due to the model using an overly simplistic treatment of unknown words, and also a further mixing problems with the sampler. For the full data set the counts are much larger in magnitude which leads to stronger modes. The sampler has difficulty escaping such modes and therefore is slower to mix. One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge (Jain and Neal, 2000). Another way is to use a variational approximation instead of MCMC sampling (Wainwright and Jordan, 2008). 5 Discussion We have demonstrated how our grammar transformation can implicitly represent an exponential space of tree fragments efficiently, allowing us to build a sampler with considerably better mixing properties than a local Gibbs sampler. The same technique was also shown to improve the parsing algorithm. These improvements are in no way limited to our particular choice of a TSG parsing model, many hierarchical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from</context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>Martin J Wainwright and Michael I Jordan. 2008. Graphical Models, Exponential Families, and Variational Inference. Now Publishers Inc., Hanover, MA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>