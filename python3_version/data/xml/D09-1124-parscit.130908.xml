<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000184">
<title confidence="0.922613">
Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge
</title>
<author confidence="0.986986">
Samer Hassan and Rada Mihalcea
</author>
<affiliation confidence="0.999409">
Department of Computer Science
University of North Texas
</affiliation>
<email confidence="0.994495">
samer@unt.edu, rada@cs.unt.edu
</email>
<sectionHeader confidence="0.99471" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840363636364">
In this paper, we address the task of cross-
lingual semantic relatedness. We intro-
duce a method that relies on the informa-
tion extracted from Wikipedia, by exploit-
ing the interlanguage links available be-
tween Wikipedia versions in multiple lan-
guages. Through experiments performed
on several language pairs, we show that
the method performs well, with a perfor-
mance comparable to monolingual mea-
sures of relatedness.
</bodyText>
<sectionHeader confidence="0.983594" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999943630434783">
Given the accelerated growth of the number of
multilingual documents on the Web and else-
where, the need for effective multilingual and
cross-lingual text processing techniques is becom-
ing increasingly important. In this paper, we
address the task of cross-lingual semantic relat-
edness, and introduce a method that relies on
Wikipedia in order to calculate the relatedness of
words across languages. For instance, given the
word factory in English and the word lavoratore
in Italian (En. worker), the method can measure
the relatedness of these two words despite the fact
that they belong to two different languages.
Measures of cross-language relatedness are use-
ful for a large number of applications, including
cross-language information retrieval (Nie et al.,
1999; Monz and Dorr, 2005), cross-language text
classification (Gliozzo and Strapparava, 2006),
lexical choice in machine translation (Och and
Ney, 2000; Bangalore et al., 2007), induction
of translation lexicons (Schafer and Yarowsky,
2002), cross-language annotation and resource
projections to a second language (Riloff et al.,
2002; Hwa et al., 2002; Mohammad et al., 2007).
The method we propose is based on a measure
of closeness between concept vectors automati-
cally built from Wikipedia, which are mapped via
the Wikipedia interlanguage links. Unlike previ-
ous methods for cross-language mapping, which
are typically limited by the availability of bilingual
dictionaries or parallel texts, the method proposed
in this paper can be used to measure the related-
ness of word pairs in any of the 250 languages for
which a Wikipedia version exists.
The paper is organized as follows. We first pro-
vide a brief overview of Wikipedia, followed by
a description of the method to build concept vec-
tors based on this encyclopedic resource. We then
show how these concept vectors can be mapped
across languages for a cross-lingual measure of
word relatedness. Through evaluations run on six
language pairs, connecting English, Spanish, Ara-
bic and Romanian, we show that the method is ef-
fective at capturing the cross-lingual relatedness of
words, with results comparable to the monolingual
measures of relatedness.
</bodyText>
<sectionHeader confidence="0.995887" genericHeader="introduction">
2 Wikipedia
</sectionHeader>
<bodyText confidence="0.9997885">
Wikipedia is a free online encyclopedia, represent-
ing the outcome of a continuous collaborative ef-
fort of a large number of volunteer contributors.
Virtually any Internet user can create or edit a
Wikipedia webpage, and this “freedom of contri-
bution” has a positive impact on both the quantity
(fast-growing number of articles) and the quality
(potential errors are quickly corrected within the
collaborative environment) of this online resource.
The basic entry in Wikipedia is an article (or
page), which defines and describes an entity or
an event, and consists of a hypertext document
with hyperlinks to other pages within or outside
Wikipedia. The role of the hyperlinks is to guide
the reader to pages that provide additional infor-
mation about the entities or events mentioned in
an article. Articles are organized into categories,
which in turn are organized into hierarchies. For
instance, the article automobile is included in the
category vehicle, which in turn has a parent cate-
</bodyText>
<page confidence="0.959292">
1192
</page>
<note confidence="0.996971692307692">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1192–1201,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
Language Articles Users
English 2,221,980 8,944,947
German 864,049 700,980
French 765,350 546,009
Polish 579,170 251,608
Japanese 562,295 284,031
Italian 540,725 354,347
Dutch 519,334 216,938
Portuguese 458,967 503,854
Spanish 444,696 966,134
Russian 359,677 226,602
</note>
<tableCaption confidence="0.999469">
Table 1: Top ten largest Wikipedias
</tableCaption>
<bodyText confidence="0.999790384615384">
gory named machine, and so forth.
Each article in Wikipedia is uniquely referenced
by an identifier, consisting of one or more words
separated by spaces or underscores and occasion-
ally a parenthetical explanation. For example, the
article for bar with the meaning of “counter for
drinks” has the unique identifier bar (counter).
Wikipedia editions are available for more than
250 languages, with a number of entries vary-
ing from a few pages to two millions articles or
more per language. Table 1 shows the ten largest
Wikipedias (as of December 2008), along with
the number of articles and approximate number of
contributors.1
Relevant for the work described in this paper are
the interlanguage links, which explicitly connect
articles in different languages. For instance, the
English article for bar (unit) is connected, among
others, to the Italian article bar (unit´a di misura)
and the Polish article bar (jednostka). On average,
about half of the articles in a Wikipedia version
include interlanguage links to articles in other lan-
guages. The number of interlanguage links per ar-
ticle varies from an average of five in the English
Wikipedia, to ten in the Spanish Wikipedia, and as
many as 23 in the Arabic Wikipedia.
</bodyText>
<sectionHeader confidence="0.941532" genericHeader="method">
3 Concept Vector Representations using
</sectionHeader>
<subsectionHeader confidence="0.678142">
Explicit Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999636857142857">
To calculate the cross-lingual relatedness of two
words, we measure the closeness of their con-
cept vector representations, which are built from
Wikipedia using explicit semantic analysis (ESA).
Encyclopedic knowledge is typically organized
into concepts (or topics), each concept being
further described using definitions, examples,
</bodyText>
<footnote confidence="0.8538145">
1http://meta.wikimedia.org/wiki/List of Wikipedias
#Grand Total
</footnote>
<bodyText confidence="0.999846538461539">
and possibly links to other concepts. ESA
(Gabrilovich and Markovitch, 2007) relies on the
distribution of words inside the encyclopedic de-
scriptions, and builds semantic representations for
a given word in the form of a vector of the encyclo-
pedic concepts in which the word appears. In this
vector representation, each encyclopedic concept
is assigned with a weight, calculated as the term
frequency of the given word inside the concept’s
article.
Formally, let C be the set of all the Wikipedia
concepts, and let a be any content word. We define
a� as the ESA concept vector of term a:
</bodyText>
<equation confidence="0.998935">
a� = {wc1, wc2...wc.} , (1)
</equation>
<bodyText confidence="0.999274777777778">
where wci is the weight of the concept ci with re-
spect to a. ESA assumes the weight wci to be the
term frequency tfi of the word a in the article cor-
responding to concept ci.
We use a revised version of the ESA algorithm.
The original ESA semantic relatedness between
the words in a given word pair a − b is defined as
the cosine similarity between their corresponding
vectors:
</bodyText>
<equation confidence="0.9962155">
a� · �b (2)
Nil 19.
</equation>
<bodyText confidence="0.999434571428571">
To illustrate, consider for example the construc-
tion of the ESA concept vector for the word bird.
The top ten concepts containing this word, along
with the associated weight (calculated using equa-
tion 7), are listed in table 2. Note that the the ESA
vector considers all the possible senses of bird, in-
cluding Bird as a surname as in e.g., “Larry Bird.”
</bodyText>
<table confidence="0.998376454545454">
Weight Wikipedia concept
51.4 Lists Of Birds By Region
44.8 Bird
40.3 British Birds Rarities Committee
32.8 Origin Of Birds
31.5 Ornithology
30.1 List Of Years In Birding And Ornithology
29.8 Bird Vocalization
27.4 Global Spread Of H5n1 In 2006
26.5 Larry Bird
22.3 Birdwatching
</table>
<tableCaption confidence="0.8091675">
Table 2: Top ten Wikipedia concepts for the word
“bird”
</tableCaption>
<bodyText confidence="0.969776">
In our ESA implementation, we make three
changes with respect to the original ESA algo-
rithm. First, we replace the cosine similarity with
</bodyText>
<equation confidence="0.92122">
Relatedness(a, b) =
</equation>
<page confidence="0.846217">
1193
</page>
<bodyText confidence="0.984379909090909">
a Lesk-like metric (Lesk, 1986), which places less
emphasis on the distributional differences between
the vector weights and more emphasis on the over-
lap (mutual coverage) between the vector features,
and thus it is likely to be more appropriate for the
sparse ESA vectors, and for the possible asymme-
try between languages. Let a and b be two terms
with the corresponding ESA concept vectors A
�
and B respectively. Let A and B represent the sets
of concepts with a non-zero weight encountered in
</bodyText>
<equation confidence="0.982234">
� �
B respectively. The coverage of A�by B�is
defined as:
G(�B |~A) = E wai (3)
i∈B
</equation>
<bodyText confidence="0.968093">
and similarly, the coverage of B~ by A~ is:
</bodyText>
<equation confidence="0.9905835">
G( ~A |~B) = E wbi (4)
i∈A
</equation>
<bodyText confidence="0.999985">
where wai and wbi represent the weight associ-
ated with concept ci in vectors A~ and B~ respec-
tively. By averaging these two asymmetric scores,
we redefine the relatedness as:
</bodyText>
<sectionHeader confidence="0.997153" genericHeader="method">
4 Cross-lingual Relatedness
</sectionHeader>
<bodyText confidence="0.99615705">
We measure the relatedness of concepts in differ-
ent languages by using their ESA concept vector
representations in their own languages, along with
the Wikipedia interlanguage links that connect ar-
ticles written in a given language to their corre-
sponding Wikipedia articles in other languages.
For example, the English Wikipedia article moon
contains interlanguage links to J i in the Ara-
bic Wikipedia, luna in the Spanish Wikipedia, and
lun˘a in the Romanian Wikipedia. The interlan-
guage links can map concepts across languages,
and correspondingly map concept vector represen-
tations in different languages.
Formally, let Cx and Cy be the sets of all
Wikipedia concepts in languages x and y, with
corresponding translations in the y and x lan-
guages, respectively. If trxy() is a translation
function that maps a concept ci E Cx into the con-
cept ci E Cy via the interlanguage links, we can
write:
</bodyText>
<equation confidence="0.998831857142857">
B |A) + G( A|B)
(5)
2
�
trxy(ci) = ci, (8)
G(
Relatedness(a, b) =
</equation>
<bodyText confidence="0.999980571428572">
Second, we refine the ESA weighting schema
to account for the length of the articles describing
the concept. Since some concepts have lengthy
descriptions, they may be favored due to their high
term frequencies when compared to more compact
descriptions. To eliminate this bias, we calculate
the weight associated with a concept ci as follows:
</bodyText>
<equation confidence="0.982543">
wci = tfi x log(M/ |ci|), (6)
</equation>
<bodyText confidence="0.999536857142857">
where tfi represents the term frequency of the
word a in concept ci, M is a constant representing
the maximum vocabulary size of Wikipedia con-
cepts, and |ci |is the size of the vocabulary used in
the description of concept ci.
Finally, we use the Wikipedia category graph
to promote category-type concepts in our feature
vectors. This is done by scaling the concept’s
weight by the inverse of the distance di to the
root category. The concepts that are not categories
are treated as leaves, and therefore their weight is
scaled down by the inverse of the maximum depth
in the category graph. The resulting weighting
scheme is:
</bodyText>
<equation confidence="0.956084">
wci = tfi x log(M/ |ci|)/di (7)
</equation>
<bodyText confidence="0.9809005">
The projection of the ESA vector t~ from lan-
guage x onto y can be written as:
</bodyText>
<equation confidence="0.996338">
{ I
trxy(�t) = wtrxy(c1)...wtrxy(cn) . (9)
</equation>
<bodyText confidence="0.9996925">
Using equations 5, 7, and 9, we can calculate the
cross-lingual semantic relatedness between any
two content terms ax and by in given languages
x and y as:
</bodyText>
<equation confidence="0.948551666666667">
.
2
(10)
</equation>
<bodyText confidence="0.999977">
Note that the weights assigned to Wikipedia
concepts inside the concept vectors are language
specific. That is, two Wikipedia concepts from
different languages, mapped via an interlanguage
link, can, and often do have different weights.
Intuitively, the relation described by the inter-
language links should be reflective and transi-
tive. However, due to Wikipedia’s editorial pol-
icy, which accredits users with the responsibility
</bodyText>
<equation confidence="0.959688">
G(tryx(
sim(ax, by) =
~B) |A) + G( A|tryx(
B))
</equation>
<page confidence="0.897408">
1194
</page>
<bodyText confidence="0.99955325">
of maintaining the articles, these properties are not
always met. Table 3 shows real cases where the
transitive and the reflective properties fail due to
missing interlanguage links.
</bodyText>
<equation confidence="0.544495">
Relation Exists
Reflectivity
Kafr-El-Dawwar Battle(en) F-4jIj.JIjir a�..a(ar) Yes
jij-01 Jir X5J. ar) F-4 Kafr-El-Dawwar Battle(en) No
Transitivity
</equation>
<tableCaption confidence="0.992662">
Table 3: Reflectivity and transitivity in Wikipedia
</tableCaption>
<bodyText confidence="0.999969916666667">
We solve this problem by iterating over the
translation tables and extracting all the missing
links by enforcing the reflectivity and the transi-
tivity properties. Table 4 shows the initial number
of interlanguage links and the discovered links for
the four languages used in our experiments. The
table also shows the coverage of the interlanguage
links, measured as the ratio between the total num-
ber of interlanguage links (initial plus discovered)
originating in the source language towards the tar-
get language, divided by the total number of arti-
cles in the source language.
</bodyText>
<table confidence="0.998976642857143">
Language pair Interlanguage links Cover.
Initial Discov.
English Spanish 293,957 12,659 0.14
English Romanian 86,719 4,641 0.04
English Arabic 56,233 3,916 0.03
Spanish English 294,266 7,328 0.58
Spanish Romanian 39,830 3,281 0.08
Spanish Arabic 33,889 3,319 0.07
Romanian English 75,685 6,783 0.46
Romanian Spanish 36,002 3,546 0.22
Romanian Arabic 15,777 1,698 0.10
Arabic English 46,072 3,170 0.33
Arabic Spanish 28,142 3,109 0.21
Arabic Romanian 15,965 1,970 0.12
</table>
<tableCaption confidence="0.846468">
Table 4: Interlanguage links (initial and discov-
ered) and their coverage in Wikipedia versions in
four languages.
</tableCaption>
<sectionHeader confidence="0.993492" genericHeader="method">
5 Experiments and Evaluations
</sectionHeader>
<bodyText confidence="0.9999436875">
We run our experiments on four languages: En-
glish, Spanish, Romanian and Arabic. For each
of these languages, we use a Wikipedia down-
load from October 2008. The articles were pre-
processed using Wikipedia Miner (Milne, 2007)
to extract structural information such as general-
ity, and interlanguage links. Furthermore, arti-
cles were also processed to remove numerical con-
tent, as well as any characters not included in the
given language’s alphabet. The content words are
stemmed, and words shorter than three characters
are removed (a heuristic which we use as an ap-
proximation for stopword removal). Table 5 shows
the number of articles in each Wikipedia version
and the size of their vocabularies, as obtained af-
ter the pre-processing step.
</bodyText>
<table confidence="0.8992926">
Articles Vocabulary
English 2,221,980 1,231,609
Spanish 520,154 406,134
Arabic 149,340 216,317
Romanian 179,440 623,358
</table>
<tableCaption confidence="0.923355">
Table 5: Number of articles and size of vocabulary
</tableCaption>
<bodyText confidence="0.96224975">
for the four Wikipedia versions
After pre-processing, the articles are indexed
to generate the ESA concept vectors. From each
Wikipedia version, we also extract other features
including article titles, interlanguage links, and
Wikipedia category graphs. The interlanguage
links are further processed to recover any missing
links, as described in the previous section.
</bodyText>
<subsectionHeader confidence="0.976602">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999968130434783">
For the evaluation, we build several cross-lingual
datasets based on the standard Miller-Charles
(Miller and Charles, 1998) and WordSimilarity-
353 (Finkelstein et al., 2001) English word relat-
edness datasets.
The Miller-Charles dataset (Miller and Charles,
1998) consists of 30-word pairs ranging from syn-
onymy pairs (e.g., car - automobile) to completely
unrelated terms (e.g., noon - string). The relat-
edness of each word pair was rated by 38 hu-
man subjects, using a scale from 0 (not-related)
to 4 (perfect synonymy). The dataset is avail-
able only in English and has been widely used
in previous semantic relatedness evaluations (e.g.,
(Resnik, 1995; Hughes and Ramage, 2007; Zesch
et al., 2008)).
The WordSimilarity-353 dataset (also known as
Finkelstein-353) (Finkelstein et al., 2001) consists
of 353 word pairs annotated by 13 human experts,
on a scale from 0 (unrelated) to 10 (very closely
related or identical). The Miller-Charles set is a
subset in the WordSimilarity-353 data set. Unlike
the Miller-Charles data set, which consists only of
</bodyText>
<equation confidence="0.894875">
Intifada(en) F-4 Intifada(es)
Intifada(es) F-4 �� ����������(ar)
�
Intifada(en) F-4 �� ����������(ar)
</equation>
<table confidence="0.9445982">
~
Yes
Yes
No
1195
Word pair
English coast - shore car - automobile brother - monk
Spanish costa - orilla coche - automovil hermano - monje
Arabic ���~�- ~~�~L: o~L�~ - oqj �!���� ~� - &amp;quot;� #��~
Romanian t¸˘arm - mal mas¸fin˘a - automobil frate - c˘alug˘ar
</table>
<tableCaption confidence="0.998958">
Table 6: Word pair translation examples
</tableCaption>
<bodyText confidence="0.999096676470588">
single words, the WordSimilarity-353 set also fea-
tures phrases (e.g., “Wednesday news”), therefore
posing an additional degree of difficulty for a re-
latedness metric applied on this data.
Native speakers of Spanish, Romanian and Ara-
bic, who were also highly proficient in English,
were asked to translate the words in the two data
sets. The annotators were provided one word pair
at a time, and asked to provide the appropriate
translation for each word while taking into account
their relatedness within the word pair. The relat-
edness was meant as a hint to disambiguate the
words, when multiple translations were possible.
The annotators were also instructed not to use
multi-word expressions in their translations. They
were also allowed to use replacement words to
overcome slang or culturally-biased terms. For ex-
ample, in the case of the word pair dollar-buck,
annotators were allowed to use ~Cw$2 as a transla-
tion for buck.
To test the ability of the bilingual judges to pro-
vide correct translations by using this annotation
setting, we carried out the following experiment.
We collected Spanish translations from five differ-
ent human judges, which were then merged into
a single selection based on the annotators’ trans-
lation agreement; the merge was done by a sixth
human judge, who also played the role of adjudi-
cator when no agreement was reached between the
initial annotators.
Subsequently, five additional human experts re-
scored the word-pair Spanish translations by using
the same scale that was used in the construction of
the English data set. The correlation between the
</bodyText>
<footnote confidence="0.541368">
2Arabic for dinars – the commonly used currency in the
Middle East.
</footnote>
<bodyText confidence="0.999853205882353">
relatedness scores assigned during this experiment
and the scores assigned in the original English ex-
periment was 0.86, indicating that the translations
provided by the bilingual judges were correct and
preserved the word relatedness.
For the translations provided by the five human
judges, in more than 74% of the cases at least three
human judges agreed on the same translation for a
word pair. When the judges did not provide iden-
tical translations, they typically used a close syn-
onym. The high agreement between their trans-
lations indicates that the annotation setting was
effective in pinpointing the correct translation for
each word, even in the case of ambiguous words.
Motivated by the validation of the annotation
setting obtained for Spanish, we used only one hu-
man annotator to collect the translations for Arabic
and Romanian. Table 6 shows examples of trans-
lations in the three languages for three word pairs
from our data sets.
Using these translations, we create six cross-
lingual data sets, one for each possible language
pair (English-Spanish, English-Arabic, English-
Romanian, Spanish-Arabic, Spanish-Romanian,
Arabic-Romanian). Given a source-target lan-
guage pair, a data set is created by first using the
source language for the first word and the target
language for the second word, and then reversing
the order, i.e., using the source language for the
second word and the target language for the first
word. The size of the data sets is thus doubled
in this way (e.g., the 30 word pairs in the English
Miller-Charles set are transformed into 60 word
pairs in the English-Spanish Miller-Charles set).
</bodyText>
<subsectionHeader confidence="0.593168">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9993905">
We evaluate the cross-lingual measure of related-
ness on each of the six language pairs. For com-
parison purposes, we also evaluate the monolin-
gual relatedness on the four languages.
For the evaluation, we use the Pearson (r)
and Spearman (ρ) correlation coefficients, which
are the standard metrics used in the past for the
evaluation of semantic relatedness (Finkelstein et
</bodyText>
<page confidence="0.969249">
1196
</page>
<bodyText confidence="0.9722496">
al., 2001; Zesch et al., 2008; Gabrilovich and
Markovitch, 2007). While the Pearson correla-
tion is highly dependent on the linear relationship
between the distributions in question, Spearman
mainly emphasizes the ability of the distributions
to maintain their relative ranking.
Tables 7 and 8 show the results of the evalua-
tions of the cross-lingual relatedness, when using
an ESA concept vector with a size of maximum
10,000 concepts.3
</bodyText>
<table confidence="0.999831363636364">
English Spanish Arabic Romanian
Miller-Charles
English 0.58 0.43 0.32 0.50
Spanish 0.44 0.20 0.38
Arabic 0.36 0.32
Romanian 0.58
WordSimilarity-353
English 0.55 0.32 0.31 0.29
Spanish 0.45 0.32 0.28
Arabic 0.28 0.25
Romanian 0.30
</table>
<tableCaption confidence="0.855556666666667">
Table 7: Pearson correlation for cross-
lingual relatedness on the Miller-Charles and
WordSimilarity-353 data sets
</tableCaption>
<table confidence="0.999926727272728">
English Spanish Arabic Romanian
Miller-Charles
English 0.75 0.56 0.27 0.55
Spanish 0.64 0.17 0.32
Arabic 0.33 0.21
Romanian 0.61
WordSimilarity-353
English 0.71 0.55 0.35 0.38
Spanish 0.50 0.29 0.30
Arabic 0.26 0.20
Romanian 0.28
</table>
<tableCaption confidence="0.932278666666667">
Table 8: Spearman correlation for cross-
lingual relatedness on the Miller-Charles and
WordSimilarity-353 data sets
</tableCaption>
<bodyText confidence="0.999887">
As a validation of our ESA implementation, we
compared the results obtained for the monolingual
English relatedness with other results reported in
the past for the same data sets. Gabrilovich and
Markovitch (2007) reported a Spearman correla-
tion of 0.72 for the Miller-Charles data set and
0.75 for the WordSimilarity-353 data set, respec-
</bodyText>
<footnote confidence="0.973086">
3The concepts are selected in reversed order of their
weight inside the vector in the respective language. Note that
the cross-lingual mapping between the concepts in the ESA
vectors is done after the selection of the top 10,000 concepts
in each language.
</footnote>
<bodyText confidence="0.995475285714286">
tively. Zesch et al. (2008) reported a Spear-
man correlation of 0.67 for the Miller-Charles set.
These values are comparable to the Spearman cor-
relation scores obtained in our experiments for the
English data sets (see Table 8), with a fairly large
improvement obtained on the Miller-Charles data
set when using our implementation.
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99997985">
Overall, our method succeeds in capturing the
cross-lingual semantic relatedness between words.
As a point of comparison, one can use the mono-
lingual measures of relatedness as reflected by the
diagonals in Tables 7 and 8.
Looking at the monolingual evaluations, the re-
sults seem to be correlated with the Wikipedia size
for the corresponding language, with the English
measure scoring the highest. These results are not
surprising, given the direct relation between the
Wikipedia size and the sparseness of the ESA con-
cept vectors. A similar trend is observed for the
cross-lingual relatedness, with higher results ob-
tained for the languages with large Wikipedia ver-
sions (e.g., English-Spanish), and lower results for
the languages with a smaller size Wikipedia (e.g.,
Arabic-Spanish).
For comparison, we ran two additional experi-
ments. In the first experiment, we compared the
coverage of our cross-lingual relatedness method
to a direct use of the translation links available in
Wikipedia. The cross-lingual relatedness is turned
into a monolingual relatedness by using the in-
terlanguage Wikipedia links to translate the first
of the two words in a cross-lingual pair into the
language of the second word in the pair.4 From
the total of 433 word pairs available in the two
data sets, this method can produce translations
for an average of 103 word pairs per language
pair. This means that the direct Wikipedia inter-
language links allow the cross-lingual relatedness
measure to be transformed into a monolingual re-
latedness in about 24% of the cases, which is a
low coverage compared to the full coverage that
can be obtained with our cross-lingual method of
relatedness.
In an attempt to raise the coverage of the trans-
lation, we ran a second experiment where we used
a state-of-the-art translation engine to translate the
first word in a pair into the language of the sec-
</bodyText>
<footnote confidence="0.9899245">
4We use all the interlanguage links obtained by combining
the initial and the discovered links, as described in Section 4.
</footnote>
<page confidence="0.996865">
1197
</page>
<bodyText confidence="0.999971020408163">
ond word in the pair. We use Google Translate,
which is a statistical machine translation engine
that relies on large parallel corpora, to find the
most likely translation for a given word. Unlike
the previous experiment, this time we can achieve
full translation coverage, and thus we are able to
produce data sets of equal size that can be used
for a comparison between relatedness measures.
Specifically, using the translation produced by the
machine translation engine for the first word in a
pair, we calculate the relatedness within the space
of the language of the second word using a mono-
lingual ESA also based on Wikipedia. The results
obtained with this method are compared against
the results obtained with our cross-lingual ESA re-
latedness.
Using a Pearson correlation, our cross-lingual
relatedness method achieves an average score
across all six language pairs of 0.36 for the Miller-
Charles data set and 0.30 for the WordSimilarity-
353 data set,5 which is higher than the 0.33 and
0.28 scores achieved for the same data sets when
using a translation obtained with Google Trans-
late followed by a monolingual measure of re-
latedness. These results are encouraging, also
given that the translation-based method is limited
to those language pairs for which a translation en-
gine exists (e.g., Google Translate covers 40 lan-
guages), whereas our method can be applied to any
language pair from the set of 250 languages for
which a Wikipedia version exists.
To gain further insights, we also determined the
impact of the vector length in the ESA concept
vector representation, by calculating the Pearson
correlation for vectors of different lengths. Fig-
ures 1 and 2 show the Pearson score as a func-
tion of the vector length for the Miller-Charles
and WordSimilarity-353 data sets. The plots show
that the cross-lingual measure of relatedness is not
significantly affected by the reduction or increase
of the vector length. Thus, the use of vectors of
length 10,000 (as used in most of our experiments)
appears as a reasonable tradeoff between accuracy
and performance.
Furthermore, by comparing the performance of
the proposed Lesk-like model to the traditional
cosine-similarity (Figures 3 and 4), we note that
the Lesk-like model outperforms the cosine model
on most language pairs. We believe that this is
</bodyText>
<footnote confidence="0.972984666666667">
5This average considers all the cross-lingual relatedness
scores listed in Table 7; it does not include the monolingual
scores listed on the table diagonal.
</footnote>
<figure confidence="0.7343545">
5000 10000 15000 20000 25000 30000
Vector length
</figure>
<figureCaption confidence="0.9876255">
Figure 1: Pearson correlation vs. ESA vector
length on the Miller-Charles data set
</figureCaption>
<figure confidence="0.9862965">
5000 10000 15000 20000 25000 30000
Vector Size
</figure>
<figureCaption confidence="0.918678">
Figure 2: Pearson correlation vs. ESA vector
length on the WordSimilarity-353 data set
</figureCaption>
<bodyText confidence="0.999946444444444">
due to the stricter correlation conditions imposed
by the cosine-metric in such sparse vector-based
representations, as compared to the more relaxed
hypothesis used by the Lesk model.
Finally, we also looked at the relation between
the number of interlanguage links found for the
concepts in a vector and the length of the vector.
Figures 5 and 6 display the average number of in-
terlanguage links as a function of the concept vec-
tor length.
By analyzing the effect of the average number
of interlanguage links found per word in the given
datasets (Figures 5 and 6), we notice that these
links increase proportionally with the vector size,
as expected. However, this increase does not lead
to any significant improvements in accuracy (Fig-
ures 1 and 2). This implies that while the presence
of interlanguage links is a prerequisite for the mea-
</bodyText>
<figure confidence="0.987358486486486">
ar↔ar
ar↔en
ar↔es
ar↔ro
en↔en
en↔es
es↔es
es↔ro
en↔ro
ro↔ro
Average Pearson
0.6
0.5
0.4
0.3
0.2
0.1
ar↔ar
ar↔en
ar↔es
ar↔ro
en↔en
en↔es
es↔es
es↔ro
en↔ro
ro↔ro
Pearson correlation 0.6
0.5
0.4
0.3
0.2
0.1
1198
Pearson
5000 10000 15000 20000 25000 30000
Vector Size
</figure>
<figureCaption confidence="0.945045">
Figure 3: Lesk vs. cosine similarity for the Miller-
Charles data set
</figureCaption>
<figure confidence="0.9775385">
5000 10000 15000 20000 25000 30000
Vector length
</figure>
<figureCaption confidence="0.9932255">
Figure 5: Number of interlanguage links vs. vec-
tor length for the Miller-Charles data set
</figureCaption>
<figure confidence="0.999914278688525">
0.7
0.6
0.5
0.4
0.3
0.2
0.1
lsk(ar↔ar
lsk(en↔en
lsk(es↔es
lsk(ro↔ro
cos(ar↔ar
cos(en↔en
cos(es↔es
cos(ro↔ro
ar↔en
ar↔es
ar↔ro
en↔es
en↔ro
es↔ro
2000
Number of interlanguage links
1500
1000
500
0
5000 10000 15000 20000 25000 30000
Vector Size
ar↔en
ar↔es
ar↔ro
en↔es
en↔ro
es↔ro
5000 10000 15000 20000 25000 30000
Vector length
lsk(ar↔ar
lsk(en↔en
lsk(es↔es
lsk(ro↔ro
cos(ar↔ar
cos(en↔en
cos(es↔es
cos(ro↔ro
Pearson 0.7
0.6
0.5
0.4
0.3
0.2
0.1
Number of interlanguage links 4000
3500
3000
2500
2000
1500
1000
500
0
</figure>
<figureCaption confidence="0.985279">
Figure 4: Lesk vs. cosine similarity for the
WordSimilarity-353 data set
</figureCaption>
<bodyText confidence="0.9999418">
sure of relatedness,6 their effect is only significant
for the top ranked concepts in a vector. Therefore,
increasing the vectors size to maximize the match-
ing of the projected dimensions does not necessar-
ily lead to accuracy improvements.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.9999002">
Measures of word relatedness were found useful in
a large number of natural language processing ap-
plications, including word sense disambiguation
(Patwardhan et al., 2003), synonym identification
(Turney, 2001), automated essay scoring (Foltz et
al., 1999), malapropism detection (Budanitsky and
Hirst, 2001), coreference resolution (Strube and
Ponzetto, 2006), and others. Most of the work to
date has focused on measures of word relatedness
for English, by using methods applied on knowl-
</bodyText>
<footnote confidence="0.99495625">
6Two languages with no interlanguage links between
them will lead to a relatedness score of zero for any word
pair across these languages, no matter how strongly related
the words are.
</footnote>
<figureCaption confidence="0.9097535">
Figure 6: Number of interlanguage links vs. vec-
tor length for the WordSimilarity-353 data set
</figureCaption>
<bodyText confidence="0.99547319047619">
edge bases (Lesk, 1986; Wu and Palmer, 1994;
Resnik, 1995; Jiang and Conrath, 1997; Hughes
and Ramage, 2007) or on large corpora (Salton
et al., 1997; Landauer et al., 1998; Turney, 2001;
Gabrilovich and Markovitch, 2007).
Although to a lesser extent, measures of word
relatedness have also been applied on other lan-
guages, including German (Zesch et al., 2007;
Zesch et al., 2008; Mohammad et al., 2007), Chi-
nese (Wang et al., 2008), Dutch (Heylen et al.,
2008) and others. Moreover, assuming resources
similar to those available for English, e.g., Word-
Net structures or large corpora, the measures of
relatedness developed for English can be in prin-
ciple applied to other languages as well.
All these methods proposed in the past have
been concerned with monolingual word related-
ness calculated within the boundaries of one lan-
guage, as opposed to cross-lingual relatedness,
which is the focus of our work.
The research area closest to the task of cross-
</bodyText>
<page confidence="0.993733">
1199
</page>
<bodyText confidence="0.999976461538462">
lingual relatedness is perhaps cross-language in-
formation retrieval, which is concerned with
matching queries posed in one language to docu-
ment collections in a second language. Note how-
ever that most of the approaches to date for cross-
language information retrieval have been based on
direct translations obtained for words in the query
or in the documents, by using bilingual dictionar-
ies (Monz and Dorr, 2005) or parallel corpora (Nie
et al., 1999). Such explicit translations can iden-
tify a direct correspondence between words in two
languages (e.g., they will find that fabbrica (It.)
and factory (En.) are translations of each other),
but will not capture similarities of a different de-
gree (e.g., they will not find that lavoratore (It.;
worker in En.) is similar to factory (En.).
Also related are the areas of word alignment
for machine translation (Och and Ney, 2000),
induction of translation lexicons (Schafer and
Yarowsky, 2002), and cross-language annotation
projections to a second language (Riloff et al.,
2002; Hwa et al., 2002; Mohammad et al.,
2007). As with cross-language information re-
trieval, these areas have primarily considered di-
rect translations between words, rather than an en-
tire spectrum of relatedness, as we do in our work.
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.9999602">
In this paper, we addressed the problem of
cross-lingual semantic relatedness, which is a
core task for a number of applications, includ-
ing cross-language information retrieval, cross-
language text classification, lexical choice for ma-
chine translation, cross-language projections of re-
sources and annotations, and others.
We introduced a method based on concept vec-
tors built from Wikipedia, which are mapped
across the interlanguage links available between
Wikipedia versions in multiple languages. Ex-
periments performed on six language pairs, con-
necting English, Spanish, Arabic and Romanian,
showed that the method is effective at captur-
ing the cross-lingual relatedness of words. The
method was shown to be competitive when com-
pared to methods based on a translation using the
direct Wikipedia links or using a statistical trans-
lation engine. Moreover, our method has wide ap-
plicability across languages, as it can be used for
any language pair from the set of 250 languages
for which a Wikipedia version exists.
The cross-lingual data sets introduced
in this paper can be downloaded from
http://lit.csci.unt.edu/index.php/Downloads.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999948777777778">
The authors are grateful to Carmen Banea for her
help with the construction of the data sets. This
material is based in part upon work supported by
the National Science Foundation CAREER award
#0747340. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion.
</bodyText>
<sectionHeader confidence="0.99836" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999487720930233">
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statis-
tical machine translation through global lexical se-
lection and sentence reconstruction. In Proceedings
of the Annual Meeting of the Association of Compu-
tational Linguistics, Prague, Czech Republic.
A. Budanitsky and G. Hirst. 2001. Semantic distance
in WordNet: An experimental, application-oriented
evaluation of five measures. In Proceedings of the
NAACL Workshop on WordNet and Other Lexical
Resources, Pittsburgh.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Plac-
ing search in context: the concept revisited. In
WWW, pages 406–414.
P. Foltz, D. Laham, and T. Landauer. 1999. Automated
essay scoring: Applications to educational technol-
ogy. In Proceedings of World Conference on Edu-
cational Multimedia, Hypermedia and Telecommu-
nications, Chesapeake, Virginia.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence,
pages 1606–1611.
A. Gliozzo and C. Strapparava. 2006. Exploiting com-
parable corpora and bilingual dictionaries for cross-
language text categorization. In Proceedings of the
Conference of the Association for Computational
Linguistics, Sydney, Australia.
K. Heylen, Y. Peirsman, D. Geeraerts, and D. Speel-
man. 2008. Modelling word similarity: an evalu-
ation of automatic synonymy extraction algorithms.
In Proceedings of the Sixth International Language
Resources and Evaluation, Marrakech, Morocco.
T. Hughes and D. Ramage. 2007. Lexical semantic
knowledge with random graph walks. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, Prague, Czech Republic.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2002), Philadelphia, July.
</reference>
<page confidence="0.6781">
1200
</page>
<reference confidence="0.999829684210526">
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Intro-
duction to latent semantic analysis. Discourse Pro-
cesses, 25.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
G. Miller and W. Charles. 1998. Contextual corre-
lates of semantic similarity. Language and Cogni-
tive Processes, 6(1).
D. Milne. 2007. Computing semantic relatedness us-
ing wikipedia link structure. In European Language
Resources Association (ELRA), editor, In Proceed-
ings of the New Zealand Computer Science Re-
search Student Conference (NZCSRSC 2007), New
Zealand.
S. Mohammad, I. Gurevych, G. Hirst, and T. Zesch.
2007. Cross-lingual distributional profiles of
concepts for measuring semantic distance. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP/CoNLL-2007), Prague, Czech Republic.
C. Monz and B.J. Dorr. 2005. Iterative translation
disambiguation for cross-language information re-
trieval. In Proceedings of the 28th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, Salvador,
Brazil.
J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. 1999.
Cross-language information retrieval based on paral-
lel texts and automatic mining of parallel texts from
the Web. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval.
F. Och and H. Ney. 2000. A comparison of align-
ment models for statistical machine translation. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING 2000), Saar-
brucken, Germany, August.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Mexico
City, February.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th
International Joint Conference on Artificial Intelli-
gence, Montreal, Canada.
E. Riloff, C. Schafer, and D. Yarowsky. 2002. In-
ducing information extraction systems for new lan-
guages via cross-language projection. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, Taipei, Taiwan, August.
G. Salton, A. Wong, and C.S. Yang. 1997. A vec-
tor space model for automatic indexing. In Read-
ings in Information Retrieval, pages 273–280. Mor-
gan Kaufmann Publishers, San Francisco, CA.
C. Schafer and D. Yarowsky. 2002. Inducing trans-
lation lexicons via diverse similarity measures and
bridge languages. In Proceedings of the 6th Confer-
ence on Natural Language Learning (CoNLL 2003),
Taipei, Taiwan, August.
M. Strube and S. P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedeness using Wikipedia. In
Proceedings of the American Association for Artifi-
cial Intelligence, Boston, MA.
P. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001), Freiburg, Germany.
X. Wang, S. Ju, and S. Wu. 2008. A survey of chi-
nese text similarity computation. In Proceedings of
the Asia Information Retrieval Symposium, Harbin,
China.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, Las Cruces, New Mexico.
T. Zesch, I. Gurevych, and M. M¨uhlh¨auser. 2007.
Comparing Wikipedia and German Wordnet by
Evaluating Semantic Relatedness on Multiple
Datasets. In Proceedings ofHuman Language Tech-
nologies: The Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
T. Zesch, C. M¨uller, and I. Gurevych. 2008. Using
Wiktionary for Computing Semantic Relatedness.
In Proceedings of the American Association for Ar-
tificial Intelligence, Chicago.
</reference>
<page confidence="0.990066">
1201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998311">Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge</title>
<author confidence="0.985253">Hassan</author>
<affiliation confidence="0.99979">Department of Computer University of North</affiliation>
<email confidence="0.999821">samer@unt.edu,rada@cs.unt.edu</email>
<abstract confidence="0.992179542168675">In this paper, we address the task of crosslingual semantic relatedness. We introduce a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages. Through experiments performed on several language pairs, we show that the method performs well, with a performance comparable to monolingual measures of relatedness. 1 Motivation Given the accelerated growth of the number of multilingual documents on the Web and elsewhere, the need for effective multilingual and cross-lingual text processing techniques is becoming increasingly important. In this paper, we the task of semantic relatand introduce a method that relies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the English and the word Italian (En. the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version exists. The paper is organized as follows. We first provide a brief overview of Wikipedia, followed by a description of the method to build concept vectors based on this encyclopedic resource. We then show how these concept vectors can be mapped across languages for a cross-lingual measure of word relatedness. Through evaluations run on six language pairs, connecting English, Spanish, Arabic and Romanian, we show that the method is effective at capturing the cross-lingual relatedness of words, with results comparable to the monolingual measures of relatedness. 2 Wikipedia Wikipedia is a free online encyclopedia, representing the outcome of a continuous collaborative effort of a large number of volunteer contributors. Virtually any Internet user can create or edit a Wikipedia webpage, and this “freedom of contribution” has a positive impact on both the quantity (fast-growing number of articles) and the quality (potential errors are quickly corrected within the collaborative environment) of this online resource. basic entry in Wikipedia is an which defines and describes an entity or an event, and consists of a hypertext document with hyperlinks to other pages within or outside Wikipedia. The role of the hyperlinks is to guide the reader to pages that provide additional information about the entities or events mentioned in article. Articles are organized into which in turn are organized into hierarchies. For the article included in the which in turn has a parent cate- 1192 of the 2009 Conference on Empirical Methods in Natural Language pages 6-7 August 2009. ACL and AFNLP</abstract>
<note confidence="0.692061583333333">Language Articles Users English 2,221,980 8,944,947 German 864,049 700,980 French 765,350 546,009 Polish 579,170 251,608 Japanese 562,295 284,031 Italian 540,725 354,347 Dutch 519,334 216,938 Portuguese 458,967 503,854 Spanish 444,696 966,134 Russian 359,677 226,602 Table 1: Top ten largest Wikipedias</note>
<abstract confidence="0.988954230769231">named and so forth. Each article in Wikipedia is uniquely referenced by an identifier, consisting of one or more words separated by spaces or underscores and occasionally a parenthetical explanation. For example, the for the meaning of for the unique identifier Wikipedia editions are available for more than 250 languages, with a number of entries varying from a few pages to two millions articles or more per language. Table 1 shows the ten largest Wikipedias (as of December 2008), along with the number of articles and approximate number of Relevant for the work described in this paper are which explicitly connect articles in different languages. For instance, the article for (unit) connected, among to the Italian article (unit´a di misura) the Polish article On average, about half of the articles in a Wikipedia version include interlanguage links to articles in other languages. The number of interlanguage links per article varies from an average of five in the English Wikipedia, to ten in the Spanish Wikipedia, and as many as 23 in the Arabic Wikipedia. 3 Concept Vector Representations using Explicit Semantic Analysis To calculate the cross-lingual relatedness of two words, we measure the closeness of their concept vector representations, which are built from Wikipedia using explicit semantic analysis (ESA). Encyclopedic knowledge is typically organized into concepts (or topics), each concept being further described using definitions, examples, of Wikipedias and possibly links to other concepts. ESA (Gabrilovich and Markovitch, 2007) relies on the distribution of words inside the encyclopedic descriptions, and builds semantic representations for a given word in the form of a vector of the encyclopedic concepts in which the word appears. In this vector representation, each encyclopedic concept is assigned with a weight, calculated as the term frequency of the given word inside the concept’s article. let the set of all the Wikipedia and let any content word. We define the ESA concept vector of term the weight of the concept reto ESA assumes the weight be the frequency the word the article corto concept We use a revised version of the ESA algorithm. The original ESA semantic relatedness between words in a given word pair defined as the cosine similarity between their corresponding vectors: To illustrate, consider for example the construcof the ESA concept vector for the word The top ten concepts containing this word, along with the associated weight (calculated using equation 7), are listed in table 2. Note that the the ESA considers all the possible senses of ina surname as in e.g., Bird.” Weight Wikipedia concept</abstract>
<address confidence="0.896344111111111">51.4 Lists Of Birds By Region 44.8 Bird 40.3 British Birds Rarities Committee 32.8 Origin Of Birds 31.5 Ornithology 30.1 List Of Years In Birding And Ornithology 29.8 Bird Vocalization 27.4 Global Spread Of H5n1 In 2006 26.5 Larry Bird</address>
<abstract confidence="0.994287872549019">22.3 Birdwatching Table 2: Top ten Wikipedia concepts for the word “bird” In our ESA implementation, we make three changes with respect to the original ESA algorithm. First, we replace the cosine similarity with = 1193 a Lesk-like metric (Lesk, 1986), which places less emphasis on the distributional differences between the vector weights and more emphasis on the overlap (mutual coverage) between the vector features, and thus it is likely to be more appropriate for the sparse ESA vectors, and for the possible asymmebetween languages. Let two terms the corresponding ESA concept vectors � Let A and B represent the sets of concepts with a non-zero weight encountered in � � The coverage of defined as: = similarly, the coverage of = the weight associwith concept vectors respectively. By averaging these two asymmetric scores, we redefine the relatedness as: 4 Cross-lingual Relatedness We measure the relatedness of concepts in different languages by using their ESA concept vector representations in their own languages, along with the Wikipedia interlanguage links that connect articles written in a given language to their corresponding Wikipedia articles in other languages. example, the English Wikipedia article interlanguage links to the Ara- Wikipedia, the Spanish Wikipedia, and the Romanian Wikipedia. The interlanguage links can map concepts across languages, and correspondingly map concept vector representations in different languages. let the sets of all concepts in languages with translations in the lanrespectively. If a translation that maps a concept the conthe interlanguage links, we can write: + (5) 2 � = = Second, we refine the ESA weighting schema to account for the length of the articles describing the concept. Since some concepts have lengthy descriptions, they may be favored due to their high term frequencies when compared to more compact descriptions. To eliminate this bias, we calculate weight associated with a concept follows: the term frequency of the concept a constant representing the maximum vocabulary size of Wikipedia conand the size of the vocabulary used in description of concept Finally, we use the Wikipedia category graph to promote category-type concepts in our feature vectors. This is done by scaling the concept’s by the inverse of the distance the root category. The concepts that are not categories are treated as leaves, and therefore their weight is scaled down by the inverse of the maximum depth in the category graph. The resulting weighting scheme is: projection of the ESA vector lanbe written as: = Using equations 5, 7, and 9, we can calculate the cross-lingual semantic relatedness between any content terms given languages . 2 (10) Note that the weights assigned to Wikipedia concepts inside the concept vectors are language specific. That is, two Wikipedia concepts from different languages, mapped via an interlanguage link, can, and often do have different weights. Intuitively, the relation described by the interlanguage links should be reflective and transitive. However, due to Wikipedia’s editorial policy, which accredits users with the responsibility = + 1194 of maintaining the articles, these properties are not always met. Table 3 shows real cases where the transitive and the reflective properties fail due to missing interlanguage links.</abstract>
<title confidence="0.964659666666667">Relation Exists Reflectivity Battle(en) Yes</title>
<author confidence="0.796901">Battle No</author>
<abstract confidence="0.959577785714286">Transitivity Table 3: Reflectivity and transitivity in Wikipedia We solve this problem by iterating over the translation tables and extracting all the missing links by enforcing the reflectivity and the transitivity properties. Table 4 shows the initial number of interlanguage links and the discovered links for the four languages used in our experiments. The table also shows the coverage of the interlanguage links, measured as the ratio between the total number of interlanguage links (initial plus discovered) originating in the source language towards the target language, divided by the total number of articles in the source language.</abstract>
<note confidence="0.905824928571429">Language pair Interlanguage links Cover. Initial Discov. English Spanish 293,957 12,659 0.14 English Romanian 86,719 4,641 0.04 English Arabic 56,233 3,916 0.03 Spanish English 294,266 7,328 0.58 Spanish Romanian 39,830 3,281 0.08 Spanish Arabic 33,889 3,319 0.07 Romanian English 75,685 6,783 0.46 Romanian Spanish 36,002 3,546 0.22 Romanian Arabic 15,777 1,698 0.10 Arabic English 46,072 3,170 0.33 Arabic Spanish 28,142 3,109 0.21 Arabic Romanian 15,965 1,970 0.12</note>
<abstract confidence="0.974396088050314">Table 4: Interlanguage links (initial and discovered) and their coverage in Wikipedia versions in four languages. 5 Experiments and Evaluations We run our experiments on four languages: English, Spanish, Romanian and Arabic. For each of these languages, we use a Wikipedia download from October 2008. The articles were preprocessed using Wikipedia Miner (Milne, 2007) to extract structural information such as generality, and interlanguage links. Furthermore, articles were also processed to remove numerical content, as well as any characters not included in the given language’s alphabet. The content words are stemmed, and words shorter than three characters are removed (a heuristic which we use as an approximation for stopword removal). Table 5 shows the number of articles in each Wikipedia version and the size of their vocabularies, as obtained after the pre-processing step. Articles Vocabulary English 2,221,980 1,231,609 Spanish 520,154 406,134 Arabic 149,340 216,317 Romanian 179,440 623,358 Table 5: Number of articles and size of vocabulary for the four Wikipedia versions After pre-processing, the articles are indexed to generate the ESA concept vectors. From each Wikipedia version, we also extract other features including article titles, interlanguage links, and Wikipedia category graphs. The interlanguage links are further processed to recover any missing links, as described in the previous section. 5.1 Data For the evaluation, we build several cross-lingual datasets based on the standard Miller-Charles (Miller and Charles, 1998) and WordSimilarity- 353 (Finkelstein et al., 2001) English word relatedness datasets. The Miller-Charles dataset (Miller and Charles, 1998) consists of 30-word pairs ranging from synpairs (e.g., to completely terms (e.g., The relatedness of each word pair was rated by 38 human subjects, using a scale from 0 (not-related) to 4 (perfect synonymy). The dataset is available only in English and has been widely used in previous semantic relatedness evaluations (e.g., (Resnik, 1995; Hughes and Ramage, 2007; Zesch et al., 2008)). The WordSimilarity-353 dataset (also known as Finkelstein-353) (Finkelstein et al., 2001) consists of 353 word pairs annotated by 13 human experts, on a scale from 0 (unrelated) to 10 (very closely related or identical). The Miller-Charles set is a subset in the WordSimilarity-353 data set. Unlike the Miller-Charles data set, which consists only of � ~ Yes Yes No 1195 Word pair English coast shore car automobile brother monk Spanish costa orilla coche automovil hermano monje Arabic ~� #��~ Romanian mal automobil frate c˘alug˘ar Table 6: Word pair translation examples single words, the WordSimilarity-353 set also feaphrases (e.g., therefore posing an additional degree of difficulty for a relatedness metric applied on this data. Native speakers of Spanish, Romanian and Arabic, who were also highly proficient in English, were asked to translate the words in the two data sets. The annotators were provided one word pair at a time, and asked to provide the appropriate translation for each word while taking into account their relatedness within the word pair. The relatedness was meant as a hint to disambiguate the words, when multiple translations were possible. The annotators were also instructed not to use multi-word expressions in their translations. They were also allowed to use replacement words to overcome slang or culturally-biased terms. For exin the case of the word pair were allowed to use as a translafor To test the ability of the bilingual judges to provide correct translations by using this annotation setting, we carried out the following experiment. We collected Spanish translations from five different human judges, which were then merged into a single selection based on the annotators’ translation agreement; the merge was done by a sixth human judge, who also played the role of adjudicator when no agreement was reached between the initial annotators. Subsequently, five additional human experts rescored the word-pair Spanish translations by using the same scale that was used in the construction of the English data set. The correlation between the for dinars – the commonly used currency in the Middle East. relatedness scores assigned during this experiment and the scores assigned in the original English experiment was 0.86, indicating that the translations provided by the bilingual judges were correct and preserved the word relatedness. For the translations provided by the five human judges, in more than 74% of the cases at least three human judges agreed on the same translation for a word pair. When the judges did not provide identical translations, they typically used a close synonym. The high agreement between their translations indicates that the annotation setting was effective in pinpointing the correct translation for each word, even in the case of ambiguous words. Motivated by the validation of the annotation setting obtained for Spanish, we used only one human annotator to collect the translations for Arabic and Romanian. Table 6 shows examples of translations in the three languages for three word pairs from our data sets. Using these translations, we create six crosslingual data sets, one for each possible language pair (English-Spanish, English-Arabic, English- Romanian, Spanish-Arabic, Spanish-Romanian, Arabic-Romanian). Given a source-target language pair, a data set is created by first using the source language for the first word and the target language for the second word, and then reversing the order, i.e., using the source language for the second word and the target language for the first word. The size of the data sets is thus doubled in this way (e.g., the 30 word pairs in the English Miller-Charles set are transformed into 60 word pairs in the English-Spanish Miller-Charles set). 5.2 Results We evaluate the cross-lingual measure of relatedness on each of the six language pairs. For comparison purposes, we also evaluate the monolingual relatedness on the four languages. the evaluation, we use the Pearson Spearman correlation coefficients, which are the standard metrics used in the past for the evaluation of semantic relatedness (Finkelstein et 1196 al., 2001; Zesch et al., 2008; Gabrilovich and Markovitch, 2007). While the Pearson correlation is highly dependent on the linear relationship between the distributions in question, Spearman mainly emphasizes the ability of the distributions to maintain their relative ranking. Tables 7 and 8 show the results of the evaluations of the cross-lingual relatedness, when using an ESA concept vector with a size of maximum</abstract>
<note confidence="0.641821315789474">English Spanish Arabic Romanian Miller-Charles English Spanish Arabic Romanian 0.58 0.43 0.32 0.50 0.38 0.32 0.58 0.44 0.20 0.36 WordSimilarity-353 English Spanish Arabic Romanian 0.55 0.32 0.31 0.29 0.28 0.25 0.30 0.45 0.32 0.28 Table 7: Pearson correlation for crosslingual relatedness on the Miller-Charles and WordSimilarity-353 data sets English Spanish Arabic Romanian Miller-Charles English Spanish Arabic Romanian 0.75 0.56 0.27 0.55 0.32 0.21 0.61 0.64 0.17 0.33 WordSimilarity-353 English Spanish Arabic Romanian 0.71 0.55 0.35 0.38 0.30 0.20 0.28</note>
<phone confidence="0.618495">0.50 0.29</phone>
<abstract confidence="0.948220186046511">0.26 Table 8: Spearman correlation for crosslingual relatedness on the Miller-Charles and WordSimilarity-353 data sets As a validation of our ESA implementation, we compared the results obtained for the monolingual English relatedness with other results reported in the past for the same data sets. Gabrilovich and Markovitch (2007) reported a Spearman correlation of 0.72 for the Miller-Charles data set and for the WordSimilarity-353 data set, respecconcepts are selected in reversed order of their weight inside the vector in the respective language. Note that the cross-lingual mapping between the concepts in the ESA vectors is done after the selection of the top 10,000 concepts in each language. tively. Zesch et al. (2008) reported a Spearman correlation of 0.67 for the Miller-Charles set. These values are comparable to the Spearman correlation scores obtained in our experiments for the English data sets (see Table 8), with a fairly large improvement obtained on the Miller-Charles data set when using our implementation. 6 Discussion Overall, our method succeeds in capturing the cross-lingual semantic relatedness between words. As a point of comparison, one can use the monolingual measures of relatedness as reflected by the diagonals in Tables 7 and 8. Looking at the monolingual evaluations, the results seem to be correlated with the Wikipedia size for the corresponding language, with the English measure scoring the highest. These results are not surprising, given the direct relation between the Wikipedia size and the sparseness of the ESA concept vectors. A similar trend is observed for the cross-lingual relatedness, with higher results obtained for the languages with large Wikipedia versions (e.g., English-Spanish), and lower results for the languages with a smaller size Wikipedia (e.g., Arabic-Spanish). For comparison, we ran two additional experiments. In the first experiment, we compared the coverage of our cross-lingual relatedness method to a direct use of the translation links available in Wikipedia. The cross-lingual relatedness is turned into a monolingual relatedness by using the interlanguage Wikipedia links to translate the first of the two words in a cross-lingual pair into the of the second word in the From the total of 433 word pairs available in the two data sets, this method can produce translations for an average of 103 word pairs per language pair. This means that the direct Wikipedia interlanguage links allow the cross-lingual relatedness measure to be transformed into a monolingual relatedness in about 24% of the cases, which is a low coverage compared to the full coverage that can be obtained with our cross-lingual method of relatedness. In an attempt to raise the coverage of the translation, we ran a second experiment where we used a state-of-the-art translation engine to translate the word in a pair into the language of the secuse all the interlanguage links obtained by combining the initial and the discovered links, as described in Section 4. 1197 ond word in the pair. We use Google Translate, which is a statistical machine translation engine that relies on large parallel corpora, to find the most likely translation for a given word. Unlike the previous experiment, this time we can achieve full translation coverage, and thus we are able to produce data sets of equal size that can be used for a comparison between relatedness measures. Specifically, using the translation produced by the machine translation engine for the first word in a pair, we calculate the relatedness within the space of the language of the second word using a monolingual ESA also based on Wikipedia. The results obtained with this method are compared against the results obtained with our cross-lingual ESA relatedness. Using a Pearson correlation, our cross-lingual relatedness method achieves an average score across all six language pairs of 0.36 for the Miller- Charles data set and 0.30 for the WordSimilaritydata which is higher than the 0.33 and 0.28 scores achieved for the same data sets when using a translation obtained with Google Translate followed by a monolingual measure of relatedness. These results are encouraging, also given that the translation-based method is limited to those language pairs for which a translation engine exists (e.g., Google Translate covers 40 languages), whereas our method can be applied to any language pair from the set of 250 languages for which a Wikipedia version exists. To gain further insights, we also determined the impact of the vector length in the ESA concept vector representation, by calculating the Pearson correlation for vectors of different lengths. Figures 1 and 2 show the Pearson score as a function of the vector length for the Miller-Charles and WordSimilarity-353 data sets. The plots show that the cross-lingual measure of relatedness is not significantly affected by the reduction or increase of the vector length. Thus, the use of vectors of length 10,000 (as used in most of our experiments) appears as a reasonable tradeoff between accuracy and performance. Furthermore, by comparing the performance of the proposed Lesk-like model to the traditional cosine-similarity (Figures 3 and 4), we note that the Lesk-like model outperforms the cosine model on most language pairs. We believe that this is average considers all the cross-lingual relatedness scores listed in Table 7; it does not include the monolingual scores listed on the table diagonal. 5000 10000 15000 20000 25000 30000 Vector length Figure 1: Pearson correlation vs. ESA vector length on the Miller-Charles data set 5000 10000 15000 20000 25000 30000 Vector Size Figure 2: Pearson correlation vs. ESA vector length on the WordSimilarity-353 data set due to the stricter correlation conditions imposed by the cosine-metric in such sparse vector-based representations, as compared to the more relaxed hypothesis used by the Lesk model. Finally, we also looked at the relation between the number of interlanguage links found for the concepts in a vector and the length of the vector. Figures 5 and 6 display the average number of interlanguage links as a function of the concept vector length. By analyzing the effect of the average number of interlanguage links found per word in the given datasets (Figures 5 and 6), we notice that these links increase proportionally with the vector size, as expected. However, this increase does not lead to any significant improvements in accuracy (Figures 1 and 2). This implies that while the presence interlanguage links is a prerequisite for the mea- Average Pearson 0.6 0.5 0.4 0.3 0.2 0.1 Pearson correlation 0.6 0.5 0.4 0.3 0.2 0.1 1198 Pearson 5000 10000 15000 20000 25000 30000 Vector Size Figure 3: Lesk vs. cosine similarity for the Miller- Charles data set 5000 10000 15000 20000 25000 30000 Vector length Figure 5: Number of interlanguage links vs. vector length for the Miller-Charles data set 0.7 0.6 0.5 0.4 0.3 0.2 0.1 2000 Number of interlanguage links</abstract>
<address confidence="0.63244575">1500 1000 500 0</address>
<phone confidence="0.648453">5000 10000 15000 20000 25000 30000</phone>
<title confidence="0.279621">Vector Size</title>
<phone confidence="0.466925">5000 10000 15000 20000 25000 30000</phone>
<note confidence="0.8144536">Vector length Pearson 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Number of interlanguage links 4000 3500 3000 2500 2000 1500 1000 500 0 Figure 4: Lesk vs. cosine similarity for the</note>
<abstract confidence="0.958846216981132">WordSimilarity-353 data set of their effect is only significant for the top ranked concepts in a vector. Therefore, increasing the vectors size to maximize the matching of the projected dimensions does not necessarily lead to accuracy improvements. 7 Related Work Measures of word relatedness were found useful in a large number of natural language processing applications, including word sense disambiguation (Patwardhan et al., 2003), synonym identification (Turney, 2001), automated essay scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness English, by using methods applied on knowllanguages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., Word- Net structures or large corpora, the measures of relatedness developed for English can be in principle applied to other languages as well. All these methods proposed in the past have concerned with relatedness calculated within the boundaries of one lanas opposed to which is the focus of our work. research area closest to the task of cross- 1199 lingual relatedness is perhaps cross-language information retrieval, which is concerned with matching queries posed in one language to document collections in a second language. Note however that most of the approaches to date for crosslanguage information retrieval have been based on direct translations obtained for words in the query or in the documents, by using bilingual dictionaries (Monz and Dorr, 2005) or parallel corpora (Nie et al., 1999). Such explicit translations can identify a direct correspondence between words in two (e.g., they will find that are translations of each other), but will not capture similarities of a different de- (e.g., they will not find that in En.) is similar to Also related are the areas of word alignment for machine translation (Och and Ney, 2000), induction of translation lexicons (Schafer and Yarowsky, 2002), and cross-language annotation projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). As with cross-language information retrieval, these areas have primarily considered direct translations between words, rather than an entire spectrum of relatedness, as we do in our work. 8 Conclusions In this paper, we addressed the problem of cross-lingual semantic relatedness, which is a core task for a number of applications, including cross-language information retrieval, crosslanguage text classification, lexical choice for machine translation, cross-language projections of resources and annotations, and others. We introduced a method based on concept vectors built from Wikipedia, which are mapped across the interlanguage links available between Wikipedia versions in multiple languages. Experiments performed on six language pairs, connecting English, Spanish, Arabic and Romanian, showed that the method is effective at capturing the cross-lingual relatedness of words. The method was shown to be competitive when compared to methods based on a translation using the direct Wikipedia links or using a statistical translation engine. Moreover, our method has wide applicability across languages, as it can be used for any language pair from the set of 250 languages for which a Wikipedia version exists. The cross-lingual data sets introduced in this paper can be downloaded from http://lit.csci.unt.edu/index.php/Downloads. Acknowledgments The authors are grateful to Carmen Banea for her help with the construction of the data sets. This material is based in part upon work supported by the National Science Foundation CAREER award sions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</abstract>
<note confidence="0.576216677419355">References S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statistical machine translation through global lexical seand sentence reconstruction. In of the Annual Meeting of the Association of Compu- Prague, Czech Republic. A. Budanitsky and G. Hirst. 2001. Semantic distance in WordNet: An experimental, application-oriented of five measures. In of the NAACL Workshop on WordNet and Other Lexical Pittsburgh. L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing search in context: the concept revisited. In pages 406–414. P. Foltz, D. Laham, and T. Landauer. 1999. Automated essay scoring: Applications to educational technol- In of World Conference on Educational Multimedia, Hypermedia and Telecommu- Chesapeake, Virginia. E. Gabrilovich and S. Markovitch. 2007. Computing semantic relatedness using wikipedia-based exsemantic analysis. In of the Inter- Joint Conference on Artificial pages 1606–1611. A. Gliozzo and C. Strapparava. 2006. Exploiting comparable corpora and bilingual dictionaries for crosstext categorization. In of the Conference of the Association for Computational Sydney, Australia. K. Heylen, Y. Peirsman, D. Geeraerts, and D. Speel-</note>
<abstract confidence="0.808282571428571">man. 2008. Modelling word similarity: an evaluation of automatic synonymy extraction algorithms. of the Sixth International Language and Marrakech, Morocco. T. Hughes and D. Ramage. 2007. Lexical semantic with random graph walks. In Proceedings of the Conference on Empirical Methods in Nat-</abstract>
<note confidence="0.748649">Language Prague, Czech Republic. R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002. Evaluating translational correspondence using annoprojection. In of the 40th Annual Meeting of the Association for Computational Lin- (ACL Philadelphia, July. 1200 J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Rein Computational Taiwan. T. K. Landauer, P. Foltz, and D. Laham. 1998. Introto latent semantic analysis. Pro- 25. M.E. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine from an ice cream cone. In of the Conference Toronto, June. G. Miller and W. Charles. 1998. Contextual correof semantic similarity. and Cogni- 6(1). D. Milne. 2007. Computing semantic relatedness using wikipedia link structure. In European Language Association (ELRA), editor, Proceedings of the New Zealand Computer Science Re- Student Conference (NZCSRSC New Zealand. S. Mohammad, I. Gurevych, G. Hirst, and T. Zesch. 2007. Cross-lingual distributional profiles of concepts for measuring semantic distance. In Proceedings of the Joint Conference on Empir-</note>
<title confidence="0.575821">ical Methods in Natural Language Processing and Computational Natural Language Learning</title>
<address confidence="0.511588">Prague, Czech Republic. C. Monz and B.J. Dorr. 2005. Iterative translation</address>
<affiliation confidence="0.58945625">disambiguation for cross-language information re- In of the 28th Annual International ACM SIGIR Conference on Research and in Information Salvador,</affiliation>
<address confidence="0.758948">Brazil.</address>
<note confidence="0.837428416666667">J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from Web. In of the 22nd annual international ACM SIGIR conference on Research and in information F. Och and H. Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th International Conference on Linguistics (COLING Saarbrucken, Germany, August. S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.</note>
<title confidence="0.943658">Using measures of semantic relatedness for word</title>
<author confidence="0.418686">In of the Fourth</author>
<affiliation confidence="0.769452">International Conference on Intelligent Text Proand Computational Mexico</affiliation>
<address confidence="0.940939">City, February.</address>
<note confidence="0.8968605">P. Resnik. 1995. Using information content to evalusemantic similarity. In of the 14th</note>
<affiliation confidence="0.497823">International Joint Conference on Artificial Intelli-</affiliation>
<address confidence="0.75816">Montreal, Canada.</address>
<note confidence="0.9179041">E. Riloff, C. Schafer, and D. Yarowsky. 2002. Inducing information extraction systems for new lanvia cross-language projection. In Proceedings of the 19th International Conference on Com- Taipei, Taiwan, August. G. Salton, A. Wong, and C.S. Yang. 1997. A vecspace model for automatic indexing. In Readin Information pages 273–280. Morgan Kaufmann Publishers, San Francisco, CA. C. Schafer and D. Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and languages. In of the 6th Conferon Natural Language Learning (CoNLL Taipei, Taiwan, August. M. Strube and S. P. Ponzetto. 2006. Wikirelate! computing semantic relatedeness using Wikipedia. In Proceedings of the American Association for Artifi- Boston, MA. P. Turney. 2001. Mining the web for synonyms: PMIversus LSA on TOEFL. In of the</note>
<affiliation confidence="0.618184">Twelfth European Conference on Machine Learning</affiliation>
<address confidence="0.862637">Freiburg, Germany.</address>
<note confidence="0.745438">X. Wang, S. Ju, and S. Wu. 2008. A survey of chitext similarity computation. In of Asia Information Retrieval Harbin, China. Z. Wu and M. Palmer. 1994. Verb semantics and lexselection. In of the 32nd Annual Meeting of the Association for Computational Lin- Las Cruces, New Mexico. T. Zesch, I. Gurevych, and M. M¨uhlh¨auser. 2007.</note>
<title confidence="0.9602888">Comparing Wikipedia and German Wordnet by Evaluating Semantic Relatedness on Multiple In ofHuman Language Technologies: The Annual Conference of the North American Chapter of the Association for Computa-</title>
<author confidence="0.472249">Using</author>
<affiliation confidence="0.3851555">Wiktionary for Computing Semantic Relatedness. of the American Association for Ar-</affiliation>
<address confidence="0.590875">Chicago. 1201</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>P Haffner</author>
<author>S Kanthak</author>
</authors>
<title>Statistical machine translation through global lexical selection and sentence reconstruction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association of Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1560" citStr="Bangalore et al., 2007" startWordPosition="228" endWordPosition="231">lies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word</context>
</contexts>
<marker>Bangalore, Haffner, Kanthak, 2007</marker>
<rawString>S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statistical machine translation through global lexical selection and sentence reconstruction. In Proceedings of the Annual Meeting of the Association of Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<location>Pittsburgh.</location>
<contexts>
<context position="28669" citStr="Budanitsky and Hirst, 2001" startWordPosition="4589" endWordPosition="4592"> Lesk vs. cosine similarity for the WordSimilarity-353 data set sure of relatedness,6 their effect is only significant for the top ranked concepts in a vector. Therefore, increasing the vectors size to maximize the matching of the projected dimensions does not necessarily lead to accuracy improvements. 7 Related Work Measures of word relatedness were found useful in a large number of natural language processing applications, including word sense disambiguation (Patwardhan et al., 2003), synonym identification (Turney, 2001), automated essay scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et </context>
</contexts>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>A. Budanitsky and G. Hirst. 2001. Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures. In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited. In</title>
<date>2001</date>
<booktitle>WWW,</booktitle>
<pages>406--414</pages>
<contexts>
<context position="14459" citStr="Finkelstein et al., 2001" startWordPosition="2308" endWordPosition="2311">Romanian 179,440 623,358 Table 5: Number of articles and size of vocabulary for the four Wikipedia versions After pre-processing, the articles are indexed to generate the ESA concept vectors. From each Wikipedia version, we also extract other features including article titles, interlanguage links, and Wikipedia category graphs. The interlanguage links are further processed to recover any missing links, as described in the previous section. 5.1 Data For the evaluation, we build several cross-lingual datasets based on the standard Miller-Charles (Miller and Charles, 1998) and WordSimilarity353 (Finkelstein et al., 2001) English word relatedness datasets. The Miller-Charles dataset (Miller and Charles, 1998) consists of 30-word pairs ranging from synonymy pairs (e.g., car - automobile) to completely unrelated terms (e.g., noon - string). The relatedness of each word pair was rated by 38 human subjects, using a scale from 0 (not-related) to 4 (perfect synonymy). The dataset is available only in English and has been widely used in previous semantic relatedness evaluations (e.g., (Resnik, 1995; Hughes and Ramage, 2007; Zesch et al., 2008)). The WordSimilarity-353 dataset (also known as Finkelstein-353) (Finkelst</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing search in context: the concept revisited. In WWW, pages 406–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Foltz</author>
<author>D Laham</author>
<author>T Landauer</author>
</authors>
<title>Automated essay scoring: Applications to educational technology.</title>
<date>1999</date>
<booktitle>In Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunications,</booktitle>
<location>Chesapeake, Virginia.</location>
<contexts>
<context position="28617" citStr="Foltz et al., 1999" startWordPosition="4583" endWordPosition="4586">500 3000 2500 2000 1500 1000 500 0 Figure 4: Lesk vs. cosine similarity for the WordSimilarity-353 data set sure of relatedness,6 their effect is only significant for the top ranked concepts in a vector. Therefore, increasing the vectors size to maximize the matching of the projected dimensions does not necessarily lead to accuracy improvements. 7 Related Work Measures of word relatedness were found useful in a large number of natural language processing applications, including word sense disambiguation (Patwardhan et al., 2003), synonym identification (Turney, 2001), automated essay scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hugh</context>
</contexts>
<marker>Foltz, Laham, Landauer, 1999</marker>
<rawString>P. Foltz, D. Laham, and T. Landauer. 1999. Automated essay scoring: Applications to educational technology. In Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunications, Chesapeake, Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="6018" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="913" endWordPosition="916">nglish Wikipedia, to ten in the Spanish Wikipedia, and as many as 23 in the Arabic Wikipedia. 3 Concept Vector Representations using Explicit Semantic Analysis To calculate the cross-lingual relatedness of two words, we measure the closeness of their concept vector representations, which are built from Wikipedia using explicit semantic analysis (ESA). Encyclopedic knowledge is typically organized into concepts (or topics), each concept being further described using definitions, examples, 1http://meta.wikimedia.org/wiki/List of Wikipedias #Grand Total and possibly links to other concepts. ESA (Gabrilovich and Markovitch, 2007) relies on the distribution of words inside the encyclopedic descriptions, and builds semantic representations for a given word in the form of a vector of the encyclopedic concepts in which the word appears. In this vector representation, each encyclopedic concept is assigned with a weight, calculated as the term frequency of the given word inside the concept’s article. Formally, let C be the set of all the Wikipedia concepts, and let a be any content word. We define a� as the ESA concept vector of term a: a� = {wc1, wc2...wc.} , (1) where wci is the weight of the concept ci with respect to a.</context>
<context position="19469" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="3117" endWordPosition="3120">e data sets is thus doubled in this way (e.g., the 30 word pairs in the English Miller-Charles set are transformed into 60 word pairs in the English-Spanish Miller-Charles set). 5.2 Results We evaluate the cross-lingual measure of relatedness on each of the six language pairs. For comparison purposes, we also evaluate the monolingual relatedness on the four languages. For the evaluation, we use the Pearson (r) and Spearman (ρ) correlation coefficients, which are the standard metrics used in the past for the evaluation of semantic relatedness (Finkelstein et 1196 al., 2001; Zesch et al., 2008; Gabrilovich and Markovitch, 2007). While the Pearson correlation is highly dependent on the linear relationship between the distributions in question, Spearman mainly emphasizes the ability of the distributions to maintain their relative ranking. Tables 7 and 8 show the results of the evaluations of the cross-lingual relatedness, when using an ESA concept vector with a size of maximum 10,000 concepts.3 English Spanish Arabic Romanian Miller-Charles English 0.58 0.43 0.32 0.50 Spanish 0.44 0.20 0.38 Arabic 0.36 0.32 Romanian 0.58 WordSimilarity-353 English 0.55 0.32 0.31 0.29 Spanish 0.45 0.32 0.28 Arabic 0.28 0.25 Romanian 0.</context>
<context position="20742" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="3306" endWordPosition="3309">slingual relatedness on the Miller-Charles and WordSimilarity-353 data sets English Spanish Arabic Romanian Miller-Charles English 0.75 0.56 0.27 0.55 Spanish 0.64 0.17 0.32 Arabic 0.33 0.21 Romanian 0.61 WordSimilarity-353 English 0.71 0.55 0.35 0.38 Spanish 0.50 0.29 0.30 Arabic 0.26 0.20 Romanian 0.28 Table 8: Spearman correlation for crosslingual relatedness on the Miller-Charles and WordSimilarity-353 data sets As a validation of our ESA implementation, we compared the results obtained for the monolingual English relatedness with other results reported in the past for the same data sets. Gabrilovich and Markovitch (2007) reported a Spearman correlation of 0.72 for the Miller-Charles data set and 0.75 for the WordSimilarity-353 data set, respec3The concepts are selected in reversed order of their weight inside the vector in the respective language. Note that the cross-lingual mapping between the concepts in the ESA vectors is done after the selection of the top 10,000 concepts in each language. tively. Zesch et al. (2008) reported a Spearman correlation of 0.67 for the Miller-Charles set. These values are comparable to the Spearman correlation scores obtained in our experiments for the English data sets (see T</context>
<context position="29350" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="4700" endWordPosition="4703">006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other languages as well. All these methods proposed in the past have been concerned with monolingual word relatedness calculated within the boundaries of</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Strapparava</author>
</authors>
<title>Exploiting comparable corpora and bilingual dictionaries for crosslanguage text categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="1477" citStr="Gliozzo and Strapparava, 2006" startWordPosition="215" endWordPosition="218"> we address the task of cross-lingual semantic relatedness, and introduce a method that relies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel text</context>
</contexts>
<marker>Gliozzo, Strapparava, 2006</marker>
<rawString>A. Gliozzo and C. Strapparava. 2006. Exploiting comparable corpora and bilingual dictionaries for crosslanguage text categorization. In Proceedings of the Conference of the Association for Computational Linguistics, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heylen</author>
<author>Y Peirsman</author>
<author>D Geeraerts</author>
<author>D Speelman</author>
</authors>
<title>Modelling word similarity: an evaluation of automatic synonymy extraction algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="29591" citStr="Heylen et al., 2008" startWordPosition="4742" endWordPosition="4745"> these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other languages as well. All these methods proposed in the past have been concerned with monolingual word relatedness calculated within the boundaries of one language, as opposed to cross-lingual relatedness, which is the focus of our work. The research area closest to the task of cross1199 lingual relatedness is perhaps cross-language information retrieval, which is concerned with matching </context>
</contexts>
<marker>Heylen, Peirsman, Geeraerts, Speelman, 2008</marker>
<rawString>K. Heylen, Y. Peirsman, D. Geeraerts, and D. Speelman. 2008. Modelling word similarity: an evaluation of automatic synonymy extraction algorithms. In Proceedings of the Sixth International Language Resources and Evaluation, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hughes</author>
<author>D Ramage</author>
</authors>
<title>Lexical semantic knowledge with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="14963" citStr="Hughes and Ramage, 2007" startWordPosition="2390" endWordPosition="2393">tasets based on the standard Miller-Charles (Miller and Charles, 1998) and WordSimilarity353 (Finkelstein et al., 2001) English word relatedness datasets. The Miller-Charles dataset (Miller and Charles, 1998) consists of 30-word pairs ranging from synonymy pairs (e.g., car - automobile) to completely unrelated terms (e.g., noon - string). The relatedness of each word pair was rated by 38 human subjects, using a scale from 0 (not-related) to 4 (perfect synonymy). The dataset is available only in English and has been widely used in previous semantic relatedness evaluations (e.g., (Resnik, 1995; Hughes and Ramage, 2007; Zesch et al., 2008)). The WordSimilarity-353 dataset (also known as Finkelstein-353) (Finkelstein et al., 2001) consists of 353 word pairs annotated by 13 human experts, on a scale from 0 (unrelated) to 10 (very closely related or identical). The Miller-Charles set is a subset in the WordSimilarity-353 data set. Unlike the Miller-Charles data set, which consists only of Intifada(en) F-4 Intifada(es) Intifada(es) F-4 �� ����������(ar) � Intifada(en) F-4 �� ����������(ar) ~ Yes Yes No 1195 Word pair English coast - shore car - automobile brother - monk Spanish costa - orilla coche - automovil </context>
<context position="29237" citStr="Hughes and Ramage, 2007" startWordPosition="4682" endWordPosition="4685">999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other languages as well. All these metho</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>T. Hughes and D. Ramage. 2007. Lexical semantic knowledge with random graph walks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
<author>P Resnik</author>
<author>A Weinberg</author>
<author>O Kolak</author>
</authors>
<title>Evaluating translational correspondence using annotation projection.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Philadelphia,</location>
<contexts>
<context position="1736" citStr="Hwa et al., 2002" startWordPosition="253" endWordPosition="256"> method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version exists. The paper is organized as follows. We first provide a brief overview of Wikipedia, followed by a descri</context>
<context position="31133" citStr="Hwa et al., 2002" startWordPosition="4990" endWordPosition="4993">pora (Nie et al., 1999). Such explicit translations can identify a direct correspondence between words in two languages (e.g., they will find that fabbrica (It.) and factory (En.) are translations of each other), but will not capture similarities of a different degree (e.g., they will not find that lavoratore (It.; worker in En.) is similar to factory (En.). Also related are the areas of word alignment for machine translation (Och and Ney, 2000), induction of translation lexicons (Schafer and Yarowsky, 2002), and cross-language annotation projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). As with cross-language information retrieval, these areas have primarily considered direct translations between words, rather than an entire spectrum of relatedness, as we do in our work. 8 Conclusions In this paper, we addressed the problem of cross-lingual semantic relatedness, which is a core task for a number of applications, including cross-language information retrieval, crosslanguage text classification, lexical choice for machine translation, cross-language projections of resources and annotations, and others. We introduced a method based on concept vectors bu</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Kolak, 2002</marker>
<rawString>R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002. Evaluating translational correspondence using annotation projection. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="29211" citStr="Jiang and Conrath, 1997" startWordPosition="4678" endWordPosition="4681"> scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other language</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<contexts>
<context position="29301" citStr="Landauer et al., 1998" startWordPosition="4694" endWordPosition="4697">ce resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other languages as well. All these methods proposed in the past have been concerned with monolingual wor</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to latent semantic analysis. Discourse Processes, 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the SIGDOC Conference</booktitle>
<location>Toronto,</location>
<contexts>
<context position="7839" citStr="Lesk, 1986" startWordPosition="1238" endWordPosition="1239">he possible senses of bird, including Bird as a surname as in e.g., “Larry Bird.” Weight Wikipedia concept 51.4 Lists Of Birds By Region 44.8 Bird 40.3 British Birds Rarities Committee 32.8 Origin Of Birds 31.5 Ornithology 30.1 List Of Years In Birding And Ornithology 29.8 Bird Vocalization 27.4 Global Spread Of H5n1 In 2006 26.5 Larry Bird 22.3 Birdwatching Table 2: Top ten Wikipedia concepts for the word “bird” In our ESA implementation, we make three changes with respect to the original ESA algorithm. First, we replace the cosine similarity with Relatedness(a, b) = 1193 a Lesk-like metric (Lesk, 1986), which places less emphasis on the distributional differences between the vector weights and more emphasis on the overlap (mutual coverage) between the vector features, and thus it is likely to be more appropriate for the sparse ESA vectors, and for the possible asymmetry between languages. Let a and b be two terms with the corresponding ESA concept vectors A � and B respectively. Let A and B represent the sets of concepts with a non-zero weight encountered in � � B respectively. The coverage of A�by B�is defined as: G(�B |~A) = E wai (3) i∈B and similarly, the coverage of B~ by A~ is: G( ~A </context>
<context position="29151" citStr="Lesk, 1986" startWordPosition="4670" endWordPosition="4671"> identification (Turney, 2001), automated essay scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness develop</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M.E. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the SIGDOC Conference 1986, Toronto, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>W Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1998</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="14410" citStr="Miller and Charles, 1998" startWordPosition="2301" endWordPosition="2304">9 Spanish 520,154 406,134 Arabic 149,340 216,317 Romanian 179,440 623,358 Table 5: Number of articles and size of vocabulary for the four Wikipedia versions After pre-processing, the articles are indexed to generate the ESA concept vectors. From each Wikipedia version, we also extract other features including article titles, interlanguage links, and Wikipedia category graphs. The interlanguage links are further processed to recover any missing links, as described in the previous section. 5.1 Data For the evaluation, we build several cross-lingual datasets based on the standard Miller-Charles (Miller and Charles, 1998) and WordSimilarity353 (Finkelstein et al., 2001) English word relatedness datasets. The Miller-Charles dataset (Miller and Charles, 1998) consists of 30-word pairs ranging from synonymy pairs (e.g., car - automobile) to completely unrelated terms (e.g., noon - string). The relatedness of each word pair was rated by 38 human subjects, using a scale from 0 (not-related) to 4 (perfect synonymy). The dataset is available only in English and has been widely used in previous semantic relatedness evaluations (e.g., (Resnik, 1995; Hughes and Ramage, 2007; Zesch et al., 2008)). The WordSimilarity-353 </context>
</contexts>
<marker>Miller, Charles, 1998</marker>
<rawString>G. Miller and W. Charles. 1998. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milne</author>
</authors>
<title>Computing semantic relatedness using wikipedia link structure.</title>
<date>2007</date>
<booktitle>In European Language Resources Association</booktitle>
<editor>(ELRA), editor,</editor>
<location>New Zealand.</location>
<contexts>
<context position="13221" citStr="Milne, 2007" startWordPosition="2126" endWordPosition="2127">81 0.08 Spanish Arabic 33,889 3,319 0.07 Romanian English 75,685 6,783 0.46 Romanian Spanish 36,002 3,546 0.22 Romanian Arabic 15,777 1,698 0.10 Arabic English 46,072 3,170 0.33 Arabic Spanish 28,142 3,109 0.21 Arabic Romanian 15,965 1,970 0.12 Table 4: Interlanguage links (initial and discovered) and their coverage in Wikipedia versions in four languages. 5 Experiments and Evaluations We run our experiments on four languages: English, Spanish, Romanian and Arabic. For each of these languages, we use a Wikipedia download from October 2008. The articles were preprocessed using Wikipedia Miner (Milne, 2007) to extract structural information such as generality, and interlanguage links. Furthermore, articles were also processed to remove numerical content, as well as any characters not included in the given language’s alphabet. The content words are stemmed, and words shorter than three characters are removed (a heuristic which we use as an approximation for stopword removal). Table 5 shows the number of articles in each Wikipedia version and the size of their vocabularies, as obtained after the pre-processing step. Articles Vocabulary English 2,221,980 1,231,609 Spanish 520,154 406,134 Arabic 149</context>
</contexts>
<marker>Milne, 2007</marker>
<rawString>D. Milne. 2007. Computing semantic relatedness using wikipedia link structure. In European Language Resources Association (ELRA), editor, In Proceedings of the New Zealand Computer Science Research Student Conference (NZCSRSC 2007), New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
<author>I Gurevych</author>
<author>G Hirst</author>
<author>T Zesch</author>
</authors>
<title>Cross-lingual distributional profiles of concepts for measuring semantic distance.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL-2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1760" citStr="Mohammad et al., 2007" startWordPosition="257" endWordPosition="260">e the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version exists. The paper is organized as follows. We first provide a brief overview of Wikipedia, followed by a description of the method to b</context>
<context position="29533" citStr="Mohammad et al., 2007" startWordPosition="4731" endWordPosition="4734">lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other languages as well. All these methods proposed in the past have been concerned with monolingual word relatedness calculated within the boundaries of one language, as opposed to cross-lingual relatedness, which is the focus of our work. The research area closest to the task of cross1199 lingual relatedness is perhaps cross-languag</context>
<context position="31157" citStr="Mohammad et al., 2007" startWordPosition="4994" endWordPosition="4997">1999). Such explicit translations can identify a direct correspondence between words in two languages (e.g., they will find that fabbrica (It.) and factory (En.) are translations of each other), but will not capture similarities of a different degree (e.g., they will not find that lavoratore (It.; worker in En.) is similar to factory (En.). Also related are the areas of word alignment for machine translation (Och and Ney, 2000), induction of translation lexicons (Schafer and Yarowsky, 2002), and cross-language annotation projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). As with cross-language information retrieval, these areas have primarily considered direct translations between words, rather than an entire spectrum of relatedness, as we do in our work. 8 Conclusions In this paper, we addressed the problem of cross-lingual semantic relatedness, which is a core task for a number of applications, including cross-language information retrieval, crosslanguage text classification, lexical choice for machine translation, cross-language projections of resources and annotations, and others. We introduced a method based on concept vectors built from Wikipedia, whic</context>
</contexts>
<marker>Mohammad, Gurevych, Hirst, Zesch, 2007</marker>
<rawString>S. Mohammad, I. Gurevych, G. Hirst, and T. Zesch. 2007. Cross-lingual distributional profiles of concepts for measuring semantic distance. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL-2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monz</author>
<author>B J Dorr</author>
</authors>
<title>Iterative translation disambiguation for cross-language information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<location>Salvador, Brazil.</location>
<contexts>
<context position="1409" citStr="Monz and Dorr, 2005" startWordPosition="208" endWordPosition="211">hniques is becoming increasingly important. In this paper, we address the task of cross-lingual semantic relatedness, and introduce a method that relies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically li</context>
<context position="30501" citStr="Monz and Dorr, 2005" startWordPosition="4889" endWordPosition="4892">with monolingual word relatedness calculated within the boundaries of one language, as opposed to cross-lingual relatedness, which is the focus of our work. The research area closest to the task of cross1199 lingual relatedness is perhaps cross-language information retrieval, which is concerned with matching queries posed in one language to document collections in a second language. Note however that most of the approaches to date for crosslanguage information retrieval have been based on direct translations obtained for words in the query or in the documents, by using bilingual dictionaries (Monz and Dorr, 2005) or parallel corpora (Nie et al., 1999). Such explicit translations can identify a direct correspondence between words in two languages (e.g., they will find that fabbrica (It.) and factory (En.) are translations of each other), but will not capture similarities of a different degree (e.g., they will not find that lavoratore (It.; worker in En.) is similar to factory (En.). Also related are the areas of word alignment for machine translation (Och and Ney, 2000), induction of translation lexicons (Schafer and Yarowsky, 2002), and cross-language annotation projections to a second language (Rilof</context>
</contexts>
<marker>Monz, Dorr, 2005</marker>
<rawString>C. Monz and B.J. Dorr. 2005. Iterative translation disambiguation for cross-language information retrieval. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Salvador, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-Y Nie</author>
<author>M Simard</author>
<author>P Isabelle</author>
<author>R Durand</author>
</authors>
<title>Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="1387" citStr="Nie et al., 1999" startWordPosition="204" endWordPosition="207">ext processing techniques is becoming increasingly important. In this paper, we address the task of cross-lingual semantic relatedness, and introduce a method that relies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, </context>
<context position="30540" citStr="Nie et al., 1999" startWordPosition="4896" endWordPosition="4899">ed within the boundaries of one language, as opposed to cross-lingual relatedness, which is the focus of our work. The research area closest to the task of cross1199 lingual relatedness is perhaps cross-language information retrieval, which is concerned with matching queries posed in one language to document collections in a second language. Note however that most of the approaches to date for crosslanguage information retrieval have been based on direct translations obtained for words in the query or in the documents, by using bilingual dictionaries (Monz and Dorr, 2005) or parallel corpora (Nie et al., 1999). Such explicit translations can identify a direct correspondence between words in two languages (e.g., they will find that fabbrica (It.) and factory (En.) are translations of each other), but will not capture similarities of a different degree (e.g., they will not find that lavoratore (It.; worker in En.) is similar to factory (En.). Also related are the areas of word alignment for machine translation (Och and Ney, 2000), induction of translation lexicons (Schafer and Yarowsky, 2002), and cross-language annotation projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Moham</context>
</contexts>
<marker>Nie, Simard, Isabelle, Durand, 1999</marker>
<rawString>J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<location>Saarbrucken, Germany,</location>
<contexts>
<context position="1535" citStr="Och and Ney, 2000" startWordPosition="224" endWordPosition="227">ce a method that relies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measur</context>
<context position="30966" citStr="Och and Ney, 2000" startWordPosition="4966" endWordPosition="4969">trieval have been based on direct translations obtained for words in the query or in the documents, by using bilingual dictionaries (Monz and Dorr, 2005) or parallel corpora (Nie et al., 1999). Such explicit translations can identify a direct correspondence between words in two languages (e.g., they will find that fabbrica (It.) and factory (En.) are translations of each other), but will not capture similarities of a different degree (e.g., they will not find that lavoratore (It.; worker in En.) is similar to factory (En.). Also related are the areas of word alignment for machine translation (Och and Ney, 2000), induction of translation lexicons (Schafer and Yarowsky, 2002), and cross-language annotation projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). As with cross-language information retrieval, these areas have primarily considered direct translations between words, rather than an entire spectrum of relatedness, as we do in our work. 8 Conclusions In this paper, we addressed the problem of cross-lingual semantic relatedness, which is a core task for a number of applications, including cross-language information retrieval, crosslanguage text classifi</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. Och and H. Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), Saarbrucken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<location>Mexico City,</location>
<contexts>
<context position="28532" citStr="Patwardhan et al., 2003" startWordPosition="4572" endWordPosition="4575">s(es↔es cos(ro↔ro Pearson 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Number of interlanguage links 4000 3500 3000 2500 2000 1500 1000 500 0 Figure 4: Lesk vs. cosine similarity for the WordSimilarity-353 data set sure of relatedness,6 their effect is only significant for the top ranked concepts in a vector. Therefore, increasing the vectors size to maximize the matching of the projected dimensions does not necessarily lead to accuracy improvements. 7 Related Work Measures of word relatedness were found useful in a large number of natural language processing applications, including word sense disambiguation (Patwardhan et al., 2003), synonym identification (Turney, 2001), automated essay scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edg</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="14938" citStr="Resnik, 1995" startWordPosition="2388" endWordPosition="2389">oss-lingual datasets based on the standard Miller-Charles (Miller and Charles, 1998) and WordSimilarity353 (Finkelstein et al., 2001) English word relatedness datasets. The Miller-Charles dataset (Miller and Charles, 1998) consists of 30-word pairs ranging from synonymy pairs (e.g., car - automobile) to completely unrelated terms (e.g., noon - string). The relatedness of each word pair was rated by 38 human subjects, using a scale from 0 (not-related) to 4 (perfect synonymy). The dataset is available only in English and has been widely used in previous semantic relatedness evaluations (e.g., (Resnik, 1995; Hughes and Ramage, 2007; Zesch et al., 2008)). The WordSimilarity-353 dataset (also known as Finkelstein-353) (Finkelstein et al., 2001) consists of 353 word pairs annotated by 13 human experts, on a scale from 0 (unrelated) to 10 (very closely related or identical). The Miller-Charles set is a subset in the WordSimilarity-353 data set. Unlike the Miller-Charles data set, which consists only of Intifada(en) F-4 Intifada(es) Intifada(es) F-4 �� ����������(ar) � Intifada(en) F-4 �� ����������(ar) ~ Yes Yes No 1195 Word pair English coast - shore car - automobile brother - monk Spanish costa - </context>
<context position="29186" citStr="Resnik, 1995" startWordPosition="4676" endWordPosition="4677">utomated essay scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>C Schafer</author>
<author>D Yarowsky</author>
</authors>
<title>Inducing information extraction systems for new languages via cross-language projection.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="1718" citStr="Riloff et al., 2002" startWordPosition="249" endWordPosition="252">ian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version exists. The paper is organized as follows. We first provide a brief overview of Wikipedia, fo</context>
<context position="31115" citStr="Riloff et al., 2002" startWordPosition="4986" endWordPosition="4989">2005) or parallel corpora (Nie et al., 1999). Such explicit translations can identify a direct correspondence between words in two languages (e.g., they will find that fabbrica (It.) and factory (En.) are translations of each other), but will not capture similarities of a different degree (e.g., they will not find that lavoratore (It.; worker in En.) is similar to factory (En.). Also related are the areas of word alignment for machine translation (Och and Ney, 2000), induction of translation lexicons (Schafer and Yarowsky, 2002), and cross-language annotation projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). As with cross-language information retrieval, these areas have primarily considered direct translations between words, rather than an entire spectrum of relatedness, as we do in our work. 8 Conclusions In this paper, we addressed the problem of cross-lingual semantic relatedness, which is a core task for a number of applications, including cross-language information retrieval, crosslanguage text classification, lexical choice for machine translation, cross-language projections of resources and annotations, and others. We introduced a method based on </context>
</contexts>
<marker>Riloff, Schafer, Yarowsky, 2002</marker>
<rawString>E. Riloff, C. Schafer, and D. Yarowsky. 2002. Inducing information extraction systems for new languages via cross-language projection. In Proceedings of the 19th International Conference on Computational Linguistics, Taipei, Taiwan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1997</date>
<booktitle>In Readings in Information Retrieval,</booktitle>
<pages>273--280</pages>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="29278" citStr="Salton et al., 1997" startWordPosition="4690" endWordPosition="4693">rst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other languages as well. All these methods proposed in the past have been concern</context>
</contexts>
<marker>Salton, Wong, Yang, 1997</marker>
<rawString>G. Salton, A. Wong, and C.S. Yang. 1997. A vector space model for automatic indexing. In Readings in Information Retrieval, pages 273–280. Morgan Kaufmann Publishers, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Schafer</author>
<author>D Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning (CoNLL</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="1624" citStr="Schafer and Yarowsky, 2002" startWordPosition="236" endWordPosition="239">ords across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version</context>
<context position="31030" citStr="Schafer and Yarowsky, 2002" startWordPosition="4974" endWordPosition="4977"> for words in the query or in the documents, by using bilingual dictionaries (Monz and Dorr, 2005) or parallel corpora (Nie et al., 1999). Such explicit translations can identify a direct correspondence between words in two languages (e.g., they will find that fabbrica (It.) and factory (En.) are translations of each other), but will not capture similarities of a different degree (e.g., they will not find that lavoratore (It.; worker in En.) is similar to factory (En.). Also related are the areas of word alignment for machine translation (Och and Ney, 2000), induction of translation lexicons (Schafer and Yarowsky, 2002), and cross-language annotation projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). As with cross-language information retrieval, these areas have primarily considered direct translations between words, rather than an entire spectrum of relatedness, as we do in our work. 8 Conclusions In this paper, we addressed the problem of cross-lingual semantic relatedness, which is a core task for a number of applications, including cross-language information retrieval, crosslanguage text classification, lexical choice for machine translation, cross-language p</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>C. Schafer and D. Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In Proceedings of the 6th Conference on Natural Language Learning (CoNLL 2003), Taipei, Taiwan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>S P Ponzetto</author>
</authors>
<title>Wikirelate! computing semantic relatedeness using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of the American Association for Artificial Intelligence,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="28721" citStr="Strube and Ponzetto, 2006" startWordPosition="4595" endWordPosition="4598">3 data set sure of relatedness,6 their effect is only significant for the top ranked concepts in a vector. Therefore, increasing the vectors size to maximize the matching of the projected dimensions does not necessarily lead to accuracy improvements. 7 Related Work Measures of word relatedness were found useful in a large number of natural language processing applications, including word sense disambiguation (Patwardhan et al., 2003), synonym identification (Turney, 2001), automated essay scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabr</context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>M. Strube and S. P. Ponzetto. 2006. Wikirelate! computing semantic relatedeness using Wikipedia. In Proceedings of the American Association for Artificial Intelligence, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Mining the web for synonyms: PMIIR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001),</booktitle>
<location>Freiburg, Germany.</location>
<contexts>
<context position="28571" citStr="Turney, 2001" startWordPosition="4578" endWordPosition="4579">0.1 Number of interlanguage links 4000 3500 3000 2500 2000 1500 1000 500 0 Figure 4: Lesk vs. cosine similarity for the WordSimilarity-353 data set sure of relatedness,6 their effect is only significant for the top ranked concepts in a vector. Therefore, increasing the vectors size to maximize the matching of the projected dimensions does not necessarily lead to accuracy improvements. 7 Related Work Measures of word relatedness were found useful in a large number of natural language processing applications, including word sense disambiguation (Patwardhan et al., 2003), synonym identification (Turney, 2001), automated essay scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 199</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. Turney. 2001. Mining the web for synonyms: PMIIR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>S Ju</author>
<author>S Wu</author>
</authors>
<title>A survey of chinese text similarity computation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Asia Information Retrieval Symposium,</booktitle>
<location>Harbin, China.</location>
<contexts>
<context position="29562" citStr="Wang et al., 2008" startWordPosition="4737" endWordPosition="4740">ro for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other languages as well. All these methods proposed in the past have been concerned with monolingual word relatedness calculated within the boundaries of one language, as opposed to cross-lingual relatedness, which is the focus of our work. The research area closest to the task of cross1199 lingual relatedness is perhaps cross-language information retrieval, whic</context>
</contexts>
<marker>Wang, Ju, Wu, 2008</marker>
<rawString>X. Wang, S. Ju, and S. Wu. 2008. A survey of chinese text similarity computation. In Proceedings of the Asia Information Retrieval Symposium, Harbin, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="29172" citStr="Wu and Palmer, 1994" startWordPosition="4672" endWordPosition="4675">ion (Turney, 2001), automated essay scoring (Foltz et al., 1999), malapropism detection (Budanitsky and Hirst, 2001), coreference resolution (Strube and Ponzetto, 2006), and others. Most of the work to date has focused on measures of word relatedness for English, by using methods applied on knowl6Two languages with no interlanguage links between them will lead to a relatedness score of zero for any word pair across these languages, no matter how strongly related the words are. Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z. Wu and M. Palmer. 1994. Verb semantics and lexical selection. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zesch</author>
<author>I Gurevych</author>
<author>M M¨uhlh¨auser</author>
</authors>
<title>Comparing Wikipedia and German Wordnet by Evaluating Semantic Relatedness on Multiple Datasets.</title>
<date>2007</date>
<booktitle>In Proceedings ofHuman Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Zesch, Gurevych, M¨uhlh¨auser, 2007</marker>
<rawString>T. Zesch, I. Gurevych, and M. M¨uhlh¨auser. 2007. Comparing Wikipedia and German Wordnet by Evaluating Semantic Relatedness on Multiple Datasets. In Proceedings ofHuman Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zesch</author>
<author>C M¨uller</author>
<author>I Gurevych</author>
</authors>
<title>Using Wiktionary for Computing Semantic Relatedness.</title>
<date>2008</date>
<booktitle>In Proceedings of the American Association for Artificial Intelligence,</booktitle>
<location>Chicago.</location>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>T. Zesch, C. M¨uller, and I. Gurevych. 2008. Using Wiktionary for Computing Semantic Relatedness. In Proceedings of the American Association for Artificial Intelligence, Chicago.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>