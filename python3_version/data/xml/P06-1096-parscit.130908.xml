<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002727">
<title confidence="0.993049">
An End-to-End Discriminative Approach to Machine Translation
</title>
<author confidence="0.998968">
Percy Liang Alexandre Bouchard-Cˆot´e Dan Klein Ben Taskar
</author>
<affiliation confidence="0.997177">
Computer Science Division, EECS Department
University of California at Berkeley
</affiliation>
<address confidence="0.754425">
Berkeley, CA 94720
</address>
<email confidence="0.990573">
{pliang, bouchard, klein, taskar}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.993637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999250529411765">
We present a perceptron-style discriminative ap-
proach to machine translation in which large feature
sets can be exploited. Unlike discriminative rerank-
ing approaches, our system can take advantage of
learned features in all stages of decoding. We first
discuss several challenges to error-driven discrim-
inative approaches. In particular, we explore dif-
ferent ways of updating parameters given a training
example. We find that making frequent but smaller
updates is preferable to making fewer but larger up-
dates. Then, we discuss an array of features and
show both how they quantitatively increase BLEU
score and how they qualitatively interact on spe-
cific examples. One particular feature we investi-
gate is a novel way to introduce learning into the
initial phrase extraction process, which has previ-
ously been entirely heuristic.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977952380953">
The generative, noisy-channel paradigm has his-
torically served as the foundation for most of the
work in statistical machine translation (Brown et
al., 1994). At the same time, discriminative meth-
ods have provided substantial improvements over
generative models on a wide range of NLP tasks.
They allow one to easily encode domain knowl-
edge in the form of features. Moreover, param-
eters are tuned to directly minimize error rather
than to maximize joint likelihood, which may not
correspond well to the task objective.
In this paper, we present an end-to-end dis-
criminative approach to machine translation. The
proposed system is phrase-based, as in Koehn et
al. (2003), but uses an online perceptron training
scheme to learn model parameters. Unlike mini-
mum error rate training (Och, 2003), our system is
able to exploit large numbers of specific features
in the same manner as static reranking systems
(Shen et al., 2004; Och et al., 2004). However,
unlike static rerankers, our system does not rely
on a baseline translation system. Instead, it up-
dates based on its own n-best lists. As parameter
estimates improve, the system produces better n-
best lists, which can in turn enable better updates
in future training iterations. In this paper, we fo-
cus on two aspects of the problem of discrimina-
tive translation: the inherent difficulty of learning
from reference translations, and the challenge of
engineering effective features for this task.
Discriminative learning from reference transla-
tions is inherently problematic because standard
discriminative methods need to know which out-
puts are correct and which are not. However, a
proposed translation that differs from a reference
translation need not be incorrect. It may differ
in word choice, literalness, or style, yet be fully
acceptable. Pushing our system to avoid such al-
ternate translations is undesirable. On the other
hand, even if a system produces a reference trans-
lation, it may do so by abusing the hidden struc-
ture (sentence segmentation and alignment). We
can therefore never be entirely sure whether or not
a proposed output is safe to update towards. We
discuss this issue in detail in Section 5, where we
show that conservative updates (which push the
system towards a local variant of the current pre-
diction) are more effective than more aggressive
updates (which try to directly update towards the
reference).
The second major contribution of this work is
an investigation of an array of features for our
model. We show how our features quantitatively
increase BLEU score, as well as how they qual-
itatively interact on specific examples. We first
consider learning weights for individual phrases
and part-of-speech patterns, showing gains from
each. We then present a novel way to parameter-
ize and introduce learning into the initial phrase
extraction process. In particular, we introduce
alignment constellation features, which allow us
to weight phrases based on the word alignment
pattern that led to their extraction. This kind of
</bodyText>
<page confidence="0.959828">
761
</page>
<note confidence="0.531979">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999942888888889">
feature provides a potential way to initially extract
phrases more aggressively and then later down-
weight undesirable patterns, essentially learning a
weighted extraction heuristic. Finally, we use POS
features to parameterize a distortion model in a
limited distortion decoder (Zens and Ney, 2004;
Tillmann and Zhang, 2005). We show that over-
all, BLEU score increases from 28.4 to 29.6 on
French-English.
</bodyText>
<sectionHeader confidence="0.996848" genericHeader="introduction">
2 Approach
</sectionHeader>
<subsectionHeader confidence="0.999354">
2.1 Translation as structured classification
</subsectionHeader>
<bodyText confidence="0.999820522727273">
Machine translation can be seen as a structured
classification task, in which the goal is to learn
a mapping from an input (French) sentence x to
an output (English) sentence y. Given this setup,
discriminative methods allow us to define a broad
class of features 4b that operate on (x, y). For ex-
ample, some features would measure the fluency
of y and others would measure the faithfulness of
y as a translation of x.
However, the translation task in this framework
differs from traditional applications of discrimina-
tive structured classification such as POS tagging
and parsing in a fundamental way. Whereas in
POS tagging, there is a one-to-one correspondence
between the words x and the tags y, the correspon-
dence between x and y in machine translation is
not only much more complex, but is in fact un-
known. Therefore, we introduce a hidden corre-
spondence structure h and work with the feature
vector 4b(x, y, h).
The phrase-based model of Koehn et al. (2003)
is an instance of this framework. In their model,
the correspondence h consists of (1) the segmen-
tation of the input sentence into phrases, (2) the
segmentation of the output sentence into the same
number of phrases, and (3) a bijection between
the input and output phrases. The feature vec-
tor 4b(x, y, h) contains four components: the log
probability of the output sentence y under a lan-
guage model, the score of translating x into y
based on a phrase table, a distortion score, and a
length penalty.1 In Section 6, we vastly increase
the number of features to take advantage of the full
power of discriminative training.
Another example of this framework is the hier-
archical model of Chiang (2005). In this model
the correspondence h is a synchronous parse tree
over input and output sentences, and features in-
clude the scores of various productions used in the
tree.
Given features 4b and a corresponding set of pa-
rameters w, a standard classification rule f is to
return the highest scoring output sentence y, max-
imizing over correspondences h:
</bodyText>
<equation confidence="0.940188">
f(x; w) = argmax w · 4b(x, y, h). (1)
y,h
</equation>
<bodyText confidence="0.998643333333333">
In the phrase-based model, computing the
argmax exactly is intractable, so we approximate
f with beam decoding.
</bodyText>
<subsectionHeader confidence="0.994067">
2.2 Perceptron-based training
</subsectionHeader>
<bodyText confidence="0.999460428571428">
To tune the parameters w of the model, we use the
averaged perceptron algorithm (Collins, 2002) be-
cause of its efficiency and past success on various
NLP tasks (Collins and Roark, 2004; Roark et al.,
2004). In principle, w could have been tuned by
maximizing conditional probability or maximiz-
ing margin. However, these two options require
either marginalization or numerical optimization,
neither of which is tractable over the space of out-
put sentences y and correspondences h. In con-
trast, the perceptron algorithm requires only a de-
coder that computes f(x; w).
Recall the traditional perceptron update rule on
an example (xi, yi) is
</bodyText>
<equation confidence="0.972818">
w ← w + 4b(xi, yt) − 4b(xi, yp), (2)
</equation>
<bodyText confidence="0.9994446">
where yt = yi is the target output and yp =
f(xi; w) = argmaxy w · 4b(xi, y) is the predic-
tion using the current parameters w.
We adapt this update rule to work with hidden
variables as follows:
</bodyText>
<equation confidence="0.978949">
w ← w + 4b(xi, yt, ht) − 4b(xi, yp, hp), (3)
</equation>
<bodyText confidence="0.9856265">
where (yp, hp) is the argmax computation in
Equation 1, and (yt, ht) is the target that we up-
date towards. If (yt, ht) is the same argmax com-
putation with the additional constraint that yt =
yi, then Equation 3 can be interpreted as a Viterbi
approximation to the stochastic gradient
EP(h|xi,yi;w)4b(xi, yi, h)−EP(y,h|xi;w)4b(xi, y, h)
for the following conditional likelihood objective:
</bodyText>
<footnote confidence="0.940609333333333">
1More components can be added to the feature vector if � exp(w · 4b(xi, yi, h)).
additional language models or phrase tables are available. P(yi  |xi) ∝
h
</footnote>
<page confidence="0.964929">
762
</page>
<figureCaption confidence="0.951381">
Figure 1: Given the current prediction (a), there
</figureCaption>
<bodyText confidence="0.967092866666667">
are two possible updates, local (b) and bold (c).
Although the bold update (c) reaches the reference
translation, a bad correspondence is used. The lo-
cal update (b) does not reach the reference, but is
more reasonable than (c).
Discriminative training with hidden variables
has been handled in this probabilistic framework
(Quattoni et al., 2004; Koo and Collins, 2005), but
we choose Equation 3 for efficiency.
It turns out that using the Viterbi approximation
(which we call bold updating) is not always the
best strategy. To appreciate the difficulty, consider
the example in Figure 1. Suppose we make the
prediction (a) with the current set of parameters.
There are often several acceptable output transla-
tions y, for example, (b) and (c). Since (c)’s output
matches the reference translation, should we up-
date towards (c)? In this case, the answer is nega-
tive. The problem with (c) is that the correspon-
dence h contains an incorrect alignment (’, a).
However, since h is unobserved, the training pro-
cedure has no way of knowing this. While the out-
put in (b) is farther from the reference, its corre-
spondence h is much more reasonable. In short,
it does not suffice for yt to be good; both yt and
ht need to be good. A major challenge in using
the perceptron algorithm for machine translation
is determining the target (yt, ht) in Equation 3.
Section 5 discusses possible targets to update to-
wards.
</bodyText>
<sectionHeader confidence="0.998657" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.9988795">
Our experiments were done on the French-English
portion of the Europarl corpus (Koehn, 2002),
</bodyText>
<table confidence="0.995507">
Dataset TRAIN DEV TEST
Years ’99–’01 ’02 ’03
# sentences 67K first 1K first 1K
# words (unk.) 715K 10.4K (35) 10.8K (48)
</table>
<tableCaption confidence="0.996605">
Table 1: The Europarl dataset split we used and
</tableCaption>
<bodyText confidence="0.993958944444444">
various statistics on length 5–15 sentences. The
number of French word tokens is given, along
with the number that were not seen among the
414K total sentences in TRAIN (which includes all
lengths).
which consists of European parliamentary pro-
ceedings from 1996 to 2003.
We split the data into three sets according to
Table 1. TRAIN served two purposes: it was
used to construct the features, and the 5–15 length
sentences were used for tuning the parameters of
those features. DEV, which consisted of the first
1K length 5–15 sentences in 2002, was used to
evaluate the performance of the system as we de-
veloped it. Note that the DEV set was not used to
tune any parameters; tuning was done exclusively
on TRAIN. At the end we ran our models once on
TEST to get final numbers.2
</bodyText>
<sectionHeader confidence="0.993501" genericHeader="method">
4 Models
</sectionHeader>
<bodyText confidence="0.999294047619047">
Our experiments used phrase-based models
(Koehn et al., 2003), which require a translation
table and language model for decoding and
feature computation. To facilitate comparison
with previous work, we created the translation
tables using the same techniques as Koehn et al.
(2003).3 The language model was a Kneser-Ney
interpolated trigram model generated using the
SRILM toolkit (Stolcke, 2002). We built our
own phrase-based beam decoder that can handle
arbitrary features.4 The contributions of features
are incrementally added into the score as decoding
2We also experimented with several combinations ofjack-
knifing to prevent overfitting, in which we selected features
on TRAIN-OLD (1996–1998 Europarl corpus) and tuned the
parameters on TRAIN, or vice-versa. However, it turned out
that using TRAIN-OLD was suboptimal since that data is less
relevant to DEV. Another alternative is to combine TRAIN-
OLD and TRAIN into one dual-purpose dataset. The differ-
ences between this and our current approach were inconclu-
sive.
</bodyText>
<footnote confidence="0.664071857142857">
3In other words, we used GIZA++ to construct a word
alignment in each direction and a growth heuristic to com-
bine them. We extracted all the substrings that are closed un-
der this high-quality word alignment and computed surface
statistics from cooccurrences counts.
4In our experiments, we used a beam size of 10, which we
found to be only slightly worse than using a beam of 100.
</footnote>
<page confidence="0.99689">
763
</page>
<bodyText confidence="0.99974035">
proceeds.
We experimented with two levels of distortion:
monotonic, where the phrasal alignment is mono-
tonic (but word reordering is still possible within
a phrase) and limited distortion, where only ad-
jacent phrases are allowed to exchange positions
(Zens and Ney, 2004). In the future, we plan to ex-
plore our discriminative framework on a full dis-
tortion model (Koehn et al., 2003) or even a hier-
archical model (Chiang, 2005).
Throughout the following experiments, we
trained the perceptron algorithm for 10 iterations.
The weights were initialized to 1 on the trans-
lation table, 1 on the language model (the blan-
ket features in Section 6), and 0 elsewhere. The
next two sections give experiments on the two key
components of a discriminative machine transla-
tion system: choosing the proper update strategy
(Section 5) and including powerful features (Sec-
tion 6).
</bodyText>
<sectionHeader confidence="0.86642" genericHeader="method">
5 Update strategies
</sectionHeader>
<bodyText confidence="0.999428214285714">
This section describes the importance of choosing
a good update strategy—the difference in BLEU
score can be as large as 1.2 between different
strategies. An update strategy specifies the target
(yt, ht) that we update towards (Equation 3) given
the current set of parameters and a provided ref-
erence translation (xi, yi). As mentioned in Sec-
tion 2.2, faithful output (i.e. yt = yi) does not
imply that updating towards (yt, ht) is desirable.
In fact, such a constrained target might not even
be reachable by the decoder, for example, if the
reference is very non-literal.
We explored the following three ways to choose
the target (yt, ht):
</bodyText>
<listItem confidence="0.993282">
• Bold updating: Update towards the highest
scoring option (y, h), where y is constrained
to be the reference yi but h is unconstrained.
Examples not reachable by the decoder are
skipped.
• Local updating: Generate an n-best list using
the current parameters. Update towards the
option with the highest BLEU score.5
</listItem>
<footnote confidence="0.9805265">
5Since BLEU score (k-BLEU with k = 4) involves com-
puting a geometric mean over i-grams, i = 1, ... , k, it is zero
if the translation does not have at least one k-gram in common
with the reference translation. Since a BLEU score of zero
is both unhelpful for choosing from the n-best and common
when computed on just a single example, we instead used a
smoothed version for choosing the target: Pi i-24−i+,y)
We still report NIST’s usual 4-gram BLEU.
</footnote>
<figureCaption confidence="0.925114">
Figure 2: The three update strategies under two
scenarios.
</figureCaption>
<listItem confidence="0.937558666666667">
• Hybrid updating: Do a bold update if the ref-
erence is reachable. Otherwise, do a local up-
date.
</listItem>
<bodyText confidence="0.998842827586207">
Figure 2 shows the space of translations
schematically. On each training example, our de-
coder produces an n-best list. The reference trans-
lation may or may not be reachable.
Bold updating most resembles the traditional
perceptron update rule (Equation 2). We are en-
sured that the target output y will be correct, al-
though the correspondence h might be bad. An-
other weakness of bold updating is that we might
not make full use of the training data.
Local updating uses every example, but its steps
are more cautious. It can be viewed as “dy-
namic reranking,” where parameters are updated
using the best option on the n-best list, similar
to standard static reranking. The key difference
is that, unlike static reranking, the parameter up-
dates propagate back to the baseline classifier, so
that the n-best list improves over time. In this re-
gard, dynamic reranking remedies one of the main
weaknesses of static reranking, which is that the
performance of the system is directly limited by
the quality of the baseline classifier.
Hybrid updating combines the two strategies:
it makes full use of the training data as in local
updating, but still tries to make swift progress to-
wards the reference translation as in bold updat-
ing.
We conducted experiments to see which of the
updating strategies worked best. We trained on
</bodyText>
<page confidence="0.981778">
764
</page>
<table confidence="0.999896">
Decoder Bold Local Hybrid
Monotonic 34.3 34.6 34.5
Limited distortion 33.5 34.7 33.6
</table>
<tableCaption confidence="0.996309">
Table 2: Comparison of BLEU scores between dif-
</tableCaption>
<bodyText confidence="0.999079772727273">
ferent updating strategies for the monotonic and
limited distortion decoders on DEV.
5000 of the 67K available examples, using the
BLANKET+LEX+POS feature set (Section 6). Ta-
ble 2 shows that local updating is the most effec-
tive, especially when using the limited distortion
decoder.
In bold updating, only a small fraction of the
5000 examples (1296 for the monotonic decoder
and 1601 for the limited distortion decoder) had
reachable reference translations, and, therefore,
contributed to parameter updates. One might
therefore hypothesize that local updating performs
better simply because it is able to leverage more
data. This is not the full story, however, since the
hybrid approach (which makes the same number
of updates) performs significantly worse than lo-
cal updating when using the limited distortion de-
coder.
To see the problem with bold updating, recall
the example in Figure 1. Bold updating tries to
reach the reference at all costs, even if it means
abusing the hidden correspondence in the process.
In the example, the alignment (’, a) is unreason-
able, but the algorithm has no way to recognize
this. Local updating is much more stable since it
only updates towards sentences in the n-best list.
When using the limited distortion decoder, bold
updating is even more problematic because the
added flexibility of phrase swaps allows more pre-
posterous alignments to be produced. Limited
distortion decoding actually performs worse than
monotonic decoding with bold updating, but bet-
ter with local updating.
Another difference between bold updating and
local updating is that the BLEU score on the train-
ing data is dramatically higher for bold updating
than for local (or hybrid) updating: 80 for the for-
mer versus 40 for the latter. This is not surprising
given that bold updating aggressively tries to ob-
tain the references. However, what is surprising is
that although bold updating appears to be overfit-
ting severely, its BLEU score on the DEV does not
suffer much in the monotonic case.
</bodyText>
<table confidence="0.998364444444444">
Model DEV BLEU TEST BLEU
Monotonic
BLANKET (untuned) 33.0 28.3
BLANKET 33.4 28.4
BLANKET+LEX 35.0 29.2
BLANKET+LEX+POS 35.3 29.6
Pharaoh (MERT) 34.5 28.8
Full-distortion
Pharaoh (MERT) 34.9 29.5
</table>
<tableCaption confidence="0.991203">
Table 3: Main results on our system with differ-
ent feature sets compared to minimum error-rate
trained Pharaoh.
</tableCaption>
<sectionHeader confidence="0.997758" genericHeader="method">
6 Features
</sectionHeader>
<bodyText confidence="0.999845307692308">
This section shows that by adding an array of
expressive features and discriminatively learn-
ing their weights, we can obtain a 2.3 increase
in BLEU score on DEV. We add these fea-
tures incrementally, first tuning blanket features
(Section 6.1), then adding lexical features (Sec-
tion 6.2), and finally adding part-of-speech (POS)
features (Section 6.3). Table 3 summarizes the
performance gains.
For the experiments in this section, we used the
local updating strategy and the monotonic decoder
for efficiency. We train on all 67K of the length 5–
15 sentences in TRAIN.6
</bodyText>
<subsectionHeader confidence="0.996792">
6.1 Blanket features
</subsectionHeader>
<bodyText confidence="0.999982222222222">
The blanket features (BLANKET) consist of the
translation log-probability and the language model
log-probability, which are two of the components
of the Pharaoh model (Section 2.1). After discrim-
inative training, the relative weight of these two
features is roughly 2:1, resulting in a BLEU score
increase from 33.0 (setting both weights to 1) to
33.4.
The following simple example gives a flavor
of the discriminative approach. The untuned
system translated the French phrase trente-cinq
langues into five languages in a DEV example.
Although the probability P(five I trente-cinq) =
0.065 is rightly much smaller than P(thirty-five
trente-cinq) = 0.279, the language model favors
five languages over thirty-five languages. The
trained system downweights the language model
and recovers the correct translation.
</bodyText>
<footnote confidence="0.9576695">
6We used sentences of length 5–15 to facilitate compar-
isons with Koehn et al. (2003) and to enable rapid experimen-
tation with various feature sets. Experiments on sentences of
length 5–50 showed similar gains in performance.
</footnote>
<page confidence="0.9951">
765
</page>
<subsectionHeader confidence="0.997116">
6.2 Lexical features
</subsectionHeader>
<bodyText confidence="0.998609157894737">
The blanket features provide a rough guide for
translation, but they are far too coarse to fix spe-
cific mistakes. We therefore add lexical fea-
tures (LEX) to allow for more fine-grained con-
trol. These features come in two varieties. Lexical
phrase features indicate the presence of a specific
translation phrase, such as (y a-t-il, are there), and
lexical language model features indicate the pres-
ence of a specific output n-gram, such as of the.
Lexical language model features have been ex-
ploited successfully in discriminative language
modeling to improve speech recognition perfor-
mance (Roark et al., 2004). We confirm the util-
ity of the two kinds of lexical features: BLAN-
KET+LEX achieves a BLEU score of 35.0, an im-
provement of 1.6 over BLANKET.
To understand the effect of adding lexical fea-
tures, consider the ten with highest and lowest
weights after training:
</bodyText>
<figure confidence="0.825432888888889">
64 any comments ? -55 (des, of)
-52 (y a-t-il, are there any)
57 any comments
46 (des, any) -38 of comments ?
These features can in fact be traced back to the
following example:
Input y a-t-il des observations ?
B are there any of comments ?
B+L are there any comments ?
</figure>
<bodyText confidence="0.99946895">
The second and third rows are the outputs of
BLANKET (wrong) and BLANKET+LEX (correct),
respectively. The correction can be accredited to
two changes in feature weights. First, the lexical
feature (y a-t-il, are there any) has been assigned
a negative weight and (y a-t-il, are there) a pos-
itive weight to counter the fact that the former
phrase incorrectly had a higher score in the origi-
nal translation table. Second, (des, of) is preferred
over (des, any), even though the former is a better
translation in isolation. This apparent degradation
causes no problems, because when des should ac-
tually be translated to of, these words are usually
embedded in larger phrases, in which case the iso-
lated translation probability plays no role.
Another example of a related phenomenon is
the following:
Input ... pour cela que j ’ ai vot´e favorablement .
B ... for that i have voted in favour .
B+L ... for this reason i voted in favour .
Counterintuitively, the phrase pair
(j ’ ai, I have) ends up with a very negative
weight. The reason behind this is that in French,
j ’ ai is often used in a paraphrastic construction
which should be translated into the simple past
in English. For that to happen, j ’ ai needs to
be aligned with I. Since (j ’ ai, I) has a small
score compare to (j ’ ai, I have) in the original
translation table, downweighting the latter pair
allows this sentence to be translated correctly.
A general trend is that literal phrase translations
are downweighted. Lessening the pressure to liter-
ally translate certain phrases allows the language
model to fill in the gaps appropriately with suit-
able non-literal translations. This point highlights
the strength of discriminative training: weights are
jointly tuned to account for the intricate interac-
tions between overlapping phrases, which is some-
thing not achievable by estimating the weights di-
rectly from surface statistics.
</bodyText>
<subsectionHeader confidence="0.995607">
6.3 Part-of-speech features
</subsectionHeader>
<bodyText confidence="0.999953466666667">
While lexical features are useful for eliminating
specific errors, they have limited ability to gener-
alize to related phrases. This suggests the use of
similar features which are abstracted to the POS
level.7 In our experiments, we used the TreeTag-
ger POS tagger (Schmid, 1994), which ships pre-
trained on several languages, to map each word
to its majority POS tag. We could also relatively
easily base our features on context-dependent POS
tags: the entire input sentence is available before
decoding begins, and the output sentence is de-
coded left-to-right and could be tagged incremen-
tally.
Where we had lexical phrase features, such
as (la realisation du droit, the right), we now
also have their POS abstractions, for instance
(DT NN IN NN, DT NN). This phrase pair is
undesirable, not because of particular lexical facts
about la realisation, but because dropping a nom-
inal head is generally to be avoided. The lexical
language model features have similar POS coun-
terparts. With these two kinds of POS features,
we obtained an 0.3 increase in BLEU score from
BLANKET+LEX to BLANKET+LEX+POS.
Finally, when we use the limited distortion de-
coder, it is important to learn when to swap adja-
cent phrases. Unlike Pharaoh, which simply has a
uniform penalty for swaps, we would like to use
context—in particular, POS information. For ex-
ample, we would like to know that if a (JJ, JJ)
</bodyText>
<footnote confidence="0.977853">
7We also tried using word clusters (Brown et al., 1992)
instead of POS but found that POS was more helpful.
</footnote>
<figure confidence="0.97222716">
63 (y a-t-il, are there)
62 there any comments
-42 there any of
-39 of comments
766
abri
sˆur
croissance
z´ero
,
ce
mˆeme
secure
refuge
,
that
same
zero
growth
rate
Features -CONST +CONST
BLANKET 31.8 32.2
BLANKET+LEX 32.2 32.5
BLANKET+LEX+POS 32.3 32.5
(a) (b) (c)
</figure>
<figureCaption confidence="0.6220575">
Figure 3: Three constellation features with exam-
ple phrase pairs. Constellations (a) and (b) have
large positive weights and (c) has a large negative
weight.
</figureCaption>
<bodyText confidence="0.99963575">
phrase is constructed after a (NN, NN) phrase,
they are reasonable candidates for swapping be-
cause of regular word-order differences between
French and English. While the bulk of our results
are presented for the monotonic case, the limited
distortion results of Table 2 use these lexical swap
features; without parameterized swap features, ac-
curacy was below the untuned monotonic baseline.
An interesting statistic is the number of nonzero
feature weights that were learned using each
feature set. BLANKET has only 4 features,
while BLANKET+LEX has 1.55 million features.8
Remarkably, BLANKET+LEX+POS has fewer
features—only 1.24 million. This is an effect
of generalization ability—POS information some-
what reduces the need for specific lexical features.
</bodyText>
<subsectionHeader confidence="0.995011">
6.4 Alignment constellation features
</subsectionHeader>
<bodyText confidence="0.988515136363636">
Koehn et al. (2003) demonstrated that choosing
the appropriate heuristic for extracting phrases is
very important. They showed that the difference
in BLEU score between various heuristics was as
large as 2.0.
The process of phrase extraction is difficult to
optimize in a non-discriminative setting: many
heuristics have been proposed (Koehn et al.,
2003), but it is not obvious which one should be
chosen for a given language pair. We propose a
natural way to handle this part of the translation
pipeline. The idea is to push the learning process
all the way down to the phrase extraction by pa-
rameterizing the phrase extraction heuristic itself.
The heuristics in Koehn et al. (2003) decide
whether to extract a given phrase pair based on the
underlying word alignments (see Figure 3 for three
examples), which we call constellations. Since we
do not know which constellations correspond to
8Both the language model and translation table compo-
nents have two features, one for known words and one for
unknown words.
</bodyText>
<tableCaption confidence="0.851641">
Table 4: DEV BLEU score increase resulting from
adding constellation features.
</tableCaption>
<bodyText confidence="0.989641454545454">
good phrase pairs, we introduce an alignment con-
stellation feature to indicate the presence of a par-
ticular alignment constellation.9
Table 4 details the effect of adding constella-
tion features on top of our previous feature sets.10
We get a minor increase in BLEU score from each
feature set, although there is no gain by adding
POS features in addition to constellation features,
probably because POS and constellation features
provide redundant information for French-English
translations.
It is interesting to look at the constellations with
highest and lowest weights, which are perhaps sur-
prising at first glance. At the top of the list are
word inversions (Figure 3 (a) and (b)), while long
monotonic constellations fall at the bottom of the
list (c). Although monotonic translations are much
more frequent than word inversions in our dataset,
when translations are monotonic, shorter segmen-
tations are preferred. This phenomenon is another
manifestation of the complex interaction of phrase
segmentations.
</bodyText>
<sectionHeader confidence="0.995214" genericHeader="method">
7 Final results
</sectionHeader>
<bodyText confidence="0.999647">
The last column of Table 3 shows the performance
of our methods on the final TEST set. Our best test
BLEU score is 29.6 using BLANKET+LEX+POS,
an increase of 1.3 BLEU over our untuned feature
set BLANKET. The discrepancy between DEV per-
formance and TEST performance is due to tem-
poral distance from TRAIN and high variance in
BLEU score.11
We also compared our model with Pharaoh
(Koehn et al., 2003). We tuned Pharaoh’s four pa-
rameters using minimum error rate training (Och,
2003) on DEV.12 We obtained an increase of 0.8
</bodyText>
<footnote confidence="0.998946888888889">
9As in the POS features, we map each phrase pair to its
majority constellation.
10Due to time constraints, we ran these experiments on
5000 training examples using bold updating.
11For example, the DEV BLEU score for BLANKET+LEX
ranges from 28.6 to 33.2, depending on which block of 1000
sentences we chose.
12We used the training scripts from the 2006 MT Shared
Task. We still tuned our model parameters on TRAIN and
</footnote>
<page confidence="0.992377">
767
</page>
<bodyText confidence="0.9972578">
BLEU over the Pharaoh, run with the monotone
flag.13 Even though we are using a monotonic de-
coder, our best results are still slightly better than
the version of Pharaoh that permits arbitrary dis-
tortion.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="method">
8 Related work
</sectionHeader>
<bodyText confidence="0.999990928571428">
In machine translation, most discriminative ap-
proaches currently fall into two general categories.
The first approach is to reuse the components of a
generative model, but tune their relative weights in
a discriminative fashion (Och and Ney, 2002; Och,
2003; Chiang, 2005). This approach only works in
practice with a small handful of parameters.
The second approach is to use reranking, in
which a baseline classifier generates an n-best list
of candidate translations, and a separate discrim-
inative classifier chooses amongst them (Shen et
al., 2004; Och et al., 2004). The major limita-
tion of a reranking system is its dependence on
the underlying baseline system, which bounds the
potential improvement from discriminative train-
ing. In machine translation, this limitation is a
real concern; it is common for all translations on
moderately-sized n-best lists to be of poor qual-
ity. For instance, Och et al. (2004) reported that
a 1000-best list was required to achieve perfor-
mance gains from reranking. In contrast, the de-
coder in our system can use the feature weights
learned in the previous iteration.
Tillmann and Zhang (2005) present a discrim-
inative approach based on local models. Their
formulation explicitly decomposed the score of
a translation into a sequence of local decisions,
while our formulation allows global estimation.
</bodyText>
<sectionHeader confidence="0.997704" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999474636363636">
We have presented a novel end-to-end discrimi-
native system for machine translation. We stud-
ied update strategies, an important issue in on-
line discriminative training for MT, and conclude
that making many smaller (conservative) updates
is better than making few large (aggressive) up-
dates. We also investigated the effect of adding
many expressive features, which yielded a 0.8 in-
crease in BLEU score over monotonic Pharaoh.
Acknowledgments We would like to thank our
reviewers for their comments. This work was sup-
</bodyText>
<footnote confidence="0.793744333333333">
only used DEV to optimize the number of training iterations.
13This result is significant with P-value 0.0585 based on
approximate randomization (Riezler and Maxwell, 2005).
</footnote>
<bodyText confidence="0.985683666666667">
ported by a FQRNT fellowship to second author
and a Microsoft Research New Faculty Fellowship
to the third author.
</bodyText>
<sectionHeader confidence="0.995558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999868611111111">
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-Based
n-gram Models of Natural Language. Computational Lin-
guistics, 18(4):467–479.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The Mathematics
of Statistical Machine Translation: Parameter Estimation.
Computational Linguistics, 19:263–311.
David Chiang. 2005. A Hierarchical Phrase-Based Model for
Statistical Machine Translation. In ACL 2005.
Michael Collins and Brian Roark. 2004. Incremental Parsing
with the Perceptron Algorithm. In ACL 2004.
Michael Collins. 2002. Discriminative Training Methods for
Hidden Markov Models: Theory and Experiments with
Perceptron Algorithms. In EMNLP 2002.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In HLT-NAACL
2003.
Philipp Koehn. 2002. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation.
Terry Koo and Michael Collins. 2005. Hidden-Variable Mod-
els for Discriminative Reranking. In EMNLP 2005.
Franz Josef Och and Hermann Ney. 2002. Discriminative
Training and Maximum Entropy Models for Statistical
Machine Translation. In ACL 2002.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, and
Anoop Sarkar. 2004. A Smorgasbord of Features for Sta-
tistical Machine Translation. In HLT-NAACL 2004.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In ACL 2003.
Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2004.
Conditional Random Fields for Object Recognition. In
NIPS 2004.
Stefan Riezler and John T. Maxwell. 2005. On Some Pit-
falls in Automatic Evaluation and Significance Testing for
MT. In Workshop on Intrinsic and Extrinsic Evaluation
Methods for MT and Summarization (MTSE).
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative Language Modeling with
Conditional Random Fields and the Perceptron Algorithm.
In ACL 2004.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Conference
on New Methods in Language Processing.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Dis-
criminative Reranking for Machine Translation. In HLT-
NAACL 2004.
Andreas Stolcke. 2002. SRILM An Extensible Language
Modeling Toolkit. In ICSLP 2002.
Christoph Tillmann and Tong Zhang. 2005. A Localized
Prediction Model for Statistical Machine Translation. In
ACL 2005.
Richard Zens and Hermann Ney. 2004. Improvements in Sta-
tistical Phrase-Based Translation. In HLT-NAACL 2004.
</reference>
<page confidence="0.996275">
768
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.474915">
<title confidence="0.999969">An End-to-End Discriminative Approach to Machine Translation</title>
<author confidence="0.999994">Percy Liang Alexandre Bouchard-Cˆot´e Dan Klein Ben Taskar</author>
<affiliation confidence="0.9998975">Computer Science Division, EECS Department University of California at Berkeley</affiliation>
<address confidence="0.999817">Berkeley, CA 94720</address>
<email confidence="0.502379">bouchard,klein,</email>
<abstract confidence="0.996927611111111">We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited. Unlike discriminative reranking approaches, our system can take advantage of learned features in all stages of decoding. We first discuss several challenges to error-driven discriminative approaches. In particular, we explore different ways of updating parameters given a training example. We find that making frequent but smaller updates is preferable to making fewer but larger updates. Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples. One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-Based n-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="24761" citStr="Brown et al., 1992" startWordPosition="4082" endWordPosition="4085">l facts about la realisation, but because dropping a nominal head is generally to be avoided. The lexical language model features have similar POS counterparts. With these two kinds of POS features, we obtained an 0.3 increase in BLEU score from BLANKET+LEX to BLANKET+LEX+POS. Finally, when we use the limited distortion decoder, it is important to learn when to swap adjacent phrases. Unlike Pharaoh, which simply has a uniform penalty for swaps, we would like to use context—in particular, POS information. For example, we would like to know that if a (JJ, JJ) 7We also tried using word clusters (Brown et al., 1992) instead of POS but found that POS was more helpful. 63 (y a-t-il, are there) 62 there any comments -42 there any of -39 of comments 766 abri sˆur croissance z´ero , ce mˆeme secure refuge , that same zero growth rate Features -CONST +CONST BLANKET 31.8 32.2 BLANKET+LEX 32.2 32.5 BLANKET+LEX+POS 32.3 32.5 (a) (b) (c) Figure 3: Three constellation features with example phrase pairs. Constellations (a) and (b) have large positive weights and (c) has a large negative weight. phrase is constructed after a (NN, NN) phrase, they are reasonable candidates for swapping because of regular word-order di</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-Based n-gram Models of Natural Language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="1281" citStr="Brown et al., 1994" startWordPosition="183" endWordPosition="186">rs given a training example. We find that making frequent but smaller updates is preferable to making fewer but larger updates. Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples. One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic. 1 Introduction The generative, noisy-channel paradigm has historically served as the foundation for most of the work in statistical machine translation (Brown et al., 1994). At the same time, discriminative methods have provided substantial improvements over generative models on a wide range of NLP tasks. They allow one to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimu</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1994</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1994. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="6462" citStr="Chiang (2005)" startWordPosition="1026" endWordPosition="1027"> the segmentation of the input sentence into phrases, (2) the segmentation of the output sentence into the same number of phrases, and (3) a bijection between the input and output phrases. The feature vector 4b(x, y, h) contains four components: the log probability of the output sentence y under a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training. Another example of this framework is the hierarchical model of Chiang (2005). In this model the correspondence h is a synchronous parse tree over input and output sentences, and features include the scores of various productions used in the tree. Given features 4b and a corresponding set of parameters w, a standard classification rule f is to return the highest scoring output sentence y, maximizing over correspondences h: f(x; w) = argmax w · 4b(x, y, h). (1) y,h In the phrase-based model, computing the argmax exactly is intractable, so we approximate f with beam decoding. 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged percept</context>
<context position="12805" citStr="Chiang, 2005" startWordPosition="2102" endWordPosition="2103">ignment and computed surface statistics from cooccurrences counts. 4In our experiments, we used a beam size of 10, which we found to be only slightly worse than using a beam of 100. 763 proceeds. We experimented with two levels of distortion: monotonic, where the phrasal alignment is monotonic (but word reordering is still possible within a phrase) and limited distortion, where only adjacent phrases are allowed to exchange positions (Zens and Ney, 2004). In the future, we plan to explore our discriminative framework on a full distortion model (Koehn et al., 2003) or even a hierarchical model (Chiang, 2005). Throughout the following experiments, we trained the perceptron algorithm for 10 iterations. The weights were initialized to 1 on the translation table, 1 on the language model (the blanket features in Section 6), and 0 elsewhere. The next two sections give experiments on the two key components of a discriminative machine translation system: choosing the proper update strategy (Section 5) and including powerful features (Section 6). 5 Update strategies This section describes the importance of choosing a good update strategy—the difference in BLEU score can be as large as 1.2 between differen</context>
<context position="29597" citStr="Chiang, 2005" startWordPosition="4863" endWordPosition="4864">chose. 12We used the training scripts from the 2006 MT Shared Task. We still tuned our model parameters on TRAIN and 767 BLEU over the Pharaoh, run with the monotone flag.13 Even though we are using a monotonic decoder, our best results are still slightly better than the version of Pharaoh that permits arbitrary distortion. 8 Related work In machine translation, most discriminative approaches currently fall into two general categories. The first approach is to reuse the components of a generative model, but tune their relative weights in a discriminative fashion (Och and Ney, 2002; Och, 2003; Chiang, 2005). This approach only works in practice with a small handful of parameters. The second approach is to use reranking, in which a baseline classifier generates an n-best list of candidate translations, and a separate discriminative classifier chooses amongst them (Shen et al., 2004; Och et al., 2004). The major limitation of a reranking system is its dependence on the underlying baseline system, which bounds the potential improvement from discriminative training. In machine translation, this limitation is a real concern; it is common for all translations on moderately-sized n-best lists to be of </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental Parsing with the Perceptron Algorithm. In</title>
<date>2004</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="7180" citStr="Collins and Roark, 2004" startWordPosition="1146" endWordPosition="1149">s, and features include the scores of various productions used in the tree. Given features 4b and a corresponding set of parameters w, a standard classification rule f is to return the highest scoring output sentence y, maximizing over correspondences h: f(x; w) = argmax w · 4b(x, y, h). (1) y,h In the phrase-based model, computing the argmax exactly is intractable, so we approximate f with beam decoding. 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f(x; w). Recall the traditional perceptron update rule on an example (xi, yi) is w ← w + 4b(xi, yt) − 4b(xi, yp), (2) where yt = yi is the target output and yp = f(xi; w) = argmaxy w · 4b(xi, y) is the prediction using the curre</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental Parsing with the Perceptron Algorithm. In ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="7091" citStr="Collins, 2002" startWordPosition="1132" endWordPosition="1133"> the correspondence h is a synchronous parse tree over input and output sentences, and features include the scores of various productions used in the tree. Given features 4b and a corresponding set of parameters w, a standard classification rule f is to return the highest scoring output sentence y, maximizing over correspondences h: f(x; w) = argmax w · 4b(x, y, h). (1) y,h In the phrase-based model, computing the argmax exactly is intractable, so we approximate f with beam decoding. 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f(x; w). Recall the traditional perceptron update rule on an example (xi, yi) is w ← w + 4b(xi, yt) − 4b(xi, yp), (2) where yt = yi is the </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL</booktitle>
<contexts>
<context position="1793" citStr="Koehn et al. (2003)" startWordPosition="266" endWordPosition="269">cally served as the foundation for most of the work in statistical machine translation (Brown et al., 1994). At the same time, discriminative methods have provided substantial improvements over generative models on a wide range of NLP tasks. They allow one to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter estimates improve, the system produces better nbest lists, which can in turn enable better updates in future training iterations. In this paper, we focus on two aspects </context>
<context position="5762" citStr="Koehn et al. (2003)" startWordPosition="902" endWordPosition="905">f y and others would measure the faithfulness of y as a translation of x. However, the translation task in this framework differs from traditional applications of discriminative structured classification such as POS tagging and parsing in a fundamental way. Whereas in POS tagging, there is a one-to-one correspondence between the words x and the tags y, the correspondence between x and y in machine translation is not only much more complex, but is in fact unknown. Therefore, we introduce a hidden correspondence structure h and work with the feature vector 4b(x, y, h). The phrase-based model of Koehn et al. (2003) is an instance of this framework. In their model, the correspondence h consists of (1) the segmentation of the input sentence into phrases, (2) the segmentation of the output sentence into the same number of phrases, and (3) a bijection between the input and output phrases. The feature vector 4b(x, y, h) contains four components: the log probability of the output sentence y under a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of d</context>
<context position="11031" citStr="Koehn et al., 2003" startWordPosition="1817" endWordPosition="1820">iamentary proceedings from 1996 to 2003. We split the data into three sets according to Table 1. TRAIN served two purposes: it was used to construct the features, and the 5–15 length sentences were used for tuning the parameters of those features. DEV, which consisted of the first 1K length 5–15 sentences in 2002, was used to evaluate the performance of the system as we developed it. Note that the DEV set was not used to tune any parameters; tuning was done exclusively on TRAIN. At the end we ran our models once on TEST to get final numbers.2 4 Models Our experiments used phrase-based models (Koehn et al., 2003), which require a translation table and language model for decoding and feature computation. To facilitate comparison with previous work, we created the translation tables using the same techniques as Koehn et al. (2003).3 The language model was a Kneser-Ney interpolated trigram model generated using the SRILM toolkit (Stolcke, 2002). We built our own phrase-based beam decoder that can handle arbitrary features.4 The contributions of features are incrementally added into the score as decoding 2We also experimented with several combinations ofjackknifing to prevent overfitting, in which we sele</context>
<context position="12761" citStr="Koehn et al., 2003" startWordPosition="2092" endWordPosition="2095">gs that are closed under this high-quality word alignment and computed surface statistics from cooccurrences counts. 4In our experiments, we used a beam size of 10, which we found to be only slightly worse than using a beam of 100. 763 proceeds. We experimented with two levels of distortion: monotonic, where the phrasal alignment is monotonic (but word reordering is still possible within a phrase) and limited distortion, where only adjacent phrases are allowed to exchange positions (Zens and Ney, 2004). In the future, we plan to explore our discriminative framework on a full distortion model (Koehn et al., 2003) or even a hierarchical model (Chiang, 2005). Throughout the following experiments, we trained the perceptron algorithm for 10 iterations. The weights were initialized to 1 on the translation table, 1 on the language model (the blanket features in Section 6), and 0 elsewhere. The next two sections give experiments on the two key components of a discriminative machine translation system: choosing the proper update strategy (Section 5) and including powerful features (Section 6). 5 Update strategies This section describes the importance of choosing a good update strategy—the difference in BLEU s</context>
<context position="20092" citStr="Koehn et al. (2003)" startWordPosition="3292" endWordPosition="3295">ing in a BLEU score increase from 33.0 (setting both weights to 1) to 33.4. The following simple example gives a flavor of the discriminative approach. The untuned system translated the French phrase trente-cinq langues into five languages in a DEV example. Although the probability P(five I trente-cinq) = 0.065 is rightly much smaller than P(thirty-five trente-cinq) = 0.279, the language model favors five languages over thirty-five languages. The trained system downweights the language model and recovers the correct translation. 6We used sentences of length 5–15 to facilitate comparisons with Koehn et al. (2003) and to enable rapid experimentation with various feature sets. Experiments on sentences of length 5–50 showed similar gains in performance. 765 6.2 Lexical features The blanket features provide a rough guide for translation, but they are far too coarse to fix specific mistakes. We therefore add lexical features (LEX) to allow for more fine-grained control. These features come in two varieties. Lexical phrase features indicate the presence of a specific translation phrase, such as (y a-t-il, are there), and lexical language model features indicate the presence of a specific output n-gram, such</context>
<context position="26052" citStr="Koehn et al. (2003)" startWordPosition="4284" endWordPosition="4287">presented for the monotonic case, the limited distortion results of Table 2 use these lexical swap features; without parameterized swap features, accuracy was below the untuned monotonic baseline. An interesting statistic is the number of nonzero feature weights that were learned using each feature set. BLANKET has only 4 features, while BLANKET+LEX has 1.55 million features.8 Remarkably, BLANKET+LEX+POS has fewer features—only 1.24 million. This is an effect of generalization ability—POS information somewhat reduces the need for specific lexical features. 6.4 Alignment constellation features Koehn et al. (2003) demonstrated that choosing the appropriate heuristic for extracting phrases is very important. They showed that the difference in BLEU score between various heuristics was as large as 2.0. The process of phrase extraction is difficult to optimize in a non-discriminative setting: many heuristics have been proposed (Koehn et al., 2003), but it is not obvious which one should be chosen for a given language pair. We propose a natural way to handle this part of the translation pipeline. The idea is to push the learning process all the way down to the phrase extraction by parameterizing the phrase </context>
<context position="28559" citStr="Koehn et al., 2003" startWordPosition="4688" endWordPosition="4691">nt than word inversions in our dataset, when translations are monotonic, shorter segmentations are preferred. This phenomenon is another manifestation of the complex interaction of phrase segmentations. 7 Final results The last column of Table 3 shows the performance of our methods on the final TEST set. Our best test BLEU score is 29.6 using BLANKET+LEX+POS, an increase of 1.3 BLEU over our untuned feature set BLANKET. The discrepancy between DEV performance and TEST performance is due to temporal distance from TRAIN and high variance in BLEU score.11 We also compared our model with Pharaoh (Koehn et al., 2003). We tuned Pharaoh’s four parameters using minimum error rate training (Och, 2003) on DEV.12 We obtained an increase of 0.8 9As in the POS features, we map each phrase pair to its majority constellation. 10Due to time constraints, we ran these experiments on 5000 training examples using bold updating. 11For example, the DEV BLEU score for BLANKET+LEX ranges from 28.6 to 33.2, depending on which block of 1000 sentences we chose. 12We used the training scripts from the 2006 MT Shared Task. We still tuned our model parameters on TRAIN and 767 BLEU over the Pharaoh, run with the monotone flag.13 E</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Multilingual Corpus for Evaluation of Machine Translation.</title>
<date>2002</date>
<contexts>
<context position="10011" citStr="Koehn, 2002" startWordPosition="1638" endWordPosition="1639">t the correspondence h contains an incorrect alignment (’, a). However, since h is unobserved, the training procedure has no way of knowing this. While the output in (b) is farther from the reference, its correspondence h is much more reasonable. In short, it does not suffice for yt to be good; both yt and ht need to be good. A major challenge in using the perceptron algorithm for machine translation is determining the target (yt, ht) in Equation 3. Section 5 discusses possible targets to update towards. 3 Dataset Our experiments were done on the French-English portion of the Europarl corpus (Koehn, 2002), Dataset TRAIN DEV TEST Years ’99–’01 ’02 ’03 # sentences 67K first 1K first 1K # words (unk.) 715K 10.4K (35) 10.8K (48) Table 1: The Europarl dataset split we used and various statistics on length 5–15 sentences. The number of French word tokens is given, along with the number that were not seen among the 414K total sentences in TRAIN (which includes all lengths). which consists of European parliamentary proceedings from 1996 to 2003. We split the data into three sets according to Table 1. TRAIN served two purposes: it was used to construct the features, and the 5–15 length sentences were u</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. Europarl: A Multilingual Corpus for Evaluation of Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Hidden-Variable Models for Discriminative Reranking. In</title>
<date>2005</date>
<booktitle>EMNLP</booktitle>
<contexts>
<context position="8875" citStr="Koo and Collins, 2005" startWordPosition="1439" endWordPosition="1442"> following conditional likelihood objective: 1More components can be added to the feature vector if � exp(w · 4b(xi, yi, h)). additional language models or phrase tables are available. P(yi |xi) ∝ h 762 Figure 1: Given the current prediction (a), there are two possible updates, local (b) and bold (c). Although the bold update (c) reaches the reference translation, a bad correspondence is used. The local update (b) does not reach the reference, but is more reasonable than (c). Discriminative training with hidden variables has been handled in this probabilistic framework (Quattoni et al., 2004; Koo and Collins, 2005), but we choose Equation 3 for efficiency. It turns out that using the Viterbi approximation (which we call bold updating) is not always the best strategy. To appreciate the difficulty, consider the example in Figure 1. Suppose we make the prediction (a) with the current set of parameters. There are often several acceptable output translations y, for example, (b) and (c). Since (c)’s output matches the reference translation, should we update towards (c)? In this case, the answer is negative. The problem with (c) is that the correspondence h contains an incorrect alignment (’, a). However, sinc</context>
</contexts>
<marker>Koo, Collins, 2005</marker>
<rawString>Terry Koo and Michael Collins. 2005. Hidden-Variable Models for Discriminative Reranking. In EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="29571" citStr="Och and Ney, 2002" startWordPosition="4857" endWordPosition="4860">ch block of 1000 sentences we chose. 12We used the training scripts from the 2006 MT Shared Task. We still tuned our model parameters on TRAIN and 767 BLEU over the Pharaoh, run with the monotone flag.13 Even though we are using a monotonic decoder, our best results are still slightly better than the version of Pharaoh that permits arbitrary distortion. 8 Related work In machine translation, most discriminative approaches currently fall into two general categories. The first approach is to reuse the components of a generative model, but tune their relative weights in a discriminative fashion (Och and Ney, 2002; Och, 2003; Chiang, 2005). This approach only works in practice with a small handful of parameters. The second approach is to use reranking, in which a baseline classifier generates an n-best list of candidate translations, and a separate discriminative classifier chooses amongst them (Shen et al., 2004; Och et al., 2004). The major limitation of a reranking system is its dependence on the underlying baseline system, which bounds the potential improvement from discriminative training. In machine translation, this limitation is a real concern; it is common for all translations on moderately-si</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
<author>Sanjeev Khudanpur</author>
<author>Anoop Sarkar</author>
</authors>
<title>A Smorgasbord of Features for Statistical Machine Translation. In HLT-NAACL</title>
<date>2004</date>
<contexts>
<context position="2065" citStr="Och et al., 2004" startWordPosition="312" endWordPosition="315">domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter estimates improve, the system produces better nbest lists, which can in turn enable better updates in future training iterations. In this paper, we focus on two aspects of the problem of discriminative translation: the inherent difficulty of learning from reference translations, and the challenge of engineering effective features for this task. Discriminative learning from reference translations is inherently problematic because standard</context>
<context position="29895" citStr="Och et al., 2004" startWordPosition="4909" endWordPosition="4912">t permits arbitrary distortion. 8 Related work In machine translation, most discriminative approaches currently fall into two general categories. The first approach is to reuse the components of a generative model, but tune their relative weights in a discriminative fashion (Och and Ney, 2002; Och, 2003; Chiang, 2005). This approach only works in practice with a small handful of parameters. The second approach is to use reranking, in which a baseline classifier generates an n-best list of candidate translations, and a separate discriminative classifier chooses amongst them (Shen et al., 2004; Och et al., 2004). The major limitation of a reranking system is its dependence on the underlying baseline system, which bounds the potential improvement from discriminative training. In machine translation, this limitation is a real concern; it is common for all translations on moderately-sized n-best lists to be of poor quality. For instance, Och et al. (2004) reported that a 1000-best list was required to achieve performance gains from reranking. In contrast, the decoder in our system can use the feature weights learned in the previous iteration. Tillmann and Zhang (2005) present a discriminative approach b</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, and Anoop Sarkar. 2004. A Smorgasbord of Features for Statistical Machine Translation. In HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation. In</title>
<date>2003</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="1914" citStr="Och, 2003" startWordPosition="287" endWordPosition="288">criminative methods have provided substantial improvements over generative models on a wide range of NLP tasks. They allow one to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter estimates improve, the system produces better nbest lists, which can in turn enable better updates in future training iterations. In this paper, we focus on two aspects of the problem of discriminative translation: the inherent difficulty of learning from reference translations, and the ch</context>
<context position="28641" citStr="Och, 2003" startWordPosition="4703" endWordPosition="4704">ons are preferred. This phenomenon is another manifestation of the complex interaction of phrase segmentations. 7 Final results The last column of Table 3 shows the performance of our methods on the final TEST set. Our best test BLEU score is 29.6 using BLANKET+LEX+POS, an increase of 1.3 BLEU over our untuned feature set BLANKET. The discrepancy between DEV performance and TEST performance is due to temporal distance from TRAIN and high variance in BLEU score.11 We also compared our model with Pharaoh (Koehn et al., 2003). We tuned Pharaoh’s four parameters using minimum error rate training (Och, 2003) on DEV.12 We obtained an increase of 0.8 9As in the POS features, we map each phrase pair to its majority constellation. 10Due to time constraints, we ran these experiments on 5000 training examples using bold updating. 11For example, the DEV BLEU score for BLANKET+LEX ranges from 28.6 to 33.2, depending on which block of 1000 sentences we chose. 12We used the training scripts from the 2006 MT Shared Task. We still tuned our model parameters on TRAIN and 767 BLEU over the Pharaoh, run with the monotone flag.13 Even though we are using a monotonic decoder, our best results are still slightly b</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Michael Collins</author>
<author>Trevor Darrell</author>
</authors>
<title>Conditional Random Fields for Object Recognition. In</title>
<date>2004</date>
<booktitle>NIPS</booktitle>
<contexts>
<context position="8851" citStr="Quattoni et al., 2004" startWordPosition="1435" endWordPosition="1438">;w)4b(xi, y, h) for the following conditional likelihood objective: 1More components can be added to the feature vector if � exp(w · 4b(xi, yi, h)). additional language models or phrase tables are available. P(yi |xi) ∝ h 762 Figure 1: Given the current prediction (a), there are two possible updates, local (b) and bold (c). Although the bold update (c) reaches the reference translation, a bad correspondence is used. The local update (b) does not reach the reference, but is more reasonable than (c). Discriminative training with hidden variables has been handled in this probabilistic framework (Quattoni et al., 2004; Koo and Collins, 2005), but we choose Equation 3 for efficiency. It turns out that using the Viterbi approximation (which we call bold updating) is not always the best strategy. To appreciate the difficulty, consider the example in Figure 1. Suppose we make the prediction (a) with the current set of parameters. There are often several acceptable output translations y, for example, (b) and (c). Since (c)’s output matches the reference translation, should we update towards (c)? In this case, the answer is negative. The problem with (c) is that the correspondence h contains an incorrect alignme</context>
</contexts>
<marker>Quattoni, Collins, Darrell, 2004</marker>
<rawString>Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2004. Conditional Random Fields for Object Recognition. In NIPS 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On Some Pitfalls in Automatic Evaluation and Significance Testing for MT.</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Methods for MT and Summarization (MTSE).</booktitle>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On Some Pitfalls in Automatic Evaluation and Significance Testing for MT. In Workshop on Intrinsic and Extrinsic Evaluation Methods for MT and Summarization (MTSE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm. In</title>
<date>2004</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="7201" citStr="Roark et al., 2004" startWordPosition="1150" endWordPosition="1153">he scores of various productions used in the tree. Given features 4b and a corresponding set of parameters w, a standard classification rule f is to return the highest scoring output sentence y, maximizing over correspondences h: f(x; w) = argmax w · 4b(x, y, h). (1) y,h In the phrase-based model, computing the argmax exactly is intractable, so we approximate f with beam decoding. 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f(x; w). Recall the traditional perceptron update rule on an example (xi, yi) is w ← w + 4b(xi, yt) − 4b(xi, yp), (2) where yt = yi is the target output and yp = f(xi; w) = argmaxy w · 4b(xi, y) is the prediction using the current parameters w. We a</context>
<context position="20867" citStr="Roark et al., 2004" startWordPosition="3415" endWordPosition="3418"> features The blanket features provide a rough guide for translation, but they are far too coarse to fix specific mistakes. We therefore add lexical features (LEX) to allow for more fine-grained control. These features come in two varieties. Lexical phrase features indicate the presence of a specific translation phrase, such as (y a-t-il, are there), and lexical language model features indicate the presence of a specific output n-gram, such as of the. Lexical language model features have been exploited successfully in discriminative language modeling to improve speech recognition performance (Roark et al., 2004). We confirm the utility of the two kinds of lexical features: BLANKET+LEX achieves a BLEU score of 35.0, an improvement of 1.6 over BLANKET. To understand the effect of adding lexical features, consider the ten with highest and lowest weights after training: 64 any comments ? -55 (des, of) -52 (y a-t-il, are there any) 57 any comments 46 (des, any) -38 of comments ? These features can in fact be traced back to the following example: Input y a-t-il des observations ? B are there any of comments ? B+L are there any comments ? The second and third rows are the outputs of BLANKET (wrong) and BLAN</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm. In ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="23600" citStr="Schmid, 1994" startWordPosition="3887" endWordPosition="3888">appropriately with suitable non-literal translations. This point highlights the strength of discriminative training: weights are jointly tuned to account for the intricate interactions between overlapping phrases, which is something not achievable by estimating the weights directly from surface statistics. 6.3 Part-of-speech features While lexical features are useful for eliminating specific errors, they have limited ability to generalize to related phrases. This suggests the use of similar features which are abstracted to the POS level.7 In our experiments, we used the TreeTagger POS tagger (Schmid, 1994), which ships pretrained on several languages, to map each word to its majority POS tag. We could also relatively easily base our features on context-dependent POS tags: the entire input sentence is available before decoding begins, and the output sentence is decoded left-to-right and could be tagged incrementally. Where we had lexical phrase features, such as (la realisation du droit, the right), we now also have their POS abstractions, for instance (DT NN IN NN, DT NN). This phrase pair is undesirable, not because of particular lexical facts about la realisation, but because dropping a nomin</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Franz Josef Och</author>
</authors>
<title>Discriminative Reranking for Machine Translation. In HLTNAACL</title>
<date>2004</date>
<contexts>
<context position="2046" citStr="Shen et al., 2004" startWordPosition="308" endWordPosition="311">e to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter estimates improve, the system produces better nbest lists, which can in turn enable better updates in future training iterations. In this paper, we focus on two aspects of the problem of discriminative translation: the inherent difficulty of learning from reference translations, and the challenge of engineering effective features for this task. Discriminative learning from reference translations is inherently problemat</context>
<context position="29876" citStr="Shen et al., 2004" startWordPosition="4905" endWordPosition="4908">sion of Pharaoh that permits arbitrary distortion. 8 Related work In machine translation, most discriminative approaches currently fall into two general categories. The first approach is to reuse the components of a generative model, but tune their relative weights in a discriminative fashion (Och and Ney, 2002; Och, 2003; Chiang, 2005). This approach only works in practice with a small handful of parameters. The second approach is to use reranking, in which a baseline classifier generates an n-best list of candidate translations, and a separate discriminative classifier chooses amongst them (Shen et al., 2004; Och et al., 2004). The major limitation of a reranking system is its dependence on the underlying baseline system, which bounds the potential improvement from discriminative training. In machine translation, this limitation is a real concern; it is common for all translations on moderately-sized n-best lists to be of poor quality. For instance, Och et al. (2004) reported that a 1000-best list was required to achieve performance gains from reranking. In contrast, the decoder in our system can use the feature weights learned in the previous iteration. Tillmann and Zhang (2005) present a discri</context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative Reranking for Machine Translation. In HLTNAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP</booktitle>
<contexts>
<context position="11366" citStr="Stolcke, 2002" startWordPosition="1868" endWordPosition="1869"> performance of the system as we developed it. Note that the DEV set was not used to tune any parameters; tuning was done exclusively on TRAIN. At the end we ran our models once on TEST to get final numbers.2 4 Models Our experiments used phrase-based models (Koehn et al., 2003), which require a translation table and language model for decoding and feature computation. To facilitate comparison with previous work, we created the translation tables using the same techniques as Koehn et al. (2003).3 The language model was a Kneser-Ney interpolated trigram model generated using the SRILM toolkit (Stolcke, 2002). We built our own phrase-based beam decoder that can handle arbitrary features.4 The contributions of features are incrementally added into the score as decoding 2We also experimented with several combinations ofjackknifing to prevent overfitting, in which we selected features on TRAIN-OLD (1996–1998 Europarl corpus) and tuned the parameters on TRAIN, or vice-versa. However, it turned out that using TRAIN-OLD was suboptimal since that data is less relevant to DEV. Another alternative is to combine TRAINOLD and TRAIN into one dual-purpose dataset. The differences between this and our current a</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM An Extensible Language Modeling Toolkit. In ICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A Localized Prediction Model for Statistical Machine Translation. In</title>
<date>2005</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="4660" citStr="Tillmann and Zhang, 2005" startWordPosition="715" endWordPosition="718">us to weight phrases based on the word alignment pattern that led to their extraction. This kind of 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, Sydney, July 2006. c�2006 Association for Computational Linguistics feature provides a potential way to initially extract phrases more aggressively and then later downweight undesirable patterns, essentially learning a weighted extraction heuristic. Finally, we use POS features to parameterize a distortion model in a limited distortion decoder (Zens and Ney, 2004; Tillmann and Zhang, 2005). We show that overall, BLEU score increases from 28.4 to 29.6 on French-English. 2 Approach 2.1 Translation as structured classification Machine translation can be seen as a structured classification task, in which the goal is to learn a mapping from an input (French) sentence x to an output (English) sentence y. Given this setup, discriminative methods allow us to define a broad class of features 4b that operate on (x, y). For example, some features would measure the fluency of y and others would measure the faithfulness of y as a translation of x. However, the translation task in this frame</context>
<context position="30459" citStr="Tillmann and Zhang (2005)" startWordPosition="5001" endWordPosition="5004">er chooses amongst them (Shen et al., 2004; Och et al., 2004). The major limitation of a reranking system is its dependence on the underlying baseline system, which bounds the potential improvement from discriminative training. In machine translation, this limitation is a real concern; it is common for all translations on moderately-sized n-best lists to be of poor quality. For instance, Och et al. (2004) reported that a 1000-best list was required to achieve performance gains from reranking. In contrast, the decoder in our system can use the feature weights learned in the previous iteration. Tillmann and Zhang (2005) present a discriminative approach based on local models. Their formulation explicitly decomposed the score of a translation into a sequence of local decisions, while our formulation allows global estimation. 9 Conclusion We have presented a novel end-to-end discriminative system for machine translation. We studied update strategies, an important issue in online discriminative training for MT, and conclude that making many smaller (conservative) updates is better than making few large (aggressive) updates. We also investigated the effect of adding many expressive features, which yielded a 0.8 </context>
</contexts>
<marker>Tillmann, Zhang, 2005</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2005. A Localized Prediction Model for Statistical Machine Translation. In ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in Statistical Phrase-Based Translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL</booktitle>
<contexts>
<context position="4633" citStr="Zens and Ney, 2004" startWordPosition="711" endWordPosition="714">atures, which allow us to weight phrases based on the word alignment pattern that led to their extraction. This kind of 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, Sydney, July 2006. c�2006 Association for Computational Linguistics feature provides a potential way to initially extract phrases more aggressively and then later downweight undesirable patterns, essentially learning a weighted extraction heuristic. Finally, we use POS features to parameterize a distortion model in a limited distortion decoder (Zens and Ney, 2004; Tillmann and Zhang, 2005). We show that overall, BLEU score increases from 28.4 to 29.6 on French-English. 2 Approach 2.1 Translation as structured classification Machine translation can be seen as a structured classification task, in which the goal is to learn a mapping from an input (French) sentence x to an output (English) sentence y. Given this setup, discriminative methods allow us to define a broad class of features 4b that operate on (x, y). For example, some features would measure the fluency of y and others would measure the faithfulness of y as a translation of x. However, the tra</context>
<context position="12649" citStr="Zens and Ney, 2004" startWordPosition="2071" endWordPosition="2074">nstruct a word alignment in each direction and a growth heuristic to combine them. We extracted all the substrings that are closed under this high-quality word alignment and computed surface statistics from cooccurrences counts. 4In our experiments, we used a beam size of 10, which we found to be only slightly worse than using a beam of 100. 763 proceeds. We experimented with two levels of distortion: monotonic, where the phrasal alignment is monotonic (but word reordering is still possible within a phrase) and limited distortion, where only adjacent phrases are allowed to exchange positions (Zens and Ney, 2004). In the future, we plan to explore our discriminative framework on a full distortion model (Koehn et al., 2003) or even a hierarchical model (Chiang, 2005). Throughout the following experiments, we trained the perceptron algorithm for 10 iterations. The weights were initialized to 1 on the translation table, 1 on the language model (the blanket features in Section 6), and 0 elsewhere. The next two sections give experiments on the two key components of a discriminative machine translation system: choosing the proper update strategy (Section 5) and including powerful features (Section 6). 5 Upd</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in Statistical Phrase-Based Translation. In HLT-NAACL 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>