<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.9948955">
Identifying Agreement and Disagreement in Conversational Speech:
Use of Bayesian Networks to Model Pragmatic Dependencies
</title>
<author confidence="0.633093">
Michel Galley , Kathleen McKeown , Julia Hirschberg ,
</author>
<note confidence="0.871020666666667">
and Elizabeth Shriberg
SRI International
Speech Technology and Research Laboratory
</note>
<address confidence="0.780228">
333 Ravenswood Avenue
Menlo Park, CA 94025, USA
</address>
<email confidence="0.979509">
ees@speech.sri.com
</email>
<affiliation confidence="0.974697">
Columbia University
Computer Science Department
</affiliation>
<address confidence="0.9312285">
1214 Amsterdam Avenue
New York, NY 10027, USA
</address>
<email confidence="0.998866">
galley,kathy,julia @cs.columbia.edu
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999515416666667">
We describe a statistical approach for modeling
agreements and disagreements in conversational in-
teraction. Our approach first identifies adjacency
pairs using maximum entropy ranking based on a
set of lexical, durational, and structural features that
look both forward and backward in the discourse.
We then classify utterances as agreement or dis-
agreement using these adjacency pairs and features
that represent various pragmatic influences of pre-
vious agreement or disagreement on the current ut-
terance. Our approach achieves 86.9% accuracy, a
4.9% increase over previous work.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911086956522">
One of the main features of meetings is the occur-
rence of agreement and disagreement among par-
ticipants. Often meetings include long stretches
of controversial discussion before some consensus
decision is reached. Our ultimate goal is auto-
mated summarization of multi-participant meetings
and we hypothesize that the ability to automatically
identify agreement and disagreement between par-
ticipants will help us in the summarization task.
For example, a summary might resemble minutes of
meetings with major decisions reached (consensus)
along with highlighted points of the pros and cons
for each decision. In this paper, we present a method
to automatically classify utterances as agreement,
disagreement, or neither.
Previous work in automatic identification of
agreement/disagreement (Hillard et al., 2003)
demonstrates that this is a feasible task when var-
ious textual, durational, and acoustic features are
available. We build on their approach and show
that we can get an improvement in accuracy when
contextual information is taken into account. Our
approach first identifies adjacency pairs using maxi-
mum entropy ranking based on a set of lexical, dura-
tional and structural features that look both forward
and backward in the discourse. This allows us to ac-
quire, and subsequently process, knowledge about
who speaks to whom. We hypothesize that prag-
matic features that center around previous agree-
ment between speakers in the dialog will influence
the determination of agreement/disagreement. For
example, if a speaker disagrees with another per-
son once in the conversation, is he more likely to
disagree with him again? We model context using
Bayesian networks that allows capturing of these
pragmatic dependencies. Our accuracy for classify-
ing agreements and disagreements is 86.9%, which
is a 4.9% improvement over (Hillard et al., 2003).
In the following sections, we begin by describ-
ing the annotated corpus that we used for our ex-
periments. We then turn to our work on identify-
ing adjacency pairs. In the section on identification
of agreement/disagreement, we describe the contex-
tual features that we model and the implementation
of the classifier. We close with a discussion of future
work.
</bodyText>
<sectionHeader confidence="0.990027" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999976131578947">
The ICSI Meeting corpus (Janin et al., 2003) is
a collection of 75 meetings collected at the In-
ternational Computer Science Institute (ICSI), one
among the growing number of corpora of human-
to-human multi-party conversations. These are nat-
urally occurring, regular weekly meetings of vari-
ous ICSI research teams. Meetings in general run
just under an hour each; they have an average of 6.5
participants.
These meetings have been labeled with adja-
cency pairs (AP), which provide information about
speaker interaction. They reflect the structure of
conversations as paired utterances such as question-
answer and offer-acceptance, and their labeling is
used in our work to determine who are the ad-
dressees in agreements and disagreements. The an-
notation of the corpus with adjacency pairs is de-
scribed in (Shriberg et al., 2004; Dhillon et al.,
2004).
Seven of those meetings were segmented into
spurts, defined as periods of speech that have no
pauses greater than .5 second, and each spurt was
labeled with one of the four categories: agreement,
disagreement, backchannel, and other.1 We used
spurt segmentation as our unit of analysis instead of
sentence segmentation, because our ultimate goal is
to build a system that can be fully automated, and
in that respect, spurt segmentation is easy to ob-
tain. Backchannels (e.g. “uhhuh” and “okay”) were
treated as a separate category, since they are gener-
ally used by listeners to indicate they are following
along, while not necessarily indicating agreement.
The proportion of classes is the following: 11.9%
are agreements, 6.8% are disagreements, 23.2% are
backchannels, and 58.1% are others. Inter-labeler
reliability estimated on 500 spurts with 2 labelers
was considered quite acceptable, since the kappa
coefficient was .63 (Cohen, 1960).
</bodyText>
<sectionHeader confidence="0.988151" genericHeader="method">
3 Adjacency Pairs
</sectionHeader>
<subsectionHeader confidence="0.973227">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999676935483871">
Adjacency pairs (AP) are considered fundamental
units of conversational organization (Schegloff and
Sacks, 1973). Their identification is central to our
problem, since we need to know the identity of
addressees in agreements and disagreements, and
adjacency pairs provide a means of acquiring this
knowledge. An adjacency pair is said to consist of
two parts (later referred to as A and B) that are or-
dered, adjacent, and produced by different speakers.
The first part makes the second one immediately rel-
evant, as a question does with an answer, or an offer
does with an acceptance. Extensive work in con-
versational analysis uses a less restrictive definition
of adjacency pair that does not impose any actual
adjacency requirement; this requirement is prob-
lematic in many respects (Levinson, 1983). Even
when APs are not directly adjacent, the same con-
straints between pairs and mechanisms for select-
ing the next speaker remain in place (e.g. the case
of embedded question and answer pairs). This re-
laxation on a strict adjacency requirement is partic-
ularly important in interactions of multiple speak-
ers since other speakers have more opportunities to
insert utterances between the two elements of the
AP construction (e.g. interrupted, abandoned or ig-
nored utterances; backchannels; APs with multiple
second elements, e.g. a question followed by an-
swers of multiple speakers).2
Information provided by adjacency pairs can be
used to identify the target of an agreeing or dis-
agreeing utterance. We define the problem of AP
</bodyText>
<footnote confidence="0.99243025">
1Part of these annotated meetings were provided by the au-
thors of (Hillard et al., 2003).
2The percentage of APs labeled in our data that have non-
contiguous parts is about 21%.
</footnote>
<bodyText confidence="0.999444363636363">
identification as follows: given the second element
(B) of an adjacency pair, determine who is the
speaker of the first element (A). A quite effective
baseline algorithm is to select as speaker of utter-
ance A the most recent speaker before the occur-
rence of utterance B. This strategy selects the right
speaker in 79.8% of the cases in the 50 meetings that
were annotated with adjacency pairs. The next sub-
section describes the machine learning framework
used to significantly outperform this already quite
effective baseline algorithm.
</bodyText>
<subsectionHeader confidence="0.999708">
3.2 Maximum Entropy Ranking
</subsectionHeader>
<bodyText confidence="0.997722555555555">
We view the problem as an instance of statisti-
cal ranking, a general machine learning paradigm
used for example in statistical parsing (Collins,
2000) and question answering (Ravichandran et al.,
2003).3 The problem is to select, given a set of
possible candidates (in our case, po-
tential A speakers), the one candidate that maxi-
mizes a given conditional probability distribution.
We use maximum entropy modeling (Berger et
al., 1996) to directly model the conditional proba-
bility , where each in
The only role of the denominator is to ensure
that is a proper probability distribution. It is
defined as:
To find the most probable speaker of part A, we use
the following decision rule:
Note that we have also attempted to model the
problem as a binary classification problem where
</bodyText>
<footnote confidence="0.9931085">
3The approach is generally called re-ranking in cases where
candidates are assigned an initial rank beforehand.
</footnote>
<bodyText confidence="0.998357">
is
an observation associated with the corresponding
speaker.is represented here by only one vari-
able for notational ease, but it possibly represents
several lexical, durational, structural, and acoustic
observations. Given feature functions
and model parameters , the prob-
ability of the maximum entropy model is defined as:
each speaker is either classified as speaker A or
not, but we abandoned that approach, since it gives
much worse performance. This finding is consis-
tent with previous work (Ravichandran et al., 2003)
that compares maximum entropy classification and
re-ranking on a question answering task.
</bodyText>
<subsectionHeader confidence="0.978251">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999996216216216">
We will now describe the features used to train the
maximum entropy model mentioned previously. To
rank all speakers (aside from the B speaker) and to
determine how likely each one is to be the A speaker
of the adjacency pair involving speaker B, we use
four categories of features: structural, durational,
lexical, and dialog act (DA) information. For the
remainder of this section, we will interchangeably
use A to designate either the potential A speaker or
the most recent utterance4 of that speaker, assuming
the distinction is generally unambiguous. We use
B to designate either the B speaker or the current
spurt for which we need to identify a corresponding
A part.
The feature sets are listed in Table 1. Struc-
tural features encode some helpful information re-
garding ordering and overlap of spurts. Note that
with only the first feature listed in the table, the
maximum entropy ranker matches exactly the per-
formance of the baseline algorithm (79.8% accu-
racy). Regarding lexical features, we used a count-
based feature selection algorithm to remove many
first-word and last-word features that occur infre-
quently and that are typically uninformative for the
task at hand. Remaining features essentially con-
tained function words, in particular sentence-initial
indicators of questions (“where”, “when”, and so
on).
Note that all features in Table 1 are “backward-
looking”, in the sense that they result from an anal-
ysis of context preceding B. For many of them, we
built equivalent “forward-looking” features that per-
tain to the closest utterance of the potential speaker
A that follows part B. The motivation for extracting
these features is that speaker A is generally expected
to react if he or she is addressed, and thus, to take
the floor soon after B is produced.
</bodyText>
<sectionHeader confidence="0.717122" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.999501">
We used the labeled adjacency pairs of 50 meetings
and selected 80% of the pairs for training. To train
the maximum entropy ranking model, we used the
generalized iterative scaling algorithm (Darroch and
Ratcliff, 1972) as implemented in YASMET.5
</bodyText>
<footnote confidence="0.984965333333333">
4We build features for both the entire speaker turn of A and
the most recent spurt of A.
5http://www.isi.edu/˜och/YASMET.html
</footnote>
<tableCaption confidence="0.997587">
Table 1. Speaker ranking features
</tableCaption>
<table confidence="0.999439">
Feature sets Accuracy
Baseline 79.80%
Structural 83.97%
Durational 84.71%
Lexical 75.43%
Structural and durational 87.88%
All 89.38%
All (only backward looking) 86.99%
All (Gaussian smoothing, FS) 90.20%
</table>
<tableCaption confidence="0.999409">
Table 2. Speaker ranking accuracy
</tableCaption>
<bodyText confidence="0.998184181818182">
Table 2 summarizes the accuracy of our statistical
ranker on the test data with different feature sets: the
performance is 89.39% when using all feature sets,
and reaches 90.2% after applying Gaussian smooth-
ing and using incremental feature selection as de-
scribed in (Berger et al., 1996) and implemented in
the yasmetFS package.6 Note that restricting our-
selves to only backward looking features decreases
the performance significantly, as we can see in Ta-
ble 2.
We also wanted to determine if information about
</bodyText>
<footnote confidence="0.656573">
6http://www.isi.edu/˜ravichan/YASMET.html
</footnote>
<figure confidence="0.620948555555556">
Structural features:
number of speakers taking the floor between A
and B
number of spurts between A and B
number of spurts of speaker B between A and B
do A and B overlap?
Durational features:
duration of A
if A and B do not overlap: time separating A and
B
if they do overlap: duration of overlap
seconds of overlap with any other speaker
speech rate in A
Lexical features:
number of words in A
number of content words in A
ratio of words of A (respectively B) that are also
in B (respectively A)
ratio of content words of A (respectively B) that
are also in B (respectively A)
number of -grams present both in A and B (we
built 3 features for ranging from 2 to 4)
first and last word of A
number of instances at any position of A of
each cue word listed in (Hirschberg and Litman,
1994)
does A contain the first/last name of speaker B?
</figure>
<bodyText confidence="0.99989919047619">
dialog acts (DA) helps the ranking task. If we
hypothesize that only a limited set of paired DAs
(e.g. offer-accept, question-answer, and apology-
downplay) can be realized as adjacency pairs, then
knowing the DA category of the B part and of all
potential A parts should help in finding the most
meaningful dialog act tag among all potential A
parts; for example, the question-accept pair is ad-
mittedly more likely to correspond to an AP than
e.g. backchannel-accept. We used the DA annota-
tion that we also had available, and used the DA tag
sequence of part A and B as a feature.7
When we add the DA feature set, the accuracy
reaches 91.34%, which is only slightly better than
our 90.20% accuracy, which indicates that lexical,
durational, and structural features capture most of
the informativeness provided by DAs. This im-
proved accuracy with DA information should of
course not be considered as the actual accuracy of
our system, since DA information is difficult to ac-
quire automatically (Stolcke et al., 2000).
</bodyText>
<sectionHeader confidence="0.988657" genericHeader="method">
4 Agreements and Disagreements
</sectionHeader>
<subsectionHeader confidence="0.870543">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999968777777778">
This section focusses on the use of contextual in-
formation, in particular the influence of previous
agreements and disagreements and detected adja-
cency pairs, to improve the classification of agree-
ments and disagreements. We first define the classi-
fication problem, then describe non-contextual fea-
tures, provide some empirical evidence justifying
our choice of contextual features, and finally eval-
uate the classifier.
</bodyText>
<subsectionHeader confidence="0.963543">
4.2 Agreement/Disagreement Classification
</subsectionHeader>
<bodyText confidence="0.987863875">
We need to first introduce some notational con-
ventions and define the classification problem
with the agreement/disagreement tagset. In our
classification problem, each spurt among the
spurts of a meeting must be assigned a tag
AGREEDISAGREEBACKCHANNELOTHER.
To specify the speaker of the spurt (e.g. speaker
B), the notation will sometimes be augmented to
incorporate speaker information, as with , and
to designate the addressee of B (e.g. listener A),
we will use the notation . For example,
AGREE simply means that B agrees with
A in the spurt of index . This notation makes
it obvious that we do not necessarily assume
that agreements and disagreements are reflexive
7The annotation of DA is particularly fine-grained with a
choice of many optional tags that can be associated with each
DA. To deal with this problem, we used various scaled-down
versions of the original tagset.
relations. We define:
as the tag of the most recent spurt before that
is produced by Y and addresses X. This definition
will help our multi-party analyses of agreement and
disagreement behaviors.
</bodyText>
<subsectionHeader confidence="0.998534">
4.3 Local Features
</subsectionHeader>
<bodyText confidence="0.999976827586207">
Many of the local features described in this subsec-
tion are similar in spirit to the ones used in the pre-
vious work of (Hillard et al., 2003). We did not use
acoustic features, since the main purpose of the cur-
rent work is to explore the use of contextual infor-
mation.
Table 3 lists the features that were found most
helpful at identifying agreements and disagree-
ments. Regarding lexical features, we selected a
list of lexical items we believed are instrumental
in the expression of agreements and disagreements:
agreement markers, e.g. “yes” and “right”, as listed
in (Cohen, 2002), general cue phrases, e.g. “but”
and “alright” (Hirschberg and Litman, 1994), and
adjectives with positive or negative polarity (Hatzi-
vassiloglou and McKeown, 1997). We incorpo-
rated a set of durational features that were described
in the literature as good predictors of agreements:
utterance length distinguishes agreement from dis-
agreement, the latter tending to be longer since the
speaker elaborates more on the reasons and circum-
stances of her disagreement than for an agreement
(Cohen, 2002). Duration is also a good predictor
of backchannels, since they tend to be quite short.
Finally, a fair amount of silence and filled pauses
is sometimes an indicator of disagreement, since it
is a dispreferred response in most social contexts
and can be associated with hesitation (Pomerantz,
1984).
</bodyText>
<subsectionHeader confidence="0.997745">
4.4 Contextual Features: An Empirical Study
</subsectionHeader>
<bodyText confidence="0.962952272727273">
We first performed several empirical analyses in or-
der to determine to what extent contextual informa-
tion helps in discriminating between agreement and
disagreement. By integrating the interpretation of
the pragmatic function of an utterance into a wider
context, we aim to detect cases of mismatch be-
tween a correct pragmatic interpretation and the sur-
face form of the utterance, e.g. the case of weak or
“empty” agreement, which has some properties of
downright agreement (lexical items of positive po-
larity), but which is commonly considered to be a
disagreement (Pomerantz, 1984).
While the actual classification problem incorpo-
rates four classes, the BACKCHANNEL class is ig-
Table 3. Local features for agreement and disagreement
classification
nored here to make the empirical study easier to in-
terpret. We assume in that study that accurate AP
labeling is available, but for the purpose of building
and testing a classifier, we use only automatically
extracted adjacency pair information. We tested the
validity of four pragmatic assumptions:
</bodyText>
<listItem confidence="0.980592">
1. previous tag dependency: a tag is influ-
enced by its predecessor
2. same-interactants previous tag depen-
</listItem>
<bodyText confidence="0.976289285714286">
dency: a tag is influenced by
, the most recent tag of
the same speaker addressing the same listener;
for example, it might be reasonable to assume
that if speaker B disagrees with A, B is likely
to disagree with A in his or her next speech
addressing A.
</bodyText>
<listItem confidence="0.9948534">
3. reflexivity: a tag is influenced by
; the assumption is that
is influenced by the polarity (agreement or dis-
agreement) of what A said last to B.
4. transitivity: assuming there is a speaker
</listItem>
<bodyText confidence="0.977972653465346">
for which
exists, then a tag is influ-
enced by and
; an ex-
ample of such an influence is a case where
speaker first agrees with , then speaker
disagrees with , from which one could possi-
bly conclude that is actually in disagreement
with .
Table 4 presents the results of our empirical eval-
uation of the first three assumptions. For compar-
ison, the distribution of classes is the following:
18.8% are agreements, 10.6% disagreements, and
70.6% other. The dependencies empirically eval-
uated in the two last columns are non-local; they
create dependencies between spurts separated by an
arbitrarily long time span. Such long range depen-
dencies are often undesirable, since the influence of
one spurt on the other is often weak or too diffi-
cult to capture with our model. Hence, we made a
Markov assumption by limiting context to an arbi-
trarily chosen value . In this analysis subsection
and for all classification results presented thereafter,
we used a value of .
The table yields some interesting results, show-
ing quite significant variations in class distribution
when it is conditioned on various types of contex-
tual information. We can see for example, that
the proportion of agreements and disagreements (re-
spectively 18.8% and 10.6%) changes to 13.9% and
20.9% respectively when we restrict the counts to
spurts that are preceded by a DISAGREE. Simi-
larly, that distribution changes to 21.3% and 7.3%
when the previous tag is an AGREE. The variable
is even more noticeable between probabilities
and
Structural features:
is the previous/next spurt of the same speaker?
is the previous/next spurt involving the same B
speaker?
Durational features:
duration of the spurt
seconds of overlap with any other speaker
seconds of silence during the spurt
speech rate in the spurt
Lexical features:
number of words in the spurt
number of content words in the spurt
perplexity of the spurt with respect to four lan-
guage models, one for each class
first and last word of the spurt
number of instances of adjectives with positive
polarity (Hatzivassiloglou and McKeown, 1997)
idem, with adjectives of negative polarity
number of instances in the spurt of each cue
phrase and agreement/disagreement token listed
in (Hirschberg and Litman, 1994; Cohen, 2002)
. In 26.1% of the
cases where a given speaker B disagrees with A, he
or she will continue to disagree in the next exchange
involving the same speaker and the same listener.
Similarly with the same probability distribution, a
tendency to agree is confirmed in 25% of the cases.
The results in the last column are quite different
from the two preceding ones. While agreements in
response to agreements ( AGREEAGREE )
are slightly less probable than agreements with-
out conditioning on any previous tag ( AGREE
), the probability of an agreement produced
in response to a disagreement is quite high (with
23.4%), even higher than the proportion of agree-
ments in the entire data (18.8%). This last result
would arguably be quite different with more quar-
relsome meeting participants.
Table 5 represents results concerning the fourth
pragmatic assumption. While none of the results
characterize any strong conditioning of by
and , we can nevertheless notice some interest-
ing phenomena. For example, there is a tendency
for agreements to be transitive, i.e. if X agrees with
A and B agrees with X within a limited segment of
speech, then agreement between B and A is con-
firmed in 22.5% of the cases, while the probabil-
ity of the agreement class is only 18.8%. The only
slightly surprising result appears in the last column
of the table, from which we cannot conclude that
disagreement with a disagreement is equivalent to
agreement. This might be explained by the fact that
these sequences of agreement and disagreement do
not necessarily concern the same propositional con-
tent.
The probability distributions presented here are
admittedly dependent on the meeting genre and par-
ticularly speaker personalities. Nonetheless, we be-
lieve this model can as well be used to capture
salient interactional patterns specific to meetings
with different social dynamics.
We will next discuss our choice of a statisti-
cal model to classify sequence data that can deal
with non-local label dependencies, such as the ones
tested in our empirical study.
</bodyText>
<subsectionHeader confidence="0.994963">
4.5 Sequence Classification with Maximum
Entropy Models
</subsectionHeader>
<bodyText confidence="0.999717965517241">
Extensive research has targeted the problem of la-
beling sequence information to solve a variety of
problems in natural language processing. Hidden
Markov models (HMM) are widely used and con-
siderably well understood models for sequence la-
beling. Their drawback is that, as most genera-
tive models, they are generally computed to max-
imize the joint likelihood of the training data. In
order to define a probability distribution over the
sequences of observation and labels, it is necessary
to enumerate all possible sequences of observations.
Such enumeration is generally prohibitive when the
model incorporates many interacting features and
long-range dependencies (the reader can find a dis-
cussion of the problem in (McCallum et al., 2000)).
Conditional models address these concerns.
Conditional Markov models (CMM) (Ratnaparkhi,
1996; Klein and Manning, 2002) have been
successfully used in sequence labeling tasks incor-
porating rich feature sets. In a left-to-right CMM as
shown in Figure 1(a), the probability of a sequence
of L tags is decomposed as:
is the vector of observations and
each is the index of a spurt. The probability dis-
tribution associated with each state of
the Markov chain only depends on the preceding tag
and the local observation . However, in order
to incorporate more than one label dependency and,
in particular, to take into account the four pragmatic
</bodyText>
<figureCaption confidence="0.990782">
Figure 1. (a) Left-to-right CMM. (b) More complex
Bayesian network. Assuming for example that
</figureCaption>
<bodyText confidence="0.962240459459459">
and , there is then a direct dependency be-
tween and , and the probability model becomes
. This is a sim-
plifying example; in practice, each label is dependent on
a fixed number of other labels.
contextual dependencies discussed in the previous
subsection, we must augment the structure of our
model to obtain a more general one. Such a model
is shown in Figure 1(b), a Bayesian network model
that is well-understood and that has precisely de-
fined semantics.
To this Bayesian network representation, we ap-
ply maximum entropy modeling to define a proba-
bility distribution at each node ( ) dependent on the
observation variable and the five contextual tags
used in the four pragmatic dependencies.8 For no-
tational simplicity, the contextual tags representing
these pragmatic dependencies are represented here
as a vector ( ,, and so on).
Given feature functions
Again, the only role of the denominator is to
ensure that sums to 1, and need not be computed
when searching for the most probable tags. Note
that in our case, the structure of the Bayesian net-
work is known and need not be inferred, since AP
identification is performed before the actual agree-
ment and disagreement classification. Since tag se-
quences are known during training, the inference of
a model for sequence labels is no more difficult than
inferring a model in a non-sequential case.
We compute the most probable sequence by
performing a left-to-right decoding using a beam
search. The algorithm is exactly the same as the one
described in (Ratnaparkhi, 1996) to find the most
probable part-of-speech sequence. We used a large
beam of size =100, which is not computationally
prohibitive, since the tagset contains only four ele-
</bodyText>
<footnote confidence="0.988811666666667">
8The transitivity dependency is conditioned on two tags,
while all others on only one. These five contextual tags are de-
faulted to OTHER when dependency spans exceed the threshold
</footnote>
<table confidence="0.647387764705882">
of .
cJh cJh cJh c�2� c3
(a)[] (b)[I
d[h d, dfl, dJ dye
(both
local and contextual, like previous tag features)
and model parameters ,the
probability of the model is defined as:
AGREEAGREE .213 .250 .175
OTHERAGREE .713 .643 .737
DISAGREEAGREE .073 .107 .088
AGREEOTHER .187 .115 .177
OTHEROTHER .714 .784 .710
DISAGREEOTHER .098 .100 .113
AGREEDISAGREE .139 .087 .234
OTHERDISAGREE .651 .652 .638
DISAGREEDISAGREE .209 .261 .128
</table>
<tableCaption confidence="0.996845">
Table 4. Contextual dependencies (previous tag, same-interactants previous tag, and reflexivity)
</tableCaption>
<table confidence="0.9979745">
, where and
AGREE AGREE DISAGREE DISAGREE
AGREE DISAGREE AGREE DISAGREE
AGREE .225 .147 .131 .152
OTHER .658 .677 .683 .668
DISAGREE .117 .177 .186 .180
</table>
<tableCaption confidence="0.999517">
Table 5. Contextual dependencies (transitivity)
</tableCaption>
<bodyText confidence="0.9997498">
ments. Note however that this algorithm can lead to
search errors. An alternative would be to use a vari-
ant of the Viterbi algorithm, which was successfully
used in (McCallum et al., 2000) to decode the most
probable sequence in a CMM.
</bodyText>
<sectionHeader confidence="0.821969" genericHeader="evaluation">
4.6 Results
</sectionHeader>
<bodyText confidence="0.998449692307692">
We had 8135 spurts available for training and test-
ing, and performed two sets of experiments to evalu-
ate the performance of our system. The tools used to
perform the training are the same as those described
in section 3.4. In the first set of experiments, we re-
produced the experimental setting of (Hillard et al.,
2003), a three-way classification (BACKCHANNEL
and OTHER are merged) using hand-labeled data of
a single meeting as a test set and the remaining data
as training material; for this experiment, we used
the same training set as (Hillard et al., 2003). Per-
formance is reported in Table 6. In the second set
of experiments, we aimed at reducing the expected
variance of our experimental results and performed
N-fold cross-validation in a four-way classification
task, at each step retaining the hand-labeled data of
a meeting for testing and the rest of the data for
training. Table 7 summarizes the performance of
our classifier with the different feature sets in this
classification task, distinguishing the case where the
four label-dependency pragmatic features are avail-
able during decoding from the case where they are
not.
First, the analysis of our results shows that with
our three local feature sets only, we obtain substan-
tially better results than (Hillard et al., 2003). This
</bodyText>
<table confidence="0.966521636363636">
Feature sets Accuracy
(Hillard et al., 2003) 82%
Lexical 84.95%
Structural and durational 71.23%
All (no label dependencies) 85.62%
All (with label dependencies) 86.92%
Table 6. 3-way classification accuracy
Feature sets Label dep. No label dep.
Lexical 83.54% 82.62%
Structural, durational 62.10% 58.86%
All 84.07% 83.11%
</table>
<tableCaption confidence="0.99935">
Table 7. 4-way classification accuracy
</tableCaption>
<bodyText confidence="0.999941461538462">
might be due to some additional features the latter
work didn’t exploit (e.g. structural features and ad-
jective polarity), and to the fact that the learning al-
gorithm used in our experiments might be more ac-
curate than decision trees in the given task. Second,
the table corroborates the findings of (Hillard et al.,
2003) that lexical information make the most help-
ful local features. Finally, we observe that by in-
corporating label-dependency features representing
pragmatic influences, we further improve the perfor-
mance (about 1% in Table 7). This seems to indicate
that modeling label dependencies in our classifica-
tion problem is useful.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999960178571429">
We have shown how identification of adjacency
pairs can help in designing features representing
pragmatic dependencies between agreement and
disagreement labels. These features are shown to
be informative and to help the classification task,
yielding a substantial improvement (1.3% to reach
a 86.9% accuracy in three-way classification).
We also believe that the present work may be use-
ful in other computational pragmatic research fo-
cusing on multi-party dialogs, such as dialog act
(DA) classification. Most previous work in that area
is limited to interaction between two speakers (e.g.
Switchboard, (Stolcke et al., 2000)). When more
than two speakers are involved, the question of who
is the addressee of an utterance is crucial, since it
generally determines what DAs are relevant after the
addressee’s last utterance. So, knowledge about ad-
jacency pairs is likely to help DA classification.
In future work, we plan to extend our inference
process to treat speaker ranking (i.e. AP identifica-
tion) and agreement/disagreement classification as
a single, joint inference problem. Contextual in-
formation about agreements and disagreements can
also provide useful cues regarding who is the ad-
dressee of a given utterance. We also plan to incor-
porate acoustic features to increase the robustness of
our procedure in the case where only speech recog-
nition output is available.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999629">
We are grateful to Mari Ostendorf and Dustin
Hillard for providing us with their agreement and
disagreement labeled data.
This material is based on research supported by
the National Science Foundation under Grant No.
IIS-012196. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999297367647059">
A. Berger, S. Della Pietra, and V Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–72.
J. Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and Psychological
measurements, 20:37–46.
S. Cohen. 2002. A computerized scale for monitor-
ing levels of agreement during a conversation. In
Proc. of the 26th Penn Linguistics Colloquium.
M. Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. 17th Interna-
tional Conf. on Machine Learning, pages 175–
182.
J. N. Darroch and D. Ratcliff. 1972. Generalized
iterative scaling for log-linear models. Annals of
Mathematical Statistics, 43:1470–1480.
R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg.
2004. Meeting recorder project: Dialog act label-
ing guide. Technical Report TR-04-002, ICSI.
V. Hatzivassiloglou and K. McKeown. 1997. Pre-
dicting the semantic orientation of adjectives. In
Proc. ofACL.
D. Hillard, M. Ostendorf, and E Shriberg. 2003.
Detection of agreement vs. disagreement in meet-
ings: training with unlabeled data. In Proc. of
HLT/NAACL.
J. Hirschberg and D. Litman. 1994. Empirical stud-
ies on the disambiguation of cue phrases. Com-
putational Linguistics, 19(3):501–530.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gel-
bart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg,
A. Stolcke, and C. Wooters. 2003. The ICSI
meeting corpus. In Proc. of ICASSP-03, Hong
Kong.
D. Klein and C. D. Manning. 2002. Conditional
structure versus conditional estimation in NLP
models. Technical report.
S. Levinson. 1983. Pragmatics. Cambridge Uni-
versity Press.
A. McCallum, D. Freitag, and F. Pereira. 2000.
Maximum entropy markov models for informa-
tion extraction and segmentation. In Proc. of
ICML.
A. Pomerantz. 1984. Agreeing and disagree-
ing with assessments: some features of pre-
ferred/dispreferred turn shapes. In J.M. Atkinson
and J.C. Heritage, editors, Structures of Social
Action, pages 57–101.
A. Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proc. of EMNLP.
D. Ravichandran, E. Hovy, and F. J. Och. 2003.
Statistical QA - classifier vs re-ranker: What’s
the difference? In Proc. of the ACL Workshop
on Multilingual Summarization and Question An-
swering.
E. A. Schegloff and H Sacks. 1973. Opening up
closings. Semiotica, 7-4:289–327.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and
H. Carvey. 2004. The ICSI meeting recorder dia-
log act (MRDA) corpus. In SIGdial Workshop on
Discourse and Dialogue, pages 97–100.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg,
R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van
Ess-Dykema, and M. Meteer. 2000. Dialogue
act modeling for automatic tagging and recog-
nition of conversational speech. Computational
Linguistics, 26(3):339–373.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.549691">
<title confidence="0.9989195">Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies</title>
<author confidence="0.7825185">Galley McKeown Hirschberg Shriberg</author>
<affiliation confidence="0.999936">SRI International Speech Technology and Research Laboratory</affiliation>
<address confidence="0.9946985">333 Ravenswood Avenue Menlo Park, CA 94025, USA</address>
<email confidence="0.999855">ees@speech.sri.com</email>
<affiliation confidence="0.999668">Columbia University Computer Science Department</affiliation>
<address confidence="0.9998175">1214 Amsterdam Avenue New York, NY 10027, USA</address>
<email confidence="0.998519">galley,kathy,julia@cs.columbia.edu</email>
<abstract confidence="0.997470153846154">We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="7802" citStr="Berger et al., 1996" startWordPosition="1207" endWordPosition="1210">jacency pairs. The next subsection describes the machine learning framework used to significantly outperform this already quite effective baseline algorithm. 3.2 Maximum Entropy Ranking We view the problem as an instance of statistical ranking, a general machine learning paradigm used for example in statistical parsing (Collins, 2000) and question answering (Ravichandran et al., 2003).3 The problem is to select, given a set of possible candidates (in our case, potential A speakers), the one candidate that maximizes a given conditional probability distribution. We use maximum entropy modeling (Berger et al., 1996) to directly model the conditional probability , where each in The only role of the denominator is to ensure that is a proper probability distribution. It is defined as: To find the most probable speaker of part A, we use the following decision rule: Note that we have also attempted to model the problem as a binary classification problem where 3The approach is generally called re-ranking in cases where candidates are assigned an initial rank beforehand. is an observation associated with the corresponding speaker.is represented here by only one variable for notational ease, but it possibly repr</context>
<context position="11607" citStr="Berger et al., 1996" startWordPosition="1815" endWordPosition="1818">ost recent spurt of A. 5http://www.isi.edu/˜och/YASMET.html Table 1. Speaker ranking features Feature sets Accuracy Baseline 79.80% Structural 83.97% Durational 84.71% Lexical 75.43% Structural and durational 87.88% All 89.38% All (only backward looking) 86.99% All (Gaussian smoothing, FS) 90.20% Table 2. Speaker ranking accuracy Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smoothing and using incremental feature selection as described in (Berger et al., 1996) and implemented in the yasmetFS package.6 Note that restricting ourselves to only backward looking features decreases the performance significantly, as we can see in Table 2. We also wanted to determine if information about 6http://www.isi.edu/˜ravichan/YASMET.html Structural features: number of speakers taking the floor between A and B number of spurts between A and B number of spurts of speaker B between A and B do A and B overlap? Durational features: duration of A if A and B do not overlap: time separating A and B if they do overlap: duration of overlap seconds of overlap with any other s</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra, and V Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological measurements,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="5067" citStr="Cohen, 1960" startWordPosition="769" endWordPosition="770">our ultimate goal is to build a system that can be fully automated, and in that respect, spurt segmentation is easy to obtain. Backchannels (e.g. “uhhuh” and “okay”) were treated as a separate category, since they are generally used by listeners to indicate they are following along, while not necessarily indicating agreement. The proportion of classes is the following: 11.9% are agreements, 6.8% are disagreements, 23.2% are backchannels, and 58.1% are others. Inter-labeler reliability estimated on 500 spurts with 2 labelers was considered quite acceptable, since the kappa coefficient was .63 (Cohen, 1960). 3 Adjacency Pairs 3.1 Overview Adjacency pairs (AP) are considered fundamental units of conversational organization (Schegloff and Sacks, 1973). Their identification is central to our problem, since we need to know the identity of addressees in agreements and disagreements, and adjacency pairs provide a means of acquiring this knowledge. An adjacency pair is said to consist of two parts (later referred to as A and B) that are ordered, adjacent, and produced by different speakers. The first part makes the second one immediately relevant, as a question does with an answer, or an offer does wit</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological measurements, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cohen</author>
</authors>
<title>A computerized scale for monitoring levels of agreement during a conversation.</title>
<date>2002</date>
<booktitle>In Proc. of the 26th Penn Linguistics Colloquium.</booktitle>
<contexts>
<context position="15916" citStr="Cohen, 2002" startWordPosition="2537" endWordPosition="2538">ehaviors. 4.3 Local Features Many of the local features described in this subsection are similar in spirit to the ones used in the previous work of (Hillard et al., 2003). We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information. Table 3 lists the features that were found most helpful at identifying agreements and disagreements. Regarding lexical features, we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements: agreement markers, e.g. “yes” and “right”, as listed in (Cohen, 2002), general cue phrases, e.g. “but” and “alright” (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997). We incorporated a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002). Duration is also a good predictor of backchannels, since they tend to be quite short. Finally, a fai</context>
<context position="20605" citStr="Cohen, 2002" startWordPosition="3301" endWordPosition="3302">onal features: duration of the spurt seconds of overlap with any other speaker seconds of silence during the spurt speech rate in the spurt Lexical features: number of words in the spurt number of content words in the spurt perplexity of the spurt with respect to four language models, one for each class first and last word of the spurt number of instances of adjectives with positive polarity (Hatzivassiloglou and McKeown, 1997) idem, with adjectives of negative polarity number of instances in the spurt of each cue phrase and agreement/disagreement token listed in (Hirschberg and Litman, 1994; Cohen, 2002) . In 26.1% of the cases where a given speaker B disagrees with A, he or she will continue to disagree in the next exchange involving the same speaker and the same listener. Similarly with the same probability distribution, a tendency to agree is confirmed in 25% of the cases. The results in the last column are quite different from the two preceding ones. While agreements in response to agreements ( AGREEAGREE ) are slightly less probable than agreements without conditioning on any previous tag ( AGREE ), the probability of an agreement produced in response to a disagreement is quite high (wit</context>
</contexts>
<marker>Cohen, 2002</marker>
<rawString>S. Cohen. 2002. A computerized scale for monitoring levels of agreement during a conversation. In Proc. of the 26th Penn Linguistics Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. 17th International Conf. on Machine Learning,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="7518" citStr="Collins, 2000" startWordPosition="1164" endWordPosition="1165">eaker of the first element (A). A quite effective baseline algorithm is to select as speaker of utterance A the most recent speaker before the occurrence of utterance B. This strategy selects the right speaker in 79.8% of the cases in the 50 meetings that were annotated with adjacency pairs. The next subsection describes the machine learning framework used to significantly outperform this already quite effective baseline algorithm. 3.2 Maximum Entropy Ranking We view the problem as an instance of statistical ranking, a general machine learning paradigm used for example in statistical parsing (Collins, 2000) and question answering (Ravichandran et al., 2003).3 The problem is to select, given a set of possible candidates (in our case, potential A speakers), the one candidate that maximizes a given conditional probability distribution. We use maximum entropy modeling (Berger et al., 1996) to directly model the conditional probability , where each in The only role of the denominator is to ensure that is a proper probability distribution. It is defined as: To find the most probable speaker of part A, we use the following decision rule: Note that we have also attempted to model the problem as a binary</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In Proc. 17th International Conf. on Machine Learning, pages 175– 182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="10893" citStr="Darroch and Ratcliff, 1972" startWordPosition="1708" endWordPosition="1711">n the sense that they result from an analysis of context preceding B. For many of them, we built equivalent “forward-looking” features that pertain to the closest utterance of the potential speaker A that follows part B. The motivation for extracting these features is that speaker A is generally expected to react if he or she is addressed, and thus, to take the floor soon after B is produced. 3.4 Results We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training. To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET.5 4We build features for both the entire speaker turn of A and the most recent spurt of A. 5http://www.isi.edu/˜och/YASMET.html Table 1. Speaker ranking features Feature sets Accuracy Baseline 79.80% Structural 83.97% Durational 84.71% Lexical 75.43% Structural and durational 87.88% All 89.38% All (only backward looking) 86.99% All (Gaussian smoothing, FS) 90.20% Table 2. Speaker ranking accuracy Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43:1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>H Carvey</author>
<author>E Shriberg</author>
</authors>
<title>Meeting recorder project: Dialog act labeling guide.</title>
<date>2004</date>
<tech>Technical Report TR-04-002, ICSI.</tech>
<contexts>
<context position="4125" citStr="Dhillon et al., 2004" startWordPosition="621" endWordPosition="624"> These are naturally occurring, regular weekly meetings of various ICSI research teams. Meetings in general run just under an hour each; they have an average of 6.5 participants. These meetings have been labeled with adjacency pairs (AP), which provide information about speaker interaction. They reflect the structure of conversations as paired utterances such as questionanswer and offer-acceptance, and their labeling is used in our work to determine who are the addressees in agreements and disagreements. The annotation of the corpus with adjacency pairs is described in (Shriberg et al., 2004; Dhillon et al., 2004). Seven of those meetings were segmented into spurts, defined as periods of speech that have no pauses greater than .5 second, and each spurt was labeled with one of the four categories: agreement, disagreement, backchannel, and other.1 We used spurt segmentation as our unit of analysis instead of sentence segmentation, because our ultimate goal is to build a system that can be fully automated, and in that respect, spurt segmentation is easy to obtain. Backchannels (e.g. “uhhuh” and “okay”) were treated as a separate category, since they are generally used by listeners to indicate they are fol</context>
</contexts>
<marker>Dhillon, Bhagat, Carvey, Shriberg, 2004</marker>
<rawString>R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg. 2004. Meeting recorder project: Dialog act labeling guide. Technical Report TR-04-002, ICSI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="16081" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="2557" endWordPosition="2561">rk of (Hillard et al., 2003). We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information. Table 3 lists the features that were found most helpful at identifying agreements and disagreements. Regarding lexical features, we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements: agreement markers, e.g. “yes” and “right”, as listed in (Cohen, 2002), general cue phrases, e.g. “but” and “alright” (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997). We incorporated a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002). Duration is also a good predictor of backchannels, since they tend to be quite short. Finally, a fair amount of silence and filled pauses is sometimes an indicator of disagreement, since it is a dispreferred response in most social contexts and can be associated wi</context>
<context position="20424" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="3272" endWordPosition="3275">n AGREE. The variable is even more noticeable between probabilities and Structural features: is the previous/next spurt of the same speaker? is the previous/next spurt involving the same B speaker? Durational features: duration of the spurt seconds of overlap with any other speaker seconds of silence during the spurt speech rate in the spurt Lexical features: number of words in the spurt number of content words in the spurt perplexity of the spurt with respect to four language models, one for each class first and last word of the spurt number of instances of adjectives with positive polarity (Hatzivassiloglou and McKeown, 1997) idem, with adjectives of negative polarity number of instances in the spurt of each cue phrase and agreement/disagreement token listed in (Hirschberg and Litman, 1994; Cohen, 2002) . In 26.1% of the cases where a given speaker B disagrees with A, he or she will continue to disagree in the next exchange involving the same speaker and the same listener. Similarly with the same probability distribution, a tendency to agree is confirmed in 25% of the cases. The results in the last column are quite different from the two preceding ones. While agreements in response to agreements ( AGREEAGREE ) are</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>V. Hatzivassiloglou and K. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>E Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: training with unlabeled data.</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="1871" citStr="Hillard et al., 2003" startWordPosition="259" endWordPosition="262">sus decision is reached. Our ultimate goal is automated summarization of multi-participant meetings and we hypothesize that the ability to automatically identify agreement and disagreement between participants will help us in the summarization task. For example, a summary might resemble minutes of meetings with major decisions reached (consensus) along with highlighted points of the pros and cons for each decision. In this paper, we present a method to automatically classify utterances as agreement, disagreement, or neither. Previous work in automatic identification of agreement/disagreement (Hillard et al., 2003) demonstrates that this is a feasible task when various textual, durational, and acoustic features are available. We build on their approach and show that we can get an improvement in accuracy when contextual information is taken into account. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational and structural features that look both forward and backward in the discourse. This allows us to acquire, and subsequently process, knowledge about who speaks to whom. We hypothesize that pragmatic features that center around previous agreemen</context>
<context position="6714" citStr="Hillard et al., 2003" startWordPosition="1031" endWordPosition="1034"> This relaxation on a strict adjacency requirement is particularly important in interactions of multiple speakers since other speakers have more opportunities to insert utterances between the two elements of the AP construction (e.g. interrupted, abandoned or ignored utterances; backchannels; APs with multiple second elements, e.g. a question followed by answers of multiple speakers).2 Information provided by adjacency pairs can be used to identify the target of an agreeing or disagreeing utterance. We define the problem of AP 1Part of these annotated meetings were provided by the authors of (Hillard et al., 2003). 2The percentage of APs labeled in our data that have noncontiguous parts is about 21%. identification as follows: given the second element (B) of an adjacency pair, determine who is the speaker of the first element (A). A quite effective baseline algorithm is to select as speaker of utterance A the most recent speaker before the occurrence of utterance B. This strategy selects the right speaker in 79.8% of the cases in the 50 meetings that were annotated with adjacency pairs. The next subsection describes the machine learning framework used to significantly outperform this already quite effe</context>
<context position="15474" citStr="Hillard et al., 2003" startWordPosition="2463" endWordPosition="2466">sume that agreements and disagreements are reflexive 7The annotation of DA is particularly fine-grained with a choice of many optional tags that can be associated with each DA. To deal with this problem, we used various scaled-down versions of the original tagset. relations. We define: as the tag of the most recent spurt before that is produced by Y and addresses X. This definition will help our multi-party analyses of agreement and disagreement behaviors. 4.3 Local Features Many of the local features described in this subsection are similar in spirit to the ones used in the previous work of (Hillard et al., 2003). We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information. Table 3 lists the features that were found most helpful at identifying agreements and disagreements. Regarding lexical features, we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements: agreement markers, e.g. “yes” and “right”, as listed in (Cohen, 2002), general cue phrases, e.g. “but” and “alright” (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown</context>
<context position="27339" citStr="Hillard et al., 2003" startWordPosition="4400" endWordPosition="4403">.180 Table 5. Contextual dependencies (transitivity) ments. Note however that this algorithm can lead to search errors. An alternative would be to use a variant of the Viterbi algorithm, which was successfully used in (McCallum et al., 2000) to decode the most probable sequence in a CMM. 4.6 Results We had 8135 spurts available for training and testing, and performed two sets of experiments to evaluate the performance of our system. The tools used to perform the training are the same as those described in section 3.4. In the first set of experiments, we reproduced the experimental setting of (Hillard et al., 2003), a three-way classification (BACKCHANNEL and OTHER are merged) using hand-labeled data of a single meeting as a test set and the remaining data as training material; for this experiment, we used the same training set as (Hillard et al., 2003). Performance is reported in Table 6. In the second set of experiments, we aimed at reducing the expected variance of our experimental results and performed N-fold cross-validation in a four-way classification task, at each step retaining the hand-labeled data of a meeting for testing and the rest of the data for training. Table 7 summarizes the performan</context>
<context position="29003" citStr="Hillard et al., 2003" startWordPosition="4664" endWordPosition="4667"> Structural and durational 71.23% All (no label dependencies) 85.62% All (with label dependencies) 86.92% Table 6. 3-way classification accuracy Feature sets Label dep. No label dep. Lexical 83.54% 82.62% Structural, durational 62.10% 58.86% All 84.07% 83.11% Table 7. 4-way classification accuracy might be due to some additional features the latter work didn’t exploit (e.g. structural features and adjective polarity), and to the fact that the learning algorithm used in our experiments might be more accurate than decision trees in the given task. Second, the table corroborates the findings of (Hillard et al., 2003) that lexical information make the most helpful local features. Finally, we observe that by incorporating label-dependency features representing pragmatic influences, we further improve the performance (about 1% in Table 7). This seems to indicate that modeling label dependencies in our classification problem is useful. 5 Conclusion We have shown how identification of adjacency pairs can help in designing features representing pragmatic dependencies between agreement and disagreement labels. These features are shown to be informative and to help the classification task, yielding a substantial </context>
</contexts>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>D. Hillard, M. Ostendorf, and E Shriberg. 2003. Detection of agreement vs. disagreement in meetings: training with unlabeled data. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="12662" citStr="Hirschberg and Litman, 1994" startWordPosition="2006" endWordPosition="2009"> B overlap? Durational features: duration of A if A and B do not overlap: time separating A and B if they do overlap: duration of overlap seconds of overlap with any other speaker speech rate in A Lexical features: number of words in A number of content words in A ratio of words of A (respectively B) that are also in B (respectively A) ratio of content words of A (respectively B) that are also in B (respectively A) number of -grams present both in A and B (we built 3 features for ranging from 2 to 4) first and last word of A number of instances at any position of A of each cue word listed in (Hirschberg and Litman, 1994) does A contain the first/last name of speaker B? dialog acts (DA) helps the ranking task. If we hypothesize that only a limited set of paired DAs (e.g. offer-accept, question-answer, and apologydownplay) can be realized as adjacency pairs, then knowing the DA category of the B part and of all potential A parts should help in finding the most meaningful dialog act tag among all potential A parts; for example, the question-accept pair is admittedly more likely to correspond to an AP than e.g. backchannel-accept. We used the DA annotation that we also had available, and used the DA tag sequence </context>
<context position="15993" citStr="Hirschberg and Litman, 1994" startWordPosition="2546" endWordPosition="2549">ibed in this subsection are similar in spirit to the ones used in the previous work of (Hillard et al., 2003). We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information. Table 3 lists the features that were found most helpful at identifying agreements and disagreements. Regarding lexical features, we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements: agreement markers, e.g. “yes” and “right”, as listed in (Cohen, 2002), general cue phrases, e.g. “but” and “alright” (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997). We incorporated a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002). Duration is also a good predictor of backchannels, since they tend to be quite short. Finally, a fair amount of silence and filled pauses is sometimes an indicator of disagreeme</context>
<context position="20591" citStr="Hirschberg and Litman, 1994" startWordPosition="3297" endWordPosition="3300">ng the same B speaker? Durational features: duration of the spurt seconds of overlap with any other speaker seconds of silence during the spurt speech rate in the spurt Lexical features: number of words in the spurt number of content words in the spurt perplexity of the spurt with respect to four language models, one for each class first and last word of the spurt number of instances of adjectives with positive polarity (Hatzivassiloglou and McKeown, 1997) idem, with adjectives of negative polarity number of instances in the spurt of each cue phrase and agreement/disagreement token listed in (Hirschberg and Litman, 1994; Cohen, 2002) . In 26.1% of the cases where a given speaker B disagrees with A, he or she will continue to disagree in the next exchange involving the same speaker and the same listener. Similarly with the same probability distribution, a tendency to agree is confirmed in 25% of the cases. The results in the last column are quite different from the two preceding ones. While agreements in response to agreements ( AGREEAGREE ) are slightly less probable than agreements without conditioning on any previous tag ( AGREE ), the probability of an agreement produced in response to a disagreement is q</context>
</contexts>
<marker>Hirschberg, Litman, 1994</marker>
<rawString>J. Hirschberg and D. Litman. 1994. Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19(3):501–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In Proc. of ICASSP-03,</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="3323" citStr="Janin et al., 2003" startWordPosition="493" endWordPosition="496">text using Bayesian networks that allows capturing of these pragmatic dependencies. Our accuracy for classifying agreements and disagreements is 86.9%, which is a 4.9% improvement over (Hillard et al., 2003). In the following sections, we begin by describing the annotated corpus that we used for our experiments. We then turn to our work on identifying adjacency pairs. In the section on identification of agreement/disagreement, we describe the contextual features that we model and the implementation of the classifier. We close with a discussion of future work. 2 Corpus The ICSI Meeting corpus (Janin et al., 2003) is a collection of 75 meetings collected at the International Computer Science Institute (ICSI), one among the growing number of corpora of humanto-human multi-party conversations. These are naturally occurring, regular weekly meetings of various ICSI research teams. Meetings in general run just under an hour each; they have an average of 6.5 participants. These meetings have been labeled with adjacency pairs (AP), which provide information about speaker interaction. They reflect the structure of conversations as paired utterances such as questionanswer and offer-acceptance, and their labelin</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI meeting corpus. In Proc. of ICASSP-03, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Conditional structure versus conditional estimation in NLP models.</title>
<date>2002</date>
<tech>Technical report.</tech>
<contexts>
<context position="23554" citStr="Klein and Manning, 2002" startWordPosition="3772" endWordPosition="3775">beling. Their drawback is that, as most generative models, they are generally computed to maximize the joint likelihood of the training data. In order to define a probability distribution over the sequences of observation and labels, it is necessary to enumerate all possible sequences of observations. Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies (the reader can find a discussion of the problem in (McCallum et al., 2000)). Conditional models address these concerns. Conditional Markov models (CMM) (Ratnaparkhi, 1996; Klein and Manning, 2002) have been successfully used in sequence labeling tasks incorporating rich feature sets. In a left-to-right CMM as shown in Figure 1(a), the probability of a sequence of L tags is decomposed as: is the vector of observations and each is the index of a spurt. The probability distribution associated with each state of the Markov chain only depends on the preceding tag and the local observation . However, in order to incorporate more than one label dependency and, in particular, to take into account the four pragmatic Figure 1. (a) Left-to-right CMM. (b) More complex Bayesian network. Assuming fo</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. Conditional structure versus conditional estimation in NLP models. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Levinson</author>
</authors>
<title>Pragmatics.</title>
<date>1983</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5899" citStr="Levinson, 1983" startWordPosition="901" endWordPosition="902">the identity of addressees in agreements and disagreements, and adjacency pairs provide a means of acquiring this knowledge. An adjacency pair is said to consist of two parts (later referred to as A and B) that are ordered, adjacent, and produced by different speakers. The first part makes the second one immediately relevant, as a question does with an answer, or an offer does with an acceptance. Extensive work in conversational analysis uses a less restrictive definition of adjacency pair that does not impose any actual adjacency requirement; this requirement is problematic in many respects (Levinson, 1983). Even when APs are not directly adjacent, the same constraints between pairs and mechanisms for selecting the next speaker remain in place (e.g. the case of embedded question and answer pairs). This relaxation on a strict adjacency requirement is particularly important in interactions of multiple speakers since other speakers have more opportunities to insert utterances between the two elements of the AP construction (e.g. interrupted, abandoned or ignored utterances; backchannels; APs with multiple second elements, e.g. a question followed by answers of multiple speakers).2 Information provi</context>
</contexts>
<marker>Levinson, 1983</marker>
<rawString>S. Levinson. 1983. Pragmatics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="23432" citStr="McCallum et al., 2000" startWordPosition="3757" endWordPosition="3760"> language processing. Hidden Markov models (HMM) are widely used and considerably well understood models for sequence labeling. Their drawback is that, as most generative models, they are generally computed to maximize the joint likelihood of the training data. In order to define a probability distribution over the sequences of observation and labels, it is necessary to enumerate all possible sequences of observations. Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies (the reader can find a discussion of the problem in (McCallum et al., 2000)). Conditional models address these concerns. Conditional Markov models (CMM) (Ratnaparkhi, 1996; Klein and Manning, 2002) have been successfully used in sequence labeling tasks incorporating rich feature sets. In a left-to-right CMM as shown in Figure 1(a), the probability of a sequence of L tags is decomposed as: is the vector of observations and each is the index of a spurt. The probability distribution associated with each state of the Markov chain only depends on the preceding tag and the local observation . However, in order to incorporate more than one label dependency and, in particula</context>
<context position="26959" citStr="McCallum et al., 2000" startWordPosition="4332" endWordPosition="4335">R .714 .784 .710 DISAGREEOTHER .098 .100 .113 AGREEDISAGREE .139 .087 .234 OTHERDISAGREE .651 .652 .638 DISAGREEDISAGREE .209 .261 .128 Table 4. Contextual dependencies (previous tag, same-interactants previous tag, and reflexivity) , where and AGREE AGREE DISAGREE DISAGREE AGREE DISAGREE AGREE DISAGREE AGREE .225 .147 .131 .152 OTHER .658 .677 .683 .668 DISAGREE .117 .177 .186 .180 Table 5. Contextual dependencies (transitivity) ments. Note however that this algorithm can lead to search errors. An alternative would be to use a variant of the Viterbi algorithm, which was successfully used in (McCallum et al., 2000) to decode the most probable sequence in a CMM. 4.6 Results We had 8135 spurts available for training and testing, and performed two sets of experiments to evaluate the performance of our system. The tools used to perform the training are the same as those described in section 3.4. In the first set of experiments, we reproduced the experimental setting of (Hillard et al., 2003), a three-way classification (BACKCHANNEL and OTHER are merged) using hand-labeled data of a single meeting as a test set and the remaining data as training material; for this experiment, we used the same training set as</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pomerantz</author>
</authors>
<title>Agreeing and disagreeing with assessments: some features of preferred/dispreferred turn shapes.</title>
<date>1984</date>
<booktitle>Structures of Social Action,</booktitle>
<pages>57--101</pages>
<editor>In J.M. Atkinson and J.C. Heritage, editors,</editor>
<contexts>
<context position="16712" citStr="Pomerantz, 1984" startWordPosition="2660" endWordPosition="2661">ed a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002). Duration is also a good predictor of backchannels, since they tend to be quite short. Finally, a fair amount of silence and filled pauses is sometimes an indicator of disagreement, since it is a dispreferred response in most social contexts and can be associated with hesitation (Pomerantz, 1984). 4.4 Contextual Features: An Empirical Study We first performed several empirical analyses in order to determine to what extent contextual information helps in discriminating between agreement and disagreement. By integrating the interpretation of the pragmatic function of an utterance into a wider context, we aim to detect cases of mismatch between a correct pragmatic interpretation and the surface form of the utterance, e.g. the case of weak or “empty” agreement, which has some properties of downright agreement (lexical items of positive polarity), but which is commonly considered to be a d</context>
</contexts>
<marker>Pomerantz, 1984</marker>
<rawString>A. Pomerantz. 1984. Agreeing and disagreeing with assessments: some features of preferred/dispreferred turn shapes. In J.M. Atkinson and J.C. Heritage, editors, Structures of Social Action, pages 57–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="23528" citStr="Ratnaparkhi, 1996" startWordPosition="3770" endWordPosition="3771">els for sequence labeling. Their drawback is that, as most generative models, they are generally computed to maximize the joint likelihood of the training data. In order to define a probability distribution over the sequences of observation and labels, it is necessary to enumerate all possible sequences of observations. Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies (the reader can find a discussion of the problem in (McCallum et al., 2000)). Conditional models address these concerns. Conditional Markov models (CMM) (Ratnaparkhi, 1996; Klein and Manning, 2002) have been successfully used in sequence labeling tasks incorporating rich feature sets. In a left-to-right CMM as shown in Figure 1(a), the probability of a sequence of L tags is decomposed as: is the vector of observations and each is the index of a spurt. The probability distribution associated with each state of the Markov chain only depends on the preceding tag and the local observation . However, in order to incorporate more than one label dependency and, in particular, to take into account the four pragmatic Figure 1. (a) Left-to-right CMM. (b) More complex Bay</context>
<context position="25695" citStr="Ratnaparkhi, 1996" startWordPosition="4134" endWordPosition="4135">re that sums to 1, and need not be computed when searching for the most probable tags. Note that in our case, the structure of the Bayesian network is known and need not be inferred, since AP identification is performed before the actual agreement and disagreement classification. Since tag sequences are known during training, the inference of a model for sequence labels is no more difficult than inferring a model in a non-sequential case. We compute the most probable sequence by performing a left-to-right decoding using a beam search. The algorithm is exactly the same as the one described in (Ratnaparkhi, 1996) to find the most probable part-of-speech sequence. We used a large beam of size =100, which is not computationally prohibitive, since the tagset contains only four ele8The transitivity dependency is conditioned on two tags, while all others on only one. These five contextual tags are defaulted to OTHER when dependency spans exceed the threshold of . cJh cJh cJh c�2� c3 (a)[] (b)[I d[h d, dfl, dJ dye (both local and contextual, like previous tag features) and model parameters ,the probability of the model is defined as: AGREEAGREE .213 .250 .175 OTHERAGREE .713 .643 .737 DISAGREEAGREE .073 .10</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
<author>F J Och</author>
</authors>
<title>Statistical QA - classifier vs re-ranker: What’s the difference?</title>
<date>2003</date>
<booktitle>In Proc. of the ACL Workshop on Multilingual Summarization and Question Answering.</booktitle>
<contexts>
<context position="7569" citStr="Ravichandran et al., 2003" startWordPosition="1169" endWordPosition="1172"> effective baseline algorithm is to select as speaker of utterance A the most recent speaker before the occurrence of utterance B. This strategy selects the right speaker in 79.8% of the cases in the 50 meetings that were annotated with adjacency pairs. The next subsection describes the machine learning framework used to significantly outperform this already quite effective baseline algorithm. 3.2 Maximum Entropy Ranking We view the problem as an instance of statistical ranking, a general machine learning paradigm used for example in statistical parsing (Collins, 2000) and question answering (Ravichandran et al., 2003).3 The problem is to select, given a set of possible candidates (in our case, potential A speakers), the one candidate that maximizes a given conditional probability distribution. We use maximum entropy modeling (Berger et al., 1996) to directly model the conditional probability , where each in The only role of the denominator is to ensure that is a proper probability distribution. It is defined as: To find the most probable speaker of part A, we use the following decision rule: Note that we have also attempted to model the problem as a binary classification problem where 3The approach is gene</context>
</contexts>
<marker>Ravichandran, Hovy, Och, 2003</marker>
<rawString>D. Ravichandran, E. Hovy, and F. J. Och. 2003. Statistical QA - classifier vs re-ranker: What’s the difference? In Proc. of the ACL Workshop on Multilingual Summarization and Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Schegloff</author>
<author>H Sacks</author>
</authors>
<title>Opening up closings.</title>
<date>1973</date>
<journal>Semiotica,</journal>
<pages>7--4</pages>
<contexts>
<context position="5212" citStr="Schegloff and Sacks, 1973" startWordPosition="786" endWordPosition="789">channels (e.g. “uhhuh” and “okay”) were treated as a separate category, since they are generally used by listeners to indicate they are following along, while not necessarily indicating agreement. The proportion of classes is the following: 11.9% are agreements, 6.8% are disagreements, 23.2% are backchannels, and 58.1% are others. Inter-labeler reliability estimated on 500 spurts with 2 labelers was considered quite acceptable, since the kappa coefficient was .63 (Cohen, 1960). 3 Adjacency Pairs 3.1 Overview Adjacency pairs (AP) are considered fundamental units of conversational organization (Schegloff and Sacks, 1973). Their identification is central to our problem, since we need to know the identity of addressees in agreements and disagreements, and adjacency pairs provide a means of acquiring this knowledge. An adjacency pair is said to consist of two parts (later referred to as A and B) that are ordered, adjacent, and produced by different speakers. The first part makes the second one immediately relevant, as a question does with an answer, or an offer does with an acceptance. Extensive work in conversational analysis uses a less restrictive definition of adjacency pair that does not impose any actual a</context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>E. A. Schegloff and H Sacks. 1973. Opening up closings. Semiotica, 7-4:289–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>J Ang</author>
<author>H Carvey</author>
</authors>
<title>The ICSI meeting recorder dialog act (MRDA) corpus.</title>
<date>2004</date>
<booktitle>In SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="4102" citStr="Shriberg et al., 2004" startWordPosition="617" endWordPosition="620">ti-party conversations. These are naturally occurring, regular weekly meetings of various ICSI research teams. Meetings in general run just under an hour each; they have an average of 6.5 participants. These meetings have been labeled with adjacency pairs (AP), which provide information about speaker interaction. They reflect the structure of conversations as paired utterances such as questionanswer and offer-acceptance, and their labeling is used in our work to determine who are the addressees in agreements and disagreements. The annotation of the corpus with adjacency pairs is described in (Shriberg et al., 2004; Dhillon et al., 2004). Seven of those meetings were segmented into spurts, defined as periods of speech that have no pauses greater than .5 second, and each spurt was labeled with one of the four categories: agreement, disagreement, backchannel, and other.1 We used spurt segmentation as our unit of analysis instead of sentence segmentation, because our ultimate goal is to build a system that can be fully automated, and in that respect, spurt segmentation is easy to obtain. Backchannels (e.g. “uhhuh” and “okay”) were treated as a separate category, since they are generally used by listeners t</context>
</contexts>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004. The ICSI meeting recorder dialog act (MRDA) corpus. In SIGdial Workshop on Discourse and Dialogue, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>R Martin</author>
<author>C Van Ess-Dykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M. Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>