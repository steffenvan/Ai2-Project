<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000442">
<title confidence="0.979249">
Automatic Labeling of Semantic Roles
</title>
<author confidence="0.999428">
Daniel Gildea Daniel Jurafsky
</author>
<affiliation confidence="0.981308">
University of California, Berkeley, and Department of Linguistics
International Computer Science Institute University of Colorado, Boulder
</affiliation>
<email confidence="0.999004">
gildea@cs.berkeley.edu jurafsky@colorado.edu
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999855555555555">
We present a system for identify-
ing the semantic relationships, or se-
mantic roles, filled by constituents of
a sentence within a semantic frame.
Various lexical and syntactic fea-
tures are derived from parse trees
and used to derive statistical clas-
sifiers from hand-annotated training
data.
</bodyText>
<sectionHeader confidence="0.997907" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997346969697">
Identifying the semantic roles filled by con-
stituents of a sentence can provide a level of
shallow semantic analysis useful in solving a
number of natural language processing tasks.
Semantic roles represent the participants in
an action or relationship captured by a se-
mantic frame. For example, the frame for one
sense of the verb &amp;quot;crash&amp;quot; includes the roles
AGENT, VEHICLE and TO-LOCATION.
This shallow semantic level of interpreta-
tion can be used for many purposes. Cur-
rent information extraction systems often use
domain-specific frame-and-slot templates to
extract facts about, for example, financial
news or interesting political events. A shal-
low semantic level of representation is a more
domain-independent, robust level of represen-
tation. Identifying these roles, for example,
could allow a system to determine that in
the sentence &amp;quot;The first one crashed&amp;quot; the sub-
ject is the vehicle, but in the sentence &amp;quot;The
first one crashed it&amp;quot; the subject is the agent,
which would help in information extraction in
this domain. Another application is in word-
sense disambiguation, where the roles associ-
ated with a word can be cues to its sense. For
example, Lapata and Brew (1999) and others
have shown that the different syntactic sub-
catgorization frames of a verb like &amp;quot;serve&amp;quot; can
be used to help disambiguate a particular in-
stance of the word &amp;quot;serve&amp;quot;. Adding seman-
tic role subcategorization information to this
syntactic information could extend this idea
to use richer semantic knowledge. Semantic
roles could also act as an important inter-
mediate representation in statistical machine
translation or automatic text summarization
and in the emerging field of Text Data Mining
(TDM) (Hearst, 1999). Finally, incorporat-
ing semantic roles into probabilistic models of
language should yield more accurate parsers
and better language models for speech recog-
nition.
This paper proposes an algorithm for au-
tomatic semantic analysis, assigning a se-
mantic role to constituents in a sentence.
Our approach to semantic analysis is to
treat the problem of semantic role labeling
like the similar problems of parsing, part of
speech tagging, and word sense disambigua-
tion. We apply statistical techniques that
have been successful for these tasks, including
probabilistic parsing and statistical classifica-
tion. Our statistical algorithms are trained
on a hand-labeled dataset: the FrameNet
database (Baker et al., 1998). The FrameNet
database defines a tagset of semantic roles
called frame elements, and includes roughly
50,000 sentences from the British National
Corpus which have been hand-labeled with
these frame elements. The next section de-
scribes the set of frame elements/semantic
roles used by our system. In the rest of this
paper we report on our current system, as well
as a number of preliminary experiments on
extensions to the system.
</bodyText>
<sectionHeader confidence="0.979244" genericHeader="introduction">
2 Semantic Roles
</sectionHeader>
<bodyText confidence="0.999658534883721">
Historically, two types of semantic roles have
been studied: abstract roles such as AGENT
and PATiENT, and roles specific to individual
verbs such as EATER and EATEN for &amp;quot;eat&amp;quot;.
The FrameNet project proposes roles at an in-
termediate level, that of the semantic frame.
Frames are defined as schematic representa-
tions of situations involving various partici-
pants, props, and other conceptual roles (Fill-
more, 1976). For example, the frame &amp;quot;conver-
sation&amp;quot;, shown in Figure 1, is invoked by the
semantically related verbs &amp;quot;argue&amp;quot;, &amp;quot;banter&amp;quot;,
&amp;quot;debate&amp;quot;, &amp;quot;converse&amp;quot;, and &amp;quot;gossip&amp;quot; as well
as the nouns &amp;quot;argument&amp;quot;, &amp;quot;dispute&amp;quot;, &amp;quot;discus-
sion&amp;quot; and &amp;quot;tiff&amp;quot;. The roles defined for this
frame, and shared by all its lexical entries,
include PRoTAGoNiST1 and PRoTAGoNiST2
or simply PRoTAGoNiSTS for the participants
in the conversation, as well as MEDiUM, and
Topic. Example sentences are shown in Ta-
ble 1. Defining semantic roles at the frame
level avoids some of the difficulties of at-
tempting to find a small set of universal, ab-
stract thematic roles, or case roles such as
AGENT, PATiENT, etc (as in, among many
others, (Fillmore, 1968) (Jackendoff, 1972)).
Abstract thematic roles can be thought of
as being frame elements defined in abstract
frames such as &amp;quot;action&amp;quot; and &amp;quot;motion&amp;quot; which
are at the top of in inheritance hierarchy of
semantic frames (Fillmore and Baker, 2000).
The preliminary version of the FrameNet
corpus used for our experiments contained 67
frames from 12 general semantic domains cho-
sen for annotation. Examples of domains (see
Figure 1) include &amp;quot;motion&amp;quot;, &amp;quot;cognition&amp;quot; and
&amp;quot;communication&amp;quot;. Within these frames, ex-
amples of a total of 1462 distinct lexical pred-
icates, or target words, were annotated: 927
verbs, 339 nouns, and 175 adjectives. There
are a total of 49,013 annotated sentences, and
99,232 annotated frame elements (which do
not include the target words themselves).
</bodyText>
<sectionHeader confidence="0.999797" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999901871794872">
Assignment of semantic roles is an impor-
tant part of language understanding, and has
been attacked by many computational sys-
tems. Traditional parsing and understand-
ing systems, including implementations of
unification-based grammars such as HPSG
(Pollard and Sag, 1994), rely on hand-
developed grammars which must anticipate
each way in which semantic roles may be real-
ized syntactically. Writing such grammars is
time-consuming, and typically such systems
have limited coverage.
Data-driven techniques have recently been
applied to template-based semantic interpre-
tation in limited domains by &amp;quot;shallow&amp;quot; sys-
tems that avoid complex feature structures,
and often perform only shallow syntactic
analysis. For example, in the context of
the Air Traveler Information System (ATIS)
for spoken dialogue, Miller et al. (1996) com-
puted the probability that a constituent such
as &amp;quot;Atlanta&amp;quot; filled a semantic slot such as
DESTiNATioN in a semantic frame for air
travel. In a data-driven approach to infor-
mation extraction, Riloff (1993) builds a dic-
tionary of patterns for filling slots in a spe-
cific domain such as terrorist attacks, and
Riloff and Schmelzenbach (1998) extend this
technique to automatically derive entire case
frames for words in the domain. These last
systems make use of a limited amount of hand
labor to accept or reject automatically gen-
erated hypotheses. They show promise for
a more sophisticated approach to generalize
beyond the relatively small number of frames
considered in the tasks. More recently, a do-
main independent system has been trained on
general function tags such as MANNER and
TEMpoRAL by Blaheta and Charniak (2000).
</bodyText>
<sectionHeader confidence="0.999047" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.999971333333333">
We divide the task of labeling frame elements
into two subtasks: that of identifying the
boundaries of the frame elements in the sen-
tences, and that of labeling each frame ele-
ment, given its boundaries, with the correct
role. We first give results for a system which
</bodyText>
<figure confidence="0.998973097560976">
dispute−n
admire−v blame−n
appreciate−v
Domain: Cognition
Frame: Judgment Frame: Categorization
Frame Elements: Judge Frame Elements: Cognizer
Evaluee Item
Reason Category
Role Criterion
blame−v fault−n
admiration−n disapprove−v
confer−v
talk−v
discussion−n
Frame:
Frame Elements:
converse−v
Conversation
tiff−n
Domain: Communication
Protagonist−1
Protagonist−2
Protagonists
Topic
Medium
gossip−v
dispute−n
debate−v
Frame Elements: Speaker
Addressee
Message
Frame:
Frame: Questioning
Frame Elements: Speaker
Addressee
Message
Topic
Medium
Statement
Topic
Medium
</figure>
<figureCaption confidence="0.97524">
Figure 1: Sample domains and frames from the FrameNet lexicon.
</figureCaption>
<figure confidence="0.850913055555556">
Frame Element
Example (in italics) with target verb
Example (in italics) with target noun
Protagonist 1
Protagonist 2
Protagonists
Topic
Medium
Kim argued with Pat
Kim argued with Pat
Kim and Pat argued
Kim and Pat argued about politics
Kim and Pat argued in French
Kim had an argument with Pat
Kim had an argument with Pat
Kim and Pat had an argument
Kim and Pat had an argument about politics
Kim and pat had an argument in French
</figure>
<tableCaption confidence="0.7761945">
Table 1: Examples of semantic roles, or frame elements, for target words &amp;quot;argue&amp;quot; and &amp;quot;argu-
ment&amp;quot; from the &amp;quot;conversation&amp;quot; frame
</tableCaption>
<bodyText confidence="0.99899325">
labels roles using human-annotated bound-
aries, returning to the question of automat-
ically identifying the boundaries in Section
5.3.
</bodyText>
<subsectionHeader confidence="0.9966015">
4.1 Features Used in Assigning
Semantic Roles
</subsectionHeader>
<bodyText confidence="0.996716581395349">
The system is a statistical one, based on train-
ing a classifier on a labeled training set, and
testing on an unlabeled test set. The sys-
tem is trained by first using the Collins parser
(Collins, 1997) to parse the 36,995 train-
ing sentences, matching annotated frame el-
ements to parse constituents, and extracting
various features from the string of words and
the parse tree. During testing, the parser is
run on the test sentences and the same fea-
tures extracted. Probabilities for each possi-
ble semantic role r are then computed from
the features. The probability computation
will be described in the next section; the fea-
tures include:
Phrase Type: This feature indicates the
syntactic type of the phrase expressing
the semantic roles: examples include
noun phrase (NP), verb phrase (VP), and
clause (S). Phrase types were derived au-
tomatically from parse trees generated by
the parser, as shown in Figure 2. The
parse constituent spanning each set of
words annotated as a frame element was
found, and the constituent&apos;s nonterminal
label was taken as the phrase type. As
an example of how this feature is useful,
in communication frames, the SpEAKER
is likely appear a a noun phrase, Topic
as a prepositional phrase or noun phrase,
and MEDiUM as a prepostional phrase, as
in: &amp;quot;We talked about the proposal over
the phone.&amp;quot; When no parse constituent
was found with boundaries matching
those of a frame element during testing,
the largest constituent beginning at the
frame element&apos;s left boundary and lying
entirely within the element was used to
calculate the features.
Grammatical Function: This feature at-
tempts to indicate a constituent&apos;s syntac-
tic relation to the rest of the sentence,
He heard the sound of liquid slurping in a metal container as Farrell approached him from behind
</bodyText>
<figure confidence="0.999519470588235">
NP
IN
NNP
S
VP
SBAR
S
NP
VP
NP
NP PP
PRP VBD
VBD
PRP IN
NP
NN
Theme Target Goal Source
</figure>
<figureCaption confidence="0.940792">
Figure 2: A sample sentence with parser output (above) and FrameNet annotation (below).
Parse constituents corresponding to frame elements are highlighted.
</figureCaption>
<bodyText confidence="0.999845018018019">
for example as a subject or object of a
verb. As with phrase type, this feature
was read from parse trees returned by
the parser. After experimentation with
various versions of this feature, we re-
stricted it to apply only to NPs, as it was
found to have little effect on other phrase
types. Each NP&apos;s nearest S or VP ances-
tor was found in the parse tree; NPs with
an S ancestor were given the grammati-
cal function subject and those with a VP
ancestor were labeled object. In general,
agenthood is closely correlated with sub-
jecthood. For example, in the sentence
&amp;quot;He drove the car over the cliff&amp;quot;, the first
NP is more likely to fill the AGENT role
than the second or third.
Position: This feature simply indicates
whether the constituent to be labeled oc-
curs before or after the predicate defin-
ing the semantic frame. We expected
this feature to be highly correlated with
grammatical function, since subjects will
generally appear before a verb, and
objects after. Moreover, this feature
may overcome the shortcomings of read-
ing grammatical function from a con-
stituent&apos;s ancestors in the parse tree, as
well as errors in the parser output.
Voice: The distinction between active and
passive verbs plays an important role
in the connection between semantic role
and grammatical function, since direct
objects of active verbs correspond to sub-
jects of passive verbs. From the parser
output, verbs were classified as active or
passive by building a set of 10 passive-
identifying patterns. Each of the pat-
terns requires both a passive auxiliary
(some form of &amp;quot;to be&amp;quot; or &amp;quot;to get&amp;quot;) and a
past participle.
Head Word: As previously noted, we ex-
pected lexical dependencies to be ex-
tremely important in labeling semantic
roles, as indicated by their importance
in related tasks such as parsing. Since
the parser used assigns each constituent
a head word as an integral part of the
parsing model, we were able to read the
head words of the constituents from the
parser output. For example, in a commu-
nication frame, noun phrases headed by
&amp;quot;Bill&amp;quot;, &amp;quot;brother&amp;quot;, or &amp;quot;he&amp;quot; are more likely
to be the SpEAKER, while those headed
by &amp;quot;proposal&amp;quot;, &amp;quot;story&amp;quot;, or &amp;quot;question&amp;quot; are
more likely to be the Topic.
For our experiments, we divided the
FrameNet corpus as follows: one-tenth of the
annotated sentences for each target word were
reserved as a test set, and another one-tenth
were set aside as a tuning set for developing
our system. A few target words with fewer
than ten examples were removed from the cor-
pus. In our corpus, the average number of
sentences per target word is only 34, and the
number of sentences per frame is 732 — both
relatively small amounts of data on which to
train frame element classifiers.
Although we expect our features to inter-
act in various ways, the data are too sparse
to calculate probabilities directly on the full
set of features. For this reason, we built our
classifier by combining probabilities from dis-
tributions conditioned on a variety of combi-
nations of features.
An important caveat in using the FrameNet
database is that sentences are not chosen for
annotation at random, and therefore are not
necessarily statistically representative of the
corpus as a whole. Rather, examples are cho-
sen to illustrate typical usage patterns for
each word. We intend to remedy this in fu-
ture versions of this work by bootstrapping
our statistics using unannotated text.
Table 2 shows the probability distributions
used in the final version of the system. Cov-
erage indicates the percentage of the test data
for which the conditioning event had been
seen in training data. Accuracy is the propor-
tion of covered test data for which the correct
role is predicted, and Performance, simply
the product of coverage and accuracy, is the
overall percentage of test data for which the
correct role is predicted. Accuracy is some-
what similar to the familiar metric of pre-
cision in that it is calculated over cases for
which a decision is made, and performance is
similar to recall in that it is calculated over all
true frame elements. However, unlike a tradi-
tional precision/recall trade-off, these results
have no threshold to adjust, and the task is a
multi-way classification rather than a binary
decision. The distributions calculated were
simply the empirical distributions from the
training data. That is, occurrences of each
role and each set of conditioning events were
counted in a table, and probabilities calcu-
lated by dividing the counts for each role by
the total number of observations for each con-
ditioning event. For example, the distribution
P(rlpt, t) was calculated sas follows:
</bodyText>
<equation confidence="0.993792">
P(rlpt, t) = #(r, pt, t)
#(pt, t)
</equation>
<bodyText confidence="0.9996565">
Some sample probabilities calculated from
the training are shown in Table 3.
</bodyText>
<sectionHeader confidence="0.999677" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999639666666667">
Results for different methods of combining
the probability distributions described in the
previous section are shown in Table 4. The
linear interpolation method simply averages
the probabilities given by each of the distri-
butions in Table 2:
</bodyText>
<equation confidence="0.9990036">
P(rlconstituent) = A1P(rlt) +
A2P(rlpt,t) + A3P(rlpt,gf,t) +
A4P(rlpt, position, voice) +
A5P(rlpt, position, voice, t) + A6P(rlh) +
A7P(rlh, t) + A$P(rlh, pt, t)
</equation>
<bodyText confidence="0.9976875">
where Ei Ai = 1. The geometric mean, ex-
pressed in the log domain, is similar:
</bodyText>
<equation confidence="0.99358">
P(rlconstituent) = 1 Z exp{A1logP(rlt) +
A2logP(rlpt, t) + A3logP(rlpt, gf, t) +
A4logP(rlpt, position, voice) +
A5logP(rlpt, position, voice, t) +
A6logP(rlh) + A7logP(rlh, t) +
A$logP(rlh, pt, t)}
</equation>
<bodyText confidence="0.960726428571429">
where Z is a normalizing constant ensuring
that Pr P(rlconstituent) = 1.
The results shown in Table 4 reflect equal
values of A for each distribution defined for
the relevant conditioning event (but exclud-
ing distributions for which the conditioning
event was not seen in the training data).
</bodyText>
<table confidence="0.999865111111111">
Distribution Coverage Accuracy Performance
P(rjt) 100% 40.9% 40.9%
P(rjpt, t) 92.5 60.1 55.6
P(rjpt, gf, t) 92.0 66.6 61.3
P(rjpt, position, voice) 98.8 57.1 56.4
P(rjpt, position, voice, t) 90.8 70.1 63.7
P(rjh) 80.3 73.6 59.1
P(rjh, t) 56.0 86.6 48.5
P(rjh, pt, t) 50.1 87.4 43.8
</table>
<tableCaption confidence="0.9487165">
Table 2: Distributions Calculated for Semantic Role Identification: r indicates semantic role,
pt phrase type, gf grammatical function, h head word, and t target word, or predicate.
</tableCaption>
<equation confidence="0.960054555555555">
P(rjpt, gf, t)
Count in training data
P(r =AGTjpt =NP, gf =Subj, t =abduct) = :46
P(r =THMjpt =NP, gf =Subj, t =abduct) = :54
P(r =THMjpt =NP, gf =Obj, t =abduct) = 1
P(r =AGTjpt =PP, t =abduct) = :33
P(r =THMjpt =PP, t =abduct) = :33
P(r =COTHMjpt =PP, t =abduct) = :33
P(r =MANRjpt =ADVP, t =abduct) = 1
</equation>
<bodyText confidence="0.96289875">
Table 3: Sample probabilities for P(rjpt, gf, t) calculated from training data for the verb abduct.
The variable gf is only defined for noun phrases. The roles defined for the removing frame in
the motion domain are: AGENT, THEME, COTHEME (&amp;quot;... had been abducted with him&amp;quot;) and
MANNER.
</bodyText>
<figure confidence="0.975350857142857">
6
7
9
1
1
1
1
</figure>
<bodyText confidence="0.999427714285714">
Other schemes for choosing values of A, in-
cluding giving more weight to distributions
for which more training data was available,
were found to have relatively little effect. We
attribute this to the fact that the evaluation
depends only the the ranking of the probabil-
ities rather than their exact values.
</bodyText>
<figureCaption confidence="0.948019333333333">
Figure 3: Lattice organization of the distri-
butions from Table 2, with more specific dis-
tributions towards the top.
</figureCaption>
<bodyText confidence="0.999329222222222">
In the &amp;quot;backoff&amp;quot; combination method, a
lattice was constructed over the distributions
in Table 2 from more specific conditioning
events to less specific, as shown in Figure
3. The less specific distributions were used
only when no data was present for any more
specific distribution. As before, probabilities
were combined with both linear interpolation
and a geometric mean.
</bodyText>
<table confidence="0.998453333333333">
Combining Method Correct
Linear Interpolation 79.5%
Geometric Mean 79.6
Backoff, linear interpolation 80.4
Backoff, geometric mean 79.6
Baseline: Most common role 40.9
</table>
<tableCaption confidence="0.940772">
Table 4: Results on Development Set, 8148
observations
</tableCaption>
<bodyText confidence="0.999321571428571">
The final system performed at 80.4% ac-
curacy, which can be compared to the 40.9%
achieved by always choosing the most prob-
able role for each target word, essentially
chance performance on this task. Results for
this system on test data, held out during de-
velopment of the system, are shown in Table
</bodyText>
<equation confidence="0.982776428571429">
P(r  |h, pt, t)
P(r  |pt, gf, t)
P(r  |pt, position, voice, t)
P(r  |h)
P(r  |t)
P(r  |h, t) P(r  |pt, t) P(r  |pt, position, voice)
Baseline
</equation>
<bodyText confidence="0.987401">
correct, without use of any of the syntactic
features.
</bodyText>
<figure confidence="0.88077675">
Linear
Backoff
80.4%
76.9
</figure>
<tableCaption confidence="0.700841666666667">
Table 5: Results on Test Set, using backoff
linear interpolation system. The test set con-
sists of 7900 observations.
</tableCaption>
<sectionHeader confidence="0.423839" genericHeader="evaluation">
5.
</sectionHeader>
<subsectionHeader confidence="0.953857">
5.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999981378378379">
It is interesting to note that looking at a con-
stituent&apos;s position relative to the target word
along with active/passive information per-
formed as well as reading grammatical func-
tion off the parse tree. A system using gram-
matical function, along with the head word,
phrase type, and target word, but no passive
information, scored 79.2%. A similar system
using position rather than grammatical func-
tion scored 78.8% — nearly identical perfor-
mance. However, using head word, phrase
type, and target word without either position
or grammatical function yielded only 76.3%,
indicating that while the two features accom-
plish a similar goal, it is important to include
some measure of the constituent&apos;s syntactic
relationship to the target word. Our final sys-
tem incorporated both features, giving a fur-
ther, though not significant, improvement. As
a guideline for interpreting these results, with
8176 observations, the threshold for statisti-
cal signifance with p &lt; :05 is a 1.0% absolute
difference in performance.
Use of the active/passive feature made a
further improvement: our system using po-
sition but no grammatical function or pas-
sive information scored 78.8%; adding passive
information brought performance to 80.5%.
Roughly 5% of the examples were identified
as passive uses.
Head words proved to be very accurate in-
dicators of a constituent&apos;s semantic role when
data was available for a given head word,
confirming the importance of lexicalization
shown in various other tasks. While the dis-
tribution P(rlh, t) can only be evaluated for
56.0% of the data, of those cases it gets 86.7%
</bodyText>
<subsectionHeader confidence="0.998732">
5.2 Lexical Clustering
</subsectionHeader>
<bodyText confidence="0.999993060606061">
In order to address the sparse coverage of lex-
ical head word statistics, an experiment was
carried out using an automatic clustering of
head words of the type described in (Lin,
1998). A soft clustering of nouns was per-
formed by applying the co-occurrence model
of (Hofmann and Puzicha, 1998) to a large
corpus of observed direct object relationships
between verbs and nouns. The clustering was
computed from an automatically parsed ver-
sion of the British National Corpus, using the
parser of (Carroll and Rooth, 1998). The ex-
periment was performed using only frame el-
ements with a noun as head word. This al-
lowed a smoothed estimate of P(rIh, nt, t) to
be computed as &amp; P(rJc, nt, t)P(cJh), sum-
ming over the automatically derived clusters c
to which a nominal head word h might belong.
This allows the use of head word statistics
even when the headword h has not been seen
in conjunction was the target word t in the
training data. While the unclustered nominal
head word feature is correct for 87.6% of cases
where data for P(rlh, nt, t) is available, such
data was available for only 43.7% of nominal
head words. The clustered head word alone
correctly classified 79.7% of the cases where
the head word was in the vocabulary used
for clustering; 97.9% of instances of nominal
head words were in the vocabulary. Adding
clustering statistics for NP constituents into
the full system increased overall performance
from 80.4% to 81.2%.
</bodyText>
<subsectionHeader confidence="0.9329655">
5.3 Automatic Identification of
Frame Element Boundaries
</subsectionHeader>
<bodyText confidence="0.999980444444444">
The experiments described above have used
human annotated frame element boundaries
— here we address how well the frame ele-
ments can be found automatically. Exper-
iments were conducted using features simi-
lar to those described above to identify con-
stituents in a sentence&apos;s parse tree that were
likely to be frame elements. The system
was given the human-annotated target word
</bodyText>
<figure confidence="0.967413">
Development Set
Test Set
40.9%
40.6%
</figure>
<bodyText confidence="0.9903355">
and the frame as inputs, whereas a full lan-
guage understanding system would also iden-
tify which frames come into play in a sen-
tence — essentially the task of word sense
disambiguation. The main feature used was
the path from the target word through the
parse tree to the constituent in question, rep-
resented as a string of parse tree nonterminals
</bodyText>
<figureCaption confidence="0.7095225">
linked by symbols indicating upward or down-
ward movement through the tree, as shown in
Figure 4.
Figure 4: In this example, the path from the
</figureCaption>
<bodyText confidence="0.968449365853658">
frame element &amp;quot;He&amp;quot; to the target word &amp;quot;ate&amp;quot;
can be represented as NP &amp;quot; S # VP # V, with
&amp;quot; indicating upward movement in the parse
tree and # downward movement.
The other features used were the iden-
tity of the target word and the identity of
the constituent&apos;s head word. The probabil-
ity distributions calculated from the train-
ing data were P(fejpath), P(fejpath, t), and
P(fejh, t), where fe indicates an event where
the parse constituent in question is a frame el-
ement, path the path through the parse tree
from the target word to the parse constituent,
t the identity of the target word, and h the
head word of the parse constituent. By vary-
ing the probability threshold at which a deci-
sion is made, one can plot a precision/recall
curve as shown in Figure 5. P(fejpath, t)
performs relatively poorly due to fragmenta-
tion of the training data (recall only about 30
sentences are available for each target word).
While the lexical statistic P(fejh, t) alone is
not useful as a classifier, using it in linear in-
terpolation with the path statistics improves
results. Note that this method can only iden-
tify frame elements that have a correspond-
ing constituent in the automatically gener-
ated parse tree. For this reason, it is inter-
esting to calculate how many true frame el-
ements overlap with the results of the sys-
tem, relaxing the criterion that the bound-
aries must match exactly. Results for partial
matching are shown in Table 6.
When the automatically identified con-
stituents were fed through the role labeling
system described above, 79.6% of the con-
stituents which had been correctly identified
in the first stage were assigned the correct role
in the second, roughly equivalent to the per-
formance when assigning roles to constituents
identified by hand.
</bodyText>
<figureCaption confidence="0.86367875">
Figure 5: Precison/Recall plot for various
methods of identifying frame elements. Recall
is calculated over only frame elements with
matching parse constituents.
</figureCaption>
<sectionHeader confidence="0.995036" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999986230769231">
Our preliminary system is able to automati-
cally label semantic roles with fairly high ac-
curacy, indicating promise for applications in
various natural language tasks. Lexical statis-
tics computed on constituent head words were
found to be the most important of the fea-
tures used. While lexical statistics are quite
accurate on the data covered by observations
in the training set, the sparsity of the data
when conditioned on lexical items meant that
combining features was the key to high over-
all performance. While the combined sys-
tem was far more accurate than any feature
</bodyText>
<figure confidence="0.990095586206896">
S
NP VP
V NP
Det N
He
Pro
ate
frametarget pancakes
element
some
word
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
precision
recall
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.
0
1
1
P(fe|path)
P(fe|path, t)
.75*P(fe  |path)+.25*P(fe  |h, t)
</figure>
<table confidence="0.367950833333333">
Type of Overlap Identified Constituents Number
Exactly Matching Boundaries 66% 5421
Identified constituent entirely within true frame element 8 663
True frame element entirely within identified constituent 7 599
Partial overlap 0 26
No match to true frame element 13 972
</table>
<tableCaption confidence="0.492143">
Table 6: Results on Identifying Frame Elements (FEs), including partial matches. Results
</tableCaption>
<bodyText confidence="0.996427333333333">
obtained using P(fejpath) with threshold at .5. A total of 7681 constituents were identified as
FEs, 8167 FEs were present in hand annotations, of which matching parse constituents were
present for 7053 (86%).
taken alone, the specific method of combina-
tion used was less important.
We plan to continue this work by integrat-
ing semantic role identification with parsing,
by bootstrapping the system on larger, and
more representative, amounts of data, and by
attempting to generalize from the set of pred-
icates chosen by FrameNet for annotation to
general text.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99948196875">
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The berkeley framenet project.
In Proceedings of the COLING-ACL, Montreal,
Canada.
Dan Blaheta and Eugene Charniak. 2000. As-
signing function tags to parsed text. In Pro-
ceedings of the 1st Annual Meeting of the North
American Chapter of the ACL (NAACL), Seat-
tle, Washington.
Glenn Carroll and Mats Rooth. 1998. Va-
lence induction with a head-lexicalized pcfg. In
Proceedings of the 3rd Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP 3), Granada, Spain.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
ACL.
Charles J. Fillmore and Collin F. Baker. 2000.
Framenet: Frame semantics meets the corpus.
In Linguistic Society of America, January.
Charles Fillmore. 1968. The case for case. In
Bach and Harms, editors, Universals in Lin-
guistic Theory, pages 1-88. Holt, Rinehart, and
Winston, New York.
Charles J. Fillmore. 1976. Frame semantics
and the nature of language. In Annals of the
New York Academy of Sciences: Conference on
the Origin and Development of Language and
Speech, volume 280, pages 20{32.
Marti Hearst. 1999. Untangling text data mining.
In Proceedings of the 37rd Annual Meeting of
the ACL.
Thomas Hofmann and Jan Puzicha. 1998. Sta-
tistical models for co-occurrence data. Memo,
Massachussetts Institute of Technology Artifi-
cial Intelligence Laboratory, February.
Ray Jackendoff. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge,
Massachusetts.
Maria Lapata and Chris Brew. 1999. Using
subcategorization to resolve verb class ambigu-
ity. In Joint SIGDAT Conference on Empiri-
cal Methods in NLP and Very Large Corpora,
Maryland.
Dekang Lin. 1998. Automatic retrieval and clus-
tering of similar words. In Proceedings of the
COLING-ACL, Montreal, Canada.
Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical
approach to natural language interfaces. In
Proceedings of the 34th Annual Meeting of the
ACL.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University
of Chicago Press, Chicago.
Ellen Riloff and Mark Schmelzenbach. 1998. An
empirical approach to conceptual case frame ac-
quisition. In Proceedings of the Sixth Workshop
on Very Large Corpora.
Ellen Riloff. 1993. Automatically constructing
a dictionary for information extraction tasks.
In Proceedings of the Eleventh National Con-
ference on Artificial Intelligence (AAAI).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806973">
<title confidence="0.999856">Automatic Labeling of Semantic Roles</title>
<author confidence="0.99992">Daniel Gildea Daniel Jurafsky</author>
<affiliation confidence="0.9170675">University of California, Berkeley, and Department of Linguistics International Computer Science Institute University of Colorado, Boulder</affiliation>
<email confidence="0.999701">gildea@cs.berkeley.edujurafsky@colorado.edu</email>
<abstract confidence="0.9967285">We present a system for identifythe semantic relationships, or sefilled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2974" citStr="Baker et al., 1998" startWordPosition="447" endWordPosition="450">ore accurate parsers and better language models for speech recognition. This paper proposes an algorithm for automatic semantic analysis, assigning a semantic role to constituents in a sentence. Our approach to semantic analysis is to treat the problem of semantic role labeling like the similar problems of parsing, part of speech tagging, and word sense disambiguation. We apply statistical techniques that have been successful for these tasks, including probabilistic parsing and statistical classification. Our statistical algorithms are trained on a hand-labeled dataset: the FrameNet database (Baker et al., 1998). The FrameNet database defines a tagset of semantic roles called frame elements, and includes roughly 50,000 sentences from the British National Corpus which have been hand-labeled with these frame elements. The next section describes the set of frame elements/semantic roles used by our system. In the rest of this paper we report on our current system, as well as a number of preliminary experiments on extensions to the system. 2 Semantic Roles Historically, two types of semantic roles have been studied: abstract roles such as AGENT and PATiENT, and roles specific to individual verbs such as E</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Blaheta</author>
<author>Eugene Charniak</author>
</authors>
<title>Assigning function tags to parsed text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Chapter of the ACL (NAACL),</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="6967" citStr="Blaheta and Charniak (2000)" startWordPosition="1077" endWordPosition="1080">1993) builds a dictionary of patterns for filling slots in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalize beyond the relatively small number of frames considered in the tasks. More recently, a domain independent system has been trained on general function tags such as MANNER and TEMpoRAL by Blaheta and Charniak (2000). 4 Methodology We divide the task of labeling frame elements into two subtasks: that of identifying the boundaries of the frame elements in the sentences, and that of labeling each frame element, given its boundaries, with the correct role. We first give results for a system which dispute−n admire−v blame−n appreciate−v Domain: Cognition Frame: Judgment Frame: Categorization Frame Elements: Judge Frame Elements: Cognizer Evaluee Item Reason Category Role Criterion blame−v fault−n admiration−n disapprove−v confer−v talk−v discussion−n Frame: Frame Elements: converse−v Conversation tiff−n Domai</context>
</contexts>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>Dan Blaheta and Eugene Charniak. 2000. Assigning function tags to parsed text. In Proceedings of the 1st Annual Meeting of the North American Chapter of the ACL (NAACL), Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized pcfg.</title>
<date>1998</date>
<booktitle>In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing (EMNLP 3),</booktitle>
<location>Granada,</location>
<contexts>
<context position="21208" citStr="Carroll and Rooth, 1998" startWordPosition="3432" endWordPosition="3435">(rlh, t) can only be evaluated for 56.0% of the data, of those cases it gets 86.7% 5.2 Lexical Clustering In order to address the sparse coverage of lexical head word statistics, an experiment was carried out using an automatic clustering of head words of the type described in (Lin, 1998). A soft clustering of nouns was performed by applying the co-occurrence model of (Hofmann and Puzicha, 1998) to a large corpus of observed direct object relationships between verbs and nouns. The clustering was computed from an automatically parsed version of the British National Corpus, using the parser of (Carroll and Rooth, 1998). The experiment was performed using only frame elements with a noun as head word. This allowed a smoothed estimate of P(rIh, nt, t) to be computed as &amp; P(rJc, nt, t)P(cJh), summing over the automatically derived clusters c to which a nominal head word h might belong. This allows the use of head word statistics even when the headword h has not been seen in conjunction was the target word t in the training data. While the unclustered nominal head word feature is correct for 87.6% of cases where data for P(rlh, nt, t) is available, such data was available for only 43.7% of nominal head words. Th</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized pcfg. In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing (EMNLP 3), Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="8813" citStr="Collins, 1997" startWordPosition="1363" endWordPosition="1364">Kim and Pat had an argument Kim and Pat had an argument about politics Kim and pat had an argument in French Table 1: Examples of semantic roles, or frame elements, for target words &amp;quot;argue&amp;quot; and &amp;quot;argument&amp;quot; from the &amp;quot;conversation&amp;quot; frame labels roles using human-annotated boundaries, returning to the question of automatically identifying the boundaries in Section 5.3. 4.1 Features Used in Assigning Semantic Roles The system is a statistical one, based on training a classifier on a labeled training set, and testing on an unlabeled test set. The system is trained by first using the Collins parser (Collins, 1997) to parse the 36,995 training sentences, matching annotated frame elements to parse constituents, and extracting various features from the string of words and the parse tree. During testing, the parser is run on the test sentences and the same features extracted. Probabilities for each possible semantic role r are then computed from the features. The probability computation will be described in the next section; the features include: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and claus</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Collin F Baker</author>
</authors>
<title>Framenet: Frame semantics meets the corpus.</title>
<date>2000</date>
<booktitle>In Linguistic Society of America,</booktitle>
<contexts>
<context position="4780" citStr="Fillmore and Baker, 2000" startWordPosition="738" endWordPosition="741"> and PRoTAGoNiST2 or simply PRoTAGoNiSTS for the participants in the conversation, as well as MEDiUM, and Topic. Example sentences are shown in Table 1. Defining semantic roles at the frame level avoids some of the difficulties of attempting to find a small set of universal, abstract thematic roles, or case roles such as AGENT, PATiENT, etc (as in, among many others, (Fillmore, 1968) (Jackendoff, 1972)). Abstract thematic roles can be thought of as being frame elements defined in abstract frames such as &amp;quot;action&amp;quot; and &amp;quot;motion&amp;quot; which are at the top of in inheritance hierarchy of semantic frames (Fillmore and Baker, 2000). The preliminary version of the FrameNet corpus used for our experiments contained 67 frames from 12 general semantic domains chosen for annotation. Examples of domains (see Figure 1) include &amp;quot;motion&amp;quot;, &amp;quot;cognition&amp;quot; and &amp;quot;communication&amp;quot;. Within these frames, examples of a total of 1462 distinct lexical predicates, or target words, were annotated: 927 verbs, 339 nouns, and 175 adjectives. There are a total of 49,013 annotated sentences, and 99,232 annotated frame elements (which do not include the target words themselves). 3 Related Work Assignment of semantic roles is an important part of langua</context>
</contexts>
<marker>Fillmore, Baker, 2000</marker>
<rawString>Charles J. Fillmore and Collin F. Baker. 2000. Framenet: Frame semantics meets the corpus. In Linguistic Society of America, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>The case for case.</title>
<date>1968</date>
<booktitle>Universals in Linguistic Theory,</booktitle>
<pages>1--88</pages>
<editor>In Bach and Harms, editors,</editor>
<location>Holt, Rinehart, and Winston, New York.</location>
<contexts>
<context position="4541" citStr="Fillmore, 1968" startWordPosition="702" endWordPosition="703">elated verbs &amp;quot;argue&amp;quot;, &amp;quot;banter&amp;quot;, &amp;quot;debate&amp;quot;, &amp;quot;converse&amp;quot;, and &amp;quot;gossip&amp;quot; as well as the nouns &amp;quot;argument&amp;quot;, &amp;quot;dispute&amp;quot;, &amp;quot;discussion&amp;quot; and &amp;quot;tiff&amp;quot;. The roles defined for this frame, and shared by all its lexical entries, include PRoTAGoNiST1 and PRoTAGoNiST2 or simply PRoTAGoNiSTS for the participants in the conversation, as well as MEDiUM, and Topic. Example sentences are shown in Table 1. Defining semantic roles at the frame level avoids some of the difficulties of attempting to find a small set of universal, abstract thematic roles, or case roles such as AGENT, PATiENT, etc (as in, among many others, (Fillmore, 1968) (Jackendoff, 1972)). Abstract thematic roles can be thought of as being frame elements defined in abstract frames such as &amp;quot;action&amp;quot; and &amp;quot;motion&amp;quot; which are at the top of in inheritance hierarchy of semantic frames (Fillmore and Baker, 2000). The preliminary version of the FrameNet corpus used for our experiments contained 67 frames from 12 general semantic domains chosen for annotation. Examples of domains (see Figure 1) include &amp;quot;motion&amp;quot;, &amp;quot;cognition&amp;quot; and &amp;quot;communication&amp;quot;. Within these frames, examples of a total of 1462 distinct lexical predicates, or target words, were annotated: 927 verbs, 339</context>
</contexts>
<marker>Fillmore, 1968</marker>
<rawString>Charles Fillmore. 1968. The case for case. In Bach and Harms, editors, Universals in Linguistic Theory, pages 1-88. Holt, Rinehart, and Winston, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics and the nature of language.</title>
<date>1976</date>
<booktitle>In Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech,</booktitle>
<volume>280</volume>
<pages>20--32</pages>
<contexts>
<context position="3834" citStr="Fillmore, 1976" startWordPosition="586" endWordPosition="588">f frame elements/semantic roles used by our system. In the rest of this paper we report on our current system, as well as a number of preliminary experiments on extensions to the system. 2 Semantic Roles Historically, two types of semantic roles have been studied: abstract roles such as AGENT and PATiENT, and roles specific to individual verbs such as EATER and EATEN for &amp;quot;eat&amp;quot;. The FrameNet project proposes roles at an intermediate level, that of the semantic frame. Frames are defined as schematic representations of situations involving various participants, props, and other conceptual roles (Fillmore, 1976). For example, the frame &amp;quot;conversation&amp;quot;, shown in Figure 1, is invoked by the semantically related verbs &amp;quot;argue&amp;quot;, &amp;quot;banter&amp;quot;, &amp;quot;debate&amp;quot;, &amp;quot;converse&amp;quot;, and &amp;quot;gossip&amp;quot; as well as the nouns &amp;quot;argument&amp;quot;, &amp;quot;dispute&amp;quot;, &amp;quot;discussion&amp;quot; and &amp;quot;tiff&amp;quot;. The roles defined for this frame, and shared by all its lexical entries, include PRoTAGoNiST1 and PRoTAGoNiST2 or simply PRoTAGoNiSTS for the participants in the conversation, as well as MEDiUM, and Topic. Example sentences are shown in Table 1. Defining semantic roles at the frame level avoids some of the difficulties of attempting to find a small set of universal, abs</context>
</contexts>
<marker>Fillmore, 1976</marker>
<rawString>Charles J. Fillmore. 1976. Frame semantics and the nature of language. In Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech, volume 280, pages 20{32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Untangling text data mining.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37rd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2263" citStr="Hearst, 1999" startWordPosition="343" endWordPosition="344">he roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the different syntactic subcatgorization frames of a verb like &amp;quot;serve&amp;quot; can be used to help disambiguate a particular instance of the word &amp;quot;serve&amp;quot;. Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging field of Text Data Mining (TDM) (Hearst, 1999). Finally, incorporating semantic roles into probabilistic models of language should yield more accurate parsers and better language models for speech recognition. This paper proposes an algorithm for automatic semantic analysis, assigning a semantic role to constituents in a sentence. Our approach to semantic analysis is to treat the problem of semantic role labeling like the similar problems of parsing, part of speech tagging, and word sense disambiguation. We apply statistical techniques that have been successful for these tasks, including probabilistic parsing and statistical classificatio</context>
</contexts>
<marker>Hearst, 1999</marker>
<rawString>Marti Hearst. 1999. Untangling text data mining. In Proceedings of the 37rd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
<author>Jan Puzicha</author>
</authors>
<title>Statistical models for co-occurrence data.</title>
<date>1998</date>
<institution>Memo, Massachussetts Institute of Technology Artificial Intelligence Laboratory,</institution>
<contexts>
<context position="20982" citStr="Hofmann and Puzicha, 1998" startWordPosition="3396" endWordPosition="3399"> Head words proved to be very accurate indicators of a constituent&apos;s semantic role when data was available for a given head word, confirming the importance of lexicalization shown in various other tasks. While the distribution P(rlh, t) can only be evaluated for 56.0% of the data, of those cases it gets 86.7% 5.2 Lexical Clustering In order to address the sparse coverage of lexical head word statistics, an experiment was carried out using an automatic clustering of head words of the type described in (Lin, 1998). A soft clustering of nouns was performed by applying the co-occurrence model of (Hofmann and Puzicha, 1998) to a large corpus of observed direct object relationships between verbs and nouns. The clustering was computed from an automatically parsed version of the British National Corpus, using the parser of (Carroll and Rooth, 1998). The experiment was performed using only frame elements with a noun as head word. This allowed a smoothed estimate of P(rIh, nt, t) to be computed as &amp; P(rJc, nt, t)P(cJh), summing over the automatically derived clusters c to which a nominal head word h might belong. This allows the use of head word statistics even when the headword h has not been seen in conjunction was</context>
</contexts>
<marker>Hofmann, Puzicha, 1998</marker>
<rawString>Thomas Hofmann and Jan Puzicha. 1998. Statistical models for co-occurrence data. Memo, Massachussetts Institute of Technology Artificial Intelligence Laboratory, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Interpretation in Generative Grammar.</title>
<date>1972</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="4560" citStr="Jackendoff, 1972" startWordPosition="704" endWordPosition="705">ue&amp;quot;, &amp;quot;banter&amp;quot;, &amp;quot;debate&amp;quot;, &amp;quot;converse&amp;quot;, and &amp;quot;gossip&amp;quot; as well as the nouns &amp;quot;argument&amp;quot;, &amp;quot;dispute&amp;quot;, &amp;quot;discussion&amp;quot; and &amp;quot;tiff&amp;quot;. The roles defined for this frame, and shared by all its lexical entries, include PRoTAGoNiST1 and PRoTAGoNiST2 or simply PRoTAGoNiSTS for the participants in the conversation, as well as MEDiUM, and Topic. Example sentences are shown in Table 1. Defining semantic roles at the frame level avoids some of the difficulties of attempting to find a small set of universal, abstract thematic roles, or case roles such as AGENT, PATiENT, etc (as in, among many others, (Fillmore, 1968) (Jackendoff, 1972)). Abstract thematic roles can be thought of as being frame elements defined in abstract frames such as &amp;quot;action&amp;quot; and &amp;quot;motion&amp;quot; which are at the top of in inheritance hierarchy of semantic frames (Fillmore and Baker, 2000). The preliminary version of the FrameNet corpus used for our experiments contained 67 frames from 12 general semantic domains chosen for annotation. Examples of domains (see Figure 1) include &amp;quot;motion&amp;quot;, &amp;quot;cognition&amp;quot; and &amp;quot;communication&amp;quot;. Within these frames, examples of a total of 1462 distinct lexical predicates, or target words, were annotated: 927 verbs, 339 nouns, and 175 adj</context>
</contexts>
<marker>Jackendoff, 1972</marker>
<rawString>Ray Jackendoff. 1972. Semantic Interpretation in Generative Grammar. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Chris Brew</author>
</authors>
<title>Using subcategorization to resolve verb class ambiguity.</title>
<date>1999</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora,</booktitle>
<location>Maryland.</location>
<contexts>
<context position="1743" citStr="Lapata and Brew (1999)" startWordPosition="261" endWordPosition="264">es to extract facts about, for example, financial news or interesting political events. A shallow semantic level of representation is a more domain-independent, robust level of representation. Identifying these roles, for example, could allow a system to determine that in the sentence &amp;quot;The first one crashed&amp;quot; the subject is the vehicle, but in the sentence &amp;quot;The first one crashed it&amp;quot; the subject is the agent, which would help in information extraction in this domain. Another application is in wordsense disambiguation, where the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the different syntactic subcatgorization frames of a verb like &amp;quot;serve&amp;quot; can be used to help disambiguate a particular instance of the word &amp;quot;serve&amp;quot;. Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging field of Text Data Mining (TDM) (Hearst, 1999). Finally, incorporating semantic roles into probabilistic models of language sh</context>
</contexts>
<marker>Lapata, Brew, 1999</marker>
<rawString>Maria Lapata and Chris Brew. 1999. Using subcategorization to resolve verb class ambiguity. In Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="20873" citStr="Lin, 1998" startWordPosition="3380" endWordPosition="3381">ion brought performance to 80.5%. Roughly 5% of the examples were identified as passive uses. Head words proved to be very accurate indicators of a constituent&apos;s semantic role when data was available for a given head word, confirming the importance of lexicalization shown in various other tasks. While the distribution P(rlh, t) can only be evaluated for 56.0% of the data, of those cases it gets 86.7% 5.2 Lexical Clustering In order to address the sparse coverage of lexical head word statistics, an experiment was carried out using an automatic clustering of head words of the type described in (Lin, 1998). A soft clustering of nouns was performed by applying the co-occurrence model of (Hofmann and Puzicha, 1998) to a large corpus of observed direct object relationships between verbs and nouns. The clustering was computed from an automatically parsed version of the British National Corpus, using the parser of (Carroll and Rooth, 1998). The experiment was performed using only frame elements with a noun as head word. This allowed a smoothed estimate of P(rIh, nt, t) to be computed as &amp; P(rJc, nt, t)P(cJh), summing over the automatically derived clusters c to which a nominal head word h might belo</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>David Stallard</author>
<author>Robert Bobrow</author>
<author>Richard Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="6137" citStr="Miller et al. (1996)" startWordPosition="941" endWordPosition="944">ations of unification-based grammars such as HPSG (Pollard and Sag, 1994), rely on handdeveloped grammars which must anticipate each way in which semantic roles may be realized syntactically. Writing such grammars is time-consuming, and typically such systems have limited coverage. Data-driven techniques have recently been applied to template-based semantic interpretation in limited domains by &amp;quot;shallow&amp;quot; systems that avoid complex feature structures, and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as &amp;quot;Atlanta&amp;quot; filled a semantic slot such as DESTiNATioN in a semantic frame for air travel. In a data-driven approach to information extraction, Riloff (1993) builds a dictionary of patterns for filling slots in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approa</context>
</contexts>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>Scott Miller, David Stallard, Robert Bobrow, and Richard Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proceedings of the 34th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>HeadDriven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="5590" citStr="Pollard and Sag, 1994" startWordPosition="861" endWordPosition="864">lude &amp;quot;motion&amp;quot;, &amp;quot;cognition&amp;quot; and &amp;quot;communication&amp;quot;. Within these frames, examples of a total of 1462 distinct lexical predicates, or target words, were annotated: 927 verbs, 339 nouns, and 175 adjectives. There are a total of 49,013 annotated sentences, and 99,232 annotated frame elements (which do not include the target words themselves). 3 Related Work Assignment of semantic roles is an important part of language understanding, and has been attacked by many computational systems. Traditional parsing and understanding systems, including implementations of unification-based grammars such as HPSG (Pollard and Sag, 1994), rely on handdeveloped grammars which must anticipate each way in which semantic roles may be realized syntactically. Writing such grammars is time-consuming, and typically such systems have limited coverage. Data-driven techniques have recently been applied to template-based semantic interpretation in limited domains by &amp;quot;shallow&amp;quot; systems that avoid complex feature structures, and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. HeadDriven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Mark Schmelzenbach</author>
</authors>
<title>An empirical approach to conceptual case frame acquisition.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="6479" citStr="Riloff and Schmelzenbach (1998)" startWordPosition="998" endWordPosition="1001">d to template-based semantic interpretation in limited domains by &amp;quot;shallow&amp;quot; systems that avoid complex feature structures, and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as &amp;quot;Atlanta&amp;quot; filled a semantic slot such as DESTiNATioN in a semantic frame for air travel. In a data-driven approach to information extraction, Riloff (1993) builds a dictionary of patterns for filling slots in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalize beyond the relatively small number of frames considered in the tasks. More recently, a domain independent system has been trained on general function tags such as MANNER and TEMpoRAL by Blaheta and Charniak (2000). 4 Methodology We divide the task of labeling frame elements into two subtasks: that of identifying the boundar</context>
</contexts>
<marker>Riloff, Schmelzenbach, 1998</marker>
<rawString>Ellen Riloff and Mark Schmelzenbach. 1998. An empirical approach to conceptual case frame acquisition. In Proceedings of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<booktitle>In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="6345" citStr="Riloff (1993)" startWordPosition="977" endWordPosition="978">s time-consuming, and typically such systems have limited coverage. Data-driven techniques have recently been applied to template-based semantic interpretation in limited domains by &amp;quot;shallow&amp;quot; systems that avoid complex feature structures, and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as &amp;quot;Atlanta&amp;quot; filled a semantic slot such as DESTiNATioN in a semantic frame for air travel. In a data-driven approach to information extraction, Riloff (1993) builds a dictionary of patterns for filling slots in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalize beyond the relatively small number of frames considered in the tasks. More recently, a domain independent system has been trained on general function tags such as MANNER and TEMpoRAL by Blahe</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>