<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9975">
Lexicalising Word Order Constraints for
Implemented Linearisation Grammar
</title>
<author confidence="0.978409">
Yo Sato
</author>
<affiliation confidence="0.8871385">
Department of Computer Science
King’s College London
</affiliation>
<email confidence="0.996893">
yo.sato@kcl.ac.uk
</email>
<sectionHeader confidence="0.996632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866727272727">
This paper presents a way in which a lex-
icalised HPSG grammar can handle word
order constraints in a computational pars-
ing system, without invoking an additional
layer of representation for word order,
such as Reape’s Word Order Domain. The
key proposal is to incorporate into lexi-
cal heads the WOC (Word Order Con-
straints) feature, which is used to constrain
the word order of its projection. We also
overview our parsing algorithm.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930175">
It is a while since the linearisation technique was
introduced into HPSG by Reape (1993; 1994) as
a way to overcome the inadequacy of the con-
ventional phrase structure rule based grammars in
handling ‘freer’ word order of languages such as
German and Japanese. In parallel in computa-
tional linguistics, it has long been proposed that
more flexible parsing techniques may be required
to adequately handle such languages, but hitherto
a practical system using linearisation has eluded
large-scale implementation. There are at least two
obstacles: its higher computational cost accom-
panied with non-CFG algorithms it requires, and
the difficulty to state word order information suc-
cinctly in a grammar that works well with a non-
CFG parsing engine.
In a recent development, the ‘cost’ issue has
been tackled by Daniels and Meurers (2004), who
propose to narrow down on search space while us-
ing a non-CFG algorithm. The underlying princi-
ple is to give priority to the full generative capac-
ity, let the parser overgenerate at default but re-
strict generation for efficiency thereafter. While
sharing this principle, I will attempt to further
streamline the computation of linearisation, focus-
ing mainly on the issue of grammar formalism.
Specifically, I would like to show that the lex-
icalisation of word order constraints is possible
with some conservative modifications to the stan-
dard HPSG (Pollard and Sag, 1987; Pollard and
Sag, 1994). This will have the benefit of making
the representation of linearisation grammar sim-
pler and more parsing friendly than Reape’s influ-
ential Word Order Domain theory.
In what follows, after justifying the need for
non-CFG parsing and reviewing Reape’s theory, I
will propose to introduce into HPSG the Word Or-
der Constraint (WOC) feature for lexical heads. I
will then describe the parsing algorithm that refers
to this feature to constrain the search for efficiency.
</bodyText>
<subsectionHeader confidence="0.999366">
1.1 Limitation of CFG Parsing
</subsectionHeader>
<bodyText confidence="0.999979266666667">
One of the main obstacles for CFG parsing is
the discontinuity in natural languages caused by
‘interleaving’ of elements from different phrases
(Shieber, 1985). Although there are well-known
syntactic techniques to enhance CFG as in GPSG
(Gazdar et al., 1985), there remain constructions
that show ‘genuine’ discontinuity of the kind that
cannot be properly dealt with by CFG.
Such ‘difficult’ discontinuity typically occurs
when it is combined with scrambling – another
symptomatic phenomenon of free word order lan-
guages – of a verb’s complements. The follow-
ing is an example from German, where scrambling
and discontinuity co-occur in what is called ‘inco-
herent’ object control verb construction.
</bodyText>
<note confidence="0.39402">
(1) Ich glaube, dass der Fritz dem Frank
I believe Comp Fritz(Nom) Frank(Dat)
das Buch zu lesen erlaubt.
the book(Acc) to read allow
‘I think that Fritz allows Frank to read the book’
</note>
<page confidence="0.99412">
23
</page>
<note confidence="0.741038">
(1’) Ich glaube, dass der Fritz [das Buch] dem Frank
[zu lesen] erlaubt
Ich glaube, dass dem Frank [das Buch] der Fritz
[zu lesen] erlaubt
Ich glaube, dass [das Buch] dem Frank der Fritz
[zu lesen] erlaubt
...
</note>
<bodyText confidence="0.972172605263158">
Here (1) is in the ‘canonical’ word order while the
examples in (1’) are its scrambled variants. In
the traditional ‘bi-clausal’ analysis according to
which the object control verb subcategorises for
a zu-infinitival VP complement as well as nomi-
nal complements, this embedded VP, das Buch zu
lesen, becomes discontinuous in the latter exam-
ples (in square brackets).
One CFG response is to use ‘mono-clausal’
analysis or argument composition(Hinrichs and
Nakazawa, 1990), according to which the higher
verb and lower verb (in the above example er-
lauben and zu lesen) are combined to form a sin-
gle verbal complex, which in turn subcategorises
for nominal complements (das Buch, der Fritz and
dem Frank). Under this treatment both the ver-
bal complex and the sequence of complements are
rendered continuous, rendering all the above ex-
amples CFG-parseable.
However, this does not quite save the CFG
parseability, in the face of the fact that you could
extrapose the lower V + NP, as in the following.
(2) Ich glaube, dass der Fritz dem Frank [erlaubt], das
Buch [zu lesen].
Now we have a discontinuity of ‘verbal complex’
instead of complements (the now discontinuous
verbal complex is marked with square brackets).
Thus either way, some discontinuity is inevitable.
Such discontinuity is by no means a marginal
phenomenon limited to German. Parallel phenom-
ena are observed in the object control verbs in
Korean and Japanese ((Sato, 2004) for examples).
These languages also show a variety of ‘genuine’
discontinuity of other sorts, which do not lend
itself to a straightforward CFG parsing (Yatabe,
1996). The CFG-recalcitrant constructions exist in
abundance, pointing to an acute need for non-CFG
parsing.
</bodyText>
<subsectionHeader confidence="0.980239">
1.2 Reape’s Word Order Domain
</subsectionHeader>
<bodyText confidence="0.999104">
The most influential proposal to accommodate
such discontinuity/scrambling in HPSG is Reape’s
Word Order Domain, or DOM, a feature that con-
stitutes an additional layer separate from the dom-
inance structure of phrases (Reape, 1993; Reape,
1994). DOM encodes the phonologically realised
(‘linearised’) list of signs: the daughter signs of a
</bodyText>
<figure confidence="0.493697090909091">
⎡
phrase
⎢ DOM(1 0 2 0 3 0...0 n)
⎢ ⎢*&amp;quot;phrase ��
⎢ ⎢DOM 1
⎢ HD-DTR
⎢ UNIONED +
⎢ ⎢phrase hrase phrase
NHD-DTRs DOM 2 M s DOM n
&amp;quot;UNIONED JfpDO
UNIONED + UNIONED +
</figure>
<figureCaption confidence="0.999784">
Figure 1: Word Order Domain
</figureCaption>
<bodyText confidence="0.9998911">
phrase in the HD-DTR and NHD-DTRS features
are linearly ordered as in Figure 1.
The feature UNIONED in the daughters indi-
cates whether discontinuity amongst their con-
stituents is allowed. Computationally, the positive
(‘+’) value of the feature dictates (the DOMs of)
the daughters to be sequence unioned (represented
by the operator Q) into the mother DOM: details
apart, this operation essentially merges two lists in
a way that allows interleaving of their elements.
In Reape’s theory, LP constraints come from
an entirely different source. There is nothing as
yet that blocks, for instance, the ungrammatical
zu lesen das Buch VP sequence. The relevant
constraint, i.e. COMPS≺ZU-INF-V in German, is
stated in the LP component of the theory. Thus
with the interaction of the UNIONED feature and
LP statements, the grammar rules out the unac-
ceptable sequences while endorsing grammatical
ones such as the examples in (1’).
One important aspect of Reape’s theory is that
DOM is a list of whole signs rather than of any
part of them such as PHON. This is necessi-
tated by the fact that in order to determine how
DOM should be constructed, the daughters’ inter-
nal structure need to be referred to, above all, the
UNIONED feature. In other words, the internal
features of the daughters must be accessible.
While this is a powerful system that overcomes
the inadequacies of phrase-structure rules, some
may feel this is a rather heavy-handed way to
solve the problems. Above all, much information
is repeated, as all the signs are effectively stated
twice, once in the phrase structure and again in
DOM. Also, the fact that discontinuity and lin-
ear precedence are handled by two distinct mecha-
nisms seems somewhat questionable, as these two
factors are computationally closely related. These
properties are not entirely attractive features for a
computational grammar.
</bodyText>
<equation confidence="0.533812">
⎤
⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥
</equation>
<page confidence="0.943088">
24
</page>
<bodyText confidence="0.666465">
v
</bodyText>
<sectionHeader confidence="0.697658" genericHeader="method">
2 Lexicalising Word Order Constraints
</sectionHeader>
<subsectionHeader confidence="0.934414">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999991098039216">
Our theoretical goal is, in a nutshell, to achieve
what Reape does, namely handling discontinuity
and linear precedence, in a simpler, more lexical-
ist manner. My central proposal consists in incor-
porating the Word Order Constraint (WOC) fea-
ture into the lexical heads, rather than positing an
additional tier for linearisation. Some new sub-
features will also be introduced.
The value of the WOC feature is a set of word-
order related constraints. It may contain any re-
lational constraint the grammar writer may want
with the proviso of its formalisability, but for the
current proposal, I include two subfeatures ADJ
(adjacency) and LP, both of which, being binary
relations, are represented as a set of ordered pairs,
the members of which must either be the head it-
self or its sisters. Figure 2 illustrates what such
feature structure looks like with an English verb
provide, as in provide him with a book.
We will discuss the new PHON subfeatures in
the next section – for now it would suffice to con-
sider them to constitute the standard PHON list –
so let us focus on WOC here. The WOC feature of
this verb says, for its projection (VP), three con-
straints have to be observed. Firstly, the ADJ sub-
feature says that the indirect object NP has to be
in the adjacent position to the verb (‘provide yes-
terday him with a book’ is not allowed). Secondly,
the first two elements of the LP value encode a
head-initial constraint for English VPs, namely
that a head verb has to be preceded by its com-
plements. Lastly, the last pair in the same set says
the indirect object must precede the with-PP (‘pro-
vide with a book him’ is not allowed). Notice that
this specification leaves room for some disconti-
nuity, as there is no ADJ requirement between the
indirect NP and with-PP. Hence, provide him yes-
terday with a book is allowed.
The key idea here is that since the complements
of a lexical head are available in its COMPS fea-
ture, it should be possible to state the relative lin-
ear order which holds between the head and a
complement, as well as between complements, in-
side the feature structure of the head.
Admittedly word order would naturally be con-
sidered to reside in a phrase, string of words.
It might be argued, on the ground that a head’s
COMPS feature simply consists of the categories
it selects for in exclusion of the PHON feature,
that with this architecture one would inevitably
encounter the ‘accessibility’ problem discussed in
</bodyText>
<equation confidence="0.8329569">
⎡ ⎤
verb ⎡ ⎤
⎢ phon-wd ⎥
⎢ ⎥
⎢ ⎦
PHON ⎣CONSTITUENTS{provide} ⎥
⎢ ⎥
⎢CONSTRAINTS{} ⎥
⎢ �
⎢hnp i hpp i~ ⎥ ⎥
</equation>
<figure confidence="0.976034272727273">
np
⎢ ⎥
⎢ COMPS , pp
case Acc pform with ⎥
⎢ ⎥
⎢ ⎡ ⎤ ⎥
⎢woc ⎥
⎢ WOC ADJn ` v np ⎥
⎣ ⎣n
v , np �,� v , pp �,� np , pp ~o ⎦ ⎦
LP
</figure>
<figureCaption confidence="0.904961">
Figure 2: Example of lexical head with WOC fea-
ture
</figureCaption>
<bodyText confidence="0.999195411764706">
Section 1.2: in order to ensure the enforceability
of word order constraints, an access must be se-
cured to the values of the internal features includ-
ing the PHON values. However, this problem can
be overcome, as we will see, if due arrangements
are in place.
The main benefit of this mechanism is that it
paves way to an entirely lexicon-based rule spec-
ification, so that, on one hand, duplication of in-
formation between lexical specification and phrase
structure rules can be reduced and on the other, a
wide variety of lexical properties can be flexibly
handled. If the word order constraints, which have
been regarded as the bastion of rule-based gram-
mars, is shown to be lexically handled, it is one
significant step further to a fully lexicalist gram-
mar.
</bodyText>
<subsectionHeader confidence="0.999072">
2.2 New Head-Argument Schema
</subsectionHeader>
<bodyText confidence="0.999847904761905">
What is crucial for this WOC-incorporated gram-
mar is how the required word order constraints
stated in WOC are passed on and enforced in its
projection. I attempt to formalise this in the form
of Head-Argument Schema, by modifying Head-
Complement Schema of Pollard and Sag (1994).
There are two key revisions: an enriched PHON
feature that contains word order constraints and
percolation of these constraints emanating from
the WOC feature in the head.
The revised Schema is shown in Figure 3. For
simplicity only the LP subfeature is dealt with,
since the ADJ subfeature would work exactly the
same way. The set notations attached underneath
states the restriction on the value of WOC, namely
that all the signs that appear in the constraint
pairs must be ‘relevant’, i.e. must also appear as
daughters (included in ‘DtrSet’, the set of the head
daughter and non-head daughters). Naturally, they
also cannot be the same signs (x7ky).
Let me discuss some auxiliary modifications
</bodyText>
<page confidence="0.959456">
25
</page>
<bodyText confidence="0.675911">
⎡head-arg-phrase
</bodyText>
<equation confidence="0.938477378378378">
⎡ ⎤
⎢phon ⎢r
CONSTITS l �{ { ph 1, pa1 ,..., pai ,..., paj ,... pan 1
PHON⎢--- 111 r r J
CONSTRTS  |LP S (`...,( pai paj ca1 cai caj can 1
ARGS O l l o J
⎢ ⎡word
⎢ �
⎢ CONSTITS� ph � �
⎢ ⎢
⎢ ⎢ PHN
⎢ ⎢ CONSTRS{}
⎢ ⎢ ⎡ ⎤ ⎡
⎢ ⎢ ⎤
signsign
⎢ ⎢� � � �
⎢ ⎢ ⎣ CONSTITS pa1 ⎦,..., ai ⎣
a1
⎢ ⎢ CONSTITS pai ⎦,
* PHN
⎢ ⎢ PHN
CONSTRS ca1 CONSTRS cai
⎢ HD-DTR hd ⎢
⎢ ⎢⎡ ⎤
⎢ ⎢ ARGS args &amp;quot;sign #
sign
⎢ ⎢ �
⎢ ⎢ �CONSTITS paj hCONSTITS pan i
⎣
⎢ ⎢ ..., aj ⎦,..., an
PHN PHN
⎢ ⎢ CONSTRS caj CONSTRS can
⎢ ⎢
⎢ ⎣ n
⎣⎢...,� ai , aj �,... o
WOC  |LP wocs
NHD-DTRs args
</equation>
<bodyText confidence="0.739548">
where wocs C_ {(x,y)|x#y, x,yEDtrSet}
</bodyText>
<figure confidence="0.651686">
DtrSet = {hd}U args
</figure>
<figureCaption confidence="0.87653">
Figure 3: Head-Argument Schema with WOC feature
</figureCaption>
<figure confidence="0.3253135">
+ ⎤
⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
</figure>
<bodyText confidence="0.999838074074074">
first. Firstly, we change the feature name from
COMPS to ARGS because we assume a non-
configurational flat structure, as is commonly the
case with linearisation grammar. Another change
I propose is to make ARGS a list of underspeci-
fied signs instead of SYNSEMs as standardly as-
sumed (Pollard and Sag, 1994). In fact, this is a
position taken in an older version of HPSG (Pol-
lard and Sag, 1987) but rejected on the ground of
the locality of subcategorisation. The main reason
for this reversal is to facilitate the ‘accessibility’
we discussed earlier. As unification and percola-
tion of the PHON information is involved in the
Schema, it is much more straightforward to for-
mulate with signs. Though the change may not
be quite defensible solely on this ground,1 there is
reason to leave the locality principle as an option
for languages of which it holds rather than hard-
wire it into the Schema, since some authors raise
doubt as for the universal applicability of the lo-
cality principle e.g. (Meurers, 1999).
Turning to a more substantial modification, our
new PHON feature consists of two subfeatures,
CONSTITUENTS (or CONSTITS) and CON-
STRAINTS (or CONSTRS). The former encodes
the set that comprises the phonology of words of
which the string consists. Put simply, it is the un-
</bodyText>
<tableCaption confidence="0.57037275">
1Another potential problem is cyclicity, since the sign-
valued ARGS feature contains the WOC feature, which could
contain the head itself. This has to be fixed for the systems
that do not allow cyclicity.
</tableCaption>
<bodyText confidence="0.997689777777778">
ordered version of the standard PHON list. The
CONSTRAINTS feature represents the concata-
native constraints applicable to the string. Thus,
the PHON feature overall represents the legitimate
word order patterns in an underspecified way, i.e.
any of the possible string combinations that obey
the constraints. Let me illustrate with a VP ex-
ample, say, consisting of meet, often and Tom, for
which we assume that the following word order
patterns are acceptable,
(meet, Tom, often), (often, meet, Tom)
but not the followings:
(meet, often, Tom), (Tom, often, meet),
(Tom, meet, often), (often, Tom, meet).
This situation can be captured by the following
feature specification for PHON, which encodes
any of the acceptable strings above in an under-
specified way.
</bodyText>
<equation confidence="0.966283625">
⎡ ⎤
CONSTITS{often, Tom, meet} ⎤
⎢ ⎡ ⎤ ⎥ ⎥
⎢ ⎥ ⎥
�D�meet�,�Tom�E�
⎢ ⎥
⎢ ⎢ ADJ ⎥ ⎥
⎥
⎢ ⎢ ⎥ ⎥
⎥
⎢ CONSTRS ⎢
⎣ ⎣ ⎥
�D�meet�,�Tom�E� ⎥ ⎥
⎥
⎦ ⎦ ⎦
LP
</equation>
<bodyText confidence="0.999986666666667">
The key point is that now the computation of
word order can be done based on the information
inside the PHON feature, though indeed the CON-
STR values have to come from outside – the word
order crucially depends on SYNSEM-related val-
ues of the daughter signs.
</bodyText>
<equation confidence="0.722704666666667">
⎡
⎢ ⎢ ⎢ ⎢ ⎢PHON
⎢ ⎣
</equation>
<page confidence="0.977525">
26
</page>
<bodyText confidence="0.999979027777778">
Let us now go back to the Schema in Figure 3
and see how to determine the CONSTR values to
enter the PHON feature. This is achieved by look-
ing up the WOC constraints in the head (let’s call
this Step 1) and pushing the relevant constraints
into the PHON feature of its mother, according to
the type of constraints (Step 2).
For readability Figure 3 only states explicitly
a special case – where one LP constraint holds
of two of the arguments – but the reader is
asked to interpret ai and aj in the head daughter’s
WOC|LP to represent any two signs chosen from
the ‘DTRS’ list (including the head, hd). 2 The
structure sharing of ai and aj between WOC|LP
and ARGS indicates that the LP constraint applies
to these two arguments in this order, i.e. ai�aj.
Thus through unification, it is determined which
constraints apply to which pairs of daughter signs
inside the head. This corresponds to Step 1.
Now, only for these WOC-applicable daughter
signs, the PHON|CONSTIITS values are paired up
for each constraint (in this case (pai, paj)) and
pushed into the mother’s PHON|CONSTRS fea-
ture. This corresponds to Step 2.
Notice also that the CONSTRAINTS subfeature
is cumulatively inherited. All the non-head daugh-
ters’ CONSTR values (ca1,...,can) – the word or-
der constraints applicable to each of these daugh-
ters – are also passed up, collecting effectively
all the CONSTR values of its daughters and de-
scendants. This means the information concern-
ing word order, as tied to particular string pairs, is
never lost and passed up all the way through. Thus
the WOC constraints can be enforced at any point
where both members of the string pair in question
are instantiated.
</bodyText>
<subsectionHeader confidence="0.997721">
2.3 A Worked Example
</subsectionHeader>
<bodyText confidence="0.966541228571429">
Let us now go through an example of applying
the Schema, again with the German subordinate
clause, das Buch der Fritz dem Frank zu lesen er-
laubt (and other acceptable variants). Our goal is
to enforce the ADJ and LP constraints in a flexible
enough way, allowing the acceptable sequences
such as those we saw in Section 1.2.1. while
blocking the constraint-violating instances.
The instantiated Schema is shown in Figure 4.
Let us start with a rather deeply embedded level,
the embedded verb zu-lesen, marked v2, found in-
side vp (the last and largest NHD-DTR) as its HD-
2For the generality of the number of ARGS elements,
which should be taken to be any number including zero, the
recursive definition as detailed in (Richter and Sailer, 1995)
can be adopted.
DTR, which I suppose to be one lexical item for
simplicity. This is one of the lexical heads from
which the WOC constraints emanate. Find, in
this item’s WOC, a general LP constraint for zu-
Infinitiv VPs, COMPS�V, namely np3�v2. Then
the PHON|CONSTITS values of these signs are
searched for and found in the daughters, namely
pnp3 and pv2. These values are paired up and
passed into the CONSTRS|LP value of its mother
VP. Notice also that into this value the NHD-
DTRs’ CONSTR|LP values, in this case only
lpnp3 ({das}�{Buch}), are also unioned, consti-
tuting lpvp: we are here witnessing the cumula-
tive inheritance of constraints explained earlier.
Turn attention now to the percolation of ADJ sub-
feature: no ADJ requirement is found between
das Buch and zu-lesen (v2’s WOC|ADJ is empty),
though ADJ is required one node below, between
das and Buch (np3’s PHN|CONSTR|ADJ). Thus
no new ADJ pair is added to the mother VP’s
PHON|CONSTR feature.
Exactly the same process is repeated for the
projection of erlauben (v1), where its WOC
again contains only LP requirements. With the
PHON|CONSTITS values of the relevant signs
found and paired up ({Fritz,der}�{erlaubt} and
{Frank,dem}�{erlaubt}), they are pushed into its
mother’s PHON|CONSTRS|LP value, which is
also unioned with the PHON|CONSTRS values of
the NHD-DTRS. Notice this time that there is no
LP requirement between the zu-Infinitiv VP, das
Buch zu-lesen, and the higher verb, erlaubt. This
is intended to allow for extraposition.3
The eventual effect of the cumulative constraint
inheritance can be more clearly seen in the sub-
AVM underneath, which shows the PHON part of
the whole feature structure with its values instan-
tiated. After a succession of applications of the
Head-Argument Schema, we now have a pool of
WOCs sufficient to block unwanted word order
patterns while endorsing legitimate ones. The rep-
resentation of the PHON feature being underspec-
ified, it corresponds to any of the appropriately
constrained order patterns. der Fritz dem Frank
zu lesen das Buch erlaubt would be ruled out by
the violation of the last LP constraint, der Fritz er-
laubt dem Frank das Buch zu lesen by the second,
and so on.
The reader might be led to think, because of
3The lack of this LP requirement also entails some
marginally acceptable instances, such as der Fritz dem Frank
das Buch erlaubt zu lesen, considered ungrammatical by
many. These instances can be blocked, however, by intro-
ducing more complex WOCs. See Sato (forthcoming a).
</bodyText>
<page confidence="0.988635">
27
</page>
<figure confidence="0.602537">
subordinate-clause
FNSTITS pv1 U pnp1 U pnp2 U pvp
ADJ adnp1 U adnp2 U adnp3
NSTRS Ir/
LP&lt; ( pnp1 , pv1 \,/ pnp2 , pv1 )oU lpnp1 U lpnp2 U lpvp
ARGSO
⎡ ⎤
verb ⎢PHON  |CONSTITS pv1�erlaubt� ⎥
⎢ ⎢ ARGS�np1 , np2 , vp � ⎥ ⎥
⎢ ⎥
⎢WOC
</figure>
<equation confidence="0.98059814893617">
np1 , v1 X np2 , v1 )o#
&amp;quot;ADJ&amp;quot;n
⎡np ⎡CONSTITS pnp1 ~Frank, deml
⎢ ⎢ ⎢ ⎢ ⎡ �D�Frank�,�der�E�
⎢ ⎢
⎢ ⎢ ADJ adnp2
⎢ ⎢
PHON ⎢
⎢ ⎢ ⎢
CONSTRS ⎢ ⎣ ⎣ �D�der�,�Frank�E�
⎢ LP lpnp2
⎣SYNSEM  |...  |CASE Dat
⎡vp
PHON
FNSTITS pvp : pv2 U pnp3
ADJ adnp3
NSTRS
LP lpvp n(/
pnp3 ,pv2 )o U lpnp3
⎢ ⎢ ⎢ ⎢ARGSO
⎢ ⎡ ⎤
v ⎢ ⎢ ⎢ ⎢PHON  |CONSTITS pv2�zu-lesen� ⎥
⎢ ⎢ ARGS� np3 � ⎥
⎢ ⎢ ⎥
⎢ HD-DTR v2 ⎢
⎢ ⎢ &amp;quot;ADJ�� # ⎥ ⎥
⎢ ⎣ n
np3 , v2 ~o ⎦
WOC
⎢ LP
⎢ ⎢ ⎢ ⎡
⎢ np
⎢ ⎡CONSTITS pnp3 j Buch,dasj ⎢ ⎢
⎢ ⎢
⎢ ⎢ ⎢⎡
* �D�Buch�,�das�E�
⎢ ⎢ ⎢ ⎢ ADJ adnp3
np3
⎢ ⎢ ⎢
⎢ NHD-DTRS ⎢ ⎢
PHON
⎢ CONSTRS ⎢ ⎢ ⎢ ⎣
⎢ ⎢ ⎣�D�das�,�Buch�E�
LP lpnp3
⎢ ⎢
⎣ ⎣
SYNSEM  |...  |CASE Acc
</equation>
<subsectionHeader confidence="0.347945">
Instantiated PHON part of the above:
</subsectionHeader>
<bodyText confidence="0.401674">
⎡ CONSTITS{erlaubt, Fritz, der, Frank, dem, zu-lesen, Buch, das�
</bodyText>
<figure confidence="0.568246333333334">
⎢ �D�Fritz�,�der�E D�Frank�,�dem�E D�Buch�,�das�E�
⎢ ⎡
⎢ ADJ ,
⎢ ⎢ ,
PHON ⎢ ⎢ ⎧
⎢⎢ D�der�,�Fritz�E
⎢ CONSTRS ⎢ ⎨⎪ D�Fritz,der�,�erlaubt�E D�Frank,dem�,�erlaubt�E
, , ,
⎢ ⎢
⎣ ⎣ LP D�das�,�Buch�E
⎩ ⎪D�dem�,�Frank�E D�Buch,das�,�zu-lesen�E
, ,
</figure>
<figureCaption confidence="0.992159">
Figure 4: An application of Head-Argument Schema
</figureCaption>
<figure confidence="0.999563745762712">
PHON
HD-DTR v1
�
⎡ CONSTITS pnp1 j Fritz, derj ⎢ ⎡ �D�Fritz�,�der�E�
⎢ ⎢ ADJ adnp1
⎢ ⎢
⎢ ⎢
CONSTRS
⎣ ⎣ �D�der�,�Fritz�E�
LP lpnp1
SYNSEM  |...  |CASE Nom
np1 np
PHON
⎤
⎤ ⎥ ⎥
⎥ ⎥
⎥ ⎥ ⎥
⎦ ⎦
⎤
⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥
, np2
⎤
⎤ ⎥ ⎥
⎥ ⎥
⎥ ⎥ ⎥
⎦ ⎦
vp
⎤
⎤ ⎥ ⎥ ⎥
⎥ ⎥
⎥ ⎥
⎦ ⎦
⎤
⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
⎤ ⎥ ⎥ ⎥
⎥ ⎥
⎥ ⎥
⎥ ⎥
⎥ + ⎥
⎥ ⎥
⎥ ⎥
⎥ ⎥
⎥ ⎥
⎥ ⎥
⎦ ⎦ ⎥
⎤
⎤ ⎥ ⎥
⎥ ⎥ ⎥
⎫ ⎥ ⎥ ⎥
⎬ ⎪⎥ ⎥
⎥ ⎥ ⎥
⎦ ⎦
⎭⎪
NHD-DTRs
⎤
⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
+ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
⎤
⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥
</figure>
<page confidence="0.997264">
28
</page>
<bodyText confidence="0.999850888888889">
the monotonic inheritance of constraints, that the
WOC compliance cannot be checked until the
stage of final projection. While this is generally
true for freer word order languages considering
various scenarios such as bottom-up generation,
one can conduct the WOC check immediately after
the instantiation of relevant categories in parsing,
the fact we can exploit in our implementation, as
we will now see.
</bodyText>
<sectionHeader confidence="0.991126" genericHeader="method">
3 Constrained Free Word Order Parsing
</sectionHeader>
<subsectionHeader confidence="0.97351">
3.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.999881897435897">
In this section our parsing algorithm that works
with the lexicalised linearisation grammar out-
lined above is briefly overviewed.4 It expands on
two existing ideas: bitmasks for non-CFG parsing
and dynamic constraint application.
Bitmasks are used to indicate the positions of
a parsed words, wherever they have been found.
Reape (1991) presents a non-CFG tabular parsing
algorithm using them, for ‘permutation complete’
language, which accepts all the permutations and
discontinuous realisations of words. To take for
an example a simple English NP that comprises
the, thick and book, this parser accepts not only
their 3! permutations but discontinuous realisa-
tions thereof in a longer string, such as [book, -,
the, -, thick] (‘-’ indicates the positions of con-
stituents from other phrases).
Clearly, the problem here is overgeneration and
(in)efficiency. In the current form the worst-
case complexity will be exponential (O(n!·2n), n =
length of string). In response, Daniels and Meur-
ers (2004) propose to restrict search space dur-
ing the parse with two additional bitmasks, pos-
itive and negative masks, which encode the bits
that must be and must not be occupied, respec-
tively, based on what has been found thus far and
the relevant word order constraints. For example,
given the constraints that Det precedes Nom and
Det must be adjacent to Nom and supposing the
parser has found Det in the third position of a five
word string like above, the negative mask [ x, x,
the, -, -] is created, where x indicates the position
that cannot be occupied by Nom, as well as the
positive mask [ * , das, *, -], where * indicates the
positions that must be occupied by Nom. Thus,
you can stop the parser from searching the posi-
tions the categories yet to be found cannot occupy,
or force it to search only the positions they have to
occupy.
</bodyText>
<footnote confidence="0.546234">
4For full details see Sato (forthcoming b).
</footnote>
<bodyText confidence="0.993551138888889">
A remaining important job is to how to state the
constraints themselves in a grammar that works
with this architecture, and Daniels and Meurers’
answer is a rather traditional one: stating them in
phrase structure rules as LP attachments. They
modify HPSG rather extensively in a way simi-
lar to GPSG, in what they call ‘Generalised ID/LP
Grammar’. However, as we have been arguing,
this is not an inevitable move. It is possible to keep
the general contour of the standard HPSG largely
intact.
The way our parser interacts with the grammar
is fundamentally different. We take full advan-
tage of the information that now resides in lexi-
cal heads. Firstly, rules are dynamically generated
from the subcategorisation information (ARGS
feature) in the head. Secondly, the constraints
are picked up from the WOC feature when lexical
heads are encountered and carried in edges, elimi-
nating the need for positive/negative masks. When
an active edge is about to embrace the next cate-
gory, these constraints are checked and enforced,
limiting the search space thereby.
After the lexicon lookup, the parser generates
rules from the found lexical head and forms lexi-
cal edges. It is also at this stage that the WOC is
picked up and pushed into the edge, along with the
rule generated:
(Mumma Hd-Dtr e Nhd1 Nhd2...Nhd,,,; WOCs)
where WOCs is the set of ADJ and LP constraints
picked up, if any. This edge now tries to find the
rest – non-head daughters. The following is the
representation of an edge when the parsing pro-
ceeds to the stage where some non-head daughter,
in this representation Dtri, has been parsed, and
Dtrj is to be searched for.
</bodyText>
<subsectionHeader confidence="0.419361">
(Mumma Dtr1 Dtr2...Dtrie Dtrj...Dtrn; WOCs)
</subsectionHeader>
<bodyText confidence="0.9998156875">
When Dtrj is found, the parser does not immedi-
ately move the dot. At this point the WOC com-
pliance check with the relevant WOC constraint –
the one(s) involving Dtri and Dtrj – is conducted
on these two daughters. The compliance check is
a simple list operation. It picks the bitmasks of
the two daughters in question and checks whether
the occupied positions of one daughter precede/are
adjacent to those of the other.
The failure of this check would prevent the dot
move from taking place. Thus, edges that violate
the word order constraints would not be created,
thereby preventing wasteful search. This is the
same feature as Daniels and Meurers’, and there-
fore the efficiency in terms of the number of edges
is identical. The main difference is that we use
</bodyText>
<page confidence="0.993995">
29
</page>
<bodyText confidence="0.9993125">
the information inside the feature structure with-
out having media like positive/negative masks.
</bodyText>
<subsectionHeader confidence="0.987886">
3.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999984">
I have implemented the algorithm in Prolog and
coded the HPSG feature structure in the way de-
scribed using ProFIT (Erbach, 1995). It is a head-
corner, bottom-up chart parser, roughly based on
Gazdar and Mellish (1989). The main modifi-
cation consists of introducing bitmasks and the
word order checking procedure described above.
I created small grammars for Japanese and Ger-
man and put them to the parser, to confirm that
linearisation-heavy constructions such as object
control construction can be successfully parsed,
with the WOC constraints enforced.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="method">
4 Future Tasks
</sectionHeader>
<bodyText confidence="0.999559909090909">
What we have seen is an outline of my initial pro-
posal and there are numerous tasks yet to be tack-
led. First of all, now that the constraints are writ-
ten in individual lexical items, we are in need of
appropriate typing in terms of word order con-
straints, in order to be able to state succinctly gen-
eral constraints such as the head-final/initial con-
straint. In other words, it is crucial to devise an
appropriate type hierarchy.
Another potential problem concerns the gen-
erality of our theoretical framework. I have fo-
cused on the Head-Argument structure in this pa-
per, but if the present theory were to be of gen-
eral use, non-argument constructions, such as the
Head-Modifier structure, must be accounted for.
Also, the cases where the head of a phrase is itself
a phrase may pose a challenge, if such a phrasal
head were to determine the word order of its pro-
jection. Since it is desirable for computational
transparency not to use emergent constraints, I will
attempt to get all the word order constraints ul-
timately propagated and monotonically inherited
from the lexical level. Though some word order
constraints may turn out to have to be written into
the phrasal head directly, I am confident that the
majority, if not all, of the constraints can be stated
in the lexicon. These issues are tackled in a sepa-
rate paper (Sato, forthcoming a).
In terms of efficiency, more study has to be re-
quired to identify the exact complexity of my algo-
rithm. Also, with a view to using it for a practical
system, an evaluation of the efficiency on the ac-
tual machine will be crucial.
</bodyText>
<sectionHeader confidence="0.994277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843111111111">
M. Daniels and D. Meurers. 2004. GIDLP: A gram-
mar format for linearization-based HPSG. In Pro-
ceedings of the HPSG04 Conference.
G. Erbach. 1995. ProFIT: Prolog with features, in-
heritance and templates. Proceedings of the Seventh
Conference of the European Association for Compu-
tational Linguistics.
G. Gazdar and C. Mellish. 1989. Natural Language
Processing in Prolog. Addison Wesley.
G. Gazdar, E. Klein, G. Pullum, and I. Sag. 1985. Gen-
eralized Phrase Structure Grammar. Harvard UP.
E. Hinrichs and T. Nakazawa. 1990. Subcategorization
and VP structure in German. In S. Hughes et al.,
editor, Proceedings of the Third Symposium on Ger-
manic Linguistics.
D. Meurers. 1999. Raising Spirits (and assigning them
case). Groninger Arbeiten zur Germanistischen Lin-
guistik, Groningen Univ.
C. Pollard and I. Sag. 1987. Information-Based Syntax
and Semantics. CSLI.
C. Pollard and I. Sag. 1994. Head-Driven Phrase
Structure Grammar. CSLI.
M. Reape. 1991. Parsing bounded discontinuous
constituents: Generalisation of some common algo-
rithms. DIANA Report, Edinburgh Univ.
M. Reape. 1993. A Formal Theory of Word Order.
Ph.D. thesis, Edinburgh University.
M. Reape. 1994. Domain union and word order vari-
ation in German. In J. Nerbonne et al., editor, Ger-
man in Head-Driven Phrase Structure Grammar.
F. Richter and M. Sailer. 1995. Remarks on lineariza-
tion. Magisterarbeit, Tübingen Univ.
Y. Sato. 2004. Discontinuous constituency and non-
CFG parsing. http://www.dcs.kcl.ac.uk/pg/satoyo.
Y. Sato. forthcoming a. Two alternatives for lexicalist
linearisation grammar: Locality Principle revisited.
Y. Sato. forthcoming b. Constrained free word order
parsing for lexicalist grammar.
S. Shieber. 1985. Evidence against the context free-
ness of natural languages. Linguistics and Philoso-
phy, 8(3):333–43.
S. Yatabe. 1996. Long-distance scrambling via partial
compaction. In M. Koizumi et al., editor, Formal
Approaches to Japanese Linguistics 2. MIT Press,
Cambridge, Mass.
</reference>
<page confidence="0.998812">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902985">
<title confidence="0.984287">Lexicalising Word Order Constraints for Implemented Linearisation Grammar</title>
<author confidence="0.999814">Yo Sato</author>
<affiliation confidence="0.982838">Department of Computer Science King’s College London</affiliation>
<email confidence="0.997923">yo.sato@kcl.ac.uk</email>
<abstract confidence="0.99562125">This paper presents a way in which a lexicalised HPSG grammar can handle word order constraints in a computational parsing system, without invoking an additional layer of representation for word order, such as Reape’s Word Order Domain. The proposal is to incorporate into lexiheads WOC (Word Order Constraints) feature, which is used to constrain the word order of its projection. We also overview our parsing algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Daniels</author>
<author>D Meurers</author>
</authors>
<title>GIDLP: A grammar format for linearization-based HPSG.</title>
<date>2004</date>
<booktitle>In Proceedings of the HPSG04 Conference.</booktitle>
<contexts>
<context position="1444" citStr="Daniels and Meurers (2004)" startWordPosition="223" endWordPosition="226"> word order of languages such as German and Japanese. In parallel in computational linguistics, it has long been proposed that more flexible parsing techniques may be required to adequately handle such languages, but hitherto a practical system using linearisation has eluded large-scale implementation. There are at least two obstacles: its higher computational cost accompanied with non-CFG algorithms it requires, and the difficulty to state word order information succinctly in a grammar that works well with a nonCFG parsing engine. In a recent development, the ‘cost’ issue has been tackled by Daniels and Meurers (2004), who propose to narrow down on search space while using a non-CFG algorithm. The underlying principle is to give priority to the full generative capacity, let the parser overgenerate at default but restrict generation for efficiency thereafter. While sharing this principle, I will attempt to further streamline the computation of linearisation, focusing mainly on the issue of grammar formalism. Specifically, I would like to show that the lexicalisation of word order constraints is possible with some conservative modifications to the standard HPSG (Pollard and Sag, 1987; Pollard and Sag, 1994).</context>
<context position="23945" citStr="Daniels and Meurers (2004)" startWordPosition="4295" endWordPosition="4299"> algorithm using them, for ‘permutation complete’ language, which accepts all the permutations and discontinuous realisations of words. To take for an example a simple English NP that comprises the, thick and book, this parser accepts not only their 3! permutations but discontinuous realisations thereof in a longer string, such as [book, -, the, -, thick] (‘-’ indicates the positions of constituents from other phrases). Clearly, the problem here is overgeneration and (in)efficiency. In the current form the worstcase complexity will be exponential (O(n!·2n), n = length of string). In response, Daniels and Meurers (2004) propose to restrict search space during the parse with two additional bitmasks, positive and negative masks, which encode the bits that must be and must not be occupied, respectively, based on what has been found thus far and the relevant word order constraints. For example, given the constraints that Det precedes Nom and Det must be adjacent to Nom and supposing the parser has found Det in the third position of a five word string like above, the negative mask [ x, x, the, -, -] is created, where x indicates the position that cannot be occupied by Nom, as well as the positive mask [ * , das, </context>
</contexts>
<marker>Daniels, Meurers, 2004</marker>
<rawString>M. Daniels and D. Meurers. 2004. GIDLP: A grammar format for linearization-based HPSG. In Proceedings of the HPSG04 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erbach</author>
</authors>
<title>ProFIT: Prolog with features, inheritance and templates.</title>
<date>1995</date>
<booktitle>Proceedings of the Seventh Conference of the European Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27512" citStr="Erbach, 1995" startWordPosition="4915" endWordPosition="4916">t to those of the other. The failure of this check would prevent the dot move from taking place. Thus, edges that violate the word order constraints would not be created, thereby preventing wasteful search. This is the same feature as Daniels and Meurers’, and therefore the efficiency in terms of the number of edges is identical. The main difference is that we use 29 the information inside the feature structure without having media like positive/negative masks. 3.2 Implementation I have implemented the algorithm in Prolog and coded the HPSG feature structure in the way described using ProFIT (Erbach, 1995). It is a headcorner, bottom-up chart parser, roughly based on Gazdar and Mellish (1989). The main modification consists of introducing bitmasks and the word order checking procedure described above. I created small grammars for Japanese and German and put them to the parser, to confirm that linearisation-heavy constructions such as object control construction can be successfully parsed, with the WOC constraints enforced. 4 Future Tasks What we have seen is an outline of my initial proposal and there are numerous tasks yet to be tackled. First of all, now that the constraints are written in in</context>
</contexts>
<marker>Erbach, 1995</marker>
<rawString>G. Erbach. 1995. ProFIT: Prolog with features, inheritance and templates. Proceedings of the Seventh Conference of the European Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>C Mellish</author>
</authors>
<title>Natural Language Processing in Prolog.</title>
<date>1989</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="27600" citStr="Gazdar and Mellish (1989)" startWordPosition="4928" endWordPosition="4931">e from taking place. Thus, edges that violate the word order constraints would not be created, thereby preventing wasteful search. This is the same feature as Daniels and Meurers’, and therefore the efficiency in terms of the number of edges is identical. The main difference is that we use 29 the information inside the feature structure without having media like positive/negative masks. 3.2 Implementation I have implemented the algorithm in Prolog and coded the HPSG feature structure in the way described using ProFIT (Erbach, 1995). It is a headcorner, bottom-up chart parser, roughly based on Gazdar and Mellish (1989). The main modification consists of introducing bitmasks and the word order checking procedure described above. I created small grammars for Japanese and German and put them to the parser, to confirm that linearisation-heavy constructions such as object control construction can be successfully parsed, with the WOC constraints enforced. 4 Future Tasks What we have seen is an outline of my initial proposal and there are numerous tasks yet to be tackled. First of all, now that the constraints are written in individual lexical items, we are in need of appropriate typing in terms of word order cons</context>
</contexts>
<marker>Gazdar, Mellish, 1989</marker>
<rawString>G. Gazdar and C. Mellish. 1989. Natural Language Processing in Prolog. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<location>Harvard UP.</location>
<contexts>
<context position="2801" citStr="Gazdar et al., 1985" startWordPosition="440" endWordPosition="443">ial Word Order Domain theory. In what follows, after justifying the need for non-CFG parsing and reviewing Reape’s theory, I will propose to introduce into HPSG the Word Order Constraint (WOC) feature for lexical heads. I will then describe the parsing algorithm that refers to this feature to constrain the search for efficiency. 1.1 Limitation of CFG Parsing One of the main obstacles for CFG parsing is the discontinuity in natural languages caused by ‘interleaving’ of elements from different phrases (Shieber, 1985). Although there are well-known syntactic techniques to enhance CFG as in GPSG (Gazdar et al., 1985), there remain constructions that show ‘genuine’ discontinuity of the kind that cannot be properly dealt with by CFG. Such ‘difficult’ discontinuity typically occurs when it is combined with scrambling – another symptomatic phenomenon of free word order languages – of a verb’s complements. The following is an example from German, where scrambling and discontinuity co-occur in what is called ‘incoherent’ object control verb construction. (1) Ich glaube, dass der Fritz dem Frank I believe Comp Fritz(Nom) Frank(Dat) das Buch zu lesen erlaubt. the book(Acc) to read allow ‘I think that Fritz allows</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>G. Gazdar, E. Klein, G. Pullum, and I. Sag. 1985. Generalized Phrase Structure Grammar. Harvard UP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hinrichs</author>
<author>T Nakazawa</author>
</authors>
<title>Subcategorization and VP structure in German.</title>
<date>1990</date>
<booktitle>Proceedings of the Third Symposium on Germanic Linguistics.</booktitle>
<editor>In S. Hughes et al., editor,</editor>
<contexts>
<context position="4109" citStr="Hinrichs and Nakazawa, 1990" startWordPosition="651" endWordPosition="654"> [zu lesen] erlaubt Ich glaube, dass dem Frank [das Buch] der Fritz [zu lesen] erlaubt Ich glaube, dass [das Buch] dem Frank der Fritz [zu lesen] erlaubt ... Here (1) is in the ‘canonical’ word order while the examples in (1’) are its scrambled variants. In the traditional ‘bi-clausal’ analysis according to which the object control verb subcategorises for a zu-infinitival VP complement as well as nominal complements, this embedded VP, das Buch zu lesen, becomes discontinuous in the latter examples (in square brackets). One CFG response is to use ‘mono-clausal’ analysis or argument composition(Hinrichs and Nakazawa, 1990), according to which the higher verb and lower verb (in the above example erlauben and zu lesen) are combined to form a single verbal complex, which in turn subcategorises for nominal complements (das Buch, der Fritz and dem Frank). Under this treatment both the verbal complex and the sequence of complements are rendered continuous, rendering all the above examples CFG-parseable. However, this does not quite save the CFG parseability, in the face of the fact that you could extrapose the lower V + NP, as in the following. (2) Ich glaube, dass der Fritz dem Frank [erlaubt], das Buch [zu lesen]. </context>
</contexts>
<marker>Hinrichs, Nakazawa, 1990</marker>
<rawString>E. Hinrichs and T. Nakazawa. 1990. Subcategorization and VP structure in German. In S. Hughes et al., editor, Proceedings of the Third Symposium on Germanic Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Meurers</author>
</authors>
<title>Raising Spirits (and assigning them case). Groninger Arbeiten zur Germanistischen Linguistik,</title>
<date>1999</date>
<location>Groningen Univ.</location>
<contexts>
<context position="14079" citStr="Meurers, 1999" startWordPosition="2450" endWordPosition="2451"> but rejected on the ground of the locality of subcategorisation. The main reason for this reversal is to facilitate the ‘accessibility’ we discussed earlier. As unification and percolation of the PHON information is involved in the Schema, it is much more straightforward to formulate with signs. Though the change may not be quite defensible solely on this ground,1 there is reason to leave the locality principle as an option for languages of which it holds rather than hardwire it into the Schema, since some authors raise doubt as for the universal applicability of the locality principle e.g. (Meurers, 1999). Turning to a more substantial modification, our new PHON feature consists of two subfeatures, CONSTITUENTS (or CONSTITS) and CONSTRAINTS (or CONSTRS). The former encodes the set that comprises the phonology of words of which the string consists. Put simply, it is the un1Another potential problem is cyclicity, since the signvalued ARGS feature contains the WOC feature, which could contain the head itself. This has to be fixed for the systems that do not allow cyclicity. ordered version of the standard PHON list. The CONSTRAINTS feature represents the concatanative constraints applicable to th</context>
</contexts>
<marker>Meurers, 1999</marker>
<rawString>D. Meurers. 1999. Raising Spirits (and assigning them case). Groninger Arbeiten zur Germanistischen Linguistik, Groningen Univ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Information-Based Syntax and Semantics.</title>
<date>1987</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="2019" citStr="Pollard and Sag, 1987" startWordPosition="316" endWordPosition="319">has been tackled by Daniels and Meurers (2004), who propose to narrow down on search space while using a non-CFG algorithm. The underlying principle is to give priority to the full generative capacity, let the parser overgenerate at default but restrict generation for efficiency thereafter. While sharing this principle, I will attempt to further streamline the computation of linearisation, focusing mainly on the issue of grammar formalism. Specifically, I would like to show that the lexicalisation of word order constraints is possible with some conservative modifications to the standard HPSG (Pollard and Sag, 1987; Pollard and Sag, 1994). This will have the benefit of making the representation of linearisation grammar simpler and more parsing friendly than Reape’s influential Word Order Domain theory. In what follows, after justifying the need for non-CFG parsing and reviewing Reape’s theory, I will propose to introduce into HPSG the Word Order Constraint (WOC) feature for lexical heads. I will then describe the parsing algorithm that refers to this feature to constrain the search for efficiency. 1.1 Limitation of CFG Parsing One of the main obstacles for CFG parsing is the discontinuity in natural lan</context>
<context position="13465" citStr="Pollard and Sag, 1987" startWordPosition="2344" endWordPosition="2348"> ⎣⎢...,� ai , aj �,... o WOC |LP wocs NHD-DTRs args where wocs C_ {(x,y)|x#y, x,yEDtrSet} DtrSet = {hd}U args Figure 3: Head-Argument Schema with WOC feature + ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ first. Firstly, we change the feature name from COMPS to ARGS because we assume a nonconfigurational flat structure, as is commonly the case with linearisation grammar. Another change I propose is to make ARGS a list of underspecified signs instead of SYNSEMs as standardly assumed (Pollard and Sag, 1994). In fact, this is a position taken in an older version of HPSG (Pollard and Sag, 1987) but rejected on the ground of the locality of subcategorisation. The main reason for this reversal is to facilitate the ‘accessibility’ we discussed earlier. As unification and percolation of the PHON information is involved in the Schema, it is much more straightforward to formulate with signs. Though the change may not be quite defensible solely on this ground,1 there is reason to leave the locality principle as an option for languages of which it holds rather than hardwire it into the Schema, since some authors raise doubt as for the universal applicability of the locality principle e.g. (</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>C. Pollard and I. Sag. 1987. Information-Based Syntax and Semantics. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="2043" citStr="Pollard and Sag, 1994" startWordPosition="320" endWordPosition="323">iels and Meurers (2004), who propose to narrow down on search space while using a non-CFG algorithm. The underlying principle is to give priority to the full generative capacity, let the parser overgenerate at default but restrict generation for efficiency thereafter. While sharing this principle, I will attempt to further streamline the computation of linearisation, focusing mainly on the issue of grammar formalism. Specifically, I would like to show that the lexicalisation of word order constraints is possible with some conservative modifications to the standard HPSG (Pollard and Sag, 1987; Pollard and Sag, 1994). This will have the benefit of making the representation of linearisation grammar simpler and more parsing friendly than Reape’s influential Word Order Domain theory. In what follows, after justifying the need for non-CFG parsing and reviewing Reape’s theory, I will propose to introduce into HPSG the Word Order Constraint (WOC) feature for lexical heads. I will then describe the parsing algorithm that refers to this feature to constrain the search for efficiency. 1.1 Limitation of CFG Parsing One of the main obstacles for CFG parsing is the discontinuity in natural languages caused by ‘interl</context>
<context position="11628" citStr="Pollard and Sag (1994)" startWordPosition="1958" endWordPosition="1961">d phrase structure rules can be reduced and on the other, a wide variety of lexical properties can be flexibly handled. If the word order constraints, which have been regarded as the bastion of rule-based grammars, is shown to be lexically handled, it is one significant step further to a fully lexicalist grammar. 2.2 New Head-Argument Schema What is crucial for this WOC-incorporated grammar is how the required word order constraints stated in WOC are passed on and enforced in its projection. I attempt to formalise this in the form of Head-Argument Schema, by modifying HeadComplement Schema of Pollard and Sag (1994). There are two key revisions: an enriched PHON feature that contains word order constraints and percolation of these constraints emanating from the WOC feature in the head. The revised Schema is shown in Figure 3. For simplicity only the LP subfeature is dealt with, since the ADJ subfeature would work exactly the same way. The set notations attached underneath states the restriction on the value of WOC, namely that all the signs that appear in the constraint pairs must be ‘relevant’, i.e. must also appear as daughters (included in ‘DtrSet’, the set of the head daughter and non-head daughters)</context>
<context position="13378" citStr="Pollard and Sag, 1994" startWordPosition="2327" endWordPosition="2330">j hCONSTITS pan i ⎣ ⎢ ⎢ ..., aj ⎦,..., an PHN PHN ⎢ ⎢ CONSTRS caj CONSTRS can ⎢ ⎢ ⎢ ⎣ n ⎣⎢...,� ai , aj �,... o WOC |LP wocs NHD-DTRs args where wocs C_ {(x,y)|x#y, x,yEDtrSet} DtrSet = {hd}U args Figure 3: Head-Argument Schema with WOC feature + ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ first. Firstly, we change the feature name from COMPS to ARGS because we assume a nonconfigurational flat structure, as is commonly the case with linearisation grammar. Another change I propose is to make ARGS a list of underspecified signs instead of SYNSEMs as standardly assumed (Pollard and Sag, 1994). In fact, this is a position taken in an older version of HPSG (Pollard and Sag, 1987) but rejected on the ground of the locality of subcategorisation. The main reason for this reversal is to facilitate the ‘accessibility’ we discussed earlier. As unification and percolation of the PHON information is involved in the Schema, it is much more straightforward to formulate with signs. Though the change may not be quite defensible solely on this ground,1 there is reason to leave the locality principle as an option for languages of which it holds rather than hardwire it into the Schema, since some </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. Sag. 1994. Head-Driven Phrase Structure Grammar. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Reape</author>
</authors>
<title>Parsing bounded discontinuous constituents: Generalisation of some common algorithms.</title>
<date>1991</date>
<tech>DIANA Report,</tech>
<institution>Edinburgh University.</institution>
<location>Edinburgh</location>
<contexts>
<context position="23284" citStr="Reape (1991)" startWordPosition="4195" endWordPosition="4196">idering various scenarios such as bottom-up generation, one can conduct the WOC check immediately after the instantiation of relevant categories in parsing, the fact we can exploit in our implementation, as we will now see. 3 Constrained Free Word Order Parsing 3.1 Algorithm In this section our parsing algorithm that works with the lexicalised linearisation grammar outlined above is briefly overviewed.4 It expands on two existing ideas: bitmasks for non-CFG parsing and dynamic constraint application. Bitmasks are used to indicate the positions of a parsed words, wherever they have been found. Reape (1991) presents a non-CFG tabular parsing algorithm using them, for ‘permutation complete’ language, which accepts all the permutations and discontinuous realisations of words. To take for an example a simple English NP that comprises the, thick and book, this parser accepts not only their 3! permutations but discontinuous realisations thereof in a longer string, such as [book, -, the, -, thick] (‘-’ indicates the positions of constituents from other phrases). Clearly, the problem here is overgeneration and (in)efficiency. In the current form the worstcase complexity will be exponential (O(n!·2n), n</context>
</contexts>
<marker>Reape, 1991</marker>
<rawString>M. Reape. 1991. Parsing bounded discontinuous constituents: Generalisation of some common algorithms. DIANA Report, Edinburgh Univ. M. Reape. 1993. A Formal Theory of Word Order. Ph.D. thesis, Edinburgh University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Reape</author>
</authors>
<title>Domain union and word order variation in German. In</title>
<date>1994</date>
<booktitle>German in Head-Driven Phrase Structure Grammar.</booktitle>
<editor>J. Nerbonne et al., editor,</editor>
<contexts>
<context position="5617" citStr="Reape, 1994" startWordPosition="896" endWordPosition="897">ved in the object control verbs in Korean and Japanese ((Sato, 2004) for examples). These languages also show a variety of ‘genuine’ discontinuity of other sorts, which do not lend itself to a straightforward CFG parsing (Yatabe, 1996). The CFG-recalcitrant constructions exist in abundance, pointing to an acute need for non-CFG parsing. 1.2 Reape’s Word Order Domain The most influential proposal to accommodate such discontinuity/scrambling in HPSG is Reape’s Word Order Domain, or DOM, a feature that constitutes an additional layer separate from the dominance structure of phrases (Reape, 1993; Reape, 1994). DOM encodes the phonologically realised (‘linearised’) list of signs: the daughter signs of a ⎡ phrase ⎢ DOM(1 0 2 0 3 0...0 n) ⎢ ⎢*&amp;quot;phrase �� ⎢ ⎢DOM 1 ⎢ HD-DTR ⎢ UNIONED + ⎢ ⎢phrase hrase phrase NHD-DTRs DOM 2 M s DOM n &amp;quot;UNIONED JfpDO UNIONED + UNIONED + Figure 1: Word Order Domain phrase in the HD-DTR and NHD-DTRS features are linearly ordered as in Figure 1. The feature UNIONED in the daughters indicates whether discontinuity amongst their constituents is allowed. Computationally, the positive (‘+’) value of the feature dictates (the DOMs of) the daughters to be sequence unioned (represen</context>
</contexts>
<marker>Reape, 1994</marker>
<rawString>M. Reape. 1994. Domain union and word order variation in German. In J. Nerbonne et al., editor, German in Head-Driven Phrase Structure Grammar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Richter</author>
<author>M Sailer</author>
</authors>
<title>Remarks on linearization. Magisterarbeit,</title>
<date>1995</date>
<location>Tübingen Univ.</location>
<contexts>
<context position="18172" citStr="Richter and Sailer, 1995" startWordPosition="3163" endWordPosition="3166"> dem Frank zu lesen erlaubt (and other acceptable variants). Our goal is to enforce the ADJ and LP constraints in a flexible enough way, allowing the acceptable sequences such as those we saw in Section 1.2.1. while blocking the constraint-violating instances. The instantiated Schema is shown in Figure 4. Let us start with a rather deeply embedded level, the embedded verb zu-lesen, marked v2, found inside vp (the last and largest NHD-DTR) as its HD2For the generality of the number of ARGS elements, which should be taken to be any number including zero, the recursive definition as detailed in (Richter and Sailer, 1995) can be adopted. DTR, which I suppose to be one lexical item for simplicity. This is one of the lexical heads from which the WOC constraints emanate. Find, in this item’s WOC, a general LP constraint for zuInfinitiv VPs, COMPS�V, namely np3�v2. Then the PHON|CONSTITS values of these signs are searched for and found in the daughters, namely pnp3 and pv2. These values are paired up and passed into the CONSTRS|LP value of its mother VP. Notice also that into this value the NHDDTRs’ CONSTR|LP values, in this case only lpnp3 ({das}�{Buch}), are also unioned, constituting lpvp: we are here witnessin</context>
</contexts>
<marker>Richter, Sailer, 1995</marker>
<rawString>F. Richter and M. Sailer. 1995. Remarks on linearization. Magisterarbeit, Tübingen Univ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Sato</author>
</authors>
<title>Discontinuous constituency and nonCFG</title>
<date>2004</date>
<contexts>
<context position="5073" citStr="Sato, 2004" startWordPosition="814" endWordPosition="815">above examples CFG-parseable. However, this does not quite save the CFG parseability, in the face of the fact that you could extrapose the lower V + NP, as in the following. (2) Ich glaube, dass der Fritz dem Frank [erlaubt], das Buch [zu lesen]. Now we have a discontinuity of ‘verbal complex’ instead of complements (the now discontinuous verbal complex is marked with square brackets). Thus either way, some discontinuity is inevitable. Such discontinuity is by no means a marginal phenomenon limited to German. Parallel phenomena are observed in the object control verbs in Korean and Japanese ((Sato, 2004) for examples). These languages also show a variety of ‘genuine’ discontinuity of other sorts, which do not lend itself to a straightforward CFG parsing (Yatabe, 1996). The CFG-recalcitrant constructions exist in abundance, pointing to an acute need for non-CFG parsing. 1.2 Reape’s Word Order Domain The most influential proposal to accommodate such discontinuity/scrambling in HPSG is Reape’s Word Order Domain, or DOM, a feature that constitutes an additional layer separate from the dominance structure of phrases (Reape, 1993; Reape, 1994). DOM encodes the phonologically realised (‘linearised’)</context>
</contexts>
<marker>Sato, 2004</marker>
<rawString>Y. Sato. 2004. Discontinuous constituency and nonCFG parsing. http://www.dcs.kcl.ac.uk/pg/satoyo. Y. Sato. forthcoming a. Two alternatives for lexicalist linearisation grammar: Locality Principle revisited. Y. Sato. forthcoming b. Constrained free word order parsing for lexicalist grammar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Evidence against the context freeness of natural languages.</title>
<date>1985</date>
<journal>Linguistics and Philosophy,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="2701" citStr="Shieber, 1985" startWordPosition="426" endWordPosition="427">epresentation of linearisation grammar simpler and more parsing friendly than Reape’s influential Word Order Domain theory. In what follows, after justifying the need for non-CFG parsing and reviewing Reape’s theory, I will propose to introduce into HPSG the Word Order Constraint (WOC) feature for lexical heads. I will then describe the parsing algorithm that refers to this feature to constrain the search for efficiency. 1.1 Limitation of CFG Parsing One of the main obstacles for CFG parsing is the discontinuity in natural languages caused by ‘interleaving’ of elements from different phrases (Shieber, 1985). Although there are well-known syntactic techniques to enhance CFG as in GPSG (Gazdar et al., 1985), there remain constructions that show ‘genuine’ discontinuity of the kind that cannot be properly dealt with by CFG. Such ‘difficult’ discontinuity typically occurs when it is combined with scrambling – another symptomatic phenomenon of free word order languages – of a verb’s complements. The following is an example from German, where scrambling and discontinuity co-occur in what is called ‘incoherent’ object control verb construction. (1) Ich glaube, dass der Fritz dem Frank I believe Comp Fri</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>S. Shieber. 1985. Evidence against the context freeness of natural languages. Linguistics and Philosophy, 8(3):333–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yatabe</author>
</authors>
<title>Long-distance scrambling via partial compaction.</title>
<date>1996</date>
<booktitle>Formal Approaches to Japanese Linguistics 2.</booktitle>
<editor>In M. Koizumi et al., editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="5240" citStr="Yatabe, 1996" startWordPosition="840" endWordPosition="841">llowing. (2) Ich glaube, dass der Fritz dem Frank [erlaubt], das Buch [zu lesen]. Now we have a discontinuity of ‘verbal complex’ instead of complements (the now discontinuous verbal complex is marked with square brackets). Thus either way, some discontinuity is inevitable. Such discontinuity is by no means a marginal phenomenon limited to German. Parallel phenomena are observed in the object control verbs in Korean and Japanese ((Sato, 2004) for examples). These languages also show a variety of ‘genuine’ discontinuity of other sorts, which do not lend itself to a straightforward CFG parsing (Yatabe, 1996). The CFG-recalcitrant constructions exist in abundance, pointing to an acute need for non-CFG parsing. 1.2 Reape’s Word Order Domain The most influential proposal to accommodate such discontinuity/scrambling in HPSG is Reape’s Word Order Domain, or DOM, a feature that constitutes an additional layer separate from the dominance structure of phrases (Reape, 1993; Reape, 1994). DOM encodes the phonologically realised (‘linearised’) list of signs: the daughter signs of a ⎡ phrase ⎢ DOM(1 0 2 0 3 0...0 n) ⎢ ⎢*&amp;quot;phrase �� ⎢ ⎢DOM 1 ⎢ HD-DTR ⎢ UNIONED + ⎢ ⎢phrase hrase phrase NHD-DTRs DOM 2 M s DOM n </context>
</contexts>
<marker>Yatabe, 1996</marker>
<rawString>S. Yatabe. 1996. Long-distance scrambling via partial compaction. In M. Koizumi et al., editor, Formal Approaches to Japanese Linguistics 2. MIT Press, Cambridge, Mass.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>