<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002377">
<title confidence="0.986007">
Unlexicalised Hidden Variable Models of Split Dependency Grammars*
</title>
<author confidence="0.991012">
Gabriele Antonio Musillo
</author>
<affiliation confidence="0.998833666666667">
Department of Computer Science
and Department of Linguistics
University of Geneva
</affiliation>
<address confidence="0.738748">
1211 Geneva 4, Switzerland
</address>
<email confidence="0.98838">
musillo4@etu.unige.ch
</email>
<author confidence="0.99421">
Paola Merlo
</author>
<affiliation confidence="0.999133">
Department of Linguistics
University of Geneva
</affiliation>
<address confidence="0.741428">
1211 Geneva 4, Switzerland
</address>
<email confidence="0.996166">
merlo@lettres.unige.ch
</email>
<sectionHeader confidence="0.997476" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.68353">
This paper investigates transforms of split
dependency grammars into unlexicalised
context-free grammars annotated with hidden
symbols. Our best unlexicalised grammar
achieves an accuracy of 88% on the Penn
Treebank data set, that represents a 50%
reduction in error over previously published
results on unlexicalised dependency parsing.
</bodyText>
<sectionHeader confidence="0.998908" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998038857142857">
Recent research in natural language parsing has
extensively investigated probabilistic models of
phrase-structure parse trees. As well as being the
most commonly used probabilistic models of parse
trees, probabilistic context-free grammars (PCFGs)
are the best understood. As shown in (Klein and
Manning, 2003), the ability of PCFG models to dis-
ambiguate phrases crucially depends on the expres-
siveness of the symbolic backbone they use.
Treebank-specific heuristics have commonly been
used both to alleviate inadequate independence
assumptions stipulated by naive PCFGs (Collins,
1999; Charniak, 2000). Such methods stand in sharp
contrast to partially supervised techniques that have
recently been proposed to induce hidden grammati-
cal representations that are finer-grained than those
that can be read off the parsed sentences in tree-
banks (Henderson, 2003; Matsuzaki et al., 2005;
Prescher, 2005; Petrov et al., 2006).
*Part of this work was done when Gabriele Musillo was
visiting the MIT Computer Science and Artificial Intelligence
Laboratory, funded by a grant from the Swiss NSF (PBGE2-
117146). Many thanks to Michael Collins and Xavier Carreras
for their insightful comments on the work presented here.
This paper presents extensions of such gram-
mar induction techniques to dependency grammars.
Our extensions rely on transformations of depen-
dency grammars into efficiently parsable context-
free grammars (CFG) annotated with hidden sym-
bols. Because dependency grammars are reduced to
CFGs, any learning algorithm developed for PCFGs
can be applied to them. Specifically, we use the
Inside-Outside algorithm defined in (Pereira and
Schabes, 1992) to learn transformed dependency
grammars annotated with hidden symbols. What
distinguishes our work from most previous work on
dependency parsing is that our models are not lexi-
calised. Our models are instead decorated with hid-
den symbols that are designed to capture both lex-
ical and structural information relevant to accurate
dependency parsing without having to rely on any
explicit supervision.
</bodyText>
<sectionHeader confidence="0.882643" genericHeader="method">
2 Transforms of Dependency Grammars
</sectionHeader>
<bodyText confidence="0.996655384615385">
Contrary to phrase-structure grammars that stipulate
the existence of phrasal nodes, dependency gram-
mars assume that syntactic structures are connected
acyclic graphs consisting of vertices representing
terminal tokens related by directed edges represent-
ing dependency relations. Such terminal symbols
are most commonly assumed to be words. In our un-
lexicalised models reported below, they are instead
assumed to be part-of-speech (PoS) tags. A typical
dependency graph is illustrated in Figure 1 below.
Various projective dependency grammars exem-
plify the concept of split bilexical dependency gram-
mar (SBG) defined in (Eisner, 2000). 1 SBGs are
</bodyText>
<footnote confidence="0.922356">
1An SBG is a tuple (V, W, L, R) such that:
</footnote>
<page confidence="0.982636">
213
</page>
<reference confidence="0.208465">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 213–216,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<figureCaption confidence="0.995517">
Figure 1: A projective dependency graph for the sentence Nica hit Miles with the trumpet paired with its second-order
unlexicalised derivation tree annotated with hidden variables.
</figureCaption>
<figure confidence="0.997594078947368">
kk R1 root ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]
k
R1 root/R1 V BDBSS 1
RV BDB
RRR
L1 NNF \L1DTE
0NNF
0NNPC
0V BDB RpV BDB /R1
RR NNPC
RR
L1 V BDB \L1 NNPA
ll
RRR 1rIND R1IND/R1NNF
RRR
1r 0IND 1r
NNPC NNF
LN1NF
r
1V BDB RRR
lll
R1 V BDB /R1 IND
llll
RRRR
1l DTE
0DTE
lll
Nica hit
Miles with
the trumpet
YYYYYYYYYYYYY
l
1
RIND
L1
V BDB
1l NNPA
0NNPA
</figure>
<bodyText confidence="0.9984463">
closely related to CFGs as they both define struc-
tures that are rooted ordered projective trees. Such a
close relationship is clarified in this section.
It follows from the equivalence of finite au-
tomata and regular grammars that any SBG can
be transformed into an equivalent CFG. Let D =
(V, W, L, R) be a SBG and G = (N, W, P, S) a
CFG. To transform D into G we to define the set
P of productions, the set N of non-terminals, and
the start symbol S as follows:
</bodyText>
<listItem confidence="0.96652055">
• For each v in W, transform the automaton Lv
into a right-linear grammar GLv whose start
symbol is L&apos;v; by construction, GLv consists of
rules such as Lpv , u Lqv or Lpv —* E, where ter-
minal symbols such as u belong to W and non-
terminals such as Lpv correspond to the states of
the Lv automaton; include all E-productions in
P, and, if a rule such as Lpv —* u Lqv is in GLv,
include the rule Lpv —* 2l u Lqv in P.
• For each v in V , transform the automaton Rv
into a left-linear grammar GRv whose start
symbol is R&apos;v; by construction, GRv consists
• V is a set of terminal symbols which include a distin-
guished element root;
• L is a function that, for any v ∈ W(= V − { root}),
returns a finite automaton that recognises the well-formed
sequences in W* of left dependents of v;
• R is a function that, for each v ∈ V , returns a finite
automaton that recognises the well-formed sequences of
right dependents in W* for v.
</listItem>
<bodyText confidence="0.985550142857143">
of rules such as Rpv Rqv u or Rpv E,
where terminal symbols such as u belongs to
W and non-terminals such as Rpv correspond
to the states of the Rv automaton; include all E-
productions in P, and, if a rule such as Rpv �
Rqv u is in GRv, include the rule Rpv Rqv 2ru
in P.
</bodyText>
<listItem confidence="0.891730166666667">
• For each symbol 2l u occurring in P, include the
productions 2l u —* L&apos;u 1lu, 1lu , 0u R&apos;u, and
0u� u in P; for each symbol 2ru in P, include
the productions 2ru 1ru R&apos;u, 1ru � L&apos;u 0u,
and 0u —* u in P.
• Set the start symbol S to R&apos;root. 2
</listItem>
<bodyText confidence="0.99295025">
Parsing CFGs resulting from such transforms
runs in O(n4). The head index v decorating non-
terminals such as 1lv, 1rv, 0v, Lpv and Rqv can be com-
puted in O(1) given the left and right indices of the
sub-string wi,j they cover. 3 Observe, however, that
if 2lv or 2rv derives wi,j, then v does not functionally
depend on either i or j. Because it is possible for the
head index v of 2lv or 2rv to vary from i to j, v has
to be tracked by the parser, resulting in an overall
O(n4) time complexity.
In the following, we show how to transform
our O(n4) CFGs into O(n3) grammars by ap-
</bodyText>
<footnote confidence="0.947476">
2CFGs resulting from such transformations can further be
normalised by removing the e-productions from P.
</footnote>
<bodyText confidence="0.714939333333333">
3Indeed, if 1lv or 0v derives wi,j, then v = i; if 1rv derives
wi,j, then v = j; if wi,j is derived from Lpv, then v = j + 1;
and if wi,j is derived from Rqv, then v = i − 1.
</bodyText>
<page confidence="0.994617">
214
</page>
<bodyText confidence="0.978523166666667">
plying transformations, closely related to those in
(McAllester, 1999) and (Johnson, 2007), that elimi-
nate the 2lv and 2rv symbols.
We only detail the elimination of the symbols 2rv.
The elimination of the 2lv symbols can be derived
symmetrically. By construction, a 2rv symbol is the
right successor of a non-terminal Rpu. Consequently,
2rv can only occur in a derivation such as
α Rpu Q α Rqu 2rv Q α Rqu 1r v R1 v Q.
To substitute for the problematic 2rvnon-terminal in
the above derivation, we derive the form Rqu 1rv R1v
from Rpu/R1v R1v where Rpu/R1v is a new non-
terminal whose right-hand side is Rqu 1rv. We thus
transform the above derivation into the derivation
α Rpu Q α Rpu/R1v R1vQ α Rqu 1r vR1v Q. 4
Because u = i − 1 and v = j if Rpu/R1v derives
wi,j, and u = j + 1 and v = i if Lpu\L1v derives
wi,j, the parsing algorithm does not have to track
any head indices and can consequently parse strings
in O(n3) time.
The grammars described above can be further
transformed to capture linear second-order depen-
dencies involving three distinct head indices. A
second-order dependency structure is illustrated in
Figure 1 that involves two adjacent dependents,
Miles and with, of a single head, hit.
To see how linear second-order dependencies can
be captured, consider the following derivation of a
sequence of right dependents of a head u:
α Rpu/R1v Q α Rqu 1rv Q α Rqu/R1w
The form Rqu/R1w R1w 1v mentions three heads: u
is the the head that governs both v and w, and w
precedes v. To encode the linear relationship be-
tween w and v, we redefine the right-hand side of
Rpu/R1v as Rqu/R1w (R1w, 1rv) and include the pro-
duction (R1w, 1rv) —, R1w 1rv in the productions.
The relationship between the dependents w and v of
the head u is captured, because Rpu/R1v jointly gen-
erates R1w and 1rv. 5
Any second-order grammar resulting from trans-
forming the derivations of right and left dependents
4Symmetrically, the derivation α Lu 3 ` α 2v Lu 3 `
α L1v 1v L�� 3 involving the 2v symbol is transformed into
α Lu 3 ` α L1v Lu\L1v 3 ` α L1v 1v Lu 3.
5Symmetrically, to transform the derivation of a sequence of
left dependents of u, we redefine the right-hand side of Lu\L1v
as h1v, L1�i Lu\L1� and include the production h1v, L1 �i →
1v L1� in the set of rules.
in the way described above can be parsed in O(n3),
because the head indices decorating its symbols can
be computed in O(1).
In the following section, we show how to enrich
both our first-order and second-order grammars with
hidden variables.
</bodyText>
<sectionHeader confidence="0.991874" genericHeader="method">
3 Hidden Variable Models
</sectionHeader>
<bodyText confidence="0.999980666666667">
Because they do not stipulate the existence of
phrasal nodes, commonly used unlabelled depen-
dency models are not sufficiently expressive to dis-
criminate between distinct projections of a given
head. Both our first-order and second-order gram-
mars conflate distributionally distinct projections if
they are projected from the same head. 6
To capture various distinct projections of a head,
we annotate each of the symbols that refers to it with
a unique hidden variable. We thus constrain the dis-
tribution of the possible values of the hidden vari-
ables in a linguistically meaningful way. Figure 1 il-
lustrates such constraints: the same hidden variable
B decorates each occurrence of the PoS tag VBD of
the head hit.
Enforcing such agreement constraints between
hidden variables provides a principled way to cap-
ture not only phrasal information but also lexical in-
formation. Lexical pieces of information conveyed
by a minimal projection such as 0V BDB in Figure 1
will consistently be propagated through the deriva-
tion tree and will condition the generation of the
right and left dependents of hit.
In addition, states such as p and q that decorate
non-terminal symbols such as Rpu or Lq u can also
capture structural information, because they can en-
code the most recent steps in the derivation history.
In the models reported in the next section, these
states are assumed to be hidden and a distribution
over their possible values is automatically induced.
</bodyText>
<sectionHeader confidence="0.994134" genericHeader="method">
4 Empirical Work and Discussion
</sectionHeader>
<bodyText confidence="0.999026666666667">
The models reported below were trained, validated,
and tested on the commonly used sections from the
Penn Treebank. Projective dependency trees, ob-
</bodyText>
<footnote confidence="0.740729333333333">
6As observed in (Collins, 1999), an unambiguous verbal
head such as prove bearing the VB tag may project a clause with
an overt subject as well as a clause without an overt subject, but
only the latter is a possible dependent of subject control verbs
such as try.
R1w 1rv Q.
</footnote>
<page confidence="0.897261">
215
</page>
<table confidence="0.999873090909091">
Development Data – section 24 per word per sentence
FOM: q = 1, h = 1 75.7 9.9
SOM: q = 1, h = 1 80.5 16.2
FOM: q = 2, h = 2 81.9 17.4
FOM: q = 2, h = 4 84.7 22.0
SOM: q = 2, h = 2 84.3 21.5
SOM: q = 1, h = 4 87.0 25.8
Test Data – section 23 per word per sentence
(Eisner and Smith, 2005) 75.6 NA
SOM: q = 1, h = 4 88.0 30.6
(McDonald, 2006) 91.5 36.7
</table>
<tableCaption confidence="0.7806312">
Table 1: Accuracy results on the development and test
data set, where q denotes the number of hidden states and
h the number of hidden values annotating a PoS tag in-
volved in our first-order (FOM) and second-order (SOM)
models.
</tableCaption>
<bodyText confidence="0.99999125">
tained using the rules stated in (Yamada and Mat-
sumoto, 2003), were transformed into first-order and
second-order structures. CFGs extracted from such
structures were then annotated with hidden variables
encoding the constraints described in the previous
section and trained until convergence by means of
the Inside-Outside algorithm defined in (Pereira and
Schabes, 1992) and applied in (Matsuzaki et al.,
2005). To efficiently decode our hidden variable
models, we pruned the search space as in (Petrov et
al., 2006). To evaluate the performance of our mod-
els, we report two of the standard measures: the per
word and per sentence accuracy (McDonald, 2006).
Figures reported in the upper section of Table 1
measure the effect on accuracy of the transforms
we designed. Our baseline first-order model (q =
1, h = 1) reaches a poor per word accuracy that sug-
gests that information conveyed by bare PoS tags is
not fine-grained enough to accurately predict depen-
dencies. Results reported in the second line shows
that modelling adjacency relations between depen-
dents as second-order models do is relevant to accu-
racy. The third line indicates that annotating both
the states and the PoS tags of a first-order model
with two hidden values is sufficient to reach a per-
formance comparable to the one achieved by a naive
second-order model. However, comparing the re-
sults obtained by our best first-order models to the
accuracy achieved by our best second-order model
conclusively shows that first-order models exploit
such dependencies to a much lesser extent. Overall,
such results provide a first solution to the problem
left open in (Johnson, 2007) as to whether second-
order transforms are relevant to parsing accuracy or
not.
The lower section of Table 1 reports the results
achieved by our best model on the test data set and
compare them both to those obtained by the only un-
lexicalised dependency model we know of (Eisner
and Smith, 2005) and to those achieved by the state-
of-the-art dependency parser in (McDonald, 2006).
While clearly not state-of-the-art, the performance
achieved by our best model suggests that massive
lexicalisation of dependency models might not be
necessary to achieve competitive performance. Fu-
ture work will lie in investigating the issue of lex-
icalisation in the context of dependency parsing by
weakly lexicalising our hidden variable models.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999726714285714">
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL’00.
Michael John Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In IWPT’05.
Jason Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In H.Bunt and A. Nijholt, eds., Ad-
vances in Probabilistic and Other Parsing Technologies,
pages 29–62. Kluwer Academic Publishers.
Jamie Henderson. 2003. Inducing history representations for
broad-coverage statistical parsing. In NAACL-HLT’03.
Mark Johnson. 2007. Transforming projective bilexical de-
pendency grammars into efficiently-parsable cfgs with
unfold-fold. In ACL’06.
Dan Klein and Christopher D. Manning. 2003. Accurate unlex-
icalized parsing. In ACL’03.
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2005.
Probabilistic CFG with latent annotations. In ACL’05.
David McAllester. 1999. A reformulation of eisner and
satta’s cubit time parser for split head automata gram-
mars. http://ttic.uchicago.edu/˜dmcallester.
Ryan McDonald. 2006. Discriminative Training and Spanning
Tree Algorithms for Dependency Parsing. Ph.D. thesis,
University of Pennsylvania.
Fernando Pereira and Yves Schabes. 1992. Inside-outside rees-
timation form partially bracketed corpora. In ACL’92.
Slav Petrov, Leon Barrett Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree
annotation. In ACL’06.
Detlef Prescher. 2005. Head-driven PCFGs with latent-head
statistics. In IWPT’05.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vectore machines. In IWPT’03.
</reference>
<page confidence="0.999134">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.876052">
<title confidence="0.997917">Hidden Variable Models of Split Dependency</title>
<author confidence="0.999845">Gabriele Antonio Musillo</author>
<affiliation confidence="0.999911333333333">Department of Computer Science and Department of Linguistics University of Geneva</affiliation>
<address confidence="0.99621">1211 Geneva 4, Switzerland</address>
<email confidence="0.903881">musillo4@etu.unige.ch</email>
<author confidence="0.999424">Paola Merlo</author>
<affiliation confidence="0.9999785">Department of Linguistics University of Geneva</affiliation>
<address confidence="0.998754">1211 Geneva 4, Switzerland</address>
<email confidence="0.982349">merlo@lettres.unige.ch</email>
<abstract confidence="0.999324333333333">This paper investigates transforms of split dependency grammars into unlexicalised context-free grammars annotated with hidden symbols. Our best unlexicalised grammar an accuracy of the Penn data set, that represents a reduction in error over previously published results on unlexicalised dependency parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>213--216</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 213–216,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In NAACL’00.</booktitle>
<contexts>
<context position="1296" citStr="Charniak, 2000" startWordPosition="171" endWordPosition="172">Introduction Recent research in natural language parsing has extensively investigated probabilistic models of phrase-structure parse trees. As well as being the most commonly used probabilistic models of parse trees, probabilistic context-free grammars (PCFGs) are the best understood. As shown in (Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). *Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2- 117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work pres</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In NAACL’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1279" citStr="Collins, 1999" startWordPosition="169" endWordPosition="170">ncy parsing. 1 Introduction Recent research in natural language parsing has extensively investigated probabilistic models of phrase-structure parse trees. As well as being the most commonly used probabilistic models of parse trees, probabilistic context-free grammars (PCFGs) are the best understood. As shown in (Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). *Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2- 117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Noah A Smith</author>
</authors>
<title>Parsing with soft and hard constraints on dependency length.</title>
<date>2005</date>
<booktitle>In IWPT’05.</booktitle>
<marker>Eisner, Smith, 2005</marker>
<rawString>Jason Eisner and Noah A. Smith. 2005. Parsing with soft and hard constraints on dependency length. In IWPT’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and their cubic-time parsing algorithms.</title>
<date>2000</date>
<booktitle>In H.Bunt and A. Nijholt, eds., Advances in Probabilistic and Other Parsing Technologies,</booktitle>
<pages>29--62</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Eisner, 2000</marker>
<rawString>Jason Eisner. 2000. Bilexical grammars and their cubic-time parsing algorithms. In H.Bunt and A. Nijholt, eds., Advances in Probabilistic and Other Parsing Technologies, pages 29–62. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Henderson</author>
</authors>
<title>Inducing history representations for broad-coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In NAACL-HLT’03.</booktitle>
<contexts>
<context position="1553" citStr="Henderson, 2003" startWordPosition="210" endWordPosition="211">Gs) are the best understood. As shown in (Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). *Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2- 117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>Jamie Henderson. 2003. Inducing history representations for broad-coverage statistical parsing. In NAACL-HLT’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Transforming projective bilexical dependency grammars into efficiently-parsable cfgs with unfold-fold.</title>
<date>2007</date>
<booktitle>In ACL’06.</booktitle>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Transforming projective bilexical dependency grammars into efficiently-parsable cfgs with unfold-fold. In ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL’03.</booktitle>
<contexts>
<context position="1004" citStr="Klein and Manning, 2003" startWordPosition="128" endWordPosition="131">plit dependency grammars into unlexicalised context-free grammars annotated with hidden symbols. Our best unlexicalised grammar achieves an accuracy of 88% on the Penn Treebank data set, that represents a 50% reduction in error over previously published results on unlexicalised dependency parsing. 1 Introduction Recent research in natural language parsing has extensively investigated probabilistic models of phrase-structure parse trees. As well as being the most commonly used probabilistic models of parse trees, probabilistic context-free grammars (PCFGs) are the best understood. As shown in (Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Junichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL’05.</booktitle>
<contexts>
<context position="1577" citStr="Matsuzaki et al., 2005" startWordPosition="212" endWordPosition="215">understood. As shown in (Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). *Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2- 117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because dependency grammars are</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McAllester</author>
</authors>
<title>A reformulation of eisner and satta’s cubit time parser for split head automata grammars.</title>
<date>1999</date>
<note>http://ttic.uchicago.edu/˜dmcallester.</note>
<marker>McAllester, 1999</marker>
<rawString>David McAllester. 1999. A reformulation of eisner and satta’s cubit time parser for split head automata grammars. http://ttic.uchicago.edu/˜dmcallester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Training and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Inside-outside reestimation form partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In ACL’92.</booktitle>
<contexts>
<context position="2350" citStr="Pereira and Schabes, 1992" startWordPosition="328" endWordPosition="331">lligence Laboratory, funded by a grant from the Swiss NSF (PBGE2- 117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because dependency grammars are reduced to CFGs, any learning algorithm developed for PCFGs can be applied to them. Specifically, we use the Inside-Outside algorithm defined in (Pereira and Schabes, 1992) to learn transformed dependency grammars annotated with hidden symbols. What distinguishes our work from most previous work on dependency parsing is that our models are not lexicalised. Our models are instead decorated with hidden symbols that are designed to capture both lexical and structural information relevant to accurate dependency parsing without having to rely on any explicit supervision. 2 Transforms of Dependency Grammars Contrary to phrase-structure grammars that stipulate the existence of phrasal nodes, dependency grammars assume that syntactic structures are connected acyclic gra</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Inside-outside reestimation form partially bracketed corpora. In ACL’92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL’06.</booktitle>
<contexts>
<context position="1615" citStr="Petrov et al., 2006" startWordPosition="218" endWordPosition="221">ng, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). *Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2- 117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because dependency grammars are reduced to CFGs, any learning algorit</context>
</contexts>
<marker>Petrov, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
</authors>
<title>Head-driven PCFGs with latent-head statistics.</title>
<date>2005</date>
<booktitle>In IWPT’05.</booktitle>
<contexts>
<context position="1593" citStr="Prescher, 2005" startWordPosition="216" endWordPosition="217">(Klein and Manning, 2003), the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use. Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs (Collins, 1999; Charniak, 2000). Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006). *Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2- 117146). Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here. This paper presents extensions of such grammar induction techniques to dependency grammars. Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols. Because dependency grammars are reduced to CFGs</context>
</contexts>
<marker>Prescher, 2005</marker>
<rawString>Detlef Prescher. 2005. Head-driven PCFGs with latent-head statistics. In IWPT’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vectore machines.</title>
<date>2003</date>
<booktitle>In IWPT’03.</booktitle>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vectore machines. In IWPT’03.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>