<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.052199">
<title confidence="0.989629">
Helpfulness-Guided Review Summarization
</title>
<author confidence="0.997949">
Wenting Xiong
</author>
<affiliation confidence="0.99896">
University of Pittsburgh
</affiliation>
<address confidence="0.695522">
210 South Bouquet Street, Pittsburgh, PA 15260
</address>
<email confidence="0.998848">
wex12@cs.pitt.edu
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999437666666667">
Review mining and summarization has been
a hot topic for the past decade. A lot of ef-
fort has been devoted to aspect detection and
sentiment analysis under the assumption that
every review has the same utility for related
tasks. However, reviews are not equally help-
ful as indicated by user-provided helpfulness
assessment associated with the reviews. In
this thesis, we propose a novel review sum-
marization framework which summarizes re-
view content under the supervision of auto-
mated assessment of review helpfulness. This
helpfulness-guided framework can be easily
adapted to traditional review summarization
tasks, for a wide range of domains.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998395625">
Nowadays, as reviews thrive on the web, more and
more people wade through these online resources
to inform their own decision making. Due to the
rapid growth of the review volume, the ability of
automatically summarizing online reviews becomes
critical to allowing people to make use of them.
This makes review mining and summarization an
increasingly hot topic over the past decade. Gen-
erally speaking, there are two main paradigms in
review summarization. One is aspect-based opin-
ion summarization, which aims to differentiate and
summarize opinions regarding specific subject as-
pects. It usually involves fine-grained analysis of
both review topics and review sentiment. The other
is more summarization-oriented, prior work under
this category either assumes a shared topic or aims to
</bodyText>
<page confidence="0.978033">
77
</page>
<bodyText confidence="0.977541707317073">
produce general summaries. In this case, the focus
is the summarization, extracting salient information
from reviews and organizing them properly. Com-
pared with traditional text summarizers, sentiment-
informed summarizers generally perform better as
shown by human evaluation results (Carenini et al.,
2006; Lerman et al., 2009).
However, one implicit assumption shared by most
prior work is that all reviews are of the same util-
ity in review summarization tasks, while reviews
that comment on the same aspect and are associ-
ated with the same rating may have difference in-
fluence to users, as indicated by user-provided help-
fulness assessment (e.g. “helpful” votes on Ama-
zon.com). We believe that user-generated helpful-
ness votes/ratings suggest people’s point of interest
in review exploration. Intuitively, when users re-
fer to online reviews for guidance, reviews that are
considered helpful by more people naturally receive
more attention and credit, and thus should be given
more weight in review summarization. Following
this intuition, we hypothesize that introducing re-
view helpfulness information into review summa-
rization can yield more useful review summaries.
In addition, we are also motivated by the chal-
lenges that we faced when summarizing educational
peer reviews in which the review entity is also text.
In the peer-review domain, traditional algorithms
of identifying review aspects may suffer as reviews
contain both reviewers’ evaluations of a paper and
reviewers’ references to the paper. Such heteroge-
neous sources of review content bring challenges to
aspect identification, and the educational perspective
of peer review directly affects the characteristics of
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 77–83,
Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics
desired summaries, which has not ye been taken into
consideration in any of the current summarization
techniques. We expect the helpfulness assessment
of peer reviews can identify important information
that should be captured in peer-review summaries.
</bodyText>
<sectionHeader confidence="0.998698" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9999904">
The proposed work is grounded in the following
areas: review-helpfulness analysis, review summa-
rization and supervised topic modeling. In this sec-
tion, we will discuss existing work in the literature
and explain how the proposed work relates to them.
</bodyText>
<subsectionHeader confidence="0.995734">
2.1 Review-helpfulness analysis
</subsectionHeader>
<bodyText confidence="0.99998955">
In the literature, most researchers take a supervised
approach in modeling review helpfulness. They ei-
ther aggregate binary helpfulness votes for each re-
view into a numerical score, or directly use numer-
ical helpfulness ratings. Kim et. al (2006) took the
first attempt, using regression to model review help-
fulness based on various linguistic features. They
reported that the combination of review length, re-
view unigrams and product rating statistics per-
formed best. Along this line, other studies showed
the perceived review helpfulness depends not only
on the review content, but also on some other fac-
tors. Ghose et. al (2008) found that the reviewer’s
reviewing history also matters. However, they ob-
served that review-subjectivity, review-readability
and other reviewer-related features are interchange-
able for predicting review helpfulness. In addition,
the empirical study on Amazon reviews conducted
by Danescu-Niculescu-Mizil et. al (2009) revealed
that the perceived helpfulness is also affected by
how a review relates to the other reviews of the same
product. However, given our goal of using review
helpfulness assessment to guide summarization to-
wards generating more useful summaries rather than
to explain each individual helpfulness rating, we
will ignore the interaction of helpfulness assessment
among reviews of the same target.
Furthermore, the utility of features in modeling
review helpfulness may vary with the review do-
main. Mudambi et. al (2010) showed that for
product reviews, the product type moderates both
the product ratings and review length on the per-
ceived review helpfulness. For educational peer re-
views, in X (2011) we showed that cognitive con-
structs which predict feedback implementation can
further improve our helpfulness model upon general
linguistic features. These findings seem to suggest
that the review helpfulness model should be domain-
dependent, due to the specific semantics of “helpful-
ness” defined in context of the domain.
</bodyText>
<subsectionHeader confidence="0.998989">
2.2 Review summarization
</subsectionHeader>
<bodyText confidence="0.999986564102564">
One major paradigm of review summarization is
aspect-based summarization, which is based on
identifying aspects and associating opinion senti-
ment with them. (Although this line of work is
closely related to sentiment analysis, it is not the
focus of this proposed work.) While initially peo-
ple use information retrieval techniques to recog-
nize aspect terms and opinion expressions (Hu and
Liu, 2004; Popescu and Etzioni, 2005), recent work
seems to favor generative statistical models more
(Mei et al., 2007; Lu and Zhai, 2008; Titov and Mc-
Donald, 2008b; Titov and McDonald, 2008a; Blei
and McAuliffe, 2010; Brody and Elhadad, 2010;
Mukherjee and Liu, 2012; Sauper and Barzilay,
2013). One typical problem with these models is
that many discovered aspects are not meaningful to
end-users. Some of these studies focus on distin-
guishing aspects in terms of sentiment variation by
modeling aspects together with sentiment (Titov and
McDonald, 2008a; Lu and Zhai, 2008; Mukherjee
and Liu, 2012; Sauper and Barzilay, 2013). How-
ever, little attention is given to differentiating review
content directly regarding their utilities in review
exploration. Mukherjee and Liu (2012) attempted
to address this issue by introducing user-provided
aspect terms as seeds for learning review aspects,
though this approach might not be easily generalized
to other domains, as users’ point of interest could
vary with the review domain.
Another paradigm of review summarization is
more summarization-oriented. In contrast, such ap-
proaches do not require the step of identifying as-
pects, instead, they either assume the input text share
the same aspect or aim to produce general sum-
maries. These studies are closely related to the tra-
ditional NLP task of text summarization. Generally
speaking, the goal of text summarization is to retain
the most important points of the input text within a
shorter length. Either extractively or abstractively,
</bodyText>
<page confidence="0.989398">
78
</page>
<bodyText confidence="0.999985173913044">
one important task is to determine the informative-
ness of a text element. In addition to reducing in-
formation redundancy, different heuristics were pro-
posed within the context of opinion summarization.
Stoyanov and Cardie (2008) focused on identifying
opinion entities (opinion, source, target) and pre-
senting them in a structured way (templates or di-
agrams). Lerman et. al (2009) reported that users
preferred sentiment informed summaries based on
their analysis of human evaluation of various sum-
marization models, while Kim and Zhai (2009) fur-
ther considered an effective review summary as rep-
resentative contrastive opinion pairs. Different from
all above, Ganesan et. al (2010) represented text
input as token-based graphs based on the token or-
der in the string. They rank summary candidates by
scoring paths after removing redundant information
from the graph. For any summarization framework
discussed above, the helpfulness of the review ele-
ments (e.g. sentences, opinion entities, or words),
which can be derived from the review overall help-
fulness, captures informativeness from another di-
mension that has not been taken into account yet.
</bodyText>
<subsectionHeader confidence="0.998772">
2.3 Supervised content modeling
</subsectionHeader>
<bodyText confidence="0.99992492">
As review summarization is meant to help users ac-
quire useful information effectively, what and how
to summarize may vary with user needs. To discover
user preferences, Ando and Ishizaki (2012) man-
ually analyzed travel reviews to identify the most
influential review sentences objectively and subjec-
tively, while Mukherjee and Liu (2012) extract and
categorize review aspects through semi-supervised
modeling using user-provided seeds (categories of
terms). In contrast, we are interested in using user-
provided helpfulness ratings for guidance. As these
helpfulness ratings are existing meta data of reviews,
we will need no additional input from users. Specif-
ically, we propose to use supervised LDA (Blei and
McAuliffe, 2010) to model review content under the
supervision of review helpfulness ratings. Similar
approach is widely adopted in sentiment analysis,
where review aspects are learned in the presence
of sentiment predictions (Blei and McAuliffe, 2010;
Titov and McDonald, 2008a). Furthermore, Brana-
van et. al (2009) showed that joint modeling of text
and user annotations benefits extractive summariza-
tion. Therefore, we hypothesize modeling review
content together with review helpfulness is benefi-
cial to review summarization as well.
</bodyText>
<sectionHeader confidence="0.997282" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999994642857143">
We plan to experiment on three representative re-
view domains: product reviews, book reviews and
peer reviews. The first one is mostly studied, while
the later two types are more complex, as the review
content consists of both reviewer’s evaluations of the
target and reviewer’s references to the target, which
is also text. This property makes review summariza-
tion more challenging.
For product reviews and book reviews, we plan
to use Amazon reviews provided by Jindal and Liu
(2008), which is a widely used data set in review
mining and sentiment analysis. We consider the
helpfulness assessment of an Amazon review as the
ratio of “helpful” votes over all votes (Kim et al.,
2006). For educational peer reviews, we plan to use
an annotated corpus (Nelson and Schunn, 2009) col-
lected from an online peer-review reciprocal system,
which we used in our prior work (Xiong and Litman,
2011). Two experts (a writing instructor and a con-
tent instructor) were asked to rate the helpfulness of
each peer review on a scale from one to five (Pearson
correlation r = 0.425, p ≤ 0.01). For our study, we
consider the average ratings given by the two experts
(which roughly follow a normal distribution) as the
gold standard of review helpfulness ratings. To be
consistent with the other review domains, we nor-
malize peer-review helpfulness ratings in the range
between 0 and 1.
</bodyText>
<sectionHeader confidence="0.999322" genericHeader="method">
4 Proposed work
</sectionHeader>
<bodyText confidence="0.9998676">
The proposed thesis work consists of three parts:
1) review content analysis using user-provided help-
fulness ratings, 2) automatically predicting review
helpfulness and 3) a helpfulness-guided review sum-
marization framework.
</bodyText>
<subsectionHeader confidence="0.992328">
4.1 Review content analysis
</subsectionHeader>
<bodyText confidence="0.9998645">
Before advocating the proposed idea, we would test
our two hypothesis: 1) user-provided review help-
fulness assessment reflects review content differ-
ence. 2) Considering review content in terms of in-
ternal content (e.g. reviewers’ opinions) vs. exter-
nal content (e.g. book content), the internal content
</bodyText>
<page confidence="0.992452">
79
</page>
<subsectionHeader confidence="0.537775">
4.2 Automated review helpfulness assessment
</subsectionHeader>
<bodyText confidence="0.994929204545455">
influences the perceived review helpfulness more
than the external content.
We propose to use two kind of instruments, one is
Linguistic Inquiry Word Count (LIWC)1, which is
a manually created dictionary of words; the other is
the set of review topics learned by Latent Dirich-
let Allocation (LDA) (Blei et al., 2003; Blei and
McAuliffe, 2010). LIWC analyzes text input based
on language usages both syntactically and semanti-
cally, which reveals review content patterns at a high
level; LDA can be used to model sentence-level re-
view topics which are domain specific.
For the LIWC-based analysis, we test whether
each category count has a significant effect on the
numerical helpfulness ratings using paired T-test.
For LDA-based analysis, we demonstrate the dif-
ference by show how the learned topics vary when
helpfulness information is introduced as supervi-
sion. Specifically, by comparing the topics learned
from the unsupervised LDA and those learned from
the supervised LDA (with helpfulness ratings), we
expect to show that the supervision of helpfulness
ratings can yield more meaningful aspect clusters.
It is important to note that in both approaches
a review is considered as a bag of words, which
might be problematic if the review has both internal
and external content. Considering this, we hypoth-
esize that the content difference captured by user-
provided helpfulness ratings is mainly in the review-
ers’ evaluation rather than in the content of external
sources (hypothesis 2). We plan to test this hypoth-
esis on both book reviews and peer reviews by ana-
lyzing review content in two conditions: in the first
condition (the control condition), all content is pre-
served; in the second condition, the external content
is excluded. If we observe more content variance
in the second condition than the first one, the sec-
ond hypothesis is true. Thus we will separate review
internal and external content in the later summariza-
tion step. For simplification, in the second condi-
tion, we only consider the topic words of the exter-
nal content; we plan to use a corpus-based approach
to identify these topic terms and filter them out to
reduce the impact of external content.
</bodyText>
<footnote confidence="0.569265">
1Url: http://www.liwc.net. We are using LIWC2007.
</footnote>
<bodyText confidence="0.999776139534884">
Considering how review usefulness would be inte-
grated in the proposed summarization framework,
we propose two models for predicting review help-
fulness at different levels of granularity.
A discriminative model to learn review global
helpfulness. Previously we (2011) built a discrim-
inative model for predicting the helpfulness of ed-
ucational peer reviews based on prior work of au-
tomatically predicting review helpfulness of prod-
uct reviews (Kim et al., 2006). We considered both
domain-general features and domain-specific fea-
tures. The domain-general features include structure
features (e.g. review length), semantic features, and
descriptive statistics of the product ratings (Kim et
al., 2006); the domain-specific features include the
percentage of external content in reviews and cog-
nitive and social science features that are specific
to the peer-review domain. To extend this idea to
other types of reviews: for product reviews, we con-
sider product aspect-related terms as the topic words
of the external content; for book reviews, we take
into account author’s profile information (number
of books, the mean average book ratings). As we
showed that replacing review unigrams with manu-
ally crafted keyword categories can further improve
the helpfulness model of peer reviews, we plan to
investigate whether review unigrams are generally
replaceable by review LIWC features for modeling
review helpfulness.
A generative model to learn review local help-
fulness. In order to utilize user-provided helpfulness
information in a decomposable fashion, we propose
to use sLDA (Blei and McAuliffe, 2010) to model
review content with review helpfulness information
at the review level, so that the learned latent topics
will be predictive of review helpfulness. In addition
to evaluating the model’s predictive power and the
quality of the learned topics, we will also investi-
gate the extent to which the model’s performance is
affected by the size of the training set, as we may
need to use automatically predicted review helpful-
ness instead, if user-provided helpfulness informa-
tion is not available.
</bodyText>
<page confidence="0.991679">
80
</page>
<subsectionHeader confidence="0.998233">
4.3 Helpfulness-guided review summarization
</subsectionHeader>
<bodyText confidence="0.999889638297873">
In the proposed work, we plan to investigate various
methods of supervising an extractive review summa-
rizer using the proposed helpfulness models. The
simplest method (M1) is to control review helpful-
ness of the summarization input by removing re-
views that are predicted of low helpfulness. A sim-
ilar method (M2) is to use post-processing rather
than pre-processing – reorder the selected summary
candidates (e.g. sentences) based on their predicted
helpfulness. The helpfulness of a summary sentence
can be either inferred from the local-helpfulness
model (sLDA), or aggregated from review-level
helpfulness ratings of the review(s) from which the
sentence is extracted. The third one (M3) works
together with a specific summarization algorithm,
interpolating traditional informativeness assessment
with novel helpfulness metrics based on the pro-
posed helpfulness models.
For demonstration, we plan to prototype the pro-
posed framework based on MEAD* (Carenini et al.,
2006), which is an extension of MEAD (an open-
source framework for multi-document summariza-
tion (Radev et al., 2004)) for summarizing evalu-
ative text. MEAD* defines sentence informative-
ness based on features extracted through standard
aspect-based review mining (Hu and Liu, 2004). As
a human-centric design, we plan to evaluate the pro-
posed framework in a user study in terms of pair-
wise comparison of the reviews generated by differ-
ent summarizers (M1, M2, M3 and MEAD*). Al-
though fully automated summarization metrics are
available (e.g. Jensen-Shannon Divergence (Louis
and Nenkova, 2009)), they favor summaries that
have a similar word distribution to the input and thus
do not suit our task of review summarization.
To show the generality of the proposed ideas, we
plan to evaluate the utility of introducing review
helpfulness in aspect ranking as well, which is an
important sub-task of review opinion analysis. If
our hypothesis (1) is true, we would expect aspect
ranking based on helpfulness-involved metrics out-
performing the baseline which does not use review
helpfulness (Yu et al., 2011). This evaluation will
be done on product reviews and peer reviews, as the
previous work was based on product reviews, while
peer reviews tend to have an objective aspect rank-
ing (provided by domain experts).
</bodyText>
<sectionHeader confidence="0.977826" genericHeader="conclusions">
5 Contributions
</sectionHeader>
<bodyText confidence="0.999051">
The proposed thesis mainly contributes to review
mining and summarization.
</bodyText>
<listItem confidence="0.954351947368421">
1. Investigate the impact of the source of review
content on review helpfulness. While a lot of
studies focus on product reviews, we based our
analysis on a wider range of domains, including
peer reviews, which have not been well studied
before.
2. Propose two models to automatically assess re-
view helpfulness at different levels of granu-
larity. While the review-level global helpful-
ness model takes into account domain-specific
semantics of helpfulness of reviews, the lo-
cal helpfulness model learns review helpfulness
jointly with review topics. This local helpful-
ness model allows us to decompose overall re-
view helpfulness into small elements, so that
review helpfulness can be easily combined with
metrics of other dimensions in assessing the
importance of summarization candidates.
3. Propose a user-centric review summarization
</listItem>
<bodyText confidence="0.983066111111111">
framework that utilizes user-provided helpful-
ness assessment as supervision. Compared
with previous work, we take a data driven ap-
proach in modeling review helpfulness as well
as helpfulness-related topics, which requires
no extra human input of user-preference and
can be adapted to typical review summarization
tasks such as aspect selection/ranking, sum-
mary sentence ordering, etc.
</bodyText>
<sectionHeader confidence="0.998219" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9834521">
M. Ando and S. Ishizaki. 2012. Analysis of travel re-
view data from readers point of view. WASSA 2012,
page 47.
D.M. Blei and J.D. McAuliffe. 2010. Supervised topic
models. arXiv preprint arXiv:1003.0783.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet allocation. the Journal of machine Learning
research, 3:993–1022.
SRK Branavan, H. Chen, J. Eisenstein, and R. Barzilay.
2009. Learning document-level semantic properties
</reference>
<page confidence="0.993386">
81
</page>
<reference confidence="0.999455826923077">
from free-text annotations. Journal of Artificial Intel-
ligence Research, 34(2):569.
S. Brody and N. Elhadad. 2010. An unsupervised aspect-
sentiment model for online reviews. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 804–812. Associa-
tion for Computational Linguistics.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-document
summarization of evaluative text. In In Proceedings
of the 11st Conference of the European Chapter of the
Association for Computational Linguistics. Citeseer.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinber g, and Lillian Lee. 2009. How opin-
ions are received by online communities: A case study
on Amazon.com helpfulness votes. In Proceedings of
WWW, pages 141–150.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ’10, pages 340–
348, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Anindya Ghose and Panagiotis G. Ipeirotis. 2008. Esti-
mating the socio-economic impact of product reviews.
In NYU Stern Research Working Paper CeDER.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168–177. ACM.
N. Jindal and B. Liu. 2008. Opinion spam and analysis.
In Proceedings of the international conference on Web
search and web data mining, pages 219–230.
H.D. Kim and C.X. Zhai. 2009. Generating comparative
summaries of contradictory opinions in text. In Pro-
ceedings of the 18th ACM conference on Information
and knowledge management, pages 385–394. ACM.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco
Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP2006), pages 423–430, Sydney, Australia,
July.
K. Lerman, S. Blair-Goldensohn, and R. McDonald.
2009. Sentiment summarization: Evaluating and
learning user preferences. In Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 514–522.
Association for Computational Linguistics.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization without
human models. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing: Volume 1-Volume 1, pages 306–314. Associ-
ation for Computational Linguistics.
Y. Lu and C. Zhai. 2008. Opinion integration through
semi-supervised topic modeling. In Proceedings of
the 17th international conference on World Wide Web,
pages 121–130. ACM.
Q. Mei, X. Ling, M. Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the 16th interna-
tional conference on World Wide Web, pages 171–180.
ACM.
S.M. Mudambi and D. Schuff. 2010. What makes a
helpful online review? a study of customer reviews
on amazon. com. MIS quarterly, 34(1):185–200.
A. Mukherjee and B. Liu. 2012. aspect extraction
through semi-supervised modeling. In Proceedings of
50th anunal meeting of association for computational
Linguistics (acL-2012)(accepted for publication).
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. In Instructional Sci-
ence, volume 37, pages 375–401.
A.M. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 339–346. Association for Computational Lin-
guistics.
D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer,
A. Celebi, S. Dimitrov, E. Drabek, A. Hakim, W. Lam,
D. Liu, et al. 2004. Mead-a platform for multidocu-
ment multilingual text summarization. In Proceedings
of LREC, volume 2004.
C. Sauper and R. Barzilay. 2013. Automatic aggregation
by joint modeling of aspects and values. Journal of
Artificial Intelligence Research, 46:89–127.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proceedings of
the 22nd International Conference on Computational
Linguistics-Volume 1, pages 817–824. Association for
Computational Linguistics.
I. Titov and R. McDonald. 2008a. A joint model of text
and aspect ratings for sentiment summarization. Ur-
bana, 51:61801.
I. Titov and R. McDonald. 2008b. Modeling online re-
views with multi-grain topic models. In Proceedings
of the 17th international conference on World Wide
Web, pages 111–120. ACM.
Wenting Xiong and Diane Litman. 2011. Automatically
predicting peer-review helpfulness. In Proceedings of
</reference>
<page confidence="0.980476">
82
</page>
<reference confidence="0.998956857142857">
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 502–507.
J. Yu, Z.J. Zha, M. Wang, and T.S. Chua. 2011. Aspect
ranking: identifying important product aspects from
online consumer reviews. Computational Linguistics,
pages 1496–1505.
</reference>
<page confidence="0.999311">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.634262">
<title confidence="0.999069">Helpfulness-Guided Review Summarization</title>
<author confidence="0.656306">Wenting</author>
<affiliation confidence="0.995239">University of</affiliation>
<address confidence="0.997041">210 South Bouquet Street, Pittsburgh, PA</address>
<email confidence="0.995952">wex12@cs.pitt.edu</email>
<abstract confidence="0.997705625">Review mining and summarization has been a hot topic for the past decade. A lot of effort has been devoted to aspect detection and sentiment analysis under the assumption that every review has the same utility for related tasks. However, reviews are not equally helpful as indicated by user-provided helpfulness assessment associated with the reviews. In this thesis, we propose a novel review summarization framework which summarizes review content under the supervision of automated assessment of review helpfulness. This helpfulness-guided framework can be easily adapted to traditional review summarization tasks, for a wide range of domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Ando</author>
<author>S Ishizaki</author>
</authors>
<title>Analysis of travel review data from readers point of view. WASSA</title>
<date>2012</date>
<pages>47</pages>
<contexts>
<context position="9301" citStr="Ando and Ishizaki (2012)" startWordPosition="1406" endWordPosition="1409">er in the string. They rank summary candidates by scoring paths after removing redundant information from the graph. For any summarization framework discussed above, the helpfulness of the review elements (e.g. sentences, opinion entities, or words), which can be derived from the review overall helpfulness, captures informativeness from another dimension that has not been taken into account yet. 2.3 Supervised content modeling As review summarization is meant to help users acquire useful information effectively, what and how to summarize may vary with user needs. To discover user preferences, Ando and Ishizaki (2012) manually analyzed travel reviews to identify the most influential review sentences objectively and subjectively, while Mukherjee and Liu (2012) extract and categorize review aspects through semi-supervised modeling using user-provided seeds (categories of terms). In contrast, we are interested in using userprovided helpfulness ratings for guidance. As these helpfulness ratings are existing meta data of reviews, we will need no additional input from users. Specifically, we propose to use supervised LDA (Blei and McAuliffe, 2010) to model review content under the supervision of review helpfulne</context>
</contexts>
<marker>Ando, Ishizaki, 2012</marker>
<rawString>M. Ando and S. Ishizaki. 2012. Analysis of travel review data from readers point of view. WASSA 2012, page 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J D McAuliffe</author>
</authors>
<title>Supervised topic models. arXiv preprint arXiv:1003.0783.</title>
<date>2010</date>
<contexts>
<context position="6605" citStr="Blei and McAuliffe, 2010" startWordPosition="991" endWordPosition="994">iew summarization One major paradigm of review summarization is aspect-based summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). However, little attention is given to differentiating review content directly regarding their utilities in review exploration. Mukherjee and Liu (2012) attempted to address this issue by </context>
<context position="9835" citStr="Blei and McAuliffe, 2010" startWordPosition="1484" endWordPosition="1487"> summarize may vary with user needs. To discover user preferences, Ando and Ishizaki (2012) manually analyzed travel reviews to identify the most influential review sentences objectively and subjectively, while Mukherjee and Liu (2012) extract and categorize review aspects through semi-supervised modeling using user-provided seeds (categories of terms). In contrast, we are interested in using userprovided helpfulness ratings for guidance. As these helpfulness ratings are existing meta data of reviews, we will need no additional input from users. Specifically, we propose to use supervised LDA (Blei and McAuliffe, 2010) to model review content under the supervision of review helpfulness ratings. Similar approach is widely adopted in sentiment analysis, where review aspects are learned in the presence of sentiment predictions (Blei and McAuliffe, 2010; Titov and McDonald, 2008a). Furthermore, Branavan et. al (2009) showed that joint modeling of text and user annotations benefits extractive summarization. Therefore, we hypothesize modeling review content together with review helpfulness is beneficial to review summarization as well. 3 Data We plan to experiment on three representative review domains: product r</context>
<context position="12693" citStr="Blei and McAuliffe, 2010" startWordPosition="1936" endWordPosition="1939">is: 1) user-provided review helpfulness assessment reflects review content difference. 2) Considering review content in terms of internal content (e.g. reviewers’ opinions) vs. external content (e.g. book content), the internal content 79 4.2 Automated review helpfulness assessment influences the perceived review helpfulness more than the external content. We propose to use two kind of instruments, one is Linguistic Inquiry Word Count (LIWC)1, which is a manually created dictionary of words; the other is the set of review topics learned by Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei and McAuliffe, 2010). LIWC analyzes text input based on language usages both syntactically and semantically, which reveals review content patterns at a high level; LDA can be used to model sentence-level review topics which are domain specific. For the LIWC-based analysis, we test whether each category count has a significant effect on the numerical helpfulness ratings using paired T-test. For LDA-based analysis, we demonstrate the difference by show how the learned topics vary when helpfulness information is introduced as supervision. Specifically, by comparing the topics learned from the unsupervised LDA and th</context>
<context position="16181" citStr="Blei and McAuliffe, 2010" startWordPosition="2480" endWordPosition="2483"> as the topic words of the external content; for book reviews, we take into account author’s profile information (number of books, the mean average book ratings). As we showed that replacing review unigrams with manually crafted keyword categories can further improve the helpfulness model of peer reviews, we plan to investigate whether review unigrams are generally replaceable by review LIWC features for modeling review helpfulness. A generative model to learn review local helpfulness. In order to utilize user-provided helpfulness information in a decomposable fashion, we propose to use sLDA (Blei and McAuliffe, 2010) to model review content with review helpfulness information at the review level, so that the learned latent topics will be predictive of review helpfulness. In addition to evaluating the model’s predictive power and the quality of the learned topics, we will also investigate the extent to which the model’s performance is affected by the size of the training set, as we may need to use automatically predicted review helpfulness instead, if user-provided helpfulness information is not available. 80 4.3 Helpfulness-guided review summarization In the proposed work, we plan to investigate various m</context>
</contexts>
<marker>Blei, McAuliffe, 2010</marker>
<rawString>D.M. Blei and J.D. McAuliffe. 2010. Supervised topic models. arXiv preprint arXiv:1003.0783.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="12666" citStr="Blei et al., 2003" startWordPosition="1932" endWordPosition="1935">st our two hypothesis: 1) user-provided review helpfulness assessment reflects review content difference. 2) Considering review content in terms of internal content (e.g. reviewers’ opinions) vs. external content (e.g. book content), the internal content 79 4.2 Automated review helpfulness assessment influences the perceived review helpfulness more than the external content. We propose to use two kind of instruments, one is Linguistic Inquiry Word Count (LIWC)1, which is a manually created dictionary of words; the other is the set of review topics learned by Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei and McAuliffe, 2010). LIWC analyzes text input based on language usages both syntactically and semantically, which reveals review content patterns at a high level; LDA can be used to model sentence-level review topics which are domain specific. For the LIWC-based analysis, we test whether each category count has a significant effect on the numerical helpfulness ratings using paired T-test. For LDA-based analysis, we demonstrate the difference by show how the learned topics vary when helpfulness information is introduced as supervision. Specifically, by comparing the topics learned from </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>H Chen</author>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Learning document-level semantic properties from free-text annotations.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>Branavan, Chen, Eisenstein, Barzilay, 2009</marker>
<rawString>SRK Branavan, H. Chen, J. Eisenstein, and R. Barzilay. 2009. Learning document-level semantic properties from free-text annotations. Journal of Artificial Intelligence Research, 34(2):569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>N Elhadad</author>
</authors>
<title>An unsupervised aspectsentiment model for online reviews. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>804--812</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6630" citStr="Brody and Elhadad, 2010" startWordPosition="995" endWordPosition="998">r paradigm of review summarization is aspect-based summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). However, little attention is given to differentiating review content directly regarding their utilities in review exploration. Mukherjee and Liu (2012) attempted to address this issue by introducing user-provided</context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>S. Brody and N. Elhadad. 2010. An unsupervised aspectsentiment model for online reviews. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 804–812. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>R Ng</author>
<author>A Pauls</author>
</authors>
<title>Multi-document summarization of evaluative text. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics. Citeseer.</booktitle>
<contexts>
<context position="1911" citStr="Carenini et al., 2006" startWordPosition="280" endWordPosition="283">ed opinion summarization, which aims to differentiate and summarize opinions regarding specific subject aspects. It usually involves fine-grained analysis of both review topics and review sentiment. The other is more summarization-oriented, prior work under this category either assumes a shared topic or aims to 77 produce general summaries. In this case, the focus is the summarization, extracting salient information from reviews and organizing them properly. Compared with traditional text summarizers, sentimentinformed summarizers generally perform better as shown by human evaluation results (Carenini et al., 2006; Lerman et al., 2009). However, one implicit assumption shared by most prior work is that all reviews are of the same utility in review summarization tasks, while reviews that comment on the same aspect and are associated with the same rating may have difference influence to users, as indicated by user-provided helpfulness assessment (e.g. “helpful” votes on Amazon.com). We believe that user-generated helpfulness votes/ratings suggest people’s point of interest in review exploration. Intuitively, when users refer to online reviews for guidance, reviews that are considered helpful by more peop</context>
<context position="17701" citStr="Carenini et al., 2006" startWordPosition="2709" endWordPosition="2712">re-processing – reorder the selected summary candidates (e.g. sentences) based on their predicted helpfulness. The helpfulness of a summary sentence can be either inferred from the local-helpfulness model (sLDA), or aggregated from review-level helpfulness ratings of the review(s) from which the sentence is extracted. The third one (M3) works together with a specific summarization algorithm, interpolating traditional informativeness assessment with novel helpfulness metrics based on the proposed helpfulness models. For demonstration, we plan to prototype the proposed framework based on MEAD* (Carenini et al., 2006), which is an extension of MEAD (an opensource framework for multi-document summarization (Radev et al., 2004)) for summarizing evaluative text. MEAD* defines sentence informativeness based on features extracted through standard aspect-based review mining (Hu and Liu, 2004). As a human-centric design, we plan to evaluate the proposed framework in a user study in terms of pairwise comparison of the reviews generated by different summarizers (M1, M2, M3 and MEAD*). Although fully automated summarization metrics are available (e.g. Jensen-Shannon Divergence (Louis and Nenkova, 2009)), they favor </context>
</contexts>
<marker>Carenini, Ng, Pauls, 2006</marker>
<rawString>G. Carenini, R. Ng, and A. Pauls. 2006. Multi-document summarization of evaluative text. In In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Gueorgi Kossinets</author>
<author>Jon Kleinber g</author>
<author>Lillian Lee</author>
</authors>
<title>How opinions are received by online communities: A case study on Amazon.com helpfulness votes.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>141--150</pages>
<marker>Danescu-Niculescu-Mizil, Kossinets, g, Lee, 2009</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets, Jon Kleinber g, and Lillian Lee. 2009. How opinions are received by online communities: A case study on Amazon.com helpfulness votes. In Proceedings of WWW, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kavita Ganesan</author>
<author>ChengXiang Zhai</author>
<author>Jiawei Han</author>
</authors>
<title>Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>340--348</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Ganesan, Zhai, Han, 2010</marker>
<rawString>Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010. Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 340– 348, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anindya Ghose</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Estimating the socio-economic impact of product reviews.</title>
<date>2008</date>
<booktitle>In NYU Stern Research Working Paper CeDER.</booktitle>
<marker>Ghose, Ipeirotis, 2008</marker>
<rawString>Anindya Ghose and Panagiotis G. Ipeirotis. 2008. Estimating the socio-economic impact of product reviews. In NYU Stern Research Working Paper CeDER.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6397" citStr="Hu and Liu, 2004" startWordPosition="957" endWordPosition="960">inguistic features. These findings seem to suggest that the review helpfulness model should be domaindependent, due to the specific semantics of “helpfulness” defined in context of the domain. 2.2 Review summarization One major paradigm of review summarization is aspect-based summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper </context>
<context position="17975" citStr="Hu and Liu, 2004" startWordPosition="2750" endWordPosition="2753">s) from which the sentence is extracted. The third one (M3) works together with a specific summarization algorithm, interpolating traditional informativeness assessment with novel helpfulness metrics based on the proposed helpfulness models. For demonstration, we plan to prototype the proposed framework based on MEAD* (Carenini et al., 2006), which is an extension of MEAD (an opensource framework for multi-document summarization (Radev et al., 2004)) for summarizing evaluative text. MEAD* defines sentence informativeness based on features extracted through standard aspect-based review mining (Hu and Liu, 2004). As a human-centric design, we plan to evaluate the proposed framework in a user study in terms of pairwise comparison of the reviews generated by different summarizers (M1, M2, M3 and MEAD*). Although fully automated summarization metrics are available (e.g. Jensen-Shannon Divergence (Louis and Nenkova, 2009)), they favor summaries that have a similar word distribution to the input and thus do not suit our task of review summarization. To show the generality of the proposed ideas, we plan to evaluate the utility of introducing review helpfulness in aspect ranking as well, which is an importa</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Jindal</author>
<author>B Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the international conference on Web</booktitle>
<pages>219--230</pages>
<contexts>
<context position="10848" citStr="Jindal and Liu (2008)" startWordPosition="1640" endWordPosition="1643">Therefore, we hypothesize modeling review content together with review helpfulness is beneficial to review summarization as well. 3 Data We plan to experiment on three representative review domains: product reviews, book reviews and peer reviews. The first one is mostly studied, while the later two types are more complex, as the review content consists of both reviewer’s evaluations of the target and reviewer’s references to the target, which is also text. This property makes review summarization more challenging. For product reviews and book reviews, we plan to use Amazon reviews provided by Jindal and Liu (2008), which is a widely used data set in review mining and sentiment analysis. We consider the helpfulness assessment of an Amazon review as the ratio of “helpful” votes over all votes (Kim et al., 2006). For educational peer reviews, we plan to use an annotated corpus (Nelson and Schunn, 2009) collected from an online peer-review reciprocal system, which we used in our prior work (Xiong and Litman, 2011). Two experts (a writing instructor and a content instructor) were asked to rate the helpfulness of each peer review on a scale from one to five (Pearson correlation r = 0.425, p ≤ 0.01). For our </context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>N. Jindal and B. Liu. 2008. Opinion spam and analysis. In Proceedings of the international conference on Web search and web data mining, pages 219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H D Kim</author>
<author>C X Zhai</author>
</authors>
<title>Generating comparative summaries of contradictory opinions in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management,</booktitle>
<pages>385--394</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8469" citStr="Kim and Zhai (2009)" startWordPosition="1278" endWordPosition="1281">input text within a shorter length. Either extractively or abstractively, 78 one important task is to determine the informativeness of a text element. In addition to reducing information redundancy, different heuristics were proposed within the context of opinion summarization. Stoyanov and Cardie (2008) focused on identifying opinion entities (opinion, source, target) and presenting them in a structured way (templates or diagrams). Lerman et. al (2009) reported that users preferred sentiment informed summaries based on their analysis of human evaluation of various summarization models, while Kim and Zhai (2009) further considered an effective review summary as representative contrastive opinion pairs. Different from all above, Ganesan et. al (2010) represented text input as token-based graphs based on the token order in the string. They rank summary candidates by scoring paths after removing redundant information from the graph. For any summarization framework discussed above, the helpfulness of the review elements (e.g. sentences, opinion entities, or words), which can be derived from the review overall helpfulness, captures informativeness from another dimension that has not been taken into accoun</context>
</contexts>
<marker>Kim, Zhai, 2009</marker>
<rawString>H.D. Kim and C.X. Zhai. 2009. Generating comparative summaries of contradictory opinions in text. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 385–394. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Patrick Pantel</author>
<author>Tim Chklovski</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Automatically assessing review helpfulness.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006),</booktitle>
<pages>423--430</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="11047" citStr="Kim et al., 2006" startWordPosition="1675" endWordPosition="1678">uct reviews, book reviews and peer reviews. The first one is mostly studied, while the later two types are more complex, as the review content consists of both reviewer’s evaluations of the target and reviewer’s references to the target, which is also text. This property makes review summarization more challenging. For product reviews and book reviews, we plan to use Amazon reviews provided by Jindal and Liu (2008), which is a widely used data set in review mining and sentiment analysis. We consider the helpfulness assessment of an Amazon review as the ratio of “helpful” votes over all votes (Kim et al., 2006). For educational peer reviews, we plan to use an annotated corpus (Nelson and Schunn, 2009) collected from an online peer-review reciprocal system, which we used in our prior work (Xiong and Litman, 2011). Two experts (a writing instructor and a content instructor) were asked to rate the helpfulness of each peer review on a scale from one to five (Pearson correlation r = 0.425, p ≤ 0.01). For our study, we consider the average ratings given by the two experts (which roughly follow a normal distribution) as the gold standard of review helpfulness ratings. To be consistent with the other review</context>
<context position="15037" citStr="Kim et al., 2006" startWordPosition="2311" endWordPosition="2314">-based approach to identify these topic terms and filter them out to reduce the impact of external content. 1Url: http://www.liwc.net. We are using LIWC2007. Considering how review usefulness would be integrated in the proposed summarization framework, we propose two models for predicting review helpfulness at different levels of granularity. A discriminative model to learn review global helpfulness. Previously we (2011) built a discriminative model for predicting the helpfulness of educational peer reviews based on prior work of automatically predicting review helpfulness of product reviews (Kim et al., 2006). We considered both domain-general features and domain-specific features. The domain-general features include structure features (e.g. review length), semantic features, and descriptive statistics of the product ratings (Kim et al., 2006); the domain-specific features include the percentage of external content in reviews and cognitive and social science features that are specific to the peer-review domain. To extend this idea to other types of reviews: for product reviews, we consider product aspect-related terms as the topic words of the external content; for book reviews, we take into accou</context>
</contexts>
<marker>Kim, Pantel, Chklovski, Pennacchiotti, 2006</marker>
<rawString>Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pennacchiotti. 2006. Automatically assessing review helpfulness. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006), pages 423–430, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lerman</author>
<author>S Blair-Goldensohn</author>
<author>R McDonald</author>
</authors>
<title>Sentiment summarization: Evaluating and learning user preferences.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>514--522</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1933" citStr="Lerman et al., 2009" startWordPosition="284" endWordPosition="287">n, which aims to differentiate and summarize opinions regarding specific subject aspects. It usually involves fine-grained analysis of both review topics and review sentiment. The other is more summarization-oriented, prior work under this category either assumes a shared topic or aims to 77 produce general summaries. In this case, the focus is the summarization, extracting salient information from reviews and organizing them properly. Compared with traditional text summarizers, sentimentinformed summarizers generally perform better as shown by human evaluation results (Carenini et al., 2006; Lerman et al., 2009). However, one implicit assumption shared by most prior work is that all reviews are of the same utility in review summarization tasks, while reviews that comment on the same aspect and are associated with the same rating may have difference influence to users, as indicated by user-provided helpfulness assessment (e.g. “helpful” votes on Amazon.com). We believe that user-generated helpfulness votes/ratings suggest people’s point of interest in review exploration. Intuitively, when users refer to online reviews for guidance, reviews that are considered helpful by more people naturally receive m</context>
</contexts>
<marker>Lerman, Blair-Goldensohn, McDonald, 2009</marker>
<rawString>K. Lerman, S. Blair-Goldensohn, and R. McDonald. 2009. Sentiment summarization: Evaluating and learning user preferences. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 514–522. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatically evaluating content selection in summarization without human models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>306--314</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18287" citStr="Louis and Nenkova, 2009" startWordPosition="2800" endWordPosition="2803">based on MEAD* (Carenini et al., 2006), which is an extension of MEAD (an opensource framework for multi-document summarization (Radev et al., 2004)) for summarizing evaluative text. MEAD* defines sentence informativeness based on features extracted through standard aspect-based review mining (Hu and Liu, 2004). As a human-centric design, we plan to evaluate the proposed framework in a user study in terms of pairwise comparison of the reviews generated by different summarizers (M1, M2, M3 and MEAD*). Although fully automated summarization metrics are available (e.g. Jensen-Shannon Divergence (Louis and Nenkova, 2009)), they favor summaries that have a similar word distribution to the input and thus do not suit our task of review summarization. To show the generality of the proposed ideas, we plan to evaluate the utility of introducing review helpfulness in aspect ranking as well, which is an important sub-task of review opinion analysis. If our hypothesis (1) is true, we would expect aspect ranking based on helpfulness-involved metrics outperforming the baseline which does not use review helpfulness (Yu et al., 2011). This evaluation will be done on product reviews and peer reviews, as the previous work w</context>
</contexts>
<marker>Louis, Nenkova, 2009</marker>
<rawString>Annie Louis and Ani Nenkova. 2009. Automatically evaluating content selection in summarization without human models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 306–314. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lu</author>
<author>C Zhai</author>
</authors>
<title>Opinion integration through semi-supervised topic modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web,</booktitle>
<pages>121--130</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6525" citStr="Lu and Zhai, 2008" startWordPosition="978" endWordPosition="981">ific semantics of “helpfulness” defined in context of the domain. 2.2 Review summarization One major paradigm of review summarization is aspect-based summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). However, little attention is given to differentiating review content directly regarding their utilities in </context>
</contexts>
<marker>Lu, Zhai, 2008</marker>
<rawString>Y. Lu and C. Zhai. 2008. Opinion integration through semi-supervised topic modeling. In Proceedings of the 17th international conference on World Wide Web, pages 121–130. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>X Ling</author>
<author>M Wondra</author>
<author>H Su</author>
<author>C X Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>171--180</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6506" citStr="Mei et al., 2007" startWordPosition="974" endWordPosition="977">t, due to the specific semantics of “helpfulness” defined in context of the domain. 2.2 Review summarization One major paradigm of review summarization is aspect-based summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). However, little attention is given to differentiating review content directly regarding </context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Q. Mei, X. Ling, M. Wondra, H. Su, and C.X. Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, pages 171–180. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Mudambi</author>
<author>D Schuff</author>
</authors>
<title>What makes a helpful online review? a study of customer reviews on amazon.</title>
<date>2010</date>
<booktitle>com. MIS quarterly,</booktitle>
<pages>34--1</pages>
<marker>Mudambi, Schuff, 2010</marker>
<rawString>S.M. Mudambi and D. Schuff. 2010. What makes a helpful online review? a study of customer reviews on amazon. com. MIS quarterly, 34(1):185–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>B Liu</author>
</authors>
<title>aspect extraction through semi-supervised modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of 50th anunal meeting of association for computational Linguistics</booktitle>
<note>(acL-2012)(accepted for publication).</note>
<contexts>
<context position="6655" citStr="Mukherjee and Liu, 2012" startWordPosition="999" endWordPosition="1002">arization is aspect-based summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). However, little attention is given to differentiating review content directly regarding their utilities in review exploration. Mukherjee and Liu (2012) attempted to address this issue by introducing user-provided aspect terms as seeds fo</context>
<context position="9445" citStr="Mukherjee and Liu (2012)" startWordPosition="1427" endWordPosition="1430">work discussed above, the helpfulness of the review elements (e.g. sentences, opinion entities, or words), which can be derived from the review overall helpfulness, captures informativeness from another dimension that has not been taken into account yet. 2.3 Supervised content modeling As review summarization is meant to help users acquire useful information effectively, what and how to summarize may vary with user needs. To discover user preferences, Ando and Ishizaki (2012) manually analyzed travel reviews to identify the most influential review sentences objectively and subjectively, while Mukherjee and Liu (2012) extract and categorize review aspects through semi-supervised modeling using user-provided seeds (categories of terms). In contrast, we are interested in using userprovided helpfulness ratings for guidance. As these helpfulness ratings are existing meta data of reviews, we will need no additional input from users. Specifically, we propose to use supervised LDA (Blei and McAuliffe, 2010) to model review content under the supervision of review helpfulness ratings. Similar approach is widely adopted in sentiment analysis, where review aspects are learned in the presence of sentiment predictions </context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>A. Mukherjee and B. Liu. 2012. aspect extraction through semi-supervised modeling. In Proceedings of 50th anunal meeting of association for computational Linguistics (acL-2012)(accepted for publication).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melissa M Nelson</author>
<author>Christian D Schunn</author>
</authors>
<title>The nature of feedback: how different types of peer feedback affect writing performance.</title>
<date>2009</date>
<journal>In Instructional Science,</journal>
<volume>37</volume>
<pages>375--401</pages>
<contexts>
<context position="11139" citStr="Nelson and Schunn, 2009" startWordPosition="1690" endWordPosition="1693">e later two types are more complex, as the review content consists of both reviewer’s evaluations of the target and reviewer’s references to the target, which is also text. This property makes review summarization more challenging. For product reviews and book reviews, we plan to use Amazon reviews provided by Jindal and Liu (2008), which is a widely used data set in review mining and sentiment analysis. We consider the helpfulness assessment of an Amazon review as the ratio of “helpful” votes over all votes (Kim et al., 2006). For educational peer reviews, we plan to use an annotated corpus (Nelson and Schunn, 2009) collected from an online peer-review reciprocal system, which we used in our prior work (Xiong and Litman, 2011). Two experts (a writing instructor and a content instructor) were asked to rate the helpfulness of each peer review on a scale from one to five (Pearson correlation r = 0.425, p ≤ 0.01). For our study, we consider the average ratings given by the two experts (which roughly follow a normal distribution) as the gold standard of review helpfulness ratings. To be consistent with the other review domains, we normalize peer-review helpfulness ratings in the range between 0 and 1. 4 Propo</context>
</contexts>
<marker>Nelson, Schunn, 2009</marker>
<rawString>Melissa M. Nelson and Christian D. Schunn. 2009. The nature of feedback: how different types of peer feedback affect writing performance. In Instructional Science, volume 37, pages 375–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>339--346</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6425" citStr="Popescu and Etzioni, 2005" startWordPosition="961" endWordPosition="964">. These findings seem to suggest that the review helpfulness model should be domaindependent, due to the specific semantics of “helpfulness” defined in context of the domain. 2.2 Review summarization One major paradigm of review summarization is aspect-based summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). However</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A.M. Popescu and O. Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 339–346. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>T Allison</author>
<author>S Blair-Goldensohn</author>
<author>J Blitzer</author>
<author>A Celebi</author>
<author>S Dimitrov</author>
<author>E Drabek</author>
<author>A Hakim</author>
<author>W Lam</author>
<author>D Liu</author>
</authors>
<title>Mead-a platform for multidocument multilingual text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>volume</volume>
<contexts>
<context position="17811" citStr="Radev et al., 2004" startWordPosition="2727" endWordPosition="2730">e helpfulness of a summary sentence can be either inferred from the local-helpfulness model (sLDA), or aggregated from review-level helpfulness ratings of the review(s) from which the sentence is extracted. The third one (M3) works together with a specific summarization algorithm, interpolating traditional informativeness assessment with novel helpfulness metrics based on the proposed helpfulness models. For demonstration, we plan to prototype the proposed framework based on MEAD* (Carenini et al., 2006), which is an extension of MEAD (an opensource framework for multi-document summarization (Radev et al., 2004)) for summarizing evaluative text. MEAD* defines sentence informativeness based on features extracted through standard aspect-based review mining (Hu and Liu, 2004). As a human-centric design, we plan to evaluate the proposed framework in a user study in terms of pairwise comparison of the reviews generated by different summarizers (M1, M2, M3 and MEAD*). Although fully automated summarization metrics are available (e.g. Jensen-Shannon Divergence (Louis and Nenkova, 2009)), they favor summaries that have a similar word distribution to the input and thus do not suit our task of review summariza</context>
</contexts>
<marker>Radev, Allison, Blair-Goldensohn, Blitzer, Celebi, Dimitrov, Drabek, Hakim, Lam, Liu, 2004</marker>
<rawString>D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. Celebi, S. Dimitrov, E. Drabek, A. Hakim, W. Lam, D. Liu, et al. 2004. Mead-a platform for multidocument multilingual text summarization. In Proceedings of LREC, volume 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sauper</author>
<author>R Barzilay</author>
</authors>
<title>Automatic aggregation by joint modeling of aspects and values.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>46--89</pages>
<contexts>
<context position="6683" citStr="Sauper and Barzilay, 2013" startWordPosition="1003" endWordPosition="1006"> summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). However, little attention is given to differentiating review content directly regarding their utilities in review exploration. Mukherjee and Liu (2012) attempted to address this issue by introducing user-provided aspect terms as seeds for learning review aspects, t</context>
</contexts>
<marker>Sauper, Barzilay, 2013</marker>
<rawString>C. Sauper and R. Barzilay. 2013. Automatic aggregation by joint modeling of aspects and values. Journal of Artificial Intelligence Research, 46:89–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
</authors>
<title>Topic identification for fine-grained opinion analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>817--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8155" citStr="Stoyanov and Cardie (2008)" startWordPosition="1230" endWordPosition="1233">ire the step of identifying aspects, instead, they either assume the input text share the same aspect or aim to produce general summaries. These studies are closely related to the traditional NLP task of text summarization. Generally speaking, the goal of text summarization is to retain the most important points of the input text within a shorter length. Either extractively or abstractively, 78 one important task is to determine the informativeness of a text element. In addition to reducing information redundancy, different heuristics were proposed within the context of opinion summarization. Stoyanov and Cardie (2008) focused on identifying opinion entities (opinion, source, target) and presenting them in a structured way (templates or diagrams). Lerman et. al (2009) reported that users preferred sentiment informed summaries based on their analysis of human evaluation of various summarization models, while Kim and Zhai (2009) further considered an effective review summary as representative contrastive opinion pairs. Different from all above, Ganesan et. al (2010) represented text input as token-based graphs based on the token order in the string. They rank summary candidates by scoring paths after removing</context>
</contexts>
<marker>Stoyanov, Cardie, 2008</marker>
<rawString>V. Stoyanov and C. Cardie. 2008. Topic identification for fine-grained opinion analysis. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 817–824. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<pages>51--61801</pages>
<location>Urbana,</location>
<contexts>
<context position="6551" citStr="Titov and McDonald, 2008" startWordPosition="982" endWordPosition="986">helpfulness” defined in context of the domain. 2.2 Review summarization One major paradigm of review summarization is aspect-based summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). However, little attention is given to differentiating review content directly regarding their utilities in review exploration. Mukher</context>
<context position="10096" citStr="Titov and McDonald, 2008" startWordPosition="1522" endWordPosition="1525">iew aspects through semi-supervised modeling using user-provided seeds (categories of terms). In contrast, we are interested in using userprovided helpfulness ratings for guidance. As these helpfulness ratings are existing meta data of reviews, we will need no additional input from users. Specifically, we propose to use supervised LDA (Blei and McAuliffe, 2010) to model review content under the supervision of review helpfulness ratings. Similar approach is widely adopted in sentiment analysis, where review aspects are learned in the presence of sentiment predictions (Blei and McAuliffe, 2010; Titov and McDonald, 2008a). Furthermore, Branavan et. al (2009) showed that joint modeling of text and user annotations benefits extractive summarization. Therefore, we hypothesize modeling review content together with review helpfulness is beneficial to review summarization as well. 3 Data We plan to experiment on three representative review domains: product reviews, book reviews and peer reviews. The first one is mostly studied, while the later two types are more complex, as the review content consists of both reviewer’s evaluations of the target and reviewer’s references to the target, which is also text. This pro</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>I. Titov and R. McDonald. 2008a. A joint model of text and aspect ratings for sentiment summarization. Urbana, 51:61801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web,</booktitle>
<pages>111--120</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6551" citStr="Titov and McDonald, 2008" startWordPosition="982" endWordPosition="986">helpfulness” defined in context of the domain. 2.2 Review summarization One major paradigm of review summarization is aspect-based summarization, which is based on identifying aspects and associating opinion sentiment with them. (Although this line of work is closely related to sentiment analysis, it is not the focus of this proposed work.) While initially people use information retrieval techniques to recognize aspect terms and opinion expressions (Hu and Liu, 2004; Popescu and Etzioni, 2005), recent work seems to favor generative statistical models more (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a; Blei and McAuliffe, 2010; Brody and Elhadad, 2010; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). One typical problem with these models is that many discovered aspects are not meaningful to end-users. Some of these studies focus on distinguishing aspects in terms of sentiment variation by modeling aspects together with sentiment (Titov and McDonald, 2008a; Lu and Zhai, 2008; Mukherjee and Liu, 2012; Sauper and Barzilay, 2013). However, little attention is given to differentiating review content directly regarding their utilities in review exploration. Mukher</context>
<context position="10096" citStr="Titov and McDonald, 2008" startWordPosition="1522" endWordPosition="1525">iew aspects through semi-supervised modeling using user-provided seeds (categories of terms). In contrast, we are interested in using userprovided helpfulness ratings for guidance. As these helpfulness ratings are existing meta data of reviews, we will need no additional input from users. Specifically, we propose to use supervised LDA (Blei and McAuliffe, 2010) to model review content under the supervision of review helpfulness ratings. Similar approach is widely adopted in sentiment analysis, where review aspects are learned in the presence of sentiment predictions (Blei and McAuliffe, 2010; Titov and McDonald, 2008a). Furthermore, Branavan et. al (2009) showed that joint modeling of text and user annotations benefits extractive summarization. Therefore, we hypothesize modeling review content together with review helpfulness is beneficial to review summarization as well. 3 Data We plan to experiment on three representative review domains: product reviews, book reviews and peer reviews. The first one is mostly studied, while the later two types are more complex, as the review content consists of both reviewer’s evaluations of the target and reviewer’s references to the target, which is also text. This pro</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>I. Titov and R. McDonald. 2008b. Modeling online reviews with multi-grain topic models. In Proceedings of the 17th international conference on World Wide Web, pages 111–120. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenting Xiong</author>
<author>Diane Litman</author>
</authors>
<title>Automatically predicting peer-review helpfulness.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>502--507</pages>
<contexts>
<context position="11252" citStr="Xiong and Litman, 2011" startWordPosition="1709" endWordPosition="1712">d reviewer’s references to the target, which is also text. This property makes review summarization more challenging. For product reviews and book reviews, we plan to use Amazon reviews provided by Jindal and Liu (2008), which is a widely used data set in review mining and sentiment analysis. We consider the helpfulness assessment of an Amazon review as the ratio of “helpful” votes over all votes (Kim et al., 2006). For educational peer reviews, we plan to use an annotated corpus (Nelson and Schunn, 2009) collected from an online peer-review reciprocal system, which we used in our prior work (Xiong and Litman, 2011). Two experts (a writing instructor and a content instructor) were asked to rate the helpfulness of each peer review on a scale from one to five (Pearson correlation r = 0.425, p ≤ 0.01). For our study, we consider the average ratings given by the two experts (which roughly follow a normal distribution) as the gold standard of review helpfulness ratings. To be consistent with the other review domains, we normalize peer-review helpfulness ratings in the range between 0 and 1. 4 Proposed work The proposed thesis work consists of three parts: 1) review content analysis using user-provided helpful</context>
</contexts>
<marker>Xiong, Litman, 2011</marker>
<rawString>Wenting Xiong and Diane Litman. 2011. Automatically predicting peer-review helpfulness. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 502–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yu</author>
<author>Z J Zha</author>
<author>M Wang</author>
<author>T S Chua</author>
</authors>
<title>Aspect ranking: identifying important product aspects from online consumer reviews. Computational Linguistics,</title>
<date>2011</date>
<pages>1496--1505</pages>
<contexts>
<context position="18797" citStr="Yu et al., 2011" startWordPosition="2883" endWordPosition="2886">ly automated summarization metrics are available (e.g. Jensen-Shannon Divergence (Louis and Nenkova, 2009)), they favor summaries that have a similar word distribution to the input and thus do not suit our task of review summarization. To show the generality of the proposed ideas, we plan to evaluate the utility of introducing review helpfulness in aspect ranking as well, which is an important sub-task of review opinion analysis. If our hypothesis (1) is true, we would expect aspect ranking based on helpfulness-involved metrics outperforming the baseline which does not use review helpfulness (Yu et al., 2011). This evaluation will be done on product reviews and peer reviews, as the previous work was based on product reviews, while peer reviews tend to have an objective aspect ranking (provided by domain experts). 5 Contributions The proposed thesis mainly contributes to review mining and summarization. 1. Investigate the impact of the source of review content on review helpfulness. While a lot of studies focus on product reviews, we based our analysis on a wider range of domains, including peer reviews, which have not been well studied before. 2. Propose two models to automatically assess review h</context>
</contexts>
<marker>Yu, Zha, Wang, Chua, 2011</marker>
<rawString>J. Yu, Z.J. Zha, M. Wang, and T.S. Chua. 2011. Aspect ranking: identifying important product aspects from online consumer reviews. Computational Linguistics, pages 1496–1505.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>