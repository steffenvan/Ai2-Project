<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000130">
<title confidence="0.990616">
Comparable Entity Mining from Comparative Questions
</title>
<author confidence="0.997633">
Shasha Li1,Chin-Yew Lin2,Young-In Song2,Zhoujun Li3
</author>
<affiliation confidence="0.861801">
1National University of Defense Technology, Changsha, China
2Microsoft Research Asia, Beijing, China
3Beihang University, Beijing, China
</affiliation>
<email confidence="0.944571">
shashali@nudt.edu.cn1, {cyl,yosong}@microsoft.com2,
lizj@buaa.edu.cn3
</email>
<sectionHeader confidence="0.998425" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999617117647059">
Comparing one thing with another is a typical
part of human decision making process. How-
ever, it is not always easy to know what to
compare and what are the alternatives. To ad-
dress this difficulty, we present a novel way to
automatically mine comparable entities from
comparative questions that users posted on-
line. To ensure high precision and high recall,
we develop a weakly-supervised bootstrapping
method for comparative question identification
and comparable entity extraction by leveraging
a large online question archive. The experi-
mental results show our method achieves F1-
measure of 82.5% in comparative question
identification and 83.3% in comparable entity
extraction. Both significantly outperform an
existing state-of-the-art method.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999849536585366">
Comparing alternative options is one essential
step in decision-making that we carry out every
day. For example, if someone is interested in cer-
tain products such as digital cameras, he or she
would want to know what the alternatives are
and compare different cameras before making a
purchase. This type of comparison activity is
very common in our daily life but requires high
knowledge skill. Magazines such as Consumer
Reports and PC Magazine and online media such
as CNet.com strive in providing editorial com-
parison content and surveys to satisfy this need.
In the World Wide Web era, a comparison ac-
tivity typically involves: search for relevant web
pages containing information about the targeted
products, find competing products, read reviews,
and identify pros and cons. In this paper, we fo-
cus on finding a set of comparable entities given
a user&apos;s input entity. For example, given an enti-
ty, Nokia N95 (a cellphone), we want to find
comparable entities such as Nokia N82, iPhone
and so on.
In general, it is difficult to decide if two enti-
ties are comparable or not since people do com-
pare apples and oranges for various reasons. For
example, “Ford” and “BMW” might be compa-
rable as “car manufacturers” or as “market seg-
ments that their products are targeting”, but we
rarely see people comparing “Ford Focus” (car
model) and “BMW 328i”. Things also get more
complicated when an entity has several functio-
nalities. For example, one might compare
“iPhone” and “PSP” as “portable game player”
while compare “iPhone” and “Nokia N95” as
“mobile phone”. Fortunately, plenty of compara-
tive questions are posted online, which provide
evidences for what people want to compare, e.g.
“Which to buy, iPod or iPhone?”. We call “iPod”
and “iPhone” in this example as comparators. In
this paper, we define comparative questions and
comparators as:
</bodyText>
<listItem confidence="0.981893">
• Comparative question: A question that in-
tends to compare two or more entities and it
has to mention these entities explicitly in the
question.
• Comparator: An entity which is a target of
comparison in a comparative question.
</listItem>
<bodyText confidence="0.743186">
According to these definitions, Q1 and Q2 be-
low are not comparative questions while Q3 is.
“iPod Touch” and “Zune HD” are comparators.
</bodyText>
<listItem confidence="0.84942325">
Q1: “Which one is better?”
Q2: “Is Lumix GH-1 the best camera?”
Q3: “What&apos;s the difference between iPod
Touch and Zune HD?”
</listItem>
<bodyText confidence="0.983239333333333">
The goal of this work is mining comparators
from comparative questions. The results would
be very useful in helping users&apos; exploration of
</bodyText>
<page confidence="0.972124">
650
</page>
<note confidence="0.94341">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 650–658,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999894861111111">
alternative choices by suggesting comparable
entities based on other users‟ prior requests.
To mine comparators from comparative ques-
tions, we first have to detect whether a question
is comparative or not. According to our defini-
tion, a comparative question has to be a question
with intent to compare at least two entities.
Please note that a question containing at least
two entities is not a comparative question if it
does not have comparison intent. However, we
observe that a question is very likely to be a
comparative question if it contains at least two
entities. We leverage this insight and develop a
weakly supervised bootstrapping method to iden-
tify comparative questions and extract compara-
tors simultaneously.
To our best knowledge, this is the first attempt
to specially address the problem on finding good
comparators to support users‟ comparison activi-
ty. We are also the first to propose using com-
parative questions posted online that reflect what
users truly care about as the medium from which
we mine comparable entities. Our weakly super-
vised method achieves 82.5% F1-measure in
comparative question identification, 83.3% in
comparator extraction, and 76.8% in end-to-end
comparative question identification and compa-
rator extraction which outperform the most rele-
vant state-of-the-art method by Jindal &amp; Liu
(2006b) significantly.
The rest of this paper is organized as follows.
The next section discusses previous works. Sec-
tion 3 presents our weakly-supervised method for
comparator mining. Section 4 reports the evalua-
tions of our techniques, and we conclude the pa-
per and discuss future work in Section 5.
</bodyText>
<sectionHeader confidence="0.999863" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.909626">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.99994695652174">
In terms of discovering related items for an enti-
ty, our work is similar to the research on recom-
mender systems, which recommend items to a
user. Recommender systems mainly rely on simi-
larities between items and/or their statistical cor-
relations in user log data (Linden et al., 2003).
For example, Amazon recommends products to
its customers based on their own purchase histo-
ries, similar customers‟ purchase histories, and
similarity between products. However, recom-
mending an item is not equivalent to finding a
comparable item. In the case of Amazon, the
purpose of recommendation is to entice their cus-
tomers to add more items to their shopping carts
by suggesting similar or related items. While in
the case of comparison, we would like to help
users explore alternatives, i.e. helping them make
a decision among comparable items.
For example, it is reasonable to recommend
“iPod speaker” or “iPod batteries” if a user is
interested in “iPod”, but we would not compare
them with “iPod”. However, items that are com-
parable with “iPod” such as “iPhone” or “PSP”
which were found in comparative questions post-
ed by users are difficult to be predicted simply
based on item similarity between them. Although
they are all music players, “iPhone” is mainly a
mobile phone, and “PSP” is mainly a portable
game device. They are similar but also different
therefore beg comparison with each other. It is
clear that comparator mining and item recom-
mendation are related but not the same.
Our work on comparator mining is related to
the research on entity and relation extraction in
information extraction (Cardie, 1997; Califf and
Mooney, 1999; Soderland, 1999; Radev et al.,
2002; Carreras et al., 2003). Specifically, the
most relevant work is by Jindal and Liu (2006a
and 2006b) on mining comparative sentences and
relations. Their methods applied class sequential
rules (CSR) (Chapter 2, Liu 2006) and label se-
quential rules (LSR) (Chapter 2, Liu 2006)
learned from annotated corpora to identify com-
parative sentences and extract comparative rela-
tions respectively in the news and review do-
mains. The same techniques can be applied to
comparative question identification and compa-
rator mining from questions. However, their me-
thods typically can achieve high precision but
suffer from low recall (Jindal and Liu, 2006b)
(J&amp;L). However, ensuring high recall is crucial
in our intended application scenario where users
can issue arbitrary queries. To address this prob-
lem, we develop a weakly-supervised bootstrap-
ping pattern learning method by effectively leve-
raging unlabeled questions.
Bootstrapping methods have been shown to be
very effective in previous information extraction
research (Riloff, 1996; Riloff and Jones, 1999;
Ravichandran and Hovy, 2002; Mooney and Bu-
nescu, 2005; Kozareva et al., 2008). Our work is
similar to them in terms of methodology using
bootstrapping technique to extract entities with a
specific relation. However, our task is different
from theirs in that it requires not only extracting
entities (comparator extraction) but also ensuring
that the entities are extracted from comparative
questions (comparative question identification),
which is generally not required in IE task.
</bodyText>
<page confidence="0.997671">
651
</page>
<subsectionHeader confidence="0.99856">
2.2 Jindal &amp; Liu 2006
</subsectionHeader>
<bodyText confidence="0.999746333333333">
In this subsection, we provide a brief summary
of the comparative mining method proposed by
Jindal and Liu (2006a and 2006b), which is used
as baseline for comparison and represents the
state-of-the-art in this area. We first introduce
the definition of CSR and LSR rule used in their
approach, and then describe their comparative
mining method. Readers should refer to J&amp;L&apos;s
original papers for more details.
</bodyText>
<sectionHeader confidence="0.896185" genericHeader="method">
CSR and LSR
</sectionHeader>
<bodyText confidence="0.992372347826087">
CSR is a classification rule. It maps a sequence
pattern S(s1s2 ... sn) to a class C. In our problem,
C is either comparative or non-comparative.
Given a collection of sequences with class in-
formation, every CSR is associated to two para-
meters: support and confidence. Support is the
proportion of sequences in the collection contain-
ing S as a subsequence. Confidence is the propor-
tion of sequences labeled as C in the sequences
containing the S. These parameters are important
to evaluate whether a CSR is reliable or not.
LSR is a labeling rule. It maps an input se-
quence pattern S(s1s2 ... si ... sn) to a labeled
sequence S′(s1s2 ... li ... sn) by replacing one to-
ken (si) in the input sequence with a designated
label (li). This token is referred as the anchor.
The anchor in the input sequence could be ex-
tracted if its corresponding label in the labeled
sequence is what we want (in our case, a compa-
rator). LSRs are also mined from an annotated
corpus, therefore each LSR also have two para-
meters: support and confidence. They are simi-
larly defined as in CSR.
</bodyText>
<subsectionHeader confidence="0.963147">
Supervised Comparative Mining Method
</subsectionHeader>
<bodyText confidence="0.999326454545455">
J&amp;L treated comparative sentence identification
as a classification problem and comparative rela-
tion extraction as an information extraction prob-
lem. They first manually created a set of 83 key-
words such as beat, exceed, and outperform that
are likely indicators of comparative sentences.
These keywords were then used as pivots to
create part-of-speech (POS) sequence data. A
manually annotated corpus with class informa-
tion, i.e. comparative or non-comparative, was
used to create sequences and CSRs were mined.
A Naïve Bayes classifier was trained using the
CSRs as features. The classifier was then used to
identify comparative sentences.
Given a set of comparative sentences, J&amp;L
manually annotated two comparators with labels
$ES1 and $ES2 and the feature compared with
label $FT for each sentence. J&amp;L&apos;s method was
only applied to noun and pronoun. To differen-
tiate noun and pronoun that are not comparators
or features, they added the fourth label $NEF, i.e.
non-entity-feature. These labels were used as
pivots together with special tokens li &amp; rj1 (token
position), #start (beginning of a sentence), and
#end (end of a sentence) to generate sequence
data, sequences with single label only and mini-
mum support greater than 1% are retained, and
then LSRs were created. When applying the
learned LSRs for extraction, LSRs with higher
confidence were applied first.
J&amp;L&apos;s method have been proved effective in
their experimental setups. However, it has the
following weaknesses:
</bodyText>
<listItem confidence="0.999764083333333">
• The performance of J&amp;L&apos;s method relies
heavily on a set of comparative sentence in-
dicative keywords. These keywords were
manually created and they offered no guide-
lines to select keywords for inclusion. It is
also difficult to ensure the completeness of
the keyword list.
• Users can express comparative sentences or
questions in many different ways. To have
high recall, a large annotated training corpus
is necessary. This is an expensive process.
• Example CSRs and LSRs given in Jindal &amp;
</listItem>
<bodyText confidence="0.758789384615384">
Liu (2006b) are mostly a combination of
POS tags and keywords. It is a surprise that
their rules achieved high precision but low
recall. They attributed most errors to POS
tagging errors. However, we suspect that
their rules might be too specific and overfit
their small training set (about 2,600 sen-
tences). We would like to increase recall,
avoid overfitting, and allow rules to include
discriminative lexical tokens to retain preci-
sion.
In the next section, we introduce our method to
address these shortcomings.
</bodyText>
<sectionHeader confidence="0.852778" genericHeader="method">
3 Weakly Supervised Method for Com-
parator Mining
</sectionHeader>
<bodyText confidence="0.856822857142857">
Our weakly supervised method is a pattern-based
approach similar to J&amp;L&apos;s method, but it is dif-
ferent in many aspects: Instead of using separate
CSRs and LSRs, our method aims to learn se-
1 li marks a token is at the ith position to the left of the pivot
and rj marks a token is at jth position to the right of the
pivot where i and j are between 1 and 4 in J&amp;L (2006b).
</bodyText>
<page confidence="0.991408">
652
</page>
<figure confidence="0.635428888888889">
quential patterns which can be used to identify
Sequential Patterns
&lt;#start which city is better, $C or $C ? #end&gt;
&lt;, $C or $C ? #end&gt;
&lt;#start $C/NN or $C/NN ? #end&gt;
&lt;which NN is better, $C or $C ?&gt;
&lt;which city is JJR, $C or $C ?&gt;
&lt;which NN is JJR, $C or $C ?&gt;
...
</figure>
<tableCaption confidence="0.900325333333333">
Table 1: Candidate indicative extraction pattern (IEP)
examples of the question “which city is better, NYC or
Paris?”
</tableCaption>
<bodyText confidence="0.999893970588235">
comparative question and extract comparators
simultaneously.
In our approach, a sequential pattern is defined
as a sequence S(s1s2 ... si ... sn) where si can be a
word, a POS tag, or a symbol denoting either a
comparator ($C), or the beginning (#start) or the
end of a question (#end). A sequential pattern is
called an indicative extraction pattern (IEP) if it
can be used to identify comparative questions
and extract comparators in them with high relia-
bility. We will formally define the reliability
score of a pattern in the next section.
Once a question matches an IEP, it is classified
as a comparative question and the token se-
quences corresponding to the comparator slots in
the IEP are extracted as comparators. When a
question can match multiple IEPs, the longest
IEP is used 2 . Therefore, instead of manually
creating a list of indicative keywords, we create a
set of IEPs. We will show how to acquire IEPs
automatically using a bootstrapping procedure
with minimum supervision by taking advantage
of a large unlabeled question collection in the
following subsections. The evaluations shown in
section 4 confirm that our weakly supervised
method can achieve high recall while retain high
precision.
This pattern definition is inspired by the work
of Ravichandran and Hovy (2002). Table 1
shows some examples of such sequential pat-
terns. We also allow POS constraint on compara-
tors as shown in the pattern “&lt;, $C/NN or $C/NN
? #end&gt;”. It means that a valid comparator must
have a NN POS tag.
</bodyText>
<subsectionHeader confidence="0.998621">
3.1 Mining Indicative Extraction Patterns
</subsectionHeader>
<bodyText confidence="0.81038975">
Our weakly supervised IEP mining approach is
based on two key assumptions:
2 It is because the longest IEP is likely to be the most specif-
ic and relevant pattern for the given question.
</bodyText>
<figureCaption confidence="0.997766">
Figure 1: Overview of the bootstrapping alogorithm
</figureCaption>
<listItem confidence="0.9880676">
• If a sequential pattern can be used to extract
many reliable comparator pairs, it is very likely
to be an IEP.
• If a comparator pair can be extracted by an
IEP, the pair is reliable.
</listItem>
<bodyText confidence="0.999421333333333">
Based on these two assumptions, we design
our bootstrapping algorithm as shown in Figure 1.
The bootstrapping process starts with a single
IEP. From it, we extract a set of initial seed com-
parator pairs. For each comparator pair, all ques-
tions containing the pair are retrieved from a
question collection and regarded as comparative
questions. From the comparative questions and
comparator pairs, all possible sequential patterns
are generated and evaluated by measuring their
reliability score defined later in the Pattern Eval-
uation section. Patterns evaluated as reliable ones
are IEPs and are added into an IEP repository.
Then, new comparator pairs are extracted from
the question collection using the latest IEPs. The
new comparators are added to a reliable compa-
rator repository and used as new seeds for pattern
learning in the next iteration. All questions from
which reliable comparators are extracted are re-
moved from the collection to allow finding new
patterns efficiently in later iterations. The
process iterates until no more new patterns can
be found from the question collection.
There are two key steps in our method: (1)
pattern generation and (2) pattern evaluation. In
the following subsections, we will explain them
in details.
</bodyText>
<subsectionHeader confidence="0.734135">
Pattern Generation
</subsectionHeader>
<bodyText confidence="0.999935">
To generate sequential patterns, we adapt the
surface text pattern mining method introduced in
(Ravichandran and Hovy, 2002). For any given
comparative question and its comparator pairs,
comparators in the question are replaced with
symbol $Cs. Two symbols, #start and #end, are
attached to the beginning and the end of a sen-
</bodyText>
<page confidence="0.997241">
653
</page>
<bodyText confidence="0.978125666666667">
tence in the question. Then, the following three
kinds of sequential patterns are generated from
sequences of questions:
</bodyText>
<listItem confidence="0.998451185185185">
• Lexical patterns: Lexical patterns indicate
sequential patterns consisting of only words
and symbols ($C, #start, and #end). They are
generated by suffix tree algorithm (Gusfield,
1997) with two constraints: A pattern should
contain more than one $C, and its frequency
in collection should be more than an empiri-
cally determined number 𝛽.
• Generalized patterns: A lexical pattern can
be too specific. Thus, we generalize lexical
patterns by replacing one or more words with
their POS tags. 2𝑛 − 1 generalized patterns
can be produced from a lexical pattern con-
taining N words excluding $Cs.
• Specialized patterns: In some cases, a pat-
tern can be too general. For example, al-
though a question “ipod or zune?” is com-
parative, the pattern “&lt;$C or $C&gt;” is too
general, and there can be many non-
comparative questions matching the pattern,
for instance, “true or false?”. For this reason,
we perform pattern specialization by adding
POS tags to all comparator slots. For exam-
ple, from the lexical pattern “&lt;$C or $C&gt;”
and the question “ipod or zune?”, “&lt;$C/NN
or $C/NN?&gt;” will be produced as a specia-
lized pattern.
</listItem>
<bodyText confidence="0.999860833333333">
Note that generalized patterns are generated from
lexical patterns and the specialized patterns are
generated from the combined set of generalized
patterns and lexical patterns. The final set of
candidate patterns is a mixture of lexical patterns,
generalized patterns and specialized patterns.
</bodyText>
<subsectionHeader confidence="0.807679">
Pattern Evaluation
</subsectionHeader>
<bodyText confidence="0.9999708">
According to our first assumption, a reliability
score 𝑅𝑘 (𝑝𝑖) for a candidate pattern 𝑝𝑖 at itera-
tion k can be defined as follows:
, where 𝑝𝑖 can extract known reliable comparator
pairs 𝑐𝑝𝑗. 𝐶𝑃𝑘−1 indicates the reliable compara-
tor pair repository accumulated until the
(𝑘 − 1)𝑡ℎ iteration. 𝑁𝑄 (𝑥) means the number of
questions satisfying a condition x. The condition
𝑝𝑖 → 𝑐𝑝𝑗 denotes that 𝑐𝑝𝑗 can be extracted from
a question by applying pattern 𝑝𝑖 while the con-
dition 𝑝𝑖 →∗ denotes any question containing
pattern 𝑝𝑖.
However, Equation (1) can suffer from in-
complete knowledge about reliable comparator
pairs. For example, very few reliable pairs are
generally discovered in early stage of bootstrap-
ping. In this case, the value of Equation (1)
might be underestimated which could affect the
effectiveness of equation (1) on distinguishing
IEPs from non-reliable patterns. We mitigate this
problem by a lookahead procedure. Let us denote
the set of candidate patterns at the iteration k by
𝑃 𝑘. We define the support 𝑆 for comparator pair
𝑐𝑝 𝑖 which can be extracted by 𝑃 𝑘 and does not
exist in the current reliable set:
</bodyText>
<equation confidence="0.996631">
𝑆 𝑐𝑝 𝑖 = 𝑁𝑄 ( 𝑃 𝑘 → 𝑐𝑝 𝑖) (2)
</equation>
<bodyText confidence="0.999121555555555">
where 𝑃 𝑘 → 𝑐𝑝 𝑖 means that one of the patterns in
𝑃 𝑘 can extract 𝑐𝑝 𝑖 in certain questions. Intuitive-
ly, if 𝑐𝑝 𝑖 can be extracted by many candidate
patterns in 𝑃 𝑘, it is likely to be extracted as a
reliable one in the next iteration. Based on this
intuition, a pair 𝑐𝑝 𝑖 whose support S is more than
a threshold 𝛼 is regarded as a likely-reliable pair.
Using likely-reliable pairs, lookahead reliability
score 𝑅 𝑝𝑖 is defined:
</bodyText>
<equation confidence="0.8974675">
𝑁𝑄(𝑝𝑖→∗) (3)
, where 𝐶𝑃 𝑟𝑒𝑙
</equation>
<bodyText confidence="0.9770445">
𝑘 indicates a set of likely-reliable
pairs based on 𝑃 𝑘.
By interpolating Equation (1) and (3), the final
reliability score 𝑅(𝑝𝑖)𝑓𝑖𝑛𝑎𝑙
𝑘 for a pattern is de-
fined as follows:
</bodyText>
<equation confidence="0.938298">
𝑅(𝑝𝑖)𝑓𝑖𝑛𝑎𝑙
𝑘 = 𝜆∙ 𝑅𝑘 𝑝𝑖 + (1 − 𝜆) ∙ 𝑅 𝑘(𝑝𝑖) (4)
</equation>
<bodyText confidence="0.9984815">
Using Equation (4), we evaluate all candidate
patterns and select patterns whose score is more
than threshold 𝛾 as IEPs. All necessary parame-
ter values are empirically determined. We will
explain how to determine our parameters in sec-
tion 4.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994364">
4.1 Experiment Setup
Source Data
</subsectionHeader>
<bodyText confidence="0.999560666666667">
All experiments were conducted on about 60M
questions mined from Yahoo! Answers‟ question
title field. The reason that we used only a title
</bodyText>
<equation confidence="0.997641125">
𝑅𝑘 𝑝𝑖 =
∀𝑐𝑝𝑗 ∈𝐶𝑃 𝑘−1
𝑁𝑄(𝑝𝑖→∗) (1)
𝑁𝑄 (𝑝𝑖→𝑐𝑝𝑗 )
𝑅 𝑘 𝑝𝑖 =
∀𝑐𝑝 𝑖∈𝐶𝑃 𝑟𝑒𝑙
𝑘
𝑁𝑄 (𝑝 𝑖→𝑐𝑝 i)
</equation>
<page confidence="0.994308">
654
</page>
<bodyText confidence="0.999489333333333">
field is that they clearly express a main intention
of an asker with a form of simple questions in
general.
</bodyText>
<subsectionHeader confidence="0.698443">
Evaluation Data
</subsectionHeader>
<bodyText confidence="0.999933354166667">
Two separate data sets were created for evalua-
tion. First, we collected 5,200 questions by sam-
pling 200 questions from each Yahoo! Answers
category3. Two annotators were asked to label
each question manually as comparative, non-
comparative, or unknown. Among them, 139
(2.67%) questions were classified as comparative,
4,934 (94.88%) as non-comparative, and 127
(2.44%) as unknown questions which are diffi-
cult to assess. We call this set SET-A.
Because there are only 139 comparative ques-
tions in SET-A, we created another set which
contains more comparative questions. We ma-
nually constructed a keyword set consisting of 53
words such as “or” and “prefer”, which are good
indicators of comparative questions. In SET-A,
97.4% of comparative questions contains one or
more keywords from the keyword set. We then
randomly selected another 100 questions from
each Yahoo! Answers category with one extra
condition that all questions have to contain at
least one keyword. These questions were labeled
in the same way as SET-A except that their com-
parators were also annotated. This second set of
questions is referred as SET-B. It contains 853
comparative questions and 1,747 non-
comparative questions. For comparative question
identification experiments, we used all labeled
questions in SET-A and SET-B. For comparator
extraction experiments, we used only SET-B. All
the remaining unlabeled questions (called as
SET-R) were used for training our weakly super-
vised method.
As a baseline method, we carefully imple-
mented J&amp;L&apos;s method. Specifically, CSRs for
comparative question identification were learned
from the labeled questions, and then a statistical
classifier was built by using CSR rules as fea-
tures. We examined both SVM and Naïve Bayes
(NB) models as reported in their experiments.
For the comparator extraction, LSRs were
learned from SET-B and applied for comparator
extraction.
To start the bootstrapping procedure, we ap-
plied the IEP “&lt;#start nn/$c vs/cc nn/$c ?/.
#end&gt;” to all the questions in SET-R and ga-
thered 12,194 comparator pairs as the initial
seeds. For our weakly supervised method, there
</bodyText>
<page confidence="0.50079">
3 There are 26 top level categories in Yahoo! Answers.
</page>
<bodyText confidence="0.999930419354839">
are four parameters, i.e. α, β, y, and A, need to be
determined empirically. We first mined all poss-
ible candidate patterns from the suffix tree using
the initial seeds. From these candidate patterns,
we applied them to SET-R and got a new set of
59,410 candidate comparator pairs. Among these
new candidate comparator pairs, we randomly
selected 100 comparator pairs and manually clas-
sified them into reliable or non-reliable compara-
tors. Then we found a that maximized precision
without hurting recall by investigating frequen-
cies of pairs in the labeled set. By this method, a
was set to 3 in our experiments. Similarly, the
threshold parameters fl and y for pattern evalua-
tion were set to 10 and 0.8 respectively. For the
interpolation parameter A in Equation (3), we
simply set the value to 0.5 by assuming that two
reliability scores are equally important.
As evaluation measures for comparative ques-
tion identification and comparator extraction, we
used precision, recall, and F1-measure. All re-
sults were obtained from 5-fold cross validation.
Note that J&amp;L&apos;s method needs a training data but
ours use the unlabeled data (SET-R) with weakly
supervised method to find parameter setting.
This 5-fold evaluation data is not in the unla-
beled data. Both methods were tested on the
same test split in the 5-fold cross validation. All
evaluation scores are averaged across all 5 folds.
For question processing, we used our own sta-
tistical POS tagger developed in-house4.
</bodyText>
<subsectionHeader confidence="0.975510666666667">
4.2 Experiment Results
Comparative Question Identification and
Comparator Extraction
</subsectionHeader>
<bodyText confidence="0.999972230769231">
Table 2 shows our experimental results. In the
table, “Identification only” indicates the perfor-
mances in comparative question identification,
“Extraction only” denotes the performances of
comparator extraction when only comparative
questions are used as input, and “All” indicates
the end-to-end performances when question
identification results were used in comparator
extraction. Note that the results of J&amp;L&apos;s method
on our collections are very comparable to what is
reported in their paper.
In terms of precision, the J&amp;L&apos;s method is
competitive to our method in comparative ques-
</bodyText>
<footnote confidence="0.926343">
4 We used NLC-PosTagger which is developed by NLC
group of Microsoft Research Asia. It uses the modified
Penn Treebank POS set for its output; for example, NNS
(plural nouns), NN (nouns), NP (noun phrases), NPS (plural
noun phrases), VBZ (verb, present tense, 3rd person singu-
lar), JJ (adjective), RB(adverb), and so on.
</footnote>
<page confidence="0.993716">
655
</page>
<table confidence="0.999623125">
Identification only Extraction only All
(SET-A+SET-B) (SET-B) (SET-B)
J&amp;L (CSR) Our J&amp;L Our J&amp;L Our
Method (LSR) Method Method
SVM NB SVM NB
Recall 0.601 0.537 0.817* 0.621 0.760* 0.373 0.363 0.760*
Precision 0.847 0.851 0.833 0.861 0.916* 0.729 0.703 0.776*
F-score 0.704 0.659 0.825* 0.722 0.833* 0.493 0.479 0.768*
</table>
<tableCaption confidence="0.993127333333333">
Table 2: Performance comparison between our method and Jindal and Bing&apos;s Method (denoted as J&amp;L).
The values with * indicate statistically significant improvements over J&amp;L (CSR) SVM or J&amp;L (LSR)
according to t-test at p &lt; 0.01 level.
</tableCaption>
<bodyText confidence="0.999893345454545">
tion identification. However, the recall is signifi-
cantly lower than ours. In terms of recall, our
method outperforms J&amp;L&apos;s method by 35% and
22% in comparative question identification and
comparator extraction respectively. In our analy-
sis, the low recall of J&amp;L&apos;s method is mainly
caused by low coverage of learned CSR patterns
over the test set.
In the end-to-end experiments, our weakly su-
pervised method performs significantly better
than J&amp;L&apos;s method. Our method is about 55%
better in F1-measure. This result also highlights
another advantage of our method that identifies
comparative questions and extracts comparators
simultaneously using one single pattern. J&amp;L&apos;s
method uses two kinds of pattern rules, i.e. CSRs
and LSRs. Its performance drops significantly
due to error propagations. F1-measure of J&amp;L&apos;s
method in “All” is about 30% and 32% worse
than the scores of “Identification only” and “Ex-
traction” only respectively, our method only
shows small amount of performance decrease
(approximately 7-8%).
We also analyzed the effect of pattern genera-
lization and specialization. Table 3 shows the
results. Despite of the simplicity of our methods,
they significantly contribute to performance im-
provements. This result shows the importance of
learning patterns flexibly to capture various
comparative question expressions. Among the
6,127 learned IEPs in our database, 5,930 pat-
terns are generalized ones, 171 are specialized
ones, and only 26 patterns are non-generalized
and specialized ones.
To investigate the robustness of our bootstrap-
ping algorithm for different seed configurations,
we compare the performances between two dif-
ferent seed IEPs. The results are shown in Table
4. As shown in the table, the performance of our
bootstrapping algorithm is stable regardless of
significantly different number of seed pairs gen-
erated by the two IEPs. This result implies that
our bootstrapping algorithm is not sensitive to
the choice of IEP.
Table 5 also shows the robustness of our boot-
strapping algorithm. In Table 5, `All’ indicates
the performances that all comparator pairs from a
single seed IEP is used for the bootstrapping, and
`Partial&apos; indicate the performances using only
1,000 randomly sampled pairs from `All’. As
shown in the table, there is no significant per-
formance difference.
In addition, we conducted error analysis for
the cases where our method fails to extract cor-
rect comparator pairs:
</bodyText>
<listItem confidence="0.847791666666667">
• 23.75% of errors on comparator extraction
are due to wrong pattern selection by our
simple maximum IEP length strategy.
• The remaining 67.63% of errors come from
comparative questions which cannot be cov-
ered by the learned IEPs.
</listItem>
<table confidence="0.99901775">
Recall Precision F-score
Original Patterns 0.689 0. 449 0.544
+ Specialized 0.731 0.602 0.665
+ Generalized 0.760 0.776 0.768
</table>
<tableCaption confidence="0.991635">
Table 3: Effect of pattern specialization and Generali-
zation in the end-to-end experiments.
</tableCaption>
<table confidence="0.999260428571429">
Seed patterns # of resulted F-score
seed pairs
&lt;#start nn/$c vs/cc nn/$c 12,194 0.768
?/. #end&gt;
&lt;#start which/wdt is/vb 1,478 0.760
better/jjr , nn/$c or/cc
nn/$c ?/. #end&gt;
</table>
<tableCaption confidence="0.977756">
Table 4: Performance variation over different initial
seed IEPs in the end-to-end experiments
</tableCaption>
<table confidence="0.978632666666667">
Set (# of seed pairs) Recall Precision F-score
All (12,194) 0.760 0.774 0.768
Partial (1,000) 0.724 0.763 0.743
</table>
<tableCaption confidence="0.990734333333333">
Table 5: Performance variation over different sizes of
seed pairs generated from a single initial seed IEP
“&lt;#start nn/$c vs/cc nn/$c ?/. #end&gt;”.
</tableCaption>
<page confidence="0.988444">
656
</page>
<note confidence="0.773827818181818">
Chanel Gap iPod Kobe Canon
1 Dior Old Navy Zune Lebron Nikon
2 Louis Vuitton American Eagle mp3 player Jordan Sony
3 Coach Banana Republic PSP MJ Kodak
4 Gucci Guess by Marciano cell phone Shaq Panasonic
5 Prada ACP Ammunition iPhone Wade Casio
6 Lancome Old Navy brand Creative Zen T-mac Olympus
7 Versace Hollister Zen Lebron James Hp
8 LV Aeropostal iPod nano Nash Lexmark
9 Mac American Eagle outfitters iPod touch KG Pentax
10 Dooney Guess iRiver Bonds Xerox
</note>
<tableCaption confidence="0.992255">
Table 6: Examples of comparators for different entities
</tableCaption>
<table confidence="0.999731111111111">
Chanel Gap iPod Kobe Canon
Chanel handbag Gap coupons iPod nano Kobe Bryant stats Canon t2i
Chanel sunglass Gap outlet iPod touch Lakers Kobe Canon printers
Chanel earrings Gap card iPod best buy Kobe espn Canon printer drivers
Chanel watches Gap careers iTunes Kobe Dallas Mavericks Canon downloads
Chanel shoes Gap casting call Apple Kobe NBA Canon copiers
Chanel jewelry Gap adventures iPod shuffle Kobe 2009 Canon scanner
Chanel clothing Old navy iPod support Kobe san Antonio Canon lenses
Dior Banana republic iPod classic Kobe Bryant 24 Nikon
</table>
<tableCaption confidence="0.995726">
Table 7: Related queries returned by Google related searches for the same target entities in Table 6. The bold
ones indicate overlapped queries to the comparators in Table 6.
</tableCaption>
<subsectionHeader confidence="0.964532">
Examples of Comparator Extraction
</subsectionHeader>
<bodyText confidence="0.999858545454545">
By applying our bootstrapping method to the
entire source data (60M questions), 328,364
unique comparator pairs were extracted from
679,909 automatically identified comparative
questions.
Table 6 lists top 10 frequently compared enti-
ties for a target item, such as Chanel, Gap, in our
question archive. As shown in the table, our
comparator mining method successfully discov-
ers realistic comparators. For example, for `Cha-
nel’, most results are high-end fashion brands
such as `Dior’ or `Louis Vuitton’, while the rank-
ing results for `Gap’ usually contains similar ap-
parel brands for young people, such as `Old Navy’
or `Banana Republic’. For the basketball player
`Kobe&apos;, most of the top ranked comparators are
also famous basketball players. Some interesting
comparators are shown for `Canon&apos; (the compa-
ny name). It is famous for different kinds of its
products, for example, digital cameras and prin-
ters, so it can be compared to different kinds of
companies. For example, it is compared to `HP’,
`Lexmark’, or `Xerox’, the printer manufacturers,
and also compared to `Nikon’, `Sony’, or `Kodak’,
the digital camera manufactures. Besides gener-
al entities such as a brand or company name, our
method also found an interesting comparable
entity for a specific item in the experiments. For
example, our method recommends `Nikon d40i&apos;,
`Canon rebel xti&apos;, `Canon rebel xt&apos;, `Nikon
d3000&apos;, `Pentax k100d&apos;, `Canon eos 1000d&apos; as
comparators for the specific camera product `Ni-
kon 40d&apos;.
Table 7 can show the difference between our
comparator mining and query/item recommenda-
tion. As shown in the table, `Google related
searches&apos; generally suggests a mixed set of two
kinds of related queries for a target entity: (1)
queries specified with subtopics for an original
query (e.g., `Chanel handbag&apos; for `Chanel&apos;) and
(2) its comparable entities (e.g., `Dior&apos; for `Cha-
nel&apos;). It confirms one of our claims that compara-
tor mining and query/item recommendation are
related but not the same.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999975888888889">
In this paper, we present a novel weakly super-
vised method to identify comparative questions
and extract comparator pairs simultaneously. We
rely on the key insight that a good comparative
question identification pattern should extract
good comparators, and a good comparator pair
should occur in good comparative questions to
bootstrap the extraction and identification
process. By leveraging large amount of unla-
beled data and the bootstrapping process with
slight supervision to determine four parameters,
we found 328,364 unique comparator pairs and
6,869 extraction patterns without the need of
creating a set of comparative question indicator
keywords.
The experimental results show that our me-
thod is effective in both comparative question
identification and comparator extraction. It sig-
</bodyText>
<page confidence="0.994461">
657
</page>
<bodyText confidence="0.999934238095238">
nificantly improves recall in both tasks while
maintains high precision. Our examples show
that these comparator pairs reflect what users are
really interested in comparing.
Our comparator mining results can be used for
a commerce search or product recommendation
system. For example, automatic suggestion of
comparable entities can assist users in their com-
parison activities before making their purchase
decisions. Also, our results can provide useful
information to companies which want to identify
their competitors.
In the future, we would like to improve extrac-
tion pattern application and mine rare extraction
patterns. How to identify comparator aliases such
as `LV’ and `Louis Vuitton‟ and how to separate
ambiguous entities such “Paris vs. London” as
location and “Paris vs. Nicole” as celebrity are
all interesting research topics. We also plan to
develop methods to summarize answers pooled
by a given comparator pair.
</bodyText>
<sectionHeader confidence="0.9996" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998715">
This work was done when the first author
worked as an intern at Microsoft Research Asia.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999740169811321">
Mary Elaine Califf and Raymond J. Mooney. 1999.
Relational learning of pattern-match rules for in-
formation extraction. In Proceedings of AAAI’99
/IAAI’99.
Claire Cardie. 1997. Empirical methods in informa-
tion extraction. AI magazine, 18:65–79.
Dan Gusfield. 1997. Algorithms on strings, trees, and
sequences: computer science and computational
biology. Cambridge University Press, New York,
NY, USA
Taher H. Haveliwala. 2002. Topic-sensitive pagerank.
In Proceedings of WWW ’02, pages 517–526.
Glen Jeh and Jennifer Widom. 2003. Scaling persona-
lized web search. In Proceedings of WWW ’03,
pages 271–279.
Nitin Jindal and Bing Liu. 2006a. Identifying compar-
ative sentences in text documents. In Proceedings
of SIGIR ’06, pages 244–251.
Nitin Jindal and Bing Liu. 2006b. Mining compara-
tive sentences and relations. In Proceedings of
AAAI ’06.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL-08: HLT, pages 1048–1056.
Greg Linden, Brent Smith and Jeremy York. 2003.
Amazon.com Recommendations: Item-to-Item
Collaborative Filtering. IEEE Internet Computing,
pages 76-80.
Raymond J. Mooney and Razvan Bunescu. 2005.
Mining knowledge from text using information ex-
traction. ACM SIGKDD Exploration Newsletter,
7(1):3–10.
Dragomir Radev, Weiguo Fan, Hong Qi, and Harris
Wu and Amardeep Grewal. 2002. Probabilistic
question answering on the web. Journal of the
American Society for Information Science and
Technology, pages 408–419.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a question ans-
wering system. In Proceedings of ACL ’02, pages
41–47.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level
bootstrapping. In Proceedings of AAAI ’99
/IAAI ’99, pages 474–479.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of
the 13th National Conference on Artificial Intelli-
gence, pages 1044–1049.
Stephen Soderland. 1999. Learning information ex-
traction rules for semi-structured and free text. Ma-
chine Learning, 34(1-3):233–272.
</reference>
<page confidence="0.997308">
658
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577253">
<title confidence="0.844691">Comparable Entity Mining from Comparative Questions</title>
<affiliation confidence="0.972014">University of Defense Technology, Changsha, China</affiliation>
<address confidence="0.860179">Research Asia, Beijing, China University, Beijing, China</address>
<abstract confidence="0.998848611111111">Comparing one thing with another is a typical part of human decision making process. However, it is not always easy to know what to compare and what are the alternatives. To address this difficulty, we present a novel way to automatically mine comparable entities from comparative questions that users posted online. To ensure high precision and high recall, we develop a weakly-supervised bootstrapping method for comparative question identification and comparable entity extraction by leveraging a large online question archive. The experimental results show our method achieves F1measure of 82.5% in comparative question identification and 83.3% in comparable entity extraction. Both significantly outperform an existing state-of-the-art method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Relational learning of pattern-match rules for information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI’99 /IAAI’99.</booktitle>
<contexts>
<context position="7049" citStr="Califf and Mooney, 1999" startWordPosition="1110" endWordPosition="1113">e comparable with “iPod” such as “iPhone” or “PSP” which were found in comparative questions posted by users are difficult to be predicted simply based on item similarity between them. Although they are all music players, “iPhone” is mainly a mobile phone, and “PSP” is mainly a portable game device. They are similar but also different therefore beg comparison with each other. It is clear that comparator mining and item recommendation are related but not the same. Our work on comparator mining is related to the research on entity and relation extraction in information extraction (Cardie, 1997; Califf and Mooney, 1999; Soderland, 1999; Radev et al., 2002; Carreras et al., 2003). Specifically, the most relevant work is by Jindal and Liu (2006a and 2006b) on mining comparative sentences and relations. Their methods applied class sequential rules (CSR) (Chapter 2, Liu 2006) and label sequential rules (LSR) (Chapter 2, Liu 2006) learned from annotated corpora to identify comparative sentences and extract comparative relations respectively in the news and review domains. The same techniques can be applied to comparative question identification and comparator mining from questions. However, their methods typical</context>
</contexts>
<marker>Califf, Mooney, 1999</marker>
<rawString>Mary Elaine Califf and Raymond J. Mooney. 1999. Relational learning of pattern-match rules for information extraction. In Proceedings of AAAI’99 /IAAI’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
</authors>
<title>Empirical methods in information extraction.</title>
<date>1997</date>
<booktitle>AI magazine,</booktitle>
<pages>18--65</pages>
<contexts>
<context position="7024" citStr="Cardie, 1997" startWordPosition="1108" endWordPosition="1109"> items that are comparable with “iPod” such as “iPhone” or “PSP” which were found in comparative questions posted by users are difficult to be predicted simply based on item similarity between them. Although they are all music players, “iPhone” is mainly a mobile phone, and “PSP” is mainly a portable game device. They are similar but also different therefore beg comparison with each other. It is clear that comparator mining and item recommendation are related but not the same. Our work on comparator mining is related to the research on entity and relation extraction in information extraction (Cardie, 1997; Califf and Mooney, 1999; Soderland, 1999; Radev et al., 2002; Carreras et al., 2003). Specifically, the most relevant work is by Jindal and Liu (2006a and 2006b) on mining comparative sentences and relations. Their methods applied class sequential rules (CSR) (Chapter 2, Liu 2006) and label sequential rules (LSR) (Chapter 2, Liu 2006) learned from annotated corpora to identify comparative sentences and extract comparative relations respectively in the news and review domains. The same techniques can be applied to comparative question identification and comparator mining from questions. Howev</context>
</contexts>
<marker>Cardie, 1997</marker>
<rawString>Claire Cardie. 1997. Empirical methods in information extraction. AI magazine, 18:65–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on strings, trees, and sequences: computer science and computational biology.</title>
<date>1997</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA</location>
<contexts>
<context position="17351" citStr="Gusfield, 1997" startWordPosition="2812" endWordPosition="2813"> patterns, we adapt the surface text pattern mining method introduced in (Ravichandran and Hovy, 2002). For any given comparative question and its comparator pairs, comparators in the question are replaced with symbol $Cs. Two symbols, #start and #end, are attached to the beginning and the end of a sen653 tence in the question. Then, the following three kinds of sequential patterns are generated from sequences of questions: • Lexical patterns: Lexical patterns indicate sequential patterns consisting of only words and symbols ($C, #start, and #end). They are generated by suffix tree algorithm (Gusfield, 1997) with two constraints: A pattern should contain more than one $C, and its frequency in collection should be more than an empirically determined number 𝛽. • Generalized patterns: A lexical pattern can be too specific. Thus, we generalize lexical patterns by replacing one or more words with their POS tags. 2𝑛 − 1 generalized patterns can be produced from a lexical pattern containing N words excluding $Cs. • Specialized patterns: In some cases, a pattern can be too general. For example, although a question “ipod or zune?” is comparative, the pattern “&lt;$C or $C&gt;” is too general, and there can be m</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on strings, trees, and sequences: computer science and computational biology. Cambridge University Press, New York, NY, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In Proceedings of WWW ’02,</booktitle>
<pages>517--526</pages>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In Proceedings of WWW ’02, pages 517–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glen Jeh</author>
<author>Jennifer Widom</author>
</authors>
<title>Scaling personalized web search.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW ’03,</booktitle>
<pages>271--279</pages>
<marker>Jeh, Widom, 2003</marker>
<rawString>Glen Jeh and Jennifer Widom. 2003. Scaling personalized web search. In Proceedings of WWW ’03, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Identifying comparative sentences in text documents.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR ’06,</booktitle>
<pages>244--251</pages>
<contexts>
<context position="7175" citStr="Jindal and Liu (2006" startWordPosition="1131" endWordPosition="1134">predicted simply based on item similarity between them. Although they are all music players, “iPhone” is mainly a mobile phone, and “PSP” is mainly a portable game device. They are similar but also different therefore beg comparison with each other. It is clear that comparator mining and item recommendation are related but not the same. Our work on comparator mining is related to the research on entity and relation extraction in information extraction (Cardie, 1997; Califf and Mooney, 1999; Soderland, 1999; Radev et al., 2002; Carreras et al., 2003). Specifically, the most relevant work is by Jindal and Liu (2006a and 2006b) on mining comparative sentences and relations. Their methods applied class sequential rules (CSR) (Chapter 2, Liu 2006) and label sequential rules (LSR) (Chapter 2, Liu 2006) learned from annotated corpora to identify comparative sentences and extract comparative relations respectively in the news and review domains. The same techniques can be applied to comparative question identification and comparator mining from questions. However, their methods typically can achieve high precision but suffer from low recall (Jindal and Liu, 2006b) (J&amp;L). However, ensuring high recall is cruci</context>
<context position="8760" citStr="Jindal and Liu (2006" startWordPosition="1371" endWordPosition="1374">avichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal &amp; Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed by Jindal and Liu (2006a and 2006b), which is used as baseline for comparison and represents the state-of-the-art in this area. We first introduce the definition of CSR and LSR rule used in their approach, and then describe their comparative mining method. Readers should refer to J&amp;L&apos;s original papers for more details. CSR and LSR CSR is a classification rule. It maps a sequence pattern S(s1s2 ... sn) to a class C. In our problem, C is either comparative or non-comparative. Given a collection of sequences with class information, every CSR is associated to two parameters: support and confidence. Support is the propor</context>
<context position="5086" citStr="Jindal &amp; Liu (2006" startWordPosition="786" endWordPosition="789">usly. To our best knowledge, this is the first attempt to specially address the problem on finding good comparators to support users‟ comparison activity. We are also the first to propose using comparative questions posted online that reflect what users truly care about as the medium from which we mine comparable entities. Our weakly supervised method achieves 82.5% F1-measure in comparative question identification, 83.3% in comparator extraction, and 76.8% in end-to-end comparative question identification and comparator extraction which outperform the most relevant state-of-the-art method by Jindal &amp; Liu (2006b) significantly. The rest of this paper is organized as follows. The next section discusses previous works. Section 3 presents our weakly-supervised method for comparator mining. Section 4 reports the evaluations of our techniques, and we conclude the paper and discuss future work in Section 5. 2 Related Work 2.1 Overview In terms of discovering related items for an entity, our work is similar to the research on recommender systems, which recommend items to a user. Recommender systems mainly rely on similarities between items and/or their statistical correlations in user log data (Linden et a</context>
<context position="8647" citStr="Jindal &amp; Liu 2006" startWordPosition="1352" endWordPosition="1355">hown to be very effective in previous information extraction research (Riloff, 1996; Riloff and Jones, 1999; Ravichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal &amp; Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed by Jindal and Liu (2006a and 2006b), which is used as baseline for comparison and represents the state-of-the-art in this area. We first introduce the definition of CSR and LSR rule used in their approach, and then describe their comparative mining method. Readers should refer to J&amp;L&apos;s original papers for more details. CSR and LSR CSR is a classification rule. It maps a sequence pattern S(s1s2 ... sn) to a class C. In our problem, C is either comparative or non-comparative. Given a collection of sequences</context>
<context position="12162" citStr="Jindal &amp; Liu (2006" startWordPosition="1933" endWordPosition="1936">t. J&amp;L&apos;s method have been proved effective in their experimental setups. However, it has the following weaknesses: • The performance of J&amp;L&apos;s method relies heavily on a set of comparative sentence indicative keywords. These keywords were manually created and they offered no guidelines to select keywords for inclusion. It is also difficult to ensure the completeness of the keyword list. • Users can express comparative sentences or questions in many different ways. To have high recall, a large annotated training corpus is necessary. This is an expensive process. • Example CSRs and LSRs given in Jindal &amp; Liu (2006b) are mostly a combination of POS tags and keywords. It is a surprise that their rules achieved high precision but low recall. They attributed most errors to POS tagging errors. However, we suspect that their rules might be too specific and overfit their small training set (about 2,600 sentences). We would like to increase recall, avoid overfitting, and allow rules to include discriminative lexical tokens to retain precision. In the next section, we introduce our method to address these shortcomings. 3 Weakly Supervised Method for Comparator Mining Our weakly supervised method is a pattern-ba</context>
</contexts>
<marker>Jindal, Liu, 2006</marker>
<rawString>Nitin Jindal and Bing Liu. 2006a. Identifying comparative sentences in text documents. In Proceedings of SIGIR ’06, pages 244–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Mining comparative sentences and relations.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI ’06.</booktitle>
<contexts>
<context position="7175" citStr="Jindal and Liu (2006" startWordPosition="1131" endWordPosition="1134">predicted simply based on item similarity between them. Although they are all music players, “iPhone” is mainly a mobile phone, and “PSP” is mainly a portable game device. They are similar but also different therefore beg comparison with each other. It is clear that comparator mining and item recommendation are related but not the same. Our work on comparator mining is related to the research on entity and relation extraction in information extraction (Cardie, 1997; Califf and Mooney, 1999; Soderland, 1999; Radev et al., 2002; Carreras et al., 2003). Specifically, the most relevant work is by Jindal and Liu (2006a and 2006b) on mining comparative sentences and relations. Their methods applied class sequential rules (CSR) (Chapter 2, Liu 2006) and label sequential rules (LSR) (Chapter 2, Liu 2006) learned from annotated corpora to identify comparative sentences and extract comparative relations respectively in the news and review domains. The same techniques can be applied to comparative question identification and comparator mining from questions. However, their methods typically can achieve high precision but suffer from low recall (Jindal and Liu, 2006b) (J&amp;L). However, ensuring high recall is cruci</context>
<context position="8760" citStr="Jindal and Liu (2006" startWordPosition="1371" endWordPosition="1374">avichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal &amp; Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed by Jindal and Liu (2006a and 2006b), which is used as baseline for comparison and represents the state-of-the-art in this area. We first introduce the definition of CSR and LSR rule used in their approach, and then describe their comparative mining method. Readers should refer to J&amp;L&apos;s original papers for more details. CSR and LSR CSR is a classification rule. It maps a sequence pattern S(s1s2 ... sn) to a class C. In our problem, C is either comparative or non-comparative. Given a collection of sequences with class information, every CSR is associated to two parameters: support and confidence. Support is the propor</context>
<context position="5086" citStr="Jindal &amp; Liu (2006" startWordPosition="786" endWordPosition="789">usly. To our best knowledge, this is the first attempt to specially address the problem on finding good comparators to support users‟ comparison activity. We are also the first to propose using comparative questions posted online that reflect what users truly care about as the medium from which we mine comparable entities. Our weakly supervised method achieves 82.5% F1-measure in comparative question identification, 83.3% in comparator extraction, and 76.8% in end-to-end comparative question identification and comparator extraction which outperform the most relevant state-of-the-art method by Jindal &amp; Liu (2006b) significantly. The rest of this paper is organized as follows. The next section discusses previous works. Section 3 presents our weakly-supervised method for comparator mining. Section 4 reports the evaluations of our techniques, and we conclude the paper and discuss future work in Section 5. 2 Related Work 2.1 Overview In terms of discovering related items for an entity, our work is similar to the research on recommender systems, which recommend items to a user. Recommender systems mainly rely on similarities between items and/or their statistical correlations in user log data (Linden et a</context>
<context position="8647" citStr="Jindal &amp; Liu 2006" startWordPosition="1352" endWordPosition="1355">hown to be very effective in previous information extraction research (Riloff, 1996; Riloff and Jones, 1999; Ravichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal &amp; Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed by Jindal and Liu (2006a and 2006b), which is used as baseline for comparison and represents the state-of-the-art in this area. We first introduce the definition of CSR and LSR rule used in their approach, and then describe their comparative mining method. Readers should refer to J&amp;L&apos;s original papers for more details. CSR and LSR CSR is a classification rule. It maps a sequence pattern S(s1s2 ... sn) to a class C. In our problem, C is either comparative or non-comparative. Given a collection of sequences</context>
<context position="12162" citStr="Jindal &amp; Liu (2006" startWordPosition="1933" endWordPosition="1936">t. J&amp;L&apos;s method have been proved effective in their experimental setups. However, it has the following weaknesses: • The performance of J&amp;L&apos;s method relies heavily on a set of comparative sentence indicative keywords. These keywords were manually created and they offered no guidelines to select keywords for inclusion. It is also difficult to ensure the completeness of the keyword list. • Users can express comparative sentences or questions in many different ways. To have high recall, a large annotated training corpus is necessary. This is an expensive process. • Example CSRs and LSRs given in Jindal &amp; Liu (2006b) are mostly a combination of POS tags and keywords. It is a surprise that their rules achieved high precision but low recall. They attributed most errors to POS tagging errors. However, we suspect that their rules might be too specific and overfit their small training set (about 2,600 sentences). We would like to increase recall, avoid overfitting, and allow rules to include discriminative lexical tokens to retain precision. In the next section, we introduce our method to address these shortcomings. 3 Weakly Supervised Method for Comparator Mining Our weakly supervised method is a pattern-ba</context>
</contexts>
<marker>Jindal, Liu, 2006</marker>
<rawString>Nitin Jindal and Bing Liu. 2006b. Mining comparative sentences and relations. In Proceedings of AAAI ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1048--1056</pages>
<contexts>
<context position="8216" citStr="Kozareva et al., 2008" startWordPosition="1287" endWordPosition="1290">r mining from questions. However, their methods typically can achieve high precision but suffer from low recall (Jindal and Liu, 2006b) (J&amp;L). However, ensuring high recall is crucial in our intended application scenario where users can issue arbitrary queries. To address this problem, we develop a weakly-supervised bootstrapping pattern learning method by effectively leveraging unlabeled questions. Bootstrapping methods have been shown to be very effective in previous information extraction research (Riloff, 1996; Riloff and Jones, 1999; Ravichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal &amp; Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed by Jindal and Liu (2006a and 2006b), which is used as baseline for comparison a</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings of ACL-08: HLT, pages 1048–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Linden</author>
<author>Brent Smith</author>
<author>Jeremy York</author>
</authors>
<title>Amazon.com Recommendations: Item-to-Item Collaborative Filtering.</title>
<date>2003</date>
<journal>IEEE Internet Computing,</journal>
<pages>76--80</pages>
<contexts>
<context position="5695" citStr="Linden et al., 2003" startWordPosition="888" endWordPosition="891">&amp; Liu (2006b) significantly. The rest of this paper is organized as follows. The next section discusses previous works. Section 3 presents our weakly-supervised method for comparator mining. Section 4 reports the evaluations of our techniques, and we conclude the paper and discuss future work in Section 5. 2 Related Work 2.1 Overview In terms of discovering related items for an entity, our work is similar to the research on recommender systems, which recommend items to a user. Recommender systems mainly rely on similarities between items and/or their statistical correlations in user log data (Linden et al., 2003). For example, Amazon recommends products to its customers based on their own purchase histories, similar customers‟ purchase histories, and similarity between products. However, recommending an item is not equivalent to finding a comparable item. In the case of Amazon, the purpose of recommendation is to entice their customers to add more items to their shopping carts by suggesting similar or related items. While in the case of comparison, we would like to help users explore alternatives, i.e. helping them make a decision among comparable items. For example, it is reasonable to recommend “iPo</context>
</contexts>
<marker>Linden, Smith, York, 2003</marker>
<rawString>Greg Linden, Brent Smith and Jeremy York. 2003. Amazon.com Recommendations: Item-to-Item Collaborative Filtering. IEEE Internet Computing, pages 76-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
<author>Razvan Bunescu</author>
</authors>
<title>Mining knowledge from text using information extraction.</title>
<date>2005</date>
<journal>ACM SIGKDD Exploration Newsletter,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="8192" citStr="Mooney and Bunescu, 2005" startWordPosition="1282" endWordPosition="1286">entification and comparator mining from questions. However, their methods typically can achieve high precision but suffer from low recall (Jindal and Liu, 2006b) (J&amp;L). However, ensuring high recall is crucial in our intended application scenario where users can issue arbitrary queries. To address this problem, we develop a weakly-supervised bootstrapping pattern learning method by effectively leveraging unlabeled questions. Bootstrapping methods have been shown to be very effective in previous information extraction research (Riloff, 1996; Riloff and Jones, 1999; Ravichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal &amp; Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed by Jindal and Liu (2006a and 2006b), which is used as b</context>
</contexts>
<marker>Mooney, Bunescu, 2005</marker>
<rawString>Raymond J. Mooney and Razvan Bunescu. 2005. Mining knowledge from text using information extraction. ACM SIGKDD Exploration Newsletter, 7(1):3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Weiguo Fan</author>
<author>Hong Qi</author>
<author>Harris Wu</author>
<author>Amardeep Grewal</author>
</authors>
<title>Probabilistic question answering on the web.</title>
<date>2002</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<pages>408--419</pages>
<contexts>
<context position="7086" citStr="Radev et al., 2002" startWordPosition="1116" endWordPosition="1119">or “PSP” which were found in comparative questions posted by users are difficult to be predicted simply based on item similarity between them. Although they are all music players, “iPhone” is mainly a mobile phone, and “PSP” is mainly a portable game device. They are similar but also different therefore beg comparison with each other. It is clear that comparator mining and item recommendation are related but not the same. Our work on comparator mining is related to the research on entity and relation extraction in information extraction (Cardie, 1997; Califf and Mooney, 1999; Soderland, 1999; Radev et al., 2002; Carreras et al., 2003). Specifically, the most relevant work is by Jindal and Liu (2006a and 2006b) on mining comparative sentences and relations. Their methods applied class sequential rules (CSR) (Chapter 2, Liu 2006) and label sequential rules (LSR) (Chapter 2, Liu 2006) learned from annotated corpora to identify comparative sentences and extract comparative relations respectively in the news and review domains. The same techniques can be applied to comparative question identification and comparator mining from questions. However, their methods typically can achieve high precision but suf</context>
</contexts>
<marker>Radev, Fan, Qi, Wu, Grewal, 2002</marker>
<rawString>Dragomir Radev, Weiguo Fan, Hong Qi, and Harris Wu and Amardeep Grewal. 2002. Probabilistic question answering on the web. Journal of the American Society for Information Science and Technology, pages 408–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL ’02,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="8166" citStr="Ravichandran and Hovy, 2002" startWordPosition="1278" endWordPosition="1281">ed to comparative question identification and comparator mining from questions. However, their methods typically can achieve high precision but suffer from low recall (Jindal and Liu, 2006b) (J&amp;L). However, ensuring high recall is crucial in our intended application scenario where users can issue arbitrary queries. To address this problem, we develop a weakly-supervised bootstrapping pattern learning method by effectively leveraging unlabeled questions. Bootstrapping methods have been shown to be very effective in previous information extraction research (Riloff, 1996; Riloff and Jones, 1999; Ravichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal &amp; Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed by Jindal and Liu (2006a and </context>
<context position="14766" citStr="Ravichandran and Hovy (2002)" startWordPosition="2388" endWordPosition="2391"> comparator slots in the IEP are extracted as comparators. When a question can match multiple IEPs, the longest IEP is used 2 . Therefore, instead of manually creating a list of indicative keywords, we create a set of IEPs. We will show how to acquire IEPs automatically using a bootstrapping procedure with minimum supervision by taking advantage of a large unlabeled question collection in the following subsections. The evaluations shown in section 4 confirm that our weakly supervised method can achieve high recall while retain high precision. This pattern definition is inspired by the work of Ravichandran and Hovy (2002). Table 1 shows some examples of such sequential patterns. We also allow POS constraint on comparators as shown in the pattern “&lt;, $C/NN or $C/NN ? #end&gt;”. It means that a valid comparator must have a NN POS tag. 3.1 Mining Indicative Extraction Patterns Our weakly supervised IEP mining approach is based on two key assumptions: 2 It is because the longest IEP is likely to be the most specific and relevant pattern for the given question. Figure 1: Overview of the bootstrapping alogorithm • If a sequential pattern can be used to extract many reliable comparator pairs, it is very likely to be an </context>
<context position="16838" citStr="Ravichandran and Hovy, 2002" startWordPosition="2730" endWordPosition="2733">mparator repository and used as new seeds for pattern learning in the next iteration. All questions from which reliable comparators are extracted are removed from the collection to allow finding new patterns efficiently in later iterations. The process iterates until no more new patterns can be found from the question collection. There are two key steps in our method: (1) pattern generation and (2) pattern evaluation. In the following subsections, we will explain them in details. Pattern Generation To generate sequential patterns, we adapt the surface text pattern mining method introduced in (Ravichandran and Hovy, 2002). For any given comparative question and its comparator pairs, comparators in the question are replaced with symbol $Cs. Two symbols, #start and #end, are attached to the beginning and the end of a sen653 tence in the question. Then, the following three kinds of sequential patterns are generated from sequences of questions: • Lexical patterns: Lexical patterns indicate sequential patterns consisting of only words and symbols ($C, #start, and #end). They are generated by suffix tree algorithm (Gusfield, 1997) with two constraints: A pattern should contain more than one $C, and its frequency in </context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL ’02, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI ’99 /IAAI ’99,</booktitle>
<pages>474--479</pages>
<contexts>
<context position="8137" citStr="Riloff and Jones, 1999" startWordPosition="1274" endWordPosition="1277"> techniques can be applied to comparative question identification and comparator mining from questions. However, their methods typically can achieve high precision but suffer from low recall (Jindal and Liu, 2006b) (J&amp;L). However, ensuring high recall is crucial in our intended application scenario where users can issue arbitrary queries. To address this problem, we develop a weakly-supervised bootstrapping pattern learning method by effectively leveraging unlabeled questions. Bootstrapping methods have been shown to be very effective in previous information extraction research (Riloff, 1996; Riloff and Jones, 1999; Ravichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal &amp; Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed </context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of AAAI ’99 /IAAI ’99, pages 474–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the 13th National Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="8113" citStr="Riloff, 1996" startWordPosition="1272" endWordPosition="1273">ains. The same techniques can be applied to comparative question identification and comparator mining from questions. However, their methods typically can achieve high precision but suffer from low recall (Jindal and Liu, 2006b) (J&amp;L). However, ensuring high recall is crucial in our intended application scenario where users can issue arbitrary queries. To address this problem, we develop a weakly-supervised bootstrapping pattern learning method by effectively leveraging unlabeled questions. Bootstrapping methods have been shown to be very effective in previous information extraction research (Riloff, 1996; Riloff and Jones, 1999; Ravichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal &amp; Liu 2006 In this subsection, we provide a brief summary of the comparative</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the 13th National Conference on Artificial Intelligence, pages 1044–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
</authors>
<title>Learning information extraction rules for semi-structured and free text.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="7066" citStr="Soderland, 1999" startWordPosition="1114" endWordPosition="1115">such as “iPhone” or “PSP” which were found in comparative questions posted by users are difficult to be predicted simply based on item similarity between them. Although they are all music players, “iPhone” is mainly a mobile phone, and “PSP” is mainly a portable game device. They are similar but also different therefore beg comparison with each other. It is clear that comparator mining and item recommendation are related but not the same. Our work on comparator mining is related to the research on entity and relation extraction in information extraction (Cardie, 1997; Califf and Mooney, 1999; Soderland, 1999; Radev et al., 2002; Carreras et al., 2003). Specifically, the most relevant work is by Jindal and Liu (2006a and 2006b) on mining comparative sentences and relations. Their methods applied class sequential rules (CSR) (Chapter 2, Liu 2006) and label sequential rules (LSR) (Chapter 2, Liu 2006) learned from annotated corpora to identify comparative sentences and extract comparative relations respectively in the news and review domains. The same techniques can be applied to comparative question identification and comparator mining from questions. However, their methods typically can achieve hi</context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>Stephen Soderland. 1999. Learning information extraction rules for semi-structured and free text. Machine Learning, 34(1-3):233–272.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>