<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.831627666666667">
Untangling the Cross-Lingual Link Structure of Wikipedia
Gerard de Melo
Max Planck Institute for Informatics
Saarbr¨ucken, Germany
demelo@mpi-inf.mpg.de
Gerhard Weikum
Max Planck Institute for Informatics
Saarbr¨ucken, Germany
weikum@mpi-inf.mpg.de
</note>
<sectionHeader confidence="0.979884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999796875">
Wikipedia articles in different languages
are connected by interwiki links that are
increasingly being recognized as a valu-
able source of cross-lingual information.
Unfortunately, large numbers of links are
imprecise or simply wrong. In this pa-
per, techniques to detect such problems are
identified. We formalize their removal as
an optimization task based on graph re-
pair operations. We then present an al-
gorithm with provable properties that uses
linear programming and a region growing
technique to tackle this challenge. This
allows us to transform Wikipedia into a
much more consistent multilingual regis-
ter of the world’s entities and concepts.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994494869565218">
Motivation. The open community-maintained en-
cyclopedia Wikipedia has not only turned the In-
ternet into a more useful and linguistically di-
verse source of information, but is also increas-
ingly being used in computational applications as
a large-scale source of linguistic and encyclope-
dic knowledge. To allow cross-lingual navigation,
Wikipedia offers cross-lingual interwiki links that
for instance connect the Indonesian article about
Albert Einstein to the corresponding articles in
over 100 other languages. Such links are extraor-
dinarily valuable for cross-lingual applications.
In the ideal case, a set of articles connected di-
rectly or indirectly via such links would all de-
scribe the same entity or concept. Due to concep-
tual drift, different granularities, as well as mis-
takes made by editors, we frequently find con-
cepts as different as economics and manager in the
same connected component. Filtering out inaccu-
rate links enables us to exploit Wikipedia’s multi-
linguality in a much safer manner and allows us to
create a multilingual register of named entities.
Contribution. Our research contributions are:
</bodyText>
<listItem confidence="0.884227090909091">
1) We identify criteria to detect inaccurate connec-
tions in Wikipedia’s cross-lingual link structure.
2) We formalize the task of removing such links
as an optimization problem. 3) We introduce an
algorithm that attempts to repair the cross-lingual
graph in a minimally invasive way. This algorithm
has an approximation guarantee with respect to
optimal solutions. 4) We show how this algorithm
can be used to combine all editions of Wikipedia
into a single large-scale multilingual register of
named entities and concepts.
</listItem>
<sectionHeader confidence="0.944156" genericHeader="method">
2 Detecting Inaccurate Links
</sectionHeader>
<bodyText confidence="0.99986704">
In this paper, we model the union of cross-lingual
links provided by all editions of Wikipedia as an
undirected graph G = (V, E) with edge weights
w(e) for e E E. In our experiments, we simply
honour each individual link equally by defining
w(e) = 2 if there are reciprocal links between the
two pages, 1 if there is a single link, and 0 other-
wise. However, our framework is flexible enough
to deal with more advanced weighting schemes,
e.g. one could easily plug in cross-lingual mea-
sures of semantic relatedness between article texts.
It turns out that an astonishing number of con-
nected components in this graph harbour inac-
curate links between articles. For instance, the
Esperanto article ‘Germana Imperiestro’ is about
German emporers and another Esperanto article
‘Germana Imperiestra Regno’ is about the Ger-
man Empire, but, as of June 2010, both are linked
to the English and German articles about the Ger-
man Empire. Over time, some inaccurate links
may be fixed, but in this and in large numbers of
other cases, the imprecise connection has persisted
for many years. In order to detect such cases, we
need to have some way of specifying that two ar-
ticles are likely to be distinct.
</bodyText>
<page confidence="0.976016">
844
</page>
<note confidence="0.9875845">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 844–853,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9954795">
Figure 1: Connected component with inaccurate
links (simplified)
</figureCaption>
<subsectionHeader confidence="0.956157">
2.1 Distinctness Assertions
</subsectionHeader>
<bodyText confidence="0.999276">
Figure 1 shows a connected component that con-
flates the concept of television as a medium with
the concept of TV sets as devices. Among other
things, we would like to state that ‘Television’ and
‘T.V.’ are distinct from ‘Television set’ and ‘TV
set’. In general, we may have several sets of enti-
ties Di,1, ... , Di,l,, for which we assume that any
two entities u,v from different sets are pairwise
distinct with some degree of confidence or weight.
In our example, Di,1 = {‘Television’,‘T.V.’}
would be one set, and Di,2 = {‘Television set’,‘TV
set’} would be another set, which means that we
are assuming ‘Television’, for example, to be dis-
tinct from both ‘Television set’ and ‘TV set’.
</bodyText>
<construct confidence="0.817709125">
Definition 1. (Distinctness Assertions) Given a
set of nodes V, a distinctness assertion is a col-
lection Di = (Di,1, ... , Di,l,) of pairwise dis-
joint (i.e. Di,j n Di,k = 0 for j =� k) sub-
sets Di,j C V that expresses that any two nodes
u E Di,j, v E Di,k from different subsets (j =� k)
are asserted to be distinct from each other with
some weight w(Di) E R.
</construct>
<bodyText confidence="0.999703722222222">
We found that many components with inaccurate
links can be identified automatically with the fol-
lowing distinctness assertions.
Criterion 1. (Distinctness between articles from
the same Wikipedia edition) For each language-
specific edition of Wikipedia, a separate asser-
tion (Di,1, Di,2, ... ) can be made, where each
Di,j contains an individual article together with
its respective redirection pages. Two articles from
the same Wikipedia very likely describe distinct
concepts unless they are redirects of each other.
For example, ‘Georgia (country)’ is distinct from
‘Georgia (U.S. State)’. Additionally, there are also
redirects that are clearly marked by a category or
template as involving topic drift, e.g. redirects
from songs to albums or artists, from products to
companies, etc. We keep such redirects in a Di,j
distinct from the one of their redirect targets.
</bodyText>
<listItem confidence="0.663638285714286">
Criterion 2. (Distinctness between categories
from the same Wikipedia edition) For each
language-specific edition of Wikipedia, a separate
assertion (Di,1, Di,2, ... ) is made, where each
Di,j contains a category page together with any
redirects. For instance, ‘Category:Writers’ is dis-
tinct from ‘Category:Writing’.
</listItem>
<bodyText confidence="0.974861470588235">
Criterion 3. (Distinctness for links with anchor
identifiers) The English ‘Division by zero’, for in-
stance, links to the German ‘Null#Division’. The
latter is only a part of a larger article about the
number zero in general, so we can make a dis-
tinctness assertion to separate ‘Division by zero’
from ‘Null’. In general, for each interwiki link or
redirection with an anchor identifier, we add an as-
sertion (Di,1, Di,2) where Di,1,Di,2 represent the
respective articles without anchor identifiers.
These three types of distinctness assertions are
instantiated for all articles and categories of all
Wikipedia editions. The assertion weights are tun-
able; the simplest choice is using a uniform weight
for all assertions (note that these weights are dif-
ferent from the edge weights in the graph). We
will revisit this issue in our experiments.
</bodyText>
<subsectionHeader confidence="0.999888">
2.2 Enforcing Consistency
</subsectionHeader>
<bodyText confidence="0.999411583333333">
Given a graph G representing cross-lingual links
between Wikipedia pages, as well as distinctness
assertions D1, ... , Dn with weights w(Di), we
may find that nodes that are asserted to be dis-
tinct are in the same connected component. We
can then try to apply repair operations to recon-
cile the graph’s link structure with the distinctness
asssertions and obtain global consistency. There
are two ways to modify the input, and for each
we can also consider the corresponding weights
as a sort of cost that quantifies how much we are
changing the original input:
</bodyText>
<listItem confidence="0.9970764">
a) Edge cutting: We may remove an edge e E
E from the graph, paying cost w(e).
b) Distinctness assertion relaxation: We may
remove a node v E V from a distinctness as-
sertion Di, paying cost w(Di).
</listItem>
<page confidence="0.99812">
845
</page>
<bodyText confidence="0.991970052631579">
Removing edges allows us to split connected com-
ponents into multiple smaller components, thereby
ensuring that two nodes asserted to be distinct are
no longer connected directly or indirectly. In Fig-
ure 1, for instance, we could delete the edge from
the Spanish ‘TV set’ article to the Japanese ‘televi-
sion’ article. In constrast, removing nodes from
distinctness assertions means that we decide to
give up our claim of them being distinct, instead
allowing them to share a connected component.
Our reliance on costs is based on the assump-
tion that the link structure or topology of the graph
provides the best indication of which cross-lingual
links to remove. In Figure 1, we have distinct-
ness assertions between nodes in two densely con-
nected clusters that are tied together only by a sin-
gle spurious link. In such cases, edge removals
can easily yield separate connected components.
When, however, the two nodes are strongly con-
nected via many different paths with high weights,
we may instead opt for removing one of the two
nodes from the distinctness assertion.
The aim will be to balance the costs for remov-
ing edges from the graph with the costs for remov-
ing nodes from distinctness assertions to produce
a consistent solution with a minimal total repair
cost. We accommodate our knowledge about dis-
tinctness while staying as close as possible to what
Wikipedia provides as input.
This can be formalized as the Weighted
Distinctness-Based Graph Separation (WDGS)
problem. Let G be an undirected graph with a set
of vertices V and a set of edges E weighted by
w : E — R. If we use a set C C_ V to spec-
ify which edges we want to cut from the original
graph, and sets Ui to specify which nodes we want
to remove from distinctness assertions, we can be-
gin by defining WDGS solutions as follows.
</bodyText>
<construct confidence="0.9909955">
Definition 2. (WDGS Solution). Given a graph
G = (V, E) and n distinctness assertions D1, ... ,
Dn, a tuple (C, U1, ... , Un) is a valid WDGS so-
lution if and only if Vi, j, k =� j, u E Di,j \ Ui,
v E Di,k \ Ui: P(u, v, E \ C) = 0, i.e. the set of
paths from u to v in the graph (V, E \ C) is empty.
Definition 3. (WDGS Cost). Let w : E — R
be a weight function for edges e E E, and w(Di)
(i = 1... n) be weights for the distinctness as-
sertions. The (total) cost of a WDGS solution
</construct>
<equation confidence="0.998395">
S = (C, U1, ... , Un) is then defined as
c(S) = c(C, U1, ... , Un)
n
= w(e) + |Ui |w(Di)
eEC i=1
</equation>
<bodyText confidence="0.562797416666667">
Definition 4. (WDGS). A WDGS problem instance
P consists of a graph G = (V, E) with edge
weights w(e) and n distinctness assertions D1,
..., Dn with weights w(Di). The objective con-
sists in finding a solution (C, U1, ... , Un) with
minimal cost c(C, U1, ... , Un).
It turns out that finding optimal solutions effi-
ciently is a hard problem (proofs in Appendix A).
Theorem 1. WDGS is NP-hard and APX-hard. If
the Unique Games Conjecture (Khot, 2002) holds,
then it is NP-hard to approximate WDGS within
any constant factor α &gt; 0.
</bodyText>
<sectionHeader confidence="0.983199" genericHeader="method">
3 Approximation Algorithm
</sectionHeader>
<bodyText confidence="0.984460166666667">
Due to the hardness of WDGS, we devise a
polynomial-time approximation algorithm with an
approximation factor of 4ln(nq + 1) where n is
the number of distinctness assertions and q =
max |Di,j|. This means that for all problem in-
i,j
</bodyText>
<equation confidence="0.848707">
stances P, we can guarantee
c(S(P)) &lt; 4ln(nq + 1),
c(S (P)) —
</equation>
<bodyText confidence="0.999307523809524">
where S(P) is the solution determined by our al-
gorithm, and S*(P) is an optimal solution. Note
that this approximation guarantee is independent
of how long each Di is, and that it merely repre-
sents an upper bound on the worst case scenario.
In practice, the results tend to be much closer to
the optimum, as will be shown in Section 4.
Our algorithm first solves a linear program (LP)
relaxation of the original problem, which gives
us hints as to which edges should most likely be
cut and which nodes should most likely be re-
moved from distinctness assertions. Note that this
is a continuous LP, not an integer linear program
(ILP); the latter would not be tractable due to the
large number of variables and constraints of the
problem. After solving the linear program, a new
– extended – graph is constructed and the optimal
LP solution is used to define a distance metric on
it. The final solution is obtained by smartly se-
lecting regions in this extended graph as the in-
dividual output components, employing a region
</bodyText>
<page confidence="0.989678">
846
</page>
<bodyText confidence="0.9969358">
growing technique in the spirit of the seminal work
by Leighton and Rao (1999). Edges that cross the
boundaries of these regions are cut.
Definition 5. Given a WDGS instance, we define a
linear program of the following form:
</bodyText>
<equation confidence="0.9523671">
minimize
ui,vw(Di)
subject to
pi,j,v = ui,v ∀i, j&lt;li, v ∈ Di,j (1)
pi,j,v + ui,v ≥ 1 ∀i, j&lt;li, v ∈ U Di,k (2)
k&gt;j
pi,j,v ≤ pi,j,u + de ∀i, j&lt;li, e=(u,v) ∈ E (3)
de ≥ 0 ∀e ∈ E
ui,v ≥ 0 ∀i, v ∈
pi,j,v ≥ 0 ∀i, j&lt;li, v∈V (6)
</equation>
<bodyText confidence="0.999902264705883">
The LP uses decision variables de and ui,v, and
auxiliary variables pi,j,v that we refer to as poten-
tial variables. The de variables indicate whether
(in the continuous LP: to what degree) an edge
e should be deleted, and the ui,v variables indi-
cate whether (to what degree) v should be removed
from a distinctness assertion Di. The LP objec-
tive function corresponds to Definition 3, aiming
to minimize the total costs. A potential variable
pi,j,v reflects a sort of potential difference between
an assertion Di,j and a node v. If pi,j,v = 0, then v
is still connected to nodes in Di,j. Constraints (1)
and (2) enforce potential differences between Di,j
and all nodes in Di,k with k &gt; j. For instance,
for distinctness between ‘New York City’ and ‘New
York’ (the state), they might require ‘New York’
to have a potential of 1, while ‘New York City’
has a potential of 0. The potential variables are
tied to the deletion variables de for edges in Con-
straint (3) as well as to the ui,v in Constraints (1)
and (2). This means that the potential difference
pi,j,v + ui,v ≥ 1 can only be obtained if edges are
deleted on every path between ‘New York City’ and
‘New York’, or if at least one of these two nodes is
removed from the distinctness assertion (by setting
the corresponding ui,v to non-zero values). Con-
straints (4), (5), (6) ensure non-negativity.
Having solved the linear program, the next ma-
jor step is to convert the optimal LP solution into
the final – discrete – solution. We cannot rely
on standard rounding methods to turn the optimal
fractional values of the de and ui,v variables into
a valid solution. Often, all solution variables have
small values and rounding will merely produce an
</bodyText>
<equation confidence="0.464219">
empty (C, U1, ... , Un) = (∅, ∅, ... , ∅). Instead,
</equation>
<bodyText confidence="0.949543772727273">
a more sophisticated technique is necessary. The
optimal solution of the LP can be used to define
an extended graph G0 with a distance metric d be-
tween nodes. The algorithm then operates on this
graph, in each iteration selecting regions that be-
come output components and removing them from
the graph. A simple example is shown in Figure 2.
The extended graph contains additional nodes and
edges representing distinctness assertions. Cutting
one of these additional edges corresponds to re-
moving a node from a distinctness assertion.
Definition 6. Given G = (V, E) and distinct-
ness assertions D1, ... , Dn with weights w(Di),
we define an undirected graph G0 = (V 0, E0)
where V 0 = V ∪ {vi,v  |i = 1... n, w(Di) &gt;
0, v ∈ S j Di,j}, E0 = {e ∈ E  |w(e) &gt; 0} ∪
{(v, vi,v)  |v ∈ Di,j, w(Di) &gt; 0}. We accordingly
extend the definition of w(e) to additionally cover
the new edges by defining w(e) = w(Di) for e =
(v, vi,v). We also extend it for sets 5 of edges by
defining w(5) = P e∈S w(e). Finally, we define a
node distance metric
</bodyText>
<equation confidence="0.9903484">
0 u = v
de (u, v) ∈ E
ui,v u = vi,v
ui,u v = vi,u
d(u0, v0) otherwise,
</equation>
<bodyText confidence="0.9928495">
where P(u, v, E0) denotes the set of acyclic paths
between two nodes in E0. We further fix
</bodyText>
<equation confidence="0.9428605">
X�f= d(u, v) w(e)
(u,v)∈E&apos;
</equation>
<bodyText confidence="0.994619333333333">
as the weight of the fractional solution of the LP
(cf is a constant based on the original E0, irre-
spective of later modifications to the graph).
</bodyText>
<construct confidence="0.818634666666667">
Definition 7. Around a given node v in G0, we
consider regions R(v, r) ⊆ V with radius r. The
cut C(v, r) of a given region is defined as the set
of edges in G0 with one endpoint within the region
and one outside the region:
R(v, r) = {v0 ∈ V 0  |d(v, v0) ≤ r}
C(v, r) = {e ∈ E0  ||e ∩ R(v, r) |= 1}
For sets of nodes 5 ⊆ V, we define R(5, r) =
S R(v, r) and C(5, r) = S C(v, r).
</construct>
<figure confidence="0.974484956521739">
v∈S v∈S
Pn
i=1
dew(e) +
P
e∈E
Pli
j=1
P
v∈Di,j
(4)
Di,j (5)
l i
jU
d(u, v) = ⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
min
p∈
P(u,v,E&apos;)
P
(u1,v�)
∈p
</figure>
<page confidence="0.535397">
847
</page>
<figureCaption confidence="0.8281206">
Figure 2: Extended graph with two added nodes
v1,u, v1,v representing distinctness between ‘Tele-
visi´on’ and ‘Televisor’, and a region around v1,u
that would cut the link from the Japanese ‘Televi-
sion’ to ‘Televisor’
</figureCaption>
<equation confidence="0.898685166666667">
Definition 8. Given q = max |Di,j|, we approxi-
i,j
mate the optimal cost of regions as:
c(v, r) = E d(u, u&apos;) w(e) (1)
e=(u,u&apos;)EE&apos;:
eCR(v,r)
+ 1: (r − d(v, v&apos;)) w(e)
eEC(v,r)
v&apos;EenR(v,r)
1 1: ��f + c(v,r) (2)
��(S, r) = vES
nq
</equation>
<bodyText confidence="0.975431434782609">
The first summand accounts for the edges en-
tirely within the region, and the second one ac-
counts for the edges in C(v, r) to the extent that
they are within the radius. The definition of c(S, r)
contains an additional slack component that is re-
quired for the approximation guarantee proof.
Based on these definitions, Algorithm 3.1 uses
the LP solution to construct the extended graph.
It then repeatedly, as long as there is an unsatis-
fied assertion Di, chooses a set S of nodes con-
taining one node from each relevant Di,j. Around
the nodes in S it simultaneously grows |S |regions
with the same radius, a technique previously sug-
gested by Avidor and Langberg (2007). These re-
gions are essentially output components that de-
termine the solution. Repeatedly choosing the
radius that minimizes w(C(S,r)) allows us to ob-
c(S,r)
tain the approximation guarantee, because the dis-
tances in this extended graph are based on the so-
lution of the LP. The properties of this algorithm
are given by the following two theorems (proofs in
Appendix A).
</bodyText>
<construct confidence="0.697668857142857">
Theorem 2. The algorithm yields a valid WDGS
solution (C, U1, ... , Un).
Theorem 3. The algorithm yields a solution
(C, U1, ... , Un) with an approximation factor of
4ln(nq + 1) with respect to the cost of the op-
timal WDGS solution (C*, U*1 , ... , U*n), where n
is the number of distinctness assertions and q =
</construct>
<bodyText confidence="0.561675">
max |Di,j|. This solution can be obtained in poly-
i,j
nomial time.
</bodyText>
<sectionHeader confidence="0.999218" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.757109">
4.1 Wikipedia
</subsectionHeader>
<bodyText confidence="0.999458785714286">
We downloaded February 2010 XML dumps of
all available editions of Wikipedia, in total 272
editions that amount to 86.5 GB uncompressed.
From these dumps we produced two datasets.
Dataset A captures cross-lingual interwiki links
between pages, in total 77.07 million undirected
edges (146.76 million original links). Dataset
B additionally includes 2.2 million redirect-based
edges. Wikipedia deals with interwiki links to
redirects transparently, however there are many
redirects with titles that do not co-refer, e.g. redi-
rects from members of a band to the band, or from
aspects of a topic to the topic in general. We only
included redirects in the following cases:
</bodyText>
<listItem confidence="0.893964714285714">
• the titles of redirect and redirect target match
after Unicode NFKD normalization, diacrit-
ics removal, case conversion, and removal of
punctuation characters
• the redirect uses certain templates or cate-
gories that indicate co-reference with the tar-
get (alternative names, abbreviations, etc.)
</listItem>
<bodyText confidence="0.989614">
We treated them like reciprocal interwiki links by
assigning them a weight of 2.
</bodyText>
<subsectionHeader confidence="0.999751">
4.2 Application of Algorithm
</subsectionHeader>
<bodyText confidence="0.9999451">
The choice of distinctness assertion weights de-
pends on how lenient we wish to be towards con-
ceptual drift, allowing us to opt for more fine- or
more coarse-grained distinctions. In our experi-
ments, we decided to prefer fine-grained concep-
tual distinctions, and settled on a weight of 100.
We analysed over 20 million connected com-
ponents in each dataset, checking for distinctness
assertions. For the roughly 110,000 connected
components with relevant distinctness assertions,
</bodyText>
<page confidence="0.983979">
848
</page>
<construct confidence="0.459948">
Algorithm 3.1 WDGS Approximation Algorithm
</construct>
<listItem confidence="0.955117086956522">
1: procedure SELECT(V, E, V &apos;, E&apos;, w, D1, ... , Dn, l1, ... , ln)
2: solve linear program given by Definition 5 &gt; determine optimal fractional solution
3: construct G&apos; = (V &apos;, E&apos;) &gt; extended graph (Definition 6)
4: C ← {e ∈ E  |w(e) = 0} &gt; cut zero-weighted edges
li−1
5: Ui ← U Di,j ∀i : w(Di) = 0 &gt; remove zero-weighted Di
j=1
6: while ∃i, j, k &gt; j, u ∈ Di,j, v ∈ Di,k : P(vi,u, vi,v, E&apos;) =6 ∅ do &gt; find unsatisfied assertion
7: 5 ← ∅ &gt; set of nodes around which regions will be grown
8: for all j in 1... li − 1 do &gt; arbitrarily choose node from each Di,j
9: if ∃v ∈ Di,j : vi,v ∈ V &apos; then 5 ← 5 ∪ vi,v
10: D ← {d(u, v) ≤ 21|u ∈ 5, v ∈ V&apos;} ∪ {1 } &gt; set of distances
11: choose E such that ∀d, d&apos; ∈ D : 0 &lt; E « |d − d&apos; |&gt; infinitesimally small
12: r ← argmin w(C(5, r))
r=d−e: dED\101 &gt; choose optimal radius (ties broken arbitrarily)
��(5, r)
13: V &apos; ← V &apos; \ R(5, r) &gt; remove regions from G&apos;
14: E&apos; ← {e ∈ E&apos;  |e ⊆ V &apos;}
15: C ← C ∪ (C(5, r) ∩ E) &gt; update global solution
16: for all i&apos; in 1 ... n do
17: Ui, ← Ui, ∪ {v  |(vi,,v, v) ∈ C(5, r)}
18: for all j in 1... li, do Di,,j ← Di,,j ∩ V &apos; &gt; prune distinctness assertions
19: return (C, U1, ... , Un)
</listItem>
<bodyText confidence="0.999778090909091">
we applied our algorithm, relying on the commer-
cial CPLEX tool to solve the linear programs. In
most cases, the LP solving took less than a second,
however the LP sizes grow exponentially with the
number of nodes and hence the time complex-
ity increases similarly. In about 300 cases per
dataset, CPLEX took too long and was automat-
ically killed or the linear program was a priori
deemed too large to complete in a short amount
of time. For these cases, we adopted an alternative
strategy described later on.
Table 1 provides the experimental results for the
two datasets. Dataset B is more connected and
thus has fewer connected components with more
pairs of nodes asserted to be distinct by distinct-
ness assertions. The LP given by Definition 5
provides fractional solutions that constitute lower
bounds on the optimal solution (cf. also Lemma
5 in Appendix A), so the optimal solution can-
not have a cost lower than the fractional LP solu-
tion. Table 1 shows that in practice, our algorithm
achieves near-optimal results.
</bodyText>
<subsectionHeader confidence="0.999014">
4.3 Linguistic Adequacy
</subsectionHeader>
<bodyText confidence="0.998771666666667">
The near-optimal results of our algorithm apply
with respect to our problem formalization, which
aims at repairing the graph in a minimally inva-
</bodyText>
<tableCaption confidence="0.996554">
Table 1: Algorithm Results
</tableCaption>
<table confidence="0.996910944444444">
Dataset A Dataset B
Connected 23,356,027 21,161,631
components
– with distinctness 112,857 113,714
assertions
– algorithm applied 112,580 113,387
successfully
Distinctness 380,694 379,724
assertions
Node pairs con- 916,554 1,047,299
sidered distinct
Lower bound on 1,255,111 1,245,004
optimal cost
Cost of our solution 1,306,747 1,294,196
Factor 1.04 1.04
Edges to be deleted 1,209,798 1,199,181
(undirected)
Nodes to be merged 603 573
</table>
<bodyText confidence="0.9637826">
sive way. It may happen, however, that the graph’s
topology is misleading, and that in a specific case
deleting many cross-lingual links to separate two
entities is more appropriate than looking for a
conservative way to separate them. This led us
</bodyText>
<page confidence="0.996736">
849
</page>
<bodyText confidence="0.9997842">
to study the linguistic adequacy. Two annotators
evaluated 200 randomly selected separated pairs
from Dataset A consisting of an English and a
German article, with an inter-annotator agreement
(Cohen n) of 0.656. Examples are given in Table
2. We obtained a precision of 87.97% ± 0.04%
(Wilson score interval) against the consensus an-
notation. Many of the errors are the result of ar-
ticles having many inaccurate outgoing links, in
which case they may be assigned to the wrong
component. In other cases, we noted duplicate ar-
ticles in Wikipedia.
Occasionally, we also observed differences in
scope, where one article would actually describe
two related concepts in a single page. Our algo-
rithm will then either make a somewhat arbitrary
assignment to the component of either the first or
second concept, or the broader generalization of
the two concepts becomes a separate, more gen-
eral connected component.
</bodyText>
<subsectionHeader confidence="0.99832">
4.4 Large Problem Instances
</subsectionHeader>
<bodyText confidence="0.9999962">
When problem instances become too large, the lin-
ear programs can become too unwieldy for lin-
ear optimization software to cope with on current
hardware. In such cases, the graphs tend to be very
sparsely connected, consisting of many smaller,
more densely connected subgraphs. We thus in-
vestigated graph partitioning heuristics to decom-
pose larger graphs into smaller parts that can more
easily be handled with our algorithm. The METIS
algorithms (Karypis and Kumar, 1998) can de-
compose graphs with hundreds of thousands of
nodes almost instantly, but favour equally sized
clusters over lower cut costs. We obtained parti-
tionings with costs orders of magnitude lower us-
ing the heuristic by Dhillon et al. (2007).
</bodyText>
<subsectionHeader confidence="0.998415">
4.5 Database of Named Entities
</subsectionHeader>
<bodyText confidence="0.99999206060606">
The partitioning heuristics allowed us to process
all entries in the complete set of Wikipedia dumps
and produce a clean output set of connected com-
ponents where each Wikipedia article or category
belongs to a connected component consisting of
pages about the same entity or concept. We can re-
gard these connected components as equivalence
classes. This means that we obtain a large-scale
multilingual database of named entities and their
translations. We are also able to more safely trans-
fer information cross-lingually between editions.
For example, when an article a has a category c in
the French Wikipedia, we can suggest the corre-
sponding Indonesian category for the correspond-
ing Indonesian article.
Moreover, we believe that this database will
help extend resources like DBPedia and YAGO
that to date have exclusively used the English
Wikipedia as their repository of entities and
classes. With YAGO’s category heuristics, even
entirely non-English connected components can
be assigned a class in WordNet as long as at least
one of the relevant categories has an English page.
So, the French Wikipedia article on the Dutch
schooner ‘JR Tolkien’, despite the lack of a cor-
responding English article, can be assigned to the
WordNet synset for ‘ship’. Using YAGO’s plu-
ral heuristic to distinguish classes (Einstein is a
physicist) from topic descriptors (Einstein belongs
to the topic physics), we determined that over 4.8
million connected components can be linked to
WordNet, greatly surpassing the 3.2 million arti-
cles covered by the English Wikipedia alone.
</bodyText>
<sectionHeader confidence="0.999818" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999747785714286">
A number of projects have used Wikipedia as a
database of named entities (Ponzetto and Strube,
2007; Silberer et al., 2008). The most well-
known are probably DBpedia (Auer et al., 2007),
which serves as a hub in the Linked Data Web,
Freebase1, which combines human input and au-
tomatic extractors, and YAGO (Suchanek et al.,
2007), which adds an ontological structure on top
of Wikipedia’s entities. Wikipedia has been used
cross-lingually for cross-lingual IR (Nguyen et al.,
2009), question answering (Ferr´andez et al., 2007)
as well as for learning transliterations (Pasternack
and Roth, 2009), among other things.
Mihalcea and Csomai (2007) have studied pre-
dicting new links within a single edition of
Wikipedia. Sorg and Cimiano (2008) considered
the problem of suggesting new cross-lingual links,
which could be used as additional inputs in our
problem. Adar et al. (2009) and Bouma et al.
(2009) show how cross-lingual links can be used
to propagate information from one Wikipedia’s in-
foboxes to another edition.
Our aggregation consistency algorithm uses
theoretical ideas put forward by researchers study-
ing graph cuts (Leighton and Rao, 1999; Garg et
al., 1996; Avidor and Langberg, 2007). Our prob-
lem setting is related to that of correlation cluster-
ing (Bansal et al., 2004), where a graph consist-
</bodyText>
<footnote confidence="0.992151">
1http://www.freebase.com/
</footnote>
<page confidence="0.997806">
850
</page>
<tableCaption confidence="0.998194">
Table 2: Examples of separated concepts
</tableCaption>
<table confidence="0.997021777777778">
English concept German concept Explanation
(translated)
Coffee percolator French Press different types of brewing devices
Baqa-Jatt Baqa al-Gharbiyye Baqa-Jatt is a city resulting from a merger
of Baqa al-Gharbiyye and Jatt
Leucothoe (plant) Leucothea (Orchamos) the second refers to a figure of Greek
mythology
Old Belarusian language Ruthenian language the second is often considered slightly
broader
</table>
<bodyText confidence="0.999733230769231">
ing of positively and negatively labelled similar-
ity edges is clustered such that similar items are
grouped together, however our approach is much
more generic than conventional correlation clus-
tering. Charikar et al. (2005) studied a variation
of correlation clustering that is similar to WDGS,
but since a negative edge would have to be added
between each relevant pair of entities in a distinct-
ness assertion, the approximation guarantee would
only be O(log(n |V |2)). Minimally invasive re-
pair operations on graphs have also been stud-
ied for graph similarity computation (Zeng et al.,
2009), where two graphs are provided as input.
</bodyText>
<sectionHeader confidence="0.987682" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999990541666667">
We have presented an algorithmic framework for
the problem of co-reference that produces consis-
tent partitions by intelligently removing edges or
allowing nodes to remain connected. This algo-
rithm has successfully been applied to Wikipedia’s
cross-lingual graph, where we identified and elim-
inated surprisingly large numbers of inaccurate
connections, leading to a large-scale multilingual
register of names.
In future work, we would like to investigate
how our algorithm behaves in extended settings,
e.g. we can use heuristics to connect isolated,
unconnected articles to likely candidates in other
Wikipedias using weighted edges. This can be
extended to include mappings from multiple lan-
guages to WordNet synsets, with the hope that
the weights and link structure will then allow the
algorithm to make the final disambiguation deci-
sion. Additional scenarios include dealing with
co-reference on the Linked Data Web or mappings
between thesauri. As such resources are increas-
ingly being linked to Wikipedia and DBpedia, we
believe that our techniques will prove useful in
making mappings more consistent.
</bodyText>
<subsectionHeader confidence="0.787224">
A Proofs
</subsectionHeader>
<bodyText confidence="0.943152380952381">
Proof (Theorem 1). We shall reduce the mini-
mum multicut problem to WDGS. The hardness
claims then follow from Chawla et al. (2005).
Given a graph G = (V, E) with a positive cost
c(e) for each e ∈ E, and a set D = {(si, ti)  |i =
1... k} of k demand pairs, our goal is to find
a multicut M with respect to D with minimum
total cost EeEM c(e). We convert each demand
pair (si, ti) into a distinctness assertion Di =
({si}, {ti}) with weight w(Di) = 1+EeEE c(e).
An optimal WDGS solution (C, U1, ... , Uk) with
cost c then implies a multicut C with the same
weight, because each w(Di) &gt; EeEE c(e), so
all demand pairs will be satisfied. C is a minimal
multicut because any multicut C&apos; with lower cost
would imply a valid WDGS solution (C&apos;, ∅, ... , ∅)
with a cost lower than the optimal one, which is a
contradiction.
Lemma 4. The linear program given by Defini-
tion 5 enforces that for any i,j,k =6 j,u ∈ Di,j,
v ∈ Di,k, and any path v0, ... , vt with v0 = u,
</bodyText>
<equation confidence="0.981471">
vt = v we obtain ui,u+Et−1
l=0 d(vl,vl+1)+ui,v ≥ 1.
</equation>
<bodyText confidence="0.995609466666666">
The integer linear program obtained by aug-
menting Definition 5 with integer constraints
de, ui,v,pi,j,v ∈ {0, 1} (for all applicable e, i, j,
v) produces optimal solutions (C, U1, ... , Uk) for
WDGS problems, obtained as C = ({e ∈ E  |de =
1}, Ui = {v  |ui,v = 1}.
Proof. Without loss of generality, let us assume
that j &lt; k. The LP constraints give us pi,j,vt ≤
pi,j,vt−1 +d(vt−1,vt), ... , pi,j,v1 ≤ pi,j,v0 +d(v0,v1),
as well as pi,j,v0 = ui,u and pi,j,vt + ui,v ≥ 1.
Hence 1 ≤ pi,j,vt+ui,v ≤ ui,u+El=0 d(vl,vl+1)+
ui,v.
With added integrality constraints, we obtain ei-
ther u ∈ Ui, v ∈ Ui, or at least one edge along any
path from u to v is cut, i.e. P(u, v, E \ C) = ∅.
</bodyText>
<page confidence="0.994815">
851
</page>
<bodyText confidence="0.998442428571429">
This proves that any ILP solution enduces a valid
WDGS solution (Definition 2).
Clearly, the integer program’s objective func-
tion minimizes c(C, U1, ... , Un) (Definition 3) if
C = ({e ∈ E  |de = 1}, Ui = {v  |ui,v = 1}.
To see that the solutions are optimal, it thus suf-
fices to observe that any optimal WDGS solution
(C∗, U∗1 , ... , U∗n) yields a feasible ILP solution
de = IC*(e), ui,v = IUi (v).
Proof (Theorem 2). ri &lt; 21 holds for any ra-
dius ri chosen by the algorithm, so for any re-
gion R(v0, r) grown around a node v0, and any
two nodes u, v within that region, the triangle in-
equality gives us d(u, v) ≤ d(u, v0) + d(v0, v) &lt;
</bodyText>
<equation confidence="0.7410705">
2 + 1
1 2 = 1 (maximal distance condition). At
</equation>
<bodyText confidence="0.999773382352941">
the same time, by Lemma 4 and Definition 6 for
any u ∈ Di,j, v ∈ Di,k (j =6 k), we obtain
d(vi,u, vi,v) = d(vi,u, u) + d(u, v) + d(v, vi,v) ≥
1. With the maximal distance condition above, this
means that vi,u and vi,v cannot be in the same re-
gion. Hence u, v cannot be in the same region,
unless the edge from vi,u to u is cut (in which case
u will be placed in Ui) or the edge from v to vi,v
is cut (in which case v will be placed in Ui). Since
each region is separated from other regions via C,
we obtain that ∀i, j, k =6 j, u, v: u ∈ Di,j \ Ui,
v ∈ Di,k \ Ui implies P(u, v, E \ C) = ∅, so a
valid solution is obtained.
Lemma 5 (essentially due to Garg et al. (1996)).
For any i where ∃j, k &gt; j, u ∈ Di,j, v ∈ Di,k :
P(vi,u, vi,v, E0) =6 ∅ and w(Di) &gt; 0, there exists
an r such that w(C(S, r)) ≤ 2ln(nq + 1) ˆc(S, r),
0 ≤ r &lt; 21 for any set S consisting of vi,v nodes.
Proof. Define w(S, r) = Ev∈S w(C(v, r)). We
will prove that there exists an appropriate r with
w(C(S, r)) ≤ w(S, r) ≤ 2ln(nq+1) ˆc(S, r). As-
sume, for reductio ad absurdum, that ∀r ∈ [0, 12) :
w(S, r) &gt; 2ln(nq + 1)ˆc(S, r). As we expand
the radius r, we note that ˆc(S, r) ddr = w(S, r)
whereever cˆ is differentiable with respect to r.
There are only a finite number of points r1,. . . ,rl−1
in (0, 12) where this is not the case (namely, when
∃u ∈ S, v ∈ V 0 : d(u, v) = ri). Also note
that cˆ increases monotonically for increasing val-
ues of r, and that it is universally greater than
zero (since there is a path between vi,u, vi,v). Set
r0 = 0, rl = 21 and choose E such that 0 &lt; E �
min{rj+1 − rj  |j &lt; l}. Our assumption then
implies:
</bodyText>
<equation confidence="0.976355">
Jrr11_−1+e �sr, dr
� l
&gt;Erj − rj−1 − 2E 2ln(nq + 1)
j=1
ln ˆc(S, rj − E) − ln ˆc(S, rj−1 + E)
&gt; (12 − 2lc) 2ln(nq + 1)
ln ˆc(S, 21 − E) − ln ˆc(S, 0)
&gt; (1 − 4lc)ln(nq + 1)
ˆc(S, 1
ˆc(S,0) &gt; (nq + 1)1−4l�
2 −�)
ˆc(S, 2 1 − �) &gt; (nq + 1)1−4l�ˆc(S, 0)
</equation>
<bodyText confidence="0.999523648648649">
For small E, the right term can get arbitrarily close
to (nq + 1)ˆc(S, 0) ≥ ˆcf + ˆc(S, 0), which is strictly
larger than ˆc(S, 21 − E) no matter how small E be-
comes, so the initial assumption is false.
Proof (Theorem 3). Let Si, ri denote the set
S and radius r chosen in particular iterations,
and ci the corresponding costs incurred: ci =
w(C(Si, r) ∩ E) + |Ui|w(Di) = w(C(Di, r)).
Note that any ri chosen by the algorithm will in
fact fulfil the criterion described by Lemma 5, be-
cause ri is chosen to minimize the ratio between
the two terms, and the minimizing r ∈ [0, 12)
must be among the r considered by the algo-
rithm (w(C(Di, r)) only changes at one of those
points, so the minimum is reached by approach-
ing the points from the left). Hence, we obtain
ci ≤ 2ln(n + 1)ˆc(Si, ri). For our global solution,
note that there is no overlap between the regions
chosen within an iteration, since regions have a
radius strictly smaller than 12, while vi,u, vi,v for
u ∈ Di,j, v ∈ Di,k, j =6 k have a distance of
at least 1. Nor is there any overlap between re-
gions from different iterations, because in each it-
eration the selected regions are removed from G0.
Globally, we therefore obtain c(C, U1, ... , Un) =
Ei ci &lt; 2 ln(nq + 1) Ei ˆc(Si, ri) ≤ 2ln(nq +
1)2ˆcf (observe that i ≤ nq). Since ˆcf is the ob-
jective score for the fractional LP relaxation solu-
tion of the WDGS ILP (Lemma 4), we obtain ˆcf ≤
c(C∗, U∗1 , ... , U∗n), and thus c(C, U1, ... , Un) &lt;
4ln(n + 1)c(C∗, U∗1 , ... , U∗n).
To obtain a solution in polynomial time, note
that the LP size is polynomial with respect to nq
and may be solved using a polynomial algorithm
(Karmarkar, 1984). The subsequent steps run in
O(nq) iterations, each growing up to |V  |regions
using O(|V |2) uniform cost searches.
</bodyText>
<figure confidence="0.892595">
El
j=1
El
j=1
</figure>
<page confidence="0.992127">
852
</page>
<sectionHeader confidence="0.993053" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999879144144144">
Eytan Adar, Michael Skinner, and Daniel S. Weld.
2009. Information arbitrage across multi-lingual
Wikipedia. In Ricardo A. Baeza-Yates, Paolo Boldi,
Berthier A. Ribeiro-Neto, and Berkant Barla Cam-
bazoglu, editors, Proceedings of the 2nd Interna-
tional Conference on Web Search and Web Data
Mining, WSDM 2009, pages 94–103. ACM.
S¨oren Auer, Chris Bizer, Jens Lehmann, Georgi Kobi-
larov, Richard Cyganiak, and Zachary Ives. 2007.
DBpedia: a nucleus for a web of open data. In
Aberer et al., editor, The Semantic Web, 6th Interna-
tional Semantic Web Conference, 2nd Asian Seman-
tic Web Conference, ISWC 2007 + ASWC 2007, Bu-
san, Korea, November 11–15, 2007, Lecture Notes
in Computer Science 4825. Springer.
Adi Avidor and Michael Langberg. 2007. The multi-
multiway cut problem. Theoretical Computer Sci-
ence, 377(1-3):35–42.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learning, 56(1-
3):89–113.
Gosse Bouma, Sergio Duarte, and Zahurul Islam.
2009. Cross-lingual alignment and completion of
Wikipedia templates. In CLIAWS3 ’09: Proceed-
ings of the Third International Workshop on Cross
Lingual Information Access, pages 21–29, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Moses Charikar, Venkatesan Guruswami, and Anthony
Wirth. 2005. Clustering with qualitative informa-
tion. Journal of Computer and System Sciences,
71(3):360–383.
Shuchi Chawla, Robert Krauthgamer, Ravi Kumar, Yu-
val Rabani, and D. Sivakumar. 2005. On the hard-
ness of approximating multicut and sparsest-cut. In
In Proceedings of the 20th Annual IEEE Conference
on Computational Complexity, pages 144–153.
Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis.
2007. Weighted graph cuts without eigenvectors.
a multilevel approach. IEEE Trans. Pattern Anal.
Mach. Intell., 29(11):1944–1957.
Sergio Ferr´andez, Antonio Toral, ´Oscar Ferr´andez, An-
tonio Ferr´andez, and Rafael Mu˜noz. 2007. Ap-
plying Wikipedia’s multilingual knowledge to cross-
lingual question answering. In NLDB, pages 352–
363.
Naveen Garg, Vijay V. Vazirani, and Mihalis Yan-
nakakis. 1996. Approximate max-flow min-
(multi)cut theorems and their applications. SIAM
Journal on Computing (SICOMP), 25:698–707.
Narendra Karmarkar. 1984. A new polynomial-time
algorithm for linear programming. In STOC ’84:
Proceedings of the 16th Annual ACM Symposium on
Theory of Computing, pages 302–311, New York,
NY, USA. ACM.
George Karypis and Vipin Kumar. 1998. A fast and
high quality multilevel scheme for partitioning irreg-
ular graphs. SIAM Journal on Scientific Computing,
20(1):359–392.
Subhash Khot. 2002. On the power of unique 2-prover
1-round games. In STOC ’02: Proceedings of the
34th Annual ACM Symposium on Theory of Com-
puting, pages 767–775, New York, NY, USA. ACM.
Tom Leighton and Satish Rao. 1999. Multicommodity
max-flow min-cut theorems and their use in design-
ing approximation algorithms. Journal of the ACM,
46(6):787–832.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge. In
Proceedings of the 16th ACM Conference on Infor-
mation and Knowledge Management (CIKM 2007),
pages 233–242, New York, NY, USA. ACM.
D. Nguyen, A. Overwijk, C. Hauff, R.B. Trieschnigg,
D. Hiemstra, and F.M.G. Jong de. 2009. Wiki-
Translate: query translation for cross-lingual infor-
mation retrieval using only Wikipedia. In Carol
Peters, Thomas Deselaers, Nicola Ferro, and Julio
Gonzalo, editors, Evaluating Systems for Multilin-
gual and Multimodal Information Access, Lecture
Notes in Computer Science 5706, pages 58–65.
Jeff Pasternack and Dan Roth. 2009. Learning bet-
ter transliterations. In CIKM ’09: Proceeding of the
18th ACM Conference on Information and Knowl-
edge Management, pages 177–186, New York, NY,
USA. ACM.
Simone Paolo Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
AAAI 2007: Proceedings of the 22nd Conference
on Artificial Intelligence, pages 1440–1445. AAAI
Press.
Carina Silberer, Wolodja Wentland, Johannes Knopp,
and Matthias Hartung. 2008. Building a multilin-
gual lexical resource for named entity disambigua-
tion, translation and transliteration. In European,
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC’08), Mar-
rakech, Morocco.
Philipp Sorg and Philipp Cimiano. 2008. Enrich-
ing the crosslingual link structure of Wikipedia - a
classification-based approach. In Proceedings of the
AAAI 2008 Workshop on Wikipedia and Artifical In-
telligence.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of the 16th International
World Wide Web conference, WWW, New York, NY,
USA. ACM Press.
Zhiping Zeng, Anthony K. H. Tung, Jianyong Wang,
Jianhua Feng, and Lizhu Zhou. 2009. Comparing
stars: On approximating graph edit distance. Pro-
ceedings of the VLDB Endowment, 2(1):25–36.
</reference>
<page confidence="0.99933">
853
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.153839">
<title confidence="0.99794">Untangling the Cross-Lingual Link Structure of Wikipedia</title>
<author confidence="0.581950666666667">Gerard de_Melo Max Planck Institute for Informatics Saarbr¨ucken</author>
<author confidence="0.581950666666667">Germany</author>
<email confidence="0.997923">demelo@mpi-inf.mpg.de</email>
<author confidence="0.838932">Gerhard Weikum Max Planck Institute for Informatics Saarbr¨ucken</author>
<author confidence="0.838932">Germany</author>
<email confidence="0.999489">weikum@mpi-inf.mpg.de</email>
<abstract confidence="0.997126764705882">Wikipedia articles in different languages are connected by interwiki links that are increasingly being recognized as a valuable source of cross-lingual information. Unfortunately, large numbers of links are imprecise or simply wrong. In this paper, techniques to detect such problems are identified. We formalize their removal as an optimization task based on graph repair operations. We then present an algorithm with provable properties that uses linear programming and a region growing technique to tackle this challenge. This allows us to transform Wikipedia into a much more consistent multilingual register of the world’s entities and concepts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
<author>Michael Skinner</author>
<author>Daniel S Weld</author>
</authors>
<title>Information arbitrage across multi-lingual Wikipedia. In</title>
<date>2009</date>
<booktitle>Proceedings of the 2nd International Conference on Web Search and Web Data Mining, WSDM 2009,</booktitle>
<pages>94--103</pages>
<editor>Ricardo A. Baeza-Yates, Paolo Boldi, Berthier A. Ribeiro-Neto, and Berkant Barla Cambazoglu, editors,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="26939" citStr="Adar et al. (2009)" startWordPosition="4692" endWordPosition="4695">uman input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of correlation clustering (Bansal et al., 2004), where a graph consist1http://www.freebase.com/ 850 Table 2: Examples of separated concepts English concept German concept Explanation (translated) Coffee percolator French Press different ty</context>
</contexts>
<marker>Adar, Skinner, Weld, 2009</marker>
<rawString>Eytan Adar, Michael Skinner, and Daniel S. Weld. 2009. Information arbitrage across multi-lingual Wikipedia. In Ricardo A. Baeza-Yates, Paolo Boldi, Berthier A. Ribeiro-Neto, and Berkant Barla Cambazoglu, editors, Proceedings of the 2nd International Conference on Web Search and Web Data Mining, WSDM 2009, pages 94–103. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Chris Bizer</author>
<author>Jens Lehmann</author>
<author>Georgi Kobilarov</author>
<author>Richard Cyganiak</author>
<author>Zachary Ives</author>
</authors>
<title>DBpedia: a nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11–15, 2007, Lecture Notes in Computer Science</booktitle>
<pages>4825</pages>
<editor>In Aberer et al., editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="26246" citStr="Auer et al., 2007" startWordPosition="4584" endWordPosition="4587">ien’, despite the lack of a corresponding English article, can be assigned to the WordNet synset for ‘ship’. Using YAGO’s plural heuristic to distinguish classes (Einstein is a physicist) from topic descriptors (Einstein belongs to the topic physics), we determined that over 4.8 million connected components can be linked to WordNet, greatly surpassing the 3.2 million articles covered by the English Wikipedia alone. 5 Related Work A number of projects have used Wikipedia as a database of named entities (Ponzetto and Strube, 2007; Silberer et al., 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1, which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cro</context>
</contexts>
<marker>Auer, Bizer, Lehmann, Kobilarov, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Chris Bizer, Jens Lehmann, Georgi Kobilarov, Richard Cyganiak, and Zachary Ives. 2007. DBpedia: a nucleus for a web of open data. In Aberer et al., editor, The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11–15, 2007, Lecture Notes in Computer Science 4825. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adi Avidor</author>
<author>Michael Langberg</author>
</authors>
<title>The multimultiway cut problem.</title>
<date>2007</date>
<journal>Theoretical Computer Science,</journal>
<pages>377--1</pages>
<contexts>
<context position="17370" citStr="Avidor and Langberg (2007)" startWordPosition="3060" endWordPosition="3063">entirely within the region, and the second one accounts for the edges in C(v, r) to the extent that they are within the radius. The definition of c(S, r) contains an additional slack component that is required for the approximation guarantee proof. Based on these definitions, Algorithm 3.1 uses the LP solution to construct the extended graph. It then repeatedly, as long as there is an unsatisfied assertion Di, chooses a set S of nodes containing one node from each relevant Di,j. Around the nodes in S it simultaneously grows |S |regions with the same radius, a technique previously suggested by Avidor and Langberg (2007). These regions are essentially output components that determine the solution. Repeatedly choosing the radius that minimizes w(C(S,r)) allows us to obc(S,r) tain the approximation guarantee, because the distances in this extended graph are based on the solution of the LP. The properties of this algorithm are given by the following two theorems (proofs in Appendix A). Theorem 2. The algorithm yields a valid WDGS solution (C, U1, ... , Un). Theorem 3. The algorithm yields a solution (C, U1, ... , Un) with an approximation factor of 4ln(nq + 1) with respect to the cost of the optimal WDGS solutio</context>
<context position="27259" citStr="Avidor and Langberg, 2007" startWordPosition="4742" endWordPosition="4745">sternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of correlation clustering (Bansal et al., 2004), where a graph consist1http://www.freebase.com/ 850 Table 2: Examples of separated concepts English concept German concept Explanation (translated) Coffee percolator French Press different types of brewing devices Baqa-Jatt Baqa al-Gharbiyye Baqa-Jatt is a city resulting from a merger of Baqa al-Gharbiyye and Jatt Leucothoe (plant) Leucothea (Orchamos) the second refers to a figure of Greek mythology Old Belarusian language Ruthenian language the second is often considered slightly broader ing of positivel</context>
</contexts>
<marker>Avidor, Langberg, 2007</marker>
<rawString>Adi Avidor and Michael Langberg. 2007. The multimultiway cut problem. Theoretical Computer Science, 377(1-3):35–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Bansal</author>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<date>2004</date>
<booktitle>Correlation clustering. Machine Learning,</booktitle>
<pages>56--1</pages>
<contexts>
<context position="27347" citStr="Bansal et al., 2004" startWordPosition="4758" endWordPosition="4761">ng new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of correlation clustering (Bansal et al., 2004), where a graph consist1http://www.freebase.com/ 850 Table 2: Examples of separated concepts English concept German concept Explanation (translated) Coffee percolator French Press different types of brewing devices Baqa-Jatt Baqa al-Gharbiyye Baqa-Jatt is a city resulting from a merger of Baqa al-Gharbiyye and Jatt Leucothoe (plant) Leucothea (Orchamos) the second refers to a figure of Greek mythology Old Belarusian language Ruthenian language the second is often considered slightly broader ing of positively and negatively labelled similarity edges is clustered such that similar items are grou</context>
</contexts>
<marker>Bansal, Blum, Chawla, 2004</marker>
<rawString>Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004. Correlation clustering. Machine Learning, 56(1-3):89–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Sergio Duarte</author>
<author>Zahurul Islam</author>
</authors>
<title>Cross-lingual alignment and completion of Wikipedia templates.</title>
<date>2009</date>
<booktitle>In CLIAWS3 ’09: Proceedings of the Third International Workshop on Cross Lingual Information Access,</booktitle>
<pages>21--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="26963" citStr="Bouma et al. (2009)" startWordPosition="4697" endWordPosition="4700">c extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of correlation clustering (Bansal et al., 2004), where a graph consist1http://www.freebase.com/ 850 Table 2: Examples of separated concepts English concept German concept Explanation (translated) Coffee percolator French Press different types of brewing devices B</context>
</contexts>
<marker>Bouma, Duarte, Islam, 2009</marker>
<rawString>Gosse Bouma, Sergio Duarte, and Zahurul Islam. 2009. Cross-lingual alignment and completion of Wikipedia templates. In CLIAWS3 ’09: Proceedings of the Third International Workshop on Cross Lingual Information Access, pages 21–29, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
<author>Venkatesan Guruswami</author>
<author>Anthony Wirth</author>
</authors>
<title>Clustering with qualitative information.</title>
<date>2005</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>71</volume>
<issue>3</issue>
<contexts>
<context position="28067" citStr="Charikar et al. (2005)" startWordPosition="4860" endWordPosition="4863">sh concept German concept Explanation (translated) Coffee percolator French Press different types of brewing devices Baqa-Jatt Baqa al-Gharbiyye Baqa-Jatt is a city resulting from a merger of Baqa al-Gharbiyye and Jatt Leucothoe (plant) Leucothea (Orchamos) the second refers to a figure of Greek mythology Old Belarusian language Ruthenian language the second is often considered slightly broader ing of positively and negatively labelled similarity edges is clustered such that similar items are grouped together, however our approach is much more generic than conventional correlation clustering. Charikar et al. (2005) studied a variation of correlation clustering that is similar to WDGS, but since a negative edge would have to be added between each relevant pair of entities in a distinctness assertion, the approximation guarantee would only be O(log(n |V |2)). Minimally invasive repair operations on graphs have also been studied for graph similarity computation (Zeng et al., 2009), where two graphs are provided as input. 6 Conclusions and Future Work We have presented an algorithmic framework for the problem of co-reference that produces consistent partitions by intelligently removing edges or allowing nod</context>
</contexts>
<marker>Charikar, Guruswami, Wirth, 2005</marker>
<rawString>Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. 2005. Clustering with qualitative information. Journal of Computer and System Sciences, 71(3):360–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuchi Chawla</author>
<author>Robert Krauthgamer</author>
<author>Ravi Kumar</author>
<author>Yuval Rabani</author>
<author>D Sivakumar</author>
</authors>
<title>On the hardness of approximating multicut and sparsest-cut. In</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th Annual IEEE Conference on Computational Complexity,</booktitle>
<pages>144--153</pages>
<contexts>
<context position="29757" citStr="Chawla et al. (2005)" startWordPosition="5124" endWordPosition="5127">dges. This can be extended to include mappings from multiple languages to WordNet synsets, with the hope that the weights and link structure will then allow the algorithm to make the final disambiguation decision. Additional scenarios include dealing with co-reference on the Linked Data Web or mappings between thesauri. As such resources are increasingly being linked to Wikipedia and DBpedia, we believe that our techniques will prove useful in making mappings more consistent. A Proofs Proof (Theorem 1). We shall reduce the minimum multicut problem to WDGS. The hardness claims then follow from Chawla et al. (2005). Given a graph G = (V, E) with a positive cost c(e) for each e ∈ E, and a set D = {(si, ti) |i = 1... k} of k demand pairs, our goal is to find a multicut M with respect to D with minimum total cost EeEM c(e). We convert each demand pair (si, ti) into a distinctness assertion Di = ({si}, {ti}) with weight w(Di) = 1+EeEE c(e). An optimal WDGS solution (C, U1, ... , Uk) with cost c then implies a multicut C with the same weight, because each w(Di) &gt; EeEE c(e), so all demand pairs will be satisfied. C is a minimal multicut because any multicut C&apos; with lower cost would imply a valid WDGS solution</context>
</contexts>
<marker>Chawla, Krauthgamer, Kumar, Rabani, Sivakumar, 2005</marker>
<rawString>Shuchi Chawla, Robert Krauthgamer, Ravi Kumar, Yuval Rabani, and D. Sivakumar. 2005. On the hardness of approximating multicut and sparsest-cut. In In Proceedings of the 20th Annual IEEE Conference on Computational Complexity, pages 144–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>Yuqiang Guan</author>
<author>Brian Kulis</author>
</authors>
<title>Weighted graph cuts without eigenvectors. a multilevel approach.</title>
<date>2007</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>29</volume>
<issue>11</issue>
<contexts>
<context position="24446" citStr="Dhillon et al. (2007)" startWordPosition="4296" endWordPosition="4299">linear optimization software to cope with on current hardware. In such cases, the graphs tend to be very sparsely connected, consisting of many smaller, more densely connected subgraphs. We thus investigated graph partitioning heuristics to decompose larger graphs into smaller parts that can more easily be handled with our algorithm. The METIS algorithms (Karypis and Kumar, 1998) can decompose graphs with hundreds of thousands of nodes almost instantly, but favour equally sized clusters over lower cut costs. We obtained partitionings with costs orders of magnitude lower using the heuristic by Dhillon et al. (2007). 4.5 Database of Named Entities The partitioning heuristics allowed us to process all entries in the complete set of Wikipedia dumps and produce a clean output set of connected components where each Wikipedia article or category belongs to a connected component consisting of pages about the same entity or concept. We can regard these connected components as equivalence classes. This means that we obtain a large-scale multilingual database of named entities and their translations. We are also able to more safely transfer information cross-lingually between editions. For example, when an articl</context>
</contexts>
<marker>Dhillon, Guan, Kulis, 2007</marker>
<rawString>Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. 2007. Weighted graph cuts without eigenvectors. a multilevel approach. IEEE Trans. Pattern Anal. Mach. Intell., 29(11):1944–1957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Ferr´andez</author>
<author>Antonio Toral</author>
<author>´Oscar Ferr´andez</author>
<author>Antonio Ferr´andez</author>
<author>Rafael Mu˜noz</author>
</authors>
<title>Applying Wikipedia’s multilingual knowledge to crosslingual question answering.</title>
<date>2007</date>
<booktitle>In NLDB,</booktitle>
<pages>352--363</pages>
<marker>Ferr´andez, Toral, Ferr´andez, Ferr´andez, Mu˜noz, 2007</marker>
<rawString>Sergio Ferr´andez, Antonio Toral, ´Oscar Ferr´andez, Antonio Ferr´andez, and Rafael Mu˜noz. 2007. Applying Wikipedia’s multilingual knowledge to crosslingual question answering. In NLDB, pages 352– 363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naveen Garg</author>
<author>Vijay V Vazirani</author>
<author>Mihalis Yannakakis</author>
</authors>
<title>Approximate max-flow min(multi)cut theorems and their applications.</title>
<date>1996</date>
<journal>SIAM Journal on Computing (SICOMP),</journal>
<pages>25--698</pages>
<contexts>
<context position="27231" citStr="Garg et al., 1996" startWordPosition="4738" endWordPosition="4741">ransliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of correlation clustering (Bansal et al., 2004), where a graph consist1http://www.freebase.com/ 850 Table 2: Examples of separated concepts English concept German concept Explanation (translated) Coffee percolator French Press different types of brewing devices Baqa-Jatt Baqa al-Gharbiyye Baqa-Jatt is a city resulting from a merger of Baqa al-Gharbiyye and Jatt Leucothoe (plant) Leucothea (Orchamos) the second refers to a figure of Greek mythology Old Belarusian language Ruthenian language the second is often considered sligh</context>
<context position="32661" citStr="Garg et al. (1996)" startWordPosition="5737" endWordPosition="5740">u ∈ Di,j, v ∈ Di,k (j =6 k), we obtain d(vi,u, vi,v) = d(vi,u, u) + d(u, v) + d(v, vi,v) ≥ 1. With the maximal distance condition above, this means that vi,u and vi,v cannot be in the same region. Hence u, v cannot be in the same region, unless the edge from vi,u to u is cut (in which case u will be placed in Ui) or the edge from v to vi,v is cut (in which case v will be placed in Ui). Since each region is separated from other regions via C, we obtain that ∀i, j, k =6 j, u, v: u ∈ Di,j \ Ui, v ∈ Di,k \ Ui implies P(u, v, E \ C) = ∅, so a valid solution is obtained. Lemma 5 (essentially due to Garg et al. (1996)). For any i where ∃j, k &gt; j, u ∈ Di,j, v ∈ Di,k : P(vi,u, vi,v, E0) =6 ∅ and w(Di) &gt; 0, there exists an r such that w(C(S, r)) ≤ 2ln(nq + 1) ˆc(S, r), 0 ≤ r &lt; 21 for any set S consisting of vi,v nodes. Proof. Define w(S, r) = Ev∈S w(C(v, r)). We will prove that there exists an appropriate r with w(C(S, r)) ≤ w(S, r) ≤ 2ln(nq+1) ˆc(S, r). Assume, for reductio ad absurdum, that ∀r ∈ [0, 12) : w(S, r) &gt; 2ln(nq + 1)ˆc(S, r). As we expand the radius r, we note that ˆc(S, r) ddr = w(S, r) whereever cˆ is differentiable with respect to r. There are only a finite number of points r1,. . . ,rl−1 in (0</context>
</contexts>
<marker>Garg, Vazirani, Yannakakis, 1996</marker>
<rawString>Naveen Garg, Vijay V. Vazirani, and Mihalis Yannakakis. 1996. Approximate max-flow min(multi)cut theorems and their applications. SIAM Journal on Computing (SICOMP), 25:698–707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narendra Karmarkar</author>
</authors>
<title>A new polynomial-time algorithm for linear programming.</title>
<date>1984</date>
<booktitle>In STOC ’84: Proceedings of the 16th Annual ACM Symposium on Theory of Computing,</booktitle>
<pages>302--311</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Karmarkar, 1984</marker>
<rawString>Narendra Karmarkar. 1984. A new polynomial-time algorithm for linear programming. In STOC ’84: Proceedings of the 16th Annual ACM Symposium on Theory of Computing, pages 302–311, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Karypis</author>
<author>Vipin Kumar</author>
</authors>
<title>A fast and high quality multilevel scheme for partitioning irregular graphs.</title>
<date>1998</date>
<journal>SIAM Journal on Scientific Computing,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="24207" citStr="Karypis and Kumar, 1998" startWordPosition="4256" endWordPosition="4259">t or second concept, or the broader generalization of the two concepts becomes a separate, more general connected component. 4.4 Large Problem Instances When problem instances become too large, the linear programs can become too unwieldy for linear optimization software to cope with on current hardware. In such cases, the graphs tend to be very sparsely connected, consisting of many smaller, more densely connected subgraphs. We thus investigated graph partitioning heuristics to decompose larger graphs into smaller parts that can more easily be handled with our algorithm. The METIS algorithms (Karypis and Kumar, 1998) can decompose graphs with hundreds of thousands of nodes almost instantly, but favour equally sized clusters over lower cut costs. We obtained partitionings with costs orders of magnitude lower using the heuristic by Dhillon et al. (2007). 4.5 Database of Named Entities The partitioning heuristics allowed us to process all entries in the complete set of Wikipedia dumps and produce a clean output set of connected components where each Wikipedia article or category belongs to a connected component consisting of pages about the same entity or concept. We can regard these connected components as </context>
</contexts>
<marker>Karypis, Kumar, 1998</marker>
<rawString>George Karypis and Vipin Kumar. 1998. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM Journal on Scientific Computing, 20(1):359–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhash Khot</author>
</authors>
<title>On the power of unique 2-prover 1-round games.</title>
<date>2002</date>
<booktitle>In STOC ’02: Proceedings of the 34th Annual ACM Symposium on Theory of Computing,</booktitle>
<pages>767--775</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10761" citStr="Khot, 2002" startWordPosition="1811" endWordPosition="1812">r the distinctness assertions. The (total) cost of a WDGS solution S = (C, U1, ... , Un) is then defined as c(S) = c(C, U1, ... , Un) n = w(e) + |Ui |w(Di) eEC i=1 Definition 4. (WDGS). A WDGS problem instance P consists of a graph G = (V, E) with edge weights w(e) and n distinctness assertions D1, ..., Dn with weights w(Di). The objective consists in finding a solution (C, U1, ... , Un) with minimal cost c(C, U1, ... , Un). It turns out that finding optimal solutions efficiently is a hard problem (proofs in Appendix A). Theorem 1. WDGS is NP-hard and APX-hard. If the Unique Games Conjecture (Khot, 2002) holds, then it is NP-hard to approximate WDGS within any constant factor α &gt; 0. 3 Approximation Algorithm Due to the hardness of WDGS, we devise a polynomial-time approximation algorithm with an approximation factor of 4ln(nq + 1) where n is the number of distinctness assertions and q = max |Di,j|. This means that for all problem ini,j stances P, we can guarantee c(S(P)) &lt; 4ln(nq + 1), c(S (P)) — where S(P) is the solution determined by our algorithm, and S*(P) is an optimal solution. Note that this approximation guarantee is independent of how long each Di is, and that it merely represents a</context>
</contexts>
<marker>Khot, 2002</marker>
<rawString>Subhash Khot. 2002. On the power of unique 2-prover 1-round games. In STOC ’02: Proceedings of the 34th Annual ACM Symposium on Theory of Computing, pages 767–775, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Leighton</author>
<author>Satish Rao</author>
</authors>
<title>Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms.</title>
<date>1999</date>
<journal>Journal of the ACM,</journal>
<volume>46</volume>
<issue>6</issue>
<contexts>
<context position="12265" citStr="Leighton and Rao (1999)" startWordPosition="2077" endWordPosition="2080">e cut and which nodes should most likely be removed from distinctness assertions. Note that this is a continuous LP, not an integer linear program (ILP); the latter would not be tractable due to the large number of variables and constraints of the problem. After solving the linear program, a new – extended – graph is constructed and the optimal LP solution is used to define a distance metric on it. The final solution is obtained by smartly selecting regions in this extended graph as the individual output components, employing a region 846 growing technique in the spirit of the seminal work by Leighton and Rao (1999). Edges that cross the boundaries of these regions are cut. Definition 5. Given a WDGS instance, we define a linear program of the following form: minimize ui,vw(Di) subject to pi,j,v = ui,v ∀i, j&lt;li, v ∈ Di,j (1) pi,j,v + ui,v ≥ 1 ∀i, j&lt;li, v ∈ U Di,k (2) k&gt;j pi,j,v ≤ pi,j,u + de ∀i, j&lt;li, e=(u,v) ∈ E (3) de ≥ 0 ∀e ∈ E ui,v ≥ 0 ∀i, v ∈ pi,j,v ≥ 0 ∀i, j&lt;li, v∈V (6) The LP uses decision variables de and ui,v, and auxiliary variables pi,j,v that we refer to as potential variables. The de variables indicate whether (in the continuous LP: to what degree) an edge e should be deleted, and the ui,v v</context>
<context position="27212" citStr="Leighton and Rao, 1999" startWordPosition="4734" endWordPosition="4737">s well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of correlation clustering (Bansal et al., 2004), where a graph consist1http://www.freebase.com/ 850 Table 2: Examples of separated concepts English concept German concept Explanation (translated) Coffee percolator French Press different types of brewing devices Baqa-Jatt Baqa al-Gharbiyye Baqa-Jatt is a city resulting from a merger of Baqa al-Gharbiyye and Jatt Leucothoe (plant) Leucothea (Orchamos) the second refers to a figure of Greek mythology Old Belarusian language Ruthenian language the second is oft</context>
</contexts>
<marker>Leighton, Rao, 1999</marker>
<rawString>Tom Leighton and Satish Rao. 1999. Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms. Journal of the ACM, 46(6):787–832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify!: Linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th ACM Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>233--242</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="26705" citStr="Mihalcea and Csomai (2007)" startWordPosition="4654" endWordPosition="4657">cts have used Wikipedia as a database of named entities (Ponzetto and Strube, 2007; Silberer et al., 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1, which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of co</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify!: Linking documents to encyclopedic knowledge. In Proceedings of the 16th ACM Conference on Information and Knowledge Management (CIKM 2007), pages 233–242, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nguyen</author>
<author>A Overwijk</author>
<author>C Hauff</author>
<author>R B Trieschnigg</author>
<author>D Hiemstra</author>
<author>F M G Jong de</author>
</authors>
<title>WikiTranslate: query translation for cross-lingual information retrieval using only Wikipedia.</title>
<date>2009</date>
<booktitle>Evaluating Systems for Multilingual and Multimodal Information Access, Lecture Notes in Computer Science 5706,</booktitle>
<pages>58--65</pages>
<editor>In Carol Peters, Thomas Deselaers, Nicola Ferro, and Julio Gonzalo, editors,</editor>
<contexts>
<context position="26542" citStr="Nguyen et al., 2009" startWordPosition="4631" endWordPosition="4634">ted components can be linked to WordNet, greatly surpassing the 3.2 million articles covered by the English Wikipedia alone. 5 Related Work A number of projects have used Wikipedia as a database of named entities (Ponzetto and Strube, 2007; Silberer et al., 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1, which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas </context>
</contexts>
<marker>Nguyen, Overwijk, Hauff, Trieschnigg, Hiemstra, de, 2009</marker>
<rawString>D. Nguyen, A. Overwijk, C. Hauff, R.B. Trieschnigg, D. Hiemstra, and F.M.G. Jong de. 2009. WikiTranslate: query translation for cross-lingual information retrieval using only Wikipedia. In Carol Peters, Thomas Deselaers, Nicola Ferro, and Julio Gonzalo, editors, Evaluating Systems for Multilingual and Multimodal Information Access, Lecture Notes in Computer Science 5706, pages 58–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Pasternack</author>
<author>Dan Roth</author>
</authors>
<title>Learning better transliterations.</title>
<date>2009</date>
<booktitle>In CIKM ’09: Proceeding of the 18th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>177--186</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="26657" citStr="Pasternack and Roth, 2009" startWordPosition="4647" endWordPosition="4650">ikipedia alone. 5 Related Work A number of projects have used Wikipedia as a database of named entities (Ponzetto and Strube, 2007; Silberer et al., 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1, which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 200</context>
</contexts>
<marker>Pasternack, Roth, 2009</marker>
<rawString>Jeff Pasternack and Dan Roth. 2009. Learning better transliterations. In CIKM ’09: Proceeding of the 18th ACM Conference on Information and Knowledge Management, pages 177–186, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Deriving a large scale taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>In AAAI 2007: Proceedings of the 22nd Conference on Artificial Intelligence,</booktitle>
<pages>1440--1445</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="26161" citStr="Ponzetto and Strube, 2007" startWordPosition="4569" endWordPosition="4572">egories has an English page. So, the French Wikipedia article on the Dutch schooner ‘JR Tolkien’, despite the lack of a corresponding English article, can be assigned to the WordNet synset for ‘ship’. Using YAGO’s plural heuristic to distinguish classes (Einstein is a physicist) from topic descriptors (Einstein belongs to the topic physics), we determined that over 4.8 million connected components can be linked to WordNet, greatly surpassing the 3.2 million articles covered by the English Wikipedia alone. 5 Related Work A number of projects have used Wikipedia as a database of named entities (Ponzetto and Strube, 2007; Silberer et al., 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1, which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single editi</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007. Deriving a large scale taxonomy from Wikipedia. In AAAI 2007: Proceedings of the 22nd Conference on Artificial Intelligence, pages 1440–1445. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Wolodja Wentland</author>
<author>Johannes Knopp</author>
<author>Matthias Hartung</author>
</authors>
<title>Building a multilingual lexical resource for named entity disambiguation, translation and transliteration.</title>
<date>2008</date>
<booktitle>In European, editor, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="26185" citStr="Silberer et al., 2008" startWordPosition="4573" endWordPosition="4576">. So, the French Wikipedia article on the Dutch schooner ‘JR Tolkien’, despite the lack of a corresponding English article, can be assigned to the WordNet synset for ‘ship’. Using YAGO’s plural heuristic to distinguish classes (Einstein is a physicist) from topic descriptors (Einstein belongs to the topic physics), we determined that over 4.8 million connected components can be linked to WordNet, greatly surpassing the 3.2 million articles covered by the English Wikipedia alone. 5 Related Work A number of projects have used Wikipedia as a database of named entities (Ponzetto and Strube, 2007; Silberer et al., 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1, which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg an</context>
</contexts>
<marker>Silberer, Wentland, Knopp, Hartung, 2008</marker>
<rawString>Carina Silberer, Wolodja Wentland, Johannes Knopp, and Matthias Hartung. 2008. Building a multilingual lexical resource for named entity disambiguation, translation and transliteration. In European, editor, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Sorg</author>
<author>Philipp Cimiano</author>
</authors>
<title>Enriching the crosslingual link structure of Wikipedia - a classification-based approach.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI 2008 Workshop on Wikipedia and Artifical Intelligence.</booktitle>
<contexts>
<context position="26801" citStr="Sorg and Cimiano (2008)" startWordPosition="4670" endWordPosition="4673">, 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1, which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of correlation clustering (Bansal et al., 2004), where a graph consist1http://www.freebase.com/ 850 T</context>
</contexts>
<marker>Sorg, Cimiano, 2008</marker>
<rawString>Philipp Sorg and Philipp Cimiano. 2008. Enriching the crosslingual link structure of Wikipedia - a classification-based approach. In Proceedings of the AAAI 2008 Workshop on Wikipedia and Artifical Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A Core of Semantic Knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International World Wide Web conference, WWW,</booktitle>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="26390" citStr="Suchanek et al., 2007" startWordPosition="4609" endWordPosition="4612">o distinguish classes (Einstein is a physicist) from topic descriptors (Einstein belongs to the topic physics), we determined that over 4.8 million connected components can be linked to WordNet, greatly surpassing the 3.2 million articles covered by the English Wikipedia alone. 5 Related Work A number of projects have used Wikipedia as a database of named entities (Ponzetto and Strube, 2007; Silberer et al., 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1, which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual lin</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowledge. In Proceedings of the 16th International World Wide Web conference, WWW, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiping Zeng</author>
<author>Anthony K H Tung</author>
<author>Jianyong Wang</author>
<author>Jianhua Feng</author>
<author>Lizhu Zhou</author>
</authors>
<title>Comparing stars: On approximating graph edit distance.</title>
<date>2009</date>
<booktitle>Proceedings of the VLDB Endowment,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="28437" citStr="Zeng et al., 2009" startWordPosition="4921" endWordPosition="4924">idered slightly broader ing of positively and negatively labelled similarity edges is clustered such that similar items are grouped together, however our approach is much more generic than conventional correlation clustering. Charikar et al. (2005) studied a variation of correlation clustering that is similar to WDGS, but since a negative edge would have to be added between each relevant pair of entities in a distinctness assertion, the approximation guarantee would only be O(log(n |V |2)). Minimally invasive repair operations on graphs have also been studied for graph similarity computation (Zeng et al., 2009), where two graphs are provided as input. 6 Conclusions and Future Work We have presented an algorithmic framework for the problem of co-reference that produces consistent partitions by intelligently removing edges or allowing nodes to remain connected. This algorithm has successfully been applied to Wikipedia’s cross-lingual graph, where we identified and eliminated surprisingly large numbers of inaccurate connections, leading to a large-scale multilingual register of names. In future work, we would like to investigate how our algorithm behaves in extended settings, e.g. we can use heuristics</context>
</contexts>
<marker>Zeng, Tung, Wang, Feng, Zhou, 2009</marker>
<rawString>Zhiping Zeng, Anthony K. H. Tung, Jianyong Wang, Jianhua Feng, and Lizhu Zhou. 2009. Comparing stars: On approximating graph edit distance. Proceedings of the VLDB Endowment, 2(1):25–36.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>