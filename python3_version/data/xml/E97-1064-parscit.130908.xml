<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000250">
<title confidence="0.997108">
A Structured Language Model
</title>
<author confidence="0.991272">
Ciprian Chelba
</author>
<affiliation confidence="0.673966333333333">
The Johns Hopkins University
CLSP, Barton Hall 320
3400 N. Charles Street, Baltimore, MD-21218
</affiliation>
<email confidence="0.944104">
chelbAjhu.edu
</email>
<sectionHeader confidence="0.993962" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999904454545455">
The paper presents a language model that
develops syntactic structure and uses it to
extract meaningful information from the
word history, thus enabling the use of
long distance dependencies. The model as-
signs probability to every joint sequence
of words-binary-parse-structure with head-
word annotation. The model, its proba-
bilistic parametrization, and a set of ex-
periments meant to evaluate its predictive
power are presented.
</bodyText>
<figureCaption confidence="0.8391235">
the dog I heard yesterday barked
Figure 1: Partial parse
</figureCaption>
<figure confidence="0.955353">
b_1.11 h_O
w_q w_r w_( r+1) . w_k w_{ k-t-1) w_n &lt;/s&gt;
</figure>
<figureCaption confidence="0.993261">
Figure 2: A word-parse k-prefix
</figureCaption>
<sectionHeader confidence="0.995876" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956333333333">
The main goal of the proposed project is to develop
a language model(LM) that uses syntactic structure.
The principles that guided this propogal were:
</bodyText>
<listItem confidence="0.9874274">
• the model will develop syntactic knowledge as a
built-in feature; it will assign a probability to every
joint sequence of words-binary-parse-structure;
• the model should operate in a left-to-right man-
ner so that it would be possible to decode word lat-
</listItem>
<bodyText confidence="0.967585">
tices provided by an automatic speech recognizer.
The model consists of two modules: a next word
predictor which makes use of syntactic structure as
developed by a parser. The operations of these two
modules are intertwined.
</bodyText>
<sectionHeader confidence="0.814551" genericHeader="method">
2 The Basic Idea and Terminology
</sectionHeader>
<bodyText confidence="0.9975236">
Consider predicting the word barked in the sen-
tence:
the dog I heard yesterday barked again.
A 3-gram approach would predict barked from
(heard, yesterday) whereas it is clear that the
predictor should use the word dog which is out-
side the reach of even 4-grams. Our assumption
is that what enables us to make a good predic-
tion of barked is the syntactic structure in the
past. The correct partial parse of the word his-
tory when predicting barked is shown in Figure 1.
The word dog is called the headword of the con-
stituent ( the (dog (...))) and dog is an exposed
headword when predicting barked — topmost head-
word in the largest constituent that contains it. The
syntactic structure in the past filters out irrelevant
words and points to the important ones, thus en-
abling the use of long distance information when
predicting the next word. Our model will assign a
probability P(W,T) to every sentence W with ev-
ery possible binary branching parse T and every
possible headword annotation for every constituent
of T. Let W be a sentence of length 1 words to
which we have prepended &lt;s&gt; and appended &lt;/s&gt;
so that wo =&lt;s&gt; and w1+1 =&lt;/s&gt;. Let Wk be the
word k-prefix wo wk of the sentence and Wk Tk
the word-parse k-prefix. To stress this point, a
word-parse k-prefix contains only those binary trees
whose span is completely included in the word k-
prefix, excluding wo =&lt;s&gt;. Single words can be re-
garded as root-only trees. Figure 2 shows a word-
parse k-prefix; h_O h_{-m} are the exposed head-
words. A complete parse — Figure 3 — is any bi-
nary parse of the wi &lt;/s&gt; sequence with the
restriction that &lt;/s&gt; is the only allowed headword.
</bodyText>
<page confidence="0.98852">
498
</page>
<figure confidence="0.998658333333333">
Wm&gt; h_(-2) h_1-1) h_O
T_(-m)
&lt;s&gt;
</figure>
<figureCaption confidence="0.999743">
Figure 3: Complete parse Figure 4: Before an adjoin operation
</figureCaption>
<bodyText confidence="0.98176175">
Note that (wi wi) needn&apos;t be a constituent, but
for the parses where it is, there is no restriction on
which of its words is the headword.
The model will operate by means of two modules:
</bodyText>
<listItem confidence="0.968769625">
• PREDICTOR predicts the next word wk+1 given
the word-parse k-prefix and then passes control to
the PARSER;
• PARSER grows the already existing binary
branching structure by repeatedly generating the
transitions adjoin-left or adjoin-right until it
passes control to the PREDICTOR by taking a null
transition.
</listItem>
<bodyText confidence="0.999961142857143">
The operations performed by the PARSER en-
sure that all possible binary branching parses with
all possible headword assignments for the wi • • • wk
word sequence can be generated. They are illus-
trated by Figures 4-6. The following algorithm de-
scribes how the model generates a word sequence
with a complete parse (see Figures 3-6 for notation):
</bodyText>
<equation confidence="0.676793">
Transition t; // a PARSER transition
generate &lt;s›;
do{
predict next_word; //PREDICTOR
do{ //PARSER
if(T_{-1} != &lt;s&gt; )
if(h_O == &lt;/s&gt;) t = adjoin-right;
else t = {adjoin-fleft,rightl, null};
else t = null;
}while(t != null)
}while( ! (h_O == &lt;/s&gt; &amp;et T_{-1} == &lt;s&gt;))
t = adjoin-right; // adjoin &lt;s&gt;; DONE
</equation>
<bodyText confidence="0.999324666666667">
It is easy to see that any given word sequence with a
possible parse and headword annotation is generated
by a unique sequence of model actions.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="method">
3 Probabilistic Model
</sectionHeader>
<bodyText confidence="0.902361">
The probability P(W,T) can be broken into:
</bodyText>
<equation confidence="0.943103333333333">
P(W,T) = Iliktii[P(wkIWk-171-1)•
Nk
P(tiklWk7Wk-171-1,4 ...t11_1)] where:
</equation>
<listItem confidence="0.999342">
• Wk_iTk-i is the word-parse (k - 1)-prefix
• wk is the word predicted by PREDICTOR
• Nk - 1 is the number of adjoin operations the
PARSER executes before passing control to the
PREDICTOR (the Nk-th operation at position k is
the null transition); Nk is a function of T
</listItem>
<figure confidence="0.898645">
&lt;s&gt;
</figure>
<figureCaption confidence="0.9999925">
Figure 5: Result of adjoin-left
Figure 6: Result of adjoin-right
</figureCaption>
<listItem confidence="0.9919775">
• t!` denotes the i-th PARSER operation carried
out at position k in the word string;
</listItem>
<equation confidence="0.9906486">
E {adj t,adjoin-right }, i &lt; Nk
=null,i = Nk
Our model is based on two probabilities:
P(Wk /Wk-111-1) (1)
P(4 IWk,Wk-111-1,4 • • •til 1) (2)
</equation>
<subsectionHeader confidence="0.736035">
As can be seen (Wky Wk-121-1, tt_1) is one
</subsectionHeader>
<bodyText confidence="0.9997002">
of the Nk word-parse k-prefixes of WkTk, i = 1, Nk
at position k in the sentence.
To ensure a proper probabilistic model we have
to make sure that (1) and (2) are well defined con-
ditional probabilities and that the model halts with
</bodyText>
<listItem confidence="0.947711428571428">
probability one. A few provisions need to be taken:
• P(nu11/WkTk) = 1, if T_{-1} == &lt;s&gt; ensures
that &lt;s&gt; is adjoined in the last step of the parsing
process;
• P(adjoin-right/WkTk) = 1, if h_O == &lt;/s&gt;
ensures that the headword of a complete parse is
&lt;/s›;
</listItem>
<equation confidence="0.452721">
.3e &gt; Os.t. P(wk=&lt;/s&gt;/W T 1 &gt; V
</equation>
<bodyText confidence="0.852856">
- k-1— k-1, f, — W k—Tl—k-1
ensures that the model halts with probability one.
</bodyText>
<subsectionHeader confidence="0.993368">
3.1 The first model
</subsectionHeader>
<bodyText confidence="0.998367571428571">
The first term (1) can be reduced to an n-gram LM,
P(wk/Wk-iTk-i) = P(wk/wk--1 • • • wk-n+1).
A simple alternative to this degenerate approach
would be to build a model which predicts the next
word based on the preceding p-1 exposed headwords
and n-1 words in the history, thus making the fol-
lowing equivalence classification:
</bodyText>
<equation confidence="0.936433">
[WkTk] = {h_O h_{-p+21,wk-i••wk-n+1}•
</equation>
<page confidence="0.990341">
499
</page>
<bodyText confidence="0.999937">
The approach is similar to the trigger LM(Lau93),
the difference being that in the present work triggers
are identified using the syntactic structure.
</bodyText>
<subsectionHeader confidence="0.999168">
3.2 The second model
</subsectionHeader>
<bodyText confidence="0.999988470588236">
Model (2) assigns probability to different binary
parses of the word k-prefix by chaining the ele-
mentary operations described above. The workings
of the PARSER are very similar to those of Spat-
ter (Jelinek94). It can be brought to the full power
of Spatter by changing the action of the adjoin
operation so that it takes into account the termi-
nal/nonterminal labels of the constituent proposed
by adjoin and it also predicts the nonterminal la-
bel of the newly created constituent; PREDICTOR
will now predict the next word along with its POS
tag. The best equivalence classification of the WkTk
word-parse k-prefix is yet to be determined. The
Collins parser (Collins96) shows that dependency-
grammar-like bigram constraints may be the most
adequate, so the equivalence classification [WkTk]
should contain at least {11_0, h_{-1}}.
</bodyText>
<sectionHeader confidence="0.98159" genericHeader="method">
4 Preliminary Experiments
</sectionHeader>
<bodyText confidence="0.744371666666667">
Assuming that the correct partial parse is a func-
tion of the word prefix, it makes sense to compare
the word level perplexity(PP) of a standard n-gram
LM with that of the P(wk/Wk_171-1) model. We
developed and evaluated four LMs:
• 2 bigram LMs P(wk/Wk-111-1) = P(wk/wk-i)
referred to as W and w, respectively; wk-i is the pre-
vious (word, POStag) pair;
• 2 P(wk/Wk_ITk_i) = P(wk/h0) models, re-
ferred to as H and h, respectively; ho is the previous
exposed (headword, POS/non-term tag) pair; the
passes used in this model were those assigned man-
ually in the Penn Treebank (Marcus95) after under-
going headword percolation and binasization.
All four LMs predict a word wk and they were
implemented using the Maximum Entropy Model-
ing Toolkit&apos; (Ristad97). The constraint templates
in the {W,H} models were:
</bodyText>
<equation confidence="0.700496">
4 &lt;= &lt;*&gt;_&lt;*&gt; &lt;?&gt;; 2 &lt;= &lt;?&gt;_&lt;*&gt; &lt;7&gt;;
2 &lt;= &lt;?&gt;_&lt;?&gt; &lt;?&gt;; 8 &lt;= &lt;*&gt;_&lt;7&gt; &lt;?&gt;;
</equation>
<bodyText confidence="0.960438">
and in the {w,h} models they were:
</bodyText>
<sectionHeader confidence="0.614847" genericHeader="method">
4 &lt;= &lt;*&gt;_&lt;*&gt; &lt;7&gt;; 2 &lt;= &lt;?&gt;_&lt;*&gt; &lt;7&gt;;
</sectionHeader>
<bodyText confidence="0.982403">
&lt;*&gt; denotes a don&apos;t care position, &lt;?&gt;_&lt;?&gt; a (word,
tag) pair; for example, 4 &lt;= &lt;?&gt;_&lt;*&gt; &lt;?&gt; will trig-
ger on all ((word, any tag), predicted-word) pairs
that occur more than 3 times in the training data.
The sentence boundary is not included in the PP cal-
culation. Table 1 shows the PP results along with
iftp://ftp.cs.princeton.edu/pub/packages/memt
the number of parameters for each of the 4 models
described .
</bodyText>
<table confidence="0.821193666666667">
LM I PP pararn LM I PP param
W 376 208487 w 419 103732
H 312 206540 h 410 102437
</table>
<tableCaption confidence="0.997747">
Table 1: Perplexity results
</tableCaption>
<sectionHeader confidence="0.996559" genericHeader="conclusions">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997882">
The author thanks to Frederick Jelinek, Sanjeev
Khudanpur, Eric Ristad and all the other members
of the Dependency Modeling Group (Stolcke97),
WS96 DoD Workshop at the Johns Hopkins Uni-
versity.
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998475">
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Pro-
ceedings of the 34th Annual Meeting of the As-
sociation for Computational Linguistics, 184-191,
Santa Cruz, CA.
Frederick Jelinek. 1997. Information extraction from
speech and text — course notes. The Johns Hop-
kins University, Baltimore, MD.
Frederick Jelinek, John Lafferty, David M. Mager-
man, Robert Mercer, Adwait Ratnaparkhi, Salim
Roukos. 1994. Decision Tree Parsing using a Hid-
den Derivational Model. In Proceedings of the
Human Language Technology Workshop, 272-277.
ARPA.
Raymond Lau, Ronald Rosenfeld, and Salim
Roukos. 1993. Trigger-based language models: a
maximum entropy approach. In Proceedings of the
IEEE Conference on Acoustics, Speech, and Sig-
nal Processing, volume 2, 45-48, Minneapolis.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz. 1995. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313-330.
Eric Sven Ristad. 1997. Maximum entropy model-
ing toolkit. Technical report, Department of Com-
puter Science, Princeton University, Princeton,
NJ, January 1997, v. 1.4 Beta.
Andreas Stolcke, Ciprian Chelba, David Engle,
Frederick Jelinek, Victor Jimenez, Sanjeev Khu-
danpur, Lidia Mangu, Harry Printz, Eric Sven
Ristad, Roni Rosenfeld, Delcai Wu. 1997. Struc-
ture and Performance of a Dependency Language
Model. In Proceedings of Eurospeech&apos;97, Rhodes,
Greece. To appeal..
</reference>
<page confidence="0.995609">
500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.777974">
<title confidence="0.99989">A Structured Language Model</title>
<author confidence="0.990058">Ciprian Chelba</author>
<affiliation confidence="0.999745">The Johns Hopkins University</affiliation>
<address confidence="0.9868865">CLSP, Barton Hall 320 3400 N. Charles Street, Baltimore, MD-21218</address>
<email confidence="0.999715">chelbAjhu.edu</email>
<abstract confidence="0.9948289375">The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation. The model, its probabilistic parametrization, and a set of experiments meant to evaluate its predictive power are presented. the dog I heard yesterday barked Figure 1: Partial parse h_O w_q w_r w_( r+1) . w_k w_{ k-t-1) w_n &lt;/s&gt;</abstract>
<intro confidence="0.820363">Figure 2: A word-parse k-prefix</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<location>Santa Cruz, CA.</location>
<marker>Collins, 1996</marker>
<rawString>Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, 184-191, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Information extraction from speech and text — course notes. The Johns Hopkins University,</title>
<date>1997</date>
<location>Baltimore, MD.</location>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Information extraction from speech and text — course notes. The Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John Lafferty</author>
<author>David M Magerman</author>
<author>Robert Mercer</author>
</authors>
<title>Adwait Ratnaparkhi, Salim Roukos.</title>
<date>1994</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<pages>272--277</pages>
<publisher>ARPA.</publisher>
<marker>Jelinek, Lafferty, Magerman, Mercer, 1994</marker>
<rawString>Frederick Jelinek, John Lafferty, David M. Magerman, Robert Mercer, Adwait Ratnaparkhi, Salim Roukos. 1994. Decision Tree Parsing using a Hidden Derivational Model. In Proceedings of the Human Language Technology Workshop, 272-277. ARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Lau</author>
<author>Ronald Rosenfeld</author>
<author>Salim Roukos</author>
</authors>
<title>Trigger-based language models: a maximum entropy approach.</title>
<date>1993</date>
<booktitle>In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>2</volume>
<pages>45--48</pages>
<location>Minneapolis.</location>
<marker>Lau, Rosenfeld, Roukos, 1993</marker>
<rawString>Raymond Lau, Ronald Rosenfeld, and Salim Roukos. 1993. Trigger-based language models: a maximum entropy approach. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, volume 2, 45-48, Minneapolis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1995</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz. 1995. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
</authors>
<title>Maximum entropy modeling toolkit.</title>
<date>1997</date>
<tech>Technical report,</tech>
<pages>1--4</pages>
<institution>Department of Computer Science, Princeton University,</institution>
<location>Princeton, NJ,</location>
<marker>Ristad, 1997</marker>
<rawString>Eric Sven Ristad. 1997. Maximum entropy modeling toolkit. Technical report, Department of Computer Science, Princeton University, Princeton, NJ, January 1997, v. 1.4 Beta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Ciprian Chelba</author>
<author>David Engle</author>
<author>Frederick Jelinek</author>
<author>Victor Jimenez</author>
</authors>
<title>Sanjeev Khudanpur, Lidia Mangu, Harry Printz, Eric Sven Ristad,</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech&apos;97,</booktitle>
<location>Roni Rosenfeld, Delcai Wu.</location>
<note>To appeal..</note>
<marker>Stolcke, Chelba, Engle, Jelinek, Jimenez, 1997</marker>
<rawString>Andreas Stolcke, Ciprian Chelba, David Engle, Frederick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry Printz, Eric Sven Ristad, Roni Rosenfeld, Delcai Wu. 1997. Structure and Performance of a Dependency Language Model. In Proceedings of Eurospeech&apos;97, Rhodes, Greece. To appeal..</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>