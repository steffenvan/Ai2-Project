<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000536">
<title confidence="0.989702">
Learning Local Content Shift Detectors from Document-level Information
</title>
<author confidence="0.997391">
Rich´ard Farkas
</author>
<affiliation confidence="0.9974135">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.992719">
farkas@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.995254" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998921947368421">
Information-oriented document labeling is a
special document multi-labeling task where
the target labels refer to a specific information
instead of the topic of the whole document.
These kind of tasks are usually solved by look-
ing up indicator phrases and analyzing their
local context to filter false positive matches.
Here, we introduce an approach for machine
learning local content shifters which detects
irrelevant local contexts using just the origi-
nal document-level training labels. We handle
content shifters in general, instead of learn-
ing a particular language phenomenon detec-
tor (e.g. negation or hedging) and form a sin-
gle system for document labeling and content
shift detection. Our empirical results achieved
24% error reduction – compared to supervised
baseline methods – on three document label-
ing tasks.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968255319149">
There are special document multi-labeling tasks
where the target labels refer to a specific piece of
information extractable from the document instead
of the overall topic of the document. In these kinds
of tasks the target information is usually an attribute
or relation related to the target entity (usually a per-
son or an organisation) of the document in question,
but the task is to assign class labels at the document
(entity) level. For example, the smoking habits of
the patients are frequently discussed in the textual
parts of clinical notes (Uzuner et al., 2008). In this
case the task is to find specific information in the
text – i.e. the patient in question is a smoker, past
smoker, non-smoker – but at the end an applica-
tion has to assign labels to the documents(patients).
Similarly, the soccer club names where a sportsman
played for are document(sportman)-level labels in
Wikipedia articles expressed by the Wikipedia cat-
egories. The target information in these tasks is
usually just mentioned in the document and much
of the document is irrelevant for this information
request in contrast to standard document classifi-
cation tasks where the goal is to identify the top-
ics of the whole document. On the other hand,
they are not a standard information extraction task
as the task is to assign class labels to documents,
and the training dataset contains labels just at this
level. These special tasks lie somewhere between
information extraction and document classification
and require special approaches to solve them. We
will call them Information-oriented document label-
ing throughout this paper. There are several appli-
cation areas where information-oriented document
labels are naturally present in an enormous amount
like clinical records, Wikipedia categories and user-
generated tags of news.
Previous evaluation campaigns (Uzuner et al.,
2008; Pestian et al., 2007; Uzuner, 2009) demon-
strated that information-oriented document labeling
can be effectively performed by looking up indicator
phrases which can be gathered by hand, by corpus
statistics or in a hybrid way. However these cam-
paigns also highlighted that the analysis of the local
context of the indicator phrases is crucial. For in-
stance, in the smoking habit detection task there are
a few indicator words (e.g. smokes, cigarette) and
the local context of their occurrences in texts should
</bodyText>
<page confidence="0.979864">
759
</page>
<note confidence="0.9579955">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 759–770,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999037166666667">
be analysed to see whether their semantic was rad-
ically changed (e.g. they are negated or in a past
tense), for instance:
The patient has a 20 pack-year smoking
history.
The patient denies any smoking history.
He has a greater than 100 pack year
smoking history and quit 9 to 10 years
ago.
We propose a simple but efficient approach for
information-oriented document labeling tasks by ad-
dressing the automatic detection of language phe-
nomena for a particular task which alters the sense
or information content of the indicator phrase’s oc-
currences. For example, they may be logical modi-
fiers (e.g. negation) or modal modifiers (e.g. auxil-
iaries like might and can); they may refer to a subject
which differs from the target entity of the task (e.g.
clinical notes usually contain information about the
family history of the patient); or the semantic con-
tent of the shifter may change the role of the tar-
get span of a text (e.g. a sportsman can play for or
against a particular team). We call these phenom-
ena content shifters and the task of identifying them
content shift detection (CSD).
Existing CSD approaches focus on a particular
class of language phenomena (especially negation
or hedging) and use hand-crafted rules (Chapman et
al., 2007) or a supervised learning approach that ex-
ploits corpora manually annotated at the token-level
for a particular type of content shifter (Morante et
al., 2009). Moreover higher level applications (like
document labeling and information extraction) use a
separate CSD module which is developed indepen-
dently from the target task. We argue that the nature
of content shifters is domain and task dependent, so
training corpora (at the token-level) are required for
content shifters which are important for a particular
task but the construction of such training corpora is
expensive. Here, we propose an alternative approach
which uses only document-level labels.
The input of our system is a training corpus la-
beled on the document level (e.g. a clinical dataset
consisting clinical notes and meta-data about pa-
tients). Our approach extracts indicator phrases and
trains a CSD jointly. We focus on local content
shifters and we analyse just the sentences of indi-
cator phrase occurrences. Our chief assumption is
that CSD can be learnt by exploiting the false pos-
itive occurrences of indicator phrases in the train-
ing dataset. We show that our method performs sig-
nificantly better than standard document classifiers
(which were designed for a slightly different task).
The chief contributions of our work are that (i)
we handle the CSD problem in general, so we de-
tect all content shifters instead of focusing on one
particular language phenomenon, (ii) we form a sin-
gle framework for joint CSD and document labeling,
(iii) moreover our approach does not require a dedi-
cated annotated training dataset for content shifters.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999582935483871">
Information-oriented document classification tasks
were first highlighted in the clinical domain where
medical reports contain useful information about the
patient in question, but labels are only available at
the document (patient) level. The field of clinical
NLP has been studied extensively since the 1990s
(Larkey and Croft, 1995), but the most recent results
are related to the shared task challenges organized
relatively recently (Pestian et al., 2007; Uzuner et
al., 2008; Uzuner, 2009). For example the first
I2B2 challenge in 2006 (Uzuner et al., 2008) fo-
cused on the smoking habits of the patient, the CMC
challenge in 2007 (Pestian et al., 2007) dealt with
the problem of automatically constructing ICD cod-
ing systems and the second I2B2 challenge (Uzuner,
2009) addressed the classification of discharge sum-
maries according to the question ”Who’s obese and
what co-morbidities do they have?”. These chal-
lenges were dominated by entirely or partly rule-
based systems that solved the tasks using indicator
phrase lookup and incorporated explicit mechanisms
for detecting speculation and negation.
Another domain for information-oriented docu-
ment classification might be Wikipedia, which con-
tains rich information about entities like persons,
places or organisations. Some items of information
are available about these entities in the form of cate-
gories and infoboxes assigned to articles. Automatic
document labeling methods can be trained based on
these assignments (Sch¨onhofen, 2006), but these la-
bels do not refer to the main theme of the article but
</bodyText>
<page confidence="0.988884">
760
</page>
<bodyText confidence="0.999905793103448">
to a certain type of information.
Existing content shift detection approaches focus
on a particular class of language phenomena, espe-
cially on negation and hedge recognitions. Avail-
able tools work mainly on clinical and biological
domains. The first systems were fully hand-crafted
(Light et al., 2004; Friedman et al., 1994; Chapman
et al., 2007) without any empirical evaluation on a
dedicated corpus. Recently, there have been several
corpora published with manual sentence-, event- or
token-level annotation for negation, certainty and
factuality in the biological (Medlock and Briscoe,
2007; Vincze et al., 2008), newswire (Strassel et al.,
2008; Sauri and Pustejovsky, 2009) and encyclope-
dical (Farkas et al., 2010) domains.
Exploiting these corpora, machine learning mod-
els were also developed. Solving the sentence-
level task, Medlock and Briscoe (2007) used sin-
gle words as input features in order to classify sen-
tences from biological articles as speculative or non-
speculative. Szarvas (2008) extended their method-
ology to use n-gram features and a semi-supervised
selection of the keyword features. Ganter and Strube
(2009) proposed an approach for the automatic de-
tection of sentences containing uncertainty based
on Wikipedia weasel tags and syntactic patterns.
For in-sentence negation and speculation detection,
Morante et al. (2009) developed scope – i.e. con-
tent shifted text spans – detectors for negation and
speculation following a supervised sequence label-
ing approach, while ¨Ozg¨ur and Radev (2009) devel-
oped a rule-based system that exploits syntactic pat-
terns. The goal of the CoNLL 2010 Shared Task
(Farkas et al., 2010) was to develop linguistic scope
detectors as well. The participants usually followed
a supervised sequence labeling approach or used a
rule-based system that exploits syntactic patterns.
The approach of classifying identified events into
whether they fall under negation or speculation was
followed by Sauri and Pustejovsky (2009) and the
participants of the BioNLP’09 Shared Task (Kim et
al., 2009). Here the systems investigated the syn-
tax path between the event trigger and a cue word
(which came from a small lexicon) (Kilicoglu and
Bergler, 2009; Aramaki et al., 2009).
Our approach differs from the previous works
fundamentally. We deal with the two tasks
(information-oriented document classification and
content shift detection) together and introduce a co-
learning approach for them. Our approach han-
dles content shifters in a data-driven and general-
ized way i.e. it is not specialized for a certain class
of language phenomena. Instead it tries to recog-
nize task-specific syntactic and semantic patterns
which are responsible for semantic changes or irrel-
evance. In addition, we have no access to a gold-
standard sentence-level or in-sentence-level annota-
tion but exploit document-level ones.
</bodyText>
<sectionHeader confidence="0.971084" genericHeader="conclusions">
3 Tasks and Datasets
</sectionHeader>
<bodyText confidence="0.999639333333333">
Before introducing our approach in detail we de-
scribe three tasks and datasets which were used in
our experiments in order to give an insight into the
challenges of the information-oriented document la-
beling tasks. Table 1 summarizes the key statistical
figures (the number of documents in the corpora, the
size of the label sets along with the average number
of tokens and label assignments per document) of
the datasets used for the experimental evaluations.
</bodyText>
<tableCaption confidence="0.998862">
Table 1: The datasets used in our experiments.
</tableCaption>
<table confidence="0.720040142857143">
CMC Obes Soccer
domain clinical clinical encycl.
Itrainj 978 730 4850
levall 976 507 1736
#token/d 25 1387 389
#labels 45 16 12
#label/d 1.24 4.37 1.23
</table>
<bodyText confidence="0.9774686">
The CMC ICD Coding Dataset was originally
prepared for a shared task challenge organized by
the Computational Medicine Center (CMC) in Cin-
cinatti, Ohio in 2007 (Pestian et al., 2007). It con-
tains radiology reports along with document-level
International Classification of Diseases (ICD) codes
given by three human experts. ICD is a coding of
diseases, signs, symptoms and abnormal findings. In
our experiments we used the train/evaluation split of
the shared task. The ICD coding guide states that
negative or uncertain diagnosis should not be coded
in any case.
The corpus contains very short documents. For
instance, the document
HISTORY: Left lower chest pain. Rule-out
</bodyText>
<page confidence="0.88697">
761
</page>
<bodyText confidence="0.994452450549451">
pneumonia. IMPRESSION: Normal chest. categories of Wikipedia, classifiers can be trained to
tag unlabeled texts or even add missing category as-
signments to Wikipedia (Sch¨onhofen, 2006).
For a case study we focused on learning En-
glish soccer clubs that a given sportsman played
for. Note that this task is an information-oriented
document labeling task as the clubs for which a
sportsman played are usually just mentioned (espe-
cially for smaller clubs) in the article of a player.
The Wikipedia category Footballers in England by
club contains 408 subcategories (for the present and
past). We selected the best known clubs (where the
category label for the club is assigned to more than
500 player pages). Each article referring to a player
having a category assignment to these clubs was
downloaded and the textual parts were extracted.
Then a random 3:1 train:evaluation split of the doc-
ument set was used.
4 Document-labeling with CSD
We introduce here an iterative solution which selects
indicator phrases and trains a content shift detec-
tor at the same time. Our focus will be on multi-
label document classification tasks where multiple
class labels can be assigned to a single document.
In this study we will not deal with the modeling of
inter-label dependencies, so binary (positive versus
negative) and multi-class document classifications
(where exactly one label has to be assigned to a sin-
gle document) can be regarded as special cases of
this multi-label classification problem. Our result-
ing multi-label model is then a set of binary classi-
fiers – ”assign a label” classifiers for each class label
– and the final prediction on a document is simply
the union of the labels forecasted by the individual
classifiers.
Our key assumption in the multi-label environ-
ment is that while indicator phrases have to be se-
lected on a per class basis, the content shifters can be
learnt in a class-independent (aggregated) way i.e.
we can assume that within one task, each class label
belongs to a given semantic domain (determined by
the task), thus the content shifters for their indicator
phrases are the same. This approach provides an ad-
equate amount of training samples for content shift
detector learning.
has one label 786.50 (cough) as 486 (pneumonia) is
ruled out.
The main conclusion of the shared task in 2007
was that simple rule-based systems generally out-
perform bag-of-words-based machine learning mod-
els. The rules were extracted from ICD guidelines
and/or from the training corpus using simple sta-
tistical measures, then they were checked or ex-
tended manually. Several systems of the challenge
employed a negation and speculation detection sub-
module. The (manually highly fine-tuned) top sys-
tems of the CMC shared task achieved an F-measure
of 88-89 (Pestian et al., 2007; Farkas and Szarvas,
2008).
The I2B2 Obesity Dataset was also the subject
of a clinical natural language processing shared
task. The challenge in 2008 focused on analyzing
clinical discharge summary texts and addressed the
following question: ”Who is obese and what co-
morbidities do they have?” (Uzuner, 2009). Tar-
get diseases (document labels) included obesity and
its 15 most frequent co-morbidities exhibited by
patients. In our experiments, we used the same
train/evaluation split as that of the shared task. Here
a special aspect of the corpus is that the docu-
ments are semi-structured, i.e. they contain head-
ings like discharge medications and admit diagno-
sis. By pasting the given heading to the beginning
of each sentence, we incorporated it into the local
context. The top performing systems of the shared
task employed mostly hand-crafted rules for indica-
tor selection and for negation and uncertainty detec-
tion as well. They achieved an F-measure1 of 96-97
(Uzuner, 2009; Solt et al., 2009).
Wikipedia Soccer Dataset. We constructed a cor-
pus based on Wikipedia articles and categories2.
The categories assigned to Wikipedia articles can be
regarded as labels (for example, the labels of David
Beckham in the Wikipedia are English people, ex-
patriate soccer player, male model and A.C. Milan
player, Manchester United player). Based on the
1Using the definitions of the challenge, the evaluation metric
applied here is the micro F-measure of the textual task on the
YES versus every other class.
2The dataset is available as the supplementary material.
762
</bodyText>
<tableCaption confidence="0.941844">
Table 2: Example feature representation of local contexts of Arsenal. The prefix NP stands for the lemma features
from the deepest noun phrase; D,DR and DEP marks the lemmas, roles and their combination in the dependency path,
respectively; SUBJ and SUBJD denote the lemmas and dependency roles on the ”subject path”, respectively.
</tableCaption>
<table confidence="0.986616333333333">
His brother, Paul had a long career at Newcastle. (sentenceId=1, indicator=Newcastle)
bag-of-word features syntax-based features
he, brother, Paul, have, NP#a, NP#long, NP#career, NP#at
a, long, career, at D#career, D#have, DR#prepat, DR#dobj, DEP#career#prepat, DEP#have#dobj
SUBJ#brother, SUBJ#Paul, SUBJ#he, SUBJD#he#poss
He was born in Gosforth, Newcastle and played for Arsenal. (sentenceId=2, indicator=Arsenal)
bag-of-word features syntax-based features
he, be, bear, in, Gosforth, D#play, DR#prepfor, DEP#play#prepfor
Newcastle, and, play, for SUBJ#he
</table>
<subsectionHeader confidence="0.953255">
4.1 Learning Content Shift Detectors
</subsectionHeader>
<bodyText confidence="0.980947206349207">
The key idea behind our approach is that a training
corpus for task-specific content shifter learning can
be automatically generated by exploiting the occur-
rences of indicators in various contexts. The local
context of an indicator is assumed to have altered if
it yields a false positive document-level prediction.
More precisely, a training dataset can be constructed
for learning a content shift detector in a way that the
instances are the local contexts of each occurrence of
indicator phrases in the training document set. The
instances of this content shifter training dataset are
then labeled as non-altered when the indicated
label is among the gold-standard labels of the doc-
ument in question or is labeled as altered other-
wise. On this dataset, arbitrary binary classification
models (5) can be trained.
As a feature representation of a local context of an
indicator phrase, the bag-of-words of the sentence
instance (excluding the indicator phrase itself) was
used at the beginning. Our preliminary experiments
showed that the tokens of the sentence after the indi-
cator played a negligible role, hence we represented
contexts just by tokens before the indicator.
Features concerning the syntactic context of the
given indicator were also investigated. For this, we
extended the feature set with features derived from
the constituent and dependency parses of the sen-
tence3. First, the deepest noun phrase which in-
cludes the indicator phrase was identified, then all
3We parse only the sentences which contain indicator phrase
which makes these features computable in reasonable time even
on bigger document sets.
lemmas from its subtree were gathered. From the
dependency parse, the lemmas and dependency la-
bels on the directed path from the indicator to the
root node (main path) were extracted. The directed
paths branching from this main path starting with
subject dependency were also used for feature
extraction (note that these walk in opposite direction
to that of the main path). The intuition of the latter
was that the subject of the given information – as it
can differ from the target entity of extraction – is of
great importance. We note that we recognize the in-
sentence subject and employing a co-reference mod-
ule would probably increase the value of these fea-
tures.
Table 2 exemplifies the feature representation of
local contexts of the Newcastle and Arsenal indi-
cators for the Wikipedia soccer task. In both sen-
tences, a naive system would extract Newcastle as
false positives. We want to learn content shifters
from them along with the true positive match of
Arsenal in sentence 2. From the first example the
CSD could learn even that the bag-of-word con-
tains brother or the SUBJ=brother. However, in
the second example, the bag-of-word representa-
tion is not sufficient to learn that the local context
of Newcastle is altered because it is the sub-
set of the bag-of-word representation of Arsenal’s
non-altered local context. In this case the syn-
tactic context representation can help and in our
CSD DEP=play#prepfor gets high weight for the
non-altered class.
</bodyText>
<page confidence="0.990792">
763
</page>
<subsectionHeader confidence="0.99509">
4.2 Co-learning of Indicator Selection and CSD
</subsectionHeader>
<bodyText confidence="0.997172625">
If document labels are available at training time,
an iterative approach can be used to learn the local
content shift detector and the indicator phrases as
well. The training phase of this procedure (see Al-
gorithm 1) has two outputs, namely the set of indica-
tor phrases for each label I and the content shift de-
tector S which is a binary function for determining
whether the sense of an indicator in a particular local
context is being altered. Good indicator phrases are
those that identify the class label in question when
they are present. In each step of the iteration we se-
lect indicator phrases I[l] for each label l based on
the actual state of the document set D′. Using these
I[l]s we train a CSD S. Then we apply it to the orig-
inal dataset D and we delete each local context from
the documents which was predicted to be altered by
</bodyText>
<figure confidence="0.872487090909091">
S.
Algorithm 1 Co-learning of labels and CSD
Input: L class labels, D labeled training documents
D′ +— D
repeat
for all l E L do
I[l] +— indicatorSelection(D′, l)
end for
S +— learnCSD(D′, I)
D′ +— removeAlteredParts(D, S)
until convergence
</figure>
<figureCaption confidence="0.553533">
return I, S
</figureCaption>
<bodyText confidence="0.999951">
The indicator selection and content shifter learn-
ing phases can form an iterative process. The bet-
ter the selected indicators are, the better the content
shift detectors can be learnt. By applying the content
shift detector to each token of the documents, each
part of the texts lying within the scope of a content
shifter can be removed4. By using such a cleaned
training document set (D′), better indicators can be
selected. These steps can be repeated until some
convergence criterion is reached. In our experiments
we simply used a fixed iteration number to gain an
insight into the behavior of the approach.
</bodyText>
<footnote confidence="0.4357685">
4In our first experiments introduced here, we removed the
parts of the documents classified as altered. Instead of removing
these parts they may be marked and then different features may
be extracted from them.
</footnote>
<table confidence="0.189772285714286">
Algorithm 2 Document labeling with CSD
Input: d document, I indicator sets, S CSD
pred +— 0
for all l E L do
for all o E occurrences(d, I[l]) do
if not altered(o, S) then
pred +— pred U l
</table>
<tableCaption confidence="0.675258666666667">
end if
end for
end for
</tableCaption>
<bodyText confidence="0.9835119375">
return pred
The prediction procedure of the approach (see Al-
gorithm 2) then looks for occurrences of the indica-
tor phrases in the text and checks whether they are
altered in a certain local context. A non-altered indi-
cator directly assigns a class label without any global
consistency check on assigned labels.
We note here, that the local relationship among
tokens (i.e. the local context) may be taken into
account by incorporating this information directly
into the feature space of a document classifier (as
an alternative of our co-learning procedure), but
the number of features would exponentially increase
and submodels for each indicator phrases should be
learnt which would made such a classification task
intractable.
</bodyText>
<subsectionHeader confidence="0.994143">
4.3 Indicator Phrase Selection
</subsectionHeader>
<bodyText confidence="0.999981875">
Indicator phrases are sequences of tokens whose
presence implies the positive class. We aimed to
extract phrases with the length of 1,2 or 3 (and we
used exact matching after lemmatisation). There are
several possible ways of developing indicator selec-
tion algorithms. One way is to treat it as a special
feature selection procedure where the goal is to se-
lect a set of features (uni-, bi-, trigrams of a bag-
of-word model) which achieves high recall along
with moderate precision as false positives are ex-
pected to be eliminated by the local CSD in our two-
step approach. Indicator selectors can be even de-
rived from most classifiers which are based on fea-
ture weighting (like MaxEnt and AvgPerceptron) or
feature ranking (like rule-based classifiers)5 as well.
However indicator selection is not the focus of this
</bodyText>
<footnote confidence="0.986936">
5A derivation is more complicated or unfeasible for
example-based classifiers like SVMs.
</footnote>
<page confidence="0.997748">
764
</page>
<tableCaption confidence="0.999883">
Table 3: Results obtained for local content shift detection in a precision/recall/F-measure format.
</tableCaption>
<table confidence="0.993912833333333">
CMC Obesity Soccer
Trained BoW 90.7 / 60.7 / 72.7 82.1 / 35.4 / 49.4 75.0 / 70.6 / 72.7
BoW+syntactic 88.3 / 60.2 / 71.6 84.4 / 33.3 / 47.8 81.0 / 78.9 / 79.9
Hand- CSSDB 94.7 / 53.3 / 68.2 42.0 / 57.9 / 48.7 36.8 / 9.8 / 15.5
crafted
in-sentence 80.7 / 65.2 / 72.2 70.5 / 40.5 / 51.5 N/A
</table>
<bodyText confidence="0.99211852">
work.
For our experiments, a feature evaluation-based
greedy algorithm was employed to select the set of
indicators from the pool of token uni- and bigrams.
The aim of the the indicator selection here is to cover
each positive documents while introducing a rela-
tively small amount of false positives. The greedy
algorithm iteratively selects the 1-best phrase ac-
cording to a feature evaluation metric based on the
actual state of covered documents and adds it to the
indicator phrase set. The process is iterated while
the score – in terms of the applied feature evaluation
metric – of the 1-best phrase is above a threshold
t. The quality of the selected indicator set is highly
dependent on the stopping threshold t, but as indi-
vidual feature evaluation functions are very fast and
the number of good indicators is usually low (4-5),
the whole greedy indicator selection is fast, hence t
can be fine-tuned without overfitting on the training
sets employing a cross-validation procedure. As a
feature evaluation metric we employed p(+|f) the
probability of the positive class ”+” conditioned on
the presence of a feature f because preliminary ex-
periments did not show any significant advances for
more complex metrics.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="acknowledgments">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9997851">
Experiments were carried out on the three datasets
introduced in Section 3 with local content shift de-
tection as an individual task and also to investigate
its added value to information-oriented document la-
beling.
In our experiments, we applied the sentence split-
ter and lemmatizer implementation of the Mor-
phAdorner package6 and the Stanford tokenizer
and lexicalized PCFG parser (Klein and Manning,
2003)7.
</bodyText>
<footnote confidence="0.9405735">
6morphadorner.northwestern.edu/
7The JAVA implementation of the entire framework and
</footnote>
<subsectionHeader confidence="0.998811">
5.1 Content Shifter Learning Results
</subsectionHeader>
<bodyText confidence="0.99858497368421">
In order to evaluate content shift detection as an indi-
vidual task, a set of indicator phrases have to be fixed
as an input to the CSD. We used manually collected
indicator phrases for each label for each dataset. We
utilized the terms of Farkas and Szarvas (2008) and
Farkas et al. (2009) collected for the CMC and Obe-
sity datasets, respectively and club names for the
Soccer dataset in our first branch of experiments.
Note that the clinical term sets here have been man-
ually fine-tuned as they were developed for partici-
pating systems of the shared tasks of the corpora.
Based on the occurrences of these fixed indicator
phrases, CSD training datasets were built from the
local contexts of the three datasets and binary clas-
sification was carried out by using MaxEnt. Table
3 shows the results achieved by the learnt CSDs us-
ing the bag-of-word feature representation (row 1)
along with the ones obtained by the feature set that
was extended with syntactic patterns (raw 2). Here,
the precision/recall/F-measure values measure how
many false positive matches of the indicator phrases
can be recognized (the F-measure of the altered
class), i.e. here, the true positives are local contexts
of an indicator phrase which do not indicate a docu-
ment label in the evaluation set and the local content
shift detector predicted it to be altered.
For comparison purposes, we employed manu-
ally developed CSDs which were fine-tuned for the
medical shared task datasets. Row 3 of Table 3 (we
refer to it as content shifted sentence detection base-
line (CSSDB) later on) shows the results archived
by the method which predicts every sentence to be
altered which contain any cue phrases for nega-
tion, modality and different experiencer. Note that
off-the-shelf tools are available just for these types
of content shifters. We collected cue phrases for
such a content shifted sentence detection from the
dataset adapters can be found as the supplementary material
</bodyText>
<page confidence="0.993868">
765
</page>
<bodyText confidence="0.985508119565218">
works of Chapman et al. (2007), Light et al. (2004)
and Vincze et al. (2008) and from the experiments of
Farkas and Szarvas (2008) and Farkas et al. (2009).
For the CMC and Obesity tasks, hand-crafted
in-sentence CSDs were also available (Farkas and
Szarvas, 2008; Farkas et al., 2009), i.e. they apply
heuristics – which usually tries to recognise clause
boundaries – for determining the scope of a nega-
ton/modality cue. This CSD is more fine-grained
than the sentence-level one as here a part of a sen-
tence can be detected as altered while other parts
as non-altered. The results of these detectors –
two different CSDs, both highly fine-tuned for the
corresponding shared task – are listed in the last row
of Table 3.
On the CMC dataset, our machine learning ap-
proach identified mostly negation and speculation
expressions as content shifters; the top weighted
features for the positive class of the MaxEnt model
were no, without, may and vs. They can filter out
false positive matches like
Hyperinflated without focal pneumonia..
On the Obesity dataset, similar content shifters
were learnt along with references to family mem-
bers (like the terms mother and uncle, and the fam-
ily history header). The significance of these types
of content shifters may be illustrated by the follow-
ing sentence:
History of hypertension in mother and sis-
ter.
The soccer task highlighted totally different con-
tent shifters which is also the reason for the poor per-
formance of CSSDB. The mention of a club name
which the person in question did not play for (false
positives) is usually a rival club, club of an unsuc-
cessful negotiation or club which was managed by
the footballer after his retirement. For example:
His last game was against Chelsea at
Stamford Bridge.
He was a coach at United during his son’s
playing career.
Summing up, the machine learnt CSDs proved to
be competitive with the manually fine-tuned CSD
on the three datasets. Table 3 shows that learnt
CSDs were able to eliminate a significant amount
of false positive indicator phrase matches on each
of the three datasets. The hand-crafted CSDs de-
veloped for the medical texts certainly work poorly
(an F-score of 15.5) on the Soccer dataset as content
shifters different from negation, hedge and experi-
encer are useful there. On the other hand, the content
shifters could be learnt on this dataset by our CSD
approach (achieving F-score of 79.9). In the clinical
corpora, the features from the syntactic parses just
confused the system, but they proved to be useful
on the Soccer corpus. Here, the dependency parse
achieved improvements in terms of both precision
and recall (the number of true positives increased by
137) which can be mainly attributed to the preposi-
tions against and over. The reason why it did not
advance on the clinical corpora is probably the do-
main difference between the training corpus of the
parsers and the target texts, i.e. the parsers trained
on the Wall Street Journal could not build adequate
dependency parses on clinical notes.
As a final comparison we investigated the manu-
ally annotated BioScope corpus (Vincze et al., 2008)
as a CSD. The CMC corpus is included in the Bio-
Scope corpus where text spans in the in-sentence
scope of speculation and negation were annotated.
We used this manual annotation as an oracle CSD
and got an F-measure of 75.2 (which is significantly
higher than the scores 72.2 and 72.7 archived by the
hand-crafted and trained CSD respectively). This
score can be regarded as an upper bound for the
amount of false positive indicator matches that can
be fixed by local speculation and negation detec-
tors. The remaining false positives are not covered
by the linguistically motivated annotations of Bio-
Scope, i.e. false positives recognizable by domain
knowledge (e.g. coding symptoms should be omit-
ted when a certain diagnosis that is connected with
the symptom in question is present in the document)
are not marked.
Our error analysis revealed that most of the er-
rors of the learnt CSDs is due to the lack of seman-
tic link between lexical units. For instance, on the
Soccer dataset it could learn that the token coach
occuring in the sentence in question indicates an
altered content, but it was not able to recognise
this for trainer. The reason for that is simple, the
ratio of occurrences of trainer:coach is 5:95 in the
</bodyText>
<page confidence="0.998464">
766
</page>
<tableCaption confidence="0.999815">
Table 4: Results obtained by document multi-labeling algorithms in a precision/recall/F-measure format.
</tableCaption>
<table confidence="0.988664818181818">
rowID CMC Obesity Soccer
1 Baseline SVM with CSSDB 87.7 / 76.7 / 81.8 90.0 / 81.3 / 85.4 92.2 / 75.1 / 82.8
2 MaxEnt 92.2 / 72.2 / 81.0 91.4 / 87.6 / 89.4 92.2 / 77.4 / 84.2
3 PART 83.9 / 80.6 / 82.2 87.3 / 86.4 / 86.8 81.2 / 77.0 / 79.0
4 Indicator without CSD 78.0 / 85.1 / 81.4 89.2 / 93.6 / 91.3 84.4 / 83.7 / 84.1
Selection
5 with CSSDB 79.0 / 84.1 / 81.4 94.8 / 86.6 / 91.1 85.2 / 85.5 / 85.3
6 with learnt CSD 83.1 / 83.2 / 83.2 91.7 / 92.9 / 92.3 91.7 / 85.2 / 88.3
7 after 3 iterations 82.4 / 86.8 / 84.6 92.6 / 95.4 / 94.0 92.5 / 84.0 / 88.0
8 after 10 iterations 82.4 / 86.8 / 84.6 92.7 / 95.4 / 94.0 92.5 / 84.0 / 88.0
9 Baseline MaxEnt with learnt CSD 89.9 / 77.0 / 83.0 91.9 / 90.4 / 91.1 95.0 / 78.7 / 86.1
</table>
<bodyText confidence="0.999477909090909">
training corpus. Increasing the training size may be
a simple way to overcome this shortcoming. Note
that increasing the number of labels (e.g. introduc-
ing more soccer clubs in the Soccer task) would also
directly increase the size of training dataset as we
use the occurrences of the indicator phrases belong-
ing to each of the labels for training a CSD. The so-
lution for the rare cases would require the explicit
handling of semantic relatedness (by utilising ex-
isting semantic resources or trying to automatically
identify task-specific relations).
</bodyText>
<subsectionHeader confidence="0.999115">
5.2 Document Labeling Results
</subsectionHeader>
<bodyText confidence="0.999994767857143">
The second branch of experiments investigated the
added value of CSDs in information-oriented doc-
ument labeling tasks. Table 4 summarizes the re-
sults we got on the three datasets using the micro-
averaged Fβ=1 of assigned labels (positive class).
As baseline systems we trained binary SVMs
with a linear kernel, MaxEnts and PARTs – a rule-
learner classification algorithm (Frank and Witten,
1998) – for each label using the bag-of-word rep-
resentation of the documents (implementations of
SVMligth (Joachims, 1999), MALLET (McCallum,
2002) and WEKA (Witten and Frank, 1999) were
used). The first two learners are popular choices for
document classification, while the third is similar to
our simple indicator selection procedure. We did not
tuned the parameters of the classifiers, we used the
default ones everywhere.
To have a fair comparison, we applied to pre-
processing steps on dataset of these document clas-
sifiers. First, we removed from the training and
evaluation raw documents which were predicted to
be altered by CSSDB. Second, as our indicator
selection phrase can be regarded as a special fea-
ture selection method, we carried out an Information
Gain-based feature selection (keeping the 500 best-
rated features proved to be the best solution) on the
bag-of-word representation of the documents. The
effect of these two preprocessing steps varied among
datasets. It improved the F-score of the MaxEnt
baseline document classifier by 20%, 2% and 3% on
the Obesity, CMC and Soccer datasets, respectively
(the F-measures of Table 4 are the values we got by
employing pre-processing).
The indicator selection results presented in the
rows 4-8 of Table 4 made use of the p(+|f)-based
indicator selector with a five-fold-cross-validated
stopping threshold t (introduced in Section 4.3).
Row 4 contains the results of using the selected in-
dicators without any CSD. Indicator selection with
the CSSDB was applied for the 5th row. Rows 6-
8 of Table 4 show the results obtained after one,
three and ten iterations of the full learning algo-
rithm (see Algorithm 1). For training the CSD,
we employed MaxEnt as a binary classifier for de-
tecting altered local contexts and we used the
basic BoW feature representation for the clinical
tasks while the extended (BoW+syntactic) one for
the Soccer dataset.
In the final experiment (the last row of Table
4)) we investigated whether the learnt content shift
detector can be applied as a general ”document
cleaner” tool. For this, we trained the baseline Max-
Ent document classifier with feature selection on
documents from which the text spans predicted to
be altered by the learnt CSD in the tenth iter-
ation were removed. This means that the systems
</bodyText>
<page confidence="0.883732">
767
</page>
<bodyText confidence="0.98252725">
used in row 2 and row 9 differ only in the applied 6 Conclusions
document cleaner pre-processing steps (the first one In this paper, we dealt with information-oriented
applied the CSSDB while the latter one employed document labeling tasks and investigated machine
the the learnt CSD). learning approaches for local content shift detectors
The difference between the best baseline and the from document-level labels. We demonstrated ex-
indicator selector with learnt CSD and between the perimentally that a significant amount of false posi-
best baseline and the document classifier with learnt tive matches of indicator phrases can be recognized
CSD were statistically significant8 on each dataset. by trained content shift detectors. Our trained CSD
The difference between the predictions after the 1st does not use any task or domain specific knowledge
and 3rd iterations were statistically significant on and exploits the false and true positive matches of in-
the CMC and the Obesity corpora but not signifi- dicator phrases, i.e. it uses only document-level an-
cant on the Soccer dataset. The difference between notation. This task-independent approach achieved
the 3th and 10th iterations were not significant in ei- competitive results with CSDs which were manually
ther case. Our co-learning method which integrated fine-tuned for particular datasets. The empirical re-
the document-labeling and CSD tasks significantly sults also support the idea of generalized local CSD
outperformed the baseline approaches – which use (false positive removal) opposite to developing in-
separate document cleaning and document labeling dependent CSD for particular language phenomena
steps – on the three datasets. (like negation and speculation).
On the clinical domains the automatically se- A co-learning framework for training local con-
lected indicators were disease names, symptom tent shift detectors and indicator selection was in-
names (e.g. high blood pressure), their spelling vari- troduced as well. Our method integrates document
ants, synonyms (like hypertension) and their abbre- classification and CSD learning, which are tradi-
viations (e.g htn). On the soccer domain club names, tionally used as independent submodules of appli-
synonyms (like The Saints) and stadium names (e.g. cations. Experiments on three information-oriented
Old Trafford) were selected. A label was indicated document-labeling datasets – from two application
by 3-4 indicator phrases. areas – with simple indicator selection and syntactic
Note that in these information-oriented docu- parse-based content shifter learning were performed
ment multi-labeling tasks simple indicator selection- and the results show a clear improvement over the
based document labelers alone achieved results bag-of-word-based document classification baseline
comparable to the bag-of-words-based classifiers. approaches.
The learnt content shift detectors led to an average However, the proposed content shift detec-
improvement of 3.6% in the F-measure (i.e. a 24% tor learning approach is tailored for information-
error reduction). The effect of further iterations is oriented document labeling tasks, i.e. it performs
various. As Table 4 shows, three iterations brought well when not too many and reliable indicator
an increase on the CMC and Obesity datasets but not phrases are present. In the future, we plan to in-
on the Soccer corpus. After a few iterations the set vestigate and extend the framework for the general
of indicator phrases and the content shift detector document classification task where many indicators
did not change substantially. The results achieved with complex relationships among them determine
by the MaxEnt document classifier employing the the labels of a document but local content shifters
”cleaned” training documents (last row of Table 4) can play an important role.
are significantly better (an average improvement of Acknowledgements
1.9% in the F-measure and 12% error reduction) This work was partially founded by the Research
than those by the CSSDB (row 2) but the indicator Group on Artificial Intelligence of the Hungarian
selector approach performed even better. Academy of Sciences. Rich´ard Farkas was also
funded by Deutsche Forschungsgemeinschaft grant
SFB 732.
8According to McNemar’s test with P-value of 0.001
768
</bodyText>
<sectionHeader confidence="0.983603" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891913461538">
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. TEXT2TABLE: Medical Text Summa-
rization System Based on Named Entity Recognition
and Modality Identification. In Proceedings of the
BioNLP 2009 Workshop, pages 185–192.
Wendy W. Chapman, David Chu, and John N. Dowling.
2007. Context: an algorithm for identifying contextual
features from clinical text. In Proceedings of the ACL
Workshop on BioNLP 2007, pages 81–88.
Rich´ard Farkas and Gy¨orgy Szarvas. 2008. Automatic
construction of rule-based icd-9-cm coding systems.
BMC Bioinformatics, 9(Suppl 3):S10.
Rich´ard Farkas, Gy¨orgy Szarvas, Istv´an Heged¨us, At-
tila Alm´asi, Veronika Vincze, R´obert Orm´andi, and
R´obert Busa-Fekete. 2009. Semi-automated construc-
tion of decision rules to predict morbidities from clini-
cal texts. Journal ofthe American Medical Informatics
Association, 16:601–605.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceedings
of the Fourteenth Conference on Computational Natu-
ral Language Learning (CoNLL-2010): Shared Task,
pages 1–12.
Eibe Frank and Ian H. Witten. 1998. Generating accu-
rate rule sets without global optimization. In Proc. of
Fifteenth International Conference on Machine Learn-
ing, pages 144–151.
Carol Friedman, Philip O. Alderson, John H. M. Austin,
James J. Cimino, and Stephen B. Johnson. 1994. A
General Natural-language Text Processor for Clinical
Radiology. Journal of the American Medical Infor-
matics Association, 1(2):161–174.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ing Wikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173–176.
Thorsten Joachims, 1999. Making large-scale support
vector machine learning practical, pages 169–184.
MIT Press, Cambridge, MA, USA.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic De-
pendency Based Heuristics for Biological Event Ex-
traction. In Proceedings of the BioNLP Workshop
Companion Volume for Shared Task, pages 119–127.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview of
BioNLP’09 Shared Task on Event Extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1–9.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st ACL,
pages 423–430.
Leah S. Larkey and W. Bruce Croft. 1995. Automatic as-
signment of icd9 codes to discharge summaries. Tech-
nical report.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proc. of Biolink
2004, Linking Biological Literature, Ontologies and
Databases (HLT-NAACL Workshop:), pages 17–24.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature.
In Proceedings of the ACL, pages 992–999, June.
Roser Morante, V. Van Asch, and A. van den Bosch.
2009. Joint memory-based learning of syntactic and
semantic dependencies in multiple languages. In Pro-
ceedings of CoNLL, pages 25–30.
Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific Text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398–1407.
John P. Pestian, Chris Brew, Pawel Matykiewicz,
DJ Hovermale, Neil Johnson, K. Bretonnel Cohen, and
Wlodzislaw Duch. 2007. A shared task involving
multi-label classification of clinical free text. In Pro-
ceedings of the ACL Workshop on BioNLP 2007, pages
97–104.
Roser Sauri and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227–268.
Peter Sch¨onhofen. 2006. Identifying document topics
using the wikipedia category network. In Proceedings
of the 2006 IEEE/WIC/ACM International Conference
on Web Intelligence, pages 456–462.
Ill´es Solt, Domonkos Tikk, Viktor G´al, and Zsolt Tivadar
Kardkov´acs. 2009. Semantic classification of diseases
in discharge summaries using a context-aware rule-
based classifier. J. Am. Med. Inform. Assoc., 16:580–
584, jul.
Stephanie Strassel, Mark A. Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda. 2008. Linguis-
tic resources and evaluation techniques for evaluation
of cross-document automatic content extraction. In
LREC.
Gy¨orgy Szarvas. 2008. Hedge Classification in Biomed-
ical Texts with a Weakly Supervised Selection of Key-
words. In Proceedings ofACL-08, pages 281–289.
</reference>
<page confidence="0.979035">
769
</page>
<reference confidence="0.999078866666667">
O. Uzuner, Ira Goldstein, Yuan Luo, and Isaac Kohane.
2008. Identifying Patient Smoking Status from Medi-
cal Discharge Records. Journal ofAmerican Medical
Informatics Association, 15(1):14–24.
Ozlem Uzuner. 2009. Recognizing obesity and comor-
bidities in sparse data. Journal of American Medical
Informatics Association, 16(4):561–70.
Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas,
Gy¨orgy M´ora, and J´anos Csirik. 2008. The Bio-
Scope Corpus: Biomedical Texts Annotated for Un-
certainty, Negation and their Scopes. BMC Bioinfor-
matics, 9(Suppl 11):S9.
Ian H. Witten and Eibe Frank. 1999. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
</reference>
<page confidence="0.997095">
770
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949456">
<title confidence="0.999977">Learning Local Content Shift Detectors from Document-level Information</title>
<author confidence="0.996783">Rich´ard</author>
<affiliation confidence="0.9996595">Institute for Natural Language University of</affiliation>
<email confidence="0.982072">farkas@ims.uni-stuttgart.de</email>
<abstract confidence="0.9985173">document labeling a special document multi-labeling task where the target labels refer to a specific information instead of the topic of the whole document. These kind of tasks are usually solved by looking up indicator phrases and analyzing their local context to filter false positive matches. Here, we introduce an approach for machine content shifters detects irrelevant local contexts using just the original document-level training labels. We handle content shifters in general, instead of learning a particular language phenomenon detector (e.g. negation or hedging) and form a single system for document labeling and content shift detection. Our empirical results achieved 24% error reduction – compared to supervised baseline methods – on three document labeling tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
<author>Yasuhide Miura</author>
</authors>
<title>Masatsugu Tonoike, Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko Ohe.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop,</booktitle>
<pages>185--192</pages>
<marker>Aramaki, Miura, 2009</marker>
<rawString>Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike, Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko Ohe. 2009. TEXT2TABLE: Medical Text Summarization System Based on Named Entity Recognition and Modality Identification. In Proceedings of the BioNLP 2009 Workshop, pages 185–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy W Chapman</author>
<author>David Chu</author>
<author>John N Dowling</author>
</authors>
<title>Context: an algorithm for identifying contextual features from clinical text.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on BioNLP</booktitle>
<pages>81--88</pages>
<contexts>
<context position="4850" citStr="Chapman et al., 2007" startWordPosition="761" endWordPosition="764">rs (e.g. auxiliaries like might and can); they may refer to a subject which differs from the target entity of the task (e.g. clinical notes usually contain information about the family history of the patient); or the semantic content of the shifter may change the role of the target span of a text (e.g. a sportsman can play for or against a particular team). We call these phenomena content shifters and the task of identifying them content shift detection (CSD). Existing CSD approaches focus on a particular class of language phenomena (especially negation or hedging) and use hand-crafted rules (Chapman et al., 2007) or a supervised learning approach that exploits corpora manually annotated at the token-level for a particular type of content shifter (Morante et al., 2009). Moreover higher level applications (like document labeling and information extraction) use a separate CSD module which is developed independently from the target task. We argue that the nature of content shifters is domain and task dependent, so training corpora (at the token-level) are required for content shifters which are important for a particular task but the construction of such training corpora is expensive. Here, we propose an </context>
<context position="8402" citStr="Chapman et al., 2007" startWordPosition="1317" endWordPosition="1320">mation are available about these entities in the form of categories and infoboxes assigned to articles. Automatic document labeling methods can be trained based on these assignments (Sch¨onhofen, 2006), but these labels do not refer to the main theme of the article but 760 to a certain type of information. Existing content shift detection approaches focus on a particular class of language phenomena, especially on negation and hedge recognitions. Available tools work mainly on clinical and biological domains. The first systems were fully hand-crafted (Light et al., 2004; Friedman et al., 1994; Chapman et al., 2007) without any empirical evaluation on a dedicated corpus. Recently, there have been several corpora published with manual sentence-, event- or token-level annotation for negation, certainty and factuality in the biological (Medlock and Briscoe, 2007; Vincze et al., 2008), newswire (Strassel et al., 2008; Sauri and Pustejovsky, 2009) and encyclopedical (Farkas et al., 2010) domains. Exploiting these corpora, machine learning models were also developed. Solving the sentencelevel task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological art</context>
<context position="28621" citStr="Chapman et al. (2007)" startWordPosition="4593" endWordPosition="4596">, we employed manually developed CSDs which were fine-tuned for the medical shared task datasets. Row 3 of Table 3 (we refer to it as content shifted sentence detection baseline (CSSDB) later on) shows the results archived by the method which predicts every sentence to be altered which contain any cue phrases for negation, modality and different experiencer. Note that off-the-shelf tools are available just for these types of content shifters. We collected cue phrases for such a content shifted sentence detection from the dataset adapters can be found as the supplementary material 765 works of Chapman et al. (2007), Light et al. (2004) and Vincze et al. (2008) and from the experiments of Farkas and Szarvas (2008) and Farkas et al. (2009). For the CMC and Obesity tasks, hand-crafted in-sentence CSDs were also available (Farkas and Szarvas, 2008; Farkas et al., 2009), i.e. they apply heuristics – which usually tries to recognise clause boundaries – for determining the scope of a negaton/modality cue. This CSD is more fine-grained than the sentence-level one as here a part of a sentence can be detected as altered while other parts as non-altered. The results of these detectors – two different CSDs, both hi</context>
</contexts>
<marker>Chapman, Chu, Dowling, 2007</marker>
<rawString>Wendy W. Chapman, David Chu, and John N. Dowling. 2007. Context: an algorithm for identifying contextual features from clinical text. In Proceedings of the ACL Workshop on BioNLP 2007, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>Automatic construction of rule-based icd-9-cm coding systems.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<pages>3--10</pages>
<contexts>
<context position="15076" citStr="Farkas and Szarvas, 2008" startWordPosition="2375" endWordPosition="2378">ft detector learning. has one label 786.50 (cough) as 486 (pneumonia) is ruled out. The main conclusion of the shared task in 2007 was that simple rule-based systems generally outperform bag-of-words-based machine learning models. The rules were extracted from ICD guidelines and/or from the training corpus using simple statistical measures, then they were checked or extended manually. Several systems of the challenge employed a negation and speculation detection submodule. The (manually highly fine-tuned) top systems of the CMC shared task achieved an F-measure of 88-89 (Pestian et al., 2007; Farkas and Szarvas, 2008). The I2B2 Obesity Dataset was also the subject of a clinical natural language processing shared task. The challenge in 2008 focused on analyzing clinical discharge summary texts and addressed the following question: ”Who is obese and what comorbidities do they have?” (Uzuner, 2009). Target diseases (document labels) included obesity and its 15 most frequent co-morbidities exhibited by patients. In our experiments, we used the same train/evaluation split as that of the shared task. Here a special aspect of the corpus is that the documents are semi-structured, i.e. they contain headings like di</context>
<context position="26901" citStr="Farkas and Szarvas (2008)" startWordPosition="4309" endWordPosition="4312"> to information-oriented document labeling. In our experiments, we applied the sentence splitter and lemmatizer implementation of the MorphAdorner package6 and the Stanford tokenizer and lexicalized PCFG parser (Klein and Manning, 2003)7. 6morphadorner.northwestern.edu/ 7The JAVA implementation of the entire framework and 5.1 Content Shifter Learning Results In order to evaluate content shift detection as an individual task, a set of indicator phrases have to be fixed as an input to the CSD. We used manually collected indicator phrases for each label for each dataset. We utilized the terms of Farkas and Szarvas (2008) and Farkas et al. (2009) collected for the CMC and Obesity datasets, respectively and club names for the Soccer dataset in our first branch of experiments. Note that the clinical term sets here have been manually fine-tuned as they were developed for participating systems of the shared tasks of the corpora. Based on the occurrences of these fixed indicator phrases, CSD training datasets were built from the local contexts of the three datasets and binary classification was carried out by using MaxEnt. Table 3 shows the results achieved by the learnt CSDs using the bag-of-word feature represent</context>
<context position="28721" citStr="Farkas and Szarvas (2008)" startWordPosition="4611" endWordPosition="4614">. Row 3 of Table 3 (we refer to it as content shifted sentence detection baseline (CSSDB) later on) shows the results archived by the method which predicts every sentence to be altered which contain any cue phrases for negation, modality and different experiencer. Note that off-the-shelf tools are available just for these types of content shifters. We collected cue phrases for such a content shifted sentence detection from the dataset adapters can be found as the supplementary material 765 works of Chapman et al. (2007), Light et al. (2004) and Vincze et al. (2008) and from the experiments of Farkas and Szarvas (2008) and Farkas et al. (2009). For the CMC and Obesity tasks, hand-crafted in-sentence CSDs were also available (Farkas and Szarvas, 2008; Farkas et al., 2009), i.e. they apply heuristics – which usually tries to recognise clause boundaries – for determining the scope of a negaton/modality cue. This CSD is more fine-grained than the sentence-level one as here a part of a sentence can be detected as altered while other parts as non-altered. The results of these detectors – two different CSDs, both highly fine-tuned for the corresponding shared task – are listed in the last row of Table 3. On the CM</context>
</contexts>
<marker>Farkas, Szarvas, 2008</marker>
<rawString>Rich´ard Farkas and Gy¨orgy Szarvas. 2008. Automatic construction of rule-based icd-9-cm coding systems. BMC Bioinformatics, 9(Suppl 3):S10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
</authors>
<title>Gy¨orgy Szarvas, Istv´an Heged¨us, Attila Alm´asi, Veronika Vincze, R´obert Orm´andi, and R´obert Busa-Fekete.</title>
<date>2009</date>
<journal>Journal ofthe American Medical Informatics Association,</journal>
<pages>16--601</pages>
<marker>Farkas, 2009</marker>
<rawString>Rich´ard Farkas, Gy¨orgy Szarvas, Istv´an Heged¨us, Attila Alm´asi, Veronika Vincze, R´obert Orm´andi, and R´obert Busa-Fekete. 2009. Semi-automated construction of decision rules to predict morbidities from clinical texts. Journal ofthe American Medical Informatics Association, 16:601–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>1--12</pages>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Ian H Witten</author>
</authors>
<title>Generating accurate rule sets without global optimization.</title>
<date>1998</date>
<booktitle>In Proc. of Fifteenth International Conference on Machine Learning,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="34736" citStr="Frank and Witten, 1998" startWordPosition="5665" endWordPosition="5668">SD. The solution for the rare cases would require the explicit handling of semantic relatedness (by utilising existing semantic resources or trying to automatically identify task-specific relations). 5.2 Document Labeling Results The second branch of experiments investigated the added value of CSDs in information-oriented document labeling tasks. Table 4 summarizes the results we got on the three datasets using the microaveraged Fβ=1 of assigned labels (positive class). As baseline systems we trained binary SVMs with a linear kernel, MaxEnts and PARTs – a rulelearner classification algorithm (Frank and Witten, 1998) – for each label using the bag-of-word representation of the documents (implementations of SVMligth (Joachims, 1999), MALLET (McCallum, 2002) and WEKA (Witten and Frank, 1999) were used). The first two learners are popular choices for document classification, while the third is similar to our simple indicator selection procedure. We did not tuned the parameters of the classifiers, we used the default ones everywhere. To have a fair comparison, we applied to preprocessing steps on dataset of these document classifiers. First, we removed from the training and evaluation raw documents which were</context>
</contexts>
<marker>Frank, Witten, 1998</marker>
<rawString>Eibe Frank and Ian H. Witten. 1998. Generating accurate rule sets without global optimization. In Proc. of Fifteenth International Conference on Machine Learning, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Friedman</author>
<author>Philip O Alderson</author>
<author>John H M Austin</author>
<author>James J Cimino</author>
<author>Stephen B Johnson</author>
</authors>
<title>A General Natural-language Text Processor for Clinical Radiology.</title>
<date>1994</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="8379" citStr="Friedman et al., 1994" startWordPosition="1313" endWordPosition="1316">ns. Some items of information are available about these entities in the form of categories and infoboxes assigned to articles. Automatic document labeling methods can be trained based on these assignments (Sch¨onhofen, 2006), but these labels do not refer to the main theme of the article but 760 to a certain type of information. Existing content shift detection approaches focus on a particular class of language phenomena, especially on negation and hedge recognitions. Available tools work mainly on clinical and biological domains. The first systems were fully hand-crafted (Light et al., 2004; Friedman et al., 1994; Chapman et al., 2007) without any empirical evaluation on a dedicated corpus. Recently, there have been several corpora published with manual sentence-, event- or token-level annotation for negation, certainty and factuality in the biological (Medlock and Briscoe, 2007; Vincze et al., 2008), newswire (Strassel et al., 2008; Sauri and Pustejovsky, 2009) and encyclopedical (Farkas et al., 2010) domains. Exploiting these corpora, machine learning models were also developed. Solving the sentencelevel task, Medlock and Briscoe (2007) used single words as input features in order to classify senten</context>
</contexts>
<marker>Friedman, Alderson, Austin, Cimino, Johnson, 1994</marker>
<rawString>Carol Friedman, Philip O. Alderson, John H. M. Austin, James J. Cimino, and Stephen B. Johnson. 1994. A General Natural-language Text Processor for Clinical Radiology. Journal of the American Medical Informatics Association, 1(2):161–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viola Ganter</author>
<author>Michael Strube</author>
</authors>
<title>Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia Tags and Shallow Linguistic Features.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>173--176</pages>
<contexts>
<context position="9188" citStr="Ganter and Strube (2009)" startWordPosition="1434" endWordPosition="1437">on for negation, certainty and factuality in the biological (Medlock and Briscoe, 2007; Vincze et al., 2008), newswire (Strassel et al., 2008; Sauri and Pustejovsky, 2009) and encyclopedical (Farkas et al., 2010) domains. Exploiting these corpora, machine learning models were also developed. Solving the sentencelevel task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or nonspeculative. Szarvas (2008) extended their methodology to use n-gram features and a semi-supervised selection of the keyword features. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. For in-sentence negation and speculation detection, Morante et al. (2009) developed scope – i.e. content shifted text spans – detectors for negation and speculation following a supervised sequence labeling approach, while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic patterns. The goal of the CoNLL 2010 Shared Task (Farkas et al., 2010) was to develop linguistic scope detectors as well. The participants usually followed a </context>
</contexts>
<marker>Ganter, Strube, 2009</marker>
<rawString>Viola Ganter and Michael Strube. 2009. Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia Tags and Shallow Linguistic Features. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical,</title>
<date>1999</date>
<pages>169--184</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="34853" citStr="Joachims, 1999" startWordPosition="5684" endWordPosition="5685">ic resources or trying to automatically identify task-specific relations). 5.2 Document Labeling Results The second branch of experiments investigated the added value of CSDs in information-oriented document labeling tasks. Table 4 summarizes the results we got on the three datasets using the microaveraged Fβ=1 of assigned labels (positive class). As baseline systems we trained binary SVMs with a linear kernel, MaxEnts and PARTs – a rulelearner classification algorithm (Frank and Witten, 1998) – for each label using the bag-of-word representation of the documents (implementations of SVMligth (Joachims, 1999), MALLET (McCallum, 2002) and WEKA (Witten and Frank, 1999) were used). The first two learners are popular choices for document classification, while the third is similar to our simple indicator selection procedure. We did not tuned the parameters of the classifiers, we used the default ones everywhere. To have a fair comparison, we applied to preprocessing steps on dataset of these document classifiers. First, we removed from the training and evaluation raw documents which were predicted to be altered by CSSDB. Second, as our indicator selection phrase can be regarded as a special feature sel</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims, 1999. Making large-scale support vector machine learning practical, pages 169–184. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Syntactic Dependency Based Heuristics for Biological Event Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP Workshop Companion Volume for Shared Task,</booktitle>
<pages>119--127</pages>
<contexts>
<context position="10251" citStr="Kilicoglu and Bergler, 2009" startWordPosition="1598" endWordPosition="1601">actic patterns. The goal of the CoNLL 2010 Shared Task (Farkas et al., 2010) was to develop linguistic scope detectors as well. The participants usually followed a supervised sequence labeling approach or used a rule-based system that exploits syntactic patterns. The approach of classifying identified events into whether they fall under negation or speculation was followed by Sauri and Pustejovsky (2009) and the participants of the BioNLP’09 Shared Task (Kim et al., 2009). Here the systems investigated the syntax path between the event trigger and a cue word (which came from a small lexicon) (Kilicoglu and Bergler, 2009; Aramaki et al., 2009). Our approach differs from the previous works fundamentally. We deal with the two tasks (information-oriented document classification and content shift detection) together and introduce a colearning approach for them. Our approach handles content shifters in a data-driven and generalized way i.e. it is not specialized for a certain class of language phenomena. Instead it tries to recognize task-specific syntactic and semantic patterns which are responsible for semantic changes or irrelevance. In addition, we have no access to a goldstandard sentence-level or in-sentence</context>
</contexts>
<marker>Kilicoglu, Bergler, 2009</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2009. Syntactic Dependency Based Heuristics for Biological Event Extraction. In Proceedings of the BioNLP Workshop Companion Volume for Shared Task, pages 119–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of BioNLP’09 Shared Task on Event Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="10100" citStr="Kim et al., 2009" startWordPosition="1572" endWordPosition="1575">peculation following a supervised sequence labeling approach, while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic patterns. The goal of the CoNLL 2010 Shared Task (Farkas et al., 2010) was to develop linguistic scope detectors as well. The participants usually followed a supervised sequence labeling approach or used a rule-based system that exploits syntactic patterns. The approach of classifying identified events into whether they fall under negation or speculation was followed by Sauri and Pustejovsky (2009) and the participants of the BioNLP’09 Shared Task (Kim et al., 2009). Here the systems investigated the syntax path between the event trigger and a cue word (which came from a small lexicon) (Kilicoglu and Bergler, 2009; Aramaki et al., 2009). Our approach differs from the previous works fundamentally. We deal with the two tasks (information-oriented document classification and content shift detection) together and introduce a colearning approach for them. Our approach handles content shifters in a data-driven and generalized way i.e. it is not specialized for a certain class of language phenomena. Instead it tries to recognize task-specific syntactic and sema</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of BioNLP’09 Shared Task on Event Extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="26512" citStr="Klein and Manning, 2003" startWordPosition="4247" endWordPosition="4250">ic we employed p(+|f) the probability of the positive class ”+” conditioned on the presence of a feature f because preliminary experiments did not show any significant advances for more complex metrics. 5 Experiments Experiments were carried out on the three datasets introduced in Section 3 with local content shift detection as an individual task and also to investigate its added value to information-oriented document labeling. In our experiments, we applied the sentence splitter and lemmatizer implementation of the MorphAdorner package6 and the Stanford tokenizer and lexicalized PCFG parser (Klein and Manning, 2003)7. 6morphadorner.northwestern.edu/ 7The JAVA implementation of the entire framework and 5.1 Content Shifter Learning Results In order to evaluate content shift detection as an individual task, a set of indicator phrases have to be fixed as an input to the CSD. We used manually collected indicator phrases for each label for each dataset. We utilized the terms of Farkas and Szarvas (2008) and Farkas et al. (2009) collected for the CMC and Obesity datasets, respectively and club names for the Soccer dataset in our first branch of experiments. Note that the clinical term sets here have been manual</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah S Larkey</author>
<author>W Bruce Croft</author>
</authors>
<title>Automatic assignment of icd9 codes to discharge summaries.</title>
<date>1995</date>
<tech>Technical report.</tech>
<contexts>
<context position="6820" citStr="Larkey and Croft, 1995" startWordPosition="1071" endWordPosition="1074">ral, so we detect all content shifters instead of focusing on one particular language phenomenon, (ii) we form a single framework for joint CSD and document labeling, (iii) moreover our approach does not require a dedicated annotated training dataset for content shifters. 2 Related Work Information-oriented document classification tasks were first highlighted in the clinical domain where medical reports contain useful information about the patient in question, but labels are only available at the document (patient) level. The field of clinical NLP has been studied extensively since the 1990s (Larkey and Croft, 1995), but the most recent results are related to the shared task challenges organized relatively recently (Pestian et al., 2007; Uzuner et al., 2008; Uzuner, 2009). For example the first I2B2 challenge in 2006 (Uzuner et al., 2008) focused on the smoking habits of the patient, the CMC challenge in 2007 (Pestian et al., 2007) dealt with the problem of automatically constructing ICD coding systems and the second I2B2 challenge (Uzuner, 2009) addressed the classification of discharge summaries according to the question ”Who’s obese and what co-morbidities do they have?”. These challenges were dominat</context>
</contexts>
<marker>Larkey, Croft, 1995</marker>
<rawString>Leah S. Larkey and W. Bruce Croft. 1995. Automatic assignment of icd9 codes to discharge summaries. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Xin Ying Qiu</author>
<author>Padmini Srinivasan</author>
</authors>
<title>The language of bioscience: Facts, speculations, and statements in between.</title>
<date>2004</date>
<booktitle>In Proc. of Biolink 2004, Linking Biological Literature, Ontologies and Databases (HLT-NAACL Workshop:),</booktitle>
<pages>17--24</pages>
<contexts>
<context position="8356" citStr="Light et al., 2004" startWordPosition="1309" endWordPosition="1312">laces or organisations. Some items of information are available about these entities in the form of categories and infoboxes assigned to articles. Automatic document labeling methods can be trained based on these assignments (Sch¨onhofen, 2006), but these labels do not refer to the main theme of the article but 760 to a certain type of information. Existing content shift detection approaches focus on a particular class of language phenomena, especially on negation and hedge recognitions. Available tools work mainly on clinical and biological domains. The first systems were fully hand-crafted (Light et al., 2004; Friedman et al., 1994; Chapman et al., 2007) without any empirical evaluation on a dedicated corpus. Recently, there have been several corpora published with manual sentence-, event- or token-level annotation for negation, certainty and factuality in the biological (Medlock and Briscoe, 2007; Vincze et al., 2008), newswire (Strassel et al., 2008; Sauri and Pustejovsky, 2009) and encyclopedical (Farkas et al., 2010) domains. Exploiting these corpora, machine learning models were also developed. Solving the sentencelevel task, Medlock and Briscoe (2007) used single words as input features in o</context>
<context position="28642" citStr="Light et al. (2004)" startWordPosition="4597" endWordPosition="4600">developed CSDs which were fine-tuned for the medical shared task datasets. Row 3 of Table 3 (we refer to it as content shifted sentence detection baseline (CSSDB) later on) shows the results archived by the method which predicts every sentence to be altered which contain any cue phrases for negation, modality and different experiencer. Note that off-the-shelf tools are available just for these types of content shifters. We collected cue phrases for such a content shifted sentence detection from the dataset adapters can be found as the supplementary material 765 works of Chapman et al. (2007), Light et al. (2004) and Vincze et al. (2008) and from the experiments of Farkas and Szarvas (2008) and Farkas et al. (2009). For the CMC and Obesity tasks, hand-crafted in-sentence CSDs were also available (Farkas and Szarvas, 2008; Farkas et al., 2009), i.e. they apply heuristics – which usually tries to recognise clause boundaries – for determining the scope of a negaton/modality cue. This CSD is more fine-grained than the sentence-level one as here a part of a sentence can be detected as altered while other parts as non-altered. The results of these detectors – two different CSDs, both highly fine-tuned for t</context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004. The language of bioscience: Facts, speculations, and statements in between. In Proc. of Biolink 2004, Linking Biological Literature, Ontologies and Databases (HLT-NAACL Workshop:), pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="34878" citStr="McCallum, 2002" startWordPosition="5687" endWordPosition="5688"> automatically identify task-specific relations). 5.2 Document Labeling Results The second branch of experiments investigated the added value of CSDs in information-oriented document labeling tasks. Table 4 summarizes the results we got on the three datasets using the microaveraged Fβ=1 of assigned labels (positive class). As baseline systems we trained binary SVMs with a linear kernel, MaxEnts and PARTs – a rulelearner classification algorithm (Frank and Witten, 1998) – for each label using the bag-of-word representation of the documents (implementations of SVMligth (Joachims, 1999), MALLET (McCallum, 2002) and WEKA (Witten and Frank, 1999) were used). The first two learners are popular choices for document classification, while the third is similar to our simple indicator selection procedure. We did not tuned the parameters of the classifiers, we used the default ones everywhere. To have a fair comparison, we applied to preprocessing steps on dataset of these document classifiers. First, we removed from the training and evaluation raw documents which were predicted to be altered by CSSDB. Second, as our indicator selection phrase can be regarded as a special feature selection method, we carried</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>992--999</pages>
<contexts>
<context position="8650" citStr="Medlock and Briscoe, 2007" startWordPosition="1351" endWordPosition="1354"> theme of the article but 760 to a certain type of information. Existing content shift detection approaches focus on a particular class of language phenomena, especially on negation and hedge recognitions. Available tools work mainly on clinical and biological domains. The first systems were fully hand-crafted (Light et al., 2004; Friedman et al., 1994; Chapman et al., 2007) without any empirical evaluation on a dedicated corpus. Recently, there have been several corpora published with manual sentence-, event- or token-level annotation for negation, certainty and factuality in the biological (Medlock and Briscoe, 2007; Vincze et al., 2008), newswire (Strassel et al., 2008; Sauri and Pustejovsky, 2009) and encyclopedical (Farkas et al., 2010) domains. Exploiting these corpora, machine learning models were also developed. Solving the sentencelevel task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or nonspeculative. Szarvas (2008) extended their methodology to use n-gram features and a semi-supervised selection of the keyword features. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proceedings of the ACL, pages 992–999, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>V Van Asch</author>
<author>A van den Bosch</author>
</authors>
<title>Joint memory-based learning of syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>25--30</pages>
<marker>Morante, Van Asch, van den Bosch, 2009</marker>
<rawString>Roser Morante, V. Van Asch, and A. van den Bosch. 2009. Joint memory-based learning of syntactic and semantic dependencies in multiple languages. In Proceedings of CoNLL, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arzucan ¨Ozg¨ur</author>
<author>Dragomir R Radev</author>
</authors>
<title>Detecting Speculations and their Scopes in Scientific Text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1398--1407</pages>
<marker>¨Ozg¨ur, Radev, 2009</marker>
<rawString>Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detecting Speculations and their Scopes in Scientific Text. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1398–1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John P Pestian</author>
<author>Chris Brew</author>
<author>Pawel Matykiewicz</author>
<author>DJ Hovermale</author>
<author>Neil Johnson</author>
<author>K Bretonnel Cohen</author>
<author>Wlodzislaw Duch</author>
</authors>
<title>A shared task involving multi-label classification of clinical free text.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on BioNLP</booktitle>
<pages>97--104</pages>
<contexts>
<context position="2917" citStr="Pestian et al., 2007" startWordPosition="446" endWordPosition="449"> extraction task as the task is to assign class labels to documents, and the training dataset contains labels just at this level. These special tasks lie somewhere between information extraction and document classification and require special approaches to solve them. We will call them Information-oriented document labeling throughout this paper. There are several application areas where information-oriented document labels are naturally present in an enormous amount like clinical records, Wikipedia categories and usergenerated tags of news. Previous evaluation campaigns (Uzuner et al., 2008; Pestian et al., 2007; Uzuner, 2009) demonstrated that information-oriented document labeling can be effectively performed by looking up indicator phrases which can be gathered by hand, by corpus statistics or in a hybrid way. However these campaigns also highlighted that the analysis of the local context of the indicator phrases is crucial. For instance, in the smoking habit detection task there are a few indicator words (e.g. smokes, cigarette) and the local context of their occurrences in texts should 759 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 759–770, Edin</context>
<context position="6943" citStr="Pestian et al., 2007" startWordPosition="1090" endWordPosition="1093">ork for joint CSD and document labeling, (iii) moreover our approach does not require a dedicated annotated training dataset for content shifters. 2 Related Work Information-oriented document classification tasks were first highlighted in the clinical domain where medical reports contain useful information about the patient in question, but labels are only available at the document (patient) level. The field of clinical NLP has been studied extensively since the 1990s (Larkey and Croft, 1995), but the most recent results are related to the shared task challenges organized relatively recently (Pestian et al., 2007; Uzuner et al., 2008; Uzuner, 2009). For example the first I2B2 challenge in 2006 (Uzuner et al., 2008) focused on the smoking habits of the patient, the CMC challenge in 2007 (Pestian et al., 2007) dealt with the problem of automatically constructing ICD coding systems and the second I2B2 challenge (Uzuner, 2009) addressed the classification of discharge summaries according to the question ”Who’s obese and what co-morbidities do they have?”. These challenges were dominated by entirely or partly rulebased systems that solved the tasks using indicator phrase lookup and incorporated explicit me</context>
<context position="11765" citStr="Pestian et al., 2007" startWordPosition="1840" endWordPosition="1843">1 summarizes the key statistical figures (the number of documents in the corpora, the size of the label sets along with the average number of tokens and label assignments per document) of the datasets used for the experimental evaluations. Table 1: The datasets used in our experiments. CMC Obes Soccer domain clinical clinical encycl. Itrainj 978 730 4850 levall 976 507 1736 #token/d 25 1387 389 #labels 45 16 12 #label/d 1.24 4.37 1.23 The CMC ICD Coding Dataset was originally prepared for a shared task challenge organized by the Computational Medicine Center (CMC) in Cincinatti, Ohio in 2007 (Pestian et al., 2007). It contains radiology reports along with document-level International Classification of Diseases (ICD) codes given by three human experts. ICD is a coding of diseases, signs, symptoms and abnormal findings. In our experiments we used the train/evaluation split of the shared task. The ICD coding guide states that negative or uncertain diagnosis should not be coded in any case. The corpus contains very short documents. For instance, the document HISTORY: Left lower chest pain. Rule-out 761 pneumonia. IMPRESSION: Normal chest. categories of Wikipedia, classifiers can be trained to tag unlabeled</context>
<context position="15049" citStr="Pestian et al., 2007" startWordPosition="2371" endWordPosition="2374">amples for content shift detector learning. has one label 786.50 (cough) as 486 (pneumonia) is ruled out. The main conclusion of the shared task in 2007 was that simple rule-based systems generally outperform bag-of-words-based machine learning models. The rules were extracted from ICD guidelines and/or from the training corpus using simple statistical measures, then they were checked or extended manually. Several systems of the challenge employed a negation and speculation detection submodule. The (manually highly fine-tuned) top systems of the CMC shared task achieved an F-measure of 88-89 (Pestian et al., 2007; Farkas and Szarvas, 2008). The I2B2 Obesity Dataset was also the subject of a clinical natural language processing shared task. The challenge in 2008 focused on analyzing clinical discharge summary texts and addressed the following question: ”Who is obese and what comorbidities do they have?” (Uzuner, 2009). Target diseases (document labels) included obesity and its 15 most frequent co-morbidities exhibited by patients. In our experiments, we used the same train/evaluation split as that of the shared task. Here a special aspect of the corpus is that the documents are semi-structured, i.e. th</context>
</contexts>
<marker>Pestian, Brew, Matykiewicz, Hovermale, Johnson, Cohen, Duch, 2007</marker>
<rawString>John P. Pestian, Chris Brew, Pawel Matykiewicz, DJ Hovermale, Neil Johnson, K. Bretonnel Cohen, and Wlodzislaw Duch. 2007. A shared task involving multi-label classification of clinical free text. In Proceedings of the ACL Workshop on BioNLP 2007, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Sauri</author>
<author>James Pustejovsky</author>
</authors>
<title>Factbank: a corpus annotated with event factuality.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="8735" citStr="Sauri and Pustejovsky, 2009" startWordPosition="1364" endWordPosition="1367">hift detection approaches focus on a particular class of language phenomena, especially on negation and hedge recognitions. Available tools work mainly on clinical and biological domains. The first systems were fully hand-crafted (Light et al., 2004; Friedman et al., 1994; Chapman et al., 2007) without any empirical evaluation on a dedicated corpus. Recently, there have been several corpora published with manual sentence-, event- or token-level annotation for negation, certainty and factuality in the biological (Medlock and Briscoe, 2007; Vincze et al., 2008), newswire (Strassel et al., 2008; Sauri and Pustejovsky, 2009) and encyclopedical (Farkas et al., 2010) domains. Exploiting these corpora, machine learning models were also developed. Solving the sentencelevel task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or nonspeculative. Szarvas (2008) extended their methodology to use n-gram features and a semi-supervised selection of the keyword features. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. For in</context>
<context position="10031" citStr="Sauri and Pustejovsky (2009)" startWordPosition="1560" endWordPosition="1563">developed scope – i.e. content shifted text spans – detectors for negation and speculation following a supervised sequence labeling approach, while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic patterns. The goal of the CoNLL 2010 Shared Task (Farkas et al., 2010) was to develop linguistic scope detectors as well. The participants usually followed a supervised sequence labeling approach or used a rule-based system that exploits syntactic patterns. The approach of classifying identified events into whether they fall under negation or speculation was followed by Sauri and Pustejovsky (2009) and the participants of the BioNLP’09 Shared Task (Kim et al., 2009). Here the systems investigated the syntax path between the event trigger and a cue word (which came from a small lexicon) (Kilicoglu and Bergler, 2009; Aramaki et al., 2009). Our approach differs from the previous works fundamentally. We deal with the two tasks (information-oriented document classification and content shift detection) together and introduce a colearning approach for them. Our approach handles content shifters in a data-driven and generalized way i.e. it is not specialized for a certain class of language phen</context>
</contexts>
<marker>Sauri, Pustejovsky, 2009</marker>
<rawString>Roser Sauri and James Pustejovsky. 2009. Factbank: a corpus annotated with event factuality. Language Resources and Evaluation, 43(3):227–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Sch¨onhofen</author>
</authors>
<title>Identifying document topics using the wikipedia category network.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web Intelligence,</booktitle>
<pages>456--462</pages>
<marker>Sch¨onhofen, 2006</marker>
<rawString>Peter Sch¨onhofen. 2006. Identifying document topics using the wikipedia category network. In Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web Intelligence, pages 456–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ill´es Solt</author>
</authors>
<title>Domonkos Tikk, Viktor G´al, and Zsolt Tivadar Kardkov´acs.</title>
<date>2009</date>
<journal>J. Am. Med. Inform. Assoc.,</journal>
<volume>16</volume>
<pages>584</pages>
<marker>Solt, 2009</marker>
<rawString>Ill´es Solt, Domonkos Tikk, Viktor G´al, and Zsolt Tivadar Kardkov´acs. 2009. Semantic classification of diseases in discharge summaries using a context-aware rulebased classifier. J. Am. Med. Inform. Assoc., 16:580– 584, jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Strassel</author>
<author>Mark A Przybocki</author>
<author>Kay Peterson</author>
<author>Zhiyi Song</author>
<author>Kazuaki Maeda</author>
</authors>
<title>Linguistic resources and evaluation techniques for evaluation of cross-document automatic content extraction.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="8705" citStr="Strassel et al., 2008" startWordPosition="1360" endWordPosition="1363">ion. Existing content shift detection approaches focus on a particular class of language phenomena, especially on negation and hedge recognitions. Available tools work mainly on clinical and biological domains. The first systems were fully hand-crafted (Light et al., 2004; Friedman et al., 1994; Chapman et al., 2007) without any empirical evaluation on a dedicated corpus. Recently, there have been several corpora published with manual sentence-, event- or token-level annotation for negation, certainty and factuality in the biological (Medlock and Briscoe, 2007; Vincze et al., 2008), newswire (Strassel et al., 2008; Sauri and Pustejovsky, 2009) and encyclopedical (Farkas et al., 2010) domains. Exploiting these corpora, machine learning models were also developed. Solving the sentencelevel task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or nonspeculative. Szarvas (2008) extended their methodology to use n-gram features and a semi-supervised selection of the keyword features. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags </context>
</contexts>
<marker>Strassel, Przybocki, Peterson, Song, Maeda, 2008</marker>
<rawString>Stephanie Strassel, Mark A. Przybocki, Kay Peterson, Zhiyi Song, and Kazuaki Maeda. 2008. Linguistic resources and evaluation techniques for evaluation of cross-document automatic content extraction. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>Hedge Classification in Biomedical Texts with a Weakly Supervised Selection of Keywords.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08,</booktitle>
<pages>281--289</pages>
<contexts>
<context position="9056" citStr="Szarvas (2008)" startWordPosition="1416" endWordPosition="1417">edicated corpus. Recently, there have been several corpora published with manual sentence-, event- or token-level annotation for negation, certainty and factuality in the biological (Medlock and Briscoe, 2007; Vincze et al., 2008), newswire (Strassel et al., 2008; Sauri and Pustejovsky, 2009) and encyclopedical (Farkas et al., 2010) domains. Exploiting these corpora, machine learning models were also developed. Solving the sentencelevel task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or nonspeculative. Szarvas (2008) extended their methodology to use n-gram features and a semi-supervised selection of the keyword features. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. For in-sentence negation and speculation detection, Morante et al. (2009) developed scope – i.e. content shifted text spans – detectors for negation and speculation following a supervised sequence labeling approach, while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic patterns. The goal of the </context>
<context position="15076" citStr="Szarvas, 2008" startWordPosition="2377" endWordPosition="2378"> learning. has one label 786.50 (cough) as 486 (pneumonia) is ruled out. The main conclusion of the shared task in 2007 was that simple rule-based systems generally outperform bag-of-words-based machine learning models. The rules were extracted from ICD guidelines and/or from the training corpus using simple statistical measures, then they were checked or extended manually. Several systems of the challenge employed a negation and speculation detection submodule. The (manually highly fine-tuned) top systems of the CMC shared task achieved an F-measure of 88-89 (Pestian et al., 2007; Farkas and Szarvas, 2008). The I2B2 Obesity Dataset was also the subject of a clinical natural language processing shared task. The challenge in 2008 focused on analyzing clinical discharge summary texts and addressed the following question: ”Who is obese and what comorbidities do they have?” (Uzuner, 2009). Target diseases (document labels) included obesity and its 15 most frequent co-morbidities exhibited by patients. In our experiments, we used the same train/evaluation split as that of the shared task. Here a special aspect of the corpus is that the documents are semi-structured, i.e. they contain headings like di</context>
<context position="26901" citStr="Szarvas (2008)" startWordPosition="4311" endWordPosition="4312">tion-oriented document labeling. In our experiments, we applied the sentence splitter and lemmatizer implementation of the MorphAdorner package6 and the Stanford tokenizer and lexicalized PCFG parser (Klein and Manning, 2003)7. 6morphadorner.northwestern.edu/ 7The JAVA implementation of the entire framework and 5.1 Content Shifter Learning Results In order to evaluate content shift detection as an individual task, a set of indicator phrases have to be fixed as an input to the CSD. We used manually collected indicator phrases for each label for each dataset. We utilized the terms of Farkas and Szarvas (2008) and Farkas et al. (2009) collected for the CMC and Obesity datasets, respectively and club names for the Soccer dataset in our first branch of experiments. Note that the clinical term sets here have been manually fine-tuned as they were developed for participating systems of the shared tasks of the corpora. Based on the occurrences of these fixed indicator phrases, CSD training datasets were built from the local contexts of the three datasets and binary classification was carried out by using MaxEnt. Table 3 shows the results achieved by the learnt CSDs using the bag-of-word feature represent</context>
<context position="28721" citStr="Szarvas (2008)" startWordPosition="4613" endWordPosition="4614">Table 3 (we refer to it as content shifted sentence detection baseline (CSSDB) later on) shows the results archived by the method which predicts every sentence to be altered which contain any cue phrases for negation, modality and different experiencer. Note that off-the-shelf tools are available just for these types of content shifters. We collected cue phrases for such a content shifted sentence detection from the dataset adapters can be found as the supplementary material 765 works of Chapman et al. (2007), Light et al. (2004) and Vincze et al. (2008) and from the experiments of Farkas and Szarvas (2008) and Farkas et al. (2009). For the CMC and Obesity tasks, hand-crafted in-sentence CSDs were also available (Farkas and Szarvas, 2008; Farkas et al., 2009), i.e. they apply heuristics – which usually tries to recognise clause boundaries – for determining the scope of a negaton/modality cue. This CSD is more fine-grained than the sentence-level one as here a part of a sentence can be detected as altered while other parts as non-altered. The results of these detectors – two different CSDs, both highly fine-tuned for the corresponding shared task – are listed in the last row of Table 3. On the CM</context>
</contexts>
<marker>Szarvas, 2008</marker>
<rawString>Gy¨orgy Szarvas. 2008. Hedge Classification in Biomedical Texts with a Weakly Supervised Selection of Keywords. In Proceedings ofACL-08, pages 281–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Uzuner</author>
<author>Ira Goldstein</author>
<author>Yuan Luo</author>
<author>Isaac Kohane</author>
</authors>
<title>Identifying Patient Smoking Status from Medical Discharge Records.</title>
<date>2008</date>
<journal>Journal ofAmerican Medical Informatics Association,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="1601" citStr="Uzuner et al., 2008" startWordPosition="239" endWordPosition="242">n three document labeling tasks. 1 Introduction There are special document multi-labeling tasks where the target labels refer to a specific piece of information extractable from the document instead of the overall topic of the document. In these kinds of tasks the target information is usually an attribute or relation related to the target entity (usually a person or an organisation) of the document in question, but the task is to assign class labels at the document (entity) level. For example, the smoking habits of the patients are frequently discussed in the textual parts of clinical notes (Uzuner et al., 2008). In this case the task is to find specific information in the text – i.e. the patient in question is a smoker, past smoker, non-smoker – but at the end an application has to assign labels to the documents(patients). Similarly, the soccer club names where a sportsman played for are document(sportman)-level labels in Wikipedia articles expressed by the Wikipedia categories. The target information in these tasks is usually just mentioned in the document and much of the document is irrelevant for this information request in contrast to standard document classification tasks where the goal is to i</context>
<context position="2895" citStr="Uzuner et al., 2008" startWordPosition="442" endWordPosition="445"> standard information extraction task as the task is to assign class labels to documents, and the training dataset contains labels just at this level. These special tasks lie somewhere between information extraction and document classification and require special approaches to solve them. We will call them Information-oriented document labeling throughout this paper. There are several application areas where information-oriented document labels are naturally present in an enormous amount like clinical records, Wikipedia categories and usergenerated tags of news. Previous evaluation campaigns (Uzuner et al., 2008; Pestian et al., 2007; Uzuner, 2009) demonstrated that information-oriented document labeling can be effectively performed by looking up indicator phrases which can be gathered by hand, by corpus statistics or in a hybrid way. However these campaigns also highlighted that the analysis of the local context of the indicator phrases is crucial. For instance, in the smoking habit detection task there are a few indicator words (e.g. smokes, cigarette) and the local context of their occurrences in texts should 759 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processin</context>
<context position="6964" citStr="Uzuner et al., 2008" startWordPosition="1094" endWordPosition="1097">document labeling, (iii) moreover our approach does not require a dedicated annotated training dataset for content shifters. 2 Related Work Information-oriented document classification tasks were first highlighted in the clinical domain where medical reports contain useful information about the patient in question, but labels are only available at the document (patient) level. The field of clinical NLP has been studied extensively since the 1990s (Larkey and Croft, 1995), but the most recent results are related to the shared task challenges organized relatively recently (Pestian et al., 2007; Uzuner et al., 2008; Uzuner, 2009). For example the first I2B2 challenge in 2006 (Uzuner et al., 2008) focused on the smoking habits of the patient, the CMC challenge in 2007 (Pestian et al., 2007) dealt with the problem of automatically constructing ICD coding systems and the second I2B2 challenge (Uzuner, 2009) addressed the classification of discharge summaries according to the question ”Who’s obese and what co-morbidities do they have?”. These challenges were dominated by entirely or partly rulebased systems that solved the tasks using indicator phrase lookup and incorporated explicit mechanisms for detectin</context>
</contexts>
<marker>Uzuner, Goldstein, Luo, Kohane, 2008</marker>
<rawString>O. Uzuner, Ira Goldstein, Yuan Luo, and Isaac Kohane. 2008. Identifying Patient Smoking Status from Medical Discharge Records. Journal ofAmerican Medical Informatics Association, 15(1):14–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozlem Uzuner</author>
</authors>
<title>Recognizing obesity and comorbidities in sparse data.</title>
<date>2009</date>
<journal>Journal of American Medical Informatics Association,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="2932" citStr="Uzuner, 2009" startWordPosition="450" endWordPosition="451">e task is to assign class labels to documents, and the training dataset contains labels just at this level. These special tasks lie somewhere between information extraction and document classification and require special approaches to solve them. We will call them Information-oriented document labeling throughout this paper. There are several application areas where information-oriented document labels are naturally present in an enormous amount like clinical records, Wikipedia categories and usergenerated tags of news. Previous evaluation campaigns (Uzuner et al., 2008; Pestian et al., 2007; Uzuner, 2009) demonstrated that information-oriented document labeling can be effectively performed by looking up indicator phrases which can be gathered by hand, by corpus statistics or in a hybrid way. However these campaigns also highlighted that the analysis of the local context of the indicator phrases is crucial. For instance, in the smoking habit detection task there are a few indicator words (e.g. smokes, cigarette) and the local context of their occurrences in texts should 759 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 759–770, Edinburgh, Scotland</context>
<context position="6979" citStr="Uzuner, 2009" startWordPosition="1098" endWordPosition="1099">ii) moreover our approach does not require a dedicated annotated training dataset for content shifters. 2 Related Work Information-oriented document classification tasks were first highlighted in the clinical domain where medical reports contain useful information about the patient in question, but labels are only available at the document (patient) level. The field of clinical NLP has been studied extensively since the 1990s (Larkey and Croft, 1995), but the most recent results are related to the shared task challenges organized relatively recently (Pestian et al., 2007; Uzuner et al., 2008; Uzuner, 2009). For example the first I2B2 challenge in 2006 (Uzuner et al., 2008) focused on the smoking habits of the patient, the CMC challenge in 2007 (Pestian et al., 2007) dealt with the problem of automatically constructing ICD coding systems and the second I2B2 challenge (Uzuner, 2009) addressed the classification of discharge summaries according to the question ”Who’s obese and what co-morbidities do they have?”. These challenges were dominated by entirely or partly rulebased systems that solved the tasks using indicator phrase lookup and incorporated explicit mechanisms for detecting speculation a</context>
<context position="15359" citStr="Uzuner, 2009" startWordPosition="2421" endWordPosition="2422">ining corpus using simple statistical measures, then they were checked or extended manually. Several systems of the challenge employed a negation and speculation detection submodule. The (manually highly fine-tuned) top systems of the CMC shared task achieved an F-measure of 88-89 (Pestian et al., 2007; Farkas and Szarvas, 2008). The I2B2 Obesity Dataset was also the subject of a clinical natural language processing shared task. The challenge in 2008 focused on analyzing clinical discharge summary texts and addressed the following question: ”Who is obese and what comorbidities do they have?” (Uzuner, 2009). Target diseases (document labels) included obesity and its 15 most frequent co-morbidities exhibited by patients. In our experiments, we used the same train/evaluation split as that of the shared task. Here a special aspect of the corpus is that the documents are semi-structured, i.e. they contain headings like discharge medications and admit diagnosis. By pasting the given heading to the beginning of each sentence, we incorporated it into the local context. The top performing systems of the shared task employed mostly hand-crafted rules for indicator selection and for negation and uncertain</context>
</contexts>
<marker>Uzuner, 2009</marker>
<rawString>Ozlem Uzuner. 2009. Recognizing obesity and comorbidities in sparse data. Journal of American Medical Informatics Association, 16(4):561–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Gy¨orgy Szarvas</author>
<author>Rich´ard Farkas</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
</authors>
<title>The BioScope Corpus: Biomedical Texts Annotated for Uncertainty, Negation and their Scopes.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<pages>11--9</pages>
<marker>Vincze, Szarvas, Farkas, M´ora, Csirik, 2008</marker>
<rawString>Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas, Gy¨orgy M´ora, and J´anos Csirik. 2008. The BioScope Corpus: Biomedical Texts Annotated for Uncertainty, Negation and their Scopes. BMC Bioinformatics, 9(Suppl 11):S9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="34912" citStr="Witten and Frank, 1999" startWordPosition="5691" endWordPosition="5694">sk-specific relations). 5.2 Document Labeling Results The second branch of experiments investigated the added value of CSDs in information-oriented document labeling tasks. Table 4 summarizes the results we got on the three datasets using the microaveraged Fβ=1 of assigned labels (positive class). As baseline systems we trained binary SVMs with a linear kernel, MaxEnts and PARTs – a rulelearner classification algorithm (Frank and Witten, 1998) – for each label using the bag-of-word representation of the documents (implementations of SVMligth (Joachims, 1999), MALLET (McCallum, 2002) and WEKA (Witten and Frank, 1999) were used). The first two learners are popular choices for document classification, while the third is similar to our simple indicator selection procedure. We did not tuned the parameters of the classifiers, we used the default ones everywhere. To have a fair comparison, we applied to preprocessing steps on dataset of these document classifiers. First, we removed from the training and evaluation raw documents which were predicted to be altered by CSSDB. Second, as our indicator selection phrase can be regarded as a special feature selection method, we carried out an Information Gain-based fea</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Ian H. Witten and Eibe Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>