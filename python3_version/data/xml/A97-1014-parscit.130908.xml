<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.996427">
An Annotation Scheme for Free Word Order Languages
</title>
<author confidence="0.994315">
Wojciech Skut, Brigitte Krenn, Thorsten Brants, Hans Uszkoreit
</author>
<affiliation confidence="0.963895">
Universitat des Saarlandes
</affiliation>
<address confidence="0.791566">
66041 Saarbriicken, Germany
</address>
<email confidence="0.927212">
{skut,krenn,brants,uszkoreit}Ocoli.uni-sb.de
</email>
<sectionHeader confidence="0.996291" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.978263916666667">
We describe an annotation scheme and a
tool developed for creating linguistically
annotated corpora for non-configurational
languages. Since the requirements for such
a formalism differ from those posited for
configurational languages, several featu-
res have been added, influencing the ar-
chitecture of the scheme. The resulting
scheme reflects a stratificational notion of
language, and makes only minimal assump-
tions about the interrelation of the particu-
•lar representational strata.
</bodyText>
<sectionHeader confidence="0.999218" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982166666667">
The work reported in this paper aims at provi-
ding syntactically annotated corpora (treebanks&apos;)
for stochastic grammar induction. In particular, we
focus on several methodological issues concerning
the annotation of non-configurational languages.
In section 2, we examine the appropriateness of
existing annotation schemes. On the basis of these
considerations, we formulate several additional re-
quirements. A formalism complying with these re-
quirements is described in section 3. Section 4 deals
with the treatment of selected phenomena. For a
description of the annotation tool see section 5.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="introduction">
2 Motivation
</sectionHeader>
<subsectionHeader confidence="0.987842">
2.1 Linguistically Interpreted Corpora
</subsectionHeader>
<bodyText confidence="0.99993875">
Combining raw language data with linguistic infor-
mation offers a promising basis for the development
of new efficient and robust NLP methods. Real-
world texts annotated with different strata of lin-
guistic information can be used for grammar induc-
tion. The data-drivenness of this approach presents
a clear advantage over the traditional, idealised no-
tion of competence grammar.
</bodyText>
<subsectionHeader confidence="0.984081">
2.2 Existing Treebank Formats
</subsectionHeader>
<bodyText confidence="0.986390947368421">
Corpora annotated with syntactic structures are
commonly referred to as trctbank.5. Existing tree-
bank annotation schemes exhibit a fairly uniform
architecture, as they all have to meet the same basic
requirements, namely:
Descriptivity: Grammatical phenomena are to be
described rather than explained.
Theory-independence: Annotations should not
be influenced by theory-specific considerations.
Nevertheless, different theory-specific represen-
tations shall be recoverable from the annota-
tion, cf. (Marcus et al., 1994).
Multi-stratal representation: Clear separation
of different description levels is desirable.
Data-drivenness: The scheme must provide repre-
sentational means for all phenomena occurring
in texts. Disambiguation is based on human
processing skills (cf. (Marcus et. al., 1994),
(Sampson, 1995), (Black et. al. , 1996)).
The typical treebank architecture is as follows:
Structures: A context-free backbone is augmented
with trace-filler representations of non-local de-
pendencies. The underlying argument SirlteilITC
is not represented directly, but can be recovered
from the tree and trace-filler annotations.
Syntactic category is encoded in node labels.
Grammatical functions constitute a complex la-
bel system (cf. (Bies et al., 1995), (Sampson,
1995)).
Part-of-Speech is annotated at word level.
Thus the context-free constituent backbone plays
a pivotal role in the annotation scheme. Due to
the substantial differences between existing models
of constituent structure, the question arises of how
the theory independencf requirement, can be satis-
fied. At, this point the importance of the underlying
argument structure is emphasised (cf. (Lehmann et
al., 1996), (Marcus et al., 1994), (Sampson, 1995)).
</bodyText>
<subsectionHeader confidence="0.926243">
2.3 Language-Specific Features
</subsectionHeader>
<bodyText confidence="0.9997515">
Treebanks of the format, described in the above sec-
tion have been designed for English. Therefore, the
</bodyText>
<page confidence="0.997835">
88
</page>
<bodyText confidence="0.993446666666667">
solutions they offer are not always optimal for other
language types. As for free word order languages,
the following features may cause problems:
</bodyText>
<listItem confidence="0.983054166666667">
• local and non-local dependencies form a con-
tinuum rather than clear-cut classes of pheno-
mena;
• there exists a rich inventory of discontinuous
constituency types (topicalisation, scrambling,
clause union, pied piping, extraposition, split
NPs and PPs);
• word order variation is sensitive to many fac-
tors, e.g. category, syntactic function, focus;
• the grammaticality of different word permuta-
tions does not fit the traditional binary &apos;right-
wrong&apos; pattern; it rather forms a gradual tran-
</listItem>
<bodyText confidence="0.927544761904762">
sition between the two poles.
In light of these facts, serious difficulties can be ex-
pected arising from the structural component of the
existing formalisms. Due to the frequency of discon-
tinuous constituents in non-configurational langua-
ges, the filler-trace mechanism would be used very
often, yielding syntactic trees fairly different from
the underlying predicate-argument structures.
Consider the German sentence
(1) daran wird ihn Anna erkennen, &amp;di er weint
at-it will him Anna recognise that he cries
&apos;Anna will recognise him at his cry&apos;
A sample constituent structure is given below:
The fairly short sentence contains three non-local
dependencies, marked by co-references between tra-
ces and the corresponding nodes. This hybrid repre-
sentation makes the structure less transparent, and
therefore more difficult to annotate.
Apart from this rather technical problem, two fur-
ther arguments speak against phrase structure as the
structural pivot of the annotation scheme:
</bodyText>
<listItem confidence="0.972916285714286">
• Phrase structure models stipulated for non-
configurational languages differ strongly from
each other, presenting a challenge to the inten-
ded theory-independence of the scheme.
• Constituent structure serves as an explanatory
device for word order variation, winch is difficult,
to reconcile with the descriptivity requirement,.
</listItem>
<bodyText confidence="0.999888470588235">
Finally, the structural handling of free word or-
der means stating well-formedness constraints on
structures involving many trace-filler dependencies,
which has proved tedious. Since most methods of
handling discontinuous constituents make the for-
malism more powerful, the efficiency of processing
deteriorates, too.
An alternative solution is to make argument struc-
ture the main structural component of the forma-
lism. This assumption underlies a growing num-
ber of recent syntactic theories which give up the
context-free constituent backbone, cf. (McCawley,
1987), (Dowty, 1989), (Reape, 1993), (Kathol and
Pollard, 1995). These approaches provide an ade-
quate explanation for several issues problematic for
phrase-structure grammars (clause union, extrapo-
sition, diverse second-position phenomena).
</bodyText>
<subsectionHeader confidence="0.997346">
2.4 Annotating Argument Structure
</subsectionHeader>
<bodyText confidence="0.990944590909091">
Argument structure can be represented in terms of
unordered trees (with crossing branches). In order to
reduce their ambiguity potential, rather simple, &apos;flat&apos;
trees should be employed, while more information
can be expressed by a rich system of function labels.
Furthermore, the required theory-independence
means that the form of syntactic trees should not
reflect theory-specific assumptions, e.g. every syn-
tactic structure has a unique head. Thus, notions
such as head should be distinguished at the level of
syntactic functions rather than structures. This re-
quirement speaks against the traditional sort of de-
pendency trees, in which heads a,re represented as
non-terminal nodes, cf. (Hudson, 1984).
A tree meeting these requirements is given below:
Adv V NP NP V CPL NP V
damn wird ihn Anna erkennen, dais er &apos;vein!
Such a word order independent representation has
the advantage of all structural information being en-
coded in a single data structure. A uniform repre-
sentation of local and non-local dependencies makes
the structure more transparent&apos;.
</bodyText>
<sectionHeader confidence="0.989853" genericHeader="method">
3 The Annotation Scheme
</sectionHeader>
<subsectionHeader confidence="0.995268">
3.1 Architecture
</subsectionHeader>
<bodyText confidence="0.98557425">
We distinguish the following levels of representation:
&apos;A context-free constituent backbone can still be re-
covered from the surface string and argument structure
by reattaching &apos;extracted&apos; structures to a higher node.
</bodyText>
<equation confidence="0.964794">
V Ng-, NP V
wird ihn Anna e„ erkennen., dass er weint
</equation>
<page confidence="0.993057">
89
</page>
<bodyText confidence="0.998787857142857">
Argument structure, represented in terms of un-
ordered trees.
Grammatical functions, encoded in edge labels,
e.g. SB (subject), MO (modifier), HD (head).
Syntactic categories, expressed by category la-
bels assigned to non-terminal nodes and by
part-of-speech tags assigned to terminals.
</bodyText>
<subsectionHeader confidence="0.999983">
3.2 Argument Structure
</subsectionHeader>
<bodyText confidence="0.982293318181818">
A structure for (2) is shown in fig. 2.
(2) schade, daB kein Arzt anwesend ist, der
pity that no doctor present is who
sich a.uskennt
is competent
&apos;Pity that no competent doctor is here&apos;
Note that the root node does not have a head de-
scendant (HD) as the sentence is a predicative con-
struction consisting of a subject (SB) and a predi-
cate (PD) without a copula. The subject is itself a
sentence in which the copula (zA) does occur and is
assigned the tag HD&apos;.
The tree resembles traditional constituent struc-
tures. The difference is its word order independence:
structural units (&amp;quot;phrases&amp;quot;) need not be contiguous
substrings. For instance, the extraposed relative
clause (RC) is still treated as part of the subject
NP.
As the annotation scheme does not distinguish dif-
ferent bar levels or any similar intermediate catego-
ries, only a small set of node labels is needed (cur-
rently 16 tags, S, NP, AP ...).
</bodyText>
<subsectionHeader confidence="0.999561">
3.3 Grammatical Functions
</subsectionHeader>
<bodyText confidence="0.999974368421053">
Due to the rudimentary character of the argument
structure representations, a great deal of information
has to be expressed by grammatical functions. Their
further classification must reflect different kinds of
linguistic information: morphology (e.g., case, in-
flection), category, dependency type (complementa-
tion vs. modification), thematic role, etc.&apos;
However, there is a trade-off between the granu-
larity of information encoded in the labels and the
speed and accuracy of annotation. In order to avoid
inconsistencies, the corpus is annotated in two sta-
ges: basic annotation and nfirtellte714. While in the
first phase each annotator has to annotate structures
as well as categories and functions, the refinement
call be done separately for each representation level.
During the first phase, the focus is on annotating
correct structures and a coarse-grained classification
of grammatical functions, which represent the follo-
wing areas of information:
</bodyText>
<footnote confidence="0.7648614">
2CP stands for complementizer, OA for accusative
object and RC for relative clause. NK denotes a `kernel
NP&apos; component (v. section 4.1).
&apos;For an extensive use of grammatical functions cf.
(Karlsson et al., 1995), (Voutilainen, 1994).
</footnote>
<bodyText confidence="0.999077666666667">
Dependency type: complements are further clas-
sified according to features such as category
and case: clausal complements (OC), accusa-
tive objects (OA), datives (DA), etc. Modifiers
are assigned the label MO (further classification
with respect to thematic roles is planned). Se-
parate labels are defined for dependencies that
do not fit the complement/modifier dichotomy,
e.g., pre- (GL) and postnominal genitives (GR).
</bodyText>
<subsectionHeader confidence="0.988559">
Headedness versus non-headedness:
</subsectionHeader>
<bodyText confidence="0.998052384615385">
Headed and non-headed structures are distin-
guished by the presence or absence of a branch
labeled HD.
Morphological information: Another set of la-
bels represents morphological information. PM
stands for morphological particle, a label for
German infinitival Z7t and superlative am. Se-
parable verb prefixes are labeled SVP.
During the second annotation stage, the annota-
tion is enriched with information about thematic ro-
les, quantifier scope and anaphoric reference. As al-
ready mentioned, this is done separately for each of
the three information areas.
</bodyText>
<subsectionHeader confidence="0.997232">
3.4 Structure Sharing
</subsectionHeader>
<bodyText confidence="0.992016076923077">
A phrase or a lexical item can perform multiple func-
tions in a sentence. Consider (qui verbs where the
subject of the infinitival VP is not realised syntac-
tically, but co-referent with the subject or object. of
the matrix equi verb:
(3) er bat mich zu kommen
he asked me to come
(mich is the understood subject. of kommt,n). In such
cases, an additional edge is drawn from the embed-
ded VP node to the controller, thus changing the
syntactic tree into a graph. We call such additional
edges secondary links and represent them as dotted
lines, see fig. 4, showing the structure of (3).
</bodyText>
<sectionHeader confidence="0.936031" genericHeader="method">
4 Treatment of Selected Phenomena
</sectionHeader>
<bodyText confidence="0.999981333333333">
As theory-independence is one of our objectives, the
annotation scheme incorporates a number of widely
accepted linguistic analyses, especially in the area
of verbal, adverbial and adjectival syntax. However,
some other standard analysts turn out to be proble-
matic, mainly due to the partial, idealised character
of competence grammars, which often marginalise
or ignore such important. phenomena. as &apos;deficient&apos;
(e.g. headless) constructions, appositions, temporal
expressions, etc.
In the following paragraphs, we give annotations
for a number of such phenomena..
</bodyText>
<subsectionHeader confidence="0.97617">
4.1 Noun Phrases
</subsectionHeader>
<bodyText confidence="0.971614">
Most linguistic theories treat NPs as structures hea-
ded by a unique lexical item (noun). However, this
</bodyText>
<page confidence="0.990652">
90
</page>
<bodyText confidence="0.99430925">
idealised model needs several additional assumpti-
ons in order to account for such important pheno-
mena as complex nominal NP components (cf. (4))
or nominalised adjectives (cf. (5)).
</bodyText>
<table confidence="0.439098">
(4) my uncle Peter Smith
( 5 ) der sehr Ghickliche
the very happy
the very happy one&apos;
</table>
<bodyText confidence="0.999797333333333">
In (4), different theories make different headedness
predictions. In (5), either a lexical nominalisation
rule for the adjective Gliickliche is stipulated, or the
existence of an empty nominal head. Moreover, the
so-called DP analysis views the article der as the
head of the phrase. Further differences concern the
attachment of the degree modifier sehr.
Because of the intended theory-independence of
the scheme, we annotate only the common mini-
mum. We distinguish an NP kernel consisting of
determiners, adjective phrases and nouns. All com-
ponents of this kernel are assigned the label NK and
treated as sibling nodes.
The difference between the particular NK&apos;s lies in
the positional and part-of-speech information, which
is also sufficient to recover theory-specific structures
from our `underspecified&apos; representations. For in-
stance, the first, determiner among the NK&apos;s can be
treated as the specifier of the phrase. The head of
the phrase can be determined in a similar way ac-
cording to theory-specific assumptions.
In addition, a. number of clear-cut NP cornponents
can be defined outside that, juxtapositional kernel:
pre- and postnominal genitives (GL, GR), relative
clauses (RC), clausal and sentential complements
(OC). They are all treated as siblings of NK&apos;s re-
gardless of their position (in situ or extraposed).
</bodyText>
<subsectionHeader confidence="0.993612">
4.2 Attachment Ambiguities
</subsectionHeader>
<bodyText confidence="0.9999394375">
Adjunct attachment often gives rise to structural
ambiguities or structural uncertainty. However, full
or partial disambiguation takes place in context, and
the annotators do not consider unrealistic readings.
In addition, we have adopted a simple convention
for those cases in which context information is insuf-
ficient, for total disambiguation: the highest possible
attachment, site is chosen.
A similar convention ha.s been adopted for con-
structions in which scope ambiguities have syntac-
tic effects but a one-to-one correspondence between
scope and attachment. does not seem reasonable, cf.
focus particles such as only or also. If the scope of
such a word does not directly correspond to a tree
node, the word is attached to the lowest node domi-
nating all subconstituents appearing in its scope.
</bodyText>
<subsectionHeader confidence="0.976192">
4.3 Coordination
</subsectionHeader>
<bodyText confidence="0.987331407407407">
A problem for the rudimentary argument. structure
representations is the use of incomplete structures
in natural language, i.e. phenomena such as coor-
dination and ellipsis. Since a precise structural de-
scription of non-constituent coordination would re-
quire a. rich inventory of incomplete phrase types, we
have agreed on a sort of unde.rspe.cified representa-
tions: the coordinated units are assigned structures
in which missing lexical material is not represented
at the level of primary links. Fig. 3 shows the re-
presentation of the sentence:
(6) sic wurde von preuBischen Truppen besetzt
she was by Prussian troops occupied
und 1887 dem preuliischen Staat angegliedert
and 1887 to-the Prussian state incorporated
&apos;it was occupied by Prussian troops and incorpo-
rated into Prussia in 1887&apos;
The category of the coordination is labeled CVP
here, where C stands for coordination, and VP for
the actual category. This extra marking makes it
easy to distinguish between &apos;normal&apos; and coordina-
ted categories.
Multiple coordination as well as enumerations are
annotated in the same way. An explicit coordinating
conjunction need not be present.
Structure-sharing is expressed using secondary
links.
</bodyText>
<sectionHeader confidence="0.993053" genericHeader="method">
5 The Annotation Tool
</sectionHeader>
<subsectionHeader confidence="0.820341">
5.1 Requirements
</subsectionHeader>
<bodyText confidence="0.999987153846154">
The development of linguistically interpreted cor-
pora presents a laborious and time-consuming task.
In order to make the annotation process more effi-
cient, extra effort has been put. into the development
of an annotation tool.
The tool supports immediate graphical feedback
and automatic error checking. Since our scheme per-
mits crossing edges, visualisation as bracketing and
indentation would be insufficient.. Instead, the com-
plete structure should be represented.
The tool should also permit a convenient hand-
ling of node and edge labels. In particular, variable
tagsets and label collections should be allowed.
</bodyText>
<subsectionHeader confidence="0.992924">
5.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999920846153846">
As the need for certain functionalities becomes ob-
vious with growing annotation experience, we have
decided to implement the tool in two stages. In the
first phase, the main functionality for building and
displaying unordered trees is supplied. In the se-
cond phase, secondary links and additional structu-
ral functions are supported. The implementation of
the first phase as described in the following para-
graphs is completed.
As keyboard input, is more efficient than mouse
input (cf. (Lehmann et al., 1996)) most effort, has
been put in developing an efficient keyboard inter-
face. Menus are supported as a useful way of getting
</bodyText>
<page confidence="0.997549">
91
</page>
<bodyText confidence="0.999933333333333">
help on commands and labels. In addition to pure
annotation, we can attach comments to structures.
Figure 1 shows a screen dump of the tool. The
largest part of the window contains the graphical re-
presentation of the structure being annotated. The
following commands are available:
</bodyText>
<listItem confidence="0.9997504">
• group words and/or phrases to a new phrase;
• ungroup a phrase;
• change the name of a phrase or an edge;
• re-attach a node;
• generate the postscript output of a sentence.
</listItem>
<bodyText confidence="0.999326">
The three tagsets used by the annotation tool
(for words, phrases, and edges) are variable and are
stored together with the corpus. This allows easy
modification if needed. The tool checks the appro-
priateness of the input.
For the implementation, we used Tcl/Tk Version
4.1. The corpus is stored in a SQL database.
</bodyText>
<subsectionHeader confidence="0.960212">
5.3 Automation
</subsectionHeader>
<bodyText confidence="0.9999065">
The degree of automation increases with the amount
of data available. Sentences annotated in previous
steps are used as training material for further pro-
cessing. We distinguish five degrees of automation:
</bodyText>
<listItem confidence="0.983625615384616">
0) Completely manual annotation.
1) The user determines phrase boundaries and
syntactic categories (S, NP, etc.). The program
automatically assigns grammatical function la-
bels. The annotator can alter the assigned tags.
2) The user only determines the components of a
new phrase, the program determines its syntac-
tic category and the grammatical functions of
its elements. Again, the annotator has the op-
tion of altering the assigned tags.
3) Additionally, the program performs simple
bracketing, i.e., finds &apos;kernel&apos; phrases.
4) The tagger suggests partial or complete parses.
</listItem>
<bodyText confidence="0.9997684">
So far, about 1100 sentences of our corpus have
been annotated. This amount of data suffices as
training material to reliably assign the grammatical
functions if the user determines the elements of a
phrase and its type (step 1 of the list above).
</bodyText>
<subsectionHeader confidence="0.979462">
5.4 Assigning Grammatical Function
Labels
</subsectionHeader>
<bodyText confidence="0.999942833333333">
Grammatical functions are assigned using standard
statistical part-of-speech tagging methods (cf. e.g.
(Cutting et al., 1992) and (Feldweg, 1995)).
For a phrase Q with children of type T„..., Ta
and grammatical functions G„...,GA., we use the
lexical probabilities
</bodyText>
<subsubsectionHeader confidence="0.412589">
PQ(Gi
</subsubsectionHeader>
<bodyText confidence="0.9860905">
and the contextual (trigram) probabilities
The lexical and contextual probabilities are deter-
mined separately for each type of phrase. During
annotation, the highest rated grammatical function
labels Gi are calculated using the Viterbi algorithm
and assigned to the structure, i.e., we calculate
argma.x11 PQ (Ti 1Z-1, Ti.-2) PQ (Gi ITi).
To keep the human annotator from missing errors
made by the tagger, we additionally calculate the
strongest competitor for each label G. If its pro-
bability is close to the winner (closeness is defined
by a threshold on the quotient), the assignment is
regarded as unreliable, and the annotator is asked
to confirm the assignment.
For evaluation, the already annotated sentences
were divided into two disjoint sets, one for training
(90% of the corpus), the other one for testing (10%).
The procedure was repeated 10 times with different.
partitionings.
The tagger rates 90% of all assignments as reliable
and carries them out fully automatically. Accuracy
for these cases is 97%. Most errors are due to wrong
identification of the subject and different kinds of
objects in sentences and VPs. Accuracy of the unre-
liable 10% of assignments is 75%, i.e., the annotator
has to alter the choice in 1 of 4 cases when asked for
confirmation. Overall accuracy of the tagger is 95%.
Owing to the partial automation, the average an-
notation efficiency improves by 25% (from around 4
minutes to 3 minutes per sentence).
</bodyText>
<sectionHeader confidence="0.994933" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999613857142857">
As the annotation scheme described in this paper fo-
cusses on annotating argument structure rather than
constituent trees, it differs from existing treebanks in
several aspects. These differences can be illustrated
by a comparison with the Penn Treebank annotation
scheme. The following features of our formalism are
then of particular importance:
</bodyText>
<listItem confidence="0.99632525">
• simpler (i.e. &apos;flat&apos;) representation structures
• complete absence of empty categories
• no special mechanisms for handling disconti-
nuous constituency
</listItem>
<bodyText confidence="0.999942363636364">
The current tagset comprises only 16 node labels
and 34 function tags, yet a. finely grinned classifica-
tion will take place in the near future.
We have argued that the selected approach is bet-
ter suited for producing high quality interpreted cor-
pora in languages exhibiting free constituent order.
In general, the resulting interpreted data also are
closer to semantic annotation and more neutral with
respect to particular syntactic theories.
As modern linguistics is also becoming more aware
of the importance of larger sets of naturally occur-
</bodyText>
<page confidence="0.988784">
92
</page>
<table confidence="0.998925555555556">
—General: Sentence
Corpus:
Editor:
II Parser
No.: 4 / 1269 Last edited: Thorsten, 07/02/97, 17:39:29
Comment:
Origin:
RefCorpus Testkopie
Thorsten
Ok Reload Exit refcorp tt
IN
511
—
509ED DJ
an NP rog
507 0
lai
Go EI 504 505
0 rri
Es ADV eben2 PIAT 4 5 , ob6 El ADJD gerallig3 ist
PPER keine 5000 $, KOUS die VAFIN nur
0 spelt 3 Rolle ART 501 &lt;I&gt;.
VVFIN NN El cm 1mi gm
2 Musik8 0 - I &amp;quot; 4 Neues15 &amp;quot; 6 mu&amp;quot;s
NN 2 etwasi 3 1
$? ADV PIAT $( NN VMFIr
air 1 E&gt;1
</table>
<tableCaption confidence="0.331079">
Switching to sentence no. 4... Done.
</tableCaption>
<figureCaption confidence="0.998284">
Figure 1: Screen dump of the annotation tool
</figureCaption>
<figure confidence="0.99869715">
Next
+10
+100
Prey
Go to:
Filter
Matches: 0
-10
-100
Parentlabel:
Node no..
Parentlabel:
El
End
Next
Prey
,--Dependency:
Selection:
Command:
Execute
</figure>
<bodyText confidence="0.994127518518518">
ring data., interpreted corpora. are a. valuable re-
source for theoretical and descriptive linguistic re-
search. In addition the approach provides empiri-
cal material for psycholinguistic investigation, since
preferences for the choice of certain syntactic con-
structions, linea.rizations, and attachments that have
been observed in online experiments of language pro-
duction and comprehension can now be put in rela-
tion with the frequency of these alternatives in larger
amounts of texts.
Syntactically annotated corpora of German have
been missing until now. In the second phase of the
project Verbmobil a. treebank for :30,000 German
spoken sentences as well as for the same amount of
English and Japanese sentences will be created. We
will closely coordinate the further development of
our corpus with the annotation work in Verbmobil
and with other German efforts in corpus annotation.
Since the combinatorics of syntactic constructions
creates a. demand for very large corpora., efficiency of
annotation is an important. criterion for the success
of the developed methodology and tools. Our anno-
tation tool supplies efficient manipulation and im-
mediate visualization of argument structures. Par-
tial automation included in the current version si-
gnificantly reduces the manna.1 effort. Its extension
is subject to further investigations.
</bodyText>
<sectionHeader confidence="0.989912" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.436511">
This work is part of the DFG Sonderforschungs-
bereich 378 Rcsource-Adaptim Cogniiivc Proccsses,
</bodyText>
<subsubsectionHeader confidence="0.202828">
Project C3 C.:anew/T(7d Grammar P1oc6ssing.
</subsubsectionHeader>
<bodyText confidence="0.331196142857143">
We wish to thank Tania Avgustinova, Berthold
Crysmann, Lars Konieczny, Stephan Oepen, Karel
Oliva., Christian Weil3 and two anonymous reviewers
for their helpful comments on the content of this
paper. We also wish to thank Robert MacIntyre
and Ann Taylor for valuable discussions on the Penn
Treebank annotation. Special thanks go t,o Oliver
</bodyText>
<page confidence="0.996565">
93
</page>
<bodyText confidence="0.997347666666667">
Plaehn, who implemented the annotation tool, and
to our fearless annotators Roland Hendriks, Kerstin
Klockner, Thomas Schulz, and Bernd-Paul Simon.
</bodyText>
<sectionHeader confidence="0.943652" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994625166666667">
Geoffrey Sampson. 1995. English for the Compu-
ter. The SUSANNE Corpus and Analytic Scheme.
Clarendon Press, Oxford.
Atro Voutilainen. 1994. Designing a Parsing Gram-
mar. University of Helsinki, Dept. of General Lin-
guistics. Publications No. 22.
Ann Bies et al. 1995. Bracketing Guidelines for
Treebank II Style Penn Treebank Project. Techni-
cal report, University of Pennsylvania.
Ezra Black et al. 1996. Beyond Skeleton Par-
sing: Producing a Comprehensive Large-Scale
General-English Treebank With Full Grammati-
cal Analysis. In The 16th International Confe-
TCnCe on Computational Linguistics, pages 107 -
113, Copenhagen, Denmark.
Doug Cutting, Julian Kupiec, Jan Pedersen, and Pe-
nelope Sibun. 1992. A practical part-of-speech
tagger. In PrOC( cdiugs of the ,Yrd Confer( net on
Applied Natural Language Processing (ACL), pa-
ges 133-140.
David Dowty. 1989. Towards a minimalist theory
of syntactic structure. In Tilburg Conference on
Discontinuous Constituency.
Helmut Feldweg. 1995. Implementation and evalua-
tion of a German HMM for POS disambiguation.
In Proceedings of EACL-SIGDAT-95 Workshop,
Dublin, Ireland.
Richard Hudson. 1984. Word Granlinar. Basil
Blackwell Ltd.
Fred Karlsson, Atro Voutilainen„luha. Heikkila, and
Arto Anttila. 1995. Constraint Grammar. A
Language-Independent System for Parsing Unre-
stricted Text. Mouton de Gruyter, Berlin, New
York.
Kathol, Andreas and Carl Pollard. 1995. Extra.po-
sition via Complex Domain Formation. In Pro-
ceedings of Mt. 33&amp;quot; Annual Meeting of the ACL,
pages 174-180, Cambridge, MA. Association for
Computational Linguistics.
Sabine Lehmann et al. 1996. TSNLP - Test Sui-
tes for Natural Language Processing. In Th( 16th
International Conference on Computational Lin-
guistics, pages 711 - 717, Copenhagen, Denmark.
Mitchell Marcus et al. 1994. The Penn Treebank:
Annotating Predicate Argument Structure. In
Proceedings of the Human Language Technology
Workshop, San Francisco. Morgan Kaufmann.
James McCawley. 1987. Some additional evidence
for discontinuity. In Huck and Ojeda (eds.), Di.s-
continuous Constituency: Syntax and Semantics,
pp 185-200. New York, Academic Press.
Mike Reape. 1993. A Formal Theory of Word Or-
der: A Case Study in We st G f.711InniC . Ph .D the-
sis, University of Edinburgh.
</reference>
<page confidence="0.994064">
94
</page>
<reference confidence="0.537746">
Schade da&amp;quot;s kein A zt anwesend ist der sich auskennt
ADJD S. KOUS PIAT NN ADJD VAFIN S. PRELS PRF VVFIN S.
</reference>
<figureCaption confidence="0.999976333333333">
Figure 2: Headed and non-headed structures, extraposition
Figure 3: Coordination
Figure 4: Equi construction
</figureCaption>
<figure confidence="0.997448105263158">
preu&apos;sischen
Sie wu de von
PPER VAFIN APPR ADJA
Truppen besetzt und 1887
NN VVPP KON CARD
0
On
dem preu-sischen
ART ADJA
Staatsverband angeghederl
NN VVPP
IJ
bat
VVFIN PPER PTKZU
kom men
VVINF S.
Er
PPER
ZU
</figure>
<page confidence="0.953345">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.927834">
<title confidence="0.999793">An Annotation Scheme for Free Word Order Languages</title>
<author confidence="0.979184">Wojciech Skut</author>
<author confidence="0.979184">Brigitte Krenn</author>
<author confidence="0.979184">Thorsten Brants</author>
<author confidence="0.979184">Hans Uszkoreit</author>
<affiliation confidence="0.995761">Universitat des Saarlandes</affiliation>
<address confidence="0.997155">66041 Saarbriicken, Germany</address>
<abstract confidence="0.996230384615385">We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>English for the Computer. The SUSANNE Corpus and Analytic Scheme.</title>
<date>1995</date>
<publisher>Clarendon Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="2587" citStr="Sampson, 1995" startWordPosition="352" endWordPosition="353">sic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained. Theory-independence: Annotations should not be influenced by theory-specific considerations. Nevertheless, different theory-specific representations shall be recoverable from the annotation, cf. (Marcus et al., 1994). Multi-stratal representation: Clear separation of different description levels is desirable. Data-drivenness: The scheme must provide representational means for all phenomena occurring in texts. Disambiguation is based on human processing skills (cf. (Marcus et. al., 1994), (Sampson, 1995), (Black et. al. , 1996)). The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies. The underlying argument SirlteilITC is not represented directly, but can be recovered from the tree and trace-filler annotations. Syntactic category is encoded in node labels. Grammatical functions constitute a complex label system (cf. (Bies et al., 1995), (Sampson, 1995)). Part-of-Speech is annotated at word level. Thus the context-free constituent backbone plays a pivotal role in the annotation scheme. Due t</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>Geoffrey Sampson. 1995. English for the Computer. The SUSANNE Corpus and Analytic Scheme. Clarendon Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>Designing a Parsing Grammar.</title>
<date>1994</date>
<volume>22</volume>
<publisher>Publications</publisher>
<institution>University of Helsinki, Dept. of General Linguistics.</institution>
<contexts>
<context position="10215" citStr="Voutilainen, 1994" startWordPosition="1501" endWordPosition="1502">notation and nfirtellte714. While in the first phase each annotator has to annotate structures as well as categories and functions, the refinement call be done separately for each representation level. During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: 2CP stands for complementizer, OA for accusative object and RC for relative clause. NK denotes a `kernel NP&apos; component (v. section 4.1). &apos;For an extensive use of grammatical functions cf. (Karlsson et al., 1995), (Voutilainen, 1994). Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc. Modifiers are assigned the label MO (further classification with respect to thematic roles is planned). Separate labels are defined for dependencies that do not fit the complement/modifier dichotomy, e.g., pre- (GL) and postnominal genitives (GR). Headedness versus non-headedness: Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD. Morphological information: Another set of l</context>
</contexts>
<marker>Voutilainen, 1994</marker>
<rawString>Atro Voutilainen. 1994. Designing a Parsing Grammar. University of Helsinki, Dept. of General Linguistics. Publications No. 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
</authors>
<title>Bracketing Guidelines for Treebank II Style Penn Treebank Project.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Bies, 1995</marker>
<rawString>Ann Bies et al. 1995. Bracketing Guidelines for Treebank II Style Penn Treebank Project. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
</authors>
<title>Beyond Skeleton Parsing: Producing a Comprehensive Large-Scale General-English Treebank With Full Grammatical Analysis.</title>
<date>1996</date>
<booktitle>In The 16th International ConfeTCnCe on Computational Linguistics,</booktitle>
<pages>107--113</pages>
<location>Copenhagen, Denmark.</location>
<marker>Black, 1996</marker>
<rawString>Ezra Black et al. 1996. Beyond Skeleton Parsing: Producing a Comprehensive Large-Scale General-English Treebank With Full Grammatical Analysis. In The 16th International ConfeTCnCe on Computational Linguistics, pages 107 -113, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A practical part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In PrOC( cdiugs of the ,Yrd Confer( net on Applied Natural Language Processing (ACL),</booktitle>
<pages>133--140</pages>
<contexts>
<context position="19478" citStr="Cutting et al., 1992" startWordPosition="2940" endWordPosition="2943"> Again, the annotator has the option of altering the assigned tags. 3) Additionally, the program performs simple bracketing, i.e., finds &apos;kernel&apos; phrases. 4) The tagger suggests partial or complete parses. So far, about 1100 sentences of our corpus have been annotated. This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above). 5.4 Assigning Grammatical Function Labels Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g. (Cutting et al., 1992) and (Feldweg, 1995)). For a phrase Q with children of type T„..., Ta and grammatical functions G„...,GA., we use the lexical probabilities PQ(Gi and the contextual (trigram) probabilities The lexical and contextual probabilities are determined separately for each type of phrase. During annotation, the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure, i.e., we calculate argma.x11 PQ (Ti 1Z-1, Ti.-2) PQ (Gi ITi). To keep the human annotator from missing errors made by the tagger, we additionally calculate the strongest competi</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In PrOC( cdiugs of the ,Yrd Confer( net on Applied Natural Language Processing (ACL), pages 133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Towards a minimalist theory of syntactic structure.</title>
<date>1989</date>
<booktitle>In Tilburg Conference on Discontinuous Constituency.</booktitle>
<contexts>
<context position="6159" citStr="Dowty, 1989" startWordPosition="875" endWordPosition="876">h the descriptivity requirement,. Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious. Since most methods of handling discontinuous constituents make the formalism more powerful, the efficiency of processing deteriorates, too. An alternative solution is to make argument structure the main structural component of the formalism. This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone, cf. (McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995). These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union, extraposition, diverse second-position phenomena). 2.4 Annotating Argument Structure Argument structure can be represented in terms of unordered trees (with crossing branches). In order to reduce their ambiguity potential, rather simple, &apos;flat&apos; trees should be employed, while more information can be expressed by a rich system of function labels. Furthermore, the required theory-independence means that the form of syntactic tre</context>
</contexts>
<marker>Dowty, 1989</marker>
<rawString>David Dowty. 1989. Towards a minimalist theory of syntactic structure. In Tilburg Conference on Discontinuous Constituency.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Feldweg</author>
</authors>
<title>Implementation and evaluation of a German HMM for POS disambiguation.</title>
<date>1995</date>
<booktitle>In Proceedings of EACL-SIGDAT-95 Workshop,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="19498" citStr="Feldweg, 1995" startWordPosition="2945" endWordPosition="2946">he option of altering the assigned tags. 3) Additionally, the program performs simple bracketing, i.e., finds &apos;kernel&apos; phrases. 4) The tagger suggests partial or complete parses. So far, about 1100 sentences of our corpus have been annotated. This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above). 5.4 Assigning Grammatical Function Labels Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g. (Cutting et al., 1992) and (Feldweg, 1995)). For a phrase Q with children of type T„..., Ta and grammatical functions G„...,GA., we use the lexical probabilities PQ(Gi and the contextual (trigram) probabilities The lexical and contextual probabilities are determined separately for each type of phrase. During annotation, the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure, i.e., we calculate argma.x11 PQ (Ti 1Z-1, Ti.-2) PQ (Gi ITi). To keep the human annotator from missing errors made by the tagger, we additionally calculate the strongest competitor for each label G</context>
</contexts>
<marker>Feldweg, 1995</marker>
<rawString>Helmut Feldweg. 1995. Implementation and evaluation of a German HMM for POS disambiguation. In Proceedings of EACL-SIGDAT-95 Workshop, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<title>Word Granlinar.</title>
<date>1984</date>
<publisher>Basil Blackwell Ltd.</publisher>
<contexts>
<context position="7118" citStr="Hudson, 1984" startWordPosition="1011" endWordPosition="1012"> In order to reduce their ambiguity potential, rather simple, &apos;flat&apos; trees should be employed, while more information can be expressed by a rich system of function labels. Furthermore, the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions, e.g. every syntactic structure has a unique head. Thus, notions such as head should be distinguished at the level of syntactic functions rather than structures. This requirement speaks against the traditional sort of dependency trees, in which heads a,re represented as non-terminal nodes, cf. (Hudson, 1984). A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen, dais er &apos;vein! Such a word order independent representation has the advantage of all structural information being encoded in a single data structure. A uniform representation of local and non-local dependencies makes the structure more transparent&apos;. 3 The Annotation Scheme 3.1 Architecture We distinguish the following levels of representation: &apos;A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching &apos;extracted&apos; structures to a </context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>Richard Hudson. 1984. Word Granlinar. Basil Blackwell Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heikkila</author>
<author>Arto Anttila</author>
</authors>
<title>Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter,</title>
<date>1995</date>
<location>Berlin, New York.</location>
<marker>Heikkila, Anttila, 1995</marker>
<rawString>Fred Karlsson, Atro Voutilainen„luha. Heikkila, and Arto Anttila. 1995. Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter, Berlin, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Kathol</author>
<author>Carl Pollard</author>
</authors>
<title>Extra.position via Complex Domain Formation.</title>
<date>1995</date>
<booktitle>In Proceedings of Mt. 33&amp;quot; Annual Meeting of the ACL,</booktitle>
<pages>174--180</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="6202" citStr="Kathol and Pollard, 1995" startWordPosition="879" endWordPosition="882">t,. Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious. Since most methods of handling discontinuous constituents make the formalism more powerful, the efficiency of processing deteriorates, too. An alternative solution is to make argument structure the main structural component of the formalism. This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone, cf. (McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995). These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union, extraposition, diverse second-position phenomena). 2.4 Annotating Argument Structure Argument structure can be represented in terms of unordered trees (with crossing branches). In order to reduce their ambiguity potential, rather simple, &apos;flat&apos; trees should be employed, while more information can be expressed by a rich system of function labels. Furthermore, the required theory-independence means that the form of syntactic trees should not reflect theory-specific assum</context>
</contexts>
<marker>Kathol, Pollard, 1995</marker>
<rawString>Kathol, Andreas and Carl Pollard. 1995. Extra.position via Complex Domain Formation. In Proceedings of Mt. 33&amp;quot; Annual Meeting of the ACL, pages 174-180, Cambridge, MA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Lehmann</author>
</authors>
<title>TSNLP - Test Suites for Natural Language Processing.</title>
<date>1996</date>
<booktitle>In Th( 16th International Conference on Computational Linguistics,</booktitle>
<pages>711--717</pages>
<location>Copenhagen, Denmark.</location>
<marker>Lehmann, 1996</marker>
<rawString>Sabine Lehmann et al. 1996. TSNLP - Test Suites for Natural Language Processing. In Th( 16th International Conference on Computational Linguistics, pages 711 - 717, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
</authors>
<title>The Penn Treebank: Annotating Predicate Argument Structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<marker>Marcus, 1994</marker>
<rawString>Mitchell Marcus et al. 1994. The Penn Treebank: Annotating Predicate Argument Structure. In Proceedings of the Human Language Technology Workshop, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James McCawley</author>
</authors>
<title>Some additional evidence for discontinuity.</title>
<date>1987</date>
<booktitle>In Huck and Ojeda (eds.), Di.scontinuous Constituency: Syntax and Semantics,</booktitle>
<pages>185--200</pages>
<publisher>Academic Press.</publisher>
<location>New York,</location>
<contexts>
<context position="6144" citStr="McCawley, 1987" startWordPosition="873" endWordPosition="874">, to reconcile with the descriptivity requirement,. Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious. Since most methods of handling discontinuous constituents make the formalism more powerful, the efficiency of processing deteriorates, too. An alternative solution is to make argument structure the main structural component of the formalism. This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone, cf. (McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995). These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union, extraposition, diverse second-position phenomena). 2.4 Annotating Argument Structure Argument structure can be represented in terms of unordered trees (with crossing branches). In order to reduce their ambiguity potential, rather simple, &apos;flat&apos; trees should be employed, while more information can be expressed by a rich system of function labels. Furthermore, the required theory-independence means that the form o</context>
</contexts>
<marker>McCawley, 1987</marker>
<rawString>James McCawley. 1987. Some additional evidence for discontinuity. In Huck and Ojeda (eds.), Di.scontinuous Constituency: Syntax and Semantics, pp 185-200. New York, Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
</authors>
<title>A Formal Theory of Word Order: A Case Study</title>
<date>1993</date>
<booktitle>in We st G f.711InniC . Ph .D thesis,</booktitle>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="6174" citStr="Reape, 1993" startWordPosition="877" endWordPosition="878">vity requirement,. Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious. Since most methods of handling discontinuous constituents make the formalism more powerful, the efficiency of processing deteriorates, too. An alternative solution is to make argument structure the main structural component of the formalism. This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone, cf. (McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995). These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union, extraposition, diverse second-position phenomena). 2.4 Annotating Argument Structure Argument structure can be represented in terms of unordered trees (with crossing branches). In order to reduce their ambiguity potential, rather simple, &apos;flat&apos; trees should be employed, while more information can be expressed by a rich system of function labels. Furthermore, the required theory-independence means that the form of syntactic trees should not r</context>
</contexts>
<marker>Reape, 1993</marker>
<rawString>Mike Reape. 1993. A Formal Theory of Word Order: A Case Study in We st G f.711InniC . Ph .D thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Schade das</author>
</authors>
<title>kein A zt anwesend ist der sich auskennt ADJD S.</title>
<journal>KOUS PIAT NN ADJD VAFIN S. PRELS PRF VVFIN S.</journal>
<marker>das, </marker>
<rawString>Schade da&amp;quot;s kein A zt anwesend ist der sich auskennt ADJD S. KOUS PIAT NN ADJD VAFIN S. PRELS PRF VVFIN S.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>