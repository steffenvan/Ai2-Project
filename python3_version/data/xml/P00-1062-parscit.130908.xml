<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007948">
<title confidence="0.9929605">
Utilizing the World Wide Web as an Encyclopedia:
Extracting Term Descriptions from Semi-Structured Texts
</title>
<author confidence="0.95126">
Atsushi Fujii and Tetsuya Ishikawa
</author>
<affiliation confidence="0.989059">
University of Library and Information Science
</affiliation>
<address confidence="0.973862">
1-2 Kasuga, Tsukuba, 305-8550, JAPAN
</address>
<email confidence="0.924086">
fujiiOulis.ac.jp
</email>
<sectionHeader confidence="0.97798" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999589">
In this paper, we propose a method
to extract descriptions of techni-
cal terms from Web pages in order
to utilize the World Wide Web as
an encyclopedia. We use linguistic
patterns and HTML text structures
to extract text fragments contain-
ing term descriptions. We also use
a language model to discard extra-
neous descriptions, and a clustering
method to summarize resultant de-
scriptions. We show the effective-
ness of our method by way of exper-
iments.
</bodyText>
<sectionHeader confidence="0.995599" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999829894736842">
Reflecting the growth in utilization of ma-
chine readable texts, extraction and acqui-
sition of linguistic knowledge from large
corpora has been one of the major top-
ics within the natural language processing
(NLP) community. A sample of linguistic
knowledge targeted in past research includes
grammars (Kupiec and Maxwell, 1992),
word classes (Hatzivassiloglou and McKeown,
1993) and bilingual lexicons (Smadja et al.,
1996). While human experts find it difficult to
produce exhaustive and consistent linguistic
knowledge, automatic methods can help alle-
viate problems associated with manual con-
struction.
Term descriptions, which are usually care-
fully organized in encyclopedias, are valu-
able linguistic knowledge, but have seldom
been targeted in past NLP literature. As
with other types of linguistic knowledge rely-
ing on human introspection and supervision,
constructing encyclopedias is quite expensive.
Additionally, since existing encyclopedias are
usually revised every few years, in many cases
users find it difficult to obtain descriptions for
newly created terms.
To cope with the above limitation of exist-
ing encyclopedias, it is possible to use a search
engine on the World Wide Web as a substi-
tute, expecting that certain Web pages will
describe the submitted keyword. However,
since keyword-based search engines often re-
trieve a surprisingly large number of Web
pages, it is time-consuming to identify pages
that satisfy the users information needs.
In view of this problem, we propose a
method to automatically extract term de-
scriptions from Web pages and summarize
them. In this paper, we generally use &amp;quot;Web
pages&amp;quot; to refer to those pages containing
textual contents, excluding those with only
image/audio information. Besides this, we
specifically target descriptions for technical
terms, and thus &amp;quot;terms&amp;quot; generally refer to
technical terms.
In brief, our method extracts fragments of
Web pages, based on patterns (or templates)
typically used to describe terms. Web pages
are in a sense semi-structured data, because
HTML (Hyper Text Markup Language) tags
provide the textual information contained in
a page with a certain structure. Thus, our
method relies on both linguistic and struc-
tural description patterns.
We used several NLP techniques to semi-
automatically produce linguistic patterns.
We call this approach &amp;quot;NLP-based method.&amp;quot;
</bodyText>
<figure confidence="0.989175692307692">
Web
extraction
patterns
lexicon
extraction
search engine
description
database
clustering
filtering
browser
language
model
</figure>
<bodyText confidence="0.999880348837209">
ered, newspaper articles and magazines dis-
tributed via the Web can be possible sources.
In other words, a morphological analysis is
performed periodically (e.g., weekly) to iden-
tify word tokens from those resources, in order
to enhance the lexicon. However, this is not
the central issue in this paper.
In the foreground process, given an input
term, a browser presents one or more de-
scriptions to a user. In the case where the
database does not index descriptions for the
given term, term descriptions are dynamically
extracted as in the background process. The
background process is optional, and thus term
descriptions can always be obtained dynami-
cally. However, this potentially decreases the
time efficiency for a real-time response.
Figure 2 shows a Web browser, in which
our prototype page presents several Japanese
descriptions extracted for the word &amp;quot;deeta-
mainingu (data mining).&amp;quot; For example, an
English translation for the first description is
as follows:
data mining is a process that collects
data for a certain task, and retrieves
relations latent in the data.
In Figure 2, each description uses various
expressions, but describes the same content:
data mining is a process which discovers rules
latent in given databases. It is expected that
users can understand what data mining is, by
browsing some of those descriptions. In ad-
dition, each headword (&amp;quot;deeta-mainingu&amp;quot; in
this case) positioned above each description
is linked to the Web page from which the de-
scription was extracted.
In the following sections, we first elab-
orate on the NLP/HTML-based extraction
methods in Section 3. We then elaborate
on noise reduction and clustering methods in
Sections 4 and 5, respectively. Finally, in Sec-
tion 6 we investigate the effectiveness of our
extraction method by way of experiments.
</bodyText>
<sectionHeader confidence="0.947716" genericHeader="method">
3 Extracting Term Descriptions
</sectionHeader>
<subsectionHeader confidence="0.957758">
3.1 NLP-based Extraction Method
</subsectionHeader>
<bodyText confidence="0.9726375">
The crucial content for the NLP-based extrac-
tion method is the way to produce linguistic
</bodyText>
<table confidence="0.858115043478261">
-11 Netscape: Enigma I -1 I
i7 7....,7- A-7:7:.., 5---1,L.41-,./ r7-ril,r7
A
Awing&apos; t-i I I.) %P%1 .-..., HMI )01 &apos;..)7 [&apos;&amp;quot;, 71 M
—
7.-.--,(:•,,. 4
.ry-itimt.-AtiT.-.-9:FrisrmA-c*b, -ttte3
6.)J.-.-6.)lut*.a&gt;zomtutL-C-L,NMURRU±.t.M4.,.)t.
5t-z6D3tnt:m-t6imtI, r.):,0..*A-6T-.-7-r:,..04tfiel..63-}1-7,(zm-6T--7-C
MIMWW.-M.1-CW, --WO-W-WELl&apos;a)-t1.)-c4X6
--21.40)trrtitte,±.Lt.-_,J, --&amp;-3-}IRT66-_6)50k0111#E1],±,t6W5-7,g &apos;.6.
• Data mining
•J16.WsTr-9)i,;, lattA0A610R (0*-A. 11M) Z-0.k&lt; aftEmipAA--61,4
sg.tric
• T.- - P 71&apos; = % &amp;quot;CJI : 7-.- , ons4. Tr- ,7-(=:,P- • 7)..n.u../..gyrta ±,
.,1*0)3-iih
El &apos;qW,14•-LZfigt..1:--&apos;-9ILL141..t.iii..-)L. Milt. ITIRMAI).) t.R.T6*-c-6-.
flik6D,VI-F.V,t.-_,ZIWriEht..31M)),2•M-6,b,LLJ, tilMAYLL.l:&lt;
6tsec)ro..Rt,,JIL.
R-1V-7-7.;■
»,-ttint1.66..»),L,N, t.11Min-6 .7 7, MPaGDZ.iiIIM6,,t..-tt..HA-
6Lt,5t(bt%6.
.171-7.
</table>
<figureCaption confidence="0.99642">
Figure 2: Example Japanese descriptions for
&amp;quot;deeta-mainingu (data mining).&amp;quot;
</figureCaption>
<bodyText confidence="0.997296054054054">
patterns that can be used to describe techni-
cal terms. However, human introspection is
a difficult method to exhaustively enumerate
possible description patterns.
Thus, we used NLP techniques to semi-
automatically collect description patterns
from machine readable encyclopedias, be-
cause they usually contain a significantly
large number of descriptions for existing
terms. In practice, we used the Japanese CD-
ROM World Encyclopedia (Heibonsha, 1998),
which includes approximately 80,000 entries
related to various fields.
Before collecting description patterns,
through a preliminary study on the encyclo-
pedia we used, we found that term descrip-
tions frequently contain salient patterns con-
sisting of two Japanese &amp;quot;bunsetsu&amp;quot; phrases.
The following sentence, which describes the
term &amp;quot;X,&amp;quot; contains a typical bunsetsu combi-
nation, that is, &amp;quot;X toha&amp;quot; and &amp;quot;de-aru&amp;quot;:
X toha Y de-aru (X is Y).1-
&apos;Although &amp;quot;de-aru&amp;quot; itself is not a bunsetsu phrase,
we use bunsetsu phrases to refer to combinations of
several words.
In other words, we collected description pat-
terns, based on the co-occurrence of two bun-
setsu phrases, as in the following method.
First, we collected entries associated with
technical terms listed in the World Encyclo-
pedia, and replaced headwords with a vari-
able &amp;quot;X.&amp;quot; Note that the World Encyclope-
dia describes various types of words, including
technical terms, historical people and places,
and thus description patterns vary depending
on the word type. For example, entries for
historical people usually contain when/where
the people were born and their major contri-
butions to the society.
However, for the purposes of our extrac-
tion, it is desirable to use entries solely associ-
ated with technical terms. We then consulted
the EDR machine readable technical termi-
nology dictionary, which contains approxi-
mately 120,000 terms related to the informa-
tion processing field (Japan Electronic Dictio-
nary Research Institute, 1995), and obtained
2,259 entries associated with terms listed in
the EDR dictionary.
Second, we used the ChaSen morphologi-
cal analyzer (Matsumoto et al., 1997), which
has commonly been used for much Japanese
NLP research, to segment collected entries
into words, and assign them parts-of-speech.
We also developed simple heuristics to pro-
duce bunsetsu phrases based on the part-of-
speech information.
Finally, we collected combinations of two
bunsetsu phrases, and sorted them according
to their co-occurrence frequency, in descend-
ing order. However, since the resultant bun-
setsu co-occurrences (even with higher rank-
ings) are extraneous, we supervised (verified,
corrected or discarded) the top 100 candi-
dates, and produced 20 description patterns.
Figure 3 shows a fragment of the resultant
patterns and their English glosses. In this fig-
ure, &amp;quot;X&amp;quot; and &amp;quot;Y&amp;quot; denote variables to which
technical terms and sentence fragments can
be unified, respectively.
Here, we are in a position to extract sen-
tences that match with description patterns,
from Web pages retrieved by the search engine
(see Figure 1). In this process, we do not con-
</bodyText>
<equation confidence="0.913718333333333">
Japanese English Gloss
X toha Y dearu. X is Y.
X ha Y dearu. X is Y.
Y wo X to-iu. Y is called X.
X wo Y to-sadameru. X is defined as Y.
Y wo X to-yobu. Y is called X.
</equation>
<figureCaption confidence="0.987505">
Figure 3: A fragment of linguistic description
patterns we produced.
</figureCaption>
<bodyText confidence="0.989398777777778">
duct morphological analysis on Web pages,
because of computational cost. Instead, we
first segment textual contents in Web pages
into sentences, based on the Japanese punc-
tuation system, and use a surface pattern
matching based on regular expressions.
However, in most cases term descriptions
consist of more than one sentence. This is
especially salient in the case where anaphoric
expressions and itemization are used. Thus, it
is desirable to extract a larger fragment con-
taining sentences that match with description
patterns.
In view of this problem, we first use lin-
guistic description patterns to briefly identify
a zone, and sequentially search the following
fragments relying partially on HTML tags,
until a certain fragment is extracted:2
</bodyText>
<listItem confidence="0.9993795">
(1) paragraph tagged with &lt;P&gt;. &lt;/P&gt; (or
&lt;P&gt;. .&lt;P&gt; in the case where &lt;/P&gt; is
missing),
(2) itemization tagged with &lt;UL&gt;...&lt;/UL&gt;,
</listItem>
<bodyText confidence="0.637639">
(3) N sentences identified with the Japanese
punctuation system, where the sentence
that matched with a description pattern
is positioned as near center as possible,
where we empirically set N = 3.
</bodyText>
<subsectionHeader confidence="0.996401">
3.2 HTML-based Extraction Method
</subsectionHeader>
<bodyText confidence="0.9989225">
Through a preliminary study on existing Web
pages, we identified two typical usages of
</bodyText>
<footnote confidence="0.9851056">
2Although we use HTML tags to identify appro-
priate text fragments, we call the method described
in this section NLP-based method, in a comparison
with the method in Section 3.2 that relies solely on
HTML tags.
</footnote>
<bodyText confidence="0.999948633333334">
HTML tags associated with describing tech-
nical terms.
In the first usage, a term in question is high-
lighted as a heading by way of &lt;H&gt; ... &lt;/ii&gt;,
&lt;B&gt;.. . &lt;/B&gt; or &lt;DT&gt; tag, and followed by its
description in a short fragment. In the sec-
ond usage, terms that are potentially unfa-
miliar to readers are tagged with the anchor
&lt;A&gt; tag, providing hyperlinks to other pages
(or a different position within the same page)
where they are described.
The crucial factor here is to determine
which fragment in the page is extracted as
a description. For this purpose, we use the
same rules described in Section 3.1. However,
unlike the NLP-based method, in the HTML-
based method we extract the fragment that
follows the heading and the position linked
from the anchor. However, in the case where
a term in question is tagged with &lt;DT&gt;, we
extract the following fragment tagged with
&lt;DD&gt;. Note that &lt;DT&gt; and &lt;DD&gt; are inher-
ently provided to describe terms.
The HTML-based method is expected to
extract term descriptions that cannot be ex-
tracted by the NLP-based method, and vice
versa. In fact, in Figure 2 the third and fourth
descriptions were extracted with the HTML-
based method, while the rest were extracted
with the NLP-based method.
</bodyText>
<sectionHeader confidence="0.980804" genericHeader="method">
4 Language Modeling for Filtering
</sectionHeader>
<bodyText confidence="0.999084357142857">
Given a set of Web page fragments extracted
by the NLP/HTML-based methods, we se-
lect fragments that are linguistically under-
standable, and index them into the descrip-
tion database. For this purpose, we perform
a language modeling, so as to quantify the
extent to which a given text fragment is lin-
guistically acceptable.
There are several alternative methods for
language modeling. For example, grammars
are relatively strict language modeling meth-
ods. However, we use a model based on N-
gram, which is usually more robust than that
based on grammars. In other words, text frag-
ments with lower perplexity values are more
linguistically acceptable.
In practice, we used the CMU-Cambridge
toolkit (Clarkson and Rosenfeld, 1997), and
produced a trigram-based language model
from two years of Mainichi Shimbun Japanese
newspaper articles (Mainichi Shimbun, 1994
1995), which were automatically segmented
into words by the ChaSen morphological an-
alyzer (Matsumoto et al., 1997).
In the current implementation, we empiri-
cally select as the final extraction results text
fragments whose perplexity values are lower
than 1,000.
</bodyText>
<sectionHeader confidence="0.987662" genericHeader="method">
5 Clustering Term Descriptions
</sectionHeader>
<bodyText confidence="0.99999004">
For the purpose of clustering term descrip-
tions extracted using methods in Sections 3
and 4, we use the Hierarchical Bayesian Clus-
tering (HBC) method (Iwayama and Toku-
naga, 1995), which has been used for cluster-
ing news articles and constructing thesauri.
As with a number of hierarchical cluster-
ing methods, the HBC method merges similar
items (i.e., term descriptions in our case) in
a bottom-up manner, until all the items are
merged into a single cluster. That is, a cer-
tain number of clusters can be obtained by
splitting the resultant hierarchy at a certain
level.
At the same time, the HBC method also
determines the most representative item (cen-
troid) for each cluster. Then, we present only
those centroids to users.
The similarity between items is computed
based on feature vectors that characterize
each item. In our case, vectors for each
term description consist of frequencies of con-
tent words (e.g., nouns and verbs identified
through a morphological analysis) appearing
in the description.
</bodyText>
<sectionHeader confidence="0.998728" genericHeader="evaluation">
6 Experimentation
</sectionHeader>
<subsectionHeader confidence="0.934098">
6.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999932285714286">
We investigated the effectiveness of our ex-
traction method from a scientific point of
view. However, unlike other research topics
where benchmark test collections are avail-
able to the public (e.g., information retrieval),
there are two major problems for the purpose
of our experimentation, as follows:
</bodyText>
<listItem confidence="0.9958555">
• production of test terms for which de-
scriptions are extracted,
• judgement for descriptions extracted for
those test terms.
</listItem>
<bodyText confidence="0.999850170731707">
For test terms, possible sources are those
listed in existing terminology dictionaries.
However, since the judgement can be consid-
erably expensive for a large number of test
terms, it is preferable to selectively sample a
small number of terms that potentially reflect
the interest in the real world.
In view of this problem, we used as test
terms those contained in queries in the NAC-
SIS test collection (Kando et al., 1999), which
consists of 60 Japanese queries and approx-
imately 330,000 abstracts (in either a com-
bination of English and Japanese or either
of the languages individually), collected from
technical papers published by 65 Japanese as-
sociations for various fields.3
This collection was originally produced for
the evaluation of information retrieval sys-
tems, where each query is used to retrieve
technical abstracts. Thus, the title field of
each query usually contains one or more tech-
nical terms. Besides this, since each query
was produced based partially on existing tech-
nical abstracts, they reflect the real world in-
terest, to some extent. As a result, we ex-
tracted 53 test terms, as shown in Table 1. In
this table, we romanized Japanese terms, and
inserted hyphens between each morpheme for
enhanced readability.
Note that unlike the case of information re-
trieval (e.g., a patent retrieval), where every
relevant document must be retrieved, in our
case even one description can potentially be
sufficient. In other words, in our experiments,
more weight is attached to accuracy (preci-
sion) than recall.
For the search engine in Figure 1, we used
&amp;quot;goo,&amp;quot;4 which is one of the major Japanese
Web search engines. Then, for each extracted
description, one of the authors judged it cor-
rect or incorrect.
</bodyText>
<footnote confidence="0.99958">
3http://www.rd.nacsis.ac.jp/-ntcadm/
index-en.html
4http://www.goo.ne.jp/
</footnote>
<subsectionHeader confidence="0.921004">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999906833333333">
Out of the 53 test terms extracted from the
NACSIS collection, for 44 terms goo retrieved
one or more Web pages. Among those 44
test terms, our method extracted at least one
term description for 27 terms, disregarding
the judgement. Thus, the coverage (or ap-
plicability) of our method was 61.4%. In Ta-
ble 1, the third column denotes the number
of Web pages identified by goo. However, goo
retrieves contents for only the top 1,000 pages.
Table 1 also shows the number descriptions
judged as correct (the column &amp;quot;#C&amp;quot;), the
total number of descriptions extracted (the
column &amp;quot;#T&amp;quot;), and the accuracy (the col-
umn &amp;quot;A&amp;quot;), for both cases with/without the
trigram-based language model.
Table 1 shows that the NLP/HTML-based
methods extracted appropriate term descrip-
tions with a 63.5% accuracy, and that the
trigram-based language model further im-
proved the accuracy from 63.5% to 67.9%. In
other words, only two descriptions are suffi-
cient for users to understand a term in ques-
tion. Reading a few descriptions is not time-
consuming, because they usually consist of
short paragraphs.
We also investigated the effectiveness of
clustering, where for each test term, we clus-
tered descriptions into three clusters (in the
case where there are less than four descrip-
tions, individual descriptions were regarded
as different clusters), and only descriptions
determined as representative by the HBC
method were presented as the final result. We
found that 66.7% of descriptions presented
were correct ones. In other words, users can
obtain descriptions from different viewpoints
and word senses, maintaining the extraction
accuracy obtained above (i.e., 67.9%).
However, we concede that we did not inves-
tigate whether or not each cluster corresponds
to different viewpoints in a rigorous manner.
For the polysemy problem, we investigated
all the descriptions extracted, and found that
only &amp;quot;korokeishon (collocation)&amp;quot; was associ-
ated with two word senses, that is, &amp;quot;word
collocations&amp;quot; and &amp;quot;position of machinery.&amp;quot;
Among the three representative descriptions
</bodyText>
<tableCaption confidence="0.7845315">
Table 1: Extraction accuracy for the 27 test terms (#C = the number of correct descriptions,
#T = the total number of extracted descriptions, A = accuracy (%)).
</tableCaption>
<figure confidence="0.991433754385965">
Japanese Term
Zipf-no-housoku
akusesu-seigyo
bunsho-gazou-rikai
chiteki-eejento
deeta-mainingu
denshi-sukashi
denshi-toshokan
gazou-kensaku
guruupuwea
hikari-faibaa
ichi-keisoku
identeki-arugorizumu
jinkou-chinou
jiritsu-idou-robotto
jisedai-intaanetto
kiiwaado-jidou-chuushutsu
kikai-hon&apos;yaku
korokeishon
koshou-shindan
maruchikyasuto
media-douki
nettowaaku-toporojii
nyuuraru-nettowaaku
ringu-gata-nettowaaku
shisourasu
souraa-kaa
teromea
English Gloss
Zipf&apos;s law
access control
document image understanding
intelligent agent
data mining
digital watermark
digital library
image retrieval
groupware
optical fiber
position measurement
genetic algorithm
artificial intelligence
autonomous mobile robot
next generation Internet
keyword automatic extraction
machine translation
collocation
fault diagnosis
multicast
media synchronization
network topology
neural network
ring network
thesaurus
solar car
telomere
total
</figure>
<table confidence="0.989085166666667">
#Pages w/o Trigram w Trig-ram
#C #T A #C #T A
15 1 1 100 1 1 100
6,925 10 20 50.0 10 20 50.0
43 1 1 100 1 1 100
323 3 5 60.0 3 5 60.0
3,389 37 49 75.5 30 40 75.0
2,124 29 32 90.6 29 32 90.6
7,938 10 26 38.5 8 17 47.1
1,694 1 4 25.0 1 3 33.3
19,760 14 40 35.0 12 21 57.1
10,078 17 25 68.0 14 21 66.7
735 0 3 0 0 3 0
4,686 24 31 77.4 22 28 78.6
18,190 10 19 52.6 9 13 69.2
792 2 2 100 2 2 100
1,963 6 10 60.0 6 10 60.0
25 1 1 100 1 1 100
3,141 1 10 10.0 0 8 0
547 7 16 43.8 7 15 46.7
1,682 2 5 40.0 2 4 50.0
5,758 18 25 72.0 15 22 68.2
46 1 1 100 1 1 100
438 1 4 25.0 0 3 0
9,537 37 47 78.7 36 45 80.0
44 0 1 0 0 1 0
3,399 21 23 91.3 19 20 95.0
3,698 12 21 57.1 12 21 57.1
873 26 36 72.2 25 34 73.5
109,049 292 460 63.5 266 392 67.9
</table>
<bodyText confidence="0.9996856">
for &amp;quot;korokeishon (collocation),&amp;quot; two corre-
sponded to the first sense, and one corre-
sponded to the second sense. To sum up,
the HBC clustering method correctly identi-
fied polysemy.
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999957260869565">
In this paper, we proposed a method to ex-
tract encyclopedic knowledge from the World
Wide Web.
For extracting fragments of Web pages con-
taining term descriptions, we used linguis-
tic and HTML structural patterns typically
used to describe terms. Then, we used a
language model to discard irrelevant descrip-
tions. We also used a clustering method to
summarize extracted descriptions based on
different viewpoints and word senses.
We evaluated our method by way of experi-
ments, and found that the accuracy of our ex-
traction method was practical, that is, a user
can understand a term in question, by brows-
ing two descriptions, on average. We also
found that the language model and the clus-
tering method further enhanced our frame-
work.
Future work will include experiments using
a larger number of test terms, and applica-
tion of extracted descriptions to other NLP
research.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99971325">
The authors would like to thank Hitachi Dig-
ital Heibonsha, Inc. for their support with
the CD-ROM World Encyclopedia, Makoto
Iwayama and Takenobu Tokunaga for their
support with the HBC clustering software,
and Noriko Kando (National Institute of In-
formatics, Japan) for her support with the
NACSIS collection.
</bodyText>
<sectionHeader confidence="0.987819" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997487485294118">
Philip Clarkson and Ronald Rosenfeld. 1997.
Statistical language modeling using the
CMU-Cambridge toolkit. In Proceedings of
Euro Speech &apos;97, pages 2707-2710.
Oren Etzioni. 1997. Moving up the informa-
tion food chain. AI Magazine, 18(2):11-18.
Vasileios Hatzivassiloglou and Kathleen R.
McKeown. 1993. Towards the automatic
identification of adjectival scales: Cluster-
ing adjectives according to meaning. In
Proceedings of the 31st Annual Meeting of
the Association for Computational Linguis-
tics, pages 172-182.
Hitachi Digital Heibonsha. 1998. CD-ROM
World Encyclopedia. (In Japanese).
Makoto Iwayama and Takenobu Tokunaga.
1995. Hierarchical Bayesian clustering for
automatic text classification. In Proceed-
ings of thel4th International Joint Confer-
ence on Artificial Intelligence, pages 1322-
1327.
Japan Electronic Dictionary Research Insti-
tute. 1995. EDR electronic dictionary
technical guide.
Noriko Kando, Kazuko Kuriyama, and Toshi-
hiko Nozue. 1999. NACSIS test collection
workshop (NTCIR-1). In Proceedings of
the 22nd Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 299-300.
Julian Kupiec and John Maxwell. 1992.
Training stochastic grammars from un-
labelled text corpora. In Workshop on
Statistically-Based Natural Language Pro-
gramming Techniques. AAAI Technical Re-
ports WS-92-01.
Mainichi Shimbun. 1994-1995. Mainichi
shimbun CD-ROM &apos;94-95. (In Japanese).
Yuji Matsumoto, Akira Kitauchi, Tatsuo Ya-
mashita, Osamu Imaichi, and Tomoaki
Imamura. 1997. Japanese morphological
analysis system ChaSen manual. Technical
Report NAIST-IS-TR97007, NAIST. (In
Japanese).
Andrew McCallum, Kamal Nigam, Jason
Rennie, and Kristie Seymore. 1999. A
machine learning approach to building
domain-specific search engines. In Proceed-
ings of the 16th International Joint Confer-
ence on Artificial Intelligence, pages 662-
667.
Jian-Yun Nie, Michel Simard, Pierre Is-
abelle, and Richard Durand. 1999. Cross-
language information retrieval based on
parallel texts and automatic mining of par-
allel texts from the Web. In Proceedings of
the 22nd Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 74-81.
Philip Resnik. 1999. Mining the Web for
bilingual texts. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 527-534.
Frank Smadja, Kathleen R. McKeown, and
Vasileios Hatzivassiloglou. 1996. Translat-
ing collocations for bilingual lexicons: A
statistical approach. Computational Lin-
guistics, 22 (1) :1-38.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.816830">
<title confidence="0.998664">Utilizing the World Wide Web as an Encyclopedia: Extracting Term Descriptions from Semi-Structured Texts</title>
<author confidence="0.997329">Atsushi Fujii</author>
<author confidence="0.997329">Tetsuya Ishikawa</author>
<affiliation confidence="0.999976">University of Library and Information Science</affiliation>
<address confidence="0.997842">1-2 Kasuga, Tsukuba, 305-8550, JAPAN</address>
<email confidence="0.991257">fujiiOulis.ac.jp</email>
<abstract confidence="0.988444933333333">In this paper, we propose a method to extract descriptions of technical terms from Web pages in order to utilize the World Wide Web as an encyclopedia. We use linguistic patterns and HTML text structures to extract text fragments containing term descriptions. We also use a language model to discard extraneous descriptions, and a clustering method to summarize resultant descriptions. We show the effectiveness of our method by way of experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of Euro Speech &apos;97,</booktitle>
<pages>2707--2710</pages>
<contexts>
<context position="12751" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="1954" endWordPosition="1957">that are linguistically understandable, and index them into the description database. For this purpose, we perform a language modeling, so as to quantify the extent to which a given text fragment is linguistically acceptable. There are several alternative methods for language modeling. For example, grammars are relatively strict language modeling methods. However, we use a model based on Ngram, which is usually more robust than that based on grammars. In other words, text fragments with lower perplexity values are more linguistically acceptable. In practice, we used the CMU-Cambridge toolkit (Clarkson and Rosenfeld, 1997), and produced a trigram-based language model from two years of Mainichi Shimbun Japanese newspaper articles (Mainichi Shimbun, 1994 1995), which were automatically segmented into words by the ChaSen morphological analyzer (Matsumoto et al., 1997). In the current implementation, we empirically select as the final extraction results text fragments whose perplexity values are lower than 1,000. 5 Clustering Term Descriptions For the purpose of clustering term descriptions extracted using methods in Sections 3 and 4, we use the Hierarchical Bayesian Clustering (HBC) method (Iwayama and Tokunaga, 1</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip Clarkson and Ronald Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. In Proceedings of Euro Speech &apos;97, pages 2707-2710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
</authors>
<title>Moving up the information food chain.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<pages>18--2</pages>
<marker>Etzioni, 1997</marker>
<rawString>Oren Etzioni. 1997. Moving up the information food chain. AI Magazine, 18(2):11-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>172--182</pages>
<contexts>
<context position="1090" citStr="Hatzivassiloglou and McKeown, 1993" startWordPosition="163" endWordPosition="166"> extract text fragments containing term descriptions. We also use a language model to discard extraneous descriptions, and a clustering method to summarize resultant descriptions. We show the effectiveness of our method by way of experiments. 1 Introduction Reflecting the growth in utilization of machine readable texts, extraction and acquisition of linguistic knowledge from large corpora has been one of the major topics within the natural language processing (NLP) community. A sample of linguistic knowledge targeted in past research includes grammars (Kupiec and Maxwell, 1992), word classes (Hatzivassiloglou and McKeown, 1993) and bilingual lexicons (Smadja et al., 1996). While human experts find it difficult to produce exhaustive and consistent linguistic knowledge, automatic methods can help alleviate problems associated with manual construction. Term descriptions, which are usually carefully organized in encyclopedias, are valuable linguistic knowledge, but have seldom been targeted in past NLP literature. As with other types of linguistic knowledge relying on human introspection and supervision, constructing encyclopedias is quite expensive. Additionally, since existing encyclopedias are usually revised every f</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1993</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1993. Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 172-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hitachi Digital Heibonsha</author>
</authors>
<date>1998</date>
<journal>CD-ROM World Encyclopedia. (In Japanese).</journal>
<contexts>
<context position="6515" citStr="Heibonsha, 1998" startWordPosition="942" endWordPosition="943">7-7.;■ »,-ttint1.66..»),L,N, t.11Min-6 .7 7, MPaGDZ.iiIIM6,,t..-tt..HA6Lt,5t(bt%6. .171-7. Figure 2: Example Japanese descriptions for &amp;quot;deeta-mainingu (data mining).&amp;quot; patterns that can be used to describe technical terms. However, human introspection is a difficult method to exhaustively enumerate possible description patterns. Thus, we used NLP techniques to semiautomatically collect description patterns from machine readable encyclopedias, because they usually contain a significantly large number of descriptions for existing terms. In practice, we used the Japanese CDROM World Encyclopedia (Heibonsha, 1998), which includes approximately 80,000 entries related to various fields. Before collecting description patterns, through a preliminary study on the encyclopedia we used, we found that term descriptions frequently contain salient patterns consisting of two Japanese &amp;quot;bunsetsu&amp;quot; phrases. The following sentence, which describes the term &amp;quot;X,&amp;quot; contains a typical bunsetsu combination, that is, &amp;quot;X toha&amp;quot; and &amp;quot;de-aru&amp;quot;: X toha Y de-aru (X is Y).1- &apos;Although &amp;quot;de-aru&amp;quot; itself is not a bunsetsu phrase, we use bunsetsu phrases to refer to combinations of several words. In other words, we collected description </context>
</contexts>
<marker>Heibonsha, 1998</marker>
<rawString>Hitachi Digital Heibonsha. 1998. CD-ROM World Encyclopedia. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Iwayama</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>Hierarchical Bayesian clustering for automatic text classification.</title>
<date>1995</date>
<booktitle>In Proceedings of thel4th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1322--1327</pages>
<contexts>
<context position="13355" citStr="Iwayama and Tokunaga, 1995" startWordPosition="2044" endWordPosition="2048">on and Rosenfeld, 1997), and produced a trigram-based language model from two years of Mainichi Shimbun Japanese newspaper articles (Mainichi Shimbun, 1994 1995), which were automatically segmented into words by the ChaSen morphological analyzer (Matsumoto et al., 1997). In the current implementation, we empirically select as the final extraction results text fragments whose perplexity values are lower than 1,000. 5 Clustering Term Descriptions For the purpose of clustering term descriptions extracted using methods in Sections 3 and 4, we use the Hierarchical Bayesian Clustering (HBC) method (Iwayama and Tokunaga, 1995), which has been used for clustering news articles and constructing thesauri. As with a number of hierarchical clustering methods, the HBC method merges similar items (i.e., term descriptions in our case) in a bottom-up manner, until all the items are merged into a single cluster. That is, a certain number of clusters can be obtained by splitting the resultant hierarchy at a certain level. At the same time, the HBC method also determines the most representative item (centroid) for each cluster. Then, we present only those centroids to users. The similarity between items is computed based on fe</context>
</contexts>
<marker>Iwayama, Tokunaga, 1995</marker>
<rawString>Makoto Iwayama and Takenobu Tokunaga. 1995. Hierarchical Bayesian clustering for automatic text classification. In Proceedings of thel4th International Joint Conference on Artificial Intelligence, pages 1322-1327.</rawString>
</citation>
<citation valid="false">
<date>1995</date>
<institution>Japan Electronic Dictionary Research Institute.</institution>
<note>EDR electronic dictionary technical guide.</note>
<marker>1995</marker>
<rawString>Japan Electronic Dictionary Research Institute. 1995. EDR electronic dictionary technical guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Kando</author>
<author>Kazuko Kuriyama</author>
<author>Toshihiko Nozue</author>
</authors>
<title>NACSIS test collection workshop (NTCIR-1).</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>299--300</pages>
<contexts>
<context position="15071" citStr="Kando et al., 1999" startWordPosition="2322" endWordPosition="2325"> major problems for the purpose of our experimentation, as follows: • production of test terms for which descriptions are extracted, • judgement for descriptions extracted for those test terms. For test terms, possible sources are those listed in existing terminology dictionaries. However, since the judgement can be considerably expensive for a large number of test terms, it is preferable to selectively sample a small number of terms that potentially reflect the interest in the real world. In view of this problem, we used as test terms those contained in queries in the NACSIS test collection (Kando et al., 1999), which consists of 60 Japanese queries and approximately 330,000 abstracts (in either a combination of English and Japanese or either of the languages individually), collected from technical papers published by 65 Japanese associations for various fields.3 This collection was originally produced for the evaluation of information retrieval systems, where each query is used to retrieve technical abstracts. Thus, the title field of each query usually contains one or more technical terms. Besides this, since each query was produced based partially on existing technical abstracts, they reflect the</context>
</contexts>
<marker>Kando, Kuriyama, Nozue, 1999</marker>
<rawString>Noriko Kando, Kazuko Kuriyama, and Toshihiko Nozue. 1999. NACSIS test collection workshop (NTCIR-1). In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 299-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>John Maxwell</author>
</authors>
<title>Training stochastic grammars from unlabelled text corpora.</title>
<date>1992</date>
<booktitle>In Workshop on Statistically-Based Natural Language Programming Techniques. AAAI Technical Reports</booktitle>
<pages>92--01</pages>
<contexts>
<context position="1039" citStr="Kupiec and Maxwell, 1992" startWordPosition="157" endWordPosition="160">stic patterns and HTML text structures to extract text fragments containing term descriptions. We also use a language model to discard extraneous descriptions, and a clustering method to summarize resultant descriptions. We show the effectiveness of our method by way of experiments. 1 Introduction Reflecting the growth in utilization of machine readable texts, extraction and acquisition of linguistic knowledge from large corpora has been one of the major topics within the natural language processing (NLP) community. A sample of linguistic knowledge targeted in past research includes grammars (Kupiec and Maxwell, 1992), word classes (Hatzivassiloglou and McKeown, 1993) and bilingual lexicons (Smadja et al., 1996). While human experts find it difficult to produce exhaustive and consistent linguistic knowledge, automatic methods can help alleviate problems associated with manual construction. Term descriptions, which are usually carefully organized in encyclopedias, are valuable linguistic knowledge, but have seldom been targeted in past NLP literature. As with other types of linguistic knowledge relying on human introspection and supervision, constructing encyclopedias is quite expensive. Additionally, since</context>
</contexts>
<marker>Kupiec, Maxwell, 1992</marker>
<rawString>Julian Kupiec and John Maxwell. 1992. Training stochastic grammars from unlabelled text corpora. In Workshop on Statistically-Based Natural Language Programming Techniques. AAAI Technical Reports WS-92-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mainichi Shimbun</author>
</authors>
<date>1994</date>
<note>Mainichi shimbun CD-ROM &apos;94-95. (In Japanese).</note>
<contexts>
<context position="12883" citStr="Shimbun, 1994" startWordPosition="1974" endWordPosition="1975">ntify the extent to which a given text fragment is linguistically acceptable. There are several alternative methods for language modeling. For example, grammars are relatively strict language modeling methods. However, we use a model based on Ngram, which is usually more robust than that based on grammars. In other words, text fragments with lower perplexity values are more linguistically acceptable. In practice, we used the CMU-Cambridge toolkit (Clarkson and Rosenfeld, 1997), and produced a trigram-based language model from two years of Mainichi Shimbun Japanese newspaper articles (Mainichi Shimbun, 1994 1995), which were automatically segmented into words by the ChaSen morphological analyzer (Matsumoto et al., 1997). In the current implementation, we empirically select as the final extraction results text fragments whose perplexity values are lower than 1,000. 5 Clustering Term Descriptions For the purpose of clustering term descriptions extracted using methods in Sections 3 and 4, we use the Hierarchical Bayesian Clustering (HBC) method (Iwayama and Tokunaga, 1995), which has been used for clustering news articles and constructing thesauri. As with a number of hierarchical clustering method</context>
</contexts>
<marker>Shimbun, 1994</marker>
<rawString>Mainichi Shimbun. 1994-1995. Mainichi shimbun CD-ROM &apos;94-95. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>Akira Kitauchi, Tatsuo Yamashita, Osamu Imaichi, and Tomoaki Imamura.</title>
<date>1997</date>
<note>Japanese morphological analysis system ChaSen manual. Technical Report NAIST-IS-TR97007, NAIST. (In Japanese).</note>
<marker>Matsumoto, 1997</marker>
<rawString>Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Osamu Imaichi, and Tomoaki Imamura. 1997. Japanese morphological analysis system ChaSen manual. Technical Report NAIST-IS-TR97007, NAIST. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
<author>Jason Rennie</author>
<author>Kristie Seymore</author>
</authors>
<title>A machine learning approach to building domain-specific search engines.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>662--667</pages>
<marker>McCallum, Nigam, Rennie, Seymore, 1999</marker>
<rawString>Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. 1999. A machine learning approach to building domain-specific search engines. In Proceedings of the 16th International Joint Conference on Artificial Intelligence, pages 662-667.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Yun Nie</author>
<author>Michel Simard</author>
<author>Pierre Isabelle</author>
<author>Richard Durand</author>
</authors>
<title>Crosslanguage information retrieval based on parallel texts and automatic mining of parallel texts from the Web.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--81</pages>
<marker>Nie, Simard, Isabelle, Durand, 1999</marker>
<rawString>Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Durand. 1999. Crosslanguage information retrieval based on parallel texts and automatic mining of parallel texts from the Web. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 74-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Mining the Web for bilingual texts.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>527--534</pages>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Mining the Web for bilingual texts. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 527-534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>1--38</pages>
<contexts>
<context position="1135" citStr="Smadja et al., 1996" startWordPosition="170" endWordPosition="173">o use a language model to discard extraneous descriptions, and a clustering method to summarize resultant descriptions. We show the effectiveness of our method by way of experiments. 1 Introduction Reflecting the growth in utilization of machine readable texts, extraction and acquisition of linguistic knowledge from large corpora has been one of the major topics within the natural language processing (NLP) community. A sample of linguistic knowledge targeted in past research includes grammars (Kupiec and Maxwell, 1992), word classes (Hatzivassiloglou and McKeown, 1993) and bilingual lexicons (Smadja et al., 1996). While human experts find it difficult to produce exhaustive and consistent linguistic knowledge, automatic methods can help alleviate problems associated with manual construction. Term descriptions, which are usually carefully organized in encyclopedias, are valuable linguistic knowledge, but have seldom been targeted in past NLP literature. As with other types of linguistic knowledge relying on human introspection and supervision, constructing encyclopedias is quite expensive. Additionally, since existing encyclopedias are usually revised every few years, in many cases users find it difficu</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22 (1) :1-38.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>