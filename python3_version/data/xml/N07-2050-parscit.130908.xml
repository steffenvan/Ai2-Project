<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002319">
<title confidence="0.993592">
Chinese Named Entity Recognition with Cascaded Hybrid Model
</title>
<author confidence="0.985761">
Xiaofeng YU
</author>
<affiliation confidence="0.987872666666667">
Information Systems Laboratory
Department of Systems Engineering &amp; Engineering Management
The Chinese University of Hong Kong
</affiliation>
<address confidence="0.806728">
Shatin, N.T., Hong Kong
</address>
<email confidence="0.997951">
xfyu@se.cuhk.edu.hk
</email>
<sectionHeader confidence="0.993871" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993923076923">
We propose a high-performance cascaded hy-
brid model for Chinese NER. Firstly, we use
Boosting, a standard and theoretically well-
founded machine learning method to combine a
set of weak classifiers together into a base sys-
tem. Secondly, we introduce various types of
heuristic human knowledge into Markov Logic
Networks (MLNs), an effective combination
of first-order logic and probabilistic graphi-
cal models to validate Boosting NER hypothe-
ses. Experimental results show that the cas-
caded hybrid model significantly outperforms
the state-of-the-art Boosting model.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920901960784">
Named entity recognition (NER) involves the identifica-
tion and classification of certain proper nouns in text,
such as person names (PERs), locations (LOCs), orga-
nizations (ORGs), miscellaneous names (MISCs), tem-
poral, numerical and monetary phrases. It is a well-
established task in the NLP community and is regarded
as crucial technology for many NLP applications, such as
information extraction, question answering, information
retrieval and machine translation.
Compared to European-language NER, Chinese NER
seems to be more difficult (Yu et al., 2006). Recent ap-
proaches to Chinese NER are a shift away from man-
ually constructed rules or finite state patterns towards
machine learning or statistical methods. However, rule-
based NER systems lack robustness and portability, and
machine learning approaches might be unsatisfactory to
learn linguistic information in Chinese NEs. In fact,
Chinese NEs have distinct linguistic characteristics in
their composition and human beings usually use prior
knowledge to recognize NEs. For example, about 365
of the highest frequently used surnames cover 99% Chi-
nese surnames (Sun et al., 1995). For the LOC “✗5,
✂/Beijing City”, “✗5,/Beijing” is the name part and
“✂/City” is the salient word. For the ORG “✗5,✂❄
➐/Beijing City Government”, “✗5,/Beijing” is the LOC
name part, “✂/City” is the LOC salient word and “❄
➐/Government” is the ORG salient word. Some ORGs
contain one or more PERs, LOCs, MISCs and ORGs. A
more complex example is the nested ORG “❱✗✁Ra
✂Ra✬❢→ ✿❢❜/School of Computer Science,
Wuhan University, Wuhan City, Hubei Province” which
contains two ORGs “Ra✬❢/Wuhan University” and
“→ ✿ ❢ ❜/School of Computer Science” and two
LOCs “❱ ✗ ✁/Hubei Province” and “R a✂/Wuhan
City”. The two ORGs contain ORG salient words “✬
❢/University” and “❢❜/School”, while the two LOCs
contain LOC salient words “✁/Province” and “✂/City”
respectively.
Inspired by the above observation, we propose a cas-
caded hybrid model for Chinese NER 1. First, we em-
ploy Boosting, which has theoretical justification and has
been shown to perform well on other NLP problems,
to combine weak classifiers into a strong classifier. We
then exploit a variety of heuristic human knowledge into
MLNs, a powerful combination of logic and probabil-
ity, to validate Boosting NER hypotheses. We also use
three Markov chain Monte Carlo (MCMC) algorithms
for probabilistic inference in MLNs. Experimental re-
sults show that the cascaded hybrid model yields better
NER results than the stand-alone Boosting model by a
large margin.
</bodyText>
<sectionHeader confidence="0.946003" genericHeader="introduction">
2 Boosting
</sectionHeader>
<bodyText confidence="0.999760571428571">
The main idea behind the Boosting algorithm is that a set
of many simple and moderately accurate weak classifiers
(also called weak hypotheses) can be effectively com-
bined to yield a single strong classifier (also called the
final hypothesis). The algorithm works by training weak
classifiers sequentially whose classification accuracy is
slightly better than random guessing and finally combin-
</bodyText>
<footnote confidence="0.995393333333333">
1In this paper we only focus on PERs, LOCs, ORGs and
MISCs. Since temporal, numerical and monetary phrases can
be well identified with rule-based approaches.
</footnote>
<page confidence="0.978573">
197
</page>
<note confidence="0.4698605">
Proceedings of NAACL HLT 2007, Companion Volume, pages 197–200,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999693842105263">
ing them into a highly accurate classifier. Each weak clas-
sifier searches for the hypothesis in the hypotheses space
that can best classify the current set of training examples.
Based on the evaluation of each iteration, the algorithm
re-weights the training examples, forcing the newly gen-
erated weak classifier to give higher weights to the exam-
ples that are misclassified in the previous iteration.
We use the AdaBoost.MH algorithm (Schapire and
Singer, 1999) as shown in Figure 1, an n-ary classi-
fication variant of the original well-known binary Ad-
aBoost algorithm (Freund and Schapire, 1997). It has
been demonstrated that Boosting can be used to build
language-independent NER models that perform excep-
tionally well (Wu et al. (2002), Wu et al. (2004), Carreras
et al. (2002)). In particular, reasonable Chinese NER
results were still obtained using Boosting, even though
there was no Chinese-specific tuning and the model was
only trained on one-third of the provided corpora in
SIGHAN bakeoff-3 (Yu et al., 2006).
</bodyText>
<sectionHeader confidence="0.990088" genericHeader="method">
3 Markov Logic Networks
</sectionHeader>
<bodyText confidence="0.9999614">
A Markov Network (also known as Markov Random
Field) is a model for the joint distribution of a set of
variables (Pearl, 1988). It is composed of an undirected
graph and a set of potential functions. A First-Order
Knowledge Base (KB) (Genesereth and Nislsson, 1987)
is a set of sentences or formulas in first-order logic. A
Markov Logic Network (MLN) (Richardson and Domin-
gos, 2006) is a KB with a weight attached to each formula
(or clause). Together with a set of constants representing
objects in the domain, it species a ground Markov Net-
work containing one feature for each possible grounding
of a first-order formula in the KB, with the correspond-
ing weight. The weights associated with the formulas in
an MLN jointly determine the probabilities of those for-
mulas (and vice versa) via a log-linear model. An MLN
defines a probability distribution over Herbrand interpre-
tations (possible worlds), and can be thought of as a tem-
plate for constructing Markov Networks. The probabil-
ity distribution over possible worlds x specified by the
ground Markov Network ML,C is given by
</bodyText>
<equation confidence="0.9678445">
P(X = x) = Zexp(Xwini(x )) = Z Y φi (x{i})ni(x)
(1)
</equation>
<bodyText confidence="0.988939333333333">
where Fi is the formula in first-order logic, wi is a real
number, ni (x) is the number of true groundings of Fi in
x, x{i} is the true value of the atoms appearing in Fi, and
</bodyText>
<equation confidence="0.5797645">
� = ewi.
φi �x{i}
</equation>
<subsectionHeader confidence="0.999698">
3.1 Learning Weights
</subsectionHeader>
<bodyText confidence="0.996401666666667">
Given a relational database, MLN weights can in princi-
ple be learned generatively by maximizing the likelihood
of this database. The gradient of the log-likelihood with
Input: A training set Tr = {&lt; d1, C1 &gt;, ... , &lt; dg, Cg &gt;}
where Cj C C = {c1, ..., cm} for all j = 1, ... , g.
Output: A final hypothesis 4)(d, c) = PS s=1 αs4)s(d, c).
</bodyText>
<equation confidence="0.654827333333333">
Algorithm: LetD1(dj, ci) = 1for all j = 1, ... , g and
mg
for all i = 1, ... ,m. For s = 1, ... ,S do:
</equation>
<listItem confidence="0.9995678">
• pass distribution Ds(dj, ci)to the weak classifier;
• derive the weak hypothesis 4)s from the weak
classifier;
• choose αs E R;
• set Ds+1(dj, ci) = Ds(dj,ci)exp(−αsCj[ci]Φs(dj,ci)) Zs
</listItem>
<bodyText confidence="0.603852">
where
</bodyText>
<equation confidence="0.859986">
Zs =
Pm1 Pgj=1 Ds(dj, ci )exp( − αsCj [ci] 4)s(dj, ci))
is Pm a normalization factor chosen so that
Pg j=1 Ds+1(dj, ci) = 1.
i=1
</equation>
<figureCaption confidence="0.976347">
Figure 1: The AdaBoost.MH algorithm.
</figureCaption>
<equation confidence="0.925618666666667">
X
logPw(X = x) = ni (x) − Pw(X = x&apos;)ni(x&apos;)
(2)
</equation>
<bodyText confidence="0.9999675">
where the sum is over all possible databases x&apos;, and
Pw(X = x&apos;) is P(X = x&apos;) computed using the current
weight vector w = (w1, ..., wi, ...). Unfortunately, com-
puting these expectations can be very expensive. Instead,
we can maximize the pseudo-likelihood of the data more
efficiently. If x is a possible database and xl is the lth
ground atom’s truth value, the pseudo-log-likelihood of x
given weights w is
</bodyText>
<equation confidence="0.998922">
logPw(X = x) = Xn logPw(Xl=xl  |MBx(Xl )) (3)
l=1
</equation>
<bodyText confidence="0.9998285">
where MBx (Xl) is the state of Xl’s Markov blan-
ket in the data. Computing Equation 3 and its gradient
does not require inference over the model, and is there-
fore much faster. We optimize the pseudo-log-likelihood
using the limited-memory BFGS algorithm (Liu and No-
cedal, 1989).
</bodyText>
<subsectionHeader confidence="0.877266">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.999730333333333">
If F1 and F2 are two formulas in first-order logic, C is a
finite set of constants including any constants that appear
in F1 or F2, and L is an MLN, then
</bodyText>
<equation confidence="0.99988">
P (F1  |F2, L, C) = P(F1  |F2, ML,C) (4)
P(F1 ∧ F2  |ML,C) =(5)
P(F2  |ML,C)
P
xEχF1nχF2 P(X = x  |ML,C)
= P (6)
xEχF2 P(X = x  |ML,C)
</equation>
<bodyText confidence="0.492831333333333">
respect to the weights is
∂
∂wi
</bodyText>
<page confidence="0.972975">
198
</page>
<bodyText confidence="0.999809526315789">
where XF; is the set of worlds where Fi holds, and
P(x  |ML,C) is given by Equation 1. The ques-
tion of whether a knowledge base entails a formula F
in first-order logic is the question of whether P(F |
LKB, CKB,F) = 1, where LKB is the MLN obtained by
assigning infinite weight to all the formulas in KB, and
CKB,F is the set of all constants appearing in KB or F.
The most widely used approximate solution to proba-
bilistic inference in MLNs is Markov chain Monte Carlo
(MCMC) (Gilks et al., 1996). In this framework, the
Gibbs sampling algorithm is to generate an instance from
the distribution of each variable in turn, conditional on the
current values of the other variables. One way to speed
up Gibbs sampling is by Simulated Tempering (Marinari
and Parisi, 1992), which performs simulation in a gener-
alized ensemble, and can rapidly achieve an equilibrium
state. Poon and Domingos (2006) proposed MC-SAT,
an inference algorithm that combines ideas from MCMC
and satisfiability.
</bodyText>
<sectionHeader confidence="0.99619" genericHeader="method">
4 Heuristic Human Knowledge
</sectionHeader>
<bodyText confidence="0.999891333333333">
Even though the Boosting model is able to accommodate
a large number of features, some NEs, especially LOCs,
ORGs and MISCs are difficult to identify due to lack
of linguistic knowledge. For example, some ORGs are
possibly mistagged as LOCs and/or MISCs. We incor-
porate heuristic human knowledge via MLNs to validate
the Boosting NER hypotheses. We extract 151 location
salient words and 783 organization salient words from the
LDC Chinese-English bi-directional NE lists compiled
from Xinhua News database. We also make a punctua-
tion list which contains 19 items. We make the following
assumptions to validate the Boosting results:
</bodyText>
<listItem confidence="0.937534818181818">
• Obviously, if a tagged entity ends with a location
salient word, it is a LOC. If a tagged entity ends with
an organization salient word, it is an ORG.
• If a tagged entity is close to a subsequent location
salient word, probably they should be combined to-
gether as a LOC. The closer they are, the more likely
that they should be combined.
• Heuristically, if a series of consecutive tagged en-
tities are close to a subsequent organization salient
word, they should probably be combined together
as an ORG because an ORG may contain multiple
PERs, LOCs, MISCs and ORGs.
• Similarly, if there exists a series of consecutive
tagged entities and the last one is tagged as an ORG,
it is likely that all of them should be combined as an
ORG.
• Entity length restriction: all kinds of tagged entities
cannot exceed 25 Chinese characters.
• Punctuation restriction: in general, all tagged enti-
ties cannot span any punctuation.
• Since all NEs are proper nouns, the tagged entities
should end with noun words.
</listItem>
<bodyText confidence="0.9834824">
All the above human knowledge can be formulized as
first-order logic to construct the structure of MLNs. And
all the validated Boosting results are accepted as new NE
candidates (or common nouns). We train an MLN to rec-
ognize them.
</bodyText>
<sectionHeader confidence="0.99839" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99983">
We randomly selected 15,000 and 3,000 sentences from
the People’s Daily corpus as training and test sets, respec-
tively. We used the decision stump2 as the weak classifier
in Boosting to construct a character-based Chinese NER
baseline system.
The features used were as follows:
</bodyText>
<listItem confidence="0.988235333333333">
• The current character and its POS tag.
• The characters within a window of 3 characters be-
fore and after the current character.
• The POS tags within a window of 3 characters be-
fore and after the current character.
• A small set of conjunctions of POS tags and charac-
ters within a window of 3 characters of the current
character.
• The BIO 3 chunk tags of the previous 3 characters.
</listItem>
<bodyText confidence="0.998856">
We declared 10 predicates and specified 9 first-
order formulas according to the heuristic human
knowledge in Section 4. For example, we used
person(candidate) to predicate whether a candi-
date is a PER. Formulas are recursively constructed from
atomic formulas using logical connectives and quanti-
fiers. They are constructed using four types of sym-
bols: constants, variables, functions, and predicates.
Constant symbols represent objects in the domain of
interest (e.g., “✗➡/Beijing” and “✡✇/Shanghai” are
LOCs). Variable symbols range over the objects in the
domain. Function symbols represent mappings from tu-
ples of objects to objects. Predicate symbols repre-
sent relations among objects in the domain or attributes
of objects. For example, the formula endwith(r,
p)ˆlocsalientword(p)=&gt;location(r) means
if r ends with a location salient word p, then it is a LOC.
</bodyText>
<footnote confidence="0.899680166666667">
2A decision stump is basically a one-level decision tree
where the split at the root level is based on a specific at-
tribute/value pair.
3In this representation, each character is tagged as either the
beginning of a named entity (B tag), a character inside a named
entity (I tag), or a character outside a named entity (O tag).
</footnote>
<page confidence="0.99823">
199
</page>
<bodyText confidence="0.99993637037037">
We extracted all the distinct NEs (4,475 PERs, 2,170
LOCs, 2,823 ORGs and 614 MISCs) from the 15,000-
sentence training corpus. An MLN training database,
which consists of 10 predicates and 44,810 ground
atoms was built. A ground atom is an atomic formula
all of whose arguments are ground terms (terms con-
taining no variables). For example, the ground atom
location(✗ 5, ✂) conveys that “✗ 5, ✂/Beijing
City” is a LOC.
During MLN learning, each formula is converted to
Conjunctive Normal Form (CNF), and a weight is learned
for each of its clauses. The weight of a clause is used
as the mean of a Gaussian prior for the learned weight.
These weights reflect how often the clauses are actually
observed in the training data.
We validated 352 Boosting results to construct the
MLN testing database, which contains 1,285 entries and
these entries are used as evidence for inference. Infer-
ence is performed by grounding the minimal subset of the
network required for answering the query predicates. We
applied 3 MCMC algorithms: Gibbs sampling (GS), MC-
SAT and Simulated Tempering (ST) for inference and
the comparative NER results are shown in Table 1. The
cascaded hybrid model greatly outperforms the Boosting
model. We obtained the same results using GS and ST
algorithms. And GS (or ST) yields slightly better results
than the MC-SAT algorithm.
</bodyText>
<sectionHeader confidence="0.999235" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999481454545455">
In this paper we propose a cascaded hybrid model
for Chinese NER. We incorporate human heuristics via
MLNs, which produce a set of weighted first-order
clauses to validate Boosting NER hypotheses. To the best
of our knowledge, this is the first attempt at using MLNs
for the NER problem in the NLP community. Experi-
ments on People’s Daily corpus illustrate the promise of
our approach. Directions for future work include learning
the structure of MLNs automatically and using MLNs for
information extraction and statistical relational learning
(e.g., entity relation identification).
</bodyText>
<sectionHeader confidence="0.998591" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993587230769231">
Xavier Carreras, Llu´ıs M`arquez, and Llu´ıs Padr´o. Named en-
tity extraction using AdaBoost. In Computational Natural
Language Learning (CoNLL-2002), at COLING-2002, pages
171–174, Taipei, Sep 2002.
Yoav Freund and Robert E. Schapire. A decision-theoretic gen-
eralization of on-line learning and an application to boosting.
Computer and System Sciences, 55(1):119–139, 1997.
Michael R. Genesereth and Nils J. Nislsson. Logical founda-
tions of artificial intelligence. Morgan Kaufmann Publishers
Inc., San Mateo, CA, 1987.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. Markov
chain Monte Carlo in practice. Chapman and Hall, London,
UK, 1996.
</reference>
<tableCaption confidence="0.997889">
Table 1: Comparative Chinese NER Results
</tableCaption>
<table confidence="0.999643052631579">
Precision Recall F0=1
Boosting
PER 99.39% 99.06% 99.22
LOC 87.55% 91.81% 89.63
ORG 82.15% 66.61% 73.57
MISC 80.00% 87.84% 83.74
Overall 90.26% 89.42% 89.84
Hybrid (MC-SAT)
PER 99.39% 99.06% 99.22
LOC 94.83% 91.81% 93.30
ORG 87.82% 85.69% 86.74
MISC 93.53% 85.10% 89.12
Overall 95.01% 92.78% 93.88
Hybrid (GS/ST)
PER 99.39% 99.06% 99.22
LOC 94.80% 91.91% 93.34
ORG 87.82% 86.28% 87.04
MISC 93.53% 85.10% 89.12
Overall 94.99% 92.93% 93.95
</table>
<reference confidence="0.99683934375">
Dong C. Liu and Jorge Nocedal. On the limited memory BFGS
method for large scale optimization. Mathematical Program-
ming, 45:503–528, 1989.
Enzo Marinari and Giorgio Parisi. Simulated Tempering: A
new Monte Carlo scheme. Europhysics Letters, 19:451–458,
1992.
Judea Pearl. Probabilistic reasoning in intelligent systems: net-
works of plausible inference. Morgan Kaufmann Publishers
Inc., San Francisco, CA, 1988.
Hoifung Poon and Pedro Domingos. Sound and efficient infer-
ence with probabilistic and deterministic dependencies. In
21st National Conference on Artificial Intelligence (AAAI-
06), Boston, Massachusetts, July 2006. The AAAI Press.
Matthew Richardson and Pedro Domingos. Markov logic net-
works. Machine Learning, 62(1-2):107–136, 2006.
Robert E. Schapire and Yoram Singer. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3):297–336, 1999.
Maosong Sun, Changning Huang, Haiyan Gao, and Jie Fang.
Identifying Chinese names in unrestricted texts. Journal of
Chinese Information Processing, 1995.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and
Yongsheng Yang. Boosting for named entity recognition. In
Computational Natural Language Learning (CoNLL-2002),
at COLING-2002, pages 195–198, Taipei, Sep 2002.
Dekai Wu, Grace Ngai, and Marine Carpuat. Why nitpicking
works: Evidence for Occam’s razor in error correctors. In
20th International Conference on Computational Linguistics
(COLING-2004), Geneva, 2004.
Xiaofeng Yu, Marine Carpuat, and Dekai Wu. Boosting for
Chinese named entity recognition. In 5th SIGHAN Workshop
on Chinese Language Processing, Australia, July 2006.
</reference>
<page confidence="0.996611">
200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.629582">
<title confidence="0.812417">Chinese Named Entity Recognition with Cascaded Hybrid Model Information Systems</title>
<affiliation confidence="0.990428">Department of Systems Engineering &amp; Engineering The Chinese University of Hong</affiliation>
<address confidence="0.988013">Shatin, N.T., Hong</address>
<email confidence="0.987937">xfyu@se.cuhk.edu.hk</email>
<abstract confidence="0.998517642857143">We propose a high-performance cascaded hybrid model for Chinese NER. Firstly, we use Boosting, a standard and theoretically wellfounded machine learning method to combine a set of weak classifiers together into a base system. Secondly, we introduce various types of heuristic human knowledge into Markov Logic Networks (MLNs), an effective combination of first-order logic and probabilistic graphical models to validate Boosting NER hypotheses. Experimental results show that the cascaded hybrid model significantly outperforms the state-of-the-art Boosting model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
<author>Llu´ıs Padr´o</author>
</authors>
<title>Named entity extraction using AdaBoost.</title>
<date>2002</date>
<booktitle>In Computational Natural Language Learning (CoNLL-2002), at COLING-2002,</booktitle>
<pages>171--174</pages>
<location>Taipei,</location>
<marker>Carreras, M`arquez, Padr´o, 2002</marker>
<rawString>Xavier Carreras, Llu´ıs M`arquez, and Llu´ıs Padr´o. Named entity extraction using AdaBoost. In Computational Natural Language Learning (CoNLL-2002), at COLING-2002, pages 171–174, Taipei, Sep 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>A decision-theoretic generalization of on-line learning and an application to boosting.</title>
<date>1997</date>
<journal>Computer and System Sciences,</journal>
<volume>55</volume>
<issue>1</issue>
<contexts>
<context position="4683" citStr="Freund and Schapire, 1997" startWordPosition="710" endWordPosition="713">or Computational Linguistics ing them into a highly accurate classifier. Each weak classifier searches for the hypothesis in the hypotheses space that can best classify the current set of training examples. Based on the evaluation of each iteration, the algorithm re-weights the training examples, forcing the newly generated weak classifier to give higher weights to the examples that are misclassified in the previous iteration. We use the AdaBoost.MH algorithm (Schapire and Singer, 1999) as shown in Figure 1, an n-ary classification variant of the original well-known binary AdaBoost algorithm (Freund and Schapire, 1997). It has been demonstrated that Boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). In particular, reasonable Chinese NER results were still obtained using Boosting, even though there was no Chinese-specific tuning and the model was only trained on one-third of the provided corpora in SIGHAN bakeoff-3 (Yu et al., 2006). 3 Markov Logic Networks A Markov Network (also known as Markov Random Field) is a model for the joint distribution of a set of variables (Pearl, 1988). It is composed of an u</context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Computer and System Sciences, 55(1):119–139, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Genesereth</author>
<author>Nils J Nislsson</author>
</authors>
<title>Logical foundations of artificial intelligence.</title>
<date>1987</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Mateo, CA,</location>
<contexts>
<context position="5398" citStr="Genesereth and Nislsson, 1987" startWordPosition="827" endWordPosition="830">R models that perform exceptionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). In particular, reasonable Chinese NER results were still obtained using Boosting, even though there was no Chinese-specific tuning and the model was only trained on one-third of the provided corpora in SIGHAN bakeoff-3 (Yu et al., 2006). 3 Markov Logic Networks A Markov Network (also known as Markov Random Field) is a model for the joint distribution of a set of variables (Pearl, 1988). It is composed of an undirected graph and a set of potential functions. A First-Order Knowledge Base (KB) (Genesereth and Nislsson, 1987) is a set of sentences or formulas in first-order logic. A Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a KB with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it species a ground Markov Network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. The weights associated with the formulas in an MLN jointly determine the probabilities of those formulas (and vice versa) via a log-linear model. An MLN defines a probability distribution over Herbran</context>
</contexts>
<marker>Genesereth, Nislsson, 1987</marker>
<rawString>Michael R. Genesereth and Nils J. Nislsson. Logical foundations of artificial intelligence. Morgan Kaufmann Publishers Inc., San Mateo, CA, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W R Gilks</author>
<author>S Richardson</author>
<author>D J Spiegelhalter</author>
</authors>
<title>Markov chain Monte Carlo in practice.</title>
<date>1996</date>
<publisher>Chapman and Hall,</publisher>
<location>London, UK,</location>
<contexts>
<context position="8892" citStr="Gilks et al., 1996" startWordPosition="1489" endWordPosition="1492">1 ∧ F2 |ML,C) =(5) P(F2 |ML,C) P xEχF1nχF2 P(X = x |ML,C) = P (6) xEχF2 P(X = x |ML,C) respect to the weights is ∂ ∂wi 198 where XF; is the set of worlds where Fi holds, and P(x |ML,C) is given by Equation 1. The question of whether a knowledge base entails a formula F in first-order logic is the question of whether P(F | LKB, CKB,F) = 1, where LKB is the MLN obtained by assigning infinite weight to all the formulas in KB, and CKB,F is the set of all constants appearing in KB or F. The most widely used approximate solution to probabilistic inference in MLNs is Markov chain Monte Carlo (MCMC) (Gilks et al., 1996). In this framework, the Gibbs sampling algorithm is to generate an instance from the distribution of each variable in turn, conditional on the current values of the other variables. One way to speed up Gibbs sampling is by Simulated Tempering (Marinari and Parisi, 1992), which performs simulation in a generalized ensemble, and can rapidly achieve an equilibrium state. Poon and Domingos (2006) proposed MC-SAT, an inference algorithm that combines ideas from MCMC and satisfiability. 4 Heuristic Human Knowledge Even though the Boosting model is able to accommodate a large number of features, som</context>
</contexts>
<marker>Gilks, Richardson, Spiegelhalter, 1996</marker>
<rawString>W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. Markov chain Monte Carlo in practice. Chapman and Hall, London, UK, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="8061" citStr="Liu and Nocedal, 1989" startWordPosition="1320" endWordPosition="1324">ector w = (w1, ..., wi, ...). Unfortunately, computing these expectations can be very expensive. Instead, we can maximize the pseudo-likelihood of the data more efficiently. If x is a possible database and xl is the lth ground atom’s truth value, the pseudo-log-likelihood of x given weights w is logPw(X = x) = Xn logPw(Xl=xl |MBx(Xl )) (3) l=1 where MBx (Xl) is the state of Xl’s Markov blanket in the data. Computing Equation 3 and its gradient does not require inference over the model, and is therefore much faster. We optimize the pseudo-log-likelihood using the limited-memory BFGS algorithm (Liu and Nocedal, 1989). 3.2 Inference If F1 and F2 are two formulas in first-order logic, C is a finite set of constants including any constants that appear in F1 or F2, and L is an MLN, then P (F1 |F2, L, C) = P(F1 |F2, ML,C) (4) P(F1 ∧ F2 |ML,C) =(5) P(F2 |ML,C) P xEχF1nχF2 P(X = x |ML,C) = P (6) xEχF2 P(X = x |ML,C) respect to the weights is ∂ ∂wi 198 where XF; is the set of worlds where Fi holds, and P(x |ML,C) is given by Equation 1. The question of whether a knowledge base entails a formula F in first-order logic is the question of whether P(F | LKB, CKB,F) = 1, where LKB is the MLN obtained by assigning infi</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45:503–528, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enzo Marinari</author>
<author>Giorgio Parisi</author>
</authors>
<title>Simulated Tempering: A new Monte Carlo scheme. Europhysics Letters,</title>
<date>1992</date>
<contexts>
<context position="9163" citStr="Marinari and Parisi, 1992" startWordPosition="1533" endWordPosition="1536">ula F in first-order logic is the question of whether P(F | LKB, CKB,F) = 1, where LKB is the MLN obtained by assigning infinite weight to all the formulas in KB, and CKB,F is the set of all constants appearing in KB or F. The most widely used approximate solution to probabilistic inference in MLNs is Markov chain Monte Carlo (MCMC) (Gilks et al., 1996). In this framework, the Gibbs sampling algorithm is to generate an instance from the distribution of each variable in turn, conditional on the current values of the other variables. One way to speed up Gibbs sampling is by Simulated Tempering (Marinari and Parisi, 1992), which performs simulation in a generalized ensemble, and can rapidly achieve an equilibrium state. Poon and Domingos (2006) proposed MC-SAT, an inference algorithm that combines ideas from MCMC and satisfiability. 4 Heuristic Human Knowledge Even though the Boosting model is able to accommodate a large number of features, some NEs, especially LOCs, ORGs and MISCs are difficult to identify due to lack of linguistic knowledge. For example, some ORGs are possibly mistagged as LOCs and/or MISCs. We incorporate heuristic human knowledge via MLNs to validate the Boosting NER hypotheses. We extract</context>
</contexts>
<marker>Marinari, Parisi, 1992</marker>
<rawString>Enzo Marinari and Giorgio Parisi. Simulated Tempering: A new Monte Carlo scheme. Europhysics Letters, 19:451–458, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic reasoning in intelligent systems: networks of plausible inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA,</location>
<contexts>
<context position="5259" citStr="Pearl, 1988" startWordPosition="807" endWordPosition="808">lgorithm (Freund and Schapire, 1997). It has been demonstrated that Boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). In particular, reasonable Chinese NER results were still obtained using Boosting, even though there was no Chinese-specific tuning and the model was only trained on one-third of the provided corpora in SIGHAN bakeoff-3 (Yu et al., 2006). 3 Markov Logic Networks A Markov Network (also known as Markov Random Field) is a model for the joint distribution of a set of variables (Pearl, 1988). It is composed of an undirected graph and a set of potential functions. A First-Order Knowledge Base (KB) (Genesereth and Nislsson, 1987) is a set of sentences or formulas in first-order logic. A Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a KB with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it species a ground Markov Network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. The weights associated with the formulas in an MLN jointly de</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Sound and efficient inference with probabilistic and deterministic dependencies.</title>
<date>2006</date>
<booktitle>In 21st National Conference on Artificial Intelligence (AAAI06),</booktitle>
<publisher>The AAAI Press.</publisher>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="9288" citStr="Poon and Domingos (2006)" startWordPosition="1552" endWordPosition="1555">e weight to all the formulas in KB, and CKB,F is the set of all constants appearing in KB or F. The most widely used approximate solution to probabilistic inference in MLNs is Markov chain Monte Carlo (MCMC) (Gilks et al., 1996). In this framework, the Gibbs sampling algorithm is to generate an instance from the distribution of each variable in turn, conditional on the current values of the other variables. One way to speed up Gibbs sampling is by Simulated Tempering (Marinari and Parisi, 1992), which performs simulation in a generalized ensemble, and can rapidly achieve an equilibrium state. Poon and Domingos (2006) proposed MC-SAT, an inference algorithm that combines ideas from MCMC and satisfiability. 4 Heuristic Human Knowledge Even though the Boosting model is able to accommodate a large number of features, some NEs, especially LOCs, ORGs and MISCs are difficult to identify due to lack of linguistic knowledge. For example, some ORGs are possibly mistagged as LOCs and/or MISCs. We incorporate heuristic human knowledge via MLNs to validate the Boosting NER hypotheses. We extract 151 location salient words and 783 organization salient words from the LDC Chinese-English bi-directional NE lists compiled </context>
</contexts>
<marker>Poon, Domingos, 2006</marker>
<rawString>Hoifung Poon and Pedro Domingos. Sound and efficient inference with probabilistic and deterministic dependencies. In 21st National Conference on Artificial Intelligence (AAAI06), Boston, Massachusetts, July 2006. The AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="5515" citStr="Richardson and Domingos, 2006" startWordPosition="846" endWordPosition="850"> reasonable Chinese NER results were still obtained using Boosting, even though there was no Chinese-specific tuning and the model was only trained on one-third of the provided corpora in SIGHAN bakeoff-3 (Yu et al., 2006). 3 Markov Logic Networks A Markov Network (also known as Markov Random Field) is a model for the joint distribution of a set of variables (Pearl, 1988). It is composed of an undirected graph and a set of potential functions. A First-Order Knowledge Base (KB) (Genesereth and Nislsson, 1987) is a set of sentences or formulas in first-order logic. A Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a KB with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it species a ground Markov Network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. The weights associated with the formulas in an MLN jointly determine the probabilities of those formulas (and vice versa) via a log-linear model. An MLN defines a probability distribution over Herbrand interpretations (possible worlds), and can be thought of as a template for constructing Markov Networks. The probab</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. Markov logic networks. Machine Learning, 62(1-2):107–136, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="4548" citStr="Schapire and Singer, 1999" startWordPosition="688" endWordPosition="691">e-based approaches. 197 Proceedings of NAACL HLT 2007, Companion Volume, pages 197–200, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ing them into a highly accurate classifier. Each weak classifier searches for the hypothesis in the hypotheses space that can best classify the current set of training examples. Based on the evaluation of each iteration, the algorithm re-weights the training examples, forcing the newly generated weak classifier to give higher weights to the examples that are misclassified in the previous iteration. We use the AdaBoost.MH algorithm (Schapire and Singer, 1999) as shown in Figure 1, an n-ary classification variant of the original well-known binary AdaBoost algorithm (Freund and Schapire, 1997). It has been demonstrated that Boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). In particular, reasonable Chinese NER results were still obtained using Boosting, even though there was no Chinese-specific tuning and the model was only trained on one-third of the provided corpora in SIGHAN bakeoff-3 (Yu et al., 2006). 3 Markov Logic Networks A Markov Networ</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297–336, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
</authors>
<title>Changning Huang, Haiyan Gao, and Jie Fang. Identifying Chinese names in unrestricted texts.</title>
<date>1995</date>
<journal>Journal of Chinese Information Processing,</journal>
<marker>Sun, 1995</marker>
<rawString>Maosong Sun, Changning Huang, Haiyan Gao, and Jie Fang. Identifying Chinese names in unrestricted texts. Journal of Chinese Information Processing, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Grace Ngai</author>
<author>Marine Carpuat</author>
<author>Jeppe Larsen</author>
<author>Yongsheng Yang</author>
</authors>
<title>Boosting for named entity recognition.</title>
<date>2002</date>
<booktitle>In Computational Natural Language Learning (CoNLL-2002), at COLING-2002,</booktitle>
<pages>195--198</pages>
<location>Taipei,</location>
<contexts>
<context position="4826" citStr="Wu et al. (2002)" startWordPosition="733" endWordPosition="736">n best classify the current set of training examples. Based on the evaluation of each iteration, the algorithm re-weights the training examples, forcing the newly generated weak classifier to give higher weights to the examples that are misclassified in the previous iteration. We use the AdaBoost.MH algorithm (Schapire and Singer, 1999) as shown in Figure 1, an n-ary classification variant of the original well-known binary AdaBoost algorithm (Freund and Schapire, 1997). It has been demonstrated that Boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). In particular, reasonable Chinese NER results were still obtained using Boosting, even though there was no Chinese-specific tuning and the model was only trained on one-third of the provided corpora in SIGHAN bakeoff-3 (Yu et al., 2006). 3 Markov Logic Networks A Markov Network (also known as Markov Random Field) is a model for the joint distribution of a set of variables (Pearl, 1988). It is composed of an undirected graph and a set of potential functions. A First-Order Knowledge Base (KB) (Genesereth and Nislsson, 1987) is a set of sentences or fo</context>
</contexts>
<marker>Wu, Ngai, Carpuat, Larsen, Yang, 2002</marker>
<rawString>Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and Yongsheng Yang. Boosting for named entity recognition. In Computational Natural Language Learning (CoNLL-2002), at COLING-2002, pages 195–198, Taipei, Sep 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Grace Ngai</author>
<author>Marine Carpuat</author>
</authors>
<title>Why nitpicking works: Evidence for Occam’s razor in error correctors.</title>
<date>2004</date>
<booktitle>In 20th International Conference on Computational Linguistics (COLING-2004),</booktitle>
<location>Geneva,</location>
<contexts>
<context position="4844" citStr="Wu et al. (2004)" startWordPosition="737" endWordPosition="740">e current set of training examples. Based on the evaluation of each iteration, the algorithm re-weights the training examples, forcing the newly generated weak classifier to give higher weights to the examples that are misclassified in the previous iteration. We use the AdaBoost.MH algorithm (Schapire and Singer, 1999) as shown in Figure 1, an n-ary classification variant of the original well-known binary AdaBoost algorithm (Freund and Schapire, 1997). It has been demonstrated that Boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). In particular, reasonable Chinese NER results were still obtained using Boosting, even though there was no Chinese-specific tuning and the model was only trained on one-third of the provided corpora in SIGHAN bakeoff-3 (Yu et al., 2006). 3 Markov Logic Networks A Markov Network (also known as Markov Random Field) is a model for the joint distribution of a set of variables (Pearl, 1988). It is composed of an undirected graph and a set of potential functions. A First-Order Knowledge Base (KB) (Genesereth and Nislsson, 1987) is a set of sentences or formulas in first-or</context>
</contexts>
<marker>Wu, Ngai, Carpuat, 2004</marker>
<rawString>Dekai Wu, Grace Ngai, and Marine Carpuat. Why nitpicking works: Evidence for Occam’s razor in error correctors. In 20th International Conference on Computational Linguistics (COLING-2004), Geneva, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yu</author>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Boosting for Chinese named entity recognition.</title>
<date>2006</date>
<booktitle>In 5th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Australia,</location>
<contexts>
<context position="1387" citStr="Yu et al., 2006" startWordPosition="194" endWordPosition="197">erforms the state-of-the-art Boosting model. 1 Introduction Named entity recognition (NER) involves the identification and classification of certain proper nouns in text, such as person names (PERs), locations (LOCs), organizations (ORGs), miscellaneous names (MISCs), temporal, numerical and monetary phrases. It is a wellestablished task in the NLP community and is regarded as crucial technology for many NLP applications, such as information extraction, question answering, information retrieval and machine translation. Compared to European-language NER, Chinese NER seems to be more difficult (Yu et al., 2006). Recent approaches to Chinese NER are a shift away from manually constructed rules or finite state patterns towards machine learning or statistical methods. However, rulebased NER systems lack robustness and portability, and machine learning approaches might be unsatisfactory to learn linguistic information in Chinese NEs. In fact, Chinese NEs have distinct linguistic characteristics in their composition and human beings usually use prior knowledge to recognize NEs. For example, about 365 of the highest frequently used surnames cover 99% Chinese surnames (Sun et al., 1995). For the LOC “✗5, ✂</context>
<context position="5107" citStr="Yu et al., 2006" startWordPosition="778" endWordPosition="781">use the AdaBoost.MH algorithm (Schapire and Singer, 1999) as shown in Figure 1, an n-ary classification variant of the original well-known binary AdaBoost algorithm (Freund and Schapire, 1997). It has been demonstrated that Boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). In particular, reasonable Chinese NER results were still obtained using Boosting, even though there was no Chinese-specific tuning and the model was only trained on one-third of the provided corpora in SIGHAN bakeoff-3 (Yu et al., 2006). 3 Markov Logic Networks A Markov Network (also known as Markov Random Field) is a model for the joint distribution of a set of variables (Pearl, 1988). It is composed of an undirected graph and a set of potential functions. A First-Order Knowledge Base (KB) (Genesereth and Nislsson, 1987) is a set of sentences or formulas in first-order logic. A Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a KB with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it species a ground Markov Network containing one feature for </context>
</contexts>
<marker>Yu, Carpuat, Wu, 2006</marker>
<rawString>Xiaofeng Yu, Marine Carpuat, and Dekai Wu. Boosting for Chinese named entity recognition. In 5th SIGHAN Workshop on Chinese Language Processing, Australia, July 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>