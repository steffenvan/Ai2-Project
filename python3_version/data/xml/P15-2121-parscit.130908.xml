<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007171">
<title confidence="0.9976305">
Constrained Semantic Forests for Improved Discriminative
Semantic Parsing
</title>
<author confidence="0.998067">
Wei Lu
</author>
<affiliation confidence="0.9964135">
Information Systems Technology and Design
Singapore University of Technology and Design
</affiliation>
<email confidence="0.995732">
luwei@sutd.edu.sg
</email>
<sectionHeader confidence="0.994726" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879666666667">
In this paper, we present a model for
improved discriminative semantic parsing.
The model addresses an important limi-
tation associated with our previous state-
of-the-art discriminative semantic parsing
model – the relaxed hybrid tree model
by introducing our constrained semantic
forests. We show that our model is able to
yield new state-of-the-art results on stan-
dard datasets even with simpler features.
Our system is available for download from
http://statnlp.org/research/sp/.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950444444444">
This paper addresses the problem of parsing natu-
ral language sentences into their corresponding se-
mantic representations in the form of formal logi-
cal representations. Such a task is also known as
semantic parsing (Kate and Mooney, 2006; Wong
and Mooney, 2007; Lu et al., 2008; Kwiatkowski
et al., 2010).
One state-of-the-art model for semantic pars-
ing is our recently introduced relaxed hybrid tree
model (Lu, 2014), which performs integrated lexi-
con acquisition and semantic parsing within a sin-
gle framework utilizing efficient algorithms for
training and inference. The model allows natural
language phrases to be recursively mapped to se-
mantic units, where certain long-distance depen-
dencies can be captured. It relies on representa-
tions called relaxed hybrid trees that can jointly
represent both the sentences and semantics. The
model is essentially discriminative, and allows
rich features to be incorporated.
Unfortunately, the relaxed hybrid tree model
has an important limitation: it essentially does
not allow certain sentence-semantics pairs to be
jointly encoded using the proposed relaxed hy-
brid tree representations. Thus, the model is un-
able to identify joint representations for certain
sentence-semantics pairs during the training pro-
cess, and is unable to produce desired outputs for
certain inputs during the evaluation process. In
this work, we propose a solution addressing the
above limitation, which makes our model more ro-
bust. Through experiments, we demonstrate that
our improved discriminative model for semantic
parsing, even when simpler features are used, is
able to obtain new state-of-the-art results on stan-
dard datasets.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998842206896552">
Semantic parsing has recently attracted a signif-
icant amount of attention in the community. In
this section, we provide a relatively brief discus-
sion of prior work in semantic parsing. The hy-
brid tree model (Lu et al., 2008) and the Bayesian
tree transducer based model (Jones et al., 2012)
are generative frameworks, which essentially as-
sume natural language and semantics are jointly
generated from an underlying generative process.
Such models are efficient, but are limited in their
predictive power due to the simple independence
assumptions made.
On the other hand, discriminative models are
able to exploit arbitrary features and are usually
able to give better results. Examples of such mod-
els include the WASP system (Wong and Mooney,
2006) which regards the semantic parsing prob-
lem as a statistical machine translation problem,
the UBL system (Kwiatkowski et al., 2010) which
performs CCG-based semantic parsing using a
log-linear model, as well as the relaxed hybrid tree
model (Lu, 2014) which extends the generative
hybrid tree model. This extension results in a dis-
criminative model that incorporates rich features
and allows long-distance dependencies to be cap-
tured. The relaxed hybrid tree model has achieved
the state-of-the-art results on standard benchmark
datasets across different languages.
Performing semantic parsing under other forms
</bodyText>
<page confidence="0.825548">
737
</page>
<note confidence="0.257334666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 737–742,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.924955333333333">
Ma
Ma
Ma
Mc
Mb
Md
w1 w2 w3 w4 w5
w6 w7 w8 w9 w10
(a)
Mb
w1 w2
w10
(w1 w2) w3 w4 w5 w6 w7 w8 w9 (w10)
Mb
(w3 w4 w5) w6 (w7 w8) w9
(b)
(w9)
Mc
Md
w9
w7 w8
Mc
w6
w3 w4 w5
Md
(w6)
(c)
</figure>
<figureCaption confidence="0.997511">
Figure 1: The semantics-sentence pair (a), an example hybrid tree (b), and an example relaxed hybrid
tree (c).
</figureCaption>
<bodyText confidence="0.998680454545454">
of supervision is also possible. Clarke et al. (2010)
proposed a model that learns a semantic parser for
answering questions without relying on semantic
annotations. Goldwasser et al. (2011) presented
a confidence-driven approach to semantic parsing
based on self-training. Liang et al. (2013) in-
troduced semantic parsers based on dependency
based semantics (DCS) that map sentences into
their denotations. In this work, we focus on pars-
ing sentences into their formal semantic represen-
tations.
</bodyText>
<sectionHeader confidence="0.995888" genericHeader="method">
3 Relaxed Hybrid Trees
</sectionHeader>
<bodyText confidence="0.99984325">
We briefly discuss our previously proposed re-
laxed hybrid tree model (Lu, 2014) in this section.
The model is a discriminative semantic parsing
model which extends the generative hybrid tree
model (Lu et al., 2008). Both systems are publicly
available1.
Let us use m to denote a complete semantic
representation, n to denote a complete natural lan-
guage sentence, and h to denote a complete latent
structure that jointly represents both m and n. The
model defines the conditional probability for ob-
serving a (m, h) pair for a given natural language
sentence n using a log-linear approach:
where A is the set of parameters (weights of fea-
tures) used by the model. Figure 1 (a) gives an
example sentence-semantics pair. A real example
taken from the GeoQuery dataset is shown in Fig-
ure 2.
Note that h is a complete latent structure that
jointly represents a natural language sentence and
</bodyText>
<footnote confidence="0.669465">
1http://statnlp.org/research/sp/
</footnote>
<figure confidence="0.463929333333333">
QUERY: answer(RIVER)
RIVER: exclude(RIVER, RIVER)
RIVER: river(all) RIVER: traverse(STATE)
STATE: stateid(STATENAME)
STATENAME : (0tn0)
What rivers do not run through Tennessee ?
</figure>
<figureCaption confidence="0.818691666666667">
Figure 2: An example tree-structured semantic
representation (above) and its corresponding nat-
ural language sentence (below).
</figureCaption>
<bodyText confidence="0.999948125">
its corresponding semantic representation. Typi-
cally, to limit the space of latent structures, certain
assumptions have to be made to h. In our work,
we assume that h must be from a space consisting
of relaxed hybrid tree structures (Lu, 2014).
The relaxed hybrid trees are analogous to the
hybrid trees, which was earlier introduced as a
generative framework. One major distinction be-
tween these two types of representations is that
the relaxed hybrid tree representations are able to
capture unbounded long-distance dependencies in
a principled way. Such dependencies were un-
able to be captured by hybrid tree representations
largely due to their generative settings. Figure 1
gives an example of a hybrid tree and a relaxed
hybrid tree representation encoding the sentence
w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 and the se-
mantics ma(mb(mc, md)).
In the hybrid tree structure, each word is strictly
associated with a semantic unit. For example the
word w3 is associated with the semantic unit mb.
In the relaxed hybrid tree, however, each word is
not only directly associated with exactly one se-
mantic unit m, but also indirectly associated with
</bodyText>
<equation confidence="0.98998">
·Φ(n,m,h)
PΛ(m, h|n) = eΛeΛ·Φ(n m0 h0) (1)
�m ,h0∈H(n,m0)
</equation>
<page confidence="0.989001">
738
</page>
<figure confidence="0.999189125">
ma
w1 w2
. . .
ma
mb
ma
mb
#Args Patterns
0 w
1 [w]X[w]
2 [w]X[w]Y[w],[w]Y[w]X[w]
mb
mc
mc
w1
md
w2
md
w1 w2
mc
md
(w1)
(w2)
w1 w2
</figure>
<tableCaption confidence="0.8782454">
Table 1: The patterns allowed for our model. [w]
denotes an optional sequence of natural language
words. E.g., [w]X[w] refers to the following 4
patterns: wX, Xw, wXw, and X (the pattern ex-
cluded by the relaxed hybrid tree model).
</tableCaption>
<figure confidence="0.996457">
(a) (b) (c)
</figure>
<figureCaption confidence="0.99954">
Figure 3: (a) Example semantics-sentence pair
</figureCaption>
<bodyText confidence="0.974223681818182">
that cannot be jointly represented with relaxed hy-
brid trees if pattern X is disallowed. (b) Exam-
ple relaxed hybrid tree that consists of an infinite
number of nodes when pattern X is allowed. (c)
Example hybrid tree jointly representing both the
semantics and the sentence (where pattern X is al-
lowed).
all other semantic units that are predecessors of m.
For example, the word w3 now is directly associ-
ated with mb, but is also indirectly associated with
ma. These indirect associations allow the long-
distance dependencies to be captured.
Both the hybrid tree and relaxed hybrid tree
models define patterns at each level of their latent
structure which specify how the words and child
semantic units are organized at each level. For
example, within the semantic unit ma, we have
a pattern wXw which states that we first have
words that are directly associated with ma, fol-
lowed by some words covered by its first child se-
mantic unit, then another sequence of words di-
rectly associated with ma.
</bodyText>
<subsectionHeader confidence="0.997923">
3.1 Limitations
</subsectionHeader>
<bodyText confidence="0.999984653846154">
One important difference between the hybrid tree
representations and the relaxed hybrid tree repre-
sentations is the exclusion of the pattern X in the
latter. This ensured relaxed hybrid trees with an
infinite number of nodes were not considered (Lu,
2014) when computing the denominator term of
Equation 1. In relaxed hybrid tree, &apos;H(n, m) was
implemented as a packed forest representation for
exponentially many possible relaxed hybrid trees
where pattern X was excluded.
By allowing pattern X, we allow certain seman-
tic units with no natural language word counter-
part to exist in the joint relaxed hybrid tree repre-
sentation. This may lead to possible relaxed hy-
brid tree representations consisting of an infinite
number of internal nodes (semantic units), as seen
in Figure 3 (b). When pattern X is allowed, both
ma and mb are not directly associated with any
natural language word, so we are able to further
insert arbitrarily many (compatible) semantic units
between the two units ma and mb while the re-
sulting relaxed hybrid tree remains valid. There-
fore we can construct a relaxed hybrid tree repre-
sentation that contains the given natural language
sentence w1 w2 with an infinite number of nodes.
This issue essentially prevents us from comput-
ing the denominator term of Equation 1 since it
involves an infinite number of possible m&apos; and h&apos;.
To eliminate relaxed hybrid trees consisting of
an infinite number of nodes, pattern X is dis-
allowed in the relaxed hybrid trees model (Lu,
2014). However, disallowing pattern X has led
to other issues. Specifically, for certain semantics-
sentence pairs, it is not possible to find relaxed hy-
brid trees that jointly represent them. In the exam-
ple semantics-sentence pair given in Figure 3 (a),
it is not possible to find any relaxed hybrid tree that
contains both the sentence and the semantics since
each semantic unit which takes one argument must
be associated with at least one word. On the other
hand, it is still possible to find a hybrid tree repre-
sentation for both the sentence and the semantics
where pattern X is allowed (see Figure 3 (c)).
In practice, we can alleviate this issue by ex-
tending the lengths of the sentences. For example,
we can append the special beginning-of-sentence
symbol (s) and end-of-sentence symbol (/s) to
all sentences to increase their lengths, allowing
the relaxed hybrid trees to be constructed for cer-
tain sentence-semantics pairs with short sentences.
However, such an approach does not resolve the
theoretical limitation of the model.
</bodyText>
<page confidence="0.998597">
739
</page>
<sectionHeader confidence="0.987234" genericHeader="method">
4 Constrained Semantic Forests
</sectionHeader>
<bodyText confidence="0.999877214285714">
To address this limitation, we allow pattern X to
be included when building our new discrimina-
tive semantic parsing model. However, as men-
tioned above, doing so will lead to latent struc-
tures (relaxed hybrid tree representations) of infi-
nite heights. To resolve such an issue, we instead
add an additional constraint – limiting the height
of a semantic representation to a fixed constant c,
where c is larger than the maximum height of all
the trees appearing in the training set.
Table 1 summarizes the list of patterns that our
model considers. This is essentially the same as
those considered by the hybrid tree model.
Our new objective function is as follows:
</bodyText>
<equation confidence="0.9669">
PA(m, h|n)
eA·Φ(n,m,h)
r.m&apos;∈M,h&apos;∈H&apos;(n,m&apos;) eA·Φ(n,m&apos;,h&apos;) (2)
</equation>
<bodyText confidence="0.999979891891892">
where M refers to the set of all possible seman-
tic trees whose heights are less than or equal to c,
and H0(n, m0) refers to the set of possible relaxed
hybrid tree representations where the pattern X is
allowed.
The main challenge now becomes the compu-
tation of the denominator term in Equation 2, as
the set M is still very large. To properly handle
all such semantic trees in an efficient way, we in-
troduce a constrained semantic forest (CSF) rep-
resentation of M here. Such a constrained seman-
tic forest is a packed forest representation of ex-
ponentially many possible unique semantic trees,
where we set the height of the forest to c. By con-
trast, it was not possible in our previous relaxed
hybrid tree model to introduce such a compact
representation over all possible semantic trees. In
our previous model’s implementation, we directly
constructed for each sentence n a different com-
pact representation over all possible relaxed hy-
brid trees containing n.
Setting the maximum height to c effectively
guarantees that all semantic trees contained in
the constrained semantic forest have a height no
greater than c. We then constructed the (exponen-
tially many) relaxed hybrid tree representations
based on the constrained semantic forest M and
each input sentence n. We used a single packed
forest representation to represent all such relaxed
hybrid tree representations. This allows the com-
putation of the denominator to be performed ef-
ficiently using similar dynamic programming al-
gorithms described in (Lu, 2014). Optimization
of the model parameters were done by using L-
BFGS (Liu and Nocedal, 1989), where the gradi-
ents were computed efficiently using an analogous
dynamic programming algorithm.
</bodyText>
<sectionHeader confidence="0.998036" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99992823255814">
Our experiments were conducted on the publicly
available multilingual GeoQuery dataset. Vari-
ous previous works on semantic parsing used this
dataset for evaluations (Wong and Mooney, 2006;
Kate and Mooney, 2006; Lu et al., 2008; Jones
et al., 2012). The dataset consists of 880 natural
language sentences where each sentence is cou-
pled with a formal tree-structured semantic repre-
sentation. The early version of this dataset was
annotated with English only (Wong and Mooney,
2006; Kate and Mooney, 2006), and Jones et al.
(2012) released a version that is annotated with
three additional languages: German, Greek and
Thai. To make our system directly comparable to
previous works, we used the same train/test split
used in those works (Jones et al., 2012; Lu, 2014)
for evaluation. We also followed the standard ap-
proach for evaluating the correctness of an output
semantic representation from our system. Specifi-
cally, we used a standard script to construct Prolog
queries based on the outputs, and used the queries
to retrieve answers from the GeoQuery database.
Following previous works, we regarded an out-
put semantic representation as correct if and only
if it returned the same answers as the gold stan-
dard (Jones et al., 2012; Lu, 2014).
The results of our system as well as those of
several previous systems are given in Table 2.
We compared our system’s performance against
those of several previous works. The WASP sys-
tem (Wong and Mooney, 2006) is based on statis-
tical machine translation technique while the HY-
BRIDTREE+ system (Lu et al., 2008) is based on
the generative hybrid tree model augmented with
a discriminative re-ranking stage where certain
global features are used. UBL-S (Kwiatkowski et
al., 2010) is a CCG-based semantic parsing sys-
tem. TREETRANS (Jones et al., 2012) is the sys-
tem based on tree transducers. RHT (Lu, 2014) is
the discriminative semantic parsing system based
on relaxed hybrid trees.
In practice, we set c (the maximum height of
a semantic representation) to 20 in our experi-
</bodyText>
<page confidence="0.988143">
740
</page>
<table confidence="0.9932005">
System English Thai German Greek
Acc. F Acc. F Acc. F Acc. F
WASP 71.1 77.7 71.4 75.0 65.7 74.9 70.7 78.6
HYBRIDTREE+ 76.8 81.0 73.6 76.7 62.1 68.5 69.3 74.6
UBL-S 82.1 82.1 66.4 66.4 75.0 75.0 73.6 73.7
TREETRANS 79.3 79.3 78.2 78.2 74.6 74.6 75.4 75.4
RHT (all features) 83.6 83.6 79.3 79.3 74.3 74.3 78.2 78.2
This work 86.8 86.8 80.7 80.7 75.7 75.7 79.3 79.3
</table>
<tableCaption confidence="0.95629">
Table 2: Performance of various works across four different languages. Acc.: accuracy percentage, F:
F1-measure percentage.
</tableCaption>
<bodyText confidence="0.999805609756098">
ments, which we determined based on the heights
of the semantic trees that appear in the training
data. Results showed that our system consistently
yielded higher results than all the previous sys-
tems, including our state-of-the-art relaxed hybrid
tree system (the full model, when all the features
are used), in terms of both accuracy score and F1-
measure. We would like to highlight two potential
advantages of our new model over the old RHT
model. First, our model is able to handle certain
sentence-semantics pairs which could not be han-
dled by RHT during both training and evaluation
as discussed in Section 3.1. Second, our model
considers the additional pattern X and therefore
has the capability to capture more accurate depen-
dencies between the words and semantic units.
We note that in our experiments we used a small
subset of the features used by our relaxed hy-
brid tree work. Specifically, we did not use any
long-distance features, and also did not use any
character-level features. As we have mentioned
in (Lu, 2014), although the RHT model is able
to capture unbounded long-distance dependencies,
for certain languages such as German such long-
distance features appeared to be detrimental to
the performance of the system (Lu, 2014, Table
4). Here in this work, we only used simple un-
igram features (concatenation of a semantic unit
and an individual word that appears directly below
that unit in the joint representation), pattern fea-
tures (concatenation of a semantic unit and the pat-
tern below that unit) as well as transition features
(concatenation of two semantic units that form a
parent-child relationship) described in (Lu, 2014).
While additional features could potentially lead to
better results, using simpler features would make
our model more compact and more interpretable.
We summarized in Table 3 the number of features
used in both the previous RHT system and our sys-
tem across four different languages. It can be seen
that our system only required about 2-3% of the
</bodyText>
<table confidence="0.981803">
System English Thai German Greek
RHT 2.1×106 2.3×106 2.7×106 2.6×106
This work 5.4×104 5.2×104 7.5×104 6.9×104
</table>
<tableCaption confidence="0.986583">
Table 3: Number of features involved for both
</tableCaption>
<bodyText confidence="0.988326272727273">
the RHT system and our new system using con-
strained semantic forests, across four different lan-
guages.
features used in the previous system.
We also note that the training time for our
model is longer than that of the relaxed hybrid tree
model since the space for V(n, m&apos;) is now much
larger than the space for x(n, m&apos;). In practice,
to make the overall training process faster, we im-
plemented a parallel version of the original RHT
algorithm.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993266666667">
In this work, we presented an improved discrim-
inative approach to semantic parsing. Our ap-
proach does not have the theoretical limitation
associated with our previous state-of-the-art ap-
proach. We demonstrated through experiments
that our new model was able to yield new state-
of-the-art results on a standard dataset across four
different languages, even though simpler features
were used. Since our new model involves simpler
features, including unigram features defined over
individual semantic unit – word pairs, we believe
our new model would aid the joint modeling of
both distributional and logical semantics (Lewis
and Steedman, 2013) within a single framework.
We plan to explore this avenue in the future.
</bodyText>
<sectionHeader confidence="0.995056" genericHeader="acknowledgments">
Acknoledgments
</sectionHeader>
<bodyText confidence="0.986808333333333">
The author would like to thank the anonymous
reviewers for their helpful comments. This
work was supported by SUTD grant SRG ISTD
2013 064 and was partially supported by project
61472191 under the National Natural Science
Foundation of China.
</bodyText>
<page confidence="0.99676">
741
</page>
<sectionHeader confidence="0.993832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998312428571429">
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world’s response. In Proc. of CONLL ’10, pages
18–27.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proc. of ACL ’11, pages 1486–
1495.
Bevan Keeley Jones, Mark Johnson, and Sharon Gold-
water. 2012. Semantic parsing with bayesian tree
transducers. In Proc. of ACL ’12, pages 488–496.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proc. of COLING/ACL, pages 913–920.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proc. EMNLP’10, pages 1223–
1233.
Mike Lewis and Mark Steedman. 2013. Combin-
ing distributional and logical semantics. Transac-
tions of the Association for Computational Linguis-
tics, 1:179–192.
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389–446.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503–528, December.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proc. of EMNLP ’08, pages 783–792.
Wei Lu. 2014. Semantic parsing with relaxed hybrid
trees. In Proc. of EMNLP ’14.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proc. of HLT/NAACL ’06,
pages 439–446.
Yuk Wah Wong and Raymond J Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proc. of ACL ’07.
</reference>
<page confidence="0.997223">
742
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.769763">
<title confidence="0.996766">Constrained Semantic Forests for Improved Semantic Parsing</title>
<author confidence="0.990864">Wei</author>
<affiliation confidence="0.9878315">Information Systems Technology and Singapore University of Technology and</affiliation>
<email confidence="0.897069">luwei@sutd.edu.sg</email>
<abstract confidence="0.999722545454545">In this paper, we present a model for improved discriminative semantic parsing. The model addresses an important limitation associated with our previous stateof-the-art discriminative semantic parsing – the hybrid tree introducing our semantic We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features.</abstract>
<intro confidence="0.88844">Our system is available for download from</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proc. of CONLL ’10,</booktitle>
<pages>18--27</pages>
<contexts>
<context position="4349" citStr="Clarke et al. (2010)" startWordPosition="670" endWordPosition="673">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 737–742, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Ma Ma Ma Mc Mb Md w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 (a) Mb w1 w2 w10 (w1 w2) w3 w4 w5 w6 w7 w8 w9 (w10) Mb (w3 w4 w5) w6 (w7 w8) w9 (b) (w9) Mc Md w9 w7 w8 Mc w6 w3 w4 w5 Md (w6) (c) Figure 1: The semantics-sentence pair (a), an example hybrid tree (b), and an example relaxed hybrid tree (c). of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training. Liang et al. (2013) introduced semantic parsers based on dependency based semantics (DCS) that map sentences into their denotations. In this work, we focus on parsing sentences into their formal semantic representations. 3 Relaxed Hybrid Trees We briefly discuss our previously proposed relaxed hybrid tree model (Lu, 2014) in this section. The model is a discriminative semantic</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In Proc. of CONLL ’10, pages 18–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Roi Reichart</author>
<author>James Clarke</author>
<author>Dan Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proc. of ACL ’11,</booktitle>
<pages>1486--1495</pages>
<contexts>
<context position="4486" citStr="Goldwasser et al. (2011)" startWordPosition="690" endWordPosition="693"> Natural Language Processing (Short Papers), pages 737–742, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Ma Ma Ma Mc Mb Md w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 (a) Mb w1 w2 w10 (w1 w2) w3 w4 w5 w6 w7 w8 w9 (w10) Mb (w3 w4 w5) w6 (w7 w8) w9 (b) (w9) Mc Md w9 w7 w8 Mc w6 w3 w4 w5 Md (w6) (c) Figure 1: The semantics-sentence pair (a), an example hybrid tree (b), and an example relaxed hybrid tree (c). of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training. Liang et al. (2013) introduced semantic parsers based on dependency based semantics (DCS) that map sentences into their denotations. In this work, we focus on parsing sentences into their formal semantic representations. 3 Relaxed Hybrid Trees We briefly discuss our previously proposed relaxed hybrid tree model (Lu, 2014) in this section. The model is a discriminative semantic parsing model which extends the generative hybrid tree model (Lu et al., 2008). Both systems are publicly available1. Let us use m to de</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In Proc. of ACL ’11, pages 1486– 1495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Keeley Jones</author>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Semantic parsing with bayesian tree transducers.</title>
<date>2012</date>
<booktitle>In Proc. of ACL ’12,</booktitle>
<pages>488--496</pages>
<contexts>
<context position="2654" citStr="Jones et al., 2012" startWordPosition="393" endWordPosition="396">ocess. In this work, we propose a solution addressing the above limitation, which makes our model more robust. Through experiments, we demonstrate that our improved discriminative model for semantic parsing, even when simpler features are used, is able to obtain new state-of-the-art results on standard datasets. 2 Related Work Semantic parsing has recently attracted a significant amount of attention in the community. In this section, we provide a relatively brief discussion of prior work in semantic parsing. The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which per</context>
<context position="13923" citStr="Jones et al., 2012" startWordPosition="2250" endWordPosition="2253">hybrid tree representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in (Lu, 2014). Optimization of the model parameters were done by using LBFGS (Liu and Nocedal, 1989), where the gradients were computed efficiently using an analogous dynamic programming algorithm. 5 Experiments Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). The dataset consists of 880 natural language sentences where each sentence is coupled with a formal tree-structured semantic representation. The early version of this dataset was annotated with English only (Wong and Mooney, 2006; Kate and Mooney, 2006), and Jones et al. (2012) released a version that is annotated with three additional languages: German, Greek and Thai. To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; Lu, 2014) for evaluation. We also followed the standard approach for evaluating the correctn</context>
<context position="15469" citStr="Jones et al., 2012" startWordPosition="2504" endWordPosition="2507">the same answers as the gold standard (Jones et al., 2012; Lu, 2014). The results of our system as well as those of several previous systems are given in Table 2. We compared our system’s performance against those of several previous works. The WASP system (Wong and Mooney, 2006) is based on statistical machine translation technique while the HYBRIDTREE+ system (Lu et al., 2008) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. RHT (Lu, 2014) is the discriminative semantic parsing system based on relaxed hybrid trees. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi740 System English Thai German Greek Acc. F Acc. F Acc. F Acc. F WASP 71.1 77.7 71.4 75.0 65.7 74.9 70.7 78.6 HYBRIDTREE+ 76.8 81.0 73.6 76.7 62.1 68.5 69.3 74.6 UBL-S 82.1 82.1 66.4 66.4 75.0 75.0 73.6 73.7 TREETRANS 79.3 79.3 78.2 78.2 74.6 74.6 75.4 75.4 RHT (all features) 83.6 83.6 79.3 79.3 74.3 74.3 78.2 78.2 This work 86.8 86.8 80.7 80.7 75.7 75.7 79.3 79.3 Table 2:</context>
</contexts>
<marker>Jones, Johnson, Goldwater, 2012</marker>
<rawString>Bevan Keeley Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with bayesian tree transducers. In Proc. of ACL ’12, pages 488–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>913--920</pages>
<contexts>
<context position="928" citStr="Kate and Mooney, 2006" startWordPosition="128" endWordPosition="131">t limitation associated with our previous stateof-the-art discriminative semantic parsing model – the relaxed hybrid tree model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. 1 Introduction This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). One state-of-the-art model for semantic parsing is our recently introduced relaxed hybrid tree model (Lu, 2014), which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. The model allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. It relies on representations called relaxed hybrid trees that can jointly represent both the sentences and semantics. The </context>
<context position="13885" citStr="Kate and Mooney, 2006" startWordPosition="2242" endWordPosition="2245">sentation to represent all such relaxed hybrid tree representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in (Lu, 2014). Optimization of the model parameters were done by using LBFGS (Liu and Nocedal, 1989), where the gradients were computed efficiently using an analogous dynamic programming algorithm. 5 Experiments Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). The dataset consists of 880 natural language sentences where each sentence is coupled with a formal tree-structured semantic representation. The early version of this dataset was annotated with English only (Wong and Mooney, 2006; Kate and Mooney, 2006), and Jones et al. (2012) released a version that is annotated with three additional languages: German, Greek and Thai. To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; Lu, 2014) for evaluation. We also followed the standar</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proc. of COLING/ACL, pages 913–920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic ccg grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP’10,</booktitle>
<pages>1223--1233</pages>
<contexts>
<context position="995" citStr="Kwiatkowski et al., 2010" startWordPosition="140" endWordPosition="143">iminative semantic parsing model – the relaxed hybrid tree model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. 1 Introduction This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). One state-of-the-art model for semantic parsing is our recently introduced relaxed hybrid tree model (Lu, 2014), which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. The model allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. It relies on representations called relaxed hybrid trees that can jointly represent both the sentences and semantics. The model is essentially discriminative, and allows rich features to be</context>
<context position="3244" citStr="Kwiatkowski et al., 2010" startWordPosition="483" endWordPosition="486">er based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the relaxed hybrid tree model (Lu, 2014) which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The relaxed hybrid tree model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms 737 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International J</context>
<context position="15398" citStr="Kwiatkowski et al., 2010" startWordPosition="2492" endWordPosition="2495">rded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; Lu, 2014). The results of our system as well as those of several previous systems are given in Table 2. We compared our system’s performance against those of several previous works. The WASP system (Wong and Mooney, 2006) is based on statistical machine translation technique while the HYBRIDTREE+ system (Lu et al., 2008) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. RHT (Lu, 2014) is the discriminative semantic parsing system based on relaxed hybrid trees. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi740 System English Thai German Greek Acc. F Acc. F Acc. F Acc. F WASP 71.1 77.7 71.4 75.0 65.7 74.9 70.7 78.6 HYBRIDTREE+ 76.8 81.0 73.6 76.7 62.1 68.5 69.3 74.6 UBL-S 82.1 82.1 66.4 66.4 75.0 75.0 73.6 73.7 TREETRANS 79.3 79.3 78.2 78.2 74.6 74.6 75.4 75.4 RHT (all features) 83.6 83.6 79.3 79.3 74.3 74</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higherorder unification. In Proc. EMNLP’10, pages 1223– 1233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Combining distributional and logical semantics.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--179</pages>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combining distributional and logical semantics. Transactions of the Association for Computational Linguistics, 1:179–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="4589" citStr="Liang et al. (2013)" startWordPosition="704" endWordPosition="707">tion for Computational Linguistics Ma Ma Ma Mc Mb Md w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 (a) Mb w1 w2 w10 (w1 w2) w3 w4 w5 w6 w7 w8 w9 (w10) Mb (w3 w4 w5) w6 (w7 w8) w9 (b) (w9) Mc Md w9 w7 w8 Mc w6 w3 w4 w5 Md (w6) (c) Figure 1: The semantics-sentence pair (a), an example hybrid tree (b), and an example relaxed hybrid tree (c). of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training. Liang et al. (2013) introduced semantic parsers based on dependency based semantics (DCS) that map sentences into their denotations. In this work, we focus on parsing sentences into their formal semantic representations. 3 Relaxed Hybrid Trees We briefly discuss our previously proposed relaxed hybrid tree model (Lu, 2014) in this section. The model is a discriminative semantic parsing model which extends the generative hybrid tree model (Lu et al., 2008). Both systems are publicly available1. Let us use m to denote a complete semantic representation, n to denote a complete natural language sentence, and h to den</context>
</contexts>
<marker>Liang, Jordan, Klein, 2013</marker>
<rawString>Percy Liang, Michael I Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<journal>Math. Program.,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="13563" citStr="Liu and Nocedal, 1989" startWordPosition="2196" endWordPosition="2199">eight to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) relaxed hybrid tree representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such relaxed hybrid tree representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in (Lu, 2014). Optimization of the model parameters were done by using LBFGS (Liu and Nocedal, 1989), where the gradients were computed efficiently using an analogous dynamic programming algorithm. 5 Experiments Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). The dataset consists of 880 natural language sentences where each sentence is coupled with a formal tree-structured semantic representation. The early version of this dataset was annotated with English only (Wong and Mooney, 2006; Kate an</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Math. Program., 45(3):503–528, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP ’08,</booktitle>
<pages>783--792</pages>
<contexts>
<context position="968" citStr="Lu et al., 2008" startWordPosition="136" endWordPosition="139">eof-the-art discriminative semantic parsing model – the relaxed hybrid tree model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. 1 Introduction This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). One state-of-the-art model for semantic parsing is our recently introduced relaxed hybrid tree model (Lu, 2014), which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. The model allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. It relies on representations called relaxed hybrid trees that can jointly represent both the sentences and semantics. The model is essentially discriminative, and</context>
<context position="2588" citStr="Lu et al., 2008" startWordPosition="382" endWordPosition="385">uce desired outputs for certain inputs during the evaluation process. In this work, we propose a solution addressing the above limitation, which makes our model more robust. Through experiments, we demonstrate that our improved discriminative model for semantic parsing, even when simpler features are used, is able to obtain new state-of-the-art results on standard datasets. 2 Related Work Semantic parsing has recently attracted a significant amount of attention in the community. In this section, we provide a relatively brief discussion of prior work in semantic parsing. The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine transl</context>
<context position="5028" citStr="Lu et al., 2008" startWordPosition="774" endWordPosition="777">uestions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training. Liang et al. (2013) introduced semantic parsers based on dependency based semantics (DCS) that map sentences into their denotations. In this work, we focus on parsing sentences into their formal semantic representations. 3 Relaxed Hybrid Trees We briefly discuss our previously proposed relaxed hybrid tree model (Lu, 2014) in this section. The model is a discriminative semantic parsing model which extends the generative hybrid tree model (Lu et al., 2008). Both systems are publicly available1. Let us use m to denote a complete semantic representation, n to denote a complete natural language sentence, and h to denote a complete latent structure that jointly represents both m and n. The model defines the conditional probability for observing a (m, h) pair for a given natural language sentence n using a log-linear approach: where A is the set of parameters (weights of features) used by the model. Figure 1 (a) gives an example sentence-semantics pair. A real example taken from the GeoQuery dataset is shown in Figure 2. Note that h is a complete la</context>
<context position="13902" citStr="Lu et al., 2008" startWordPosition="2246" endWordPosition="2249">all such relaxed hybrid tree representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in (Lu, 2014). Optimization of the model parameters were done by using LBFGS (Liu and Nocedal, 1989), where the gradients were computed efficiently using an analogous dynamic programming algorithm. 5 Experiments Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). The dataset consists of 880 natural language sentences where each sentence is coupled with a formal tree-structured semantic representation. The early version of this dataset was annotated with English only (Wong and Mooney, 2006; Kate and Mooney, 2006), and Jones et al. (2012) released a version that is annotated with three additional languages: German, Greek and Thai. To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; Lu, 2014) for evaluation. We also followed the standard approach for ev</context>
<context position="15231" citStr="Lu et al., 2008" startWordPosition="2467" endWordPosition="2470">cript to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database. Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; Lu, 2014). The results of our system as well as those of several previous systems are given in Table 2. We compared our system’s performance against those of several previous works. The WASP system (Wong and Mooney, 2006) is based on statistical machine translation technique while the HYBRIDTREE+ system (Lu et al., 2008) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. RHT (Lu, 2014) is the discriminative semantic parsing system based on relaxed hybrid trees. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi740 System English Thai German Greek Acc. F Acc. F Acc. F Acc. F WASP 71.1 77.7 71.4 75.0 65.7 74.9 70.7 78.6 HYBRIDTREE+ 76.8 81.0 73.6 </context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proc. of EMNLP ’08, pages 783–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
</authors>
<title>Semantic parsing with relaxed hybrid trees.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP ’14.</booktitle>
<contexts>
<context position="1108" citStr="Lu, 2014" startWordPosition="159" endWordPosition="160">r model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. 1 Introduction This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). One state-of-the-art model for semantic parsing is our recently introduced relaxed hybrid tree model (Lu, 2014), which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. The model allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. It relies on representations called relaxed hybrid trees that can jointly represent both the sentences and semantics. The model is essentially discriminative, and allows rich features to be incorporated. Unfortunately, the relaxed hybrid tree model has an important limitation: it essentially does not </context>
<context position="3364" citStr="Lu, 2014" startWordPosition="504" endWordPosition="505">ated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the relaxed hybrid tree model (Lu, 2014) which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The relaxed hybrid tree model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms 737 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 737–742, Beijing, China, July 26-31, 2015. c�2015 A</context>
<context position="4893" citStr="Lu, 2014" startWordPosition="754" endWordPosition="755">d tree (c). of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training. Liang et al. (2013) introduced semantic parsers based on dependency based semantics (DCS) that map sentences into their denotations. In this work, we focus on parsing sentences into their formal semantic representations. 3 Relaxed Hybrid Trees We briefly discuss our previously proposed relaxed hybrid tree model (Lu, 2014) in this section. The model is a discriminative semantic parsing model which extends the generative hybrid tree model (Lu et al., 2008). Both systems are publicly available1. Let us use m to denote a complete semantic representation, n to denote a complete natural language sentence, and h to denote a complete latent structure that jointly represents both m and n. The model defines the conditional probability for observing a (m, h) pair for a given natural language sentence n using a log-linear approach: where A is the set of parameters (weights of features) used by the model. Figure 1 (a) give</context>
<context position="6280" citStr="Lu, 2014" startWordPosition="972" endWordPosition="973">tural language sentence and 1http://statnlp.org/research/sp/ QUERY: answer(RIVER) RIVER: exclude(RIVER, RIVER) RIVER: river(all) RIVER: traverse(STATE) STATE: stateid(STATENAME) STATENAME : (0tn0) What rivers do not run through Tennessee ? Figure 2: An example tree-structured semantic representation (above) and its corresponding natural language sentence (below). its corresponding semantic representation. Typically, to limit the space of latent structures, certain assumptions have to be made to h. In our work, we assume that h must be from a space consisting of relaxed hybrid tree structures (Lu, 2014). The relaxed hybrid trees are analogous to the hybrid trees, which was earlier introduced as a generative framework. One major distinction between these two types of representations is that the relaxed hybrid tree representations are able to capture unbounded long-distance dependencies in a principled way. Such dependencies were unable to be captured by hybrid tree representations largely due to their generative settings. Figure 1 gives an example of a hybrid tree and a relaxed hybrid tree representation encoding the sentence w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 and the semantics ma(mb(mc, md)). In</context>
<context position="8926" citStr="Lu, 2014" startWordPosition="1428" endWordPosition="1429">cify how the words and child semantic units are organized at each level. For example, within the semantic unit ma, we have a pattern wXw which states that we first have words that are directly associated with ma, followed by some words covered by its first child semantic unit, then another sequence of words directly associated with ma. 3.1 Limitations One important difference between the hybrid tree representations and the relaxed hybrid tree representations is the exclusion of the pattern X in the latter. This ensured relaxed hybrid trees with an infinite number of nodes were not considered (Lu, 2014) when computing the denominator term of Equation 1. In relaxed hybrid tree, &apos;H(n, m) was implemented as a packed forest representation for exponentially many possible relaxed hybrid trees where pattern X was excluded. By allowing pattern X, we allow certain semantic units with no natural language word counterpart to exist in the joint relaxed hybrid tree representation. This may lead to possible relaxed hybrid tree representations consisting of an infinite number of internal nodes (semantic units), as seen in Figure 3 (b). When pattern X is allowed, both ma and mb are not directly associated w</context>
<context position="10166" citStr="Lu, 2014" startWordPosition="1636" endWordPosition="1637">so we are able to further insert arbitrarily many (compatible) semantic units between the two units ma and mb while the resulting relaxed hybrid tree remains valid. Therefore we can construct a relaxed hybrid tree representation that contains the given natural language sentence w1 w2 with an infinite number of nodes. This issue essentially prevents us from computing the denominator term of Equation 1 since it involves an infinite number of possible m&apos; and h&apos;. To eliminate relaxed hybrid trees consisting of an infinite number of nodes, pattern X is disallowed in the relaxed hybrid trees model (Lu, 2014). However, disallowing pattern X has led to other issues. Specifically, for certain semanticssentence pairs, it is not possible to find relaxed hybrid trees that jointly represent them. In the example semantics-sentence pair given in Figure 3 (a), it is not possible to find any relaxed hybrid tree that contains both the sentence and the semantics since each semantic unit which takes one argument must be associated with at least one word. On the other hand, it is still possible to find a hybrid tree representation for both the sentence and the semantics where pattern X is allowed (see Figure 3 </context>
<context position="13476" citStr="Lu, 2014" startWordPosition="2183" endWordPosition="2184">over all possible relaxed hybrid trees containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) relaxed hybrid tree representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such relaxed hybrid tree representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in (Lu, 2014). Optimization of the model parameters were done by using LBFGS (Liu and Nocedal, 1989), where the gradients were computed efficiently using an analogous dynamic programming algorithm. 5 Experiments Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). The dataset consists of 880 natural language sentences where each sentence is coupled with a formal tree-structured semantic representation. The early </context>
<context position="14918" citStr="Lu, 2014" startWordPosition="2415" endWordPosition="2416"> system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; Lu, 2014) for evaluation. We also followed the standard approach for evaluating the correctness of an output semantic representation from our system. Specifically, we used a standard script to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database. Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; Lu, 2014). The results of our system as well as those of several previous systems are given in Table 2. We compared our system’s performance against those of several previous works. The WASP system (Wong and Mooney, 2006) is based on statistical machine translation technique while the HYBRIDTREE+ system (Lu et al., 2008) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. RHT (Lu</context>
<context position="17217" citStr="Lu, 2014" startWordPosition="2805" endWordPosition="2806">odel over the old RHT model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by RHT during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our relaxed hybrid tree work. Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in (Lu, 2014), although the RHT model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (Lu, 2014, Table 4). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) d</context>
</contexts>
<marker>Lu, 2014</marker>
<rawString>Wei Lu. 2014. Semantic parsing with relaxed hybrid trees. In Proc. of EMNLP ’14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL ’06,</booktitle>
<pages>439--446</pages>
<contexts>
<context position="3113" citStr="Wong and Mooney, 2006" startWordPosition="463" endWordPosition="466">ively brief discussion of prior work in semantic parsing. The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the relaxed hybrid tree model (Lu, 2014) which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The relaxed hybrid tree model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under </context>
<context position="13862" citStr="Wong and Mooney, 2006" startWordPosition="2238" endWordPosition="2241">gle packed forest representation to represent all such relaxed hybrid tree representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in (Lu, 2014). Optimization of the model parameters were done by using LBFGS (Liu and Nocedal, 1989), where the gradients were computed efficiently using an analogous dynamic programming algorithm. 5 Experiments Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). The dataset consists of 880 natural language sentences where each sentence is coupled with a formal tree-structured semantic representation. The early version of this dataset was annotated with English only (Wong and Mooney, 2006; Kate and Mooney, 2006), and Jones et al. (2012) released a version that is annotated with three additional languages: German, Greek and Thai. To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; Lu, 2014) for evaluation. We al</context>
<context position="15130" citStr="Wong and Mooney, 2006" startWordPosition="2450" endWordPosition="2453">ng the correctness of an output semantic representation from our system. Specifically, we used a standard script to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database. Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; Lu, 2014). The results of our system as well as those of several previous systems are given in Table 2. We compared our system’s performance against those of several previous works. The WASP system (Wong and Mooney, 2006) is based on statistical machine translation technique while the HYBRIDTREE+ system (Lu et al., 2008) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. RHT (Lu, 2014) is the discriminative semantic parsing system based on relaxed hybrid trees. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi740 System English Thai German Greek</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proc. of HLT/NAACL ’06, pages 439–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proc. of ACL ’07.</booktitle>
<contexts>
<context position="951" citStr="Wong and Mooney, 2007" startWordPosition="132" endWordPosition="135"> with our previous stateof-the-art discriminative semantic parsing model – the relaxed hybrid tree model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. 1 Introduction This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). One state-of-the-art model for semantic parsing is our recently introduced relaxed hybrid tree model (Lu, 2014), which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. The model allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. It relies on representations called relaxed hybrid trees that can jointly represent both the sentences and semantics. The model is essentially di</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proc. of ACL ’07.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>