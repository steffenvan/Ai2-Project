<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056518">
<title confidence="0.991386">
Adapting a WSJ-Trained Parser to Grammatically Noisy Text
</title>
<author confidence="0.998914">
Jennifer Foster, Joachim Wagner and Josef van Genabith
</author>
<affiliation confidence="0.882270333333333">
National Centre for Language Technology
Dublin City University
Ireland
</affiliation>
<email confidence="0.991796">
jfoster, jwagner, josef@computing.dcu.ie
</email>
<sectionHeader confidence="0.993822" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999780285714286">
We present a robust parser which is trained on
a treebank of ungrammatical sentences. The
treebank is created automatically by modify-
ing Penn treebank sentences so that they con-
tain one or more syntactic errors. We eval-
uate an existing Penn-treebank-trained parser
on the ungrammatical treebank to see how it
reacts to noise in the form of grammatical er-
rors. We re-train this parser on the training
section of the ungrammatical treebank, lead-
ing to an significantly improved performance
on the ungrammatical test sets. We show how
a classifier can be used to prevent performance
degradation on the original grammatical data.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983894736842">
The focus in English parsing research in recent years
has moved from Wall Street Journal parsing to im-
proving performance on other domains. Our re-
search aim is to improve parsing performance on
text which is mildly ungrammatical, i.e. text which
is well-formed enough to be understood by people
yet which contains the kind of grammatical errors
that are routinely produced by both native and non-
native speakers of a language. The intention is not
to detect and correct the error, but rather to ignore
it. Our approach is to introduce grammatical noise
into WSJ sentences while retaining as much of the
structure of the original trees as possible. These
sentences and their associated trees are then used
as training material for a statistical parser. It is im-
portant that parsing on grammatical sentences is not
harmed and we introduce a parse-probability-based
classifier which allows both grammatical and un-
grammatical sentences to be accurately parsed.
</bodyText>
<sectionHeader confidence="0.985166" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999976162162162">
Various strategies exist to build robustness into the
parsing process: grammar constraints can be relaxed
(Fouvry, 2003), partial parses can be concatenated to
form a full parse (Penstein Ros´e and Lavie, 1997),
the input sentence can itself be transformed until a
parse can be found (Lee et al., 1995), and mal-rules
describing particular error patterns can be included
in the grammar (Schneider and McCoy, 1998). For a
parser which tends to fail when faced with ungram-
matical input, such techniques are needed. The over-
generation associated with a statistical data-driven
parser means that it does not typically fail on un-
grammatical sentences. However, it is not enough
to return some analysis for an ungrammatical sen-
tence. If the syntactic analysis is to guide semantic
analysis, it must reflect as closely as possible what
the person who produced the sentence was trying to
express. Thus, while statistical, data-driven parsing
has solved the robustness problem, it is not clear that
it is has solved the accurate robustness problem.
The problem of adapting parsers to accurately
handle ungrammatical text is an instance of the do-
main adaptation problem where the target domain is
grammatically noisy data. A parser can be adapted
to a target domain by training it on data from the new
domain – the problem is to quickly produce high-
quality training material. Our solution is to simply
modify the existing training material so that it re-
sembles material from the noisy target domain.
In order to tune a parser to syntactically ill-formed
text, a treebank is automatically transformed into an
ungrammatical treebank. This transformation pro-
cess has two parts: 1. the yield of each tree is trans-
formed into an ungrammatical sentence by introduc-
ing a syntax error; 2. each tree is minimally trans-
formed, but left intact as much as possible to reflect
the syntactic structure of the original “intended” sen-
</bodyText>
<page confidence="0.98216">
221
</page>
<reference confidence="0.221616">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221–224,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.9999085">
tence prior to error insertion. Artificial ungrammati-
calities have been used in various NLP tasks (Smith
and Eisner, 2005; Okanohara and Tsujii, 2007)
The idea of an automatically generated ungram-
matical treebank was proposed by Foster (2007).
Foster generates an ungrammatical version of the
WSJ treebank and uses this to train two statistical
parsers. The performance of both parsers signifi-
cantly improves on the artificially created ungram-
matical test data, but significantly degrades on the
original grammatical test data. We show that it
is possible to obtain significantly improved perfor-
mance on ungrammatical data without a concomi-
tant performance decline on grammatical data.
</bodyText>
<sectionHeader confidence="0.996402" genericHeader="method">
3 Generating Noisy Treebanks
</sectionHeader>
<bodyText confidence="0.999103688888889">
Generating Noisy Sentences We apply the error
introduction procedure described in detail in Foster
(2007). Errors are introduced into sentences by ap-
plying the operations of word substitution, deletion
and insertion. These operations can be iteratively
applied to generate increasingly noisy sentences.
We restrict our attention to ungrammatical sentences
with a edit-distance of one or two words from the
original sentence, because it is reasonable to expect
a parser’s performance to degrade as the input be-
comes more ill-formed. The operations of substitu-
tion, deletion and insertion are not carried out en-
tirely at random, but are subject to some constraints
derived from an empirical study of ill-formed En-
glish sentences (Foster, 2005). Three types of word
substitution errors are produced: agreement errors,
real word spelling errors and verb form errors. Any
word that is not an adjective or adverb can be deleted
from any position within the input sentence, but
some part-of-speech tags are favoured over others,
e.g. it is more likely that a determiner will be deleted
than a noun. The error creation procedure can insert
an arbitrary word at any position within a sentence
but it has a bias towards inserting a word directly af-
ter the same word or directly after a word with the
same part of speech. The empirical study also in-
fluences the frequency at which particular errors are
introduced, with missing word errors being the most
frequent, followed by extra word errors, real word
spelling errors, agreement errors, and finally, verb
form errors. Table 1 shows examples of the kind of
ill-formed sentences that are produced when we ap-
ply the procedure to Wall Street Journal sentences.
Generating Trees for Noisy Sentences The tree
structures associated with the modified sentences are
also modified, but crucially, this modification is min-
imal, since a truly robust parser should return an
analysis for a mildly ungrammatical sentence that
remains as similar as possible to the analysis it re-
turns for the original grammatical sentence.
Assume that (1) is an original treebank tree for the
sentence A storm is brewing. Example (2) is then the
tree for the ungrammatical sentence containing an
is/it confusion. No part of the original tree structure
is changed apart from the yield.
</bodyText>
<listItem confidence="0.946679625">
(1) (S (NPA storm) (VP (VBZ is) (VP (VBG brewing))))
(2) (S (NPA storm) (VP (VBZ it) (VP (VBG brewing))))
An example of a missing word error is shown in
(3) and (4). A pre-terminal dominating an empty
node is introduced into the tree at the point where
the word has been omitted.
(3) (S (NPAnnotators) (VP (VBPparse) (NP the sentences)))
(4) (S (NPAnnotators) (VP (-NONE- 0) (NP the sentences)))
</listItem>
<bodyText confidence="0.971958285714286">
An example of an extra word error is shown in (5),
(6) and (7). For this example, two ungrammatical
trees, (6) and (7), are generated because there are
two possible positions in the original tree where the
extra word can be inserted which will result in a tree
with the yield He likes of the cake and which will not
result in the creation of any additional structure.
</bodyText>
<listItem confidence="0.9958852">
(5) (S (NP He) (VP (VBZ likes) (NP (DT the) (NN cake))))
(6) (S (NP He) (VP (VBZ likes) (IN of) (NP (DT the) (NN
cake))))
(7) (S (NP He) (VP (VBZ likes) (NP (IN of) (DT the) (NN
cake))))
</listItem>
<sectionHeader confidence="0.914627" genericHeader="method">
4 Parser Adaptation Experiments
</sectionHeader>
<bodyText confidence="0.999927555555555">
In order to obtain training data for our parsing ex-
periments, we introduce syntactic noise into the
usual WSJ training material, Sections 2-21, using
the procedures outlined in Section 3, i.e. for every
sentence-tree pair in WSJ2-21, we introduce an er-
ror into the sentence and then transform the tree so
that it covers the newly created ungrammatical sen-
tence. For 4 of the 20 sections in WSJ2-21, we apply
the noise introduction procedure to its own output to
</bodyText>
<page confidence="0.979918">
222
</page>
<table confidence="0.9998365">
Error Type WSJ00
Missing Word likely to bring new attention to the problem --+ likely to new attention to the problem
Extra Word the $ 5.9 million itposted --+ the $ 5.9 million I itposted
Real Word Spell Mr Vinken is chairman ofElsevier Mr. Vinken if chairman ofElsevier
Agreement this event tookplace 35 years ago these event tookplace 35 years ago
Verb Form But the Soviets might still face legal obstacles --+ But the Soviets might still faces legal obstacles
</table>
<tableCaption confidence="0.999929">
Table 1: Automatically Generated Ungrammatical Sentences
</tableCaption>
<bodyText confidence="0.999586807692308">
create even noisier data. Our first development set is
a noisy version of WSJ00, Noisy00, produced by ap-
plying the noise introduction procedure to the 1,921
sentences in WSJ00. Our second development set is
an even noisier version of WSJ00, Noisiest00, which
is created by applying our noise introduction proce-
dure to the output of Noisy00. We apply the same
process to WSJ23 to obtain our two test sets.
For all our parsing experiments, we use the June
2006 version of the two-stage parser reported in
Charniak and Johnson (2005). Evaluation is carried
out using Parseval labelled precision/recall. For ex-
tra word errors, there may be more than one gold
standard tree (see (6) and (7)). When this happens
the parser output tree is evaluated against all gold
standard trees and the maximum f-score is chosen.
We carry out five experiments. In the first ex-
periment, E0, we apply the parser, trained on well-
formed data, to noisy input. The purpose of E0 is to
ascertain how well a parser trained on grammatical
sentences, can ignore grammatical noise. E0 pro-
vides a baseline against which the subsequent ex-
perimental results can be judged. In the E1 experi-
ments, the parser is retrained using the ungrammati-
cal version of WSJ2-21. In experiment E1error, the
parser is trained on ungrammatical material only,
i.e. the noisy version of WSJ2-21. In experiment
E1mixed, the parser is trained on grammatical and
ungrammatical material, i.e. the original WSJ2-21 is
merged with the noisy WSJ2-21. In the E2 experi-
ments, a classifier is applied to the input sentence.
If the sentence is classified as ungrammatical, a ver-
sion of the parser that has been trained on ungram-
matical data is employed. In the E2ngram experi-
ment, we train a J48 decision tree classifier. Follow-
ing Wagner et al. (2007), the decision tree features
are part-of-speech n-gram frequency counts, with n
ranging from 2 to 7 and with a subset of the BNC
as the frequency reference corpus. The decision tree
is trained on the original WSJ2-21 and the ungram-
matical WSJ2-21. In the E2prob experiment, the in-
put sentence is parsed with two parsers, the origi-
nal parser (the E0 parser) and the parser trained on
ungrammatical material (either the E1error or the
E1mixed parser). A very simple classifier is used
to decide which parser output to choose: if the E1
parser returns a higher parse probability for the most
likely tree than the E0 parser, the E1 parser output is
returned. Otherwise the E0 parser output is returned.
The baseline E0 results are in the first column of
Table 2. As expected, the performance of a parser
trained on well-formed input degrades when faced
with ungrammatical input. It is also not surprising
that its performance is worse on Noisiest00 (-8.8%
f-score) than it is on Noisy00 (-4.3%) since the Nois-
iest00 sentences contain two errors rather than one.
The E1 results occupy the second and third
columns of Table 2. An up arrow indicates a sta-
tistically significant improvement over the baseline
results, a down arrow a statistically significant de-
cline and a dash a change which is not statistically
significant (p &lt; 0.01). Training the parser on un-
grammatical data has a positive effect on its perfor-
mance on Noisy00 and Noisiest00 but has a negative
effect on its performance on WSJ00. Training on a
combination of grammatical and ungrammatical ma-
terial gives the best results for all three development
sets. Therefore, for the E2 experiments we use the
E1mixed parser rather than the E1error parser.
The E2 results are shown in the last two columns
of Table 2 and the accuracy of the two classifiers in
Table 3. Over the three test sets, the E2prob classi-
fier outperforms the E2ngram classifier. Both classi-
fiers misclassify approximately 45% of the Noisy00
sentences. However, the sentences misclassified by
the E2prob classifier are those that are handled well
by the E0 parser, and this is reflected in the pars-
ing results for Noisy00. An important feature of the
</bodyText>
<page confidence="0.996933">
223
</page>
<table confidence="0.9993968">
Dev Set P R F P R F P R F P R F P R F
E0 E1-error E1-mixed E2prob E2ngram
WSJ00 91.5 90.3 90.9 91.0− 89.4 j 90.2 91.3− 89.8 j 90.5 91.5− 90.2− 90.9 91.3− 89.9j 90.6
Noisy00 87.5 85.6 86.6 89.4 T 86.6 T 88.0 89.4 T 86.8 T 88.1 89.1 T 86.8 T 87.9 88.7T 86.2T 87.5
Noisiest00 83.5 80.8 82.1 87.6 T 83.6 T 85.6 87.6 T 83.8 T 85.7 87.2 T 83.7 T 85.4 86.6T 83.0T 84.8
</table>
<tableCaption confidence="0.982726">
Table 2: Results of Parsing Experiments
</tableCaption>
<table confidence="0.999855">
Development Set E2prob E2ngram
WSJ00 76.7% 63.3%
Noisy00 55.1% 55.6%
Noisiest00 70.2% 66.0%
</table>
<tableCaption confidence="0.961922">
Table 3: E2 Classifier Accuracy
</tableCaption>
<table confidence="0.9997518">
Test Set P R F P R F
E0 E2prob
WSJ23 91.7 90.8 91.3 91.7− 90.7− 91.2
Noisy23 87.4 85.6 86.5 89.2 T 87.0 T 88.1
Noisiest23 83.2 80.8 82.0 87.4 T 84.1 T 85.7
</table>
<tableCaption confidence="0.99947">
Table 4: Final Results for Section 23 Test Sets
</tableCaption>
<bodyText confidence="0.999507875">
E2prob classifier is that its use results in a constant
performance on the grammatical data - with no sig-
nificant degradation from the baseline.
Taking the E2prob results as our optimum, we
carry out the same experiment again on our WSJ23
test sets. The results are shown in Table 4. The same
effect can be seen for the test sets as for the devel-
opment sets - a significantly improved performance
on the ungrammatical data without an accompany-
ing performance decrease for the grammatical data.
The Noisy23 breakdown by error type is shown in
Table 5. The error type which the original parser is
most able to ignore is an agreement error. For this er-
ror type alone, the ungrammatical training material
seems to hinder the parser. The biggest improve-
ment occurs for real word spelling errors.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99895">
We have shown that it is possible to tune a WSJ-
trained statistical parser to ungrammatical text with-
</bodyText>
<table confidence="0.998156">
Error Type P R F P R F
E0 E2-prob
Missing Word 88.5 83.7 86.0 88.9 84.3 86.5
Extra Word 87.2 89.4 88.3 89.2 89.7 89.4
Real Word Spell 84.3 83.0 83.7 89.5 88.2 88.9
Agreement 90.4 88.8 89.6 90.3 88.6 89.4
Verb Form 88.6 87.0 87.8 89.1 87.9 88.5
</table>
<tableCaption confidence="0.999779">
Table 5: Noisy23: Breakdown by Error Type
</tableCaption>
<bodyText confidence="0.9979923">
out affecting its performance on grammatical text.
This has been achieved using an automatically gen-
erated ungrammatical version of the WSJ treebank
and a simple binary classifier which compares parse
probabilities. The next step in this research is to see
how the method copes on ‘real’ errors - this will re-
quire manual parsing of a suitably large test set.
Acknowledgments We thank the IRCSET Em-
bark Initiative (postdoctoral fellowship P/04/232)
for supporting this research.
</bodyText>
<sectionHeader confidence="0.999426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999597875">
Eugene Charniak and Mark Johnson. 2005. Course-to-fine n-
best-parsing and maxent discriminative reranking. In Pro-
ceedings ofACL-2005.
Jennifer Foster. 2005. Good Reasons for Noting Bad Gram-
mar: Empirical Investigations into the Parsing of Ungram-
matical Written English. Ph.D. thesis, University of Dublin,
Trinity College.
Jennifer Foster. 2007. Treebanks gone bad: Parser evaluation
and retraining using a treebank of ungrammatical sentences.
IJDAR, 10(3-4), December.
Frederik Fouvry. 2003. Robust Processing for Constraint-
based Grammar Formalisms. Ph.D. thesis, University of Es-
sex.
Kong Joo Lee, Cheol Jung Kweon, Jungyun Seo, and Gil Chang
Kim. 1995. A robust parser based on syntactic information.
In Proceedings ofEACL-1995.
Daisuke Okanohara and Jun’ichi Tsujii. 2007. A discrimi-
native language model with pseudo-negative examples. In
Proceedings ofACL-2007.
Carolyn Penstein Ros´e and Alon Lavie. 1997. An efficient dis-
tribution of labor in a two stage robust interpretation process.
In Proceedings ofEMNLP-1997.
David Schneider and Kathleen McCoy. 1998. Recognizing
syntactic errors in the writing of second language learners.
In Proceedings ofACL/COLING-1998.
Noah A. Smith and Jason Eisner. 2005. Contrastive Estima-
tion: Training Log-Linear Models on Unlabeled Data. In
Proceedings ofACL-2005.
Joachim Wagner, Jennifer Foster, and Josef van Genabith.
2007. A comparative evaluation of deep and shallow ap-
proaches to the automatic detection of common grammatical
errors. In Proceedings of EMNLP-CoNLL-2007.
</reference>
<page confidence="0.998703">
224
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897688">
<title confidence="0.999491">Adapting a WSJ-Trained Parser to Grammatically Noisy Text</title>
<author confidence="0.999793">Jennifer Foster</author>
<author confidence="0.999793">Joachim Wagner</author>
<author confidence="0.999793">Josef van_Genabith</author>
<affiliation confidence="0.9985325">National Centre for Language Technology Dublin City University</affiliation>
<address confidence="0.917686">Ireland</address>
<email confidence="0.995102">jfoster,jwagner,josef@computing.dcu.ie</email>
<abstract confidence="0.999069333333333">We present a robust parser which is trained on a treebank of ungrammatical sentences. The treebank is created automatically by modifying Penn treebank sentences so that they contain one or more syntactic errors. We evaluate an existing Penn-treebank-trained parser on the ungrammatical treebank to see how it reacts to noise in the form of grammatical errors. We re-train this parser on the training section of the ungrammatical treebank, leading to an significantly improved performance on the ungrammatical test sets. We show how a classifier can be used to prevent performance degradation on the original grammatical data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>221--224</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221–224,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Course-to-fine nbest-parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-2005.</booktitle>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Course-to-fine nbest-parsing and maxent discriminative reranking. In Proceedings ofACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>Good Reasons for Noting Bad Grammar: Empirical Investigations into the Parsing of Ungrammatical Written English.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Dublin, Trinity College.</institution>
<marker>Foster, 2005</marker>
<rawString>Jennifer Foster. 2005. Good Reasons for Noting Bad Grammar: Empirical Investigations into the Parsing of Ungrammatical Written English. Ph.D. thesis, University of Dublin, Trinity College.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences.</title>
<date>2007</date>
<journal>IJDAR,</journal>
<pages>10--3</pages>
<marker>Foster, 2007</marker>
<rawString>Jennifer Foster. 2007. Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences. IJDAR, 10(3-4), December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederik Fouvry</author>
</authors>
<title>Robust Processing for Constraintbased Grammar Formalisms.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Essex.</institution>
<contexts>
<context position="1964" citStr="Fouvry, 2003" startWordPosition="304" endWordPosition="305">or, but rather to ignore it. Our approach is to introduce grammatical noise into WSJ sentences while retaining as much of the structure of the original trees as possible. These sentences and their associated trees are then used as training material for a statistical parser. It is important that parsing on grammatical sentences is not harmed and we introduce a parse-probability-based classifier which allows both grammatical and ungrammatical sentences to be accurately parsed. 2 Background Various strategies exist to build robustness into the parsing process: grammar constraints can be relaxed (Fouvry, 2003), partial parses can be concatenated to form a full parse (Penstein Ros´e and Lavie, 1997), the input sentence can itself be transformed until a parse can be found (Lee et al., 1995), and mal-rules describing particular error patterns can be included in the grammar (Schneider and McCoy, 1998). For a parser which tends to fail when faced with ungrammatical input, such techniques are needed. The overgeneration associated with a statistical data-driven parser means that it does not typically fail on ungrammatical sentences. However, it is not enough to return some analysis for an ungrammatical se</context>
</contexts>
<marker>Fouvry, 2003</marker>
<rawString>Frederik Fouvry. 2003. Robust Processing for Constraintbased Grammar Formalisms. Ph.D. thesis, University of Essex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kong Joo Lee</author>
<author>Cheol Jung Kweon</author>
<author>Jungyun Seo</author>
<author>Gil Chang Kim</author>
</authors>
<title>A robust parser based on syntactic information.</title>
<date>1995</date>
<booktitle>In Proceedings ofEACL-1995.</booktitle>
<contexts>
<context position="2146" citStr="Lee et al., 1995" startWordPosition="334" endWordPosition="337">entences and their associated trees are then used as training material for a statistical parser. It is important that parsing on grammatical sentences is not harmed and we introduce a parse-probability-based classifier which allows both grammatical and ungrammatical sentences to be accurately parsed. 2 Background Various strategies exist to build robustness into the parsing process: grammar constraints can be relaxed (Fouvry, 2003), partial parses can be concatenated to form a full parse (Penstein Ros´e and Lavie, 1997), the input sentence can itself be transformed until a parse can be found (Lee et al., 1995), and mal-rules describing particular error patterns can be included in the grammar (Schneider and McCoy, 1998). For a parser which tends to fail when faced with ungrammatical input, such techniques are needed. The overgeneration associated with a statistical data-driven parser means that it does not typically fail on ungrammatical sentences. However, it is not enough to return some analysis for an ungrammatical sentence. If the syntactic analysis is to guide semantic analysis, it must reflect as closely as possible what the person who produced the sentence was trying to express. Thus, while s</context>
</contexts>
<marker>Lee, Kweon, Seo, Kim, 1995</marker>
<rawString>Kong Joo Lee, Cheol Jung Kweon, Jungyun Seo, and Gil Chang Kim. 1995. A robust parser based on syntactic information. In Proceedings ofEACL-1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative language model with pseudo-negative examples.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL-2007.</booktitle>
<marker>Okanohara, Tsujii, 2007</marker>
<rawString>Daisuke Okanohara and Jun’ichi Tsujii. 2007. A discriminative language model with pseudo-negative examples. In Proceedings ofACL-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Penstein Ros´e</author>
<author>Alon Lavie</author>
</authors>
<title>An efficient distribution of labor in a two stage robust interpretation process.</title>
<date>1997</date>
<booktitle>In Proceedings ofEMNLP-1997.</booktitle>
<marker>Ros´e, Lavie, 1997</marker>
<rawString>Carolyn Penstein Ros´e and Alon Lavie. 1997. An efficient distribution of labor in a two stage robust interpretation process. In Proceedings ofEMNLP-1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schneider</author>
<author>Kathleen McCoy</author>
</authors>
<title>Recognizing syntactic errors in the writing of second language learners.</title>
<date>1998</date>
<booktitle>In Proceedings ofACL/COLING-1998.</booktitle>
<contexts>
<context position="2257" citStr="Schneider and McCoy, 1998" startWordPosition="350" endWordPosition="353"> important that parsing on grammatical sentences is not harmed and we introduce a parse-probability-based classifier which allows both grammatical and ungrammatical sentences to be accurately parsed. 2 Background Various strategies exist to build robustness into the parsing process: grammar constraints can be relaxed (Fouvry, 2003), partial parses can be concatenated to form a full parse (Penstein Ros´e and Lavie, 1997), the input sentence can itself be transformed until a parse can be found (Lee et al., 1995), and mal-rules describing particular error patterns can be included in the grammar (Schneider and McCoy, 1998). For a parser which tends to fail when faced with ungrammatical input, such techniques are needed. The overgeneration associated with a statistical data-driven parser means that it does not typically fail on ungrammatical sentences. However, it is not enough to return some analysis for an ungrammatical sentence. If the syntactic analysis is to guide semantic analysis, it must reflect as closely as possible what the person who produced the sentence was trying to express. Thus, while statistical, data-driven parsing has solved the robustness problem, it is not clear that it is has solved the ac</context>
</contexts>
<marker>Schneider, McCoy, 1998</marker>
<rawString>David Schneider and Kathleen McCoy. 1998. Recognizing syntactic errors in the writing of second language learners. In Proceedings ofACL/COLING-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive Estimation: Training Log-Linear Models on Unlabeled Data.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-2005.</booktitle>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive Estimation: Training Log-Linear Models on Unlabeled Data. In Proceedings ofACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>A comparative evaluation of deep and shallow approaches to the automatic detection of common grammatical errors.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL-2007.</booktitle>
<marker>Wagner, Foster, van Genabith, 2007</marker>
<rawString>Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2007. A comparative evaluation of deep and shallow approaches to the automatic detection of common grammatical errors. In Proceedings of EMNLP-CoNLL-2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>