<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026889">
<title confidence="0.951513">
Language Models Based on Semantic Composition
</title>
<author confidence="0.997038">
Jeff Mitchell and Mirella Lapata
</author>
<affiliation confidence="0.999716">
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.960747">
Edinburgh EH8 9LW, UK
</address>
<email confidence="0.990088">
jeff.mitchell@ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992230625">
In this paper we propose a novel statistical
language model to capture long-range se-
mantic dependencies. Specifically, we ap-
ply the concept of semantic composition to
the problem of constructing predictive his-
tory representations for upcoming words.
We also examine the influence of the un-
derlying semantic space on the composi-
tion task by comparing spatial semantic
representations against topic-based ones.
The composition models yield reductions
in perplexity when combined with a stan-
dard n-gram language model over the
n-gram model alone. We also obtain per-
plexity reductions when integrating our
models with a structured language model.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999761015873016">
Statistical language modeling plays an important
role in many areas of natural language process-
ing including speech recognition, machine trans-
lation, and information retrieval. The prototypi-
cal use of language models is to assign proba-
bilities to sequences of words. By invoking the
chain rule, these probabilities are generally es-
timated as the product of conditional probabili-
ties P(wi|hi) of a word wi given the history of
preceding words hi ≡ wi−1
1 . In theory, the history
could span any number of words up to wi such as
sentences or even a paragraphs. In practice, how-
ever, it has proven challenging to deal with the
combinatorial growth in the number of possible
histories which in turn impacts reliable parame-
ter estimation. A simple and effective strategy is
to truncate the chain rule to include only the n-1
preceding words (n is often set within the range
of 3–5). The simplification reduces the number of
free parameters. However, low values of n impose
an artificially local horizon to the language model,
and compromise its ability to capture long-range
dependencies, such as syntactic relationships, se-
mantic or thematic constraints.
The literature offers many examples of how to
overcome this limitation, essentially by allowing
the modulation of probabilities by dependencies
which extend to words beyond the n-gram horizon.
Cache language models (Kuhn and de Mori, 1992)
increase the probability of words observed in the
history, e.g., by some factor which decays expo-
nentially with distance. Trigger models (Rosen-
feld, 1996) go a step further by allowing arbi-
trary word pairs to be incorporated into the cache.
Structured language models (e.g., Roark (2001))
go beyond the representation of history as a lin-
ear sequence of words to capture the syntactic con-
structions in which these words are embedded.
It is also possible to build representations of
history which are semantic rather than syntactic
(Bellegarda (2000; Coccaro and Jurafsky (1998;
Gildea and Hofmann (1999)). In this approach, es-
timates for the probabilities of upcoming words
are derived from a comparison of their semantic
content with the content of the history so far. The
semantic representations, in this case, are vectors
derived from the distributional properties of words
in a corpus, based on the insight that words which
are semantically similar will be found in similar
contexts (Harris, 1968; Firth, 1957). Although the
the construction of a semantic representation for
the history is crucial to this approach, the under-
lying vector-based models are primarily designed
to represent isolated words rather than word se-
quences. Ideally, we would like to compose the
meaning of the history out of its constituent parts.
This is by no means a new idea. Much work in lin-
guistic theory (Partee, 1995; Montague, 1974) has
been devoted to compositionality, the process of
determining the meaning of complex expressions
from simpler ones. Previous work either ignores
this issue (e.g., Bellegarda (2000)) or simply com-
</bodyText>
<page confidence="0.974246">
430
</page>
<note confidence="0.996609">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430–439,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999727454545455">
putes the centroid of the vectors representing the
history (e.g., Coccaro and Jurafsky (1998)). This is
motivated primarily by mathematical convenience
rather than by empirical evidence.
In our earlier work (Mitchell and Lapata, 2008)
we formulated composition as a function of two
vectors and introduced a variety of models based
on addition and multiplication. In this paper we
apply vector composition to the problem of con-
structing predictive history representations for lan-
guage modeling. Besides integrating composition
with language modeling, a task which is novel to
our knowledge, our approach also serves as a valu-
able testbed of our earlier framework which we
originally evaluated on a small scale verb-subject
similarity task. We also investigate how the choice
of the underlying semantic representation inter-
acts with the choice of composition function by
comparing a spatial model that represents words
as vectors in a high-dimensional space against a
probabilistic model that represents words as topic
distributions.
Our results show that the proposed composi-
tion models yield reductions in perplexity when
combined with a standard n-gram model over
the n-gram model alone. We also show that with
an appropriate composition function spatial mod-
els outperform the more sophisticated topic mod-
els. Finally, we obtain further perplexity reduc-
tions when our models are integrated with a struc-
tured language model, indicating that the two ap-
proaches to language modeling are complemen-
tary.
</bodyText>
<sectionHeader confidence="0.993362" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.992176">
2.1 Distributional Models of Semantics
</subsectionHeader>
<bodyText confidence="0.999927287878788">
The insight that words with similar meanings will
tend to be distributed in similar contexts has given
rise to a number of approaches that construct
semantic representations from corpora. Broadly
speaking, these models come in two flavors. Se-
mantic space models represent the meaning of
words in terms of vectors, with the vector compo-
nents being derived from the distributional statis-
tics of those words. Essentially, these models pro-
vide a simple procedure for constructing spatial
representations of word meaning. Topic models, in
contrast, impose a probabilistic model onto those
distributional statistics, under the assumption that
hidden topic variables drive the process that gener-
ates words. Both approaches represent the mean-
ings of words in terms of an n-dimensional series
of values, but whereas the semantic space model
treats those values as defining a vector with spatial
properties, the topic model treats them as a proba-
bility distribution.
A simple and popular (McDonald, 2000; Bul-
linaria and Levy, 2007; Lowe, 2000) way to con-
struct a semantic space model is to associate each
vector component with a particular context word,
and assign it a value based on the strength of
its co-occurrence with the target (i.e., the word
for which a semantic representation is being con-
structed). For example, in Mitchell and Lapata
(2008) we used the 2,000 most frequent content
words in a corpus as their contexts, and defined
co-occurrence in terms of the context word be-
ing present in a five word window on either side
of the target word. We calculated the ratio of the
probability of the context word given the target
word to the overall probability of the context word
and use these values as their vector components.
This procedure has the benefits of simplicity and
also of being largely free of any additional the-
oretical assumptions over and above the distribu-
tional approach to semantics. This is not to say that
more sophisticated approaches have not been de-
veloped or that they are not useful. Much work has
been devoted to enriching semantic space mod-
els with syntactic information (e.g., Grefenstette
(1994; Pad´o and Lapata (2007)), selectional pref-
erences (Erk and Pad´o, 2008) or with identifying
optimal ways of defining the vector components
(e.g., Bullinaria and Levy (2007)).
The semantic space discussed thus far is based
on word co-occurrence statistics. However, the
statistics of how words are distributed across the
documents also carry useful semantic informa-
tion. Latent Semantic Analysis (LSA, Landauer
and Dumais (1997) utilizes precisely this distribu-
tional information to uncover hidden semantic fac-
tors by means of dimensionality reduction. Singu-
lar value decomposition (SVD, Berry et al. (1994))
is applied to a word-document co-occurrence ma-
trix which is factored into a product of a number
of other matrices; one of them represents words in
terms of the semantic factors and another repre-
sents documents in terms of the same factors. The
algebraic relation between these matrices can be
used to show that any document vector is a linear
combination of the vectors representing the words
it contains. Thus, within this paradigm it is nat-
</bodyText>
<page confidence="0.998277">
431
</page>
<bodyText confidence="0.999322724137931">
ural to treat multi-word structures as a “pseudo-
document” and represent them via linear combi-
nations of word vectors.
Due to its generality, LSA has proven a valuable
analysis tool with a wide range of applications.
However, the SVD procedure is somewhat ad-hoc
lacking a sound statistical foundation. Probabilis-
tic Latent Semantic Analysis (pLSA, Hofmann
(2001)) casts the relationship between documents
and words in terms of a generative model based on
a set of hidden topics. Documents are represented
by distributions over topics and topics are distri-
butions over words. Thus the mixture of topics
in any document determines its vocabulary. Maxi-
mum likelihood estimation of these distributions
over a word-document matrix has a comparable
effect to SVD in LSA: a set of hidden semantic
factors, in this case topics, are extracted and docu-
ments and words are represented by these topics.
Latent Dirichlet Allocation (Griffiths et al.,
2007; Blei et al., 2003) enhances further the math-
ematical foundation of this approach. Whereas
pLSA treats each document as a separate, inde-
pendent mixture of topics, LDA assumes that the
topic distributions of documents are generated by
a Dirichlet distribution. Thus, LDA is a probabilis-
tic model of the whole document collection. In this
model the process of generating a document can
be described as follows:
</bodyText>
<listItem confidence="0.9820285">
1. draw a multinomial distribution θ from a
Dirichlet distribution parametrized by α
2. for each word in a document:
(a) draw a topic zk from the multinomial
distribution characterized by θ
(b) draw a word from a multinomial distri-
bution conditioned on the topic zk and
word probabilities β
</listItem>
<bodyText confidence="0.999459125">
Under this model, constructing a representation
for a multi-word sequence amounts to estimating
the topic proportions for that sequence.1 Struc-
ture here arises from the mathematical form of the
model, as opposed to any linguistic assumptions.
Without anticipating our results too much, we
should point out that several features of the LDA
model are likely to affect the representation of
</bodyText>
<footnote confidence="0.9037224">
1Estimating the posterior distribution P(θ,z|w,α,β) of
the hidden variables given an observed collection of docu-
ments w is intractable in general; however, a variety of ap-
proximate inference algorithms have been proposed in the
literature (e.g., Blei et al. (2003; Griffiths et al. (2007)).
</footnote>
<bodyText confidence="0.9996405">
multi-word sequences. Firstly, it is a top-down
generative model (the topic proportions for a doc-
ument are first selected and then this drives the
generation of words) as opposed to a bottom-up
constructive process (words modulate each other
to produce a complex representation of their com-
bination). Secondly, the top level Dirichlet distri-
bution is likely to lead to documents being dom-
inated by a small number of topics, producing
sparse vectors. And lastly, the assumption that
words are generated independently means the in-
teraction between them is not modeled.
</bodyText>
<subsectionHeader confidence="0.9847865">
2.2 Language Modeling using Semantic
Representations
</subsectionHeader>
<bodyText confidence="0.999699305555555">
A common approach to embedding semantic rep-
resentations within language modeling is to mea-
sure the semantic similarity between an upcoming
word and its history and use it to modify the prob-
abilities from an n-gram model. In this way, the
n-gram’s sensitivity to short-range dependencies
is enriched with information about longer-range
semantic coherence. Much of previous work has
taken this approach (Bellegarda, 2000; Coccaro
and Jurafsky, 1998; Wandmacher and Antoine,
2007), whilst relying on LSA to provide seman-
tic representations for individual words. Some au-
thors (Coccaro and Jurafsky, 1998; Wandmacher
and Antoine, 2007) use the geometric notion of
a vector centroid to construct representations of
history, whereas others (Bellegarda, 2000; Deng
and Khundanpur, 2003) use the idea of a “pseudo-
document”, which is derived from the algebraic
relation between documents and words assumed
within LSA. They all derive P(wi|hi), the probabil-
ity of an upcoming word given its history, from the
cosine similarity measure which must be somehow
normalized in order to yield well-formed probabil-
ity estimates.
The approach of Gildea and Hofmann (1999)
overcomes this difficulty by using representations
constructed with pLSA, which have a direct prob-
abilistic interpretation. As a result, the probabil-
ity of an upcoming word given the history can be
derived naturally and directly, avoiding the need
for ad-hoc transformations. In constructing their
representation of history, Gildea and Hofmann
(1999) use an online Expectation Maximization
process, which derives from the probabilistic basis
of pLSA, to update the history with new words.
Extensions on the basic semantic language
</bodyText>
<page confidence="0.993452">
432
</page>
<bodyText confidence="0.999959">
models sketched above involve representing the
history by multiple LSA models of varying granu-
larity in an attempt to capture topic, subtopic, and
local information (Zhang and Rudnicky, 2002); in-
corporating syntactic information by building the
semantic space over words and their syntactic an-
notations (Kanejiya et al., 2004); and treating the
LSA similarity as a feature in a maximum entropy
language model (Deng and Khundanpur, 2003).
</bodyText>
<sectionHeader confidence="0.990834" genericHeader="method">
3 Composition Models
</sectionHeader>
<bodyText confidence="0.9999925">
The problem of vector composition has re-
ceived relatively little attention within natural lan-
guage processing. Attempts to use tensor products
(Smolensky, 1990; Clark et al., 2008; Widdows,
2008) as a means of binding one vector to another
face major computational difficulties as their di-
mensionality grows exponentially with the num-
ber of constituents being composed. To overcome
this problem, other techniques (Plate, 1995) have
been proposed in which the binding of two vectors
results in a vector which has the same dimension-
ality as its components. Crucially, the success of
these methods depends on the assumption that the
vector components are randomly distributed. This
is problematic for modeling language which has
regular structure.
Given the above considerations, in Mitchell and
Lapata (2008) we introduce a general framework
for studying vector composition, which we formu-
late as a function f of two vectors u and v:
</bodyText>
<equation confidence="0.930502">
h = f (u,v) (1)
</equation>
<bodyText confidence="0.999852846153846">
where h denotes the composition of u and v. Dif-
ferent composition models arise, depending on
how f is chosen. Our earlier work (Mitchell and
Lapata, 2008) explored two broad classes of mod-
els based on additive and multiplicative functions.
Additive models are the most common method
of vector combination in the literature. They have
been applied to a wide variety of tasks includ-
ing document coherence (Foltz et al., 1998), es-
say grading (Landauer and Dumais, 1997), mod-
eling selectional restrictions (Kintsch, 2001), and
notably language modeling (Coccaro and Jurafsky,
1998; Wandmacher and Antoine, 2007):
</bodyText>
<equation confidence="0.990932">
hi = ui + vi (2)
</equation>
<bodyText confidence="0.999877846153846">
Vector addition (or averaging, which is equivalent
under the cosine similarity measure) is a computa-
tionally efficient composition model as it does not
increase the dimensionality of the resulting vector.
However, the idea of averaging is somewhat coun-
terintuitive from a linguistic perspective. Compo-
sition of simple elements onto more complex ones
must allow the construction of novel meanings
which go beyond those of the individual elements
(Pinker, 1994).
In Mitchell and Lapata (2008) we argue that
composition models based on multiplication ad-
dress this problem:
</bodyText>
<equation confidence="0.996483">
hi = ui · vi (3)
</equation>
<bodyText confidence="0.999421466666667">
Whereas the addition of vectors ‘lumps their con-
tent together’, multiplication picks out the content
relevant to their combination by scaling each com-
ponent of one with the strength of the correspond-
ing component of the other. This argument is ap-
pealing, especially if one is interested in explain-
ing how the meaning of a verb is modulated by
its subject. Here, we also develop a complemen-
tary, probabilistic argument for the validity of this
model.
Let us assume that semantic vectors are based
on components defined as the ratio of the condi-
tional probability of a context word given the tar-
get word to the overall probability of the context
word.
</bodyText>
<equation confidence="0.987695">
p(contexti|target)
vi = (4)
p(contexti)
</equation>
<bodyText confidence="0.999926636363636">
These vectors represent the distributional proper-
ties of a given target word in terms of the strength
of its co-occurrence with a set of context words.
Dividing through by the overall probability of each
context word prevents the vectors being dominated
by the most frequent context words, which will of-
ten also have the highest conditional probabilities.
Let us assume vectors u and v represent tar-
get words w1 and w2. Now, when we compose
these vectors using the multiplicative model and
the components definition in (4), we obtain:
</bodyText>
<equation confidence="0.9900586">
hi = vi · ui = p(ci|w1)
p(ci)
And by Bayes’ theorem:
hi = p(w1|ci)p(w2|ci) (6)
p(w1)p(w2)
</equation>
<bodyText confidence="0.9931665">
Assuming w1 and w2 are independent and apply-
ing Bayes’ theorem again, hi becomes:
</bodyText>
<equation confidence="0.995599666666667">
p(ci|w1w2) (7)
p(ci)
p(ci|w2) (5)
p(ci)
hi —_ p(w1w2|ci)
p(w1w2)
</equation>
<page confidence="0.984339">
433
</page>
<bodyText confidence="0.999927764705883">
By comparing to (4), we can see that the expres-
sion on the right hand side gives us something akin
to the vector components we would expect when
our target is the co-occurrence of w1 and w2. Thus,
for the multiplicative model, the combined vec-
tor hi can be thought of as an approximation to
a vector representing the distributional properties
of the phrase w1w2.
If multiplication results in a vector which is
something like the representation of w1 and w2,
then addition produces a vector which is more like
the representation of w1 or w2. Suppose we were
unsure whether a word token x was an instance
of w1 or of w2. It would be reasonable to express
the probabilities of context words around this to-
ken in terms of the probabilities for w1 and w2,
assuming complete uncertainty between them:
</bodyText>
<equation confidence="0.99807">
1 1
p(ci|x) = 2 p(ci|w1)+ 2 p(ci|w2) (8)
</equation>
<bodyText confidence="0.998757333333333">
Therefore, we could represent x with a vector,
based on these probabilities, having the compo-
nents:
</bodyText>
<equation confidence="0.997391">
1 p(ci|w2) (9)
2 p(ci)
</equation>
<bodyText confidence="0.999347411764706">
Which is exactly the vector averaging approach to
semantic composition. As more vectors are com-
bined, vector addition will lead to greater general-
ity rather than greater specificity. The multiplica-
tive approach, on the other hand, picks out the
components of the constituents that are relevant
to the combination, and represents more faithfully
the properties of their conjunction.
As an aside, we should point out that our earlier
work (Mitchell and Lapata, 2008) introduced sev-
eral other models, additive and multiplicative, be-
sides the ones discussed here. We selected the ad-
ditive model as a baseline and also due to its over-
whelming popularity in the language modeling lit-
erature. The multiplicative model presented above
performed best in our evaluation study (i.e., pre-
dicting verb-subject similarity).
</bodyText>
<sectionHeader confidence="0.995631" genericHeader="method">
4 Language Modeling
</sectionHeader>
<bodyText confidence="0.999089285714286">
Estimating Probabilities In language modeling
our aim is to derive probabilities, p(w|h), given
the semantic representations of word, w, and its
history, h, based on the assumption that probable
words should be semantically coherent with the
history. Semantic coherence is commonly mea-
sured via the cosine of the angle between two vec-
</bodyText>
<equation confidence="0.985301833333333">
tors:
w · h
sim(w,h) = (10)
|w||h|
w·h =E wihi (11)
i
</equation>
<bodyText confidence="0.999764217391304">
where w · h is the dot product of w and h. Coc-
caro and Jurafsky (1998) utilize this measure in
their approach to language modeling. Unfortu-
nately, they find it necessary to resort to a number
of ad-hoc mechanisms to turn the cosine similari-
ties into useful probabilities. The primary problem
with the cosine measure is that, although its values
lie between 0 and 1, they do not sum to 1, as prob-
abilities must. Thus, some form of normalization
is required. A further problem concerns the fact
that such a measure takes no account of the under-
lying frequency of w, which is crucial for a proba-
bilistic model. For example, encephalon and brain
are roughly synonymous, and may be equally sim-
ilar to some context, but brain may nonetheless be
much more likely, as it is generally more common.
An ideal measure would take account of the un-
derlying probabilities of the elements involved and
produce values that sum to 1. Our approach is to
modify the dot product (equation (11)) on which
the cosine measure is based. Assuming that our
vector components are given by equation (4), the
dot product becomes:
</bodyText>
<equation confidence="0.9941855">
p(ci|h) (12)
p(ci)
</equation>
<bodyText confidence="0.944815">
which we modify to derive probabilities as fol-
lows:
</bodyText>
<equation confidence="0.9987365">
p(ci|h)p(ci) (13)
p(ci)
</equation>
<bodyText confidence="0.999852769230769">
This expression now weights the sum with the in-
dependent probabilities of the context words and
the word to be predicted. That this is indeed a valid
probability can be seen by the fact it is equiva-
lent to Ei p(w|ci)p(ci|h). However, in constructing
a representation of the history h, it is more conve-
nient to work with equation (13) as it is based on
vector components and can be readily used with
the composition models presented in Mitchell and
Lapata (2008).
Equation (13) allows us to derive probabilities
from vectors representing a word and its prior his-
tory. We must also construct a representation of
</bodyText>
<equation confidence="0.999497625">
1 p(ci|w1)
+
2 p(ci)
xi =
w·h =E p(ci|w)
i p(ci)
p(w|h) = p(w)E p(ci|w)
i p(ci)
</equation>
<page confidence="0.989018">
434
</page>
<bodyText confidence="0.9982644">
the history up to the nth word of a sentence. To do
this, we combine, via some (additive or multiplica-
tive) function f, the vector representing that word
with the vector representing the history up to n−1
words:
</bodyText>
<equation confidence="0.9997905">
hn = f(wn,hn−1) (14)
h1 = w1 (15)
</equation>
<bodyText confidence="0.999741571428571">
One issue that must be resolved in implement-
ing equation (14) is that the history vector should
remain correctly normalized. In other words, the
products hi · p(ci) must themselves be a valid dis-
tribution over context words. So, after each vec-
tor composition the history vector is normalized
as follows:
</bodyText>
<equation confidence="0.9916265">
hi = ∑ˆhj ·p(ci)
j
</equation>
<bodyText confidence="0.999551705882353">
Equations (13)–(16) define a language model
that incorporates vector composition. To generate
probability estimates, it requires a set of word vec-
tors whose components are based on the ratio of
probabilities described by equation (4).
Our discussion thus far has assumed a spatial
semantic space model similar to that employed in
Mitchell and Lapata (2008). However, there is no
reason why the vectors should not be constructed
by some other means. As mentioned earlier, in the
LDA topic model, words are represented as dis-
tributions over topics. These distributions are es-
sentially components of a vector v corresponding
to the target word for which we wish to construct
a semantic representation. Analogously to equa-
tion (4), we convert these probabilities to ratios of
probabilities:
</bodyText>
<equation confidence="0.998574666666667">
p(topici|target)
vi = (17)
p(topici)
</equation>
<bodyText confidence="0.999243375">
Integrating with Other Language Models The
models defined above are based on little more than
semantic coherence. As such they will be only
weakly predictive, since they largely ignore word
order, which n-gram models primarily exploit. The
simplest means to integrate semantic information
with a standard language model involves combin-
ing two probability estimates as a weighted sum:
</bodyText>
<equation confidence="0.999698">
p(w|h) = λ1p1(w|h) + (1− λ)p2(w|h) (18)
</equation>
<bodyText confidence="0.999296833333333">
Linear interpolation is guaranteed to produce
valid probabilities, and has been used, for exam-
ple, to integrate structured language models with
n-gram models (Roark, 2001). However, it will
work best when the models being combined are
roughly equally predictive and have complemen-
tary strengths and weaknesses. If one model is
much weaker than the other, linear interpolation
will typically produce a model of intermediate
strength (i.e., worse than the better model), with
the weaker model contributing a form of smooth-
ing at best.
Therefore, based on equation (13), we express
our semantic probabilities as the product of the
unigram probability, p(w), and a semantic com-
ponent, Δ, which determines the factor by which
this probability should be scaled up or down given
the context in which it occurs.
</bodyText>
<equation confidence="0.998674666666667">
p(w|h) = p(w) ·Δ(w,h) (19)
p(ci|h)p(ci) (20)
p(ci)
</equation>
<bodyText confidence="0.995403333333333">
Thus, it seems reasonable to integrate the n-gram
model by replacing the unigram probabilities with
the n-gram versions.2
</bodyText>
<equation confidence="0.999957">
ˆp(wn) = p(wn|wn−1
n−2)·Δ(wn,h) (21)
</equation>
<bodyText confidence="0.971517">
To obtain a true probability estimate we normalize
ˆp(wn) by dividing through the sum of all word
probabilities:
</bodyText>
<equation confidence="0.998817333333333">
p(wn|wn−1
n−2,h) = ∑w ˆp(w)
ˆp(wn) (22)
</equation>
<bodyText confidence="0.999995133333334">
In integrating our semantic model with an n-gram
model, we allow the latter to handle short range
dependencies and have the former handle the
longer dependencies outside the n-gram window.
For this reason, the history h used by the semantic
model in the prediction of wn only includes words
up to wn−3 (i.e., only words outside the n-gram).
We also integrate our models with a structured
language model (Roark, 2001). However, in this
case we use linear interpolation (equation (18))
because the models are roughly equally predic-
tive and also because linear interpolation is widely
used when structured language models are com-
bined with n-grams and other information sources.
This approach also has the benefit of allowing the
</bodyText>
<equation confidence="0.9831874">
2Equation (21) can also be expressed as p(wn|wn−1
n−2,h) ≈
p(wn|wn−1
p(wn) , Which is equivalent to assuming that h is
n−2)p(wn|h)
</equation>
<bodyText confidence="0.463769">
conditionally independent of wn−1
</bodyText>
<note confidence="0.529079">
n−2 (Gildea and Hofmann,
</note>
<page confidence="0.34319">
1999).
</page>
<equation confidence="0.994832">
ˆhi (16)
Δ(w,h) = ∑ p(ci|w)
i p(ci)
</equation>
<page confidence="0.995944">
435
</page>
<bodyText confidence="0.99948125">
models to be combined without out the need to
renormalize the probabilities. In the case of the
structured language model, normalizing across the
whole vocabulary would be prohibitive.
</bodyText>
<sectionHeader confidence="0.992733" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99998130120482">
In this section we discuss our experimental design
for assessing the performance of the models pre-
sented above. We give details on our training pro-
cedure and parameter estimation, and present the
methods used for comparison with our approach.
Method Following previous work (e.g., Belle-
garda (2000)) we integrated our compositional
language models with a standard n-gram model
(see equation (21)). We experimented with addi-
tive and multiplicative composition functions, and
two semantic representations (LDA and the sim-
pler semantic space model), resulting in four com-
positional models. In addition, we compared our
models against a state of the art structured lan-
guage model in order to assess the extent to which
the information provided by the semantic repre-
sentation is complementary to syntactic structure.
Our experiments used Roark’s (2001) grammar-
based language model. Similarly to standard lan-
guage models, it computes the probability of the
next word based upon the previous words of the
sentence. This is done by computing a subset of all
possible grammatical relations for the prior words
and then estimating the probability of the next
grammatical structure and the probability of see-
ing the next word given each of the prior gram-
matical relations. When estimating the probability
of the next word, the model conditions on the two
prior heads of constituents, thereby using informa-
tion about word triples (like a trigram model).
All our models were evaluated by computing
perplexity on the test set. Roughly, this quanti-
fies the degree of unpredictability in a probabil-
ity distribution, such that a fair k-sided dice would
have a perplexity of k. More precisely, perplexity
is the reciprocal of the geometric average of the
word probabilities and a lower score indicates bet-
ter predictions.
Parameter Estimation The compositional lan-
guage models were trained on the BLLIP corpus,
a collection of texts from the Wall Street Journal
(years 1987–89). The training corpus consisted of
38,521,346 words. We used a development corpus
of 50,006 words and a test corpus of similar size.
All words were converted to lowercase and num-
bers were replaced with the symbol (num). A vo-
cabulary of 20,000 words was chosen and the re-
maining tokens were replaced with (unk).
Following Mitchell and Lapata (2008), we con-
structed a simple semantic space based on co-
occurrence statistics from the BLLIP training set.
We used the 2,000 most frequent word types as
contexts and a symmetric five word window. Vec-
tor components were defined as in equation (4).
Contrary to our earlier work, we did not lemma-
tize the corpus before constructing the vectors as
in the context of language modeling this was not
appropriate. We also trained the LDA model on
BLLIP, using Blei et al.’s (2003) implementation.3
We experimented with different numbers of topics
on the development set (from 10 to 200) and re-
port results on the test set with 100 topics. In our
experiments, the hyperparameter α was initialized
to 0.5, and the b word probabilities were initial-
ized randomly.
We integrated our compositional models with a
trigram model which we also trained on BLLIP.
The model was built using the SRILM toolkit
(Stolcke, 2002) with backoff and Good-Turing
smoothing. Ideally, we would have liked to train
Roark’s (2001) parser on the same data as that
used for the semantic models. However, this would
require a gold standard treebank several times
larger than those currently available. Following
previous work on structured language modeling
(Roark, 2001; Charniak, 2001; Chelba and Jelinek,
1998), we therefore trained the parser on sections
2–21 of the Penn Treebank containing 936,017
words. Note that Roark’s (2001) parser produces
prefix probabilities for each word of a sentence
which we converted to conditional probabilities by
dividing each current probability by the previous
one.
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9987835">
Table 1 shows perplexity results when the com-
positional models are combined with an n-gram
model. With regard to the simple semantic space
model (SSM) we observe that both additive and
multiplicative approaches to constructing history
are successful in reducing perplexity over the
n-gram baseline, with the multiplicative model
outperforming the additive one. This confirms the
</bodyText>
<footnote confidence="0.967839">
3Available from http://www.cs.princeton.edu/
˜blei/lda-c/index.html.
</footnote>
<page confidence="0.994449">
436
</page>
<table confidence="0.96281625">
Model Perplexity
n-gram 78.72
n-gram+AddSSM 76.65
n-gram + MultiplySSM 75.01
n-gram+AddLDA 76.60
n-gram+MultiplyLDA 123.93
parser 173.35
n-gram + parser 75.22
n-gram + parser + AddSSM 73.45
n-gram + parser + MultiplySSM 71.32
n-gram + parser + AddLDA 71.58
n-gram + parser + MultiplyLDA 87.93
</table>
<tableCaption confidence="0.977916">
Table 1: Perplexities for n-gram, composition and
</tableCaption>
<bodyText confidence="0.995949">
structured language models, and their combina-
tions; subscripts SSM and LSA refer to the semantic
space and LDA models, respectively.
hypothesis that for this type of semantic space the
multiplicative vector combination function pro-
duces representations which have a sounder prob-
abilistic basis.
The results for the LDA model are also reported
in the table. This model reduces perplexity with an
additive composition function, but performs worse
than the n-gram with a multiplicative function. For
comparison, Figure 1 plots the perplexity of the
combined LDA and n-gram models against the
number of topics. Increasing the number of top-
ics produces higher dimensional representations
which ought to be richer, more detailed and there-
fore more predictive. While this is true for the
additive model, a greater number of topics actu-
ally increases the perplexity of the multiplicative
model, indicating it has become less predictive.
We compared these perplexity reductions
against those obtained with a structured lan-
guage model. Following Roark (2001), we com-
bined the structured language model with a
trigram model using linear interpolation (the
weights were optimized on the development
set). This model (n-gram + parser) performs
comparably to our best compositional model
(n-gram + MultiplySSM). While both models in-
corporate long range dependencies, the parser is
trained on a hand annotated treebank, whereas the
compositional model uses raw text, albeit from
a larger corpus. Interestingly, when interpolating
the trigram with the parser and the compositional
models, we obtain additional perplexity reduc-
tions. This suggests that the semantic models are
</bodyText>
<figureCaption confidence="0.883569">
Figure 1: Perplexity versus Number of Topics for
the LDA models using additive and multiplicative
composition functions.
</figureCaption>
<bodyText confidence="0.999938142857143">
encoding useful predictive information about long
range dependencies, which is distinct from and po-
tentially complementary to the parser’s syntactic
information about such dependencies. Note that
the semantic space multiplicative model yields the
highest perplexity reduction in this suite of exper-
iments followed by the LDA additive model.
</bodyText>
<sectionHeader confidence="0.999214" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999671333333333">
In this paper we advocated the use of vector
composition models for language modeling. Us-
ing semantic representations of words outside the
n-gram window, we enhanced a trigram model
with longer range dependencies. We compared
composition models based on addition and multi-
plication and examined the influence of the under-
lying semantic space on the composition task. Our
results indicate that the multiplicative composition
function produced the most predictive representa-
tions with a simple semantic space. Interestingly,
its effect in the LDA setting was detrimental. In-
creasing the representational power of the LDA
model, by using a greater number of topics, ren-
dered the multiplicative model less predictive.
These results, together with the basic mathe-
matical structure of the LDA model, suggest that
it may not be well suited to forming represen-
tations for word sequences. In particular, the as-
sumption that words are generated independently
within documents prevents the interactions be-
tween words being modeled. This assumption,
along with the Dirichlet prior on document distri-
butions tends to lead to highly sparse word vec-
</bodyText>
<page confidence="0.995669">
437
</page>
<bodyText confidence="0.999951379310345">
tors, with a typical word being strongly associated
with only one or two topics. Multiplication of a
number of these vectors generally produces a vec-
tor in which most of these associations have been
obliterated by the sparse components, resulting in
a representation with little predictive power.
These shortcomings arise from the mathemati-
cal formulation of LDA, which is not directed at
modeling the semantic interaction between words.
An interesting future direction would be to opti-
mize the vector components of the probabilistic
model over a suitable training corpus, in order to
derive a vector model of semantics adapted specif-
ically to the task of composition. We also plan to
investigate more sophisticated composition mod-
els that take syntactic structure into account. Our
results on interpolating the compositional mod-
els with a parser indicate that there is substantial
mileage to be gained by combining syntactic and
semantic dependencies.
Acknowledgements We are grateful to Brian
Roark for making his parser available to us.
Thanks to Frank Keller and Victor Lavrenko
for insightful comments and suggestions. This
work was supported by the Economic and So-
cial Research Council [grant number PTA-030-
2006-00341] and the Engineering and Physi-
cal Sciences Research Council [grant number
GR/T04540/01].
</bodyText>
<sectionHeader confidence="0.998533" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999594575342466">
Jerome R. Bellegarda. 2000. Exploiting latent se-
mantic information in statistical language modeling.
Proceedings of the IEEE, 88(8):1279–1296.
Michael W. Berry, Susan T. Dumais, and Gavin W.
O’Brien. 1994. Using linear algebra for intelligent
information retrieval. SIAM Review, 37(4):573–595.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
J.A. Bullinaria and J.P. Levy. 2007. Extracting seman-
tic representations from word co-occurrence statis-
tics: A computational study. Behavior Research
Methods, 39:510–526.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of 35th Annual
Meeting of the Association for Computational Lin-
guistics and 8th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 116–123, Toulouse, France.
Ciprian Chelba and Frederick Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In
Proceedings of the 17th International Conference on
Computational Linguistics and 36th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 225–231, Montr´eal, Canada.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
2nd Symposium on Quantum Interaction, pages
133–140, Oxford, UK. College Publications.
Noah Coccaro and Daniel Jurafsky. 1998. Towards
better integration of semantic predictors in satistical
language modeling. In Proceedings of the 5th Inter-
national Conference on Spoken Language Process-
ing, pages 2403–2406, Sydney, Australia.
Yonggang Deng and Sanjeev Khundanpur. 2003. La-
tent semantic information in maximum entropy lan-
guage models for conversational speech recognition.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 56–63, Edmonton, AL.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897–906, Honolulu, Hawaii.
J. R. Firth. 1957. A synopsis of linguistic theory 1930–
1955. In Studies in Linguistic Analysis, pages 1–32.
Philological Society, Oxford.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence
with latent semantic analysis. Discourse Process,
15:285–307.
Daniel Gildea and Thomas Hofmann. 1999. Topic-
based language models using EM. In Proceedings of
the 6th European Conference on Speech Communi-
ation and Technology, pages 2167–2170, Budapest,
Hungary.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA, USA.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114(2):211–244.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 41(2):177–196.
Dharmendra Kanejiya, Arun Kumar, and Surendra
Prasad. 2004. Statistical language modeling with
performance benchmarks using various levels of
</reference>
<page confidence="0.986214">
438
</page>
<reference confidence="0.999290558823529">
syntactic-semantic information. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 1161–1167, Geneva, Switzerland.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173–202.
Roland Kuhn and Renato de Mori. 1992. A cache
based natural language model for speech recogni-
tion. IEEE Transactions on Pattern Analysis and
Machine Intelligence, (14):570–583.
T. K. Landauer and S. T. Dumais. 1997. A solution
to Plato’s problem: the latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104(2):211–240.
Will Lowe. 2000. Topographic Maps of Semantic
Space. Ph.D. thesis, University of Edinburgh.
Scott McDonald. 2000. Environmental Determinants
of Lexical Processing Effort. Ph.D. thesis, Univer-
sity of Edinburgh.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244, Columbus, OH.
R. Montague. 1974. English as a formal language. In
R. Montague, editor, Formal Philosophy. Yale Uni-
versity Press, New Haven, CT.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
B. Partee. 1995. Lexical semantics and compositional-
ity. In Lila Gleitman and Mark Liberman, editors,
Invitation to Cognitive Science Part I: Language,
pages 311–360. MIT Press, Cambridge, MA.
S. Pinker. 1994. The Language Instinct: How the Mind
Creates Language. HarperCollins, New York.
Tony A. Plate. 1995. Holographic reduced represen-
tations. IEEE Transactions on Neural Networks,
6(3):623–641.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Roni Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10:187–228.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures
in connectionist systems. Artificial Intelligence,
46:159–216.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901–904, Denver, CO.
Tonio Wandmacher and Jean-Yves Antoine. 2007.
Methods to integrate a language model with seman-
tic information for a word prediction component.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 506–513, Prague, Czech
Republic.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
2nd Symposium on Quantum Interaction, Oxford,
UK. College Publications.
Rong Zhang and Alexander I. Rudnicky. 2002. Im-
prove latent semantic analysis based language model
by integrating multiple level knowldege. In Pro-
ceedings of the 7th International Conference on Spo-
ken Language Processing, pages 893–897, Denver,
CO.
</reference>
<page confidence="0.999266">
439
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416110">
<title confidence="0.99955">Language Models Based on Semantic Composition</title>
<author confidence="0.99102">Mitchell</author>
<affiliation confidence="0.981827">School of Informatics, University of</affiliation>
<note confidence="0.438737">Edinburgh EH8 9LW,</note>
<abstract confidence="0.999625588235294">In this paper we propose a novel statistical language model to capture long-range semantic dependencies. Specifically, we apply the concept of semantic composition to the problem of constructing predictive history representations for upcoming words. We also examine the influence of the underlying semantic space on the composition task by comparing spatial semantic representations against topic-based ones. The composition models yield reductions in perplexity when combined with a stanlanguage model over the model alone. We also obtain perplexity reductions when integrating our models with a structured language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="2794" citStr="Bellegarda (2000" startWordPosition="432" endWordPosition="433">d the n-gram horizon. Cache language models (Kuhn and de Mori, 1992) increase the probability of words observed in the history, e.g., by some factor which decays exponentially with distance. Trigger models (Rosenfeld, 1996) go a step further by allowing arbitrary word pairs to be incorporated into the cache. Structured language models (e.g., Roark (2001)) go beyond the representation of history as a linear sequence of words to capture the syntactic constructions in which these words are embedded. It is also possible to build representations of history which are semantic rather than syntactic (Bellegarda (2000; Coccaro and Jurafsky (1998; Gildea and Hofmann (1999)). In this approach, estimates for the probabilities of upcoming words are derived from a comparison of their semantic content with the content of the history so far. The semantic representations, in this case, are vectors derived from the distributional properties of words in a corpus, based on the insight that words which are semantically similar will be found in similar contexts (Harris, 1968; Firth, 1957). Although the the construction of a semantic representation for the history is crucial to this approach, the underlying vector-based</context>
<context position="12112" citStr="Bellegarda, 2000" startWordPosition="1904" endWordPosition="1905">, producing sparse vectors. And lastly, the assumption that words are generated independently means the interaction between them is not modeled. 2.2 Language Modeling using Semantic Representations A common approach to embedding semantic representations within language modeling is to measure the semantic similarity between an upcoming word and its history and use it to modify the probabilities from an n-gram model. In this way, the n-gram’s sensitivity to short-range dependencies is enriched with information about longer-range semantic coherence. Much of previous work has taken this approach (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007), whilst relying on LSA to provide semantic representations for individual words. Some authors (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007) use the geometric notion of a vector centroid to construct representations of history, whereas others (Bellegarda, 2000; Deng and Khundanpur, 2003) use the idea of a “pseudodocument”, which is derived from the algebraic relation between documents and words assumed within LSA. They all derive P(wi|hi), the probability of an upcoming word given its history, from the cosine similarity me</context>
<context position="25985" citStr="Bellegarda (2000)" startWordPosition="4174" endWordPosition="4176">(wn|h) conditionally independent of wn−1 n−2 (Gildea and Hofmann, 1999). ˆhi (16) Δ(w,h) = ∑ p(ci|w) i p(ci) 435 models to be combined without out the need to renormalize the probabilities. In the case of the structured language model, normalizing across the whole vocabulary would be prohibitive. 5 Experimental Setup In this section we discuss our experimental design for assessing the performance of the models presented above. We give details on our training procedure and parameter estimation, and present the methods used for comparison with our approach. Method Following previous work (e.g., Bellegarda (2000)) we integrated our compositional language models with a standard n-gram model (see equation (21)). We experimented with additive and multiplicative composition functions, and two semantic representations (LDA and the simpler semantic space model), resulting in four compositional models. In addition, we compared our models against a state of the art structured language model in order to assess the extent to which the information provided by the semantic representation is complementary to syntactic structure. Our experiments used Roark’s (2001) grammarbased language model. Similarly to standard</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Jerome R. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the IEEE, 88(8):1279–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>Susan T Dumais</author>
<author>Gavin W O’Brien</author>
</authors>
<title>Using linear algebra for intelligent information retrieval.</title>
<date>1994</date>
<journal>SIAM Review,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Berry, Dumais, O’Brien, 1994</marker>
<rawString>Michael W. Berry, Susan T. Dumais, and Gavin W. O’Brien. 1994. Using linear algebra for intelligent information retrieval. SIAM Review, 37(4):573–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="9714" citStr="Blei et al., 2003" startWordPosition="1526" endWordPosition="1529">SA, Hofmann (2001)) casts the relationship between documents and words in terms of a generative model based on a set of hidden topics. Documents are represented by distributions over topics and topics are distributions over words. Thus the mixture of topics in any document determines its vocabulary. Maximum likelihood estimation of these distributions over a word-document matrix has a comparable effect to SVD in LSA: a set of hidden semantic factors, in this case topics, are extracted and documents and words are represented by these topics. Latent Dirichlet Allocation (Griffiths et al., 2007; Blei et al., 2003) enhances further the mathematical foundation of this approach. Whereas pLSA treats each document as a separate, independent mixture of topics, LDA assumes that the topic distributions of documents are generated by a Dirichlet distribution. Thus, LDA is a probabilistic model of the whole document collection. In this model the process of generating a document can be described as follows: 1. draw a multinomial distribution θ from a Dirichlet distribution parametrized by α 2. for each word in a document: (a) draw a topic zk from the multinomial distribution characterized by θ (b) draw a word from</context>
<context position="11046" citStr="Blei et al. (2003" startWordPosition="1738" endWordPosition="1741">epresentation for a multi-word sequence amounts to estimating the topic proportions for that sequence.1 Structure here arises from the mathematical form of the model, as opposed to any linguistic assumptions. Without anticipating our results too much, we should point out that several features of the LDA model are likely to affect the representation of 1Estimating the posterior distribution P(θ,z|w,α,β) of the hidden variables given an observed collection of documents w is intractable in general; however, a variety of approximate inference algorithms have been proposed in the literature (e.g., Blei et al. (2003; Griffiths et al. (2007)). multi-word sequences. Firstly, it is a top-down generative model (the topic proportions for a document are first selected and then this drives the generation of words) as opposed to a bottom-up constructive process (words modulate each other to produce a complex representation of their combination). Secondly, the top level Dirichlet distribution is likely to lead to documents being dominated by a small number of topics, producing sparse vectors. And lastly, the assumption that words are generated independently means the interaction between them is not modeled. 2.2 L</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bullinaria</author>
<author>J P Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="6592" citStr="Bullinaria and Levy, 2007" startWordPosition="1017" endWordPosition="1021">ose words. Essentially, these models provide a simple procedure for constructing spatial representations of word meaning. Topic models, in contrast, impose a probabilistic model onto those distributional statistics, under the assumption that hidden topic variables drive the process that generates words. Both approaches represent the meanings of words in terms of an n-dimensional series of values, but whereas the semantic space model treats those values as defining a vector with spatial properties, the topic model treats them as a probability distribution. A simple and popular (McDonald, 2000; Bullinaria and Levy, 2007; Lowe, 2000) way to construct a semantic space model is to associate each vector component with a particular context word, and assign it a value based on the strength of its co-occurrence with the target (i.e., the word for which a semantic representation is being constructed). For example, in Mitchell and Lapata (2008) we used the 2,000 most frequent content words in a corpus as their contexts, and defined co-occurrence in terms of the context word being present in a five word window on either side of the target word. We calculated the ratio of the probability of the context word given the t</context>
<context position="7870" citStr="Bullinaria and Levy (2007)" startWordPosition="1233" endWordPosition="1236"> word and use these values as their vector components. This procedure has the benefits of simplicity and also of being largely free of any additional theoretical assumptions over and above the distributional approach to semantics. This is not to say that more sophisticated approaches have not been developed or that they are not useful. Much work has been devoted to enriching semantic space models with syntactic information (e.g., Grefenstette (1994; Pad´o and Lapata (2007)), selectional preferences (Erk and Pad´o, 2008) or with identifying optimal ways of defining the vector components (e.g., Bullinaria and Levy (2007)). The semantic space discussed thus far is based on word co-occurrence statistics. However, the statistics of how words are distributed across the documents also carry useful semantic information. Latent Semantic Analysis (LSA, Landauer and Dumais (1997) utilizes precisely this distributional information to uncover hidden semantic factors by means of dimensionality reduction. Singular value decomposition (SVD, Berry et al. (1994)) is applied to a word-document co-occurrence matrix which is factored into a product of a number of other matrices; one of them represents words in terms of the sema</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>J.A. Bullinaria and J.P. Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>116--123</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="29238" citStr="Charniak, 2001" startWordPosition="4703" endWordPosition="4704">ur experiments, the hyperparameter α was initialized to 0.5, and the b word probabilities were initialized randomly. We integrated our compositional models with a trigram model which we also trained on BLLIP. The model was built using the SRILM toolkit (Stolcke, 2002) with backoff and Good-Turing smoothing. Ideally, we would have liked to train Roark’s (2001) parser on the same data as that used for the semantic models. However, this would require a gold standard treebank several times larger than those currently available. Following previous work on structured language modeling (Roark, 2001; Charniak, 2001; Chelba and Jelinek, 1998), we therefore trained the parser on sections 2–21 of the Penn Treebank containing 936,017 words. Note that Roark’s (2001) parser produces prefix probabilities for each word of a sentence which we converted to conditional probabilities by dividing each current probability by the previous one. 6 Results Table 1 shows perplexity results when the compositional models are combined with an n-gram model. With regard to the simple semantic space model (SSM) we observe that both additive and multiplicative approaches to constructing history are successful in reducing perplex</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–123, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>225--231</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="29265" citStr="Chelba and Jelinek, 1998" startWordPosition="4705" endWordPosition="4708">the hyperparameter α was initialized to 0.5, and the b word probabilities were initialized randomly. We integrated our compositional models with a trigram model which we also trained on BLLIP. The model was built using the SRILM toolkit (Stolcke, 2002) with backoff and Good-Turing smoothing. Ideally, we would have liked to train Roark’s (2001) parser on the same data as that used for the semantic models. However, this would require a gold standard treebank several times larger than those currently available. Following previous work on structured language modeling (Roark, 2001; Charniak, 2001; Chelba and Jelinek, 1998), we therefore trained the parser on sections 2–21 of the Penn Treebank containing 936,017 words. Note that Roark’s (2001) parser produces prefix probabilities for each word of a sentence which we converted to conditional probabilities by dividing each current probability by the previous one. 6 Results Table 1 shows perplexity results when the compositional models are combined with an n-gram model. With regard to the simple semantic space model (SSM) we observe that both additive and multiplicative approaches to constructing history are successful in reducing perplexity over the n-gram baselin</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics, pages 225–231, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd Symposium on Quantum Interaction,</booktitle>
<pages>133--140</pages>
<publisher>UK. College Publications.</publisher>
<location>Oxford,</location>
<contexts>
<context position="14019" citStr="Clark et al., 2008" startWordPosition="2190" endWordPosition="2193">e involve representing the history by multiple LSA models of varying granularity in an attempt to capture topic, subtopic, and local information (Zhang and Rudnicky, 2002); incorporating syntactic information by building the semantic space over words and their syntactic annotations (Kanejiya et al., 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003). 3 Composition Models The problem of vector composition has received relatively little attention within natural language processing. Attempts to use tensor products (Smolensky, 1990; Clark et al., 2008; Widdows, 2008) as a means of binding one vector to another face major computational difficulties as their dimensionality grows exponentially with the number of constituents being composed. To overcome this problem, other techniques (Plate, 1995) have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components. Crucially, the success of these methods depends on the assumption that the vector components are randomly distributed. This is problematic for modeling language which has regular structure. Given the above considerations, in</context>
</contexts>
<marker>Clark, Coecke, Sadrzadeh, 2008</marker>
<rawString>Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of meaning. In Proceedings of the 2nd Symposium on Quantum Interaction, pages 133–140, Oxford, UK. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Coccaro</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Towards better integration of semantic predictors in satistical language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th International Conference on Spoken Language Processing,</booktitle>
<pages>2403--2406</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2822" citStr="Coccaro and Jurafsky (1998" startWordPosition="434" endWordPosition="437">on. Cache language models (Kuhn and de Mori, 1992) increase the probability of words observed in the history, e.g., by some factor which decays exponentially with distance. Trigger models (Rosenfeld, 1996) go a step further by allowing arbitrary word pairs to be incorporated into the cache. Structured language models (e.g., Roark (2001)) go beyond the representation of history as a linear sequence of words to capture the syntactic constructions in which these words are embedded. It is also possible to build representations of history which are semantic rather than syntactic (Bellegarda (2000; Coccaro and Jurafsky (1998; Gildea and Hofmann (1999)). In this approach, estimates for the probabilities of upcoming words are derived from a comparison of their semantic content with the content of the history so far. The semantic representations, in this case, are vectors derived from the distributional properties of words in a corpus, based on the insight that words which are semantically similar will be found in similar contexts (Harris, 1968; Firth, 1957). Although the the construction of a semantic representation for the history is crucial to this approach, the underlying vector-based models are primarily design</context>
<context position="4113" citStr="Coccaro and Jurafsky (1998)" startWordPosition="637" endWordPosition="640">ly, we would like to compose the meaning of the history out of its constituent parts. This is by no means a new idea. Much work in linguistic theory (Partee, 1995; Montague, 1974) has been devoted to compositionality, the process of determining the meaning of complex expressions from simpler ones. Previous work either ignores this issue (e.g., Bellegarda (2000)) or simply com430 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430–439, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP putes the centroid of the vectors representing the history (e.g., Coccaro and Jurafsky (1998)). This is motivated primarily by mathematical convenience rather than by empirical evidence. In our earlier work (Mitchell and Lapata, 2008) we formulated composition as a function of two vectors and introduced a variety of models based on addition and multiplication. In this paper we apply vector composition to the problem of constructing predictive history representations for language modeling. Besides integrating composition with language modeling, a task which is novel to our knowledge, our approach also serves as a valuable testbed of our earlier framework which we originally evaluated o</context>
<context position="12140" citStr="Coccaro and Jurafsky, 1998" startWordPosition="1906" endWordPosition="1909"> vectors. And lastly, the assumption that words are generated independently means the interaction between them is not modeled. 2.2 Language Modeling using Semantic Representations A common approach to embedding semantic representations within language modeling is to measure the semantic similarity between an upcoming word and its history and use it to modify the probabilities from an n-gram model. In this way, the n-gram’s sensitivity to short-range dependencies is enriched with information about longer-range semantic coherence. Much of previous work has taken this approach (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007), whilst relying on LSA to provide semantic representations for individual words. Some authors (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007) use the geometric notion of a vector centroid to construct representations of history, whereas others (Bellegarda, 2000; Deng and Khundanpur, 2003) use the idea of a “pseudodocument”, which is derived from the algebraic relation between documents and words assumed within LSA. They all derive P(wi|hi), the probability of an upcoming word given its history, from the cosine similarity measure which must be somehow </context>
<context position="15364" citStr="Coccaro and Jurafsky, 1998" startWordPosition="2406" endWordPosition="2409">unction f of two vectors u and v: h = f (u,v) (1) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Our earlier work (Mitchell and Lapata, 2008) explored two broad classes of models based on additive and multiplicative functions. Additive models are the most common method of vector combination in the literature. They have been applied to a wide variety of tasks including document coherence (Foltz et al., 1998), essay grading (Landauer and Dumais, 1997), modeling selectional restrictions (Kintsch, 2001), and notably language modeling (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007): hi = ui + vi (2) Vector addition (or averaging, which is equivalent under the cosine similarity measure) is a computationally efficient composition model as it does not increase the dimensionality of the resulting vector. However, the idea of averaging is somewhat counterintuitive from a linguistic perspective. Composition of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elements (Pinker, 1994). In Mitchell and Lapata (2008) we argue that composition models based on multiplication ad</context>
<context position="19714" citStr="Coccaro and Jurafsky (1998)" startWordPosition="3138" endWordPosition="3142">age modeling literature. The multiplicative model presented above performed best in our evaluation study (i.e., predicting verb-subject similarity). 4 Language Modeling Estimating Probabilities In language modeling our aim is to derive probabilities, p(w|h), given the semantic representations of word, w, and its history, h, based on the assumption that probable words should be semantically coherent with the history. Semantic coherence is commonly measured via the cosine of the angle between two vectors: w · h sim(w,h) = (10) |w||h| w·h =E wihi (11) i where w · h is the dot product of w and h. Coccaro and Jurafsky (1998) utilize this measure in their approach to language modeling. Unfortunately, they find it necessary to resort to a number of ad-hoc mechanisms to turn the cosine similarities into useful probabilities. The primary problem with the cosine measure is that, although its values lie between 0 and 1, they do not sum to 1, as probabilities must. Thus, some form of normalization is required. A further problem concerns the fact that such a measure takes no account of the underlying frequency of w, which is crucial for a probabilistic model. For example, encephalon and brain are roughly synonymous, and </context>
</contexts>
<marker>Coccaro, Jurafsky, 1998</marker>
<rawString>Noah Coccaro and Daniel Jurafsky. 1998. Towards better integration of semantic predictors in satistical language modeling. In Proceedings of the 5th International Conference on Spoken Language Processing, pages 2403–2406, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>Sanjeev Khundanpur</author>
</authors>
<title>Latent semantic information in maximum entropy language models for conversational speech recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>56--63</pages>
<location>Edmonton, AL.</location>
<contexts>
<context position="12472" citStr="Deng and Khundanpur, 2003" startWordPosition="1954" endWordPosition="1957">history and use it to modify the probabilities from an n-gram model. In this way, the n-gram’s sensitivity to short-range dependencies is enriched with information about longer-range semantic coherence. Much of previous work has taken this approach (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007), whilst relying on LSA to provide semantic representations for individual words. Some authors (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007) use the geometric notion of a vector centroid to construct representations of history, whereas others (Bellegarda, 2000; Deng and Khundanpur, 2003) use the idea of a “pseudodocument”, which is derived from the algebraic relation between documents and words assumed within LSA. They all derive P(wi|hi), the probability of an upcoming word given its history, from the cosine similarity measure which must be somehow normalized in order to yield well-formed probability estimates. The approach of Gildea and Hofmann (1999) overcomes this difficulty by using representations constructed with pLSA, which have a direct probabilistic interpretation. As a result, the probability of an upcoming word given the history can be derived naturally and direct</context>
<context position="13817" citStr="Deng and Khundanpur, 2003" startWordPosition="2160" endWordPosition="2163">1999) use an online Expectation Maximization process, which derives from the probabilistic basis of pLSA, to update the history with new words. Extensions on the basic semantic language 432 models sketched above involve representing the history by multiple LSA models of varying granularity in an attempt to capture topic, subtopic, and local information (Zhang and Rudnicky, 2002); incorporating syntactic information by building the semantic space over words and their syntactic annotations (Kanejiya et al., 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003). 3 Composition Models The problem of vector composition has received relatively little attention within natural language processing. Attempts to use tensor products (Smolensky, 1990; Clark et al., 2008; Widdows, 2008) as a means of binding one vector to another face major computational difficulties as their dimensionality grows exponentially with the number of constituents being composed. To overcome this problem, other techniques (Plate, 1995) have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components. Crucially, the success</context>
</contexts>
<marker>Deng, Khundanpur, 2003</marker>
<rawString>Yonggang Deng and Sanjeev Khundanpur. 2003. Latent semantic information in maximum entropy language models for conversational speech recognition. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 56–63, Edmonton, AL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<location>Honolulu, Hawaii.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897–906, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory</title>
<date>1957</date>
<booktitle>In Studies in Linguistic Analysis,</booktitle>
<pages>1--32</pages>
<publisher>Philological Society,</publisher>
<location>Oxford.</location>
<contexts>
<context position="3261" citStr="Firth, 1957" startWordPosition="506" endWordPosition="507">hich these words are embedded. It is also possible to build representations of history which are semantic rather than syntactic (Bellegarda (2000; Coccaro and Jurafsky (1998; Gildea and Hofmann (1999)). In this approach, estimates for the probabilities of upcoming words are derived from a comparison of their semantic content with the content of the history so far. The semantic representations, in this case, are vectors derived from the distributional properties of words in a corpus, based on the insight that words which are semantically similar will be found in similar contexts (Harris, 1968; Firth, 1957). Although the the construction of a semantic representation for the history is crucial to this approach, the underlying vector-based models are primarily designed to represent isolated words rather than word sequences. Ideally, we would like to compose the meaning of the history out of its constituent parts. This is by no means a new idea. Much work in linguistic theory (Partee, 1995; Montague, 1974) has been devoted to compositionality, the process of determining the meaning of complex expressions from simpler ones. Previous work either ignores this issue (e.g., Bellegarda (2000)) or simply </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth. 1957. A synopsis of linguistic theory 1930– 1955. In Studies in Linguistic Analysis, pages 1–32. Philological Society, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Process,</booktitle>
<pages>15--285</pages>
<contexts>
<context position="15211" citStr="Foltz et al., 1998" startWordPosition="2385" endWordPosition="2388">he above considerations, in Mitchell and Lapata (2008) we introduce a general framework for studying vector composition, which we formulate as a function f of two vectors u and v: h = f (u,v) (1) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Our earlier work (Mitchell and Lapata, 2008) explored two broad classes of models based on additive and multiplicative functions. Additive models are the most common method of vector combination in the literature. They have been applied to a wide variety of tasks including document coherence (Foltz et al., 1998), essay grading (Landauer and Dumais, 1997), modeling selectional restrictions (Kintsch, 2001), and notably language modeling (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007): hi = ui + vi (2) Vector addition (or averaging, which is equivalent under the cosine similarity measure) is a computationally efficient composition model as it does not increase the dimensionality of the resulting vector. However, the idea of averaging is somewhat counterintuitive from a linguistic perspective. Composition of simple elements onto more complex ones must allow the construction of novel meanings w</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter Foltz, Walter Kintsch, and Thomas Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse Process, 15:285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Thomas Hofmann</author>
</authors>
<title>Topicbased language models using EM.</title>
<date>1999</date>
<booktitle>In Proceedings of the 6th European Conference on Speech Communiation and Technology,</booktitle>
<pages>2167--2170</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2849" citStr="Gildea and Hofmann (1999)" startWordPosition="438" endWordPosition="441">uhn and de Mori, 1992) increase the probability of words observed in the history, e.g., by some factor which decays exponentially with distance. Trigger models (Rosenfeld, 1996) go a step further by allowing arbitrary word pairs to be incorporated into the cache. Structured language models (e.g., Roark (2001)) go beyond the representation of history as a linear sequence of words to capture the syntactic constructions in which these words are embedded. It is also possible to build representations of history which are semantic rather than syntactic (Bellegarda (2000; Coccaro and Jurafsky (1998; Gildea and Hofmann (1999)). In this approach, estimates for the probabilities of upcoming words are derived from a comparison of their semantic content with the content of the history so far. The semantic representations, in this case, are vectors derived from the distributional properties of words in a corpus, based on the insight that words which are semantically similar will be found in similar contexts (Harris, 1968; Firth, 1957). Although the the construction of a semantic representation for the history is crucial to this approach, the underlying vector-based models are primarily designed to represent isolated wo</context>
<context position="12845" citStr="Gildea and Hofmann (1999)" startWordPosition="2014" endWordPosition="2017">sentations for individual words. Some authors (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007) use the geometric notion of a vector centroid to construct representations of history, whereas others (Bellegarda, 2000; Deng and Khundanpur, 2003) use the idea of a “pseudodocument”, which is derived from the algebraic relation between documents and words assumed within LSA. They all derive P(wi|hi), the probability of an upcoming word given its history, from the cosine similarity measure which must be somehow normalized in order to yield well-formed probability estimates. The approach of Gildea and Hofmann (1999) overcomes this difficulty by using representations constructed with pLSA, which have a direct probabilistic interpretation. As a result, the probability of an upcoming word given the history can be derived naturally and directly, avoiding the need for ad-hoc transformations. In constructing their representation of history, Gildea and Hofmann (1999) use an online Expectation Maximization process, which derives from the probabilistic basis of pLSA, to update the history with new words. Extensions on the basic semantic language 432 models sketched above involve representing the history by multip</context>
<context position="25439" citStr="Gildea and Hofmann, 1999" startWordPosition="4086" endWordPosition="4089">wn−3 (i.e., only words outside the n-gram). We also integrate our models with a structured language model (Roark, 2001). However, in this case we use linear interpolation (equation (18)) because the models are roughly equally predictive and also because linear interpolation is widely used when structured language models are combined with n-grams and other information sources. This approach also has the benefit of allowing the 2Equation (21) can also be expressed as p(wn|wn−1 n−2,h) ≈ p(wn|wn−1 p(wn) , Which is equivalent to assuming that h is n−2)p(wn|h) conditionally independent of wn−1 n−2 (Gildea and Hofmann, 1999). ˆhi (16) Δ(w,h) = ∑ p(ci|w) i p(ci) 435 models to be combined without out the need to renormalize the probabilities. In the case of the structured language model, normalizing across the whole vocabulary would be prohibitive. 5 Experimental Setup In this section we discuss our experimental design for assessing the performance of the models presented above. We give details on our training procedure and parameter estimation, and present the methods used for comparison with our approach. Method Following previous work (e.g., Bellegarda (2000)) we integrated our compositional language models with</context>
</contexts>
<marker>Gildea, Hofmann, 1999</marker>
<rawString>Daniel Gildea and Thomas Hofmann. 1999. Topicbased language models using EM. In Proceedings of the 6th European Conference on Speech Communiation and Technology, pages 2167–2170, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="7696" citStr="Grefenstette (1994" startWordPosition="1209" endWordPosition="1210">ndow on either side of the target word. We calculated the ratio of the probability of the context word given the target word to the overall probability of the context word and use these values as their vector components. This procedure has the benefits of simplicity and also of being largely free of any additional theoretical assumptions over and above the distributional approach to semantics. This is not to say that more sophisticated approaches have not been developed or that they are not useful. Much work has been devoted to enriching semantic space models with syntactic information (e.g., Grefenstette (1994; Pad´o and Lapata (2007)), selectional preferences (Erk and Pad´o, 2008) or with identifying optimal ways of defining the vector components (e.g., Bullinaria and Levy (2007)). The semantic space discussed thus far is based on word co-occurrence statistics. However, the statistics of how words are distributed across the documents also carry useful semantic information. Latent Semantic Analysis (LSA, Landauer and Dumais (1997) utilizes precisely this distributional information to uncover hidden semantic factors by means of dimensionality reduction. Singular value decomposition (SVD, Berry et al</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="9694" citStr="Griffiths et al., 2007" startWordPosition="1522" endWordPosition="1525">nt Semantic Analysis (pLSA, Hofmann (2001)) casts the relationship between documents and words in terms of a generative model based on a set of hidden topics. Documents are represented by distributions over topics and topics are distributions over words. Thus the mixture of topics in any document determines its vocabulary. Maximum likelihood estimation of these distributions over a word-document matrix has a comparable effect to SVD in LSA: a set of hidden semantic factors, in this case topics, are extracted and documents and words are represented by these topics. Latent Dirichlet Allocation (Griffiths et al., 2007; Blei et al., 2003) enhances further the mathematical foundation of this approach. Whereas pLSA treats each document as a separate, independent mixture of topics, LDA assumes that the topic distributions of documents are generated by a Dirichlet distribution. Thus, LDA is a probabilistic model of the whole document collection. In this model the process of generating a document can be described as follows: 1. draw a multinomial distribution θ from a Dirichlet distribution parametrized by α 2. for each word in a document: (a) draw a topic zk from the multinomial distribution characterized by θ </context>
<context position="11071" citStr="Griffiths et al. (2007)" startWordPosition="1742" endWordPosition="1745"> multi-word sequence amounts to estimating the topic proportions for that sequence.1 Structure here arises from the mathematical form of the model, as opposed to any linguistic assumptions. Without anticipating our results too much, we should point out that several features of the LDA model are likely to affect the representation of 1Estimating the posterior distribution P(θ,z|w,α,β) of the hidden variables given an observed collection of documents w is intractable in general; however, a variety of approximate inference algorithms have been proposed in the literature (e.g., Blei et al. (2003; Griffiths et al. (2007)). multi-word sequences. Firstly, it is a top-down generative model (the topic proportions for a document are first selected and then this drives the generation of words) as opposed to a bottom-up constructive process (words modulate each other to produce a complex representation of their combination). Secondly, the top level Dirichlet distribution is likely to lead to documents being dominated by a small number of topics, producing sparse vectors. And lastly, the assumption that words are generated independently means the interaction between them is not modeled. 2.2 Language Modeling using Se</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="3247" citStr="Harris, 1968" startWordPosition="504" endWordPosition="505">tructions in which these words are embedded. It is also possible to build representations of history which are semantic rather than syntactic (Bellegarda (2000; Coccaro and Jurafsky (1998; Gildea and Hofmann (1999)). In this approach, estimates for the probabilities of upcoming words are derived from a comparison of their semantic content with the content of the history so far. The semantic representations, in this case, are vectors derived from the distributional properties of words in a corpus, based on the insight that words which are semantically similar will be found in similar contexts (Harris, 1968; Firth, 1957). Although the the construction of a semantic representation for the history is crucial to this approach, the underlying vector-based models are primarily designed to represent isolated words rather than word sequences. Ideally, we would like to compose the meaning of the history out of its constituent parts. This is by no means a new idea. Much work in linguistic theory (Partee, 1995; Montague, 1974) has been devoted to compositionality, the process of determining the meaning of complex expressions from simpler ones. Previous work either ignores this issue (e.g., Bellegarda (200</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Mathematical Structures of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="9114" citStr="Hofmann (2001)" startWordPosition="1431" endWordPosition="1432">sents documents in terms of the same factors. The algebraic relation between these matrices can be used to show that any document vector is a linear combination of the vectors representing the words it contains. Thus, within this paradigm it is nat431 ural to treat multi-word structures as a “pseudodocument” and represent them via linear combinations of word vectors. Due to its generality, LSA has proven a valuable analysis tool with a wide range of applications. However, the SVD procedure is somewhat ad-hoc lacking a sound statistical foundation. Probabilistic Latent Semantic Analysis (pLSA, Hofmann (2001)) casts the relationship between documents and words in terms of a generative model based on a set of hidden topics. Documents are represented by distributions over topics and topics are distributions over words. Thus the mixture of topics in any document determines its vocabulary. Maximum likelihood estimation of these distributions over a word-document matrix has a comparable effect to SVD in LSA: a set of hidden semantic factors, in this case topics, are extracted and documents and words are represented by these topics. Latent Dirichlet Allocation (Griffiths et al., 2007; Blei et al., 2003)</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 41(2):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dharmendra Kanejiya</author>
<author>Arun Kumar</author>
<author>Surendra Prasad</author>
</authors>
<title>Statistical language modeling with performance benchmarks using various levels of syntactic-semantic information.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>1161--1167</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="13707" citStr="Kanejiya et al., 2004" startWordPosition="2142" endWordPosition="2145">the need for ad-hoc transformations. In constructing their representation of history, Gildea and Hofmann (1999) use an online Expectation Maximization process, which derives from the probabilistic basis of pLSA, to update the history with new words. Extensions on the basic semantic language 432 models sketched above involve representing the history by multiple LSA models of varying granularity in an attempt to capture topic, subtopic, and local information (Zhang and Rudnicky, 2002); incorporating syntactic information by building the semantic space over words and their syntactic annotations (Kanejiya et al., 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003). 3 Composition Models The problem of vector composition has received relatively little attention within natural language processing. Attempts to use tensor products (Smolensky, 1990; Clark et al., 2008; Widdows, 2008) as a means of binding one vector to another face major computational difficulties as their dimensionality grows exponentially with the number of constituents being composed. To overcome this problem, other techniques (Plate, 1995) have been proposed in which the binding </context>
</contexts>
<marker>Kanejiya, Kumar, Prasad, 2004</marker>
<rawString>Dharmendra Kanejiya, Arun Kumar, and Surendra Prasad. 2004. Statistical language modeling with performance benchmarks using various levels of syntactic-semantic information. In Proceedings of the 20th International Conference on Computational Linguistics, pages 1161–1167, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="15305" citStr="Kintsch, 2001" startWordPosition="2400" endWordPosition="2401">g vector composition, which we formulate as a function f of two vectors u and v: h = f (u,v) (1) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Our earlier work (Mitchell and Lapata, 2008) explored two broad classes of models based on additive and multiplicative functions. Additive models are the most common method of vector combination in the literature. They have been applied to a wide variety of tasks including document coherence (Foltz et al., 1998), essay grading (Landauer and Dumais, 1997), modeling selectional restrictions (Kintsch, 2001), and notably language modeling (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007): hi = ui + vi (2) Vector addition (or averaging, which is equivalent under the cosine similarity measure) is a computationally efficient composition model as it does not increase the dimensionality of the resulting vector. However, the idea of averaging is somewhat counterintuitive from a linguistic perspective. Composition of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elements (Pinker, 1994). In Mitchell and Lapata (2008) </context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Walter Kintsch. 2001. Predication. Cognitive Science, 25(2):173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Renato de Mori</author>
</authors>
<title>A cache based natural language model for speech recognition.</title>
<date>1992</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>14--570</pages>
<marker>Kuhn, de Mori, 1992</marker>
<rawString>Roland Kuhn and Renato de Mori. 1992. A cache based natural language model for speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, (14):570–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="8125" citStr="Landauer and Dumais (1997)" startWordPosition="1270" endWordPosition="1273">more sophisticated approaches have not been developed or that they are not useful. Much work has been devoted to enriching semantic space models with syntactic information (e.g., Grefenstette (1994; Pad´o and Lapata (2007)), selectional preferences (Erk and Pad´o, 2008) or with identifying optimal ways of defining the vector components (e.g., Bullinaria and Levy (2007)). The semantic space discussed thus far is based on word co-occurrence statistics. However, the statistics of how words are distributed across the documents also carry useful semantic information. Latent Semantic Analysis (LSA, Landauer and Dumais (1997) utilizes precisely this distributional information to uncover hidden semantic factors by means of dimensionality reduction. Singular value decomposition (SVD, Berry et al. (1994)) is applied to a word-document co-occurrence matrix which is factored into a product of a number of other matrices; one of them represents words in terms of the semantic factors and another represents documents in terms of the same factors. The algebraic relation between these matrices can be used to show that any document vector is a linear combination of the vectors representing the words it contains. Thus, within </context>
<context position="15254" citStr="Landauer and Dumais, 1997" startWordPosition="2392" endWordPosition="2395"> and Lapata (2008) we introduce a general framework for studying vector composition, which we formulate as a function f of two vectors u and v: h = f (u,v) (1) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Our earlier work (Mitchell and Lapata, 2008) explored two broad classes of models based on additive and multiplicative functions. Additive models are the most common method of vector combination in the literature. They have been applied to a wide variety of tasks including document coherence (Foltz et al., 1998), essay grading (Landauer and Dumais, 1997), modeling selectional restrictions (Kintsch, 2001), and notably language modeling (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007): hi = ui + vi (2) Vector addition (or averaging, which is equivalent under the cosine similarity measure) is a computationally efficient composition model as it does not increase the dimensionality of the resulting vector. However, the idea of averaging is somewhat counterintuitive from a linguistic perspective. Composition of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elem</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Lowe</author>
</authors>
<title>Topographic Maps of Semantic Space.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="6605" citStr="Lowe, 2000" startWordPosition="1022" endWordPosition="1023">se models provide a simple procedure for constructing spatial representations of word meaning. Topic models, in contrast, impose a probabilistic model onto those distributional statistics, under the assumption that hidden topic variables drive the process that generates words. Both approaches represent the meanings of words in terms of an n-dimensional series of values, but whereas the semantic space model treats those values as defining a vector with spatial properties, the topic model treats them as a probability distribution. A simple and popular (McDonald, 2000; Bullinaria and Levy, 2007; Lowe, 2000) way to construct a semantic space model is to associate each vector component with a particular context word, and assign it a value based on the strength of its co-occurrence with the target (i.e., the word for which a semantic representation is being constructed). For example, in Mitchell and Lapata (2008) we used the 2,000 most frequent content words in a corpus as their contexts, and defined co-occurrence in terms of the context word being present in a five word window on either side of the target word. We calculated the ratio of the probability of the context word given the target word to</context>
</contexts>
<marker>Lowe, 2000</marker>
<rawString>Will Lowe. 2000. Topographic Maps of Semantic Space. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McDonald</author>
</authors>
<title>Environmental Determinants of Lexical Processing Effort.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="6565" citStr="McDonald, 2000" startWordPosition="1015" endWordPosition="1016">statistics of those words. Essentially, these models provide a simple procedure for constructing spatial representations of word meaning. Topic models, in contrast, impose a probabilistic model onto those distributional statistics, under the assumption that hidden topic variables drive the process that generates words. Both approaches represent the meanings of words in terms of an n-dimensional series of values, but whereas the semantic space model treats those values as defining a vector with spatial properties, the topic model treats them as a probability distribution. A simple and popular (McDonald, 2000; Bullinaria and Levy, 2007; Lowe, 2000) way to construct a semantic space model is to associate each vector component with a particular context word, and assign it a value based on the strength of its co-occurrence with the target (i.e., the word for which a semantic representation is being constructed). For example, in Mitchell and Lapata (2008) we used the 2,000 most frequent content words in a corpus as their contexts, and defined co-occurrence in terms of the context word being present in a five word window on either side of the target word. We calculated the ratio of the probability of t</context>
</contexts>
<marker>McDonald, 2000</marker>
<rawString>Scott McDonald. 2000. Environmental Determinants of Lexical Processing Effort. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="4254" citStr="Mitchell and Lapata, 2008" startWordPosition="657" endWordPosition="660">theory (Partee, 1995; Montague, 1974) has been devoted to compositionality, the process of determining the meaning of complex expressions from simpler ones. Previous work either ignores this issue (e.g., Bellegarda (2000)) or simply com430 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430–439, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP putes the centroid of the vectors representing the history (e.g., Coccaro and Jurafsky (1998)). This is motivated primarily by mathematical convenience rather than by empirical evidence. In our earlier work (Mitchell and Lapata, 2008) we formulated composition as a function of two vectors and introduced a variety of models based on addition and multiplication. In this paper we apply vector composition to the problem of constructing predictive history representations for language modeling. Besides integrating composition with language modeling, a task which is novel to our knowledge, our approach also serves as a valuable testbed of our earlier framework which we originally evaluated on a small scale verb-subject similarity task. We also investigate how the choice of the underlying semantic representation interacts with the</context>
<context position="6914" citStr="Mitchell and Lapata (2008)" startWordPosition="1073" endWordPosition="1076">represent the meanings of words in terms of an n-dimensional series of values, but whereas the semantic space model treats those values as defining a vector with spatial properties, the topic model treats them as a probability distribution. A simple and popular (McDonald, 2000; Bullinaria and Levy, 2007; Lowe, 2000) way to construct a semantic space model is to associate each vector component with a particular context word, and assign it a value based on the strength of its co-occurrence with the target (i.e., the word for which a semantic representation is being constructed). For example, in Mitchell and Lapata (2008) we used the 2,000 most frequent content words in a corpus as their contexts, and defined co-occurrence in terms of the context word being present in a five word window on either side of the target word. We calculated the ratio of the probability of the context word given the target word to the overall probability of the context word and use these values as their vector components. This procedure has the benefits of simplicity and also of being largely free of any additional theoretical assumptions over and above the distributional approach to semantics. This is not to say that more sophistica</context>
<context position="14646" citStr="Mitchell and Lapata (2008)" startWordPosition="2286" endWordPosition="2289"> Widdows, 2008) as a means of binding one vector to another face major computational difficulties as their dimensionality grows exponentially with the number of constituents being composed. To overcome this problem, other techniques (Plate, 1995) have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components. Crucially, the success of these methods depends on the assumption that the vector components are randomly distributed. This is problematic for modeling language which has regular structure. Given the above considerations, in Mitchell and Lapata (2008) we introduce a general framework for studying vector composition, which we formulate as a function f of two vectors u and v: h = f (u,v) (1) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Our earlier work (Mitchell and Lapata, 2008) explored two broad classes of models based on additive and multiplicative functions. Additive models are the most common method of vector combination in the literature. They have been applied to a wide variety of tasks including document coherence (Foltz et al., 1998), essay grading (Landauer and Dumai</context>
<context position="15904" citStr="Mitchell and Lapata (2008)" startWordPosition="2490" endWordPosition="2493">estrictions (Kintsch, 2001), and notably language modeling (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007): hi = ui + vi (2) Vector addition (or averaging, which is equivalent under the cosine similarity measure) is a computationally efficient composition model as it does not increase the dimensionality of the resulting vector. However, the idea of averaging is somewhat counterintuitive from a linguistic perspective. Composition of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elements (Pinker, 1994). In Mitchell and Lapata (2008) we argue that composition models based on multiplication address this problem: hi = ui · vi (3) Whereas the addition of vectors ‘lumps their content together’, multiplication picks out the content relevant to their combination by scaling each component of one with the strength of the corresponding component of the other. This argument is appealing, especially if one is interested in explaining how the meaning of a verb is modulated by its subject. Here, we also develop a complementary, probabilistic argument for the validity of this model. Let us assume that semantic vectors are based on comp</context>
<context position="18890" citStr="Mitchell and Lapata, 2008" startWordPosition="3001" endWordPosition="3004">1 p(ci|x) = 2 p(ci|w1)+ 2 p(ci|w2) (8) Therefore, we could represent x with a vector, based on these probabilities, having the components: 1 p(ci|w2) (9) 2 p(ci) Which is exactly the vector averaging approach to semantic composition. As more vectors are combined, vector addition will lead to greater generality rather than greater specificity. The multiplicative approach, on the other hand, picks out the components of the constituents that are relevant to the combination, and represents more faithfully the properties of their conjunction. As an aside, we should point out that our earlier work (Mitchell and Lapata, 2008) introduced several other models, additive and multiplicative, besides the ones discussed here. We selected the additive model as a baseline and also due to its overwhelming popularity in the language modeling literature. The multiplicative model presented above performed best in our evaluation study (i.e., predicting verb-subject similarity). 4 Language Modeling Estimating Probabilities In language modeling our aim is to derive probabilities, p(w|h), given the semantic representations of word, w, and its history, h, based on the assumption that probable words should be semantically coherent w</context>
<context position="21300" citStr="Mitchell and Lapata (2008)" startWordPosition="3414" endWordPosition="3417">vector components are given by equation (4), the dot product becomes: p(ci|h) (12) p(ci) which we modify to derive probabilities as follows: p(ci|h)p(ci) (13) p(ci) This expression now weights the sum with the independent probabilities of the context words and the word to be predicted. That this is indeed a valid probability can be seen by the fact it is equivalent to Ei p(w|ci)p(ci|h). However, in constructing a representation of the history h, it is more convenient to work with equation (13) as it is based on vector components and can be readily used with the composition models presented in Mitchell and Lapata (2008). Equation (13) allows us to derive probabilities from vectors representing a word and its prior history. We must also construct a representation of 1 p(ci|w1) + 2 p(ci) xi = w·h =E p(ci|w) i p(ci) p(w|h) = p(w)E p(ci|w) i p(ci) 434 the history up to the nth word of a sentence. To do this, we combine, via some (additive or multiplicative) function f, the vector representing that word with the vector representing the history up to n−1 words: hn = f(wn,hn−1) (14) h1 = w1 (15) One issue that must be resolved in implementing equation (14) is that the history vector should remain correctly normaliz</context>
<context position="27993" citStr="Mitchell and Lapata (2008)" startWordPosition="4496" endWordPosition="4499">lexity is the reciprocal of the geometric average of the word probabilities and a lower score indicates better predictions. Parameter Estimation The compositional language models were trained on the BLLIP corpus, a collection of texts from the Wall Street Journal (years 1987–89). The training corpus consisted of 38,521,346 words. We used a development corpus of 50,006 words and a test corpus of similar size. All words were converted to lowercase and numbers were replaced with the symbol (num). A vocabulary of 20,000 words was chosen and the remaining tokens were replaced with (unk). Following Mitchell and Lapata (2008), we constructed a simple semantic space based on cooccurrence statistics from the BLLIP training set. We used the 2,000 most frequent word types as contexts and a symmetric five word window. Vector components were defined as in equation (4). Contrary to our earlier work, we did not lemmatize the corpus before constructing the vectors as in the context of language modeling this was not appropriate. We also trained the LDA model on BLLIP, using Blei et al.’s (2003) implementation.3 We experimented with different numbers of topics on the development set (from 10 to 200) and report results on the</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>English as a formal language. In</title>
<date>1974</date>
<editor>R. Montague, editor, Formal Philosophy.</editor>
<publisher>Yale University Press,</publisher>
<location>New Haven, CT.</location>
<contexts>
<context position="3665" citStr="Montague, 1974" startWordPosition="573" endWordPosition="574">his case, are vectors derived from the distributional properties of words in a corpus, based on the insight that words which are semantically similar will be found in similar contexts (Harris, 1968; Firth, 1957). Although the the construction of a semantic representation for the history is crucial to this approach, the underlying vector-based models are primarily designed to represent isolated words rather than word sequences. Ideally, we would like to compose the meaning of the history out of its constituent parts. This is by no means a new idea. Much work in linguistic theory (Partee, 1995; Montague, 1974) has been devoted to compositionality, the process of determining the meaning of complex expressions from simpler ones. Previous work either ignores this issue (e.g., Bellegarda (2000)) or simply com430 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430–439, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP putes the centroid of the vectors representing the history (e.g., Coccaro and Jurafsky (1998)). This is motivated primarily by mathematical convenience rather than by empirical evidence. In our earlier work (Mitchell and Lapata, 2008) we formula</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>R. Montague. 1974. English as a formal language. In R. Montague, editor, Formal Philosophy. Yale University Press, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Partee</author>
</authors>
<title>Lexical semantics and compositionality.</title>
<date>1995</date>
<booktitle>In Lila Gleitman and Mark Liberman, editors, Invitation to Cognitive Science Part I: Language,</booktitle>
<pages>311--360</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3648" citStr="Partee, 1995" startWordPosition="571" endWordPosition="572">ntations, in this case, are vectors derived from the distributional properties of words in a corpus, based on the insight that words which are semantically similar will be found in similar contexts (Harris, 1968; Firth, 1957). Although the the construction of a semantic representation for the history is crucial to this approach, the underlying vector-based models are primarily designed to represent isolated words rather than word sequences. Ideally, we would like to compose the meaning of the history out of its constituent parts. This is by no means a new idea. Much work in linguistic theory (Partee, 1995; Montague, 1974) has been devoted to compositionality, the process of determining the meaning of complex expressions from simpler ones. Previous work either ignores this issue (e.g., Bellegarda (2000)) or simply com430 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430–439, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP putes the centroid of the vectors representing the history (e.g., Coccaro and Jurafsky (1998)). This is motivated primarily by mathematical convenience rather than by empirical evidence. In our earlier work (Mitchell and Lapata,</context>
</contexts>
<marker>Partee, 1995</marker>
<rawString>B. Partee. 1995. Lexical semantics and compositionality. In Lila Gleitman and Mark Liberman, editors, Invitation to Cognitive Science Part I: Language, pages 311–360. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>The Language Instinct: How the Mind Creates Language. HarperCollins,</title>
<date>1994</date>
<location>New York.</location>
<contexts>
<context position="15873" citStr="Pinker, 1994" startWordPosition="2487" endWordPosition="2488">ling selectional restrictions (Kintsch, 2001), and notably language modeling (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007): hi = ui + vi (2) Vector addition (or averaging, which is equivalent under the cosine similarity measure) is a computationally efficient composition model as it does not increase the dimensionality of the resulting vector. However, the idea of averaging is somewhat counterintuitive from a linguistic perspective. Composition of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elements (Pinker, 1994). In Mitchell and Lapata (2008) we argue that composition models based on multiplication address this problem: hi = ui · vi (3) Whereas the addition of vectors ‘lumps their content together’, multiplication picks out the content relevant to their combination by scaling each component of one with the strength of the corresponding component of the other. This argument is appealing, especially if one is interested in explaining how the meaning of a verb is modulated by its subject. Here, we also develop a complementary, probabilistic argument for the validity of this model. Let us assume that sem</context>
</contexts>
<marker>Pinker, 1994</marker>
<rawString>S. Pinker. 1994. The Language Instinct: How the Mind Creates Language. HarperCollins, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony A Plate</author>
</authors>
<title>Holographic reduced representations.</title>
<date>1995</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="14266" citStr="Plate, 1995" startWordPosition="2229" endWordPosition="2230">d their syntactic annotations (Kanejiya et al., 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003). 3 Composition Models The problem of vector composition has received relatively little attention within natural language processing. Attempts to use tensor products (Smolensky, 1990; Clark et al., 2008; Widdows, 2008) as a means of binding one vector to another face major computational difficulties as their dimensionality grows exponentially with the number of constituents being composed. To overcome this problem, other techniques (Plate, 1995) have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components. Crucially, the success of these methods depends on the assumption that the vector components are randomly distributed. This is problematic for modeling language which has regular structure. Given the above considerations, in Mitchell and Lapata (2008) we introduce a general framework for studying vector composition, which we formulate as a function f of two vectors u and v: h = f (u,v) (1) where h denotes the composition of u and v. Different composition models arise</context>
</contexts>
<marker>Plate, 1995</marker>
<rawString>Tony A. Plate. 1995. Holographic reduced representations. IEEE Transactions on Neural Networks, 6(3):623–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="2534" citStr="Roark (2001)" startWordPosition="390" endWordPosition="391">nge dependencies, such as syntactic relationships, semantic or thematic constraints. The literature offers many examples of how to overcome this limitation, essentially by allowing the modulation of probabilities by dependencies which extend to words beyond the n-gram horizon. Cache language models (Kuhn and de Mori, 1992) increase the probability of words observed in the history, e.g., by some factor which decays exponentially with distance. Trigger models (Rosenfeld, 1996) go a step further by allowing arbitrary word pairs to be incorporated into the cache. Structured language models (e.g., Roark (2001)) go beyond the representation of history as a linear sequence of words to capture the syntactic constructions in which these words are embedded. It is also possible to build representations of history which are semantic rather than syntactic (Bellegarda (2000; Coccaro and Jurafsky (1998; Gildea and Hofmann (1999)). In this approach, estimates for the probabilities of upcoming words are derived from a comparison of their semantic content with the content of the history so far. The semantic representations, in this case, are vectors derived from the distributional properties of words in a corpu</context>
<context position="23521" citStr="Roark, 2001" startWordPosition="3780" endWordPosition="3781">= (17) p(topici) Integrating with Other Language Models The models defined above are based on little more than semantic coherence. As such they will be only weakly predictive, since they largely ignore word order, which n-gram models primarily exploit. The simplest means to integrate semantic information with a standard language model involves combining two probability estimates as a weighted sum: p(w|h) = λ1p1(w|h) + (1− λ)p2(w|h) (18) Linear interpolation is guaranteed to produce valid probabilities, and has been used, for example, to integrate structured language models with n-gram models (Roark, 2001). However, it will work best when the models being combined are roughly equally predictive and have complementary strengths and weaknesses. If one model is much weaker than the other, linear interpolation will typically produce a model of intermediate strength (i.e., worse than the better model), with the weaker model contributing a form of smoothing at best. Therefore, based on equation (13), we express our semantic probabilities as the product of the unigram probability, p(w), and a semantic component, Δ, which determines the factor by which this probability should be scaled up or down given</context>
<context position="24933" citStr="Roark, 2001" startWordPosition="4009" endWordPosition="4010">ons.2 ˆp(wn) = p(wn|wn−1 n−2)·Δ(wn,h) (21) To obtain a true probability estimate we normalize ˆp(wn) by dividing through the sum of all word probabilities: p(wn|wn−1 n−2,h) = ∑w ˆp(w) ˆp(wn) (22) In integrating our semantic model with an n-gram model, we allow the latter to handle short range dependencies and have the former handle the longer dependencies outside the n-gram window. For this reason, the history h used by the semantic model in the prediction of wn only includes words up to wn−3 (i.e., only words outside the n-gram). We also integrate our models with a structured language model (Roark, 2001). However, in this case we use linear interpolation (equation (18)) because the models are roughly equally predictive and also because linear interpolation is widely used when structured language models are combined with n-grams and other information sources. This approach also has the benefit of allowing the 2Equation (21) can also be expressed as p(wn|wn−1 n−2,h) ≈ p(wn|wn−1 p(wn) , Which is equivalent to assuming that h is n−2)p(wn|h) conditionally independent of wn−1 n−2 (Gildea and Hofmann, 1999). ˆhi (16) Δ(w,h) = ∑ p(ci|w) i p(ci) 435 models to be combined without out the need to renorm</context>
<context position="29222" citStr="Roark, 2001" startWordPosition="4701" endWordPosition="4702"> topics. In our experiments, the hyperparameter α was initialized to 0.5, and the b word probabilities were initialized randomly. We integrated our compositional models with a trigram model which we also trained on BLLIP. The model was built using the SRILM toolkit (Stolcke, 2002) with backoff and Good-Turing smoothing. Ideally, we would have liked to train Roark’s (2001) parser on the same data as that used for the semantic models. However, this would require a gold standard treebank several times larger than those currently available. Following previous work on structured language modeling (Roark, 2001; Charniak, 2001; Chelba and Jelinek, 1998), we therefore trained the parser on sections 2–21 of the Penn Treebank containing 936,017 words. Note that Roark’s (2001) parser produces prefix probabilities for each word of a sentence which we converted to conditional probabilities by dividing each current probability by the previous one. 6 Results Table 1 shows perplexity results when the compositional models are combined with an n-gram model. With regard to the simple semantic space model (SSM) we observe that both additive and multiplicative approaches to constructing history are successful in </context>
<context position="31412" citStr="Roark (2001)" startWordPosition="5025" endWordPosition="5026">ms worse than the n-gram with a multiplicative function. For comparison, Figure 1 plots the perplexity of the combined LDA and n-gram models against the number of topics. Increasing the number of topics produces higher dimensional representations which ought to be richer, more detailed and therefore more predictive. While this is true for the additive model, a greater number of topics actually increases the perplexity of the multiplicative model, indicating it has become less predictive. We compared these perplexity reductions against those obtained with a structured language model. Following Roark (2001), we combined the structured language model with a trigram model using linear interpolation (the weights were optimized on the development set). This model (n-gram + parser) performs comparably to our best compositional model (n-gram + MultiplySSM). While both models incorporate long range dependencies, the parser is trained on a hand annotated treebank, whereas the compositional model uses raw text, albeit from a larger corpus. Interestingly, when interpolating the trigram with the parser and the compositional models, we obtain additional perplexity reductions. This suggests that the semantic</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling. Computer Speech and Language,</title>
<date>1996</date>
<pages>10--187</pages>
<contexts>
<context position="2401" citStr="Rosenfeld, 1996" startWordPosition="367" endWordPosition="369">eters. However, low values of n impose an artificially local horizon to the language model, and compromise its ability to capture long-range dependencies, such as syntactic relationships, semantic or thematic constraints. The literature offers many examples of how to overcome this limitation, essentially by allowing the modulation of probabilities by dependencies which extend to words beyond the n-gram horizon. Cache language models (Kuhn and de Mori, 1992) increase the probability of words observed in the history, e.g., by some factor which decays exponentially with distance. Trigger models (Rosenfeld, 1996) go a step further by allowing arbitrary word pairs to be incorporated into the cache. Structured language models (e.g., Roark (2001)) go beyond the representation of history as a linear sequence of words to capture the syntactic constructions in which these words are embedded. It is also possible to build representations of history which are semantic rather than syntactic (Bellegarda (2000; Coccaro and Jurafsky (1998; Gildea and Hofmann (1999)). In this approach, estimates for the probabilities of upcoming words are derived from a comparison of their semantic content with the content of the h</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Roni Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer Speech and Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--159</pages>
<contexts>
<context position="13999" citStr="Smolensky, 1990" startWordPosition="2188" endWordPosition="2189">els sketched above involve representing the history by multiple LSA models of varying granularity in an attempt to capture topic, subtopic, and local information (Zhang and Rudnicky, 2002); incorporating syntactic information by building the semantic space over words and their syntactic annotations (Kanejiya et al., 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003). 3 Composition Models The problem of vector composition has received relatively little attention within natural language processing. Attempts to use tensor products (Smolensky, 1990; Clark et al., 2008; Widdows, 2008) as a means of binding one vector to another face major computational difficulties as their dimensionality grows exponentially with the number of constituents being composed. To overcome this problem, other techniques (Plate, 1995) have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components. Crucially, the success of these methods depends on the assumption that the vector components are randomly distributed. This is problematic for modeling language which has regular structure. Given the abov</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Paul Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46:159–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="28892" citStr="Stolcke, 2002" startWordPosition="4651" endWordPosition="4652">emmatize the corpus before constructing the vectors as in the context of language modeling this was not appropriate. We also trained the LDA model on BLLIP, using Blei et al.’s (2003) implementation.3 We experimented with different numbers of topics on the development set (from 10 to 200) and report results on the test set with 100 topics. In our experiments, the hyperparameter α was initialized to 0.5, and the b word probabilities were initialized randomly. We integrated our compositional models with a trigram model which we also trained on BLLIP. The model was built using the SRILM toolkit (Stolcke, 2002) with backoff and Good-Turing smoothing. Ideally, we would have liked to train Roark’s (2001) parser on the same data as that used for the semantic models. However, this would require a gold standard treebank several times larger than those currently available. Following previous work on structured language modeling (Roark, 2001; Charniak, 2001; Chelba and Jelinek, 1998), we therefore trained the parser on sections 2–21 of the Penn Treebank containing 936,017 words. Note that Roark’s (2001) parser produces prefix probabilities for each word of a sentence which we converted to conditional proba</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tonio Wandmacher</author>
<author>Jean-Yves Antoine</author>
</authors>
<title>Methods to integrate a language model with semantic information for a word prediction component.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>506--513</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12171" citStr="Wandmacher and Antoine, 2007" startWordPosition="1910" endWordPosition="1913">sumption that words are generated independently means the interaction between them is not modeled. 2.2 Language Modeling using Semantic Representations A common approach to embedding semantic representations within language modeling is to measure the semantic similarity between an upcoming word and its history and use it to modify the probabilities from an n-gram model. In this way, the n-gram’s sensitivity to short-range dependencies is enriched with information about longer-range semantic coherence. Much of previous work has taken this approach (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007), whilst relying on LSA to provide semantic representations for individual words. Some authors (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007) use the geometric notion of a vector centroid to construct representations of history, whereas others (Bellegarda, 2000; Deng and Khundanpur, 2003) use the idea of a “pseudodocument”, which is derived from the algebraic relation between documents and words assumed within LSA. They all derive P(wi|hi), the probability of an upcoming word given its history, from the cosine similarity measure which must be somehow normalized in order to yield we</context>
<context position="15395" citStr="Wandmacher and Antoine, 2007" startWordPosition="2410" endWordPosition="2413">nd v: h = f (u,v) (1) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Our earlier work (Mitchell and Lapata, 2008) explored two broad classes of models based on additive and multiplicative functions. Additive models are the most common method of vector combination in the literature. They have been applied to a wide variety of tasks including document coherence (Foltz et al., 1998), essay grading (Landauer and Dumais, 1997), modeling selectional restrictions (Kintsch, 2001), and notably language modeling (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007): hi = ui + vi (2) Vector addition (or averaging, which is equivalent under the cosine similarity measure) is a computationally efficient composition model as it does not increase the dimensionality of the resulting vector. However, the idea of averaging is somewhat counterintuitive from a linguistic perspective. Composition of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elements (Pinker, 1994). In Mitchell and Lapata (2008) we argue that composition models based on multiplication address this problem: hi = ui · v</context>
</contexts>
<marker>Wandmacher, Antoine, 2007</marker>
<rawString>Tonio Wandmacher and Jean-Yves Antoine. 2007. Methods to integrate a language model with semantic information for a word prediction component. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 506–513, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd Symposium on Quantum Interaction,</booktitle>
<publisher>UK. College Publications.</publisher>
<location>Oxford,</location>
<contexts>
<context position="14035" citStr="Widdows, 2008" startWordPosition="2194" endWordPosition="2195">ng the history by multiple LSA models of varying granularity in an attempt to capture topic, subtopic, and local information (Zhang and Rudnicky, 2002); incorporating syntactic information by building the semantic space over words and their syntactic annotations (Kanejiya et al., 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003). 3 Composition Models The problem of vector composition has received relatively little attention within natural language processing. Attempts to use tensor products (Smolensky, 1990; Clark et al., 2008; Widdows, 2008) as a means of binding one vector to another face major computational difficulties as their dimensionality grows exponentially with the number of constituents being composed. To overcome this problem, other techniques (Plate, 1995) have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components. Crucially, the success of these methods depends on the assumption that the vector components are randomly distributed. This is problematic for modeling language which has regular structure. Given the above considerations, in Mitchell and La</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the 2nd Symposium on Quantum Interaction, Oxford, UK. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Zhang</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Improve latent semantic analysis based language model by integrating multiple level knowldege.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing,</booktitle>
<pages>893--897</pages>
<location>Denver, CO.</location>
<contexts>
<context position="13572" citStr="Zhang and Rudnicky, 2002" startWordPosition="2122" endWordPosition="2125">listic interpretation. As a result, the probability of an upcoming word given the history can be derived naturally and directly, avoiding the need for ad-hoc transformations. In constructing their representation of history, Gildea and Hofmann (1999) use an online Expectation Maximization process, which derives from the probabilistic basis of pLSA, to update the history with new words. Extensions on the basic semantic language 432 models sketched above involve representing the history by multiple LSA models of varying granularity in an attempt to capture topic, subtopic, and local information (Zhang and Rudnicky, 2002); incorporating syntactic information by building the semantic space over words and their syntactic annotations (Kanejiya et al., 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003). 3 Composition Models The problem of vector composition has received relatively little attention within natural language processing. Attempts to use tensor products (Smolensky, 1990; Clark et al., 2008; Widdows, 2008) as a means of binding one vector to another face major computational difficulties as their dimensionality grows exponentially with the n</context>
</contexts>
<marker>Zhang, Rudnicky, 2002</marker>
<rawString>Rong Zhang and Alexander I. Rudnicky. 2002. Improve latent semantic analysis based language model by integrating multiple level knowldege. In Proceedings of the 7th International Conference on Spoken Language Processing, pages 893–897, Denver, CO.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>