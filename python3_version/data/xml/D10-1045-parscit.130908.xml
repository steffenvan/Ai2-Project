<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000688">
<title confidence="0.957034">
NLP on Spoken Documents without ASR
</title>
<author confidence="0.999249">
Mark Dredze, Aren Jansen, Glen Coppersmith, Ken Church
</author>
<affiliation confidence="0.954161">
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<email confidence="0.998802">
mdredze,aren,coppersmith,Kenneth.Church@jhu.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999507333333333">
There is considerable interest in interdis-
ciplinary combinations of automatic speech
recognition (ASR), machine learning, natu-
ral language processing, text classification and
information retrieval. Many of these boxes,
especially ASR, are often based on consid-
erable linguistic resources. We would like
to be able to process spoken documents with
few (if any) resources. Moreover, connect-
ing black boxes in series tends to multiply er-
rors, especially when the key terms are out-of-
vocabulary (OOV). The proposed alternative
applies text processing directly to the speech
without a dependency on ASR. The method
finds long (— 1 sec) repetitions in speech,
and clusters them into pseudo-terms (roughly
phrases). Document clustering and classi-
fication work surprisingly well on pseudo-
terms; performance on a Switchboard task ap-
proaches a baseline using gold standard man-
ual transcriptions.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999795622222222">
Can we do IR-like tasks without ASR? Information
retrieval (IR) typically makes use of simple features
that count terms within/across documents such as
term frequency (tf) and inverse document frequency
(IDF). Crucially, to compute these features, it is suf-
ficient to count repetitions of a term. In particular,
for many IR-like tasks, there is no need for an au-
tomatic speech recognition (ASR) system to label
terms with phonemes and/or words.
This paper builds on Jansen et al. (2010), a
method for discovering terms with zero resources.
This approach identifies long, faithfully repeated
patterns in the acoustic signal. These acoustic repe-
titions often correspond to terms useful for informa-
tion retrieval tasks. Critically, this method does not
require a phonetically interpretable acoustic model
or knowledge of the target language.
By analyzing a large untranscribed corpus of
speech, this discovery procedure identifies a vast
number of repeated regions that are subsequently
grouped using a simple graph-based clustering
method. We call the resulting groups pseudo-terms
since they typically represent a single word or phrase
spoken at multiple points throughout the corpus.
Each pseudo-term takes the place of a word or
phrase in bag of terms vector space model of a text
document, allowing us to apply standard NLP algo-
rithms. We show that despite the fully automated
and noisy method by which the pseudo-terms are
created, we can still successfully apply NLP algo-
rithms with performance approaching that achieved
with the gold standard manual transcription.
Natural language processing tools can play a key
role in understanding text document collections.
Given a large collection of text, NLP tools can clas-
sify documents by category (classification) and or-
ganize documents into similar groups for a high
level view of the collection (clustering). For exam-
ple, given a collection of news articles, these tools
can be applied so that the user can quickly see the
topics covered in the news articles, and organize the
collection to find all articles on a given topic. These
tools require little or no human input (annotation)
and work across languages.
Given a large collection of speech, we would like
</bodyText>
<page confidence="0.987524">
460
</page>
<note confidence="0.8176575">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 460–470,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99996359375">
tools that perform many of the same tasks, allow-
ing the user to understand the contents of the col-
lection while listening to only small portions of the
audio. Previous work has applied these NLP tools
to speech corpora with similar results (see Hazen
et al. (2007) and the references therein.) However,
unlike text, which requires little or no preprocess-
ing, audio files are typically first transcribed into
text before applying standard NLP tools. Automatic
speech recognition (ASR) solutions, such as large
vocabulary continuous speech recognition (LVCSR)
systems, can produce an automatic transcript from
speech, but they require significant development ef-
forts and training resources, typically hundreds of
hours of manually transcribed speech. Moreover,
the terms that may be most distinctive in particular
spoken documents often lie outside the predefined
vocabulary of an off-the-shelf LVCSR system. This
means that unlike with text, where many tools can be
applied to new languages and domains with minimal
effort, the equivalent tools for speech corpora often
require a significant investment. This greatly raises
the entry threshold for constructing even a minimal
tool set for speech corpora analysis.
The paper proceeds as follows. After a review
of related work, we describe Jansen et al. (2010),
a method for finding repetitions in speech. We
then explain how these repetitions are grouped into
pseudo-terms. Document clustering and classifica-
tion work surprisingly well on pseudo-terms; perfor-
mance on a Switchboard task approaches a baseline
based on gold standard manual transcriptions.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999989159090909">
In the low resource speech recognition regime,
most approaches have focused on coupling small
amounts of orthographically transcribed speech (10s
of hours) with much larger collections of untran-
scribed speech (100s or 1000s of hours) to train ac-
curate acoustic models with semi-supervised meth-
ods (Novotney and Schwartz, 2009). In these ef-
forts, the goal is to reduce the annotation require-
ments for the construction of competent LVCSR sys-
tems. This semi-supervised paradigm was relaxed
even further with the pursuit of self organizing units
(SOUs), phone-like units for which acoustic mod-
els are trained with completely unsupervised meth-
ods (Garcia and Gish, 2006). Even though the move
away from phonetic acoustic models improves the
universality of the architecture, small amounts of or-
thographic transcription are still required to connect
the SOUs with the lexicon.
The segmental dynamic time warping (S-DTW)
algorithm (Park and Glass, 2008) was the first truly
zero resource effort, designed to discover portions of
the lexicon directly by searching for repeated acous-
tic patterns in the speech signal. This work im-
plicitly defined a new direction for speech process-
ing research: unsupervised spoken term discovery,
the entry point of our speech corpora analysis sys-
tem. Subsequent extensions of S-DTW (Jansen et
al., 2010) permit applications to much larger speech
collections, a flexibility that is vital to our efforts.
As mentioned above, the application of NLP
methods to speech corpora have traditionally relied
on high resource ASR systems to provide automatic
word or phonetic transcripts. Spoken document
topic classification has been an application of partic-
ular interest (Hazen et al., 2007), for which the rec-
ognized words or phone n-grams are used to charac-
terize the documents. These efforts have produced
admirable results, with ASR transcript-based per-
formance approached that obtained using the gold
standard manual transcripts. Early efforts to per-
form automatic topic segmentation of speech input
without the aid of ASR systems have been promis-
ing (Malioutov et al., 2007), but have yet to exploit
the full the range of NLP tools.
</bodyText>
<sectionHeader confidence="0.989354" genericHeader="method">
3 Identifying Matched Regions
</sectionHeader>
<bodyText confidence="0.9999547">
Our goal is to identify pairs of intervals within and
across utterances of several speakers that contain
the same linguistic content, preferably meaningful
words or terms.
The spoken term discovery algorithm of Jansen et
al. (2010) efficiently searches the space of (2) in-
tervals, where n is the number of speech frames.1
Jansen et al. (2010) is based on dotplots (Church and
Helfman, 1993), a method borrowed from bioinfor-
matics for finding repetitions in DNA sequences.
</bodyText>
<footnote confidence="0.761225">
1Typically, each frame represents a 25 or 30 ms window of
speech sampled every 10 ms
</footnote>
<page confidence="0.999247">
461
</page>
<figure confidence="0.999962765957447">
Time (s)
g
n
i
s
s
e
c
o
r
p
h
c
e
e
p
s
.
s
v
g
n
i
s
s
e
c
o
r
p
t
x
e
t
text processing v s . speech processing
8 1
7 0.95
6 0.9
5 0.85
4 0.8
3 0.75
2 0.7
1 0.65
0.6
0.55
1 2 3 4 5 6 7 8
Time (s)
</figure>
<figureCaption confidence="0.992990666666667">
Figure 1: An example of a dotplot for the string “text
processing vs. speech processing” plotted against itself.
The box calls out the repeated substring: “processing.”
</figureCaption>
<subsectionHeader confidence="0.997489">
3.1 Acoustic Dotplots
</subsectionHeader>
<bodyText confidence="0.999448">
When applied to text, the dotplot construct is re-
markably simple: given character strings s1 and s2,
the dotplot is a Boolean similarity matrix K(s1, s2)
defined as
</bodyText>
<equation confidence="0.813463">
Kij(s1, s2) = δ(s1[ib s2[jD.
</equation>
<bodyText confidence="0.999819565217392">
Substrings common to s1 and s2 manifest them-
selves as diagonal line segments in the visualization
of K. Figure 1 shows an example text dotplot where
both s1 and s2 are taken to be the string “text pro-
cessing vs. speech processing.” The boxed diago-
nal line segment arises from the repeat of the word
“processing,” while the main diagonal line trivially
arises from self-similarity. Thus, the search for line
segments in K off the main diagonal provides a sim-
ple algorithmic means to identify repeated terms of
possible interest, albeit sometimes partial, in a col-
lection of text documents. The challenge is to gen-
eralize these dotplot techniques for application to
speech, an inherently noisy, real-valued data stream.
The strategy is to replace character strings with
frame-based speech representations of the form
x = x1, x2, ... xN, where each xi E Rd is a d-
dimensional vector space representation of the ith
overlapping window of the signal. Given vector time
series x = x1, x2,... xN and y = y1, y2,... yM for
two spoken documents, the acoustic dotplot is the
real-valued N xM cosine similarity matrix K(x, y)
defined as
</bodyText>
<figureCaption confidence="0.985191666666667">
Figure 2: An example of an acoustic dotplot for 8 seconds
of speech (posteriorgrams) plotted against itself. The box
calls out a repetition of interest.
</figureCaption>
<equation confidence="0.995764">
� Kij (x, y) = 1 (xi,yj )
2 1 + ��xi �� ��yj �� (1)
</equation>
<bodyText confidence="0.999993">
Even though the application to speech is a distinctly
noisier endeavor, sequences of frames repeated be-
tween the two audio clips will still produce approx-
imate diagonal lines in the visualization of the ma-
trix. The search for matched regions thus reduces
to the robust search for diagonal line segments in
K, which can be efficiently performed with standard
image processing techniques.
Included in this procedure is the application of a
diagonal median filter of duration κ seconds. The
choice of κ determines an approximate threshold
on the duration of the matched regions discovered.
Large κ values (— 1 sec) will produce a relatively
sparse list of matches corresponding to long words
or short phrases; smaller κ values (&lt; 0.5 sec)
will admit shorter words and syllables that may be
less informative from a document analysis perspec-
tive. Given the approximate nature of the procedure,
shorter κ values also admit less reliable matches.
</bodyText>
<subsectionHeader confidence="0.999625">
3.2 Posteriorgram Representation
</subsectionHeader>
<bodyText confidence="0.999933333333333">
The acoustic dotplot technique can operate on any
vector time series representation of the speech sig-
nal, including a standard spectrogram. However, at
the individual frame level, the cosine similarities be-
tween frequency spectra of distinct speakers produc-
ing the same phoneme are not guaranteed to be high.
</bodyText>
<page confidence="0.997759">
462
</page>
<figure confidence="0.993689">
Phone
Time (s)
</figure>
<figureCaption confidence="0.999807">
Figure 3: An example of a posteriorgram.
</figureCaption>
<bodyText confidence="0.99634436923077">
Thus, to perform term discovery across a multi-
speaker corpus, we require a speaker-independent
representation. Phonetic posteriorgrams are a suit-
able choice, as each frame is represented as the pos-
terior probability distribution over a set of speech
sounds given the speech observed at the particular
point in time, which is largely speaker-independent
by construction. Figure 3 shows an example poste-
riorgram for the utterance “I had to do that,” com-
puted with a multi-layer perceptron (MLP)-based
English phonetic acoustic model (see Section 5 for
details). Each row of the figure represents the pos-
terior probability of the given phone as a function of
time through the utterance and each column repre-
sents the posterior distribution over the phone set at
that particular point in time.
The construction of speaker independent acous-
tic models typically requires a significant amount of
transcribed speech. Our proposed strategy is to em-
ploy a speaker independent acoustic model trained
in a high resource language or domain to interpret
multi-speaker data in the zero resource target set-
ting.2 Indeed, we do not need to know a language
to detect when a word of sufficient length has been
repeated in it.3 By computing cosine similarities
2A similarly-minded approach was taken in Hazen et al.
(2007) and extended in Hazen and Margolis (2008), where the
authors use Hungarian phonetic trigrams features to character-
ize English spoken documents for a topic classification task.
3While in this paper our acoustic model is based on our eval-
uation corpus, this is not a requirement of our approach. Future
work will investigate performance of other acoustic models.
of phonetic posterior distribution vectors (as op-
posed to reducing the speech to a one-best phonetic
token sequence), the phone set used need not be
matched to the target language. With this approach,
a speaker-independent model trained on the phone
set of a reference language may be used to perform
speaker independent term discovery in any other.
In addition to speaker independence, the use of
phonetic posteriorgrams introduces representational
sparsity that permits efficient dotplot computation
and storage. Notice that the posteriorgram dis-
played in Figure 3 consists of mostly near-zero val-
ues. Since cosine similarity (Equation 1) between
two frames can only be high if they have significant
mass on the same phone, most comparisons need not
be made. Instead, we can apply a threshold and store
each posteriorgram as an inverted file, performing
inner product multiplies and adds only when they
contribute. Using a grid of approximately 100 cores,
we were able to perform the O(n2) dotplot computa-
tion and line segment search for 60+ hours of speech
(corresponding to a 500 terapixel dotplot) in approx-
imately 5 hours.
Figure 2 displays the posteriorgram dotplot for
8 seconds of speech against itself (i.e., x = y).
The prominent main diagonal line results from self-
similarity, and thus is ignored in the search. The
boxed diagonal line segment results from two dis-
tinct occurrences of the term one million dollars.
The large black boxes in the image result from
long stretches of silence of filled pauses; fortunately,
these are easily filtered with speech activity detec-
tion or simple measures of posteriorgram stability.
</bodyText>
<sectionHeader confidence="0.994671" genericHeader="method">
4 Creating Pseudo-Terms
</sectionHeader>
<bodyText confidence="0.999978166666667">
Spoken documents will be represented as bags of
pseudo-terms, where pseudo-terms are computed
from acoustic repetitions described in the previous
section. Let M be a set of matched regions (m),
each consisting of a pair of speech intervals con-
tained in the corpus (m = [t(i)
</bodyText>
<equation confidence="0.995892333333333">
1 , t(i)
2 ], [t(j)
1 , t(j)
</equation>
<bodyText confidence="0.900941">
2 ] indi-
cates the speech from t(i)
1 to t(i) 2is an acoustic match
to the speech from t(j) 1to t2j)). If a particular term
occurs k times, the set M can include as many as (2)
distinct elements corresponding to that term, so we
require a procedure to group them into clusters. We
call the resulting clusters pseudo-terms since each
</bodyText>
<figure confidence="0.99945316981132">
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
zh
z
y
w
v
uw
uh
th
t
sh
s
r
p
oy
ow
ng
n
m
l
k
ih
hh
g
f
ey
er
en
em
el
eh
dh
d
ch
b
ay
axr
ax
aw
ao
ah
ae
aa
sil
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
</figure>
<page confidence="0.999478">
463
</page>
<bodyText confidence="0.999832769230769">
cluster is a placeholder for a term (word or phrase)
spoken in the collection. Given the match list M
and the pseudo-term clusters, it is relatively straight-
forward to represent spoken documents as bags of
pseudo-terms.
To perform this pseudo-term clustering we repre-
sented matched regions as vertices in a graph with
edges representing similarities between these re-
gions. We employ a graph-clustering algorithm that
extracts connected components. Let G = (V, E) be
an unweighted, undirected graph with vertex set V
and edge set E. Each vi E V corresponds to a sin-
gle speech interval ([t(i)
</bodyText>
<equation confidence="0.9649">
1 , t(i)
2 ]) present in M (each
</equation>
<bodyText confidence="0.984288363636364">
m E M has a pair of such intervals, so V = 2 M )
and each eij E E is an edge between vertex vi and
vj.
The set E consists of two types of edges. The
first represents repeated speech at distinct points
in the corpus as determined by the match list M.
The second represents near-identical intervals in the
same utterance (i.e. the same speech) since a sin-
gle interval can show up in several matches in M
and the algorithm in Section 3 explicitly ignores
self-similarity. Given the intervals [t(i)
</bodyText>
<equation confidence="0.9978915">
1 , t(i)
2 ] and
[t(j)
1 , t(j)
</equation>
<bodyText confidence="0.96956125">
2 ] contained in the same utterance and with
corresponding vertices vi, vj E V , we introduce
an edge eij if fractional overlap fij exceeds some
threshold τ, where fij = max(0, rij) and
</bodyText>
<equation confidence="0.9463862">
(t(i) 2− t(i) − 1. (2)
1 ) + (t(j)
2 − t(j) 1)
rij = max t(i) t(j) − min (t(i), t(j)
( 2 ,2 ) ( 1 1 )
</equation>
<bodyText confidence="0.999760571428571">
From the graph G, we produce one pseudo-term
for each connected component. More sophisticated
edge weighting schemes would likely provide ben-
efit. In particular, we expect improved clustering
by introducing weights that reflect acoustic sim-
ilarity between match intervals, rather than rely-
ing solely upon the term discovery algorithm to
make a hard decision. Such confidence weights
would allow even shorter pseudo-terms to be con-
sidered (by reducing κ) without greatly increasing
false alarms. With such a shift, more sophisticated
graph-clustering mechanisms would be warranted
(e.g. Clauset et al. (2004)). We plan to pursue this
in future work.
</bodyText>
<table confidence="0.996833">
Counts Terms
5 keep track of
5 once a month
2 life insurance
2 capital punishment
9 paper; newspaper
3 talking to you
</table>
<tableCaption confidence="0.9110324">
Table 1: Pseudo-terms resulting from a graph clustering
of matched regions (κ = 0.75, τ = 0.95). Counts indi-
cate the number of times the times the pseudo-terms ap-
pear across 360 conversation sides in development data.
Table 1 contains several examples of pseudo-
</tableCaption>
<bodyText confidence="0.983204571428571">
terms and the matched regions included in each
group. The orthographic forms are taken from the
transcripts in the data (see Section 5). Note that for
some pseudo-terms, the words match exactly, while
for others, the phrases are distinct but phonetically
similar. However, even in this case, there is often
substantial overlap in the spoken terms.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="method">
5 Data
</sectionHeader>
<bodyText confidence="0.998869666666667">
For our experiments we used the Switchboard
Telephone Speech Corpus (Godfrey et al., 1992).
Switchboard is a collection of roughly 2,400 two-
sided telephone conversations with a single partici-
pant per side. Over 500 participants were randomly
paired and prompted with a topic for discussion.
Each conversation belongs to one of 70 pre-selected
topics with the two sides restricted to separate chan-
nels of the audio.
To develop and evaluate our methods, we cre-
ated three data sets from the Switchboard corpus: a
development data set, a held out tuning data
set and an evaluation data set. The develop-
ment data set was created by selecting the six most
commonly prompted topics (recycling, capital pun-
ishment, drug testing, family finance, job benefits,
car buying) and randomly selecting 60 sides of con-
versations evenly across the topics (total 360 con-
versation sides.) This corresponds to 35.7 hours of
audio. Note that each participant contributed at most
one conversation side per topic, so these 360 conver-
sation sides represent 360 distinct speakers. All al-
gorithm development and experimentation was con-
ducted exclusively on the development data.
For the tuning data set, we selected an additional
60 sides of conversations evenly across the same six
topics used for development, for a total of 360 con-
</bodyText>
<page confidence="0.9981">
464
</page>
<bodyText confidence="0.999926066666667">
versations and 37.5 hours of audio. This data was
used to validate our experiments on the develop-
ment data by confirming the heuristic used to select
algorithmic parameters, as described below. This
data was not used for algorithm development. The
evaluation data set was created once parameters had
been selected for a final evaluation of our methods.
We selected this data by sampling 100 conversation
sides from the next six most popular conversation
topics (family life, news media, public education,
exercise/fitness, pets, taxes), yielding 600 conversa-
tion sides containing 61.6 hours of audio.
In our experiments below, we varied the match
duration κ between 0.6 s and 1.0 s and the overlap
threshold τ between 0.75 and 1.0. We measured the
resulting effects on the number of unique pseudo-
terms generated by the process. In general, de-
creasing κ results in more matched regions increas-
ing the number of pseudo-terms. Similarly, increas-
ing τ forces fewer regions to be merged, increasing
the total number of pseudo-terms. Table 2 shows
how these parameters change the number of pseudo-
terms (features) per document and the average num-
ber of occurrences of each pseudo-term. The user
could tune these parameters to select pseudo-terms
that were long and occurred in many documents. In
the next sections, we consider how these parameters
effect performance of various learning settings.
To provide the requisite speaker independent
acoustic model, we compute English phone pos-
teriorgrams using the multi-stream multi-layer
perceptron-based architecture of Thomas et al.
(2009), trained on 300 hours of conversational tele-
phone speech. While this is admittedly a large
amount of supervision, it is important to emphasize
our zero resource term discovery algorithm does not
rely on the phonetic interpretability of this refer-
ence acoustic model. The only requirement is that
the same target language phoneme spoken by dis-
tinct speakers map to similar posterior distributions
over the reference language phoneme set. Thus,
even though we evaluate the system on matched-
language Switchboard data, it can be just as easily
applied to any target language with no language-
specific knowledge or training resources required.4
</bodyText>
<footnote confidence="0.762959666666667">
4The generalization of the speaker independence of acous-
tic models across languages is not well understood. Indeed, the
performance of our proposed system would depend to some ex-
</footnote>
<table confidence="0.999959235294118">
r T Features Feat. Frequency Feat./Doc.
0.6 0.75 5,809 2.15 34.7
0.6 0.85 23,267 2.22 143.4
0.6 0.95 117,788 2.38 779.8
0.6 1.0 333,816 2.32 2153.4
0.75 0.75 8,236 2.31 52.8
0.75 0.85 18,593 2.36 121.7
0.75 0.95 48,547 2.36 318.2
0.75 1.0 90,224 2.18 546.9
0.85 0.75 5,645 2.52 39.5
0.85 0.85 8,832 2.44 59.8
0.85 0.95 15,805 2.24 98.3
0.85 1.0 24,480 2.10 142.4
1.0 0.75 1,844 2.39 12.3
1.0 0.85 2,303 2.24 14.4
1.0 0.95 3,239 2.06 18.6
1.0 1.0 4,205 1.93 22.7
</table>
<tableCaption confidence="0.953796666666667">
Table 2: Statistics on the number of features (pseudo-
terms) generated for different settings of the match dura-
tion κ and the overlap threshold τ.
</tableCaption>
<sectionHeader confidence="0.989334" genericHeader="method">
6 Document Clustering
</sectionHeader>
<bodyText confidence="0.997657964285714">
We begin by considering document clustering, a
popular approach to discovering latent structure in
document collections. Unsupervised clustering al-
gorithms sort examples into groups, where each
group contains documents that are similar. A user
exploring a corpus can look at a few documents in
each cluster to gain an overview of the content dis-
cussed in the corpus. For example, clustering meth-
ods can be used on search results to provide quick
insight into the coverage of the returned documents
(Zeng et al., 2004).
Typically, documents are clustered based on a bag
of words representation. In the case of clustering
conversations in our collection, we would normally
obtain a transcript of the conversation and then ex-
tract a bag of words representation for clustering.
The resulting clusters may represent topics, such as
the six topics used in our switchboard data. Such
groupings, available with no topic labeled training
data, can be a valuable tool for understanding the
contents of a speech data collection. We would like
to know if similar clustering results can be obtained
without the use of a manual or automatic transcript.
In our case, we substitute the pseudo-terms discov-
ered in a conversation for the transcript, representing
tent on the phonetic similarity of the target and reference lan-
guage. Unsupervised learning of speaker independent acoustic
models remains an important area of future research.
</bodyText>
<page confidence="0.999255">
465
</page>
<bodyText confidence="0.999964357142857">
the document as a bag of pseudo-terms instead of ac-
tual words. Can a clustering algorithm achieve sim-
ilar results along topical groups with our transcript-
free representation as it can with a full transcript?
In our experiments, we use the six topic labels
provided by Switchboard as the clustering labels.
The goal is to cluster the data into six balanced
groups according to these topics. While Switch-
board topics are relatively straightforward to iden-
tify since the conversations were prompted with spe-
cific topics, we believe this task can still demon-
strate the effectiveness of our representation relative
to the baseline methods. After all, topic classifica-
tion without ASR is still a difficult task.
</bodyText>
<subsectionHeader confidence="0.950495">
6.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.998463866666667">
There are numerous approaches to evaluating clus-
tering algorithms. We consider several methods: Pu-
rity, Entropy and B-Cubed. For a full treatment of
these metrics, see Amig´o et al. (2009).
Purity measures the precision of each cluster, i.e.,
how many examples in each cluster belong to the
same true topic. Purity ranges between zero and one,
with one being optimal. While optimal purity can be
obtained by putting each document in its own clus-
ter, we fix the number of clusters in all experiments
so purity numbers are comparable. The purity of a
cluster is defined as the largest percentage of exam-
ples in a cluster that have the same topic label. Purity
of the entire clustering is the average purity of each
cluster:
</bodyText>
<equation confidence="0.811339666666667">
1 � max |ci n lj |(3)
purity(C, L) = ljEL
N ciEC
</equation>
<bodyText confidence="0.992944333333333">
where C is the clustering, L is the reference label-
ing, and N are the number of examples. Following
this notation, ci is a specific cluster and lj is a spe-
cific true label.
Entropy measures how the members of a cluster
are distributed amongst the true labels. The global
metric is computed by taking the weighted aver-
age of the entropy of the members of each cluster.
Specifically, entropy(C, L) is given by:
</bodyText>
<footnote confidence="0.228813666666667">
�� Ni P(ci, lj) 1o92 P(ci, lj) (4)
ciEC N
ljEL
</footnote>
<bodyText confidence="0.9999265">
where Ni is the number of instances in cluster i,
P(ci, lj) is the probability of seeing label lj in clus-
ter ci and the other variables are defined as above.
B-Cubed measures clustering effectiveness from
the perspective of a user’s inspecting the clustering
results (Bagga and Baldwin, 1998). B-Cubed preci-
sion can be defined as an algorithm as follows: sup-
pose a user randomly selects a single example. She
then proceeds to inspect every other example that
occurs in the same cluster. How many of these items
will have the same true label as the selected exam-
ple (precision)? B-Cubed recall operates in a sim-
ilar fashion, but it measures what percentage of all
examples that share the same label as the selected
example will appear in the selected cluster. Since B-
Cubed averages its evaluation over each document
and not each cluster, it is less sensitive to small er-
rors in large clusters as opposed to many small errors
in small clusters. We include results for B-Cubed
F1, the harmonic mean of precision and recall.
</bodyText>
<subsectionHeader confidence="0.999921">
6.2 Clustering Algorithms
</subsectionHeader>
<bodyText confidence="0.999983">
We considered several clustering algorithms: re-
peated bisection, globally optimal repeated bisec-
tion, and agglomerative clustering (see Karypis
(2003) for implementation details). Each bisection
algorithm is run 10 times and the optimal clustering
is selected according to a provided criteria function
(no true labels needed). For each clustering method,
we evaluated several criteria functions. Addition-
ally, we considered different scalings of the feature
values (the number of times the pseudo-terms ap-
pear in each document). We found that scaling each
feature by the inverse document frequency, effec-
tively TFIDF, produced the best results, so we use
that scaling in all of our experiments. We also ex-
plored various similarity metrics and found cosine
similarity to be the most effective.
We used the Cluto clustering library for all clus-
tering experiments (Karypis, 2003). In the following
section, we report results for the optimal clustering
configuration based on experiments on the develop-
ment data.
</bodyText>
<subsectionHeader confidence="0.999167">
6.3 Baselines
</subsectionHeader>
<bodyText confidence="0.999809">
We compared our pseudo-term feature set perfor-
mance to two baselines: (1) Phone Trigrams and
</bodyText>
<page confidence="0.998999">
466
</page>
<bodyText confidence="0.990341086956522">
(2) Word Transcripts. The Phone Trigram base-
line is derived automatically using an approach sim-
ilar to Hazen et al. (2007). This baseline is based
on a vanilla phone recognizer on top of the same
MLP-based acoustic model (see Section 5 and the
references therein for details) used to discover the
pseudo-terms. In particular, the phone posterior-
grams were transformed to frame-level monophone
state likelihoods (through division by the frame-
level priors). These state likelihoods were then used
along with frame-level phone transition probabilities
to Viterbi decode each conversation side. It is impor-
tant to emphasize that the reliability of phone recog-
nizers depends on the phone set matching the appli-
cation language. Using the English acoustic model
in this manner on another language will significantly
degrade the performance numbers reported below.
The Word Transcript baseline starts with Switch-
board transcripts. This baseline serves as an upper
bound of what large vocabulary recognition can pro-
vide for this task. n-gram features are computed
from the transcript. Performance is reported sepa-
rately for unigrams, bigrams and trigrams.
</bodyText>
<subsectionHeader confidence="0.715411">
6.4 Results
</subsectionHeader>
<bodyText confidence="0.999505086956522">
To optimize parameter settings, match duration (r.)
and overlap threshold (r) were swept over a wide
range (0.6 &lt; r. &lt; 1.0 and 0.75 &lt; r &lt; 1.0) using a
variety of clustering algorithms and training criteria.
Initial results on development data showed promis-
ing performance for the default 12 criteria in Cluto
(repeated bisection set to maximize the square root
of within cluster similarity). Representative results
on development data with various parameter settings
for this clustering configuration appear in Table 3.
A few observations about results on development
data. First, the three evaluation metrics are strongly
correlated. Second, for each r. the same narrow
range of r values achieve good results. In general,
settings of r &gt; 0.9 were all comparable. Essen-
tially, setting a high threshold for merging matched
regions was sufficient without further tuning. Third,
we observed that decreasing r. meant more features,
but that these additional features did not necessarily
lead to more useful features for clustering. For ex-
ample, r. = 0.70 gave a small number of reasonably
good features, while r. = 0.60 can give an order of
magnitude more features without much of a change
</bodyText>
<table confidence="0.999473833333334">
Pseudo-term Results
r z Features Purity Entropy B3 F1
0.60 0.95 117,788 0.9639 0.2348 0.9306
0.60 0.96 143,299 0.9750 0.1664 0.9518
0.60 0.97 178,559 0.9667 0.2116 0.9366
0.60 0.98 223,511 0.9528 0.2717 0.9133
0.60 0.99 333,630 0.9583 0.2641 0.9210
0.60 1.0 333,816 0.9583 0.2641 0.9210
0.70 0.93 58,303 0.9528 0.3114 0.9105
0.70 0.94 66,054 0.9667 0.2255 0.9358
0.70 0.95 74,863 0.9583 0.2669 0.9210
0.70 0.96 86,070 0.9611 0.2529 0.9260
0.70 0.97 100,623 0.9639 0.2326 0.9312
0.70 0.98 117,535 0.9556 0.2821 0.9158
0.70 0.99 161,219 0.9056 0.4628 0.8372
0.70 1.0 161,412 0.9333 0.4011 0.8760
Phone Recognizer Baseline
Type Features Purity Entropy B3 F1
Phone Trigram 28,110 0.6194 1.3657 0.5256
Manual Word Transcript Baselines
Type Features Purity Entropy B3 F1
Word Unigram 7,330 0.9917 0.0559 0.9839
Word Bigram 74,216 0.9833 0.1111 0.9678
Word Trigram 224,934 0.9889 0.0708 0.9787
</table>
<tableCaption confidence="0.8643925">
Table 3: Clustering results on development data using
globally optimal repeated bisection and 22 criteria. The
</tableCaption>
<bodyText confidence="0.99668348">
best results over the manual word transcript baselines
and for each match duration (r.) are highlighted in bold.
Pseudo-term results are better than the phonetic baseline
and almost as good as the transcript baseline.
in clustering performance. Finally, while pseudo-
term results are not as good as with the manual
transcripts (unigrams), they achieve similar results.
Compared with the phone trigram features deter-
mined by the phone recognizer output, the pseudo-
terms perform significantly better. Note that these
two automatic approaches were built using the iden-
tical MLP-based phonetic acoustic model.
We sought to select the optimal parameter settings
for running on the evaluation data using the devel-
opment data and the held out tuning data. We de-
fined the following heuristic to select the optimal pa-
rameters. We choose settings for r., r and the clus-
tering parameters that independently maximize the
performance averaged over all runs on development
data. We then selected the single run correspond-
ing to these parameter settings and checked the re-
sult on the held out tuning data. This setting was
also the best performer on the held out set, so we
used these parameters for evaluation. The best per-
forming parameters were globally optimal repeated
</bodyText>
<page confidence="0.997257">
467
</page>
<table confidence="0.999953666666667">
r T Features Purity Entropy B3 F1
0.70 0.98 123,901 0.9778 0.1574 0.9568
Phone Trigram 28,374 0.6389 1.2345 0.5513
Word Unigram 7,640 0.9972 0.0204 0.9945
Word Bigram 77,201 0.9972 0.0204 0.9945
Word Trigram 233,744 0.9972 0.0204 0.9945
</table>
<tableCaption confidence="0.783090666666667">
Table 4: Results on held out tuning data. The parameters
(globally optimal repeated bisection clustering with 22
criteria, n = 0.70 seconds and T = 0.98) were selected
using the development data and validated on tuning data.
Note that the clusters produced by each manual transcript
test were identical in this case.
</tableCaption>
<table confidence="0.9999475">
r T Features Purity Entropy B3 F1
0.70 0.98 279,239 0.9517 0.3366 0.9073
Phone Trigram 31,502 0.7000 1.0496 0.6355
Word Unigram 9939 0.9883 0.0831 0.9772
Word Bigram 110,859 0.9883 0.0910 0.9771
Word Trigram 357,440 0.9900 0.0775 0.9803
</table>
<tableCaption confidence="0.875163">
Table 5: Results on evaluation data. The parameters
(globally optimal repeated bisection clustering with 12
criteria, n = 0.7 seconds and T = 0.98) were selected
using the development data and validated on tuning data.
</tableCaption>
<bodyText confidence="0.9999508125">
bisection clustering with 12 criteria, n = 0.7 s and
T = 0.98. Note that examining Table 3 alone may
suggest other parameters, but we found our selection
method to yield optimal results on the tuning data.
Results on held out tuning and evaluation data for
this setting compared to the manual word transcripts
and phone recognizer output are shown in Tables
4 and 5. On both the tuning data and evaluation
data, we obtain similar results as on the development
data. While the manual transcript baseline is bet-
ter than our pseudo-term representations, the results
are quite competitive. This demonstrates that use-
ful clustering results can be obtained without a full-
blown word recognizer. Notice also that the pseudo-
term performance remains significantly higher than
the phone recognizer baseline on both sets.
</bodyText>
<sectionHeader confidence="0.996365" genericHeader="method">
7 Supervised Document Classification
</sectionHeader>
<bodyText confidence="0.999889754716981">
Unsupervised clustering methods are attractive since
they require no human annotations. However, ob-
taining a few labeled examples for a simple label-
ing task can be done quickly, especially with crowd
sourcing systems such as CrowdFlower and Ama-
zon’s Mechanical Turk (Snow et al., 2008; Callison-
Burch and Dredze, 2010). In this setting, a user
may listen to a few conversations and label them by
topic. A supervised classification algorithm can then
be trained on these labeled examples and used to au-
tomatically categorize the rest of the data. In this
section, we evaluate if supervised algorithms can be
trained using the pseudo-term representation of the
speech.
We set up a multi-class supervised classification
task, where each document is labeled using one of
the six Switchboard topics. A supervised learning
algorithm is trained on a sample of labeled docu-
ments and is then asked to label some test data. Re-
sults are measured in terms of accuracy. Since the
documents are a balanced sample of the six topics,
random guessing would yield an accuracy of 0.1667.
We proceed as with the clustering experiments.
We evaluate different representations for various set-
tings of n and T and different classifier parameters
on the development data. We then select the opti-
mal parameter settings and validate this selection on
the held out tuning data, before generating the final
representations for the evaluation once the optimal
parameters have been selected.
For learning we require a multi-class classifier
training algorithm. We evaluated four popular
learning algorithms: a) MIRA—a large margin on-
line learning algorithm (Crammer et al., 2006); b)
Confidence Weighted (CW) learning—a probabilis-
tic large margin online learning algorithm (Dredze
et al., 2008; Crammer et al., 2009); c) Maxi-
mum Entropy—a log-linear discriminative classi-
fier (Berger et al., 1996); and d) Support Vec-
tor Machines (SVM)—a large margin discriminator
(Joachims, 1998).5 For each experiment, we used
default settings of the parameters (tuning did not sig-
nificantly change the results) and 10 online iterations
for the online methods (MIRA, CW). Each reported
result is based on 10-fold cross validation.
Table 6 shows results for various parameter set-
tings and the four learning algorithms on develop-
ment data. As before, we observe that values for
T &gt; 0.9 tend to do well. The CW learning algo-
rithm performs the best on this data, followed by
Maximum Entropy, MIRA and SVM. The optimal
n for classification is 0.75, close to the 0.7 value
selected in clustering. As before, pseudo-terms do
</bodyText>
<footnote confidence="0.861976666666667">
5We used the “variance” formulation with k = 1 for CW
learning, Gaussian regularization for the Maximum Entropy
classifier, and a linear kernel for the SVM.
</footnote>
<page confidence="0.994881">
468
</page>
<table confidence="0.99987145">
κ τ MaxEnt SVM CW MIRA
0.60 0.99 0.8972 0.6944 0.8667 0.8972
0.60 1.0 0.8972 0.6944 0.8639 0.8944
0.70 0.97 0.9000 0.7722 0.8500 0.8056
0.70 0.98 0.8806 0.7417 0.8917 0.8167
0.70 0.99 0.9000 0.6556 0.9194 0.9056
0.70 1.0 0.8917 0.6556 0.9194 0.9083
0.75 0.94 0.8778 0.7806 0.8639 0.8056
0.75 0.95 0.8778 0.7694 0.8889 0.8111
0.75 0.96 0.9028 0.7778 0.9000 0.8778
0.75 0.97 0.9111 0.7722 0.9250 0.9278
0.75 0.98 0.9056 0.7417 0.9194 0.9167
0.85 0.85 0.8639 0.7833 0.8500 0.8167
0.85 0.90 0.8611 0.7528 0.8611 0.8583
0.85 0.91 0.8389 0.7500 0.8722 0.8556
0.85 0.92 0.8528 0.7222 0.8944 0.8556
Phone Trigram 0.6111 0.7139 0.9138 0.5000
Word Unigram 0.9472 0.8861 0.9861 0.9306
Word Bigram 0.9250 0.8833 0.9917 0.9278
Word Trigram 0.9278 0.8611 0.9889 0.9222
</table>
<tableCaption confidence="0.99406">
Table 6: The top 15 results (measured as average accu-
</tableCaption>
<bodyText confidence="0.986941625">
racy across the 4 algorithms) for pseudo-terms on de-
velopment data. The best pseudo-term and manual tran-
script results for each algorithm are bolded. All results
are based on 10-fold cross validation. Pseudo-term re-
sults are better than the phonetic baseline and almost as
good as the transcript baseline.
well, though not as well as the upper bound based
on manual transcripts. The performance for pseudo-
terms and phone trigrams are roughly comparable,
though we expect pseudo-terms to be more robust
across languages.
Using the same selection heuristic as in cluster-
ing, we select the optimal parameter settings, vali-
date them on the held out tuning data, and compute
results on evaluation data. The best performing con-
figuration was for n = 0.75 seconds and T = 0.97.
Notice these parameters are very similar to the best
parameters selected for clustering. Results on held
out tuning and evaluation data for this setting com-
pared to the manual transcripts are shown in Tables
7 and 8. As with clustering, we see good overall
performance as compared with manual transcripts.
While the performance drops, results suggest that
useful output can be obtained without a transcript.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.9997625">
We have presented a new strategy for applying stan-
dard NLP tools to speech corpora without the aid
of a large vocabulary word recognizer. Built in-
stead on top of the unsupervised discovery of term-
</bodyText>
<table confidence="0.9980285">
κ τ MaxEnt SVM CW MIRA
0.75 0.97 0.8722 0.7389 0.8972 0.8750
Phone Trigram 0.7167 0.6972 0.9056 0.5083
Word Unigram 0.9500 0.9056 0.9806 0.9250
Word Bigram 0.9444 0.9111 0.9833 0.9250
Word Trigram 0.9417 0.8972 0.9778 0.9250
</table>
<tableCaption confidence="0.894043833333333">
Table 7: Results on held out tuning data. The parameters
(n = 0.75 seconds and T = 0.97) were selected using the
development data and validated on tuning data. All re-
sults are based on 10-fold cross validation. Pseudo-term
results are very close to the transcript baseline and often
better than the phonetic baseline.
</tableCaption>
<table confidence="0.9994975">
κ τ MaxEnt SVM CW MIRA
0.75 0.97 0.8683 0.7167 0.7850 0.7150
Phone Trigram 0.8600 0.7750 0.9183 0.6233
Word Unigram 0.9533 0.9317 0.9850 0.9267
Word Bigram 0.9467 0.9200 0.9900 0.9367
Word Trigram 0.9383 0.9233 0.9817 0.9367
</table>
<tableCaption confidence="0.998076">
Table 8: Results on evaluation data. The parameters
</tableCaption>
<bodyText confidence="0.9658624">
(n = 0.75 seconds and T = 0.97) were selected using the
development data and validated on tuning data. All re-
sults are based on 10-fold cross validation. Pseudo-term
results are very close to the transcript baseline and often
better than the phonetic baseline.
like units in the speech, we perform unsupervised
topic clustering as well as supervised classification
of spoken documents with performance approaching
that achieved with the manual word transcripts, and
generally matching or exceeding that achieved with
a phonetic recognizer. Our study identified several
opportunities and challenges in the development of
NLP tools for spoken documents that rely on little
or no linguistic resources such as dictionaries and
training corpora.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998615142857143">
Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4).
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of the 17th international conference on
Computational linguistics-Volume 1, pages 79–85. As-
sociation for Computational Linguistics.
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon’s Mechanical
</reference>
<page confidence="0.995333">
469
</page>
<reference confidence="0.998714882352941">
Turk. In Workshop on Creating Speech and Language
Data With Mechanical Turk at NAACL-HLT.
K. W. Church and J. I. Helfman. 1993. Dotplot: A
program for exploring self-similarity in millions of
lines of text and code. Journal of Computational and
Graphical Statistics.
Aaron Clauset, Mark E J Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Physical Review E, 70.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research (JMLR).
Koby Crammer, Mark Dredze, and Alex Kulesza. 2009.
Multi-class confidence weighted algorithms. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
ICASSP.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In ICASSP.
Timothy J. Hazen and Anna Margolis. 2008. Discrimi-
native feature weighting using MCE training for topic
identification of spoken audio recordings. In ICASSP.
Timothy J. Hazen, Fred Richardson, and Anna Margo-
lis. 2007. Topic identification from audio recordings
using word and phone recognition lattices. In IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding.
Aren Jansen, Kenneth Church, and Hynek Hermansky.
2010. Towards spoken term discovery at scale with
zero resources. In Interspeech.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learning
(ECML).
George Karypis. 2003. CLUTO: A software package for
clustering high-dimensional data sets. Technical Re-
port 02-017, University of Minnesota, Dept. of Com-
puter Science.
Igor Malioutov, Alex Park, Regina Barzilay, and James
Glass. 2007. Making Sense of Sound: Unsupervised
Topic Segmentation Over Acoustic Input. In ACL.
Scott Novotney and Richard Schwartz. 2009. Analysis
of low-resource acoustic model self-training. In Inter-
speech.
Alex Park and James R. Glass. 2008. Unsupervised pat-
tern discovery in speech. IEEE Transactions ofAudio,
Speech, and Language Processing.
R. Snow, B. O’Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast—but is it good?: Evaluating non-
expert annotations for natural language tasks. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 254–263. Asso-
ciation for Computational Linguistics.
S. Thomas, S. Ganapathy, and H. Hermansky. 2009.
Phoneme recognition using spectral envelope and
modulation frequency features. In Proc. of ICASSP.
H.J. Zeng, Q.C. He, Z. Chen, W.Y. Ma, and J. Ma. 2004.
Learning to cluster web search results. In Conference
on Research and development in information retrieval
(SIGIR).
</reference>
<page confidence="0.998126">
470
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.452287">
<title confidence="0.999395">NLP on Spoken Documents without ASR</title>
<author confidence="0.998066">Mark Dredze</author>
<author confidence="0.998066">Aren Jansen</author>
<author confidence="0.998066">Glen Coppersmith</author>
<author confidence="0.998066">Ken</author>
<affiliation confidence="0.777153666666667">Human Language Technology Center of Center for Language and Speech Johns Hopkins</affiliation>
<email confidence="0.999765">mdredze,aren,coppersmith,Kenneth.Church@jhu.edu</email>
<abstract confidence="0.996949045454545">There is considerable interest in interdisciplinary combinations of automatic speech recognition (ASR), machine learning, natural language processing, text classification and information retrieval. Many of these boxes, especially ASR, are often based on considerable linguistic resources. We would like to be able to process spoken documents with few (if any) resources. Moreover, connecting black boxes in series tends to multiply errors, especially when the key terms are out-ofvocabulary (OOV). The proposed alternative applies text processing directly to the speech without a dependency on ASR. The method long repetitions in speech, and clusters them into pseudo-terms (roughly phrases). Document clustering and classification work surprisingly well on pseudoterms; performance on a Switchboard task approaches a baseline using gold standard manual transcriptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Javier Artiles</author>
<author>Felisa Verdejo</author>
</authors>
<title>A comparison of extrinsic clustering evaluation metrics based on formal constraints.</title>
<date>2009</date>
<journal>Information Retrieval,</journal>
<volume>12</volume>
<issue>4</issue>
<marker>Amig´o, Gonzalo, Artiles, Verdejo, 2009</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval, 12(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based crossdocument coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>79--85</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26179" citStr="Bagga and Baldwin, 1998" startWordPosition="4325" endWordPosition="4328">pecific cluster and lj is a specific true label. Entropy measures how the members of a cluster are distributed amongst the true labels. The global metric is computed by taking the weighted average of the entropy of the members of each cluster. Specifically, entropy(C, L) is given by: �� Ni P(ci, lj) 1o92 P(ci, lj) (4) ciEC N ljEL where Ni is the number of instances in cluster i, P(ci, lj) is the probability of seeing label lj in cluster ci and the other variables are defined as above. B-Cubed measures clustering effectiveness from the perspective of a user’s inspecting the clustering results (Bagga and Baldwin, 1998). B-Cubed precision can be defined as an algorithm as follows: suppose a user randomly selects a single example. She then proceeds to inspect every other example that occurs in the same cluster. How many of these items will have the same true label as the selected example (precision)? B-Cubed recall operates in a similar fashion, but it measures what percentage of all examples that share the same label as the selected example will appear in the selected cluster. Since BCubed averages its evaluation over each document and not each cluster, it is less sensitive to small errors in large clusters </context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-based crossdocument coreferencing using the vector space model. In Proceedings of the 17th international conference on Computational linguistics-Volume 1, pages 79–85. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="36381" citStr="Berger et al., 1996" startWordPosition="5944" endWordPosition="5947"> We then select the optimal parameter settings and validate this selection on the held out tuning data, before generating the final representations for the evaluation once the optimal parameters have been selected. For learning we require a multi-class classifier training algorithm. We evaluated four popular learning algorithms: a) MIRA—a large margin online learning algorithm (Crammer et al., 2006); b) Confidence Weighted (CW) learning—a probabilistic large margin online learning algorithm (Dredze et al., 2008; Crammer et al., 2009); c) Maximum Entropy—a log-linear discriminative classifier (Berger et al., 1996); and d) Support Vector Machines (SVM)—a large margin discriminator (Joachims, 1998).5 For each experiment, we used default settings of the parameters (tuning did not significantly change the results) and 10 online iterations for the online methods (MIRA, CW). Each reported result is based on 10-fold cross validation. Table 6 shows results for various parameter settings and the four learning algorithms on development data. As before, we observe that values for T &gt; 0.9 tend to do well. The CW learning algorithm performs the best on this data, followed by Maximum Entropy, MIRA and SVM. The optim</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Workshop on Creating Speech and Language Data With Mechanical Turk at NAACL-HLT.</booktitle>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In Workshop on Creating Speech and Language Data With Mechanical Turk at NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>J I Helfman</author>
</authors>
<title>Dotplot: A program for exploring self-similarity in millions of lines of text and code.</title>
<date>1993</date>
<journal>Journal of Computational and Graphical Statistics.</journal>
<contexts>
<context position="7772" citStr="Church and Helfman, 1993" startWordPosition="1187" endWordPosition="1190"> Early efforts to perform automatic topic segmentation of speech input without the aid of ASR systems have been promising (Malioutov et al., 2007), but have yet to exploit the full the range of NLP tools. 3 Identifying Matched Regions Our goal is to identify pairs of intervals within and across utterances of several speakers that contain the same linguistic content, preferably meaningful words or terms. The spoken term discovery algorithm of Jansen et al. (2010) efficiently searches the space of (2) intervals, where n is the number of speech frames.1 Jansen et al. (2010) is based on dotplots (Church and Helfman, 1993), a method borrowed from bioinformatics for finding repetitions in DNA sequences. 1Typically, each frame represents a 25 or 30 ms window of speech sampled every 10 ms 461 Time (s) g n i s s e c o r p h c e e p s . s v g n i s s e c o r p t x e t text processing v s . speech processing 8 1 7 0.95 6 0.9 5 0.85 4 0.8 3 0.75 2 0.7 1 0.65 0.6 0.55 1 2 3 4 5 6 7 8 Time (s) Figure 1: An example of a dotplot for the string “text processing vs. speech processing” plotted against itself. The box calls out the repeated substring: “processing.” 3.1 Acoustic Dotplots When applied to text, the dotplot const</context>
</contexts>
<marker>Church, Helfman, 1993</marker>
<rawString>K. W. Church and J. I. Helfman. 1993. Dotplot: A program for exploring self-similarity in millions of lines of text and code. Journal of Computational and Graphical Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Mark E J Newman</author>
<author>Cristopher Moore</author>
</authors>
<title>Finding community structure in very large networks. Physical Review E,</title>
<date>2004</date>
<contexts>
<context position="17391" citStr="Clauset et al. (2004)" startWordPosition="2873" endWordPosition="2876"> ,2 ) ( 1 1 ) From the graph G, we produce one pseudo-term for each connected component. More sophisticated edge weighting schemes would likely provide benefit. In particular, we expect improved clustering by introducing weights that reflect acoustic similarity between match intervals, rather than relying solely upon the term discovery algorithm to make a hard decision. Such confidence weights would allow even shorter pseudo-terms to be considered (by reducing κ) without greatly increasing false alarms. With such a shift, more sophisticated graph-clustering mechanisms would be warranted (e.g. Clauset et al. (2004)). We plan to pursue this in future work. Counts Terms 5 keep track of 5 once a month 2 life insurance 2 capital punishment 9 paper; newspaper 3 talking to you Table 1: Pseudo-terms resulting from a graph clustering of matched regions (κ = 0.75, τ = 0.95). Counts indicate the number of times the times the pseudo-terms appear across 360 conversation sides in development data. Table 1 contains several examples of pseudoterms and the matched regions included in each group. The orthographic forms are taken from the transcripts in the data (see Section 5). Note that for some pseudo-terms, the words</context>
</contexts>
<marker>Clauset, Newman, Moore, 2004</marker>
<rawString>Aaron Clauset, Mark E J Newman, and Cristopher Moore. 2004. Finding community structure in very large networks. Physical Review E, 70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research (JMLR).</journal>
<contexts>
<context position="36163" citStr="Crammer et al., 2006" startWordPosition="5912" endWordPosition="5915">essing would yield an accuracy of 0.1667. We proceed as with the clustering experiments. We evaluate different representations for various settings of n and T and different classifier parameters on the development data. We then select the optimal parameter settings and validate this selection on the held out tuning data, before generating the final representations for the evaluation once the optimal parameters have been selected. For learning we require a multi-class classifier training algorithm. We evaluated four popular learning algorithms: a) MIRA—a large margin online learning algorithm (Crammer et al., 2006); b) Confidence Weighted (CW) learning—a probabilistic large margin online learning algorithm (Dredze et al., 2008; Crammer et al., 2009); c) Maximum Entropy—a log-linear discriminative classifier (Berger et al., 1996); and d) Support Vector Machines (SVM)—a large margin discriminator (Joachims, 1998).5 For each experiment, we used default settings of the parameters (tuning did not significantly change the results) and 10 online iterations for the online methods (MIRA, CW). Each reported result is based on 10-fold cross validation. Table 6 shows results for various parameter settings and the f</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research (JMLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Mark Dredze</author>
<author>Alex Kulesza</author>
</authors>
<title>Multi-class confidence weighted algorithms.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="36300" citStr="Crammer et al., 2009" startWordPosition="5932" endWordPosition="5935">s settings of n and T and different classifier parameters on the development data. We then select the optimal parameter settings and validate this selection on the held out tuning data, before generating the final representations for the evaluation once the optimal parameters have been selected. For learning we require a multi-class classifier training algorithm. We evaluated four popular learning algorithms: a) MIRA—a large margin online learning algorithm (Crammer et al., 2006); b) Confidence Weighted (CW) learning—a probabilistic large margin online learning algorithm (Dredze et al., 2008; Crammer et al., 2009); c) Maximum Entropy—a log-linear discriminative classifier (Berger et al., 1996); and d) Support Vector Machines (SVM)—a large margin discriminator (Joachims, 1998).5 For each experiment, we used default settings of the parameters (tuning did not significantly change the results) and 10 online iterations for the online methods (MIRA, CW). Each reported result is based on 10-fold cross validation. Table 6 shows results for various parameter settings and the four learning algorithms on development data. As before, we observe that values for T &gt; 0.9 tend to do well. The CW learning algorithm per</context>
</contexts>
<marker>Crammer, Dredze, Kulesza, 2009</marker>
<rawString>Koby Crammer, Mark Dredze, and Alex Kulesza. 2009. Multi-class confidence weighted algorithms. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="36277" citStr="Dredze et al., 2008" startWordPosition="5928" endWordPosition="5931">sentations for various settings of n and T and different classifier parameters on the development data. We then select the optimal parameter settings and validate this selection on the held out tuning data, before generating the final representations for the evaluation once the optimal parameters have been selected. For learning we require a multi-class classifier training algorithm. We evaluated four popular learning algorithms: a) MIRA—a large margin online learning algorithm (Crammer et al., 2006); b) Confidence Weighted (CW) learning—a probabilistic large margin online learning algorithm (Dredze et al., 2008; Crammer et al., 2009); c) Maximum Entropy—a log-linear discriminative classifier (Berger et al., 1996); and d) Support Vector Machines (SVM)—a large margin discriminator (Joachims, 1998).5 For each experiment, we used default settings of the parameters (tuning did not significantly change the results) and 10 online iterations for the online methods (MIRA, CW). Each reported result is based on 10-fold cross validation. Table 6 shows results for various parameter settings and the four learning algorithms on development data. As before, we observe that values for T &gt; 0.9 tend to do well. The CW</context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted linear classification. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvin Garcia</author>
<author>Herbert Gish</author>
</authors>
<title>Keyword spotting of arbitrary words using minimal speech resources.</title>
<date>2006</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="5861" citStr="Garcia and Gish, 2006" startWordPosition="886" endWordPosition="889">me, most approaches have focused on coupling small amounts of orthographically transcribed speech (10s of hours) with much larger collections of untranscribed speech (100s or 1000s of hours) to train accurate acoustic models with semi-supervised methods (Novotney and Schwartz, 2009). In these efforts, the goal is to reduce the annotation requirements for the construction of competent LVCSR systems. This semi-supervised paradigm was relaxed even further with the pursuit of self organizing units (SOUs), phone-like units for which acoustic models are trained with completely unsupervised methods (Garcia and Gish, 2006). Even though the move away from phonetic acoustic models improves the universality of the architecture, small amounts of orthographic transcription are still required to connect the SOUs with the lexicon. The segmental dynamic time warping (S-DTW) algorithm (Park and Glass, 2008) was the first truly zero resource effort, designed to discover portions of the lexicon directly by searching for repeated acoustic patterns in the speech signal. This work implicitly defined a new direction for speech processing research: unsupervised spoken term discovery, the entry point of our speech corpora analy</context>
</contexts>
<marker>Garcia, Gish, 2006</marker>
<rawString>Alvin Garcia and Herbert Gish. 2006. Keyword spotting of arbitrary words using minimal speech resources. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Godfrey</author>
<author>E C Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="18257" citStr="Godfrey et al., 1992" startWordPosition="3020" endWordPosition="3023"> τ = 0.95). Counts indicate the number of times the times the pseudo-terms appear across 360 conversation sides in development data. Table 1 contains several examples of pseudoterms and the matched regions included in each group. The orthographic forms are taken from the transcripts in the data (see Section 5). Note that for some pseudo-terms, the words match exactly, while for others, the phrases are distinct but phonetically similar. However, even in this case, there is often substantial overlap in the spoken terms. 5 Data For our experiments we used the Switchboard Telephone Speech Corpus (Godfrey et al., 1992). Switchboard is a collection of roughly 2,400 twosided telephone conversations with a single participant per side. Over 500 participants were randomly paired and prompted with a topic for discussion. Each conversation belongs to one of 70 pre-selected topics with the two sides restricted to separate channels of the audio. To develop and evaluate our methods, we created three data sets from the Switchboard corpus: a development data set, a held out tuning data set and an evaluation data set. The development data set was created by selecting the six most commonly prompted topics (recycling, cap</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J Hazen</author>
<author>Anna Margolis</author>
</authors>
<title>Discriminative feature weighting using MCE training for topic identification of spoken audio recordings.</title>
<date>2008</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="12543" citStr="Hazen and Margolis (2008)" startWordPosition="2013" endWordPosition="2016">rior distribution over the phone set at that particular point in time. The construction of speaker independent acoustic models typically requires a significant amount of transcribed speech. Our proposed strategy is to employ a speaker independent acoustic model trained in a high resource language or domain to interpret multi-speaker data in the zero resource target setting.2 Indeed, we do not need to know a language to detect when a word of sufficient length has been repeated in it.3 By computing cosine similarities 2A similarly-minded approach was taken in Hazen et al. (2007) and extended in Hazen and Margolis (2008), where the authors use Hungarian phonetic trigrams features to characterize English spoken documents for a topic classification task. 3While in this paper our acoustic model is based on our evaluation corpus, this is not a requirement of our approach. Future work will investigate performance of other acoustic models. of phonetic posterior distribution vectors (as opposed to reducing the speech to a one-best phonetic token sequence), the phone set used need not be matched to the target language. With this approach, a speaker-independent model trained on the phone set of a reference language ma</context>
</contexts>
<marker>Hazen, Margolis, 2008</marker>
<rawString>Timothy J. Hazen and Anna Margolis. 2008. Discriminative feature weighting using MCE training for topic identification of spoken audio recordings. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J Hazen</author>
<author>Fred Richardson</author>
<author>Anna Margolis</author>
</authors>
<title>Topic identification from audio recordings using word and phone recognition lattices.</title>
<date>2007</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="3841" citStr="Hazen et al. (2007)" startWordPosition="584" endWordPosition="587">s on a given topic. These tools require little or no human input (annotation) and work across languages. Given a large collection of speech, we would like 460 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 460–470, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics tools that perform many of the same tasks, allowing the user to understand the contents of the collection while listening to only small portions of the audio. Previous work has applied these NLP tools to speech corpora with similar results (see Hazen et al. (2007) and the references therein.) However, unlike text, which requires little or no preprocessing, audio files are typically first transcribed into text before applying standard NLP tools. Automatic speech recognition (ASR) solutions, such as large vocabulary continuous speech recognition (LVCSR) systems, can produce an automatic transcript from speech, but they require significant development efforts and training resources, typically hundreds of hours of manually transcribed speech. Moreover, the terms that may be most distinctive in particular spoken documents often lie outside the predefined vo</context>
<context position="6904" citStr="Hazen et al., 2007" startWordPosition="1046" endWordPosition="1049"> the speech signal. This work implicitly defined a new direction for speech processing research: unsupervised spoken term discovery, the entry point of our speech corpora analysis system. Subsequent extensions of S-DTW (Jansen et al., 2010) permit applications to much larger speech collections, a flexibility that is vital to our efforts. As mentioned above, the application of NLP methods to speech corpora have traditionally relied on high resource ASR systems to provide automatic word or phonetic transcripts. Spoken document topic classification has been an application of particular interest (Hazen et al., 2007), for which the recognized words or phone n-grams are used to characterize the documents. These efforts have produced admirable results, with ASR transcript-based performance approached that obtained using the gold standard manual transcripts. Early efforts to perform automatic topic segmentation of speech input without the aid of ASR systems have been promising (Malioutov et al., 2007), but have yet to exploit the full the range of NLP tools. 3 Identifying Matched Regions Our goal is to identify pairs of intervals within and across utterances of several speakers that contain the same linguist</context>
<context position="12501" citStr="Hazen et al. (2007)" startWordPosition="2006" endWordPosition="2009">and each column represents the posterior distribution over the phone set at that particular point in time. The construction of speaker independent acoustic models typically requires a significant amount of transcribed speech. Our proposed strategy is to employ a speaker independent acoustic model trained in a high resource language or domain to interpret multi-speaker data in the zero resource target setting.2 Indeed, we do not need to know a language to detect when a word of sufficient length has been repeated in it.3 By computing cosine similarities 2A similarly-minded approach was taken in Hazen et al. (2007) and extended in Hazen and Margolis (2008), where the authors use Hungarian phonetic trigrams features to characterize English spoken documents for a topic classification task. 3While in this paper our acoustic model is based on our evaluation corpus, this is not a requirement of our approach. Future work will investigate performance of other acoustic models. of phonetic posterior distribution vectors (as opposed to reducing the speech to a one-best phonetic token sequence), the phone set used need not be matched to the target language. With this approach, a speaker-independent model trained o</context>
<context position="28176" citStr="Hazen et al. (2007)" startWordPosition="4647" endWordPosition="4650">ced the best results, so we use that scaling in all of our experiments. We also explored various similarity metrics and found cosine similarity to be the most effective. We used the Cluto clustering library for all clustering experiments (Karypis, 2003). In the following section, we report results for the optimal clustering configuration based on experiments on the development data. 6.3 Baselines We compared our pseudo-term feature set performance to two baselines: (1) Phone Trigrams and 466 (2) Word Transcripts. The Phone Trigram baseline is derived automatically using an approach similar to Hazen et al. (2007). This baseline is based on a vanilla phone recognizer on top of the same MLP-based acoustic model (see Section 5 and the references therein for details) used to discover the pseudo-terms. In particular, the phone posteriorgrams were transformed to frame-level monophone state likelihoods (through division by the framelevel priors). These state likelihoods were then used along with frame-level phone transition probabilities to Viterbi decode each conversation side. It is important to emphasize that the reliability of phone recognizers depends on the phone set matching the application language. </context>
</contexts>
<marker>Hazen, Richardson, Margolis, 2007</marker>
<rawString>Timothy J. Hazen, Fred Richardson, and Anna Margolis. 2007. Topic identification from audio recordings using word and phone recognition lattices. In IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aren Jansen</author>
<author>Kenneth Church</author>
<author>Hynek Hermansky</author>
</authors>
<title>Towards spoken term discovery at scale with zero resources.</title>
<date>2010</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="1649" citStr="Jansen et al. (2010)" startWordPosition="240" endWordPosition="243">ell on pseudoterms; performance on a Switchboard task approaches a baseline using gold standard manual transcriptions. 1 Introduction Can we do IR-like tasks without ASR? Information retrieval (IR) typically makes use of simple features that count terms within/across documents such as term frequency (tf) and inverse document frequency (IDF). Crucially, to compute these features, it is sufficient to count repetitions of a term. In particular, for many IR-like tasks, there is no need for an automatic speech recognition (ASR) system to label terms with phonemes and/or words. This paper builds on Jansen et al. (2010), a method for discovering terms with zero resources. This approach identifies long, faithfully repeated patterns in the acoustic signal. These acoustic repetitions often correspond to terms useful for information retrieval tasks. Critically, this method does not require a phonetically interpretable acoustic model or knowledge of the target language. By analyzing a large untranscribed corpus of speech, this discovery procedure identifies a vast number of repeated regions that are subsequently grouped using a simple graph-based clustering method. We call the resulting groups pseudo-terms since </context>
<context position="4885" citStr="Jansen et al. (2010)" startWordPosition="738" endWordPosition="741"> typically hundreds of hours of manually transcribed speech. Moreover, the terms that may be most distinctive in particular spoken documents often lie outside the predefined vocabulary of an off-the-shelf LVCSR system. This means that unlike with text, where many tools can be applied to new languages and domains with minimal effort, the equivalent tools for speech corpora often require a significant investment. This greatly raises the entry threshold for constructing even a minimal tool set for speech corpora analysis. The paper proceeds as follows. After a review of related work, we describe Jansen et al. (2010), a method for finding repetitions in speech. We then explain how these repetitions are grouped into pseudo-terms. Document clustering and classification work surprisingly well on pseudo-terms; performance on a Switchboard task approaches a baseline based on gold standard manual transcriptions. 2 Related Work In the low resource speech recognition regime, most approaches have focused on coupling small amounts of orthographically transcribed speech (10s of hours) with much larger collections of untranscribed speech (100s or 1000s of hours) to train accurate acoustic models with semi-supervised </context>
<context position="6525" citStr="Jansen et al., 2010" startWordPosition="989" endWordPosition="992">oustic models improves the universality of the architecture, small amounts of orthographic transcription are still required to connect the SOUs with the lexicon. The segmental dynamic time warping (S-DTW) algorithm (Park and Glass, 2008) was the first truly zero resource effort, designed to discover portions of the lexicon directly by searching for repeated acoustic patterns in the speech signal. This work implicitly defined a new direction for speech processing research: unsupervised spoken term discovery, the entry point of our speech corpora analysis system. Subsequent extensions of S-DTW (Jansen et al., 2010) permit applications to much larger speech collections, a flexibility that is vital to our efforts. As mentioned above, the application of NLP methods to speech corpora have traditionally relied on high resource ASR systems to provide automatic word or phonetic transcripts. Spoken document topic classification has been an application of particular interest (Hazen et al., 2007), for which the recognized words or phone n-grams are used to characterize the documents. These efforts have produced admirable results, with ASR transcript-based performance approached that obtained using the gold standa</context>
</contexts>
<marker>Jansen, Church, Hermansky, 2010</marker>
<rawString>Aren Jansen, Kenneth Church, and Hynek Hermansky. 2010. Towards spoken term discovery at scale with zero resources. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In European Conference on Machine Learning (ECML).</booktitle>
<contexts>
<context position="36465" citStr="Joachims, 1998" startWordPosition="5958" endWordPosition="5959">t tuning data, before generating the final representations for the evaluation once the optimal parameters have been selected. For learning we require a multi-class classifier training algorithm. We evaluated four popular learning algorithms: a) MIRA—a large margin online learning algorithm (Crammer et al., 2006); b) Confidence Weighted (CW) learning—a probabilistic large margin online learning algorithm (Dredze et al., 2008; Crammer et al., 2009); c) Maximum Entropy—a log-linear discriminative classifier (Berger et al., 1996); and d) Support Vector Machines (SVM)—a large margin discriminator (Joachims, 1998).5 For each experiment, we used default settings of the parameters (tuning did not significantly change the results) and 10 online iterations for the online methods (MIRA, CW). Each reported result is based on 10-fold cross validation. Table 6 shows results for various parameter settings and the four learning algorithms on development data. As before, we observe that values for T &gt; 0.9 tend to do well. The CW learning algorithm performs the best on this data, followed by Maximum Entropy, MIRA and SVM. The optimal n for classification is 0.75, close to the 0.7 value selected in clustering. As b</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In European Conference on Machine Learning (ECML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Karypis</author>
</authors>
<title>CLUTO: A software package for clustering high-dimensional data sets.</title>
<date>2003</date>
<tech>Technical Report 02-017,</tech>
<institution>University of Minnesota, Dept. of Computer Science.</institution>
<contexts>
<context position="27084" citStr="Karypis (2003)" startWordPosition="4477" endWordPosition="4478">ed recall operates in a similar fashion, but it measures what percentage of all examples that share the same label as the selected example will appear in the selected cluster. Since BCubed averages its evaluation over each document and not each cluster, it is less sensitive to small errors in large clusters as opposed to many small errors in small clusters. We include results for B-Cubed F1, the harmonic mean of precision and recall. 6.2 Clustering Algorithms We considered several clustering algorithms: repeated bisection, globally optimal repeated bisection, and agglomerative clustering (see Karypis (2003) for implementation details). Each bisection algorithm is run 10 times and the optimal clustering is selected according to a provided criteria function (no true labels needed). For each clustering method, we evaluated several criteria functions. Additionally, we considered different scalings of the feature values (the number of times the pseudo-terms appear in each document). We found that scaling each feature by the inverse document frequency, effectively TFIDF, produced the best results, so we use that scaling in all of our experiments. We also explored various similarity metrics and found c</context>
</contexts>
<marker>Karypis, 2003</marker>
<rawString>George Karypis. 2003. CLUTO: A software package for clustering high-dimensional data sets. Technical Report 02-017, University of Minnesota, Dept. of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Alex Park</author>
<author>Regina Barzilay</author>
<author>James Glass</author>
</authors>
<title>Making Sense of Sound: Unsupervised Topic Segmentation Over Acoustic Input.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7293" citStr="Malioutov et al., 2007" startWordPosition="1107" endWordPosition="1110">thods to speech corpora have traditionally relied on high resource ASR systems to provide automatic word or phonetic transcripts. Spoken document topic classification has been an application of particular interest (Hazen et al., 2007), for which the recognized words or phone n-grams are used to characterize the documents. These efforts have produced admirable results, with ASR transcript-based performance approached that obtained using the gold standard manual transcripts. Early efforts to perform automatic topic segmentation of speech input without the aid of ASR systems have been promising (Malioutov et al., 2007), but have yet to exploit the full the range of NLP tools. 3 Identifying Matched Regions Our goal is to identify pairs of intervals within and across utterances of several speakers that contain the same linguistic content, preferably meaningful words or terms. The spoken term discovery algorithm of Jansen et al. (2010) efficiently searches the space of (2) intervals, where n is the number of speech frames.1 Jansen et al. (2010) is based on dotplots (Church and Helfman, 1993), a method borrowed from bioinformatics for finding repetitions in DNA sequences. 1Typically, each frame represents a 25 </context>
</contexts>
<marker>Malioutov, Park, Barzilay, Glass, 2007</marker>
<rawString>Igor Malioutov, Alex Park, Regina Barzilay, and James Glass. 2007. Making Sense of Sound: Unsupervised Topic Segmentation Over Acoustic Input. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Novotney</author>
<author>Richard Schwartz</author>
</authors>
<title>Analysis of low-resource acoustic model self-training. In Interspeech.</title>
<date>2009</date>
<contexts>
<context position="5522" citStr="Novotney and Schwartz, 2009" startWordPosition="832" endWordPosition="835">d for finding repetitions in speech. We then explain how these repetitions are grouped into pseudo-terms. Document clustering and classification work surprisingly well on pseudo-terms; performance on a Switchboard task approaches a baseline based on gold standard manual transcriptions. 2 Related Work In the low resource speech recognition regime, most approaches have focused on coupling small amounts of orthographically transcribed speech (10s of hours) with much larger collections of untranscribed speech (100s or 1000s of hours) to train accurate acoustic models with semi-supervised methods (Novotney and Schwartz, 2009). In these efforts, the goal is to reduce the annotation requirements for the construction of competent LVCSR systems. This semi-supervised paradigm was relaxed even further with the pursuit of self organizing units (SOUs), phone-like units for which acoustic models are trained with completely unsupervised methods (Garcia and Gish, 2006). Even though the move away from phonetic acoustic models improves the universality of the architecture, small amounts of orthographic transcription are still required to connect the SOUs with the lexicon. The segmental dynamic time warping (S-DTW) algorithm (P</context>
</contexts>
<marker>Novotney, Schwartz, 2009</marker>
<rawString>Scott Novotney and Richard Schwartz. 2009. Analysis of low-resource acoustic model self-training. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Park</author>
<author>James R Glass</author>
</authors>
<title>Unsupervised pattern discovery in speech.</title>
<date>2008</date>
<journal>IEEE Transactions ofAudio, Speech, and Language Processing.</journal>
<contexts>
<context position="6142" citStr="Park and Glass, 2008" startWordPosition="928" endWordPosition="931">). In these efforts, the goal is to reduce the annotation requirements for the construction of competent LVCSR systems. This semi-supervised paradigm was relaxed even further with the pursuit of self organizing units (SOUs), phone-like units for which acoustic models are trained with completely unsupervised methods (Garcia and Gish, 2006). Even though the move away from phonetic acoustic models improves the universality of the architecture, small amounts of orthographic transcription are still required to connect the SOUs with the lexicon. The segmental dynamic time warping (S-DTW) algorithm (Park and Glass, 2008) was the first truly zero resource effort, designed to discover portions of the lexicon directly by searching for repeated acoustic patterns in the speech signal. This work implicitly defined a new direction for speech processing research: unsupervised spoken term discovery, the entry point of our speech corpora analysis system. Subsequent extensions of S-DTW (Jansen et al., 2010) permit applications to much larger speech collections, a flexibility that is vital to our efforts. As mentioned above, the application of NLP methods to speech corpora have traditionally relied on high resource ASR s</context>
</contexts>
<marker>Park, Glass, 2008</marker>
<rawString>Alex Park and James R. Glass. 2008. Unsupervised pattern discovery in speech. IEEE Transactions ofAudio, Speech, and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: Evaluating nonexpert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A.Y. Ng. 2008. Cheap and fast—but is it good?: Evaluating nonexpert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 254–263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Thomas</author>
<author>S Ganapathy</author>
<author>H Hermansky</author>
</authors>
<title>Phoneme recognition using spectral envelope and modulation frequency features.</title>
<date>2009</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="21057" citStr="Thomas et al. (2009)" startWordPosition="3467" endWordPosition="3470">r regions to be merged, increasing the total number of pseudo-terms. Table 2 shows how these parameters change the number of pseudoterms (features) per document and the average number of occurrences of each pseudo-term. The user could tune these parameters to select pseudo-terms that were long and occurred in many documents. In the next sections, we consider how these parameters effect performance of various learning settings. To provide the requisite speaker independent acoustic model, we compute English phone posteriorgrams using the multi-stream multi-layer perceptron-based architecture of Thomas et al. (2009), trained on 300 hours of conversational telephone speech. While this is admittedly a large amount of supervision, it is important to emphasize our zero resource term discovery algorithm does not rely on the phonetic interpretability of this reference acoustic model. The only requirement is that the same target language phoneme spoken by distinct speakers map to similar posterior distributions over the reference language phoneme set. Thus, even though we evaluate the system on matchedlanguage Switchboard data, it can be just as easily applied to any target language with no languagespecific kno</context>
</contexts>
<marker>Thomas, Ganapathy, Hermansky, 2009</marker>
<rawString>S. Thomas, S. Ganapathy, and H. Hermansky. 2009. Phoneme recognition using spectral envelope and modulation frequency features. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Zeng</author>
<author>Q C He</author>
<author>Z Chen</author>
<author>W Y Ma</author>
<author>J Ma</author>
</authors>
<title>Learning to cluster web search results.</title>
<date>2004</date>
<booktitle>In Conference on Research and development in information retrieval (SIGIR).</booktitle>
<contexts>
<context position="23022" citStr="Zeng et al., 2004" startWordPosition="3793" endWordPosition="3796">ed for different settings of the match duration κ and the overlap threshold τ. 6 Document Clustering We begin by considering document clustering, a popular approach to discovering latent structure in document collections. Unsupervised clustering algorithms sort examples into groups, where each group contains documents that are similar. A user exploring a corpus can look at a few documents in each cluster to gain an overview of the content discussed in the corpus. For example, clustering methods can be used on search results to provide quick insight into the coverage of the returned documents (Zeng et al., 2004). Typically, documents are clustered based on a bag of words representation. In the case of clustering conversations in our collection, we would normally obtain a transcript of the conversation and then extract a bag of words representation for clustering. The resulting clusters may represent topics, such as the six topics used in our switchboard data. Such groupings, available with no topic labeled training data, can be a valuable tool for understanding the contents of a speech data collection. We would like to know if similar clustering results can be obtained without the use of a manual or </context>
</contexts>
<marker>Zeng, He, Chen, Ma, Ma, 2004</marker>
<rawString>H.J. Zeng, Q.C. He, Z. Chen, W.Y. Ma, and J. Ma. 2004. Learning to cluster web search results. In Conference on Research and development in information retrieval (SIGIR).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>