<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.954515">
Formatting Time-Aligned ASR Transcripts for Readability
</title>
<author confidence="0.849695">
Maria Shugrina∗
</author>
<affiliation confidence="0.63076">
Google Inc.
</affiliation>
<address confidence="0.852248">
New York, NY 10011
</address>
<email confidence="0.988175">
shumash@google.com
</email>
<sectionHeader confidence="0.993551" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893571428571">
We address the problem of formatting the out-
put of an automatic speech recognition (ASR)
system for readability, while preserving word-
level timing information of the transcript. Our
system enriches the ASR transcript with punc-
tuation, capitalization and properly written
dates, times and other numeric entities, and
our approach can be applied to other format-
ting tasks. The method we describe combines
hand-crafted grammars with a class-based lan-
guage model trained on written text and relies
on Weighted Finite State Transducers (WF-
STs) for the preservation of start and end time
of each word.
</bodyText>
<sectionHeader confidence="0.975106" genericHeader="categories and subject descriptors">
1 Introduction and Prior Work
</sectionHeader>
<bodyText confidence="0.940515260869565">
The output of a typical ASR system lacks punctua-
tion, capitalization and proper formatting of entities
such as phone numbers, time expressions and dates.
Even if such automatic transcript is free of recogni-
tion errors, it is difficult for a human to parse. The
proper formatting of the transcript gains particular
importance in applications where the user relies on
ASR output for information and where information-
rich numeric entities (e.g. time expressions, mone-
tary amounts) are common. A good example of such
application is a voicemail transcription system. The
goal of our work is to transform the raw transcript
into its proper written form in order to optimize it for
the visual scanning task by the end user. We present
quantitative and qualitative evaluation of our system
with a focus on numeric entity formatting, punctua-
tion and capitalization (See Fig. 1).
Apart from text, the ASR output usually con-
tains word-level metadata such as time-alignment
and confidence. Such quantities may be useful for a
variety of applications. Although simple to recover
Thank you to Michiel Bacchiani, Martin Jansche, Michael
Riley and Cyril Allauzen for discussion and support.
</bodyText>
<subsectionHeader confidence="0.589616">
Raw Transcript:
</subsectionHeader>
<bodyText confidence="0.977044">
hi bill it’s tracy at around three thirty P M just got
an apartment for one thousand three thirty one thou-
sand four hundred a month my number is five five five
eight eight eight eight extension is three thirty bye
</bodyText>
<subsectionHeader confidence="0.827855">
Our Result:
</subsectionHeader>
<bodyText confidence="0.850114666666667">
Hi Bill, it’s Tracy at around 3:30 PM, just got an
apartment for 1,330 1,400 a month. My number is
555-8888 extension is 330. Bye.
</bodyText>
<figureCaption confidence="0.995335">
Figure 1: An example of a raw transcript with ambiguous
written forms and the output of our formatting system.
</figureCaption>
<bodyText confidence="0.999841482758621">
via word alignment after some types of formatting,
word-level quantities may be difficult to preserve if
the original text has undergone a significant transfor-
mation. We present a formal and general augmen-
tation of our WFST-based technique that preserves
word-level timing and confidence information dur-
ing arbitrary formatting.
The problems of sentence boundary detection and
punctuation of transcripts have received a substantial
amount of attention, e.g. (Beeferman et al., 1998;
Shriberg et al., 2000; Christensen et al., 2001; Liu
et al., 2006; Gravano et al., 2009). Capitalization of
ASR transcripts received less attention (Brown and
Coden, 2002; Gravano et al., 2009), but there has
also been work on case restoration in the context
of machine translation (Chelba and Acero, 2006;
Wang et al., 2006). Our work does not propose
competing methods for transcript punctuation and
capitalization. Instead, we aim to provide a com-
mon framework for a wide range of formatting tasks.
Our method extends the approach of Gravano et al.
(2009) with a general WFST formulation suitable
for formatting monetary amounts, time expressions,
dates, phone numbers, honorifics and more, in addi-
tion to punctuation and capitalization.
To our knowledge, this scope of the problem has
not been addressed in literature. Yet such format-
ting can have a high impact on transcript readabil-
ity. In this paper we focus on numeric entity format-
</bodyText>
<page confidence="0.991241">
198
</page>
<note confidence="0.7017385">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 198–206,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.5971934">
ting. In general, context independent rules fail to
adequately perform this task due to its inherent am-
biguity (See Fig. 1). For example, the spoken words
“three thirty” should be written differently in these
three contexts:
</bodyText>
<listItem confidence="0.999757333333333">
• meet meat 3:30
• you owe me 330
• dinner for three 30 minutes later
</listItem>
<bodyText confidence="0.999989103448276">
The proper written form of a numeric entity depends
on its class (time, monetary amount, etc). In this
sense, formatting is related to the problem of named
entity (NE) detection and value extraction, as de-
fined by MUC-7 (Chinchor, 1997). Several authors
have considered the problem of NE value extraction
from raw transcripts (Huang et al., 2001; Jansche
and Abney, 2002; B´echet et al., 2004; Levit et al.,
2004). This is an information extraction task that in-
volves identifying transcript words corresponding to
a particular NE class and extracting an unambigu-
ous value of that NE (e.g. the value of the date
NE “december first oh nine” is “12/01/2009”). Al-
though relevant, this information extraction does not
directly address the problem of proper formatting
and ordinarily requires a tagged corpus for training.
A parallel corpus containing raw transcriptions
and the corresponding formatted strings would facil-
itate the solution to the transcript formatting prob-
lem. However, there is no such corpus available.
Therefore, we follow the approach of Gravano et al.
and provide an approximation that exploits readily
available written text instead. In section 2 we de-
tail our method, provide a probabilistic interpreta-
tion and present a practical formulation of the solu-
tion in terms of WFSTs. Section 3 shows how to
augment the WFST formulation to preserve word-
level timing and confidence. Section 4 presents both
qualitative and quantitative evaluation of our system.
</bodyText>
<sectionHeader confidence="0.944802" genericHeader="method">
2 Method
</sectionHeader>
<bodyText confidence="0.9999535">
First, handwritten grammars are used to generate
all plausible written forms. These variants are then
scored with a language model (LM) approximating
probability over written strings. To overcome data
sparsity associated with written numeric strings, we
introduce numeric classes into the LM. In section
2.1 we give a probabilistic formulation of this ap-
proach. In section 2.2 we comment on the hand-
written grammars, and in section 2.3 we discuss the
class-based language model used for scoring. Sec-
tion 2.4 provides the WFST formulation of the solu-
tion.
</bodyText>
<subsectionHeader confidence="0.99395">
2.1 Probabilistic Formulation
</subsectionHeader>
<bodyText confidence="0.9998705">
The problem of estimating the best written form w�
of a spoken sequence of words s can be formulated
as a Machine Translation (MT) problem of translat-
ing a string s from the language of spoken strings
into a language of written strings. From a statistical
standpoint, w� can be estimated as follows:
</bodyText>
<equation confidence="0.782429">
w� = argmax {P(w|s)} ≈ argmax {P′(s|w)P′(w)},
w w
</equation>
<bodyText confidence="0.999993777777778">
where P(·) denotes probability, and P′(·) a prob-
ability approximation. The probability over written
strings P(w) can be estimated by training an n-gram
language model on amply available written text. The
absence of a parallel corpus containing sequences of
spoken words and their written renditions makes the
conditional distribution P(s|w) impossible to esti-
mate. An approximation P′(s|w) can be obtained by
defining handwritten grammars that generate multi-
ple unweighted written variants for any spoken se-
quence. For a given s, a collection of grammars en-
codes a uniform probability distribution across the
set of all written variants generated for s and as-
signs a zero probability to any string not in this set.
Such grammar-based modeling of P(s|w) combined
with statistical estimation of P(w) takes advantage
of prior knowledge, but does not share the disadvan-
tages of rigid, fully rule-based systems.
</bodyText>
<subsectionHeader confidence="0.999274">
2.2 Handwritten Grammars
</subsectionHeader>
<bodyText confidence="0.9996596">
Handwritten grammars G1...Gm are used to gener-
ate unweighted written variants for a raw string s. In
Gravano’s work (Gravano et al., 2009) the generated
variants include optional punctuation between every
two words and an optional capitalization for every
word. Our system supports a wider range of vari-
ants, including but not limited to multiple variants
of number formatting.
The handwritten grammars can be very restrictive
or very liberal, depending on the application require-
ments. For example, a grammar we use to generate
punctuation and capitalization only generates sen-
tences with the first word capitalized. This enforces
conventions and consistency, which the best scor-
ing variant could occasionally violate. On the other
</bodyText>
<page confidence="0.998935">
199
</page>
<figureCaption confidence="0.758372">
Figure 2: An FSA encoding all variants generated by the
number grammar for a spoken string “three thirty”.
</figureCaption>
<bodyText confidence="0.988926454545455">
hand, the grammar for number formatting could
be very liberal in producing written variants (See
Fig. 2). Jansche and Abney (2002) observe that
handwritten rules deterministically tagging numeric
strings of certain length as phone numbers perform
surprisingly well on phone number NE identification
in voicemail. If appropriate to the task, determinis-
tic grammars can be incorporated into the grammar
stack. The unweighted written variants generated by
applying G1...G,,,, to s are then scored with the lan-
guage model.
</bodyText>
<subsectionHeader confidence="0.990018">
2.3 Language Model
</subsectionHeader>
<bodyText confidence="0.99991788">
The probability distribution over written text P(w)
can be approximated by a Katz back-off n-gram lan-
guage model trained on written text in a domain
semantically similar to the domain for which the
ASR engine is deployed. Unlike some of the ap-
proaches used for NE identification (Jansche and
Abney, 2002; Levit et al., 2004) and sentence bound-
ary detection (Christensen et al., 2001; Shriberg et
al., 2000; Liu et al., 2006), LM-based scoring can-
not exploit a larger context than n tokens or prosodic
features. The advantage of the LM approach is the
ease of applying it to new formatting tasks: no new
tagged corpus, and only trivial changes to the pre-
processing of the training text would be required.
If the LM is to score written numeric strings, care
must be taken in modeling numbers. Representing
each written number as a token (e.g. tokens “1,235”,
“15”) during training results in a very large model
and suffers from data sparsity even with very large
training corpora. An alternative approach of model-
ing every digit as a token (e.g. “15” is comprised of
tokens “1” and “2”) fails to model sufficient context
for longer digit strings. A partially class-based LM
remedies the drawbacks of both approaches, and has
been used for tasks such as NE tagging (B´echet et
</bodyText>
<table confidence="0.957484666666667">
Class Set A
Numeric range Interpretation
2-9 single digits
10-12 up to hour in a 12-hour system
13-31 up to the largest day of the month
32-59 up to the largest minute in a time
expression
other 2-digit all other 2-digit numbers
other 3-digit all 3-digit numbers
1900 - 2099 common year numbers
other 4-digit all other 4-digit numbers
10000-99999 all 5-digit numbers; e.g. US zip-
codes
&gt; 100000 all large numbers
Class Set B
Numeric range Interpretation
0-9 one digit string
10-99 two digit string
... ...
109 − (1010 − 1) ten-digit string
&gt; 1010 longer digit string
</table>
<tableCaption confidence="0.995413">
Table 1: Two sets of number classes used in our system.
</tableCaption>
<bodyText confidence="0.840503851851852">
Each sequence of consecutive digit characters is mapped
to the appropriate class. For example, “$1,235.12” would
become “(dollar) 1 (comma) (num 100 999) (period)
(num 10 12)” in Class Set A and “(dollar) (num 1D)
(comma) (num 3D) (period) (num 2D) in Class Set B.
al., 2004). The generalization provided by classes
eliminates data sparsity, and is able to model suffi-
cient context.
We experiment with two sets of classes (See Ta-
ble 1). Class Set B, based on (B´echet et al., 2004),
marks strings of n consecutive digits as belonging to
an n-digit class, assuming nothing about the num-
ber distribution. Class Set A is based on intuition
about number distribution in text (See Table 1, Inter-
pretation). In section 4.4 we show that Class Set A
achieves better performance on number formatting.
Now that it is established that the choice of classes
affects performance, future research could focus on
finding an optimal set of number classes automat-
ically. Clustering techniques, often used to derive
class definitions from training text, could be applied.
Although more punctuation marks could be con-
sidered, we focus on periods and commas. Similarly
to Gravano et al. (2009), we map all other punctua-
tion marks in the training text to these two. In many
formatting scenarios (e.g. spelled out acronyms, nu-
meric ranges), spaces are ambiguous and significant,
</bodyText>
<figure confidence="0.994761466666667">
three
2
&lt;space&gt;
0
3
1
&lt;space&gt;
&lt;column&gt;
&lt;period&gt;
3
3 0
thirty
5
ε
4
</figure>
<page confidence="0.970723">
200
</page>
<bodyText confidence="0.997414333333333">
and it is therefore important to consider whitespace
when scoring the written variants. Because of this,
we model space as a token in the LM.
</bodyText>
<subsectionHeader confidence="0.984126">
2.4 WFST Formulation
</subsectionHeader>
<bodyText confidence="0.9961865">
The one-best1 ASR output s can be represented by a
Finite State Acceptor (FSA) S. We describe a series
of standard WFST operations on S resulting in the
FSA Wbest encoding the best estimated formatted
variant w. Current section assumes familiarity with
WFSTs; for background see (Mohri, 2009).
</bodyText>
<figure confidence="0.99708725">
(b) W variants FST
(c) Wout FSA
(d) Wclass FST
(e) Wbest FSA
</figure>
<figureCaption confidence="0.9991005">
Figure 3: An example showing transducers produced dur-
ing formatting.
</figureCaption>
<bodyText confidence="0.9998424">
We encode each grammar Gi as an unweighted
FST Ti that transduces the raw transcript to its for-
matted versions. The necessity to encode them
as FSTs restricts the set of grammars to regular
grammars (Hopcroft and Ullman, 1979), sufficiently
powerful for most formatting tasks. The back-off
n-gram LM is naturally represented as a weighted
deterministic FSA G with negative log probability
weights (Mohri et al., 2008). The deterministic map-
ping of digit strings to number class tokens can also
</bodyText>
<footnote confidence="0.899688">
1This WFST formulation can also be applied to the ASR lat-
tice or n-best list with some modification to the scoring phase.
</footnote>
<bodyText confidence="0.999693375">
be accomplished by an unweighted transducer K,
which passes all non-numeric strings unchanged.
Composing the input acceptor S with the gram-
mar transducers Ti results in a transducer W with
all written variants on the output. Projected onto its
output labels, W becomes an acceptor Wout. Wclass,
the result of the composition of Wout with K, has
all formatted written variants on the input side and
the formatted variants with digit strings replaced by
class tokens on the output. The output side of Wclass
can then be scored via composition with G to pro-
duce a weighted transducer Wscored. The shortest
path in the Tropical Semiring on Wscored contains
the estimate of the best written variant on the input
side. This algorithm can be summarized as follows
(See Fig. 3):
</bodyText>
<listItem confidence="0.9949334">
1. W=SoT1oT2...oTm
2. Wout = Projout(W)
3. Wclass = Wout o K
4. Wscored = Wclass o G
5. Wbest = Projin(BestPath(Wscored))
</listItem>
<bodyText confidence="0.999991111111111">
where o denotes FST composition, Projin and
Projout denote projection on input and output la-
bels respectively, and BestPath(X) as a function
returning an FST encoding the shortest path of X.
The key Step 2 ensures that the target written vari-
ants are not consumed in the consequent composi-
tion operations. For efficiency reasons it is advisable
to apply optimizations such as epsilon removal and
determinization to the intermediate results. 2
</bodyText>
<sectionHeader confidence="0.877419" genericHeader="method">
3 Preserving Word-Level Metadata
</sectionHeader>
<bodyText confidence="0.9999965">
We extend the WFST formulation to preserve word-
level timing and confidence information.
</bodyText>
<subsectionHeader confidence="0.99336">
3.1 Background
</subsectionHeader>
<bodyText confidence="0.99943125">
A WFST is a finite set of states and transitions
connecting them. Each transition has an input la-
bel, an output label and a weight in some semir-
ing K. A semiring is informally defined as a tou-
ple (K, ®, (9, 0, 1), where K is the set of elements,
® and ® are the addition and multiplication opera-
tions, 0 is the additive identity and multiplicative an-
nihilator, 1 is the multiplicative identity (See (Mohri,
</bodyText>
<footnote confidence="0.9723055">
2Our system implements proper failure transitions available
in the OpenFST Library (Allauzen et al., 2007).
</footnote>
<figure confidence="0.921639">
(a) S FSA
</figure>
<page confidence="0.988352">
201
</page>
<bodyText confidence="0.998574333333333">
2009)). By defining new semirings we can use stan-
dard FST operations to accomplish a wide range of
goals.
</bodyText>
<subsectionHeader confidence="0.999793">
3.2 Timing Semiring
</subsectionHeader>
<bodyText confidence="0.9999935">
In order to formulate time preservation within the
FST formalism, we define the timing semiring Kt
where each element is a pair (s, e) that can be inter-
preted as the start and end time of a word:
</bodyText>
<equation confidence="0.99885975">
Wt = {(s, e) : s,e C R&apos; U {0,oo11
(s1, e1) ® (s2, e2) = (max(s1, s2), min(e1, e2))
(s1, e1) ® (s2, e2) = (min(s1, s2), max(e1, e2))
0 = (0, oo) 1 = (oo, 0)
</equation>
<bodyText confidence="0.9998266875">
Intuitively, the addition operation takes the largest
interval contained by both operand intervals, while
multiplication returns the smallest interval fully con-
taining both operand intervals. 3 This definition ful-
fills all the semiring properties as defined in (Mohri,
2009). Note that encoding only the duration of each
word is not sufficient, as there may be time gaps
between the words due to the segmentation of the
source audio. Let S denote the Weighted Finite
State Acceptor (WFSA) encoding the raw ASR out-
put with the start and end time stored in the weight
of each arc.
In order to preserve word-level confidence in ad-
dition to timing information, a Cartesian product of
Kt and the Log semiring can be used to store both
time and confidence in an arc weight.
</bodyText>
<subsectionHeader confidence="0.99857">
3.3 Weight Synchronization
</subsectionHeader>
<bodyText confidence="0.995257777777778">
The goal is to associate the timing/confidence
weights of S with the word labels of Wbest, the best
formatted string (See Sec. 2.4). Because the weight
of each transition in S already expresses the tim-
ing/confidence corresponding to its word label, it is
sufficient to associate the labels of S with the labels
of Wbest. This is equivalent to identifying the output
labels to which each input label is transduced during
Step 1 in section 2.4. However, in general WFST op-
erations may desynchronize input and output labels
3Note that this is just a Cartesian product of min-max and
max-min semirings. The elements of Kt are not proper inter-
vals, as it is possible for s to exceed e.
and weights, as the FST structure itself does not in-
dicate a semantic correspondence between them. To
alleviate this, we guarantee such a correspondence
in our grammars by enforcing that for all paths in
any grammar FST Ti:
</bodyText>
<listItem confidence="0.961653">
• an input label appears before any of the corre-
sponding output labels, and
• output labels corresponding to a given input la-
bel appear before the next input label.
</listItem>
<bodyText confidence="0.998186333333333">
In practice, these assumptions are usually met by
handwritten grammars. Even if these assumptions
are violated for a small number of paths, only small
word-level timing discrepancies will be incurred.
Each path in W can be thought of as a sequence of
subpaths with only the first transition containing a
non-ǫ input label. We say that the input label of each
such subpath corresponds to that subpath’s output la-
bels.
</bodyText>
<figureCaption confidence="0.999327666666667">
Figure 4: A small example of time preservation section of
the algorithm. Arcs with non-unity timing weights show
parenthesized pair of start and end time.
</figureCaption>
<bodyText confidence="0.998901285714286">
The best path that has input labels corresponding
to the raw ASR output can be obtained by compos-
ing the variants FST W with the best formatted FSA
Wbest and picking any path. The timing weights are
restored to by composing the weighted S with this
result. To preserve timing we add two more steps to
Steps 1–5 in section 2.4:
</bodyText>
<listItem confidence="0.9799075">
6. Wraw:best = RmEps(AnyPath(W o Wbest))
7. Wbest = S o Mapt(Wraw:best)
</listItem>
<bodyText confidence="0.991669">
where RmEps(X) applies the epsilon-removal al-
gorithm to X (Mohri, 2009), and Mapt(X) maps
</bodyText>
<figure confidence="0.996453333333333">
ten/(1,2) &lt;sp&gt; six/(3,4)
0 1 2 3
1 0 &lt;sp&gt; 6
0 1 2 3 4
(a) S FSA
(b) Wbest FSA
ten:1 ε:0 &lt;sp&gt;:&lt;sp&gt; six:6
0 1 2 3 4
(c) Wr.:best FST
ten:1/(1,2) ε:0 &lt;sp&gt;:&lt;sp&gt; six:6/(3,4)
0 1 2 3 4
(d) Wbest FST
</figure>
<page confidence="0.995507">
202
</page>
<bodyText confidence="0.999968857142857">
all non-zero weights of X to the unity weight in the
timing semiring. Because S is an epsilon-free accep-
tor, the result Wbest will contain the original weights
of S on the arcs with the corresponding input labels
(See Fig. 4 for an example). The space-delimited
words and the corresponding weights can then be
read off by walking �Wbest.
</bodyText>
<sectionHeader confidence="0.998743" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999834555555556">
Section 4.1 presents our datasets and an evaluation
metric specific to number formatting, and section
4.2 describes our experimental system. We present
quantitative evaluation of capitalization/punctuation
performance and number formatting performance
separately in sections 4.3 and 4.4. Because the ul-
timate goal of our work is to improve the readability
of ASR transcripts, we also present the result of a
user study of transcript readability in section 4.5.
</bodyText>
<subsectionHeader confidence="0.991994">
4.1 Data and Metrics
</subsectionHeader>
<bodyText confidence="0.999335625">
The training corpus contains 185M tokens of written
text normalized to contain only comma and period
punctuation marks. A set of 176M tokens (TRS) is
used for training and a set of 7M tokens (PTS) is
held back for testing punctuation and capitalization
(See Table 3). To obtain a test input (NPTS) for our
system, PTS is lowercased and all punctuation is re-
moved.
</bodyText>
<table confidence="0.997658">
words commas periods capitals
TRS 176M 10.6M 11.8M 24.3M
PTS 7M 420K 440K 880K
</table>
<tableCaption confidence="0.999964">
Table 3: Training set TRS and test set PTS.
</tableCaption>
<bodyText confidence="0.989390428571429">
Number formatting is evaluated on a manually
formatted test set. We manually processed the set
of raw manual transcripts (NNTS) from the LDC
Voicemail Part I training set (Padmanabhan et al.,
1998) to obtain a reference number formatting set
(NTS). All numeric entities in NTS were formatted
according to the following conventions:
</bodyText>
<listItem confidence="0.969952923076923">
• all quantities under 10 are spelled out
• time is written in a 12-hour system as “xx:xx”
or “xx”
• dollar amounts are written as “$x,xxx.xx” with
cents included if spoken
• US phone numbers are written as “(xxx) xxx-
xxxx” or “xxx-xxxx”
• other phone numbers are written as digit strings
• decimals are written as “x.x”
• large amounts include commas: “x,xxx,xxx”
All contiguous sequences of words in NTS that
could be a target for number formatting were marked
as numeric entities, whether or not these words were
</listItem>
<bodyText confidence="0.896562571428572">
formatted by the labeler (for example “six” is a nu-
meric entity). To evaluate number formatting per-
formance, we process NNTS with our full experi-
mental system, then remove all capitalization and
inter-word punctuation. This result is aligned with
NTS, and each entity is scored separately as totally
correct or totally incorrect (See Table 2), yielding:
</bodyText>
<subsectionHeader confidence="0.911948">
Numeric Entity Error Rate = 100 · N
</subsectionHeader>
<bodyText confidence="0.9995724">
where I is the count of entities that did not match
the reference entity string exactly and N is the total
entity count. This error rate is independent of the
numeric entity density in the test set. The errors are
broken down into three types:
</bodyText>
<listItem confidence="0.766117692307692">
• incorrect formatting - when the system incor-
rectly formats an entity that is formatted in the
reference
• overformatting - when the system formats an
entity that stays unformatted in the reference
• underformatting - when the system does not
format an entity formatted in the reference
Out of 1801 voicemail transcripts in NTS, 1347 con-
tain at least one entity for a total of 3563 entities,
signifying a frequent occurrence of numeric entities
in voicemail. There is an average of 7 raw transcript
words per entity, suggesting that in many cases en-
tity formatting is non-trivial.
</listItem>
<subsectionHeader confidence="0.99587">
4.2 Experimental System
</subsectionHeader>
<bodyText confidence="0.999813857142857">
The experimental system includes a 5-gram LM
trained on TRS with spaces treated as tokens. Num-
ber evaluation is performed with two sets of number
classes, listed in Table 1. System A contains LM
with classes from set A, and System B contains LM
with classes from set B. The experimental setup also
includes the following grammars:
</bodyText>
<listItem confidence="0.995517222222222">
• Gphone - deterministically formats as a phone
number any string spoken like a US 7 or 10
digit phone number
• Gnumber - expands all spoken numbers to a full
range of variants, with support for time expres-
sions, ordinals, decimals, dollar amounts
• Gcap punct - generates all possible combina-
tions of commas, periods and capitals; always
capitalizes the first word of a sentence
</listItem>
<page confidence="0.999082">
203
</page>
<note confidence="0.92018625">
Raw: for six people at five five thirty cost is eleven hundred dollars
Ref: for six people at 5 5:30 cost is $1,100
Hyp: for 6 people at 5 5:30 cost is 11 $100
Score: - incorrect - correct - incorrect
</note>
<tableCaption confidence="0.977645">
Table 2: A example of a raw transcript, reference transcript with number formatting and the hypothesis produced by
the system. The entities (bold) in reference and hypothesis are aligned and scored.
</tableCaption>
<subsectionHeader confidence="0.999333">
4.3 Evaluation of Punctuation
</subsectionHeader>
<bodyText confidence="0.999391">
To evaluate the performance of capitalization and
punctuation we run System A on NPTS with only
the Gcap punct (in order not to introduce errors due
to numeric formatting). The precision, recall and F-
measure rates for periods, commas and capitals are
computed using PTS as reference (See Fig. 5).
</bodyText>
<table confidence="0.99921125">
Precision Recall F-Measure
Capitals 0.7902 0.5356 0.6385
Comma 0.5527 0.3129 0.3996
Period 0.6672 0.6783 0.6727
</table>
<figureCaption confidence="0.998793">
Figure 5: Punctuation and capitalization results.
</figureCaption>
<bodyText confidence="0.99998475">
It should be noted that a 5-gram language model
that treats spaces as words models the same history
as a 3-gram model that omits the spaces from train-
ing data. When this is taken into account, our re-
sults with a much smaller training set are compara-
ble to Gravano et al. (2009). The F-measure scores
for commas and periods are also comparable to the
prosody-based work of (Christensen et al., 2001),
with the precision of the period slightly lower, but
compensated by recall. Thus, our system can per-
form additional formatting, while retaining a reason-
able capitalization and punctuation performance.
</bodyText>
<subsectionHeader confidence="0.999876">
4.4 Evaluation of Number Formatting
</subsectionHeader>
<bodyText confidence="0.999893428571428">
We evaluate number formatting performance of Sys-
tems A and B, which use different sets of classes for
the language modeling (See Table 1). We process
NNTS with both systems and score against the ref-
erence formatted set NTS to obtain Numeric Entity
Error Rate (NEER). Class Set B naively breaks num-
bers into classes by digit count. System B using this
class set performs worse than System A by 1.7% ab-
solute (See Table 4). In particular, the overformat-
ting rate (OFR) is higher by 1.2% absolute in System
B than in System A. An example of overformatting
is the mis-formatting of the English impersonal pro-
noun “one” as the digit “1”. Such overformatting
errors are much more noticeable than the underfor-
</bodyText>
<table confidence="0.999019857142857">
NEER IFR OFR UFR
System A 16.1% 9.7% 5.4% 1.0%
exact
ignore space 11.2% 4.9% 5.4% 1.0%
System B 17.8% 10.6% 6.6% 0.6%
exact
ignore space 13.2% 6.0% 6.6% 0.6%
</table>
<tableCaption confidence="0.996044">
Table 4: The total NEER score, NEER due to incorrect
</tableCaption>
<bodyText confidence="0.952907833333334">
formatting (IFR), NEER due to overformatting (OFR)
and NEER due to underformatting (UFR); NEER rates
with whitespace errors ignored are also listed.
matting errors, which are higher by 0.4% absolute
in System A. This result shows that the choice of
classes for the class-based LM significantly impacts
number formatting performance. Superior overall
performance of System A suggests that prior knowl-
edge in the choice of classes favorably impacts per-
formance.
In order to estimate the error rate not caused by
whitespace errors, we also compute the NEER with
whitespace errors ignored. It turns out that between
4 and 5% absolute of the errors are whitespace er-
rors. Even if all whitespace errors are significant,
the 83.9% of perfectly formatted entities suggests
that the proposed formatting approach can achieve
good performance on the number formatting task.
</bodyText>
<tableCaption confidence="0.991515">
Table 5: The count of formatted entities in NTS contain-
ing various formatting characters; the counts of these en-
tities correctly formatted by the systems A and B.
</tableCaption>
<bodyText confidence="0.971505">
To estimate how well the systems perform on spe-
cific number formatting tasks we count the number
of reference entities containing certain formatting
characters and compute the number of these enti-
ties correctly formatted by Systems A and B (See
Table 5). The count of different formatting charac-
ters in NTS is small, but still provides an estimate of
the number formatting performance for a real appli-
</bodyText>
<table confidence="0.919263">
entities : . $ ,
Reference totals 3563 310 50 39 17
System A correct 3161 232 36 33 10
System B correct 2923 204 35 31 5
</table>
<page confidence="0.997828">
204
</page>
<bodyText confidence="0.999954857142857">
cation like voicemail transcription. System A per-
forms significantly better on the formatting of time
expressions containing a colon, getting 74.8% cor-
rect. The NEER of System A for entities containing
special formatting characters is under 28% for all
formatting characters except comma, which is used
inconsistently in training text.
</bodyText>
<subsectionHeader confidence="0.980696">
4.5 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999981026315789">
In addition to quantitative evaluation we have con-
ducted a small-scale study of transcript readability.
The study aims to compare raw ASR transcripts,
ASR transcripts formatted by our system and raw
manual transcripts. We have processed LDC Voice-
mail Part 1 with our ASR engine achieving an er-
ror rate of 30%, and have selected 50 voicemails
with error rate under 30% and high informational
content. Messages containing names, addresses and
numbers were preferred. The word error rate on
the selected voicemails is 20%. For each voicemail
we have constructed three semantic multiple-choice
questions, aimed at information extraction. We have
asked each of 15 volunteers to answer all 3 questions
about half of the voicemails. The questions were
shown in sequence, while the transcript remained on
the screen. The transcript for each voicemail was
randomly selected to be ASR raw, ASR formatted
or manual raw. The response time was measured in-
dividually for each question.
The analysis of the responses reveals a statis-
tically significant difference in response time be-
tween formatted and raw ASR transcripts (p = 0.02,
even allowing for per-item and per-subject effects;
see also Fig. 6) and comparable accuracy. The re-
sponse times for formatted ASR were comparable
to the response times for manual unformatted tran-
scripts. This suggests that for transcripts with low
error rates the formatting of the ASR output signif-
icantly impacts readability. This disagrees with a
similar study (Jones et al., 2003), which found no
significant difference in the comprehension rates be-
tween raw ASR transcripts and capitalized, punctu-
ated ASR output with disfluencies removed. This
could be due to a number of factors, including a dif-
ferent type of transformation performed on the ASR
transcript, a different corpus, and a lower word error
rate of transcripts in our user study.
</bodyText>
<figure confidence="0.4956655">
ASR Formatted ASR Raw Manual Raw
90.0% 90.7% 94.4%
</figure>
<figureCaption confidence="0.994628">
Figure 6: The standard R box plot of the response time for
different transcript types and the corresponding accuracy.
</figureCaption>
<sectionHeader confidence="0.996926" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999793615384615">
We present a statistical approach suitable for a wide
range of formatting tasks, including but not lim-
ited to punctuation, capitalization and numeric en-
tity formatting. The average of 2 numeric enti-
ties per voicemail in the manually processed LDC
Voicemail corpus shows that number formatting is
important for applications such as voicemail tran-
scription. Our best system achieves a Numeric En-
tity Error Rate of 16.1% on the ambiguous task of
numeric entity formatting, while retaining capital-
ization and punctuation performance comparable to
other published work. Our algorithm is concisely
formulated in terms of WFSTs and is easily ex-
tended to new formatting tasks without the need for
additional training data. In addition, the WFST for-
mulation allows word-level timing and confidence
to be retained during formatting. In order to over-
come data sparsity associated with written numbers,
we use a class-based language model and show that
the choice of number classes significantly impacts
number formatting performance. Finally, a statis-
tically significant difference in question answering
time for raw and formatted ASR transcripts in our
user study demonstrates the positive impact of the
transcript formatting on the readability of errorful
ASR transcripts.
</bodyText>
<figure confidence="0.9956135">
0 20 40 60 80 100
Time to answer (seconds)
</figure>
<page confidence="0.994237">
205
</page>
<sectionHeader confidence="0.995412" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940417910448">
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. Openfst: A general and efficient
weighted finite-state transducer library. In Proceed-
ings of CIAA, pages 11–23.
F. B´echet, A. Gorin, J. Wright, and D. Hakkani-T¨ur.
2004. Detecting and extracting named entities from
spontaneous speech in a mixed-initiative spoken dia-
logue context: How may i help you? Speech Commu-
nication, 42(2):207–225.
D. Beeferman, A. Berger, and J. Lafferty. 1998. Cyber-
punc: A lightweight punctuation annotation system for
speech. In Proceedings ofICASSP, pages 689–692.
E. Brown and A. Coden. 2002. Capitalization re-
covery for text. In Information Retrieval Techniques
for Speech Applications, pages 11–22, London, UK.
Springer-Verlag.
C. Chelba and A. Acero. 2006. Adaptation of maximum
entropy capitalizer: Little data can help a lot. Com-
puter Speech and Language, 20(4):382–399.
N. Chinchor. 1997. Muc-7 named entity task definition.
In Proceedings ofMUC-7.
H. Christensen, Y. Gotoh, and S. Renals. 2001. Punc-
tuation annotation using statistical prosody models. In
ISCA Workshop on Prosody in Speech Recognition and
Understanding.
A. Gravano, M. Jansche, and M. Bacchiani. 2009.
Restoring punctuation and capitalization in transcribed
speech. In Proceedings ofICASSP, pages 4741–4744.
IEEE Computer Society.
J. Hopcroft and J. Ullman, 1979. Introduction to au-
tomata theory, languages, and computation, pages
218–219. Addison-Wesley.
J. Huang, G. Zweig, and M. Padmanabhan. 2001. Infor-
mation extraction from voicemail. In Proceedings of
the Conference of the ACL, pages 290–297.
M. Jansche and S. P. Abney. 2002. Information extrac-
tion from voicemail transcripts. In In EMNLP.
D. Jones, F. Wolf, E. Gibson, E. Williams, E. Fedorenko,
D. Reynolds, and M. Zissman. 2003. Measuring the
readability of automatic speech-to-text transcripts. In
Proceedings of EUROSPEECH, pages 1585–1588.
M. Levit, P. Haffner, A. Gorin, H. Alshawi, and E. N¨oth.
2004. Aspects of named entity processing. In Pro-
ceedings ofINTERSPEECH.
Y. Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf,
and M. Harper. 2006. Enriching speech recognition
with automatic detection of sentence boundaries and
disfluencies. IEEE Transactions on Audio, Speech,
and Language Processing, 14(5):1526–1540.
M. Mohri, F. Pereira, and M. Riley. 2008. Speech recog-
nition with weighted finite-state transducers. In Hand-
book on Speech Processing and Speech Communica-
tion. Springer.
M. Mohri. 2009. Weighted automata algorithms. In
Handbook of Weighted Automata. Monographs in The-
oretical Computer Science., pages 213–254. Springer.
M. Padmanabhan, G. Ramaswamy, B. Ramabhadran,
P. Gopalakrishnan, and C. Dunn. 1998. Voicemail
corpus part i. Linguistic Data Consortium, Philadel-
phia.
E. Shriberg, A. Stolcke, D. Hakkani-T¨ur, and G. T¨ur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Communi-
cations, 32(1-2):127–154.
W. Wang, K. Knight, and D. Marcu. 2006. Capitaliz-
ing machine translation. In Proceedings ofHLT/ACL,
pages 1–8. Association for Computational Linguistics.
</reference>
<page confidence="0.998892">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.772521">
<title confidence="0.932388">Formatting Time-Aligned ASR Transcripts for Readability Google</title>
<author confidence="0.883383">New York</author>
<author confidence="0.883383">NY</author>
<email confidence="0.99966">shumash@google.com</email>
<abstract confidence="0.998789466666667">We address the problem of formatting the output of an automatic speech recognition (ASR) system for readability, while preserving wordlevel timing information of the transcript. Our system enriches the ASR transcript with punctuation, capitalization and properly written dates, times and other numeric entities, and our approach can be applied to other formatting tasks. The method we describe combines hand-crafted grammars with a class-based language model trained on written text and relies on Weighted Finite State Transducers (WF- STs) for the preservation of start and end time of each word.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Allauzen</author>
<author>M Riley</author>
<author>J Schalkwyk</author>
<author>W Skut</author>
<author>M Mohri</author>
</authors>
<title>Openfst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of CIAA,</booktitle>
<pages>11--23</pages>
<contexts>
<context position="15579" citStr="Allauzen et al., 2007" startWordPosition="2540" endWordPosition="2543">adata We extend the WFST formulation to preserve wordlevel timing and confidence information. 3.1 Background A WFST is a finite set of states and transitions connecting them. Each transition has an input label, an output label and a weight in some semiring K. A semiring is informally defined as a touple (K, ®, (9, 0, 1), where K is the set of elements, ® and ® are the addition and multiplication operations, 0 is the additive identity and multiplicative annihilator, 1 is the multiplicative identity (See (Mohri, 2Our system implements proper failure transitions available in the OpenFST Library (Allauzen et al., 2007). (a) S FSA 201 2009)). By defining new semirings we can use standard FST operations to accomplish a wide range of goals. 3.2 Timing Semiring In order to formulate time preservation within the FST formalism, we define the timing semiring Kt where each element is a pair (s, e) that can be interpreted as the start and end time of a word: Wt = {(s, e) : s,e C R&apos; U {0,oo11 (s1, e1) ® (s2, e2) = (max(s1, s2), min(e1, e2)) (s1, e1) ® (s2, e2) = (min(s1, s2), max(e1, e2)) 0 = (0, oo) 1 = (oo, 0) Intuitively, the addition operation takes the largest interval contained by both operand intervals, while </context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M. Mohri. 2007. Openfst: A general and efficient weighted finite-state transducer library. In Proceedings of CIAA, pages 11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F B´echet</author>
<author>A Gorin</author>
<author>J Wright</author>
<author>D Hakkani-T¨ur</author>
</authors>
<title>Detecting and extracting named entities from spontaneous speech in a mixed-initiative spoken dialogue context: How may i help you?</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<volume>42</volume>
<issue>2</issue>
<marker>B´echet, Gorin, Wright, Hakkani-T¨ur, 2004</marker>
<rawString>F. B´echet, A. Gorin, J. Wright, and D. Hakkani-T¨ur. 2004. Detecting and extracting named entities from spontaneous speech in a mixed-initiative spoken dialogue context: How may i help you? Speech Communication, 42(2):207–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Cyberpunc: A lightweight punctuation annotation system for speech.</title>
<date>1998</date>
<booktitle>In Proceedings ofICASSP,</booktitle>
<pages>689--692</pages>
<contexts>
<context position="2897" citStr="Beeferman et al., 1998" startWordPosition="457" endWordPosition="460">5-8888 extension is 330. Bye. Figure 1: An example of a raw transcript with ambiguous written forms and the output of our formatting system. via word alignment after some types of formatting, word-level quantities may be difficult to preserve if the original text has undergone a significant transformation. We present a formal and general augmentation of our WFST-based technique that preserves word-level timing and confidence information during arbitrary formatting. The problems of sentence boundary detection and punctuation of transcripts have received a substantial amount of attention, e.g. (Beeferman et al., 1998; Shriberg et al., 2000; Christensen et al., 2001; Liu et al., 2006; Gravano et al., 2009). Capitalization of ASR transcripts received less attention (Brown and Coden, 2002; Gravano et al., 2009), but there has also been work on case restoration in the context of machine translation (Chelba and Acero, 2006; Wang et al., 2006). Our work does not propose competing methods for transcript punctuation and capitalization. Instead, we aim to provide a common framework for a wide range of formatting tasks. Our method extends the approach of Gravano et al. (2009) with a general WFST formulation suitabl</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1998</marker>
<rawString>D. Beeferman, A. Berger, and J. Lafferty. 1998. Cyberpunc: A lightweight punctuation annotation system for speech. In Proceedings ofICASSP, pages 689–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brown</author>
<author>A Coden</author>
</authors>
<title>Capitalization recovery for text.</title>
<date>2002</date>
<booktitle>In Information Retrieval Techniques for Speech Applications,</booktitle>
<pages>11--22</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="3069" citStr="Brown and Coden, 2002" startWordPosition="484" endWordPosition="487">pes of formatting, word-level quantities may be difficult to preserve if the original text has undergone a significant transformation. We present a formal and general augmentation of our WFST-based technique that preserves word-level timing and confidence information during arbitrary formatting. The problems of sentence boundary detection and punctuation of transcripts have received a substantial amount of attention, e.g. (Beeferman et al., 1998; Shriberg et al., 2000; Christensen et al., 2001; Liu et al., 2006; Gravano et al., 2009). Capitalization of ASR transcripts received less attention (Brown and Coden, 2002; Gravano et al., 2009), but there has also been work on case restoration in the context of machine translation (Chelba and Acero, 2006; Wang et al., 2006). Our work does not propose competing methods for transcript punctuation and capitalization. Instead, we aim to provide a common framework for a wide range of formatting tasks. Our method extends the approach of Gravano et al. (2009) with a general WFST formulation suitable for formatting monetary amounts, time expressions, dates, phone numbers, honorifics and more, in addition to punctuation and capitalization. To our knowledge, this scope </context>
</contexts>
<marker>Brown, Coden, 2002</marker>
<rawString>E. Brown and A. Coden. 2002. Capitalization recovery for text. In Information Retrieval Techniques for Speech Applications, pages 11–22, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>A Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="3204" citStr="Chelba and Acero, 2006" startWordPosition="507" endWordPosition="510"> We present a formal and general augmentation of our WFST-based technique that preserves word-level timing and confidence information during arbitrary formatting. The problems of sentence boundary detection and punctuation of transcripts have received a substantial amount of attention, e.g. (Beeferman et al., 1998; Shriberg et al., 2000; Christensen et al., 2001; Liu et al., 2006; Gravano et al., 2009). Capitalization of ASR transcripts received less attention (Brown and Coden, 2002; Gravano et al., 2009), but there has also been work on case restoration in the context of machine translation (Chelba and Acero, 2006; Wang et al., 2006). Our work does not propose competing methods for transcript punctuation and capitalization. Instead, we aim to provide a common framework for a wide range of formatting tasks. Our method extends the approach of Gravano et al. (2009) with a general WFST formulation suitable for formatting monetary amounts, time expressions, dates, phone numbers, honorifics and more, in addition to punctuation and capitalization. To our knowledge, this scope of the problem has not been addressed in literature. Yet such formatting can have a high impact on transcript readability. In this pape</context>
</contexts>
<marker>Chelba, Acero, 2006</marker>
<rawString>C. Chelba and A. Acero. 2006. Adaptation of maximum entropy capitalizer: Little data can help a lot. Computer Speech and Language, 20(4):382–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
</authors>
<title>Muc-7 named entity task definition.</title>
<date>1997</date>
<booktitle>In Proceedings ofMUC-7.</booktitle>
<contexts>
<context position="4572" citStr="Chinchor, 1997" startWordPosition="732" endWordPosition="733">os Angeles, California, June 2010. c�2010 Association for Computational Linguistics ting. In general, context independent rules fail to adequately perform this task due to its inherent ambiguity (See Fig. 1). For example, the spoken words “three thirty” should be written differently in these three contexts: • meet meat 3:30 • you owe me 330 • dinner for three 30 minutes later The proper written form of a numeric entity depends on its class (time, monetary amount, etc). In this sense, formatting is related to the problem of named entity (NE) detection and value extraction, as defined by MUC-7 (Chinchor, 1997). Several authors have considered the problem of NE value extraction from raw transcripts (Huang et al., 2001; Jansche and Abney, 2002; B´echet et al., 2004; Levit et al., 2004). This is an information extraction task that involves identifying transcript words corresponding to a particular NE class and extracting an unambiguous value of that NE (e.g. the value of the date NE “december first oh nine” is “12/01/2009”). Although relevant, this information extraction does not directly address the problem of proper formatting and ordinarily requires a tagged corpus for training. A parallel corpus c</context>
</contexts>
<marker>Chinchor, 1997</marker>
<rawString>N. Chinchor. 1997. Muc-7 named entity task definition. In Proceedings ofMUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Christensen</author>
<author>Y Gotoh</author>
<author>S Renals</author>
</authors>
<title>Punctuation annotation using statistical prosody models.</title>
<date>2001</date>
<booktitle>In ISCA Workshop on Prosody in Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="2946" citStr="Christensen et al., 2001" startWordPosition="465" endWordPosition="468">ple of a raw transcript with ambiguous written forms and the output of our formatting system. via word alignment after some types of formatting, word-level quantities may be difficult to preserve if the original text has undergone a significant transformation. We present a formal and general augmentation of our WFST-based technique that preserves word-level timing and confidence information during arbitrary formatting. The problems of sentence boundary detection and punctuation of transcripts have received a substantial amount of attention, e.g. (Beeferman et al., 1998; Shriberg et al., 2000; Christensen et al., 2001; Liu et al., 2006; Gravano et al., 2009). Capitalization of ASR transcripts received less attention (Brown and Coden, 2002; Gravano et al., 2009), but there has also been work on case restoration in the context of machine translation (Chelba and Acero, 2006; Wang et al., 2006). Our work does not propose competing methods for transcript punctuation and capitalization. Instead, we aim to provide a common framework for a wide range of formatting tasks. Our method extends the approach of Gravano et al. (2009) with a general WFST formulation suitable for formatting monetary amounts, time expressio</context>
<context position="9449" citStr="Christensen et al., 2001" startWordPosition="1499" endWordPosition="1502">on in voicemail. If appropriate to the task, deterministic grammars can be incorporated into the grammar stack. The unweighted written variants generated by applying G1...G,,,, to s are then scored with the language model. 2.3 Language Model The probability distribution over written text P(w) can be approximated by a Katz back-off n-gram language model trained on written text in a domain semantically similar to the domain for which the ASR engine is deployed. Unlike some of the approaches used for NE identification (Jansche and Abney, 2002; Levit et al., 2004) and sentence boundary detection (Christensen et al., 2001; Shriberg et al., 2000; Liu et al., 2006), LM-based scoring cannot exploit a larger context than n tokens or prosodic features. The advantage of the LM approach is the ease of applying it to new formatting tasks: no new tagged corpus, and only trivial changes to the preprocessing of the training text would be required. If the LM is to score written numeric strings, care must be taken in modeling numbers. Representing each written number as a token (e.g. tokens “1,235”, “15”) during training results in a very large model and suffers from data sparsity even with very large training corpora. An </context>
<context position="24605" citStr="Christensen et al., 2001" startWordPosition="4107" endWordPosition="4110">s and capitals are computed using PTS as reference (See Fig. 5). Precision Recall F-Measure Capitals 0.7902 0.5356 0.6385 Comma 0.5527 0.3129 0.3996 Period 0.6672 0.6783 0.6727 Figure 5: Punctuation and capitalization results. It should be noted that a 5-gram language model that treats spaces as words models the same history as a 3-gram model that omits the spaces from training data. When this is taken into account, our results with a much smaller training set are comparable to Gravano et al. (2009). The F-measure scores for commas and periods are also comparable to the prosody-based work of (Christensen et al., 2001), with the precision of the period slightly lower, but compensated by recall. Thus, our system can perform additional formatting, while retaining a reasonable capitalization and punctuation performance. 4.4 Evaluation of Number Formatting We evaluate number formatting performance of Systems A and B, which use different sets of classes for the language modeling (See Table 1). We process NNTS with both systems and score against the reference formatted set NTS to obtain Numeric Entity Error Rate (NEER). Class Set B naively breaks numbers into classes by digit count. System B using this class set </context>
</contexts>
<marker>Christensen, Gotoh, Renals, 2001</marker>
<rawString>H. Christensen, Y. Gotoh, and S. Renals. 2001. Punctuation annotation using statistical prosody models. In ISCA Workshop on Prosody in Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gravano</author>
<author>M Jansche</author>
<author>M Bacchiani</author>
</authors>
<title>Restoring punctuation and capitalization in transcribed speech.</title>
<date>2009</date>
<booktitle>In Proceedings ofICASSP,</booktitle>
<pages>4741--4744</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="2987" citStr="Gravano et al., 2009" startWordPosition="473" endWordPosition="476">en forms and the output of our formatting system. via word alignment after some types of formatting, word-level quantities may be difficult to preserve if the original text has undergone a significant transformation. We present a formal and general augmentation of our WFST-based technique that preserves word-level timing and confidence information during arbitrary formatting. The problems of sentence boundary detection and punctuation of transcripts have received a substantial amount of attention, e.g. (Beeferman et al., 1998; Shriberg et al., 2000; Christensen et al., 2001; Liu et al., 2006; Gravano et al., 2009). Capitalization of ASR transcripts received less attention (Brown and Coden, 2002; Gravano et al., 2009), but there has also been work on case restoration in the context of machine translation (Chelba and Acero, 2006; Wang et al., 2006). Our work does not propose competing methods for transcript punctuation and capitalization. Instead, we aim to provide a common framework for a wide range of formatting tasks. Our method extends the approach of Gravano et al. (2009) with a general WFST formulation suitable for formatting monetary amounts, time expressions, dates, phone numbers, honorifics and </context>
<context position="7823" citStr="Gravano et al., 2009" startWordPosition="1246" endWordPosition="1249">ate multiple unweighted written variants for any spoken sequence. For a given s, a collection of grammars encodes a uniform probability distribution across the set of all written variants generated for s and assigns a zero probability to any string not in this set. Such grammar-based modeling of P(s|w) combined with statistical estimation of P(w) takes advantage of prior knowledge, but does not share the disadvantages of rigid, fully rule-based systems. 2.2 Handwritten Grammars Handwritten grammars G1...Gm are used to generate unweighted written variants for a raw string s. In Gravano’s work (Gravano et al., 2009) the generated variants include optional punctuation between every two words and an optional capitalization for every word. Our system supports a wider range of variants, including but not limited to multiple variants of number formatting. The handwritten grammars can be very restrictive or very liberal, depending on the application requirements. For example, a grammar we use to generate punctuation and capitalization only generates sentences with the first word capitalized. This enforces conventions and consistency, which the best scoring variant could occasionally violate. On the other 199 F</context>
<context position="12136" citStr="Gravano et al. (2009)" startWordPosition="1958" endWordPosition="1961">s, assuming nothing about the number distribution. Class Set A is based on intuition about number distribution in text (See Table 1, Interpretation). In section 4.4 we show that Class Set A achieves better performance on number formatting. Now that it is established that the choice of classes affects performance, future research could focus on finding an optimal set of number classes automatically. Clustering techniques, often used to derive class definitions from training text, could be applied. Although more punctuation marks could be considered, we focus on periods and commas. Similarly to Gravano et al. (2009), we map all other punctuation marks in the training text to these two. In many formatting scenarios (e.g. spelled out acronyms, numeric ranges), spaces are ambiguous and significant, three 2 &lt;space&gt; 0 3 1 &lt;space&gt; &lt;column&gt; &lt;period&gt; 3 3 0 thirty 5 ε 4 200 and it is therefore important to consider whitespace when scoring the written variants. Because of this, we model space as a token in the LM. 2.4 WFST Formulation The one-best1 ASR output s can be represented by a Finite State Acceptor (FSA) S. We describe a series of standard WFST operations on S resulting in the FSA Wbest encoding the best e</context>
<context position="24484" citStr="Gravano et al. (2009)" startWordPosition="4088" endWordPosition="4091">order not to introduce errors due to numeric formatting). The precision, recall and Fmeasure rates for periods, commas and capitals are computed using PTS as reference (See Fig. 5). Precision Recall F-Measure Capitals 0.7902 0.5356 0.6385 Comma 0.5527 0.3129 0.3996 Period 0.6672 0.6783 0.6727 Figure 5: Punctuation and capitalization results. It should be noted that a 5-gram language model that treats spaces as words models the same history as a 3-gram model that omits the spaces from training data. When this is taken into account, our results with a much smaller training set are comparable to Gravano et al. (2009). The F-measure scores for commas and periods are also comparable to the prosody-based work of (Christensen et al., 2001), with the precision of the period slightly lower, but compensated by recall. Thus, our system can perform additional formatting, while retaining a reasonable capitalization and punctuation performance. 4.4 Evaluation of Number Formatting We evaluate number formatting performance of Systems A and B, which use different sets of classes for the language modeling (See Table 1). We process NNTS with both systems and score against the reference formatted set NTS to obtain Numeric</context>
</contexts>
<marker>Gravano, Jansche, Bacchiani, 2009</marker>
<rawString>A. Gravano, M. Jansche, and M. Bacchiani. 2009. Restoring punctuation and capitalization in transcribed speech. In Proceedings ofICASSP, pages 4741–4744. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hopcroft</author>
<author>J Ullman</author>
</authors>
<title>Introduction to automata theory, languages, and computation,</title>
<date>1979</date>
<pages>218--219</pages>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="13204" citStr="Hopcroft and Ullman, 1979" startWordPosition="2143" endWordPosition="2146"> s can be represented by a Finite State Acceptor (FSA) S. We describe a series of standard WFST operations on S resulting in the FSA Wbest encoding the best estimated formatted variant w. Current section assumes familiarity with WFSTs; for background see (Mohri, 2009). (b) W variants FST (c) Wout FSA (d) Wclass FST (e) Wbest FSA Figure 3: An example showing transducers produced during formatting. We encode each grammar Gi as an unweighted FST Ti that transduces the raw transcript to its formatted versions. The necessity to encode them as FSTs restricts the set of grammars to regular grammars (Hopcroft and Ullman, 1979), sufficiently powerful for most formatting tasks. The back-off n-gram LM is naturally represented as a weighted deterministic FSA G with negative log probability weights (Mohri et al., 2008). The deterministic mapping of digit strings to number class tokens can also 1This WFST formulation can also be applied to the ASR lattice or n-best list with some modification to the scoring phase. be accomplished by an unweighted transducer K, which passes all non-numeric strings unchanged. Composing the input acceptor S with the grammar transducers Ti results in a transducer W with all written variants </context>
</contexts>
<marker>Hopcroft, Ullman, 1979</marker>
<rawString>J. Hopcroft and J. Ullman, 1979. Introduction to automata theory, languages, and computation, pages 218–219. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>G Zweig</author>
<author>M Padmanabhan</author>
</authors>
<title>Information extraction from voicemail.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference of the ACL,</booktitle>
<pages>290--297</pages>
<contexts>
<context position="4681" citStr="Huang et al., 2001" startWordPosition="747" endWordPosition="750">ext independent rules fail to adequately perform this task due to its inherent ambiguity (See Fig. 1). For example, the spoken words “three thirty” should be written differently in these three contexts: • meet meat 3:30 • you owe me 330 • dinner for three 30 minutes later The proper written form of a numeric entity depends on its class (time, monetary amount, etc). In this sense, formatting is related to the problem of named entity (NE) detection and value extraction, as defined by MUC-7 (Chinchor, 1997). Several authors have considered the problem of NE value extraction from raw transcripts (Huang et al., 2001; Jansche and Abney, 2002; B´echet et al., 2004; Levit et al., 2004). This is an information extraction task that involves identifying transcript words corresponding to a particular NE class and extracting an unambiguous value of that NE (e.g. the value of the date NE “december first oh nine” is “12/01/2009”). Although relevant, this information extraction does not directly address the problem of proper formatting and ordinarily requires a tagged corpus for training. A parallel corpus containing raw transcriptions and the corresponding formatted strings would facilitate the solution to the tra</context>
</contexts>
<marker>Huang, Zweig, Padmanabhan, 2001</marker>
<rawString>J. Huang, G. Zweig, and M. Padmanabhan. 2001. Information extraction from voicemail. In Proceedings of the Conference of the ACL, pages 290–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jansche</author>
<author>S P Abney</author>
</authors>
<title>Information extraction from voicemail transcripts. In</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4706" citStr="Jansche and Abney, 2002" startWordPosition="751" endWordPosition="754">s fail to adequately perform this task due to its inherent ambiguity (See Fig. 1). For example, the spoken words “three thirty” should be written differently in these three contexts: • meet meat 3:30 • you owe me 330 • dinner for three 30 minutes later The proper written form of a numeric entity depends on its class (time, monetary amount, etc). In this sense, formatting is related to the problem of named entity (NE) detection and value extraction, as defined by MUC-7 (Chinchor, 1997). Several authors have considered the problem of NE value extraction from raw transcripts (Huang et al., 2001; Jansche and Abney, 2002; B´echet et al., 2004; Levit et al., 2004). This is an information extraction task that involves identifying transcript words corresponding to a particular NE class and extracting an unambiguous value of that NE (e.g. the value of the date NE “december first oh nine” is “12/01/2009”). Although relevant, this information extraction does not directly address the problem of proper formatting and ordinarily requires a tagged corpus for training. A parallel corpus containing raw transcriptions and the corresponding formatted strings would facilitate the solution to the transcript formatting proble</context>
<context position="8659" citStr="Jansche and Abney (2002)" startWordPosition="1375" endWordPosition="1378">iants of number formatting. The handwritten grammars can be very restrictive or very liberal, depending on the application requirements. For example, a grammar we use to generate punctuation and capitalization only generates sentences with the first word capitalized. This enforces conventions and consistency, which the best scoring variant could occasionally violate. On the other 199 Figure 2: An FSA encoding all variants generated by the number grammar for a spoken string “three thirty”. hand, the grammar for number formatting could be very liberal in producing written variants (See Fig. 2). Jansche and Abney (2002) observe that handwritten rules deterministically tagging numeric strings of certain length as phone numbers perform surprisingly well on phone number NE identification in voicemail. If appropriate to the task, deterministic grammars can be incorporated into the grammar stack. The unweighted written variants generated by applying G1...G,,,, to s are then scored with the language model. 2.3 Language Model The probability distribution over written text P(w) can be approximated by a Katz back-off n-gram language model trained on written text in a domain semantically similar to the domain for whic</context>
</contexts>
<marker>Jansche, Abney, 2002</marker>
<rawString>M. Jansche and S. P. Abney. 2002. Information extraction from voicemail transcripts. In In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jones</author>
<author>F Wolf</author>
<author>E Gibson</author>
<author>E Williams</author>
<author>E Fedorenko</author>
<author>D Reynolds</author>
<author>M Zissman</author>
</authors>
<title>Measuring the readability of automatic speech-to-text transcripts.</title>
<date>2003</date>
<booktitle>In Proceedings of EUROSPEECH,</booktitle>
<pages>1585--1588</pages>
<contexts>
<context position="29176" citStr="Jones et al., 2003" startWordPosition="4856" endWordPosition="4859">ormatted or manual raw. The response time was measured individually for each question. The analysis of the responses reveals a statistically significant difference in response time between formatted and raw ASR transcripts (p = 0.02, even allowing for per-item and per-subject effects; see also Fig. 6) and comparable accuracy. The response times for formatted ASR were comparable to the response times for manual unformatted transcripts. This suggests that for transcripts with low error rates the formatting of the ASR output significantly impacts readability. This disagrees with a similar study (Jones et al., 2003), which found no significant difference in the comprehension rates between raw ASR transcripts and capitalized, punctuated ASR output with disfluencies removed. This could be due to a number of factors, including a different type of transformation performed on the ASR transcript, a different corpus, and a lower word error rate of transcripts in our user study. ASR Formatted ASR Raw Manual Raw 90.0% 90.7% 94.4% Figure 6: The standard R box plot of the response time for different transcript types and the corresponding accuracy. 5 Conclusion We present a statistical approach suitable for a wide r</context>
</contexts>
<marker>Jones, Wolf, Gibson, Williams, Fedorenko, Reynolds, Zissman, 2003</marker>
<rawString>D. Jones, F. Wolf, E. Gibson, E. Williams, E. Fedorenko, D. Reynolds, and M. Zissman. 2003. Measuring the readability of automatic speech-to-text transcripts. In Proceedings of EUROSPEECH, pages 1585–1588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Levit</author>
<author>P Haffner</author>
<author>A Gorin</author>
<author>H Alshawi</author>
<author>E N¨oth</author>
</authors>
<title>Aspects of named entity processing.</title>
<date>2004</date>
<booktitle>In Proceedings ofINTERSPEECH.</booktitle>
<marker>Levit, Haffner, Gorin, Alshawi, N¨oth, 2004</marker>
<rawString>M. Levit, P. Haffner, A. Gorin, H. Alshawi, and E. N¨oth. 2004. Aspects of named entity processing. In Proceedings ofINTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>M Harper</author>
</authors>
<title>Enriching speech recognition with automatic detection of sentence boundaries and disfluencies.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="2964" citStr="Liu et al., 2006" startWordPosition="469" endWordPosition="472">th ambiguous written forms and the output of our formatting system. via word alignment after some types of formatting, word-level quantities may be difficult to preserve if the original text has undergone a significant transformation. We present a formal and general augmentation of our WFST-based technique that preserves word-level timing and confidence information during arbitrary formatting. The problems of sentence boundary detection and punctuation of transcripts have received a substantial amount of attention, e.g. (Beeferman et al., 1998; Shriberg et al., 2000; Christensen et al., 2001; Liu et al., 2006; Gravano et al., 2009). Capitalization of ASR transcripts received less attention (Brown and Coden, 2002; Gravano et al., 2009), but there has also been work on case restoration in the context of machine translation (Chelba and Acero, 2006; Wang et al., 2006). Our work does not propose competing methods for transcript punctuation and capitalization. Instead, we aim to provide a common framework for a wide range of formatting tasks. Our method extends the approach of Gravano et al. (2009) with a general WFST formulation suitable for formatting monetary amounts, time expressions, dates, phone n</context>
<context position="9491" citStr="Liu et al., 2006" startWordPosition="1507" endWordPosition="1510">rministic grammars can be incorporated into the grammar stack. The unweighted written variants generated by applying G1...G,,,, to s are then scored with the language model. 2.3 Language Model The probability distribution over written text P(w) can be approximated by a Katz back-off n-gram language model trained on written text in a domain semantically similar to the domain for which the ASR engine is deployed. Unlike some of the approaches used for NE identification (Jansche and Abney, 2002; Levit et al., 2004) and sentence boundary detection (Christensen et al., 2001; Shriberg et al., 2000; Liu et al., 2006), LM-based scoring cannot exploit a larger context than n tokens or prosodic features. The advantage of the LM approach is the ease of applying it to new formatting tasks: no new tagged corpus, and only trivial changes to the preprocessing of the training text would be required. If the LM is to score written numeric strings, care must be taken in modeling numbers. Representing each written number as a token (e.g. tokens “1,235”, “15”) during training results in a very large model and suffers from data sparsity even with very large training corpora. An alternative approach of modeling every dig</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Hillard, Ostendorf, Harper, 2006</marker>
<rawString>Y. Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf, and M. Harper. 2006. Enriching speech recognition with automatic detection of sentence boundaries and disfluencies. IEEE Transactions on Audio, Speech, and Language Processing, 14(5):1526–1540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>F Pereira</author>
<author>M Riley</author>
</authors>
<title>Speech recognition with weighted finite-state transducers.</title>
<date>2008</date>
<booktitle>In Handbook on Speech Processing and Speech Communication.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="13395" citStr="Mohri et al., 2008" startWordPosition="2171" endWordPosition="2174">ction assumes familiarity with WFSTs; for background see (Mohri, 2009). (b) W variants FST (c) Wout FSA (d) Wclass FST (e) Wbest FSA Figure 3: An example showing transducers produced during formatting. We encode each grammar Gi as an unweighted FST Ti that transduces the raw transcript to its formatted versions. The necessity to encode them as FSTs restricts the set of grammars to regular grammars (Hopcroft and Ullman, 1979), sufficiently powerful for most formatting tasks. The back-off n-gram LM is naturally represented as a weighted deterministic FSA G with negative log probability weights (Mohri et al., 2008). The deterministic mapping of digit strings to number class tokens can also 1This WFST formulation can also be applied to the ASR lattice or n-best list with some modification to the scoring phase. be accomplished by an unweighted transducer K, which passes all non-numeric strings unchanged. Composing the input acceptor S with the grammar transducers Ti results in a transducer W with all written variants on the output. Projected onto its output labels, W becomes an acceptor Wout. Wclass, the result of the composition of Wout with K, has all formatted written variants on the input side and the</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2008</marker>
<rawString>M. Mohri, F. Pereira, and M. Riley. 2008. Speech recognition with weighted finite-state transducers. In Handbook on Speech Processing and Speech Communication. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
</authors>
<title>Weighted automata algorithms.</title>
<date>2009</date>
<booktitle>In Handbook of Weighted Automata. Monographs in Theoretical Computer Science.,</booktitle>
<pages>213--254</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="12846" citStr="Mohri, 2009" startWordPosition="2083" endWordPosition="2084">os (e.g. spelled out acronyms, numeric ranges), spaces are ambiguous and significant, three 2 &lt;space&gt; 0 3 1 &lt;space&gt; &lt;column&gt; &lt;period&gt; 3 3 0 thirty 5 ε 4 200 and it is therefore important to consider whitespace when scoring the written variants. Because of this, we model space as a token in the LM. 2.4 WFST Formulation The one-best1 ASR output s can be represented by a Finite State Acceptor (FSA) S. We describe a series of standard WFST operations on S resulting in the FSA Wbest encoding the best estimated formatted variant w. Current section assumes familiarity with WFSTs; for background see (Mohri, 2009). (b) W variants FST (c) Wout FSA (d) Wclass FST (e) Wbest FSA Figure 3: An example showing transducers produced during formatting. We encode each grammar Gi as an unweighted FST Ti that transduces the raw transcript to its formatted versions. The necessity to encode them as FSTs restricts the set of grammars to regular grammars (Hopcroft and Ullman, 1979), sufficiently powerful for most formatting tasks. The back-off n-gram LM is naturally represented as a weighted deterministic FSA G with negative log probability weights (Mohri et al., 2008). The deterministic mapping of digit strings to num</context>
<context position="16347" citStr="Mohri, 2009" startWordPosition="2683" endWordPosition="2684">rmulate time preservation within the FST formalism, we define the timing semiring Kt where each element is a pair (s, e) that can be interpreted as the start and end time of a word: Wt = {(s, e) : s,e C R&apos; U {0,oo11 (s1, e1) ® (s2, e2) = (max(s1, s2), min(e1, e2)) (s1, e1) ® (s2, e2) = (min(s1, s2), max(e1, e2)) 0 = (0, oo) 1 = (oo, 0) Intuitively, the addition operation takes the largest interval contained by both operand intervals, while multiplication returns the smallest interval fully containing both operand intervals. 3 This definition fulfills all the semiring properties as defined in (Mohri, 2009). Note that encoding only the duration of each word is not sufficient, as there may be time gaps between the words due to the segmentation of the source audio. Let S denote the Weighted Finite State Acceptor (WFSA) encoding the raw ASR output with the start and end time stored in the weight of each arc. In order to preserve word-level confidence in addition to timing information, a Cartesian product of Kt and the Log semiring can be used to store both time and confidence in an arc weight. 3.3 Weight Synchronization The goal is to associate the timing/confidence weights of S with the word label</context>
<context position="18980" citStr="Mohri, 2009" startWordPosition="3143" endWordPosition="3144">mall example of time preservation section of the algorithm. Arcs with non-unity timing weights show parenthesized pair of start and end time. The best path that has input labels corresponding to the raw ASR output can be obtained by composing the variants FST W with the best formatted FSA Wbest and picking any path. The timing weights are restored to by composing the weighted S with this result. To preserve timing we add two more steps to Steps 1–5 in section 2.4: 6. Wraw:best = RmEps(AnyPath(W o Wbest)) 7. Wbest = S o Mapt(Wraw:best) where RmEps(X) applies the epsilon-removal algorithm to X (Mohri, 2009), and Mapt(X) maps ten/(1,2) &lt;sp&gt; six/(3,4) 0 1 2 3 1 0 &lt;sp&gt; 6 0 1 2 3 4 (a) S FSA (b) Wbest FSA ten:1 ε:0 &lt;sp&gt;:&lt;sp&gt; six:6 0 1 2 3 4 (c) Wr.:best FST ten:1/(1,2) ε:0 &lt;sp&gt;:&lt;sp&gt; six:6/(3,4) 0 1 2 3 4 (d) Wbest FST 202 all non-zero weights of X to the unity weight in the timing semiring. Because S is an epsilon-free acceptor, the result Wbest will contain the original weights of S on the arcs with the corresponding input labels (See Fig. 4 for an example). The space-delimited words and the corresponding weights can then be read off by walking �Wbest. 4 Evaluation Section 4.1 presents our datasets</context>
</contexts>
<marker>Mohri, 2009</marker>
<rawString>M. Mohri. 2009. Weighted automata algorithms. In Handbook of Weighted Automata. Monographs in Theoretical Computer Science., pages 213–254. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Padmanabhan</author>
<author>G Ramaswamy</author>
<author>B Ramabhadran</author>
<author>P Gopalakrishnan</author>
<author>C Dunn</author>
</authors>
<title>Voicemail corpus part i. Linguistic Data Consortium,</title>
<date>1998</date>
<location>Philadelphia.</location>
<contexts>
<context position="20713" citStr="Padmanabhan et al., 1998" startWordPosition="3443" endWordPosition="3446">rmalized to contain only comma and period punctuation marks. A set of 176M tokens (TRS) is used for training and a set of 7M tokens (PTS) is held back for testing punctuation and capitalization (See Table 3). To obtain a test input (NPTS) for our system, PTS is lowercased and all punctuation is removed. words commas periods capitals TRS 176M 10.6M 11.8M 24.3M PTS 7M 420K 440K 880K Table 3: Training set TRS and test set PTS. Number formatting is evaluated on a manually formatted test set. We manually processed the set of raw manual transcripts (NNTS) from the LDC Voicemail Part I training set (Padmanabhan et al., 1998) to obtain a reference number formatting set (NTS). All numeric entities in NTS were formatted according to the following conventions: • all quantities under 10 are spelled out • time is written in a 12-hour system as “xx:xx” or “xx” • dollar amounts are written as “$x,xxx.xx” with cents included if spoken • US phone numbers are written as “(xxx) xxxxxxx” or “xxx-xxxx” • other phone numbers are written as digit strings • decimals are written as “x.x” • large amounts include commas: “x,xxx,xxx” All contiguous sequences of words in NTS that could be a target for number formatting were marked as </context>
</contexts>
<marker>Padmanabhan, Ramaswamy, Ramabhadran, Gopalakrishnan, Dunn, 1998</marker>
<rawString>M. Padmanabhan, G. Ramaswamy, B. Ramabhadran, P. Gopalakrishnan, and C. Dunn. 1998. Voicemail corpus part i. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D Hakkani-T¨ur</author>
<author>G T¨ur</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics.</title>
<date>2000</date>
<journal>Speech Communications,</journal>
<pages>32--1</pages>
<marker>Shriberg, Stolcke, Hakkani-T¨ur, T¨ur, 2000</marker>
<rawString>E. Shriberg, A. Stolcke, D. Hakkani-T¨ur, and G. T¨ur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communications, 32(1-2):127–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Capitalizing machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT/ACL,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3224" citStr="Wang et al., 2006" startWordPosition="511" endWordPosition="514"> general augmentation of our WFST-based technique that preserves word-level timing and confidence information during arbitrary formatting. The problems of sentence boundary detection and punctuation of transcripts have received a substantial amount of attention, e.g. (Beeferman et al., 1998; Shriberg et al., 2000; Christensen et al., 2001; Liu et al., 2006; Gravano et al., 2009). Capitalization of ASR transcripts received less attention (Brown and Coden, 2002; Gravano et al., 2009), but there has also been work on case restoration in the context of machine translation (Chelba and Acero, 2006; Wang et al., 2006). Our work does not propose competing methods for transcript punctuation and capitalization. Instead, we aim to provide a common framework for a wide range of formatting tasks. Our method extends the approach of Gravano et al. (2009) with a general WFST formulation suitable for formatting monetary amounts, time expressions, dates, phone numbers, honorifics and more, in addition to punctuation and capitalization. To our knowledge, this scope of the problem has not been addressed in literature. Yet such formatting can have a high impact on transcript readability. In this paper we focus on numeri</context>
</contexts>
<marker>Wang, Knight, Marcu, 2006</marker>
<rawString>W. Wang, K. Knight, and D. Marcu. 2006. Capitalizing machine translation. In Proceedings ofHLT/ACL, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>