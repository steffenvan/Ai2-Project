<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000520">
<title confidence="0.98776">
Accurate Semantic Class Classifier for Coreference Resolution
</title>
<author confidence="0.999738">
Zhiheng Huang1, Guangping Zeng1,2, Weiqun Xu3, and Asli Celikyilmaz1
</author>
<affiliation confidence="0.990522">
1EECS Department, University of California at Berkeley,
</affiliation>
<address confidence="0.903161">
CA 94720, USA
</address>
<email confidence="0.992383">
{zhiheng,gpzeng,asli}@eecs.berkeley.edu
</email>
<affiliation confidence="0.994201">
2Computer Science Department, School of Information Engineering,
University of Science and Technology Beijing, China
3ThinkIT, Institute of Acoustics, Chinese Academy of Sciences,
</affiliation>
<address confidence="0.651052">
Beijing, 100190, China
</address>
<email confidence="0.996927">
xuweiqun@hccl.ioa.ac.cn
</email>
<sectionHeader confidence="0.993864" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899666666667">
There have been considerable attempts
to incorporate semantic knowledge into
coreference resolution systems: different
knowledge sources such as WordNet and
Wikipedia have been used to boost the per-
formance. In this paper, we propose new
ways to extract WordNet feature. This
feature, along with other features such as
named entity feature, can be used to build
an accurate semantic class (SC) classifier.
In addition, we analyze the SC classifica-
tion errors and propose to use relaxed SC
agreement features. The proposed accu-
rate SC classifier and the relaxation of SC
agreement features on ACE2 coreference
evaluation can boost our baseline system
by 10.4% and 9.7% using MUC score and
anaphor accuracy respectively.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988647826087">
Coreference resolution is used to determine which
noun phrases (including pronouns, proper names,
and common nouns) refer to the same entities in
documents. Much work on coreference resolution
is based on (Soon et al., 2001), which built a de-
cision tree classifier to label pairs of mentions as
coreferent or not. Recent work aims to improve
the performance from two aspects: new models
and new features. The former cast the pair wise
mention classifications into various forms such as
the best path in a Bell tree (Luo et al., 2004), the
best graph cut (Nicolae and Nicolae, 2006), in-
teger linear programming (Denis and Baldridge,
2007) and graph partition based conditional model
(McCallum and Wellner, 2004). The latter de-
velop and investigate new linguistic features for
the problem. For instance, WordNet (Poesio et al.,
2004), Wikipedia (Ponzetto and Strube, 2006), se-
mantic neighbor words (Ng, 2007a), and pattern
based features (Yang and Su, 2007) have been ex-
tensively studied.
Deeper linguistic knowledge is required to en-
able the coreference resolution to reach a higher
level of performance (Kehler et al., 2004). An im-
portant type of semantic knowledge that has been
employed in coreference resolution system is the
semantic class (SC) of an NP, which can be used
to filter out the coreference between semantically
incompatible NPs. However, the difficulty is to
accurately compute the semantic class features. In
this paper, we show that the WordNet may not be
efficiently employed in the traditional way such
as (Soon et al., 2001; Ng, 2007a; Ponzetto and
Strube, 2006) to compute the semantic class fea-
tures. We introduce new ways to use the WordNet
and the experiments show its effectiveness in de-
termining the semantic classes for noun phrases.
In addition, we analyze the classification errors of
the SC classifier and propose to use relaxed SC
agreement features. With these proposed features
and other standard syntactic features (which are
commonly employed in existing coreference sys-
tems), our coreference resolution system can ob-
tain an increase of 10.4% for MUC score and 9.7%
for anaphor accuracy from the baseline in ACE2
evaluation.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999910375">
WordNet (Fellbaum, 1998) as an important knowl-
edge source has been widely employed in previ-
ous coreference resolution work. For example,
Harabagiu et al. (2001) have used WordNet rela-
tions such as synonym and is-a to mine the pat-
terns of WordNet paths for pairs of antecedents
and anaphors. Due to the nature of the rule based
coreference system (in contrast to machine learn-
ing based), the weights of relations may not be
accurately estimated. Vieira and Poesio (2000)
and Markert and Nissim (2005) have used Word-
Net synonym and hyponym etc. to determine if
an anaphor semantically relates to one previous
NP. Ponzetto and Strube (2006) have used Word-
Net semantic similarity and relatedness scores be-
tween antecedents and candidate anaphors. Their
</bodyText>
<page confidence="0.943534">
1232
</page>
<note confidence="0.9965825">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1232–1240,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999976422222222">
work is different to this work in the following: 1)
Their work involves various relations such as hy-
ponyms and meronyms while ours only makes use
of hypernyms; and 2) Their work focuses on in-
vestigating if two NPs have particular WordNet re-
lations or not, while ours focuses on using Word-
Net hypernyms for their SC classification and then
testing their SC compatibility. In doing so, we can
directly model the accuracy of semantic class clas-
sification and test its impact on coreference reso-
lution.
While the SC of a proper name is computed
fairly accurately using a named entity (NE) recog-
nizer, many coreference resolution systems sim-
ply assign to a common noun the first (i.e., most
frequent) WordNet synset as its SC (Soon et al.,
2001; Markert and Nissim, 2005). This heuris-
tics, apparently, did not lead to good performance.
The best reported ACE2 coreference resolution
system (Ng, 2007a; Ng, 2007b) has proposed an
accurate SC classifier which used heterogeneous
semantic knowledge sources. WordNet is just
one of the several knowledge sources which have
been utilized. However, the WordNet based fea-
tures is not informative compared to other features
such as the semantic neighbor feature. Similarly,
Ponzetto and Strube (2006) have discovered that
the WordNet feature is no more informative than
the community-generated Wikipedia feature. In
this paper, we focus on the investigation of vari-
ous usages of WordNet for the SC classification
task. The work which is directly comparable to
ours would be (Ng, 2007a; Ng, 2007b).
Other similar work includes the mention detec-
tion (MD) task (Florian et al., 2006) and joint
probabilistic model of coreference (Daum´e III and
Marcu, 2005). The MD task identifies the bound-
ary of a mention, its mention type (e.g., pronoun,
name), and its semantic type (e.g., person, orga-
nization). Unlike them, we do not perform the
boundary detection, as we make use of the noun
phrases directly from the noun phrase chunker and
NE recognizer. The joint probabilistic model mod-
els the MD and coreference simultaneously, while
our work focuses on them separately.
</bodyText>
<sectionHeader confidence="0.995141" genericHeader="method">
3 Semantic Class Classification
</sectionHeader>
<bodyText confidence="0.99543096969697">
In this section, we describe how we compile the
training corpus and extract features using Word-
Net. We report our results on the ACE coreference
corpus due to that it has been commonly used and
it was annotated SCs of six types.1 As in (Ng,
1Person, organization, gpe, location and facility are ex-
plicitly annotated. The rest noun phrases are other type.
2007a), we first train a classifier to predict the SC
of an NP. This SC information is used later in the
coreference resolution stage. For example, the au-
dience is classified as SC of person, and it thus
should not be coreferent with the security industry,
which is usually classified as organization. This
task is by no means trivial. First, while the classi-
fication of Tom Hanks being SC of person can be
accurately achieved by an NE recognizer, the as-
sociation of audience and person requires seman-
tic language source such as WordNet. Second, the
same noun phrase can be annotated with different
SCs under different context. For example, the au-
thorities is usually annotated as person, but it is
sometimes as organization. Even worse, the same
noun phrases are sometimes annotated with one of
the five explicitly annotated classes while some-
times are not annotated at all (thus falling into the
other SC). For example, people is annotated as
person SC explicitly 20 times and is not annotated
at all 21 times in the ACE2 testset. This inconsis-
tent annotation adversely affects the performance
of an SC classifier. And this in turn would cause
errors during coreference stage. In section 4.3, we
show how to relax the strict SC agreement feature
to address this.
</bodyText>
<subsectionHeader confidence="0.999494">
3.1 Training instance creation
</subsectionHeader>
<bodyText confidence="0.999321769230769">
We use ACE Phase 2 Coreference corpus to train
the SC classifier. Each noun phrase which is iden-
tified by the noun phrase chunker or NE recognizer
is used to create a training instance. Each instance
is represented by a set of lexical, syntactic and se-
mantic features, as described below. If the NP un-
der consideration is annotated as one of the five
ACE SCs in the corpus, then the classification of
the associated training instance is the ACE SC of
the NP. Otherwise, the instance is labeled as other.
ACE 2 corpus has a training set and a test set
which comprise of 422 and 97 texts respectively.
We divide the training set into a new training and a
development set: the former consists of 90% ran-
domly generated and stratified original training in-
stances and the latter consists of the rest 10% in-
stances. The test set remains the same as in ACE2
corpus. The size of each dataset and its SC dis-
tributions are shown in Table 1. Note that the
training and development datasets have exactly the
same distributions of SCs due to the stratification
procedure. That is, each class has the same pro-
portion in training and development datasets. We
tune the feature parameters against development
set and report performance on both development
set and test set.
</bodyText>
<page confidence="0.996487">
1233
</page>
<tableCaption confidence="0.999175">
Table 1: Distributions of SCs in ACE2 corpus.
</tableCaption>
<table confidence="0.9971895">
Size PER ORG GPE FAC LOC OTH
Train 55629 20.29 7.30 8.42 0.61 0.55 62.80
Dev 6181 20.29 7.30 8.42 0.61 0.55 62.80
Test 15360 20.48 7.57 6.90 0.85 0.41 63.79
</table>
<subsectionHeader confidence="0.998673">
3.2 Lexical features
</subsectionHeader>
<bodyText confidence="0.9987224">
Each instance is represented as a bag of features
and is fed into a classifier in training stage. We
present four binary lexical feature sets as follows.
Word unigrams and bigrams: An N-gram is
a sub-sequence of N words from a given noun
phrase. Unigram forms the bag of words feature,
and bigram forms the pairs of words feature, and
so forth. We have considered word unigram and
bigram features in our experiments.
First and last words: This feature extracts the
first and last words of an NP. For example, the first
word the and the last word store are extracted from
the NP the main store. This feature does not only
coarsely models the influence of the first word, for
example, a or the, but also models the head word,
since the head word usually is the last word in the
NP.
Head word: We use Collins style rules
(Collins, 1999) to extract the head words for given
NPs. These features should be most informative
if the training corpus is large enough.2 For exam-
ple, the head word company of the NP the com-
pany immediately determines its SC being organi-
zation. However, due to the sparseness of training
data, its potential importance is adversely affected.
</bodyText>
<subsectionHeader confidence="0.999066">
3.3 Semantic features
</subsectionHeader>
<bodyText confidence="0.998874125">
NE feature is extracted from Stanford named en-
tity recognizer (NER) (Finkel et al., 2005). Three
types of named entities: person, location and or-
ganization can be recognized for a given NP. This
feature is primarily useful for SC classification of
proper nouns.
WordNet is a large English lexicon in which se-
mantically related words are connected via cogni-
tive synonyms (synsets). The WordNet is a use-
ful tool for word semantics analysis and has been
widely used in natural language processing appli-
cations. In WordNet, synsets are organized into hi-
erarchies with hypernym/hyponym relationships:
Y is a hypernym of X if every X is a (kind of) Y
(X is called a hyponym of Y in this case).
The WordNet is employed in (Ng, 2007a) as
following to create the WN CLASS feature. For
each keyword w as shown in the right column of
2It, however, is mostly useful for nominal noun phrase and
not for the pronoun and proper noun phrases.
Table 2, if the head noun of a given NP is a hy-
ponym of w in WordNet,3 then the word w be-
comes a feature for such NP. It is explained that
these keywords are correlated with the ACE SCs
and they are obtained via experimentation with
WordNet and the ACE SCs of the NPs in the ACE
training data. However, it is likely that these hand-
crafted keywords have poor coverage for general
cases. As a result, it may not make full use of
WordNet semantic knowledge. This will be shown
in our individual feature contribution experiment
in Section 3.5.
</bodyText>
<tableCaption confidence="0.86685">
Table 2: List of keywords used in WordNet seman-
tic feature in (Ng, 2007a).
</tableCaption>
<table confidence="0.996875666666667">
ACE SC Keywords
PER person
ORG social group
FAC establishment, construction, building,
facility, workplace
GPE country, province, government, town, city,
administration, society, island, community
LOC dry land, region, landmass, body of water
geographical area, geological formation
</table>
<bodyText confidence="0.999915068965517">
There are other ways of using WordNet for se-
mantic feature extraction. For example, Ponzetto
and Strube (2006) have employed WordNet sim-
ilarity measure for coreference resolution. The
difference is that they created the feature di-
rectly at the coreference resolution stage, ie, us-
ing the WordNet similarity between the antecedent
and anaphor to determine if they are coreferent,
while we focus on using this feature to classify
an NP into a particular SC. For comparison, we
implemented a WordNet similarity based feature
(WN SIM) as follows: for a given NP head word
and a key word as listed in Table 2, the WordNet
similarity package (Seco et al., 2004) models the
length of path traveling from the head word to the
key word over the WordNet network. It then com-
putes the semantic similarity based on the path.
For example, the similarity between company and
social group is 0.77, while the similarity between
company and person is 0.59. The key word which
receives the highest similarity to the head word is
marked as a feature.
The WN CLASS feature may suffer from the
coverage problem and the WN SIM feature is
heavily dependent on the definition of similarity
metric which may turn out to be inappropriate for
coreference resolution task. To make better use of
WordNet knowledge, we attempt to directly intro-
duce hypernyms for the NP head words (we denote
</bodyText>
<footnote confidence="0.85577">
3Only the first synset of the NP is used.
</footnote>
<page confidence="0.988314">
1234
</page>
<bodyText confidence="0.999960527777778">
it as WN HYP feature). The most similar work
to ours is (Daum´e III and Marcu, 2005), in which
two most common synsets from WordNet for all
words in an NP and their hypernyms are extracted
as features. We avoid augmenting the hypernyms
for non-head words in the NP to prevent introduc-
ing noisy information, which may potentially cor-
rupt the hypernym feature space.
Considering a WordNet hypernym structure as
shown in Fig. 1 for the word company, its first
synset (an institution created to conduct business)
has a unique id of 08058098 and can also be rep-
resented by a set of description words (company
in this case). Its third synset (the state of being
with someone) has an id of 13929588 and descrip-
tion words of company, companionship, fellow-
ship, society. Each synset can be extended by its
hypernym synsets. For example, the direct hyper-
nym of the first synset is the synset of 08053576
which can be described as institution, establish-
ment. The augmentation of hypernyms for NP
head words can introduce useful information, but
can also bring noise if the head word or the synset
of head word are not correctly identified. For an
optimal use of WordNet hypernyms, four ques-
tions shall be addressed: 1) how many depths are
required to tradeoff the generality (thus more in-
formative) and the specificity (thus less noisy)? 2)
which synset of the given word is needed to be
augmented? 3) which representation (synset id or
synset word) is better? and 4) is it helpful to en-
code the hypernym depth into the hypernym fea-
ture?4 These four questions provide the guideline
to search the optimal use of WordNet. We will de-
sign experiments in Section 3.5 to determine the
optimal configuration of WN HYP feature.
</bodyText>
<figure confidence="0.600423">
company
</figure>
<figureCaption confidence="0.991824">
Figure 1: WordNet hypernym hierarchy for the
word company.
</figureCaption>
<footnote confidence="0.994470666666667">
4For example, we encode the synset 08053576 as
08053576-1, with the last digit 1 indicating the depth of hy-
pernym with regard to the entry word company.
</footnote>
<subsectionHeader confidence="0.997345">
3.4 Learning algorithm
</subsectionHeader>
<bodyText confidence="0.972651153846154">
Maximum entropy (ME) models (Berger et al.,
1996; Manning and Klein, 2003), also known as
log-linear and exponential learning models, has
been adopted in the SC classification task. Max-
imum entropy models can integrate features from
many heterogeneous information sources for clas-
sification. Each feature corresponds to a constraint
on the model. Given a training set of (C, D),
where C is a set of class labels and D is a set
of feature represented data points, the maximum
entropy model attempts to maximize the log like-
lihood
exp Ai fi (c, d)
</bodyText>
<equation confidence="0.991606">
i
� c′exp Ej Ajfi(c, d)
(1)
</equation>
<bodyText confidence="0.999828">
where fi(c, d) are feature indicator functions and
Ai are the parameters to be estimated. We use ME
models for both SC classification and mention pair
classification.
</bodyText>
<subsectionHeader confidence="0.829524">
3.5 SC classification evaluation
</subsectionHeader>
<bodyText confidence="0.999768433333334">
We design three experiments to test the accuracy
of our classifiers. The first experiment evalu-
ates the individual contribution of different fea-
ture sets to SC classification accuracy. In par-
ticular, a ME model is trained on the 55,629
training instances using the following feature sets
separately: 1) unigram, 2) bigram, 3) first-last
word, 4) head word (HW), 5) named entities
(NE), 6) HW+WN CLASS, 7) HW+WN SIM,
and 8) variants of HW+WN HYP. Note that
HW+WN CLASS is the semantic feature used in
(Ng, 2007a), HW+WN SIM is the semantic fea-
ture using WordNet similarity measure (Seco et
al., 2004), and variants of HW+WN HYP are the
work proposed in this paper. We combine head
word and the semantic features due to the fact that
WordNet features are dependent on head words
and they could be treated as units. In the second
experiment, features are fed into the ME model
incrementally until all features have been used.5
Finally, we perform the feature ablation experi-
ments. That is, we remove one feature at a time
from the entire feature set and test the accuracy
loss. The SC classification performance is mea-
sured by accuracy, i.e., the proportion of the cor-
rectly classified instances among all test instances.
Individual feature contribution Table 3 shows
the SC classification accuracy of all NPs (all)
and non-pronoun NPs (non-PN) on the develop-
ment and test datasets using individual feature
</bodyText>
<footnote confidence="0.762028">
5The optimal of HW+WN HYP configuration is used.
</footnote>
<figure confidence="0.967889">
social group
state
(13928668)
friendship,friendlyrelationship
(07950920)
(00024720)
organization,organisation
relationship
(13931145)
company, companionship,
fellowship,society
(13929588)
(08053576)
company
(08058098)
(08008335)
institution,establishment
depth 4
depth 3
depth 2
depth 1
log P(C|D, A) = � log
(c,d)∈(C,D)
</figure>
<page confidence="0.710291">
1235
</page>
<tableCaption confidence="0.55797925">
sets. Among all the lexical features, unigram fea-
Table 3: SC classification accuracy of ME using
individual feature sets for development and test
ACE2 datasets.
</tableCaption>
<table confidence="0.9992299">
Feature type dev test
all non-PN all non-PN
unigram 81.3 81.6 72.4 71.9
bigram 32.5 36.4 26.3 28.4
first-last word 80.1 80.2 71.6 71.0
HW 78.2 78.0 68.3 67.1
NE 74.0 82.8 73.1 81.9
HW+WN CLASS 79.5 79.4 70.3 69.5
HW+WN SIM 81.2 81.4 73.8 73.6
HW+WN HYP (1) 82.6 83.1 74.8 74.7
HW+WN HYP (3) 82.8 83.4 75.2 75.2
HW+WN HYP (6) 83.1 83.7 75.6 75.7
HW+WN HYP (9) 83.0 83.6 75.7 75.7
HW+WN HYP (∞) 83.1 83.7 75.8 75.9
HW+WN HYP (6) 82.8 83.3 75.6 75.7
word form
HW+WN HYP (6) 82.9 83.5 75.4 75.4
depth encoded
HW+WN HYP (6) 83.0 83.6 76.4 76.6
first synset
</table>
<bodyText confidence="0.998020333333333">
ture performs the best (81.3%) for all NPs over the
development dataset. The bigram feature performs
poorly due to the sparsity problem: NPs usually
consist of one to three words. The first-last word
feature effectively models the prefix words (such
as a and the) and the head words and thus obtains a
reasonably high accuracy of 80.1%. As mentioned
before, the head word feature may suffer from the
sparsity and it results in the accuracy of 78.2%.
We also list the accuracies for non-pronoun NP
SC classification, which are slightly different com-
pared to all NP SC classification except for bi-
gram, in which the accuracy has increased 3.9%.
Although Stanford NER performs well on
named entity recognition task, it results in ac-
curacy of 74.0% for all NP SC classification,
due to its inability to deal with pronouns such
as he and common nouns such as the govern-
ment. The removal of pronouns significantly
boosts its accuracy to 82.8%. The introduc-
tion of semantic feature HW+WN CLASS boosts
the performance to 79.5% compared to the head
word alone of 78.2%. This conforms to (Ng,
2007a) that only small gain can be achieved us-
ing WN CLASS feature. The HW+WN SIM
feature outperforms HW+WN CLASS and the
accuracy reaches 81.2%. For the variants of
HW+WN HYP, we first search the optimal depth.
This is performed by using all synsets for NP head
word, encoding the feature using synset id (rather
than synset word), and no hypernym depth is en-
coded in the features. We try various depths of
1, 3, 6, 9 and ∞, with ∞ signifies that no depth
constraint is imposed. The optimal depth of 6 is
obtained with the accuracy of 83.1% over the de-
velopment dataset. We then fix the depth of 6 to try
using synset word as features, using synset id with
depth encoded as features, and using first synset
only. The results show that the optimum is to en-
code the features using hypernym synset id with-
out hypernym depth information and all synsets
are considered for hypernym extraction. This is
slightly different from the previous finding (Soon
et al., 2001; Lin, 1998b) that a coreference res-
olution system employing only the first WordNet
synset performs slightly better than that employ-
ing more than one synset.6 The best result reaches
the accuracy of 83.1%. Although the best seman-
tic feature only outperforms the best lexical fea-
ture by 1.8% on the development dataset, its gain
in the test dataset is more significant (3.2%, from
72.4% to 75.6%).
Incremental feature contribution Once we
use the training and development datasets to find
the optimal configuration of HW+WN HYP se-
mantic feature, we use all lexical features and the
optimal HW+WN HYP feature incrementally to
train an ME model over the combination of train-
ing and development datasets. Table 4 shows
the SC classification accuracy of all NPs (all)
and non-pronoun NPs (non-PN) on the train-
ing+development (we refer it as training hereafter)
and test datasets.
Table 4: SC classification accuracy of ME using
incremental feature sets for training and test ACE2
datasets.
</bodyText>
<table confidence="0.996430222222222">
Feature type train test
all non-PN all non-PN
HW 87.8 89.0 68.6 67.6
+WN HYP 87.8 89.0 75.7 75.8
+unigram 91.5 93.3 77.7 78.1
+bigram 93.1 95.2 78.7 79.2
+first-last word 93.2 95.3 78.8 79.3
+NE 93.4 95.6 83.1 84.4
Ng 2007a - 85.0 - 83.3
</table>
<bodyText confidence="0.999733">
Note that the significant higher accuracies in
training compared to test are due to the overfit-
ting problem. The interesting evaluation thus re-
mains on the test data. As can be seen, the in-
clusion of more features results in higher perfor-
mance. This is more obvious in the test dataset
than in the training dataset. The inclusion of the
</bodyText>
<footnote confidence="0.9857115">
6In fact, the accuracy of the test data supports their claims.
The accuracy using the first synset compared to using all
synsets results in the accuracy increase from 75.6% to 76.4%
for all NPs over the test dataset.
</footnote>
<page confidence="0.993989">
1236
</page>
<bodyText confidence="0.999571972222222">
optimized WN HYP feature (ie, using all synsets’
hypernyms up to 6 depth and with synset id encod-
ing) results in 7.1% increase for all NP SC classifi-
cation over test data. This shows the effectiveness
of the WN HYP features to overcome the sparsity
of head word feature. The unigram, bigram and
first-last word features offer reasonable accuracy
gain, and the final inclusion of NE boosts the over-
all performance to 83.1% for all NP and 84.4% for
non pronoun NPs over test data. This result can
be directly compared to the SC classification ac-
curacy as reported in (Ng, 2007a), in which the
highest accuracy is 83.3% for non pronoun NPs.7
The large difference between the highest training
accuracies is due to that our classifier is trained di-
rectly on the ACE2 training dataset, while their SC
classifier was trained on BBN Entity Type Corpus
(Weischedel and Brunstein, 2005), which is five
times larger than the ACE2 corpus used by us. In
addition to WordNet, they have adopted multiple
knowledge sources which include BBN’s Identi-
Finder (this is equivalent to the Stanford NER in
our work), BLLIP corpus and Reuters Corpus,8
and dependency based thesaurus (Lin, 1998a). It
is remarkable that our SC classifier can achieve
even higher accuracy only using WordNet hyper-
nym and NE features. It is worth noting that the
small accuracy gain is indeed hard to achieve con-
sidering that the test data size is large (15360).
Feature ablation experiment We now perform
the feature ablation experiments to further deter-
mine the importance of individual features. We re-
move one feature at a time from the entire feature
set. Table 5 shows the SC classification accuracy
of all NPs (all) and non-pronoun NPs (non-PN) on
the training and test datasets respectively.
</bodyText>
<tableCaption confidence="0.952371">
Table 5: SC classification accuracy of ME by re-
moving one feature at a time for training and test
ACE2 datasets.
</tableCaption>
<table confidence="0.9992279">
Feature type train test
all non-PN all non-PN
overall 93.4 95.6 83.1 84.4
-HW 93.4 95.5 82.9 84.2
-WN HYP 93.4 95.5 82.6 83.8
-HW+WN HYP 93.4 95.5 82.3 83.5
-unigram 93.4 95.5 82.9 84.2
-bigram 92.5 94.5 82.7 84.0
-first-last word 93.4 95.5 82.9 84.1
-NE 93.2 95.3 78.8 79.3
</table>
<bodyText confidence="0.558366">
Again, the significant higher accuracies in train-
ing compared to test are due to overfitting. The re-
</bodyText>
<footnote confidence="0.99164325">
7All NP accuracy was not reported as they excluded the
pronouns in creating their training and test data.
8They use these corpus to extract patterns to induce SC of
common nouns.
</footnote>
<bodyText confidence="0.999398352941177">
moval of NE feature results in the largest accuracy
loss of 4.3% (from 83.1% to 78.8%) for all nouns
on test data. It follows WN HYP (0.5% loss) and
the bigram (0.4%). If we treat HW+WN HYP as
one feature, the removal of it results in accuracy
loss of 0.8% for all nouns on test data. The un-
igram, first-last word and head word each results
in the loss of 0.2%. The reason that the removal
of NE results in a much significant loss is due to
the fact that the NE feature is quite different from
other features. Its strength is to distinguish SCs for
proper names, while other features are more sim-
ilar (their targets are common nouns). The pro-
posed use of HW+WN HYP can bring 0.8% gain
on top of other features, higher than other informa-
tive lexical features including unigram and first-
last word.
</bodyText>
<subsectionHeader confidence="0.991052">
3.6 Error analysis
</subsectionHeader>
<bodyText confidence="0.999986227272727">
A closer look at the errors produced by our SC
classifier reveals that the second probable label is
very likely to be the actual labels if the first proba-
ble one is wrong. In fact, if we allow the classifier
to predict two most probable labels and the clas-
sification is judged to be true if the actual label is
one of the two predictions, then the classification
accuracy increases from 83.1% to 96.4%. This
is because that the same noun phrases are some-
times annotated with one of the five explicitly an-
notated classes while sometimes are not annotated
at all (thus falling into the other SC). Again for
the example of people. It is annotated as person
SC 20 times and is not annotated at all 21 times.
Given the same feature set for this instance, the
best the classifier can do is to classify it to other
semantic class. To address this annotation incon-
sistency issue, we relax the SC agreement feature
from the strict match in designing coreference res-
olution features. For example, if the first probable
SC of an NP matches the second probable SC of
another NP, we still give some partial match credit.
</bodyText>
<sectionHeader confidence="0.905843" genericHeader="method">
4 Application to Coreference Resolution
</sectionHeader>
<bodyText confidence="0.9998904">
We can now incorporate the NP SC classifier into
our ME based coreference resolution system. This
section examines how our WordNet hypernym fea-
tures help improve the coreference resolution per-
formance.
</bodyText>
<subsectionHeader confidence="0.992737">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9999685">
We use the ACE-2 (version 1.0) coreference cor-
pus. Each raw text in this corpus was prepro-
cessed automatically by a pipeline of NLP com-
ponents, including sentence boundary detection,
</bodyText>
<page confidence="0.984215">
1237
</page>
<bodyText confidence="0.998876">
POS-tagging and text chunking. The statistics of
corpus and mention extraction are shown in Table
6, where g-mention is the automatically extracted
mentions which contain the annotated (gold) men-
tions. The recalls of gold mentions are 95.88%
and 95.93% for training and test data respectively.
</bodyText>
<tableCaption confidence="0.542526">
Table 6: Statistics for corpus and extracted men-
tions.
</tableCaption>
<table confidence="0.653949333333333">
text# mention# g-mention# gold# recall(%)
train 422 61810 22990 23977 95.88
test 97 15360 5561 5797 95.93
</table>
<bodyText confidence="0.999501310344827">
Our coreference system uses Maximum En-
tropy model to determine whether two NPs are
coreferent. As in (Soon et al., 2001; Ponzetto and
Strube, 2006), we generate training instances as
follows: a positive instance is created for each
anaphoric NP, NPj, and its closest antecedent,
NPi; and a negative instance is created for NPj
paired with each of the intervening NPs, NPi+1,
NPi+2, ..., NPj−1. Each instance is represented
by syntactic or semantic features described as fol-
lows. All training data are used to train a maxi-
mum entropy model. In the test stage,we select the
closest preceding NP that is classified as corefer-
ent with NPj as the antecedent of NPj. If no such
NP exists, no antecedent is selected for NPj.
Unlike other natural language processing tasks
such as information extraction which have de facto
evaluation metrics, it is an open question which
evaluation is the most suitable one. The evalu-
ation becomes more complicated when automat-
ically extracted mentions (in contrast to the gold
mentions) are used. To facilitate the comparison
with previous work, we report performance us-
ing two different scoring metrics: the commonly-
used MUC scorer (Vilain et al., 1995) and the ac-
curacy of the anaphoric references (Ponzetto and
Strube, 2006). An anaphoric reference is correctly
resolved if it and its closest antecedent are in the
same coreference chain in the resulting partition.
</bodyText>
<subsectionHeader confidence="0.994502">
4.2 Baseline features
</subsectionHeader>
<bodyText confidence="0.999993928571429">
We briefly review the baseline features used in
this paper as follows. More detailed information
and implementations can be found at (Soon et al.,
2001; Versley et al., 2008). For example, the
ALIAS feature takes values of true or false. The
value of true means that the antecedent and the
anaphor refer to the same entity (date, person, or-
ganization or location). The ALIAS feature de-
tection works differently depending on the named
entity type. For date, the day, month, and year
values are extracted and compared. For person,
the last words of the noun phrases are compared.
For organization names, the alias detection checks
for acronym match such as IBM and International
</bodyText>
<subsectionHeader confidence="0.419985">
Business Machines Corp.
</subsectionHeader>
<bodyText confidence="0.999874795454546">
Lexical features STRING MATCH: true if
NPi and NPj have the same spelling after remov-
ing article and demonstrative pronouns, false oth-
erwise. ALIAS: true if NPj is the alias of NPi.
Grammatical features I PRONOUN: true if
NPi is a pronoun; J PRONOUN: true if NPj
is pronoun; J REFL PRONOUN: true if NPj is
reflexive pronoun; J PERS PRONOUN: true if
NPj is personal pronoun; J POSS PRONOUN:
true if NPj is possessive pronoun; J PN: true
if NPj is proper noun; J DEF: true if NPj
starts with the; J DEM: true if NPj starts with
this, that, these or those; J DEM NOMINAL:
true if NPj is a demonstrative nominal noun;
J DEM PRONOUN: true if NPj is a demonstra-
tive pronoun; PROPER NAME: true if both NPi
and NPj are proper names; NUMBER: true if
NPi and NPj agree in number; GENDER: true
if NPi and NPj agree in gender; APPOSITIVE:
true if NPi and NPj are appositions.
Distance feature DISTANCE: how many sen-
tences NPi and NPj are apart.
Semantic feature SEMCLASS: This feature is
implemented from (Soon et al., 2001). Its possible
values are true, false, or unknown. First the fol-
lowing semantic classes are defined: female, male,
person, organization, location, date, time, money,
percent, and object. Each of these defined seman-
tic classes is then mapped to a WordNet synset.
Then the semantic class determination module de-
termines the semantic class for every NP as the
first synset of the head noun of the NP. If such
synset is a hyponym of defined semantic class,
then such semantic class is assigned to the NP.
Otherwise, unknown class is assigned. Finally, the
agreement of semantic classes of NPi and NPj is
unknown if either assigned class is unknown; true
if their assigned class are the same, false other-
wise. Notice that the WordNet use in (Ng, 2007a)
and this feature apply in the same principle except
that 1) the former is used in SC classification while
the latter is used directly for coreference resolu-
tion, and 2) they have different semantic class cat-
egories.
</bodyText>
<subsectionHeader confidence="0.989644">
4.3 Proposed WordNet agreement features
</subsectionHeader>
<bodyText confidence="0.998311333333333">
For each instance which consists of NPi and NPj,
we apply our SC classifier to label them, say li and
lj respectively. We then use these two induced la-
</bodyText>
<page confidence="0.971821">
1238
</page>
<bodyText confidence="0.999839">
bels to propose the SC agreement feature for NPi
and NPj. In particular, SC STRICT is true if li
and lj are the same and they are not of other type,
false otherwise; SC COARSE is true if both li and
lj are not of other type; In addition, we propose
two other SC agreement features to cope with the
SC classification errors. SC RELAX1 is true if the
first probable of NPi, li1, is not other type and is
the same as the second probable of Nj, lj2, or vice
visa. SC RELAX2 is true if the second probable
of NPi, li2, is not other type and is the same as the
second probable label of NPj, lj2. The purpose in
using SC RELAX1 and SC RELAX2 features is
to relax the strict SC agreement feature in the hope
that partial SC match is useful for coreference res-
olution.
</bodyText>
<subsectionHeader confidence="0.99482">
4.4 Coreference results
</subsectionHeader>
<bodyText confidence="0.999972176470588">
Table 7 shows the MUC score for ACE2 corpus
and its three partitions: bnews, npaper, and nwire
using baseline and the proposed semantic features.
It also shows the accuracy of resolving anaphors
for all nouns in ACE2 corpus. SC STRICT is
the configuration that uses the baseline features
with the SEMCLASS (Soon et al., 2001) replaced
by SC STRICT, and SC COARSE, SC RELAX1,
and SC RELAX2 are incrementally included into
the SC STRICT feature set.
As can be seen, the SC STRICT significantly
boosts the performance: it improves the MUC
F score and anaphor accuracy of baseline from
57.7% to 65.7% and 37.7% to 46.3% respectively.
It is remarkable that the new use of WordNet can
obtain such significant gain in both MUC score
and anaphor accuracy. The large improvement
of the precision from 58.1% to 73.3% for all
NPs shows that the SC STRICT feature can ef-
fectively filter out the semantic incompatible pairs
of antecedents and anaphors. In accordance with
our hypothesis, the relaxation of strict SC agree-
ment by including SC COARSE, SC RELAX1
and SC RELAX2 help improve the performance
further, which is reflected by both MUC score and
anaphor accuracy. For example, compared to the
baseline, the use of all proposed four SC agree-
ment features results in the maximal accuracy gain
of 9.7% (from 37.7% to 47.4%) and the use of
SC STRICT, SC COARSE, and SC RELAX1 re-
sults in the maximal MUC score gain of 10.4%
(from 57.7% to 68.1%).
Our best MUC score is 68.1% which outper-
forms the MUC score of 64.6% as reported in
(Ng, 2007a) by 3.5%, while our best accuracy
of anaphor is 47.4%, which is 4.1% less than
the accuracy of 51.5% in (Ng, 2007a). Note
that, unlike (Ng, 2007a) which performed exten-
sive experiments using different machine learn-
ing algorithms, alternative use of features (either
constraint or normal features), and heterogeneous
knowledge sources, this paper simply uses one
learning classifier (ME model) and only employs
WordNet and Stanford NER semantic sources.
The different MUC and accuracy scores reflect
the non-trivial cases of evaluating coreference sys-
tems. While we leave out the discussion of which
evaluation is more appropriate, we focus on show-
ing that the proposed SC classifier can bring sig-
nificant boost from the baseline using both MUC
and accuracy metrics.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999905">
We have showed that the traditional use of Word-
Net in coreference resolution may not effectively
exploit the WordNet semantic knowledge. We pro-
posed new ways to extract WordNet feature. This
feature, along with other features such as named
entity feature, can be used to build an accurate se-
mantic class (SC) classifier. In addition, we ana-
lyzed the classification errors of the SC classifier
and relaxed SC agreement features to cope with
part of the classification errors. The proposed ac-
curate SC classifier and the relaxation of SC agree-
ment features can boost our baseline coreference
resolution system by 10.4% and 9.7% using MUC
score and anaphor accuracy respectively.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999887833333333">
We wish to thank Yannick Versley for his sup-
port with BART coreference resolution system and
the three anonymous reviewers for their invaluable
comments. This research was supported by British
Telecom grant CT1080028046 and BISC Program
of UC Berkeley.
</bodyText>
<sectionHeader confidence="0.999244" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998549727272727">
A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39–
71.
M. Collins 1999. Head-driven statistical models for
natural language parsing. PhD thesis, University of
Pennsylvania.
H. Daum´e III and D. Marcu. 2005. A large-scale
exploration of effective global features for a joint
entity detection and tracking model. In Proc. of
HLT/EMNLP, pages 97-104.
</reference>
<page confidence="0.998607">
1239
</page>
<tableCaption confidence="0.999579">
Table 7: MUC score and accuracy of baseline and proposed SC agreement features for ACE2 dataset.
</tableCaption>
<table confidence="0.999799333333333">
MUC score Accuracy
All bnews npaper nwire All
System R P F R P F R P F R P F
baseline 57.4 58.1 57.7 56.6 55.4 56.0 59.3 60.4 59.9 56.2 58.6 57.3 37.7
Ng 2007a 59.5 70.6 64.6 - - - - - - - - - 51.5
SC STRICT 59.6 73.3 65.7 61.6 72.8 66.7 60.3 74.9 66.8 56.8 72.1 63.5 46.3
+ SC COARSE 59.2 76.7 66.8 61.0 76.7 67.9 59.8 77.2 67.4 56.6 76.2 64.9 45.9
+ SC RELAX1 59.8 79.0 68.1 61.3 79.8 69.3 60.9 80.3 69.3 57.2 76.7 65.5 47.2
+ SC RELAX2 60.2 77.7 67.8 61.5 78.2 68.9 61.4 78.9 69.1 57.5 75.7 65.4 47.4
</table>
<reference confidence="0.999847434210526">
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In HLT-NAACL.
C. Fellbaum. 1998. An electronic lexical database.
The MIT press.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
ACL, pages 363-370.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni.
2006. Factorizing complex models: a case study in
mention detection. In Proc. of COLING/ACL, pages
473-480.
S. M. Harabagiu, R. C. Bunescu, and S. J. Maiorano.
2001. Text and knowledge mining for coreference
resolution. In Proc. of NAACL, pages 55-62.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proc. of HLT/NAACL,
pages 289-296.
D. Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proc. of COLING/ACL, pages
768-774.
D. Lin. 1998b. Using collocation statistics in informa-
tion extraction. In Proc. of MUC-7.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S.
Roukos. 2004. A mention synchronous coreference
resolution algorithm based on the Bell tree. In Proc.
of the ACL.
C. Manning and D. Klein. 2003. Optimization,
Maxent Models, and Conditional Estimation with-
out Magic. Tutorial at HLT-NAACL 2003 and ACL
2003.
K. Markert and M. Nissim. 2005. Comparing knowl-
edge sources for nominal anaphora resolution. Com-
putational Linguistics, 31(3):367-401.
A. McCallum and B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Proc. of the NIPS.
V. Ng. 2007a. Semantic Class Induction and Corefer-
ence Resolution. In Proc. of the ACL.
V. Ng. 2007b. Shallow Semantics for Coreference
Resolution. In Proc. of the IJCAI.
C. Nicolae and G. Nicolae 2006. BESTCUT: A Graph
Algorithm for Coreference Resolution. In Proc. of
the EMNLP.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004. Learning to resolve bridging references. In
Proc. of the ACL.
S. P. Ponzetto and M. Strube. 2006. Exploiting se-
mantic role labeling, WordNet and Wikipedia for
coreference resolution. In Proc. of the HLT/NAACL,
pages 192-199.
N. Seco, T. Veale, and J. Hayes. 2004. An Intrinsic
Information Content Metric for Semantic Similarity
in WordNet. Proc. of the European Conference of
Artificial Intelligence.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. A machine
learning approach to coreference resolution of noun
phrases. Computation Linguistics, 27(4):521-544.
Y. Versley, S. P. Ponzetto, M. Poesio, V. Eidelman, A.
Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
BART: a modular toolkit for coreference resolution.
ACL 2008 System demo.
R. Vieira and M. Poesio. 2000. An empirically-based
system for processing definite descriptions. Compu-
tational Linguistics, 26(4):539-593.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A model-theoretic coreference
scoreing scheme. In Proc. of MUC-6, pages 45-52.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Linguistic Data
Consortium.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automat-
ically discovered pattens. In Proc. of the ACL.
</reference>
<page confidence="0.987425">
1240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732164">
<title confidence="0.997552">Accurate Semantic Class Classifier for Coreference Resolution</title>
<author confidence="0.851367">Guangping Weiqun</author>
<author confidence="0.851367">Asli</author>
<affiliation confidence="0.882057">Department, University of California at</affiliation>
<address confidence="0.987534">CA 94720, USA</address>
<affiliation confidence="0.988691666666667">Science Department, School of Information University of Science and Technology Beijing, Institute of Acoustics, Chinese Academy of</affiliation>
<address confidence="0.999118">Beijing, 100190, China</address>
<email confidence="0.986846">xuweiqun@hccl.ioa.ac.cn</email>
<abstract confidence="0.99817652631579">There have been considerable attempts to incorporate semantic knowledge into coreference resolution systems: different knowledge sources such as WordNet and Wikipedia have been used to boost the performance. In this paper, we propose new ways to extract WordNet feature. This feature, along with other features such as named entity feature, can be used to build accurate class classifier. In addition, we analyze the SC classification errors and propose to use relaxed SC agreement features. The proposed accurate SC classifier and the relaxation of SC agreement features on ACE2 coreference evaluation can boost our baseline system by 10.4% and 9.7% using MUC score and anaphor accuracy respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>71</pages>
<contexts>
<context position="15983" citStr="Berger et al., 1996" startWordPosition="2670" endWordPosition="2673">ented? 3) which representation (synset id or synset word) is better? and 4) is it helpful to encode the hypernym depth into the hypernym feature?4 These four questions provide the guideline to search the optimal use of WordNet. We will design experiments in Section 3.5 to determine the optimal configuration of WN HYP feature. company Figure 1: WordNet hypernym hierarchy for the word company. 4For example, we encode the synset 08053576 as 08053576-1, with the last digit 1 indicating the depth of hypernym with regard to the entry word company. 3.4 Learning algorithm Maximum entropy (ME) models (Berger et al., 1996; Manning and Klein, 2003), also known as log-linear and exponential learning models, has been adopted in the SC classification task. Maximum entropy models can integrate features from many heterogeneous information sources for classification. Each feature corresponds to a constraint on the model. Given a training set of (C, D), where C is a set of class labels and D is a set of feature represented data points, the maximum entropy model attempts to maximize the log likelihood exp Ai fi (c, d) i � c′exp Ej Ajfi(c, d) (1) where fi(c, d) are feature indicator functions and Ai are the parameters t</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39– 71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10398" citStr="Collins, 1999" startWordPosition="1712" endWordPosition="1713"> from a given noun phrase. Unigram forms the bag of words feature, and bigram forms the pairs of words feature, and so forth. We have considered word unigram and bigram features in our experiments. First and last words: This feature extracts the first and last words of an NP. For example, the first word the and the last word store are extracted from the NP the main store. This feature does not only coarsely models the influence of the first word, for example, a or the, but also models the head word, since the head word usually is the last word in the NP. Head word: We use Collins style rules (Collins, 1999) to extract the head words for given NPs. These features should be most informative if the training corpus is large enough.2 For example, the head word company of the NP the company immediately determines its SC being organization. However, due to the sparseness of training data, its potential importance is adversely affected. 3.3 Semantic features NE feature is extracted from Stanford named entity recognizer (NER) (Finkel et al., 2005). Three types of named entities: person, location and organization can be recognized for a given NP. This feature is primarily useful for SC classification of p</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins 1999. Head-driven statistical models for natural language parsing. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>A large-scale exploration of effective global features for a joint entity detection and tracking model.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP,</booktitle>
<pages>97--104</pages>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>H. Daum´e III and D. Marcu. 2005. A large-scale exploration of effective global features for a joint entity detection and tracking model. In Proc. of HLT/EMNLP, pages 97-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="1848" citStr="Denis and Baldridge, 2007" startWordPosition="270" endWordPosition="273"> resolution is used to determine which noun phrases (including pronouns, proper names, and common nouns) refer to the same entities in documents. Much work on coreference resolution is based on (Soon et al., 2001), which built a decision tree classifier to label pairs of mentions as coreferent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>An electronic lexical database.</title>
<date>1998</date>
<publisher>The MIT press.</publisher>
<contexts>
<context position="3412" citStr="Fellbaum, 1998" startWordPosition="521" endWordPosition="522">o compute the semantic class features. We introduce new ways to use the WordNet and the experiments show its effectiveness in determining the semantic classes for noun phrases. In addition, we analyze the classification errors of the SC classifier and propose to use relaxed SC agreement features. With these proposed features and other standard syntactic features (which are commonly employed in existing coreference systems), our coreference resolution system can obtain an increase of 10.4% for MUC score and 9.7% for anaphor accuracy from the baseline in ACE2 evaluation. 2 Related Work WordNet (Fellbaum, 1998) as an important knowledge source has been widely employed in previous coreference resolution work. For example, Harabagiu et al. (2001) have used WordNet relations such as synonym and is-a to mine the patterns of WordNet paths for pairs of antecedents and anaphors. Due to the nature of the rule based coreference system (in contrast to machine learning based), the weights of relations may not be accurately estimated. Vieira and Poesio (2000) and Markert and Nissim (2005) have used WordNet synonym and hyponym etc. to determine if an anaphor semantically relates to one previous NP. Ponzetto and </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. An electronic lexical database. The MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="10838" citStr="Finkel et al., 2005" startWordPosition="1783" endWordPosition="1786"> the first word, for example, a or the, but also models the head word, since the head word usually is the last word in the NP. Head word: We use Collins style rules (Collins, 1999) to extract the head words for given NPs. These features should be most informative if the training corpus is large enough.2 For example, the head word company of the NP the company immediately determines its SC being organization. However, due to the sparseness of training data, its potential importance is adversely affected. 3.3 Semantic features NE feature is extracted from Stanford named entity recognizer (NER) (Finkel et al., 2005). Three types of named entities: person, location and organization can be recognized for a given NP. This feature is primarily useful for SC classification of proper nouns. WordNet is a large English lexicon in which semantically related words are connected via cognitive synonyms (synsets). The WordNet is a useful tool for word semantics analysis and has been widely used in natural language processing applications. In WordNet, synsets are organized into hierarchies with hypernym/hyponym relationships: Y is a hypernym of X if every X is a (kind of) Y (X is called a hyponym of Y in this case). T</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proc. of ACL, pages 363-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>I Zitouni</author>
</authors>
<title>Factorizing complex models: a case study in mention detection.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>473--480</pages>
<contexts>
<context position="5912" citStr="Florian et al., 2006" startWordPosition="927" endWordPosition="930">es. WordNet is just one of the several knowledge sources which have been utilized. However, the WordNet based features is not informative compared to other features such as the semantic neighbor feature. Similarly, Ponzetto and Strube (2006) have discovered that the WordNet feature is no more informative than the community-generated Wikipedia feature. In this paper, we focus on the investigation of various usages of WordNet for the SC classification task. The work which is directly comparable to ours would be (Ng, 2007a; Ng, 2007b). Other similar work includes the mention detection (MD) task (Florian et al., 2006) and joint probabilistic model of coreference (Daum´e III and Marcu, 2005). The MD task identifies the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, organization). Unlike them, we do not perform the boundary detection, as we make use of the noun phrases directly from the noun phrase chunker and NE recognizer. The joint probabilistic model models the MD and coreference simultaneously, while our work focuses on them separately. 3 Semantic Class Classification In this section, we describe how we compile the training corpus and extract features</context>
</contexts>
<marker>Florian, Jing, Kambhatla, Zitouni, 2006</marker>
<rawString>R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006. Factorizing complex models: a case study in mention detection. In Proc. of COLING/ACL, pages 473-480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Harabagiu</author>
<author>R C Bunescu</author>
<author>S J Maiorano</author>
</authors>
<title>Text and knowledge mining for coreference resolution.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="3548" citStr="Harabagiu et al. (2001)" startWordPosition="541" endWordPosition="544">termining the semantic classes for noun phrases. In addition, we analyze the classification errors of the SC classifier and propose to use relaxed SC agreement features. With these proposed features and other standard syntactic features (which are commonly employed in existing coreference systems), our coreference resolution system can obtain an increase of 10.4% for MUC score and 9.7% for anaphor accuracy from the baseline in ACE2 evaluation. 2 Related Work WordNet (Fellbaum, 1998) as an important knowledge source has been widely employed in previous coreference resolution work. For example, Harabagiu et al. (2001) have used WordNet relations such as synonym and is-a to mine the patterns of WordNet paths for pairs of antecedents and anaphors. Due to the nature of the rule based coreference system (in contrast to machine learning based), the weights of relations may not be accurately estimated. Vieira and Poesio (2000) and Markert and Nissim (2005) have used WordNet synonym and hyponym etc. to determine if an anaphor semantically relates to one previous NP. Ponzetto and Strube (2006) have used WordNet semantic similarity and relatedness scores between antecedents and candidate anaphors. Their 1232 Procee</context>
</contexts>
<marker>Harabagiu, Bunescu, Maiorano, 2001</marker>
<rawString>S. M. Harabagiu, R. C. Bunescu, and S. J. Maiorano. 2001. Text and knowledge mining for coreference resolution. In Proc. of NAACL, pages 55-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
<author>D Appelt</author>
<author>L Taylor</author>
<author>A Simma</author>
</authors>
<title>The (non)utility of predicate-argument frequencies for pronoun interpretation.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>289--296</pages>
<contexts>
<context position="2335" citStr="Kehler et al., 2004" startWordPosition="345" endWordPosition="348"> Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference between semantically incompatible NPs. However, the difficulty is to accurately compute the semantic class features. In this paper, we show that the WordNet may not be efficiently employed in the traditional way such as (Soon et al., 2001; Ng, 2007a; Ponzetto and Strube, 2006) to compute the semantic class features. We introduce new ways to use the WordNet and the experiments show its effectiveness in determining </context>
</contexts>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. In Proc. of HLT/NAACL, pages 289-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="21256" citStr="Lin, 1998" startWordPosition="3566" endWordPosition="3567">ed in the features. We try various depths of 1, 3, 6, 9 and ∞, with ∞ signifies that no depth constraint is imposed. The optimal depth of 6 is obtained with the accuracy of 83.1% over the development dataset. We then fix the depth of 6 to try using synset word as features, using synset id with depth encoded as features, and using first synset only. The results show that the optimum is to encode the features using hypernym synset id without hypernym depth information and all synsets are considered for hypernym extraction. This is slightly different from the previous finding (Soon et al., 2001; Lin, 1998b) that a coreference resolution system employing only the first WordNet synset performs slightly better than that employing more than one synset.6 The best result reaches the accuracy of 83.1%. Although the best semantic feature only outperforms the best lexical feature by 1.8% on the development dataset, its gain in the test dataset is more significant (3.2%, from 72.4% to 75.6%). Incremental feature contribution Once we use the training and development datasets to find the optimal configuration of HW+WN HYP semantic feature, we use all lexical features and the optimal HW+WN HYP feature incr</context>
<context position="24196" citStr="Lin, 1998" startWordPosition="4067" endWordPosition="4068">d in (Ng, 2007a), in which the highest accuracy is 83.3% for non pronoun NPs.7 The large difference between the highest training accuracies is due to that our classifier is trained directly on the ACE2 training dataset, while their SC classifier was trained on BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which is five times larger than the ACE2 corpus used by us. In addition to WordNet, they have adopted multiple knowledge sources which include BBN’s IdentiFinder (this is equivalent to the Stanford NER in our work), BLLIP corpus and Reuters Corpus,8 and dependency based thesaurus (Lin, 1998a). It is remarkable that our SC classifier can achieve even higher accuracy only using WordNet hypernym and NE features. It is worth noting that the small accuracy gain is indeed hard to achieve considering that the test data size is large (15360). Feature ablation experiment We now perform the feature ablation experiments to further determine the importance of individual features. We remove one feature at a time from the entire feature set. Table 5 shows the SC classification accuracy of all NPs (all) and non-pronoun NPs (non-PN) on the training and test datasets respectively. Table 5: SC cl</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998a. Automatic retrieval and clustering of similar words. In Proc. of COLING/ACL, pages 768-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using collocation statistics in information extraction.</title>
<date>1998</date>
<booktitle>In Proc. of MUC-7.</booktitle>
<contexts>
<context position="21256" citStr="Lin, 1998" startWordPosition="3566" endWordPosition="3567">ed in the features. We try various depths of 1, 3, 6, 9 and ∞, with ∞ signifies that no depth constraint is imposed. The optimal depth of 6 is obtained with the accuracy of 83.1% over the development dataset. We then fix the depth of 6 to try using synset word as features, using synset id with depth encoded as features, and using first synset only. The results show that the optimum is to encode the features using hypernym synset id without hypernym depth information and all synsets are considered for hypernym extraction. This is slightly different from the previous finding (Soon et al., 2001; Lin, 1998b) that a coreference resolution system employing only the first WordNet synset performs slightly better than that employing more than one synset.6 The best result reaches the accuracy of 83.1%. Although the best semantic feature only outperforms the best lexical feature by 1.8% on the development dataset, its gain in the test dataset is more significant (3.2%, from 72.4% to 75.6%). Incremental feature contribution Once we use the training and development datasets to find the optimal configuration of HW+WN HYP semantic feature, we use all lexical features and the optimal HW+WN HYP feature incr</context>
<context position="24196" citStr="Lin, 1998" startWordPosition="4067" endWordPosition="4068">d in (Ng, 2007a), in which the highest accuracy is 83.3% for non pronoun NPs.7 The large difference between the highest training accuracies is due to that our classifier is trained directly on the ACE2 training dataset, while their SC classifier was trained on BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which is five times larger than the ACE2 corpus used by us. In addition to WordNet, they have adopted multiple knowledge sources which include BBN’s IdentiFinder (this is equivalent to the Stanford NER in our work), BLLIP corpus and Reuters Corpus,8 and dependency based thesaurus (Lin, 1998a). It is remarkable that our SC classifier can achieve even higher accuracy only using WordNet hypernym and NE features. It is worth noting that the small accuracy gain is indeed hard to achieve considering that the test data size is large (15360). Feature ablation experiment We now perform the feature ablation experiments to further determine the importance of individual features. We remove one feature at a time from the entire feature set. Table 5 shows the SC classification accuracy of all NPs (all) and non-pronoun NPs (non-PN) on the training and test datasets respectively. Table 5: SC cl</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998b. Using collocation statistics in information extraction. In Proc. of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention synchronous coreference resolution algorithm based on the Bell tree.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="1744" citStr="Luo et al., 2004" startWordPosition="254" endWordPosition="257">by 10.4% and 9.7% using MUC score and anaphor accuracy respectively. 1 Introduction Coreference resolution is used to determine which noun phrases (including pronouns, proper names, and common nouns) refer to the same entities in documents. Much work on coreference resolution is based on (Soon et al., 2001), which built a decision tree classifier to label pairs of mentions as coreferent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An impo</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention synchronous coreference resolution algorithm based on the Bell tree. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>D Klein</author>
</authors>
<date>2003</date>
<booktitle>Optimization, Maxent Models, and Conditional Estimation without Magic. Tutorial at HLT-NAACL 2003 and ACL</booktitle>
<contexts>
<context position="16009" citStr="Manning and Klein, 2003" startWordPosition="2674" endWordPosition="2677">sentation (synset id or synset word) is better? and 4) is it helpful to encode the hypernym depth into the hypernym feature?4 These four questions provide the guideline to search the optimal use of WordNet. We will design experiments in Section 3.5 to determine the optimal configuration of WN HYP feature. company Figure 1: WordNet hypernym hierarchy for the word company. 4For example, we encode the synset 08053576 as 08053576-1, with the last digit 1 indicating the depth of hypernym with regard to the entry word company. 3.4 Learning algorithm Maximum entropy (ME) models (Berger et al., 1996; Manning and Klein, 2003), also known as log-linear and exponential learning models, has been adopted in the SC classification task. Maximum entropy models can integrate features from many heterogeneous information sources for classification. Each feature corresponds to a constraint on the model. Given a training set of (C, D), where C is a set of class labels and D is a set of feature represented data points, the maximum entropy model attempts to maximize the log likelihood exp Ai fi (c, d) i � c′exp Ej Ajfi(c, d) (1) where fi(c, d) are feature indicator functions and Ai are the parameters to be estimated. We use ME </context>
</contexts>
<marker>Manning, Klein, 2003</marker>
<rawString>C. Manning and D. Klein. 2003. Optimization, Maxent Models, and Conditional Estimation without Magic. Tutorial at HLT-NAACL 2003 and ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markert</author>
<author>M Nissim</author>
</authors>
<title>Comparing knowledge sources for nominal anaphora resolution.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--3</pages>
<contexts>
<context position="3887" citStr="Markert and Nissim (2005)" startWordPosition="600" endWordPosition="603"> obtain an increase of 10.4% for MUC score and 9.7% for anaphor accuracy from the baseline in ACE2 evaluation. 2 Related Work WordNet (Fellbaum, 1998) as an important knowledge source has been widely employed in previous coreference resolution work. For example, Harabagiu et al. (2001) have used WordNet relations such as synonym and is-a to mine the patterns of WordNet paths for pairs of antecedents and anaphors. Due to the nature of the rule based coreference system (in contrast to machine learning based), the weights of relations may not be accurately estimated. Vieira and Poesio (2000) and Markert and Nissim (2005) have used WordNet synonym and hyponym etc. to determine if an anaphor semantically relates to one previous NP. Ponzetto and Strube (2006) have used WordNet semantic similarity and relatedness scores between antecedents and candidate anaphors. Their 1232 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1232–1240, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP work is different to this work in the following: 1) Their work involves various relations such as hyponyms and meronyms while ours only makes use of hypernyms; and 2) Their work focuses on in</context>
</contexts>
<marker>Markert, Nissim, 2005</marker>
<rawString>K. Markert and M. Nissim. 2005. Comparing knowledge sources for nominal anaphora resolution. Computational Linguistics, 31(3):367-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<booktitle>In Proc. of the NIPS.</booktitle>
<contexts>
<context position="1921" citStr="McCallum and Wellner, 2004" startWordPosition="280" endWordPosition="283"> proper names, and common nouns) refer to the same entities in documents. Much work on coreference resolution is based on (Soon et al., 2001), which built a decision tree classifier to label pairs of mentions as coreferent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference bet</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>A. McCallum and B. Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In Proc. of the NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Semantic Class Induction and Coreference Resolution.</title>
<date>2007</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="2116" citStr="Ng, 2007" startWordPosition="312" endWordPosition="313">referent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference between semantically incompatible NPs. However, the difficulty is to accurately compute the semantic class features. In this paper, we show that the WordNet may not be efficiently employed in the tr</context>
<context position="5189" citStr="Ng, 2007" startWordPosition="816" endWordPosition="817">ordNet hypernyms for their SC classification and then testing their SC compatibility. In doing so, we can directly model the accuracy of semantic class classification and test its impact on coreference resolution. While the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many coreference resolution systems simply assign to a common noun the first (i.e., most frequent) WordNet synset as its SC (Soon et al., 2001; Markert and Nissim, 2005). This heuristics, apparently, did not lead to good performance. The best reported ACE2 coreference resolution system (Ng, 2007a; Ng, 2007b) has proposed an accurate SC classifier which used heterogeneous semantic knowledge sources. WordNet is just one of the several knowledge sources which have been utilized. However, the WordNet based features is not informative compared to other features such as the semantic neighbor feature. Similarly, Ponzetto and Strube (2006) have discovered that the WordNet feature is no more informative than the community-generated Wikipedia feature. In this paper, we focus on the investigation of various usages of WordNet for the SC classification task. The work which is directly comparable </context>
<context position="11473" citStr="Ng, 2007" startWordPosition="1896" endWordPosition="1897">ties: person, location and organization can be recognized for a given NP. This feature is primarily useful for SC classification of proper nouns. WordNet is a large English lexicon in which semantically related words are connected via cognitive synonyms (synsets). The WordNet is a useful tool for word semantics analysis and has been widely used in natural language processing applications. In WordNet, synsets are organized into hierarchies with hypernym/hyponym relationships: Y is a hypernym of X if every X is a (kind of) Y (X is called a hyponym of Y in this case). The WordNet is employed in (Ng, 2007a) as following to create the WN CLASS feature. For each keyword w as shown in the right column of 2It, however, is mostly useful for nominal noun phrase and not for the pronoun and proper noun phrases. Table 2, if the head noun of a given NP is a hyponym of w in WordNet,3 then the word w becomes a feature for such NP. It is explained that these keywords are correlated with the ACE SCs and they are obtained via experimentation with WordNet and the ACE SCs of the NPs in the ACE training data. However, it is likely that these handcrafted keywords have poor coverage for general cases. As a result</context>
<context position="17218" citStr="Ng, 2007" startWordPosition="2881" endWordPosition="2882">odels for both SC classification and mention pair classification. 3.5 SC classification evaluation We design three experiments to test the accuracy of our classifiers. The first experiment evaluates the individual contribution of different feature sets to SC classification accuracy. In particular, a ME model is trained on the 55,629 training instances using the following feature sets separately: 1) unigram, 2) bigram, 3) first-last word, 4) head word (HW), 5) named entities (NE), 6) HW+WN CLASS, 7) HW+WN SIM, and 8) variants of HW+WN HYP. Note that HW+WN CLASS is the semantic feature used in (Ng, 2007a), HW+WN SIM is the semantic feature using WordNet similarity measure (Seco et al., 2004), and variants of HW+WN HYP are the work proposed in this paper. We combine head word and the semantic features due to the fact that WordNet features are dependent on head words and they could be treated as units. In the second experiment, features are fed into the ME model incrementally until all features have been used.5 Finally, we perform the feature ablation experiments. That is, we remove one feature at a time from the entire feature set and test the accuracy loss. The SC classification performance </context>
<context position="20288" citStr="Ng, 2007" startWordPosition="3392" endWordPosition="3393">or non-pronoun NP SC classification, which are slightly different compared to all NP SC classification except for bigram, in which the accuracy has increased 3.9%. Although Stanford NER performs well on named entity recognition task, it results in accuracy of 74.0% for all NP SC classification, due to its inability to deal with pronouns such as he and common nouns such as the government. The removal of pronouns significantly boosts its accuracy to 82.8%. The introduction of semantic feature HW+WN CLASS boosts the performance to 79.5% compared to the head word alone of 78.2%. This conforms to (Ng, 2007a) that only small gain can be achieved using WN CLASS feature. The HW+WN SIM feature outperforms HW+WN CLASS and the accuracy reaches 81.2%. For the variants of HW+WN HYP, we first search the optimal depth. This is performed by using all synsets for NP head word, encoding the feature using synset id (rather than synset word), and no hypernym depth is encoded in the features. We try various depths of 1, 3, 6, 9 and ∞, with ∞ signifies that no depth constraint is imposed. The optimal depth of 6 is obtained with the accuracy of 83.1% over the development dataset. We then fix the depth of 6 to tr</context>
<context position="22452" citStr="Ng 2007" startWordPosition="3765" endWordPosition="3766">eature incrementally to train an ME model over the combination of training and development datasets. Table 4 shows the SC classification accuracy of all NPs (all) and non-pronoun NPs (non-PN) on the training+development (we refer it as training hereafter) and test datasets. Table 4: SC classification accuracy of ME using incremental feature sets for training and test ACE2 datasets. Feature type train test all non-PN all non-PN HW 87.8 89.0 68.6 67.6 +WN HYP 87.8 89.0 75.7 75.8 +unigram 91.5 93.3 77.7 78.1 +bigram 93.1 95.2 78.7 79.2 +first-last word 93.2 95.3 78.8 79.3 +NE 93.4 95.6 83.1 84.4 Ng 2007a - 85.0 - 83.3 Note that the significant higher accuracies in training compared to test are due to the overfitting problem. The interesting evaluation thus remains on the test data. As can be seen, the inclusion of more features results in higher performance. This is more obvious in the test dataset than in the training dataset. The inclusion of the 6In fact, the accuracy of the test data supports their claims. The accuracy using the first synset compared to using all synsets results in the accuracy increase from 75.6% to 76.4% for all NPs over the test dataset. 1236 optimized WN HYP feature </context>
<context position="32157" citStr="Ng, 2007" startWordPosition="5440" endWordPosition="5441">location, date, time, money, percent, and object. Each of these defined semantic classes is then mapped to a WordNet synset. Then the semantic class determination module determines the semantic class for every NP as the first synset of the head noun of the NP. If such synset is a hyponym of defined semantic class, then such semantic class is assigned to the NP. Otherwise, unknown class is assigned. Finally, the agreement of semantic classes of NPi and NPj is unknown if either assigned class is unknown; true if their assigned class are the same, false otherwise. Notice that the WordNet use in (Ng, 2007a) and this feature apply in the same principle except that 1) the former is used in SC classification while the latter is used directly for coreference resolution, and 2) they have different semantic class categories. 4.3 Proposed WordNet agreement features For each instance which consists of NPi and NPj, we apply our SC classifier to label them, say li and lj respectively. We then use these two induced la1238 bels to propose the SC agreement feature for NPi and NPj. In particular, SC STRICT is true if li and lj are the same and they are not of other type, false otherwise; SC COARSE is true i</context>
<context position="34885" citStr="Ng, 2007" startWordPosition="5933" endWordPosition="5934">ents and anaphors. In accordance with our hypothesis, the relaxation of strict SC agreement by including SC COARSE, SC RELAX1 and SC RELAX2 help improve the performance further, which is reflected by both MUC score and anaphor accuracy. For example, compared to the baseline, the use of all proposed four SC agreement features results in the maximal accuracy gain of 9.7% (from 37.7% to 47.4%) and the use of SC STRICT, SC COARSE, and SC RELAX1 results in the maximal MUC score gain of 10.4% (from 57.7% to 68.1%). Our best MUC score is 68.1% which outperforms the MUC score of 64.6% as reported in (Ng, 2007a) by 3.5%, while our best accuracy of anaphor is 47.4%, which is 4.1% less than the accuracy of 51.5% in (Ng, 2007a). Note that, unlike (Ng, 2007a) which performed extensive experiments using different machine learning algorithms, alternative use of features (either constraint or normal features), and heterogeneous knowledge sources, this paper simply uses one learning classifier (ME model) and only employs WordNet and Stanford NER semantic sources. The different MUC and accuracy scores reflect the non-trivial cases of evaluating coreference systems. While we leave out the discussion of which</context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>V. Ng. 2007a. Semantic Class Induction and Coreference Resolution. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Shallow Semantics for Coreference Resolution.</title>
<date>2007</date>
<booktitle>In Proc. of the IJCAI.</booktitle>
<contexts>
<context position="2116" citStr="Ng, 2007" startWordPosition="312" endWordPosition="313">referent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference between semantically incompatible NPs. However, the difficulty is to accurately compute the semantic class features. In this paper, we show that the WordNet may not be efficiently employed in the tr</context>
<context position="5189" citStr="Ng, 2007" startWordPosition="816" endWordPosition="817">ordNet hypernyms for their SC classification and then testing their SC compatibility. In doing so, we can directly model the accuracy of semantic class classification and test its impact on coreference resolution. While the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many coreference resolution systems simply assign to a common noun the first (i.e., most frequent) WordNet synset as its SC (Soon et al., 2001; Markert and Nissim, 2005). This heuristics, apparently, did not lead to good performance. The best reported ACE2 coreference resolution system (Ng, 2007a; Ng, 2007b) has proposed an accurate SC classifier which used heterogeneous semantic knowledge sources. WordNet is just one of the several knowledge sources which have been utilized. However, the WordNet based features is not informative compared to other features such as the semantic neighbor feature. Similarly, Ponzetto and Strube (2006) have discovered that the WordNet feature is no more informative than the community-generated Wikipedia feature. In this paper, we focus on the investigation of various usages of WordNet for the SC classification task. The work which is directly comparable </context>
<context position="11473" citStr="Ng, 2007" startWordPosition="1896" endWordPosition="1897">ties: person, location and organization can be recognized for a given NP. This feature is primarily useful for SC classification of proper nouns. WordNet is a large English lexicon in which semantically related words are connected via cognitive synonyms (synsets). The WordNet is a useful tool for word semantics analysis and has been widely used in natural language processing applications. In WordNet, synsets are organized into hierarchies with hypernym/hyponym relationships: Y is a hypernym of X if every X is a (kind of) Y (X is called a hyponym of Y in this case). The WordNet is employed in (Ng, 2007a) as following to create the WN CLASS feature. For each keyword w as shown in the right column of 2It, however, is mostly useful for nominal noun phrase and not for the pronoun and proper noun phrases. Table 2, if the head noun of a given NP is a hyponym of w in WordNet,3 then the word w becomes a feature for such NP. It is explained that these keywords are correlated with the ACE SCs and they are obtained via experimentation with WordNet and the ACE SCs of the NPs in the ACE training data. However, it is likely that these handcrafted keywords have poor coverage for general cases. As a result</context>
<context position="17218" citStr="Ng, 2007" startWordPosition="2881" endWordPosition="2882">odels for both SC classification and mention pair classification. 3.5 SC classification evaluation We design three experiments to test the accuracy of our classifiers. The first experiment evaluates the individual contribution of different feature sets to SC classification accuracy. In particular, a ME model is trained on the 55,629 training instances using the following feature sets separately: 1) unigram, 2) bigram, 3) first-last word, 4) head word (HW), 5) named entities (NE), 6) HW+WN CLASS, 7) HW+WN SIM, and 8) variants of HW+WN HYP. Note that HW+WN CLASS is the semantic feature used in (Ng, 2007a), HW+WN SIM is the semantic feature using WordNet similarity measure (Seco et al., 2004), and variants of HW+WN HYP are the work proposed in this paper. We combine head word and the semantic features due to the fact that WordNet features are dependent on head words and they could be treated as units. In the second experiment, features are fed into the ME model incrementally until all features have been used.5 Finally, we perform the feature ablation experiments. That is, we remove one feature at a time from the entire feature set and test the accuracy loss. The SC classification performance </context>
<context position="20288" citStr="Ng, 2007" startWordPosition="3392" endWordPosition="3393">or non-pronoun NP SC classification, which are slightly different compared to all NP SC classification except for bigram, in which the accuracy has increased 3.9%. Although Stanford NER performs well on named entity recognition task, it results in accuracy of 74.0% for all NP SC classification, due to its inability to deal with pronouns such as he and common nouns such as the government. The removal of pronouns significantly boosts its accuracy to 82.8%. The introduction of semantic feature HW+WN CLASS boosts the performance to 79.5% compared to the head word alone of 78.2%. This conforms to (Ng, 2007a) that only small gain can be achieved using WN CLASS feature. The HW+WN SIM feature outperforms HW+WN CLASS and the accuracy reaches 81.2%. For the variants of HW+WN HYP, we first search the optimal depth. This is performed by using all synsets for NP head word, encoding the feature using synset id (rather than synset word), and no hypernym depth is encoded in the features. We try various depths of 1, 3, 6, 9 and ∞, with ∞ signifies that no depth constraint is imposed. The optimal depth of 6 is obtained with the accuracy of 83.1% over the development dataset. We then fix the depth of 6 to tr</context>
<context position="22452" citStr="Ng 2007" startWordPosition="3765" endWordPosition="3766">eature incrementally to train an ME model over the combination of training and development datasets. Table 4 shows the SC classification accuracy of all NPs (all) and non-pronoun NPs (non-PN) on the training+development (we refer it as training hereafter) and test datasets. Table 4: SC classification accuracy of ME using incremental feature sets for training and test ACE2 datasets. Feature type train test all non-PN all non-PN HW 87.8 89.0 68.6 67.6 +WN HYP 87.8 89.0 75.7 75.8 +unigram 91.5 93.3 77.7 78.1 +bigram 93.1 95.2 78.7 79.2 +first-last word 93.2 95.3 78.8 79.3 +NE 93.4 95.6 83.1 84.4 Ng 2007a - 85.0 - 83.3 Note that the significant higher accuracies in training compared to test are due to the overfitting problem. The interesting evaluation thus remains on the test data. As can be seen, the inclusion of more features results in higher performance. This is more obvious in the test dataset than in the training dataset. The inclusion of the 6In fact, the accuracy of the test data supports their claims. The accuracy using the first synset compared to using all synsets results in the accuracy increase from 75.6% to 76.4% for all NPs over the test dataset. 1236 optimized WN HYP feature </context>
<context position="32157" citStr="Ng, 2007" startWordPosition="5440" endWordPosition="5441">location, date, time, money, percent, and object. Each of these defined semantic classes is then mapped to a WordNet synset. Then the semantic class determination module determines the semantic class for every NP as the first synset of the head noun of the NP. If such synset is a hyponym of defined semantic class, then such semantic class is assigned to the NP. Otherwise, unknown class is assigned. Finally, the agreement of semantic classes of NPi and NPj is unknown if either assigned class is unknown; true if their assigned class are the same, false otherwise. Notice that the WordNet use in (Ng, 2007a) and this feature apply in the same principle except that 1) the former is used in SC classification while the latter is used directly for coreference resolution, and 2) they have different semantic class categories. 4.3 Proposed WordNet agreement features For each instance which consists of NPi and NPj, we apply our SC classifier to label them, say li and lj respectively. We then use these two induced la1238 bels to propose the SC agreement feature for NPi and NPj. In particular, SC STRICT is true if li and lj are the same and they are not of other type, false otherwise; SC COARSE is true i</context>
<context position="34885" citStr="Ng, 2007" startWordPosition="5933" endWordPosition="5934">ents and anaphors. In accordance with our hypothesis, the relaxation of strict SC agreement by including SC COARSE, SC RELAX1 and SC RELAX2 help improve the performance further, which is reflected by both MUC score and anaphor accuracy. For example, compared to the baseline, the use of all proposed four SC agreement features results in the maximal accuracy gain of 9.7% (from 37.7% to 47.4%) and the use of SC STRICT, SC COARSE, and SC RELAX1 results in the maximal MUC score gain of 10.4% (from 57.7% to 68.1%). Our best MUC score is 68.1% which outperforms the MUC score of 64.6% as reported in (Ng, 2007a) by 3.5%, while our best accuracy of anaphor is 47.4%, which is 4.1% less than the accuracy of 51.5% in (Ng, 2007a). Note that, unlike (Ng, 2007a) which performed extensive experiments using different machine learning algorithms, alternative use of features (either constraint or normal features), and heterogeneous knowledge sources, this paper simply uses one learning classifier (ME model) and only employs WordNet and Stanford NER semantic sources. The different MUC and accuracy scores reflect the non-trivial cases of evaluating coreference systems. While we leave out the discussion of which</context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>V. Ng. 2007b. Shallow Semantics for Coreference Resolution. In Proc. of the IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nicolae</author>
<author>G Nicolae</author>
</authors>
<title>BESTCUT: A Graph Algorithm for Coreference Resolution.</title>
<date>2006</date>
<booktitle>In Proc. of the EMNLP.</booktitle>
<contexts>
<context position="1792" citStr="Nicolae and Nicolae, 2006" startWordPosition="262" endWordPosition="265">naphor accuracy respectively. 1 Introduction Coreference resolution is used to determine which noun phrases (including pronouns, proper names, and common nouns) refer to the same entities in documents. Much work on coreference resolution is based on (Soon et al., 2001), which built a decision tree classifier to label pairs of mentions as coreferent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been e</context>
</contexts>
<marker>Nicolae, Nicolae, 2006</marker>
<rawString>C. Nicolae and G. Nicolae 2006. BESTCUT: A Graph Algorithm for Coreference Resolution. In Proc. of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>R Mehta</author>
<author>A Maroudas</author>
<author>J Hitzeman</author>
</authors>
<title>Learning to resolve bridging references.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="2042" citStr="Poesio et al., 2004" startWordPosition="299" endWordPosition="302">et al., 2001), which built a decision tree classifier to label pairs of mentions as coreferent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference between semantically incompatible NPs. However, the difficulty is to accurately compute the semantic class features. In this</context>
</contexts>
<marker>Poesio, Mehta, Maroudas, Hitzeman, 2004</marker>
<rawString>M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004. Learning to resolve bridging references. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proc. of the HLT/NAACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="2081" citStr="Ponzetto and Strube, 2006" startWordPosition="304" endWordPosition="307">sion tree classifier to label pairs of mentions as coreferent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference between semantically incompatible NPs. However, the difficulty is to accurately compute the semantic class features. In this paper, we show that the WordNet may no</context>
<context position="4025" citStr="Ponzetto and Strube (2006)" startWordPosition="623" endWordPosition="626">llbaum, 1998) as an important knowledge source has been widely employed in previous coreference resolution work. For example, Harabagiu et al. (2001) have used WordNet relations such as synonym and is-a to mine the patterns of WordNet paths for pairs of antecedents and anaphors. Due to the nature of the rule based coreference system (in contrast to machine learning based), the weights of relations may not be accurately estimated. Vieira and Poesio (2000) and Markert and Nissim (2005) have used WordNet synonym and hyponym etc. to determine if an anaphor semantically relates to one previous NP. Ponzetto and Strube (2006) have used WordNet semantic similarity and relatedness scores between antecedents and candidate anaphors. Their 1232 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1232–1240, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP work is different to this work in the following: 1) Their work involves various relations such as hyponyms and meronyms while ours only makes use of hypernyms; and 2) Their work focuses on investigating if two NPs have particular WordNet relations or not, while ours focuses on using WordNet hypernyms for their SC classification</context>
<context position="5532" citStr="Ponzetto and Strube (2006)" startWordPosition="865" endWordPosition="868">nce resolution systems simply assign to a common noun the first (i.e., most frequent) WordNet synset as its SC (Soon et al., 2001; Markert and Nissim, 2005). This heuristics, apparently, did not lead to good performance. The best reported ACE2 coreference resolution system (Ng, 2007a; Ng, 2007b) has proposed an accurate SC classifier which used heterogeneous semantic knowledge sources. WordNet is just one of the several knowledge sources which have been utilized. However, the WordNet based features is not informative compared to other features such as the semantic neighbor feature. Similarly, Ponzetto and Strube (2006) have discovered that the WordNet feature is no more informative than the community-generated Wikipedia feature. In this paper, we focus on the investigation of various usages of WordNet for the SC classification task. The work which is directly comparable to ours would be (Ng, 2007a; Ng, 2007b). Other similar work includes the mention detection (MD) task (Florian et al., 2006) and joint probabilistic model of coreference (Daum´e III and Marcu, 2005). The MD task identifies the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, organization). Un</context>
<context position="12684" citStr="Ponzetto and Strube (2006)" startWordPosition="2101" endWordPosition="2104">es. As a result, it may not make full use of WordNet semantic knowledge. This will be shown in our individual feature contribution experiment in Section 3.5. Table 2: List of keywords used in WordNet semantic feature in (Ng, 2007a). ACE SC Keywords PER person ORG social group FAC establishment, construction, building, facility, workplace GPE country, province, government, town, city, administration, society, island, community LOC dry land, region, landmass, body of water geographical area, geological formation There are other ways of using WordNet for semantic feature extraction. For example, Ponzetto and Strube (2006) have employed WordNet similarity measure for coreference resolution. The difference is that they created the feature directly at the coreference resolution stage, ie, using the WordNet similarity between the antecedent and anaphor to determine if they are coreferent, while we focus on using this feature to classify an NP into a particular SC. For comparison, we implemented a WordNet similarity based feature (WN SIM) as follows: for a given NP head word and a key word as listed in Table 2, the WordNet similarity package (Seco et al., 2004) models the length of path traveling from the head word</context>
<context position="28427" citStr="Ponzetto and Strube, 2006" startWordPosition="4801" endWordPosition="4804">oundary detection, 1237 POS-tagging and text chunking. The statistics of corpus and mention extraction are shown in Table 6, where g-mention is the automatically extracted mentions which contain the annotated (gold) mentions. The recalls of gold mentions are 95.88% and 95.93% for training and test data respectively. Table 6: Statistics for corpus and extracted mentions. text# mention# g-mention# gold# recall(%) train 422 61810 22990 23977 95.88 test 97 15360 5561 5797 95.93 Our coreference system uses Maximum Entropy model to determine whether two NPs are coreferent. As in (Soon et al., 2001; Ponzetto and Strube, 2006), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2, ..., NPj−1. Each instance is represented by syntactic or semantic features described as follows. All training data are used to train a maximum entropy model. In the test stage,we select the closest preceding NP that is classified as coreferent with NPj as the antecedent of NPj. If no such NP exists, no antecedent is selected for NPj. Unlike other natural language</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>S. P. Ponzetto and M. Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proc. of the HLT/NAACL, pages 192-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Seco</author>
<author>T Veale</author>
<author>J Hayes</author>
</authors>
<title>An Intrinsic Information Content Metric for Semantic Similarity in WordNet.</title>
<date>2004</date>
<booktitle>Proc. of the European Conference of Artificial Intelligence.</booktitle>
<contexts>
<context position="13229" citStr="Seco et al., 2004" startWordPosition="2193" endWordPosition="2196"> for semantic feature extraction. For example, Ponzetto and Strube (2006) have employed WordNet similarity measure for coreference resolution. The difference is that they created the feature directly at the coreference resolution stage, ie, using the WordNet similarity between the antecedent and anaphor to determine if they are coreferent, while we focus on using this feature to classify an NP into a particular SC. For comparison, we implemented a WordNet similarity based feature (WN SIM) as follows: for a given NP head word and a key word as listed in Table 2, the WordNet similarity package (Seco et al., 2004) models the length of path traveling from the head word to the key word over the WordNet network. It then computes the semantic similarity based on the path. For example, the similarity between company and social group is 0.77, while the similarity between company and person is 0.59. The key word which receives the highest similarity to the head word is marked as a feature. The WN CLASS feature may suffer from the coverage problem and the WN SIM feature is heavily dependent on the definition of similarity metric which may turn out to be inappropriate for coreference resolution task. To make be</context>
<context position="17308" citStr="Seco et al., 2004" startWordPosition="2894" endWordPosition="2897">ication evaluation We design three experiments to test the accuracy of our classifiers. The first experiment evaluates the individual contribution of different feature sets to SC classification accuracy. In particular, a ME model is trained on the 55,629 training instances using the following feature sets separately: 1) unigram, 2) bigram, 3) first-last word, 4) head word (HW), 5) named entities (NE), 6) HW+WN CLASS, 7) HW+WN SIM, and 8) variants of HW+WN HYP. Note that HW+WN CLASS is the semantic feature used in (Ng, 2007a), HW+WN SIM is the semantic feature using WordNet similarity measure (Seco et al., 2004), and variants of HW+WN HYP are the work proposed in this paper. We combine head word and the semantic features due to the fact that WordNet features are dependent on head words and they could be treated as units. In the second experiment, features are fed into the ME model incrementally until all features have been used.5 Finally, we perform the feature ablation experiments. That is, we remove one feature at a time from the entire feature set and test the accuracy loss. The SC classification performance is measured by accuracy, i.e., the proportion of the correctly classified instances among </context>
</contexts>
<marker>Seco, Veale, Hayes, 2004</marker>
<rawString>N. Seco, T. Veale, and J. Hayes. 2004. An Intrinsic Information Content Metric for Semantic Similarity in WordNet. Proc. of the European Conference of Artificial Intelligence.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<journal>Computation Linguistics,</journal>
<pages>27--4</pages>
<marker>Soon, Ng, Lim, </marker>
<rawString>W. M. Soon, H. T. Ng, and D. C. Y. Lim. A machine learning approach to coreference resolution of noun phrases. Computation Linguistics, 27(4):521-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Versley</author>
<author>S P Ponzetto</author>
<author>M Poesio</author>
<author>V Eidelman</author>
<author>A Jern</author>
<author>J Smith</author>
<author>X Yang</author>
<author>A Moschitti</author>
</authors>
<title>BART: a modular toolkit for coreference resolution. ACL</title>
<date>2008</date>
<note>System demo.</note>
<contexts>
<context position="29873" citStr="Versley et al., 2008" startWordPosition="5037" endWordPosition="5040">entions (in contrast to the gold mentions) are used. To facilitate the comparison with previous work, we report performance using two different scoring metrics: the commonlyused MUC scorer (Vilain et al., 1995) and the accuracy of the anaphoric references (Ponzetto and Strube, 2006). An anaphoric reference is correctly resolved if it and its closest antecedent are in the same coreference chain in the resulting partition. 4.2 Baseline features We briefly review the baseline features used in this paper as follows. More detailed information and implementations can be found at (Soon et al., 2001; Versley et al., 2008). For example, the ALIAS feature takes values of true or false. The value of true means that the antecedent and the anaphor refer to the same entity (date, person, organization or location). The ALIAS feature detection works differently depending on the named entity type. For date, the day, month, and year values are extracted and compared. For person, the last words of the noun phrases are compared. For organization names, the alias detection checks for acronym match such as IBM and International Business Machines Corp. Lexical features STRING MATCH: true if NPi and NPj have the same spelling</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, Yang, Moschitti, 2008</marker>
<rawString>Y. Versley, S. P. Ponzetto, M. Poesio, V. Eidelman, A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008. BART: a modular toolkit for coreference resolution. ACL 2008 System demo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Vieira</author>
<author>M Poesio</author>
</authors>
<title>An empirically-based system for processing definite descriptions.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--4</pages>
<contexts>
<context position="3857" citStr="Vieira and Poesio (2000)" startWordPosition="595" endWordPosition="598">ference resolution system can obtain an increase of 10.4% for MUC score and 9.7% for anaphor accuracy from the baseline in ACE2 evaluation. 2 Related Work WordNet (Fellbaum, 1998) as an important knowledge source has been widely employed in previous coreference resolution work. For example, Harabagiu et al. (2001) have used WordNet relations such as synonym and is-a to mine the patterns of WordNet paths for pairs of antecedents and anaphors. Due to the nature of the rule based coreference system (in contrast to machine learning based), the weights of relations may not be accurately estimated. Vieira and Poesio (2000) and Markert and Nissim (2005) have used WordNet synonym and hyponym etc. to determine if an anaphor semantically relates to one previous NP. Ponzetto and Strube (2006) have used WordNet semantic similarity and relatedness scores between antecedents and candidate anaphors. Their 1232 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1232–1240, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP work is different to this work in the following: 1) Their work involves various relations such as hyponyms and meronyms while ours only makes use of hypernyms; a</context>
</contexts>
<marker>Vieira, Poesio, 2000</marker>
<rawString>R. Vieira and M. Poesio. 2000. An empirically-based system for processing definite descriptions. Computational Linguistics, 26(4):539-593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoreing scheme.</title>
<date>1995</date>
<booktitle>In Proc. of MUC-6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="29462" citStr="Vilain et al., 1995" startWordPosition="4971" endWordPosition="4974">elect the closest preceding NP that is classified as coreferent with NPj as the antecedent of NPj. If no such NP exists, no antecedent is selected for NPj. Unlike other natural language processing tasks such as information extraction which have de facto evaluation metrics, it is an open question which evaluation is the most suitable one. The evaluation becomes more complicated when automatically extracted mentions (in contrast to the gold mentions) are used. To facilitate the comparison with previous work, we report performance using two different scoring metrics: the commonlyused MUC scorer (Vilain et al., 1995) and the accuracy of the anaphoric references (Ponzetto and Strube, 2006). An anaphoric reference is correctly resolved if it and its closest antecedent are in the same coreference chain in the resulting partition. 4.2 Baseline features We briefly review the baseline features used in this paper as follows. More detailed information and implementations can be found at (Soon et al., 2001; Versley et al., 2008). For example, the ALIAS feature takes values of true or false. The value of true means that the antecedent and the anaphor refer to the same entity (date, person, organization or location)</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoreing scheme. In Proc. of MUC-6, pages 45-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>A Brunstein</author>
</authors>
<title>BBN pronoun coreference and entity type corpus. Linguistic Data Consortium.</title>
<date>2005</date>
<contexts>
<context position="23903" citStr="Weischedel and Brunstein, 2005" startWordPosition="4017" endWordPosition="4020">ity of head word feature. The unigram, bigram and first-last word features offer reasonable accuracy gain, and the final inclusion of NE boosts the overall performance to 83.1% for all NP and 84.4% for non pronoun NPs over test data. This result can be directly compared to the SC classification accuracy as reported in (Ng, 2007a), in which the highest accuracy is 83.3% for non pronoun NPs.7 The large difference between the highest training accuracies is due to that our classifier is trained directly on the ACE2 training dataset, while their SC classifier was trained on BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which is five times larger than the ACE2 corpus used by us. In addition to WordNet, they have adopted multiple knowledge sources which include BBN’s IdentiFinder (this is equivalent to the Stanford NER in our work), BLLIP corpus and Reuters Corpus,8 and dependency based thesaurus (Lin, 1998a). It is remarkable that our SC classifier can achieve even higher accuracy only using WordNet hypernym and NE features. It is worth noting that the small accuracy gain is indeed hard to achieve considering that the test data size is large (15360). Feature ablation experiment We now perform the feature ab</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>R. Weischedel and A. Brunstein. 2005. BBN pronoun coreference and entity type corpus. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
</authors>
<title>Coreference resolution using semantic relatedness information from automatically discovered pattens.</title>
<date>2007</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="2166" citStr="Yang and Su, 2007" startWordPosition="318" endWordPosition="321">rove the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference between semantically incompatible NPs. However, the difficulty is to accurately compute the semantic class features. In this paper, we show that the WordNet may not be efficiently employed in the traditional way such as (Soon et al., 2001; Ng, 2007</context>
</contexts>
<marker>Yang, Su, 2007</marker>
<rawString>X. Yang and J. Su. 2007. Coreference resolution using semantic relatedness information from automatically discovered pattens. In Proc. of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>