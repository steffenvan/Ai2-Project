<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003425">
<title confidence="0.988239">
Inducing Compact but Accurate Tree-Substitution Grammars
</title>
<author confidence="0.998222">
Trevor Cohn and Sharon Goldwater and Phil Blunsom
</author>
<affiliation confidence="0.9984465">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.9914705">
10 Crichton Street, Edinburgh EH8 9AB
Scotland, United Kingdom
</address>
<email confidence="0.999385">
{tcohn,sgwater,pblunsom}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999669769230769">
Tree substitution grammars (TSGs) are a com-
pelling alternative to context-free grammars
for modelling syntax. However, many popu-
lar techniques for estimating weighted TSGs
(under the moniker of Data Oriented Parsing)
suffer from the problems of inconsistency and
over-fitting. We present a theoretically princi-
pled model which solves these problems us-
ing a Bayesian non-parametric formulation.
Our model learns compact and simple gram-
mars, uncovering latent linguistic structures
(e.g., verb subcategorisation), and in doing so
far out-performs a standard PCFG.
</bodyText>
<sectionHeader confidence="0.999488" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991334127272728">
Many successful models of syntax are based on
Probabilistic Context Free Grammars (PCFGs)
(e.g., Collins (1999)). However, directly learning a
PCFG from a treebank results in poor parsing perfor-
mance, due largely to the unrealistic independence
assumptions imposed by the context-free assump-
tion. Considerable effort is required to coax good
results from a PCFG, in the form of grammar en-
gineering, feature selection and clever smoothing
(Collins, 1999; Charniak, 2000; Charniak and John-
son, 2005; Johnson, 1998). This effort must be re-
peated when moving to different languages, gram-
mar formalisms or treebanks. We propose that much
of this hand-coded knowledge can be obtained auto-
matically as an emergent property of the treebanked
data, thereby reducing the need for human input in
crafting the grammar.
We present a model for automatically learning a
Probabilistic Tree Substitution Grammar (PTSG),
an extension to the PCFG in which non-terminals
can rewrite as entire tree fragments (elementary
trees), not just immediate children. These large frag-
ments can be used to encode non-local context, such
as head-lexicalisation and verb sub-categorisation.
Since no annotated data is available providing TSG
derivations we must induce the PTSG productions
and their probabilities in an unsupervised way from
an ordinary treebank. This is the same problem ad-
dressed by Data Oriented Parsing (DOP, Bod et al.
(2003)), a method which uses as productions all sub-
trees of the training corpus. However, many of the
DOP estimation methods have serious shortcomings
(Johnson, 2002), namely inconsistency for DOP1
(Bod, 2003) and overfitting of the maximum like-
lihood estimate (Prescher et al., 2004).
In this paper we develop an alternative means of
learning a PTSG from a treebanked corpus, with the
twin objectives of a) finding a grammar which ac-
curately models the data and b) keeping the gram-
mar as simple as possible, with few, compact, ele-
mentary trees. This is achieved using a prior to en-
courage sparsity and simplicity in a Bayesian non-
parametric formulation. The framework allows us to
perform inference over an infinite space of gram-
mar productions in an elegant and efficient manner.
The net result is a grammar which only uses the in-
creased context afforded by the TSG when necessary
to model the data, and otherwise uses context-free
rules.1 That is, our model learns to use larger rules
when the CFG’s independence assumptions do not
hold. This contrasts with DOP, which seeks to use
all elementary trees from the training set. While our
model is able, in theory, to use all such trees, in prac-
tice the data does not justify such a large grammar.
Grammars that are only about twice the size of a
</bodyText>
<footnote confidence="0.993714333333333">
1While TSGs and CFGs describe the same string lan-
guages, TSGs can describe context-sensitive tree-languages,
which CFGs cannot.
</footnote>
<page confidence="0.819913">
548
</page>
<note confidence="0.900281">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 548–556,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999587875">
treebank PCFG provide large gains in accuracy. We
obtain additional improvements with grammars that
are somewhat larger, but still much smaller than the
DOP all-subtrees grammar. The rules in these gram-
mars are intuitive, potentially offering insights into
grammatical structure which could be used in, e.g.,
the development of syntactic ontologies and guide-
lines for future treebanking projects.
</bodyText>
<sectionHeader confidence="0.904215" genericHeader="introduction">
2 Background and related work
</sectionHeader>
<bodyText confidence="0.999472628571429">
A Tree Substitution Grammar2 (TSG) is a 4-tuple,
G = (T, N, S, R), where T is a set of terminal sym-
bols, N is a set of non-terminal symbols, S ∈ N is
the distinguished root non-terminal and R is a set
of productions (a.k.a. rules). The productions take
the form of elementary trees – tree fragments of
depth ≥ 2, where each internal node is labelled with
a non-terminal and each leaf is labelled with either a
terminal or a non-terminal. Non-terminal leaves are
called frontier non-terminals and form the substitu-
tion sites in the generative process of creating trees
with the grammar.
A derivation creates a tree by starting with the
root symbol and rewriting (substituting) it with an
elementary tree, then continuing to rewrite frontier
non-terminals with elementary trees until there are
no remaining frontier non-terminals. Unlike Con-
text Free Grammars (CFGs) a syntax tree may not
uniquely specify the derivation, as illustrated in Fig-
ure 1 which shows two derivations using different
elementary trees to produce the same tree.
A Probabilistic Tree Substitution Grammar
(PTSG), like a PCFG, assigns a probability to each
rule in the grammar. The probability of a derivation
is the product of the probabilities of its component
rules, and the probability of a tree is the sum of the
probabilities of its derivations.
As we mentioned in the introduction, work within
the DOP framework seeks to induce PTSGs from
treebanks by using all possible subtrees as rules, and
one of a variety of methods for estimating rule prob-
abilities.3 Our aim of inducing compact grammars
contrasts with that of DOP; moreover, we develop a
probabilistic estimator which avoids the shortcom-
ings of DOP1 and the maximum likelihood esti-
</bodyText>
<footnote confidence="0.982303">
2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003))
without the adjunction operator.
3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also
tackles a similar learning problem.
</footnote>
<bodyText confidence="0.999809945945946">
mate (Bod, 2000; Bod, 2003; Johnson, 2002). Re-
cent work on DOP estimation also seeks to address
these problems, drawing from estimation theory to
solve the consistency problem (Prescher et al., 2004;
Zollmann and Sima’an, 2005), or incorporating a
grammar brevity term into the learning objective
(Zuidema, 2007). Our work differs from these pre-
vious approaches in that we explicitly model a prior
over grammars within a Bayesian framework.4
Models of grammar refinement (Petrov et al.,
2006; Liang et al., 2007; Finkel et al., 2007) also
aim to automatically learn latent structure underly-
ing treebanked data. These models allow each non-
terminal to be split into a number of subcategories.
Theoretically the grammar space of our model is a
sub-space of theirs (projecting the TSG’s elementary
trees into CFG rules). However, the number of non-
terminals required to recreate our TSG grammars
in a PCFG would be exorbitant. Consequently, our
model should be better able to learn specific lexical
patterns, such as full noun-phrases and verbs with
their sub-categorisation frames, while theirs are bet-
ter suited to learning subcategories with larger mem-
bership, such as the terminals for days of the week
and noun-adjective agreement. The approaches are
orthogonal, and we expect that combining a category
refinement model with our TSG model would pro-
vide better performance than either approach alone.
Our model is similar to the Adaptor Grammar
model of Johnson et al. (2007b), which is also
a kind of Bayesian nonparametric tree-substitution
grammar. However, Adaptor Grammars require that
each sub-tree expands completely, with only termi-
nal symbols as leaves, while our own model permits
non-terminal frontier nodes. In addition, they disal-
low recursive containment of adapted non-terminals;
we impose no such constraint.
</bodyText>
<sectionHeader confidence="0.992731" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999432833333333">
Recall the nature of our task: we are given a corpus
of parse trees t and wish to infer a tree-substitution
grammar G that we can use to parse new data.
Rather than inferring a grammar directly, we go
through an intermediate step of inferring a distri-
bution over the derivations used to produce t, i.e.,
</bodyText>
<footnote confidence="0.998879333333333">
4A similar Bayesian model of TSG induction has been de-
veloped independently to this work (O’Donnell et al., 2009b;
O’Donnell et al., 2009a).
</footnote>
<page confidence="0.998023">
549
</page>
<figure confidence="0.992508125">
(a) (b)
George hates NP V broccoli
broccoli hates
S → NP (VP (V hates) NP)
NP → George
NP → broccoli
S → (NP George) (VP V (NP broccoli))
V → hates
</figure>
<figureCaption confidence="0.998247">
Figure 1: Example derivations for the same tree,
</figureCaption>
<bodyText confidence="0.972271222222222">
where arrows indicate substitution sites. The ele-
mentary trees used in (a) and (b) are shown below
as grammar productions in bracketed tree notation.
a distribution over sequences of elementary trees e
that compose to form t. We will then essentially read
the grammar off the elementary trees, as described
in Section 5. Our problem therefore becomes one of
identifying the posterior distribution of e given t,
which we can do using Bayes’ Rule:
</bodyText>
<equation confidence="0.999596">
P(e|t) ∝ P(t|e)P(e) (1)
</equation>
<bodyText confidence="0.999820529411765">
Since the sequence of elementary trees can be split
into derivations, each of which completely specifies
a tree, P(t|e) is either equal to 1 (when t and e
are consistent) or 0 (otherwise). Therefore, the work
in our model is done by the prior distribution over
elementary trees. Note that this is analogous to the
Bayesian model of word segmentation presented by
Goldwater et al. (2006); indeed, the problem of in-
ferring e from t can be viewed as a segmentation
problem, where each full tree must be segmented
into one or more elementary trees. As in Goldwater
et al. (2006), we wish to favour solutions employing
a relatively small number of elementary units (here,
elementary trees). This can be done using a Dirichlet
process (DP) prior. Specifically, we define the distri-
bution of elementary tree e with root non-terminal
symbol c as
</bodyText>
<equation confidence="0.964887">
Gc|αc, P0 ∼ DP(αc,P0(·|c))
e|c ∼ Gc
</equation>
<bodyText confidence="0.979962692307692">
where P0(·|c) (the base distribution) is a distribution
over the infinite space of trees rooted with c, and αc
(the concentration parameter) controls the model’s
tendency towards either reusing elementary trees or
creating novel ones as each training instance is en-
countered (and consequently, the tendency to infer
larger or smaller sets of elementary trees from the
observed data). We discuss the base distribution in
more detail below.
Rather than representing the distribution Gc ex-
plicitly, we integrate over all possible values of Gc.
The resulting distribution over ei, conditioned on
e&lt;i = e1 ... ei−1 and the root category c is:
</bodyText>
<equation confidence="0.983525333333333">
n&lt;&apos; + αcP0 (ei  |c)
p(ei  |e&lt;i, c, αc, P0) = &lt;i(2)
n·,c + αc
</equation>
<bodyText confidence="0.993994666666667">
where n&lt;i
ei,c is the number number of times ei has
been used to rewrite c in e&lt;i, and n&lt;i
</bodyText>
<equation confidence="0.559898">
·,c = Ee n&lt;i
</equation>
<bodyText confidence="0.999397470588235">
e,c is
the total count of rewriting c.
As with other DP models, ours can be viewed as a
cache model, where ei can be generated in one of
two ways: by drawing from the base distribution,
where the probability of any particular tree is pro-
portional to αcP0(ei|c), or by drawing from a cache
of previous expansions of c, where the probability of
any particular expansion is proportional to the num-
ber of times that expansion has been used before.
This view makes it clear that the model embodies
a “rich-get-richer” dynamic in which a few expan-
sions will occur with high probability, but many will
occur only once or twice, as is typical of natural lan-
guage. Our model is similar in this way to the Adap-
tor Grammar model of Johnson et al. (2007a).
We still need to define P0, the base distribution
over tree fragments. We use two such distributions.
The first, P0M generates each elementary tree by
a series of random decisions: whether to expand a
non-terminal, how many children to produce and
their identities. The probability of expanding a non-
terminal node labelled c is parameterised via a bino-
mial distribution, Bin(Oc), while all other decisions
are chosen uniformly at random. The second base
distribution, P0C, has a similar generative process
but draws non-terminal expansions from a treebank-
trained PCFG instead of a uniform distribution.
Both choices of P0 have the effect of biasing the
model towards simple rules with a small number of
internal nodes. The geometric increase in cost dis-
courages the model from using larger rules; for this
to occur these rules must yield a large increase in the
data likelihood. As P0C incorporates PCFG probabil-
</bodyText>
<figure confidence="0.9921765">
NP
NP
S
V
NP
NP
George
S
V
NP
VP
VP
550
S
NP,1
George
</figure>
<figureCaption confidence="0.992270333333333">
Figure 2: Gibbs state e specifying the derivation in
Figure 1a. Each node is labelled with its substitution
indicator variable.
</figureCaption>
<bodyText confidence="0.9486505">
ities, it assigns higher relative probability to larger
rules, compared to the more draconian P0M .
</bodyText>
<sectionHeader confidence="0.995866" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.99973715">
To train our model we use Gibbs sampling (Geman
and Geman, 1984), a Markov chain Monte Carlo
method in which variables are repeatedly sampled
conditioned on the values of all other variables in
the model. After a period of burn-in, each sam-
pler state (set of variable assignments) is a sample
from the posterior distribution of the model. In our
case, we wish to sample from P(e|t, α, Q), where
(α, Q) = {αc, Qc} for all categories c. To do so,
we associate a binary variable with each non-root
internal node of each tree in the training set, indi-
cating whether that node is a substitution point or
not. Each substitution point forms the root of some
elementary tree, as well as a frontier non-terminal
of an ancestor node’s elementary tree. Collectively,
the training trees and substitution variables specify
the sequence of elementary trees e that is the current
state of the sampler. Figure 2 shows an example tree
with its substitution variables, corresponding to the
TSG derivation in Figure 1a.
Our Gibbs sampler works by sampling the value
of each substitution variable, one at a time, in ran-
dom order. If d is the node associated with the sub-
stitution variable s under consideration, then the two
possible values of s define two options for e: one
in which d is internal to some elementary tree eM,
and one in which d is the substitution site con-
necting two smaller trees, eA and eB. In the ex-
ample in Figure 2, when sampling the VP node,
eM = (S NP (VP (V hates) NP)), eA = (S NP VP),
and eB = (VP (V hates) NP). To sample a value for
s, we compute the probabilities of eM and (eA, eB),
conditioned on e−: all other elementary trees in the
training set that share at most a root or frontier non-
terminal with eM, eA, or eB. This is easy to do
because the DP is exchangeable, meaning that the
probability of a set of outcomes does not depend on
their ordering. Therefore, we can treat the elemen-
tary trees under consideration as the last ones to be
sampled, and apply Equation 2, giving us
</bodyText>
<equation confidence="0.99991425">
n− eM ,cM + αcMP0(eM|cM)
P(eM|cM)= (3)
n−·,cM + αcM
P(eA, eB|cA)= n− eA,cA + αcAP0(eA|cA) (4)
n−·,cA + αcA
eB,cB + S(eA, eB) + αcBP0(eB|cB)
−
n−·,cB + S(cA, cB) + αcB
</equation>
<bodyText confidence="0.999860088235294">
where cx is the root label of ex, x E {A, B, M},
the counts n− are with respect to e−, and S(·, ·) is
the Kronecker delta function, which returns 1 when
its arguments are identical and 0 otherwise. We have
omitted e−, t, α and Q from the conditioning con-
text. The S terms in the second factor of (4) account
the changes to n− that would occur after observing
eA, which forms part of the conditioning context for
eB. If the trees eA and eB are identical, then the
count n−eB,cB would increase by one, and if the trees
share the same root non-terminal, then n−·,cB would
increase by one.
In the previous discussion, we have assumed
that the model hyperparameters, (α, Q), are known.
However, selecting their values by hand is extremely
difficult and fitting their values on heldout data is of-
ten very time consuming. For this reason we treat
the hyper-parameters as variables in our model and
infer their values during training. We choose vague
priors for each hyper-parameter, encoding our lack
of information about their values. We treat the con-
centration parameters, α, as being generated by a
vague gamma prior, αc — Gamma(0.001,1000).
We sample a new value α0c using a log-normal dis-
tribution with mean αc and variance 0.3, which is
then accepted into the distribution p(αc|e, t, α−, Q)
using the Metropolis-Hastings algorithm. We use a
Beta prior for the binomial specification parameters,
Qc — Beta(1,1). As the Beta distribution is conju-
gate to the binomial, we can directly resample the
Q parameters from the posterior, p(Qc|e, t, α, Q−) .
Both the concentration and substitution parameters
are resampled after every full Gibbs sampling itera-
tion over the training trees.
</bodyText>
<figure confidence="0.955184">
VP,0
V,0 NP,1
hates broccoli
n
×
</figure>
<page confidence="0.984043">
551
</page>
<sectionHeader confidence="0.996025" genericHeader="method">
5 Parsing
</sectionHeader>
<bodyText confidence="0.99070180952381">
We now turn to the problem of using the model
to parse novel sentences. This requires finding the
maximiser of
Jp(t|w, t) = p(t|w, e, α, Q) p(e, α, Q|t) de dα dQ
(5)
where w is the sequence of words being parsed and t
the resulting tree, t are the training trees and e their
segmentation into elementary trees.
Unfortunately solving for the maximising parse
tree in (5) is intractable. However, it can approxi-
mated using Monte Carlo techniques. Given a sam-
ple of (e, α, Q)5 we can reason over the space of
possible trees using a Metropolis-Hastings sampler
(Johnson et al., 2007a) coupled with a Monte Carlo
integral (Bod, 2003). The first step is to sample from
the posterior over derivations, p(d|w, e, α, Q). This
is achieved by drawing samples from an approxima-
tion grammar, ˜p(d|w), which are then accepted to
the true distribution using the Metropolis-Hastings
algorithm. The second step records for each sampled
derivation the CFG tree. The counts of trees consti-
tute an approximation to p(t|w, e, α, Q), from which
we can recover the maximum probability tree.
A natural proposal distribution, ˜p(d|w), is the
maximum a posterior (MAP) grammar given the el-
ementary tree analysis of our training set (analogous
to the PCFG approximation used in Johnson et al.
(2007a)). This is not practical because the approx-
imation grammar is infinite: elementary trees with
zero count in e still have some residual probabil-
ity under Po. In the absence of a better alternative,
we discard (most of) the zero-count rules from MAP
grammar. This results in a tractable grammar repre-
senting the majority of the probability mass, from
which we can sample derivations. We specifically
retain all zero-count PCFG productions observed in
the training set in order to provide greater robustness
on unseen data.
In addition to finding the maximum probability
parse (MPP), we also report results using the maxi-
mum probability derivation (MPD). While this could
be calculated in the manner as described above, we
</bodyText>
<footnote confidence="0.8184154">
5Using many samples of (e, α, β) in a Monte Carlo inte-
gral is a straight-forward extension to our parsing algorithm. We
did not observe a significant improvement in parsing accuracy
when using a multiple samples compared to a single sample,
and therefore just present results for a single sample.
</footnote>
<figure confidence="0.990328">
S → A  |B
A → A A  |B B  |(A a) (A a)  |(B a) (B a)
B → A A  |B B  |(A b) (A b)  |(B b) (B b)
</figure>
<figureCaption confidence="0.9447065">
Figure 3: TSG used to generate synthetic data. All
production probabilities are uniform.
</figureCaption>
<bodyText confidence="0.9998325">
found that using the CYK algorithm (Cocke, 1969)
to find the Viterbi derivation for p˜ yielded consis-
tently better results. This algorithm maximises an
approximated model, as opposed to approximately
optimising the true model. We also present results
using the tree with the maximum expected count of
CFG rules (MER). This uses counts of the CFG rules
applied at each span (compiled from the derivation
samples) followed by a maximisation step to find the
best tree. This is similar to the MAX-RULE-SUM
algorithm of Petrov and Klein (2007) and maximum
expected recall parsing (Goodman, 2003).
</bodyText>
<sectionHeader confidence="0.999727" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.977116615384615">
Synthetic data Before applying the model to
natural language, we first create a synthetic problem
to confirm that the model is capable of recovering
a known tree-substitution grammar. We created 50
random trees from the TSG shown in Figure 3. This
produces binary trees with A and B internal nodes
and ‘a’ and ‘b’ as terminals, such that the termi-
nals correspond to their grand-parent non-terminal
(A and a or B and b). These trees cannot be mod-
elled accurately with a CFG because expanding A
and B nodes into terminal strings requires knowing
their parent’s non-terminal.
We train the model for 100 iterations of Gibbs
sampling using annealing to speed convergence.
Annealing amounts to smoothing the distributions
in (3) and (4) by raising them to the power of T.
Our annealing schedule begins at T = 3 and lin-
early decreases to reach T = 1 in the final iteration.
The sampler converges to the correct grammar, with
the 10 rules from Figure 3.
Penn-treebank parsing We ran our natural lan-
guage experiments on the Penn treebank, using the
standard data splits (sections 2–21 for training, 22
for development and 23 for testing). As our model is
parameter free (the α and Q parameters are learnt in
training), we do not use the development set for pa-
</bodyText>
<page confidence="0.994574">
552
</page>
<bodyText confidence="0.999701791666667">
rameter tuning. We expect that fitting these param-
eters to maximise performance on the development
set would lead to a small increase in generalisation
performance, but at a significant cost in runtime. We
replace tokens with count ≤ 1 in the training sample
with one of roughly 50 generic unknown word mark-
ers which convey the token’s lexical features and po-
sition in the sentence, following Petrov et al. (2006).
We also right-binarise the trees to reduce the branch-
ing factor in the same manner as Petrov et al. (2006).
The predicted trees are evaluated using EVALB6 and
we report the F1 score over labelled constituents and
exact match accuracy over all sentences in the test-
ing sets.
In our experiments, we initialised the sampler by
setting all substitution variables to 0, thus treating
every full tree in the training set as an elementary
tree. Starting with all the variables set to 1 (corre-
sponding to CFG expansions) or a random mix of
0s and 1s considerably increases time until conver-
gence. We hypothesise that this is due to the sampler
getting stuck in modes, from which a series of lo-
cally bad decisions are required to escape. The CFG
solution seems to be a mode and therefore starting
the sampler with maximal trees helps the model to
avoid this mode.
Small data sample For our first treebank exper-
iments, we train on a small data sample by using
only section 2 of the treebank. Bayesian methods
tend to do well with small data samples, while for
larger samples the benefits diminish relative to point
estimates. The models were trained using Gibbs
sampling for 4000 iterations with annealing linearly
decreasing from T = 5 to T = 1, after which
the model performed another 1000 iterations with
T = 1. The final training sample was used in the
parsing algorithm, which used 1000 derivation sam-
ples for each test sentence. All results are the aver-
age of five independent runs.
Table 1 presents the prediction results on the de-
velopment set. The baseline is a maximum likeli-
hood PCFG. The TSG model significantly outper-
forms the baseline with either base distribution P0M
or P0C. This confirms our hypothesis that CFGs are
not sufficiently powerful to model syntax, but that
the increased context afforded to the TSG can make
a large difference. This result is even more impres-
sive when considering the difference in the sizes of
</bodyText>
<footnote confidence="0.781979">
6Seehttp://nlp.cs.nyu.edu/evalb/.
</footnote>
<table confidence="0.999836">
F1 EX # rules
PCFG 60.20 4.29 3500
TSG P0M : MPD 72.17 11.92 6609
MPP 71.27 12.33 6609
MER 74.25 12.30 6609
TSG P0C : MPD 75.24 15.18 14923
MPP 75.30 15.74 14923
MER 76.89 15.76 14923
SMT=2: MPD 71.93 11.30 16168
MER 74.32 11.77 16168
SMT=S: MPD 75.33 15.64 39758
MER 77.93 16.94 39758
</table>
<tableCaption confidence="0.99464">
Table 1: Development results for models trained on
</tableCaption>
<bodyText confidence="0.986870121212121">
section 2 of the Penn tree-bank, showing labelled
constituent F1 and exact match accuracy. Grammar
sizes are the number of rules with count ≥ 1.
grammar in the PCFG versus TSG models. The TSG
using P0M achieves its improvements with only dou-
ble as many rules, as a consequence of the prior
which encourages sparse solutions. The TSG results
with the CFG base distribution, P0C , are more ac-
curate but with larger grammars.7 This base distri-
bution assigns proportionally higher probability to
larger rules than P0M, and consequently the model
uses these additional rules in a larger grammar.
Surprisingly, the MPP technique is not systemati-
cally better than the MPD approach, with mixed re-
sults under the F1 metric. We conjecture that this is
due to sampling variance for long sentences, where
repeated samples of the same tree are exceedingly
rare. The MER technique results in considerably
better F1 scores than either MPD or MPP, with a
margin of 1.5 to 3 points. This method is less af-
fected by sampling variance due to its use of smaller
tree fragments (PCFG productions at each span).
For comparison, we trained the Berkeley split-
merge (SM) parser (Petrov et al., 2006) on the same
data and decoded using the Viterbi algorithm (MPD)
and expected rule count (MER a.k.a. MAX-RULE-
SUM). We ran two iterations of split-merge training,
after which the development F1 dropped substan-
tially (in contrast, our model is not fit to the devel-
opment data). The result is an accuracy slightly be-
low that of our model (SMτ=2). To be fairer to their
model, we adjusted the unknown word threshold to
their default setting, i.e., to apply to word types oc-
</bodyText>
<footnote confidence="0.9991945">
7The grammar is nevertheless far smaller than the full DOP
grammar on this data set, which has 700K rules.
</footnote>
<page confidence="0.998339">
553
</page>
<figure confidence="0.9954005">
0 1 2 3 4 5 6 7 8
count
</figure>
<figureCaption confidence="0.6840104">
Figure 4: Grammar statistics for a TSG P0M model
trained on section 2 of the Penn treebank, show-
ing a histogram over elementary tree depth, num-
ber of nodes, terminals (lexemes) and frontier non-
terminals (vars).
</figureCaption>
<bodyText confidence="0.999857806451613">
curring fewer than five times (SMτ=5). We expect
that tuning the treatment of unknown words in our
model would also yield further gains. The grammar
sizes are not strictly comparable, as the Berkeley bi-
narised grammars prohibit non-binary rules, and are
therefore forced to decompose each of these rules
into many child rules. But the trend is clear – our
model produces similar results to a state-of-the-art
parser, and can do so using a small grammar. With
additional rounds of split-merge training, the Berke-
ley grammar grows exponentially larger (200K rules
after six iterations).
Full treebank We now train the model using
P0M on the full training partition of the Penn tree-
bank, using sections 2–21. We run the Gibbs sampler
for 15,000 iterations while annealing from T = 5 to
T = 1, after which we finish with 5,000 iterations
at T = 1. We repeat this three times, giving an av-
erage F1 of 84.0% on the testing partition using the
maximum expected rule algorithm and 83.0% using
the Viterbi algorithm. This far surpasses the ML-
PCFG (F1 of 70.7%), and is similar to Zuidema’s
(2007) DOP result of 83.8%. However, it still well
below state-of-the art parsers (e.g., the Berkeley
parser trained using the same data representation
scores 87.7%). But we must bear in mind that these
parsers have benefited from years of tuning to the
Penn-treebank, where our model is much simpler
and is largely untuned. We anticipate that careful
data preparation and model tuning could greatly im-
prove our model’s performance.
</bodyText>
<table confidence="0.999708733333333">
NP →
(NNP Mr.) NNP
CD (NN %)
(NP CD (NN %)) (PP (IN of) NP)
(NP ($ $) CD) (NP (DT a) (NN share))
(NP (DT the) (¯NP (NN company) POS)) ¯NP
(NP QP (NN %)) (PP (IN of) NP)
(NP CD (NNS cents)) (NP (DT a) (NN share))
(NP (NNP Mr.) (¯NP NNP (POS ’s))) NN
QP (NN %)
(NP (NN president)) (PP (IN of) NP)
(NP (NNP Mr.) (¯NP NNP (POS ’s))) ¯NP
NNP ( ¯NP NNP (NNP Corp.))
NNP ( ¯NP NNP (NNP Inc.))
(NP (NN chairman)) (PP (IN of) NP)
VP →
(VBD said) (SBAR (S (NP (PRP it)) VP))
(VBD said) (SBAR (S NP VP))
(VBD rose) (¯VP (NP CD (NN %)) ¯VP)
(VBP want) S
(VBD said) (SBAR (S (NP (PRP he)) VP))
(VBZ plans) S
(VBD said) (SBAR S)
(VBZ says) (SBAR (S NP VP))
(VBP think) (SBAR S)
(VBD agreed) (S (VP (TO to) (VP VB ¯VP)))
(VBZ includes) NP
(VBZ says) (SBAR (S (NP (PRP he)) VP))
(VBZ wants) S
(VBD closed) (¯VP (PP (IN at) NP) ( ¯VP , ADVP))
</table>
<tableCaption confidence="0.9014605">
Table 3: Most frequent lexicalised expansions for
noun and verb phrases, excluding auxiliary verbs.
</tableCaption>
<sectionHeader confidence="0.999446" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999825235294118">
So what kinds of non-CFG rules is the model learn-
ing? Figure 4 shows the grammar statistics for a
TSG model trained on the small data sample. This
model has 5611 CFG rules and 1008 TSG rules.
The TSG rules vary in depth from two to nine levels
with the majority between two and four. Most rules
combine a small degree of lexicalisation and a vari-
able or two. This confirms that the model is learn-
ing local structures to encode, e.g., multi-word units,
subcategorisation frames and lexical agreement. The
few very large rules specify full parses for sentences
which were repeated in the training corpus. These
complete trees are also evident in the long tail of
node counts (up to 27; not shown in the figure) and
counts for highly lexicalised rules (up to 8).
To get a better feel for the types of rules being
learnt, it is instructive to examine the rules in the re-
</bodyText>
<figure confidence="0.986575333333333">
0 100 200 300 400 500
count of counts
depth
nodes
lexemes
vars
</figure>
<page confidence="0.989876">
554
</page>
<table confidence="0.999851125">
NP → PP → ADJP →
DT ¯NP IN NP JJ
NNS (IN in) NP RB JJ
DT NN (TO to) NP JJ (¯ADJP CC JJ)
(DT the) ¯NP TO NP JJ PP
JJ NNS (IN with) NP (RB very) JJ
NP (PP (IN of) NP) (IN of) NP RB ¯ADJP
NP PP (IN by) NP (RBR more) JJ
NP ( ¯NP (CC and) NP) (IN at) NP JJ ¯ADJP
JJ ¯NP IN (NP (DT the) ¯NP) ADJP ( ¯ADJP CC ADJP)
NN NNS (IN on) NP RB VBN
(DT the) NNS (IN from) NP RB ( ¯ADJP JJ PP)
DT (¯NP JJ NN) IN (S (VP VBG NP)) JJ (PP (TO to) NP)
NN IN (NP NP PP) ADJP (PP (IN than) NP)
JJ NN (IN into) NP (RB too) JJ
(NP DT NN) (PP (IN of) NP) (IN for) NP (RB much) JJR
</table>
<tableCaption confidence="0.993693">
Table 2: Top fifteen expansions sorted by frequency (most frequent at top), taken from the final sample of a
</tableCaption>
<bodyText confidence="0.99909094117647">
model trained on the full Penn treebank. Non-terminals shown with an over-bar denote a binarised sub span
of the given phrase type.
sultant grammar. Table 2 shows the top fifteen rules
for three phrasal categories for the model trained on
the full Penn treebank. We can see that many of these
rules are larger than CFG rules, showing that the
CFG rules alone are inadequate to model the tree-
bank. Two of the NP rules encode the prevalence
of preposition phrases headed by ‘of’ within a noun
phrase, as opposed to other prepositions. Also note-
worthy is the lexicalisation of the determiner, which
can affect the type of NP expansion. For instance,
the indefinite article is more likely to have an ad-
jectival modifier, while the definite article appears
more frequently unmodified. Highly specific tokens
are also incorporated into lexicalised rules.
Many of the verb phrase expansions have been
lexicalised, encoding the verb’s subcategorisation,
as shown in Table 3. Notice that each verb here ac-
cepts only one or a small set of argument frames,
indicating that by lexicalising the verb in the VP ex-
pansion the model can find a less ambiguous and
more parsimonious grammar.
The model also learns to use large rules to de-
scribe the majority of root node expansions (we add
a distinguished TOP node to all trees). These rules
mostly describe cases when the S category is used
for a full sentence, which most often include punc-
tuation such as the full stop and quotation marks. In
contrast, the majority of expansions for the S cat-
egory do not include any punctuation. The model
has learnt to differentiate between the two different
classes of S – full sentence versus internal clause –
due to their different expansions.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999690625">
In this work we have presented a non-parametric
Bayesian model for inducing tree substitution gram-
mars. By incorporating a structured prior over ele-
mentary rules our model is able to reason over the
infinite space of all such rules, producing compact
and simple grammars. In doing so our model learns
local structures for latent linguistic phenomena, such
as verb subcategorisation and lexical agreement. Our
experimental results show that the induced gram-
mars strongly out-perform standard PCFGs, and are
comparable to a state-of-the-art parser on small data
samples. While our results on the full treebank are
well shy of the best available parsers, we have pro-
posed a number of improvements to the model and
the parsing algorithm that could lead to state-of-the-
art performance in the future.
</bodyText>
<sectionHeader confidence="0.998972" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987099">
Rens Bod, Remko Scha, and Khalil Sima’an, editors.
2003. Data-oriented parsing. Center for the Study of
Language and Information - Studies in Computational
Linguistics. University of Chicago Press.
Rens Bod. 2000. Combining semantic and syntactic
structure for language modeling. In Proceedings of
the 6th International Conference on Spoken Language
Processing, Beijing, China.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the 10th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, Budapest, Hungary, April.
</reference>
<page confidence="0.983445">
555
</page>
<reference confidence="0.999857990384615">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173–180, Ann Arbor, Michigan, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 132–139.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of the
19th International Conference on Computational Lin-
guistics, pages 183–189, Taipei, Taiwan.
John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Courant Institute of
Mathematical Sciences, New York University.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 272–279, Prague, Czech
Republic, June.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721–741.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of COLING/ACL,
Sydney.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al. (Bod et al., 2003),
chapter 8.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007a. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of Hu-
man Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 139–146, Rochester,
New York, April.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007b. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In Advances in Neural Information Processing Sys-
tems 19.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4),
December.
Mark Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Lingusitics,
28(1):71–76, March.
Aravind Joshi. 2003. Tree adjoining grammars. In Rus-
lan Mikkov, editor, The Oxford Handbook of Computa-
tional Linguistics, pages 483–501. Oxford University
Press, Oxford, England.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688–697, Prague, Czech
Republic, June.
Timothy J. O’Donnell, Noah D. Goodman, Jesse
Snedeker, and Joshua B. Tenenbaum. 2009a. Com-
putation and reuse in language. In 31st Annual Con-
ference of the Cognitive Science Society, Amsterdam,
The Netherlands, July. To appear.
Timothy J. O’Donnell, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009b. Fragment grammar: Exploring
reuse in hierarchical generative processes. Technical
Report MIT-CSAIL-TR-2009-013, MIT.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of Hu-
man Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 404–411, Rochester,
New York, April. Association for Computational Lin-
guistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433–440, Sydney, Aus-
tralia, July.
Detlef Prescher, Remko Scha, Khalil Sima’an, and An-
dreas Zollmann. 2004. On the statistical consistency
of dop estimators. In Proceedings of the 14th Meet-
ing of Computational Linguistics in the Netherlands,
Antwerp, Belgium.
Fei Xia. 2002. Automatic grammar generation from
two different perspectives. Ph.D. thesis, University of
Pennsylvania.
Andreas Zollmann and Khalil Sima’an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics,
10(2):367–388.
Willem Zuidema. 2007. Parsimonious data-oriented
parsing. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 551–560, Prague, Czech Republic, June.
</reference>
<page confidence="0.998542">
556
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.438216">
<title confidence="0.999926">Inducing Compact but Accurate Tree-Substitution Grammars</title>
<author confidence="0.998713">Cohn Goldwater</author>
<affiliation confidence="0.995649">School of University of</affiliation>
<address confidence="0.882693">10 Crichton Street, Edinburgh EH8</address>
<note confidence="0.572795">Scotland, United</note>
<abstract confidence="0.996553571428571">Tree substitution grammars (TSGs) are a compelling alternative to context-free grammars for modelling syntax. However, many popular techniques for estimating weighted TSGs (under the moniker of Data Oriented Parsing) suffer from the problems of inconsistency and over-fitting. We present a theoretically principled model which solves these problems using a Bayesian non-parametric formulation. Our model learns compact and simple grammars, uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2003</date>
<booktitle>Data-oriented parsing. Center for the Study of Language and Information - Studies in Computational Linguistics.</booktitle>
<editor>Rens Bod, Remko Scha, and Khalil Sima’an, editors.</editor>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="2253" citStr="(2003)" startWordPosition="329" endWordPosition="329">We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary trees), not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003)), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002), namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004). In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and sim</context>
<context position="6064" citStr="(2003)" startWordPosition="957" endWordPosition="957"> of a derivation is the product of the probabilities of its component rules, and the probability of a tree is the sum of the probabilities of its derivations. As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (P</context>
</contexts>
<marker>2003</marker>
<rawString>Rens Bod, Remko Scha, and Khalil Sima’an, editors. 2003. Data-oriented parsing. Center for the Study of Language and Information - Studies in Computational Linguistics. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Combining semantic and syntactic structure for language modeling.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Conference on Spoken Language Processing,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="6206" citStr="Bod, 2000" startWordPosition="977" endWordPosition="978">of its derivations. As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. Thes</context>
</contexts>
<marker>Bod, 2000</marker>
<rawString>Rens Bod. 2000. Combining semantic and syntactic structure for language modeling. In Proceedings of the 6th International Conference on Spoken Language Processing, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An efficient implementation of a new DOP model.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="2456" citStr="Bod, 2003" startWordPosition="359" endWordPosition="360">, not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003)), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002), namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004). In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation. The framework allows us to perform inference over an infinite space of grammar productions in an elegant and efficient manner. The net result is a grammar</context>
<context position="6217" citStr="Bod, 2003" startWordPosition="979" endWordPosition="980">vations. As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models al</context>
<context position="17282" citStr="Bod, 2003" startWordPosition="2902" endWordPosition="2903"> problem of using the model to parse novel sentences. This requires finding the maximiser of Jp(t|w, t) = p(t|w, e, α, Q) p(e, α, Q|t) de dα dQ (5) where w is the sequence of words being parsed and t the resulting tree, t are the training trees and e their segmentation into elementary trees. Unfortunately solving for the maximising parse tree in (5) is intractable. However, it can approximated using Monte Carlo techniques. Given a sample of (e, α, Q)5 we can reason over the space of possible trees using a Metropolis-Hastings sampler (Johnson et al., 2007a) coupled with a Monte Carlo integral (Bod, 2003). The first step is to sample from the posterior over derivations, p(d|w, e, α, Q). This is achieved by drawing samples from an approximation grammar, ˜p(d|w), which are then accepted to the true distribution using the Metropolis-Hastings algorithm. The second step records for each sampled derivation the CFG tree. The counts of trees constitute an approximation to p(t|w, e, α, Q), from which we can recover the maximum probability tree. A natural proposal distribution, ˜p(d|w), is the maximum a posterior (MAP) grammar given the elementary tree analysis of our training set (analogous to the PCFG</context>
</contexts>
<marker>Bod, 2003</marker>
<rawString>Rens Bod. 2003. An efficient implementation of a new DOP model. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, Budapest, Hungary, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1336" citStr="Charniak and Johnson, 2005" startWordPosition="183" endWordPosition="187">ing latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary trees), not just immediate children. These large fragments can be used to encode non-local conte</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173–180, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1308" citStr="Charniak, 2000" startWordPosition="181" endWordPosition="182">rammars, uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary trees), not just immediate children. These large fragments can be us</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Daniel M Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>183--189</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="6137" citStr="Chiang and Bikel, 2002" startWordPosition="964" endWordPosition="967">ts component rules, and the probability of a tree is the sum of the probabilities of its derivations. As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to </context>
</contexts>
<marker>Chiang, Bikel, 2002</marker>
<rawString>David Chiang and Daniel M. Bikel. 2002. Recovering latent information in treebanks. In Proceedings of the 19th International Conference on Computational Linguistics, pages 183–189, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cocke</author>
</authors>
<title>Programming languages and their compilers: Preliminary notes.</title>
<date>1969</date>
<booktitle>Courant Institute of Mathematical Sciences,</booktitle>
<location>New York University.</location>
<contexts>
<context position="19168" citStr="Cocke, 1969" startWordPosition="3226" endWordPosition="3227">lity derivation (MPD). While this could be calculated in the manner as described above, we 5Using many samples of (e, α, β) in a Monte Carlo integral is a straight-forward extension to our parsing algorithm. We did not observe a significant improvement in parsing accuracy when using a multiple samples compared to a single sample, and therefore just present results for a single sample. S → A |B A → A A |B B |(A a) (A a) |(B a) (B a) B → A A |B B |(A b) (A b) |(B b) (B b) Figure 3: TSG used to generate synthetic data. All production probabilities are uniform. found that using the CYK algorithm (Cocke, 1969) to find the Viterbi derivation for p˜ yielded consistently better results. This algorithm maximises an approximated model, as opposed to approximately optimising the true model. We also present results using the tree with the maximum expected count of CFG rules (MER). This uses counts of the CFG rules applied at each span (compiled from the derivation samples) followed by a maximisation step to find the best tree. This is similar to the MAX-RULE-SUM algorithm of Petrov and Klein (2007) and maximum expected recall parsing (Goodman, 2003). 6 Experiments Synthetic data Before applying the model </context>
</contexts>
<marker>Cocke, 1969</marker>
<rawString>John Cocke. 1969. Programming languages and their compilers: Preliminary notes. Courant Institute of Mathematical Sciences, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="951" citStr="Collins (1999)" startWordPosition="127" endWordPosition="128">free grammars for modelling syntax. However, many popular techniques for estimating weighted TSGs (under the moniker of Data Oriented Parsing) suffer from the problems of inconsistency and over-fitting. We present a theoretically principled model which solves these problems using a Bayesian non-parametric formulation. Our model learns compact and simple grammars, uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent prop</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>272--279</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6724" citStr="Finkel et al., 2007" startWordPosition="1057" endWordPosition="1060">induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to learning subcategorie</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2007. The infinite tree. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272–279, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="12828" citStr="Geman and Geman, 1984" startWordPosition="2099" endWordPosition="2102">g the model towards simple rules with a small number of internal nodes. The geometric increase in cost discourages the model from using larger rules; for this to occur these rules must yield a large increase in the data likelihood. As P0C incorporates PCFG probabilNP NP S V NP NP George S V NP VP VP 550 S NP,1 George Figure 2: Gibbs state e specifying the derivation in Figure 1a. Each node is labelled with its substitution indicator variable. ities, it assigns higher relative probability to larger rules, compared to the more draconian P0M . 4 Training To train our model we use Gibbs sampling (Geman and Geman, 1984), a Markov chain Monte Carlo method in which variables are repeatedly sampled conditioned on the values of all other variables in the model. After a period of burn-in, each sampler state (set of variable assignments) is a sample from the posterior distribution of the model. In our case, we wish to sample from P(e|t, α, Q), where (α, Q) = {αc, Qc} for all categories c. To do so, we associate a binary variable with each non-root internal node of each tree in the training set, indicating whether that node is a substitution point or not. Each substitution point forms the root of some elementary tr</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="9525" citStr="Goldwater et al. (2006)" startWordPosition="1523" endWordPosition="1526"> then essentially read the grammar off the elementary trees, as described in Section 5. Our problem therefore becomes one of identifying the posterior distribution of e given t, which we can do using Bayes’ Rule: P(e|t) ∝ P(t|e)P(e) (1) Since the sequence of elementary trees can be split into derivations, each of which completely specifies a tree, P(t|e) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the work in our model is done by the prior distribution over elementary trees. Note that this is analogous to the Bayesian model of word segmentation presented by Goldwater et al. (2006); indeed, the problem of inferring e from t can be viewed as a segmentation problem, where each full tree must be segmented into one or more elementary trees. As in Goldwater et al. (2006), we wish to favour solutions employing a relatively small number of elementary units (here, elementary trees). This can be done using a Dirichlet process (DP) prior. Specifically, we define the distribution of elementary tree e with root non-terminal symbol c as Gc|αc, P0 ∼ DP(αc,P0(·|c)) e|c ∼ Gc where P0(·|c) (the base distribution) is a distribution over the infinite space of trees rooted with c, and αc (</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of COLING/ACL, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions.</title>
<date>2003</date>
<booktitle>In Bod et</booktitle>
<note>chapter 8.</note>
<contexts>
<context position="19711" citStr="Goodman, 2003" startWordPosition="3313" endWordPosition="3314">abilities are uniform. found that using the CYK algorithm (Cocke, 1969) to find the Viterbi derivation for p˜ yielded consistently better results. This algorithm maximises an approximated model, as opposed to approximately optimising the true model. We also present results using the tree with the maximum expected count of CFG rules (MER). This uses counts of the CFG rules applied at each span (compiled from the derivation samples) followed by a maximisation step to find the best tree. This is similar to the MAX-RULE-SUM algorithm of Petrov and Klein (2007) and maximum expected recall parsing (Goodman, 2003). 6 Experiments Synthetic data Before applying the model to natural language, we first create a synthetic problem to confirm that the model is capable of recovering a known tree-substitution grammar. We created 50 random trees from the TSG shown in Figure 3. This produces binary trees with A and B internal nodes and ‘a’ and ‘b’ as terminals, such that the terminals correspond to their grand-parent non-terminal (A and a or B and b). These trees cannot be modelled accurately with a CFG because expanding A and B nodes into terminal strings requires knowing their parent’s non-terminal. We train th</context>
</contexts>
<marker>Goodman, 2003</marker>
<rawString>Joshua Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. In Bod et al. (Bod et al., 2003), chapter 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="7664" citStr="Johnson et al. (2007" startWordPosition="1207" endWordPosition="1210">required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to learning subcategories with larger membership, such as the terminals for days of the week and noun-adjective agreement. The approaches are orthogonal, and we expect that combining a category refinement model with our TSG model would provide better performance than either approach alone. Our model is similar to the Adaptor Grammar model of Johnson et al. (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar. However, Adaptor Grammars require that each sub-tree expands completely, with only terminal symbols as leaves, while our own model permits non-terminal frontier nodes. In addition, they disallow recursive containment of adapted non-terminals; we impose no such constraint. 3 Model Recall the nature of our task: we are given a corpus of parse trees t and wish to infer a tree-substitution grammar G that we can use to parse new data. Rather than inferring a grammar directly, we go through an intermediate step of inferrin</context>
<context position="11555" citStr="Johnson et al. (2007" startWordPosition="1885" endWordPosition="1888">rated in one of two ways: by drawing from the base distribution, where the probability of any particular tree is proportional to αcP0(ei|c), or by drawing from a cache of previous expansions of c, where the probability of any particular expansion is proportional to the number of times that expansion has been used before. This view makes it clear that the model embodies a “rich-get-richer” dynamic in which a few expansions will occur with high probability, but many will occur only once or twice, as is typical of natural language. Our model is similar in this way to the Adaptor Grammar model of Johnson et al. (2007a). We still need to define P0, the base distribution over tree fragments. We use two such distributions. The first, P0M generates each elementary tree by a series of random decisions: whether to expand a non-terminal, how many children to produce and their identities. The probability of expanding a nonterminal node labelled c is parameterised via a binomial distribution, Bin(Oc), while all other decisions are chosen uniformly at random. The second base distribution, P0C, has a similar generative process but draws non-terminal expansions from a treebanktrained PCFG instead of a uniform distrib</context>
<context position="17232" citStr="Johnson et al., 2007" startWordPosition="2892" endWordPosition="2895">V,0 NP,1 hates broccoli n × 551 5 Parsing We now turn to the problem of using the model to parse novel sentences. This requires finding the maximiser of Jp(t|w, t) = p(t|w, e, α, Q) p(e, α, Q|t) de dα dQ (5) where w is the sequence of words being parsed and t the resulting tree, t are the training trees and e their segmentation into elementary trees. Unfortunately solving for the maximising parse tree in (5) is intractable. However, it can approximated using Monte Carlo techniques. Given a sample of (e, α, Q)5 we can reason over the space of possible trees using a Metropolis-Hastings sampler (Johnson et al., 2007a) coupled with a Monte Carlo integral (Bod, 2003). The first step is to sample from the posterior over derivations, p(d|w, e, α, Q). This is achieved by drawing samples from an approximation grammar, ˜p(d|w), which are then accepted to the true distribution using the Metropolis-Hastings algorithm. The second step records for each sampled derivation the CFG tree. The counts of trees constitute an approximation to p(t|w, e, α, Q), from which we can recover the maximum probability tree. A natural proposal distribution, ˜p(d|w), is the maximum a posterior (MAP) grammar given the elementary tree a</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007a. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 139–146, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19.</booktitle>
<contexts>
<context position="7664" citStr="Johnson et al. (2007" startWordPosition="1207" endWordPosition="1210">required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to learning subcategories with larger membership, such as the terminals for days of the week and noun-adjective agreement. The approaches are orthogonal, and we expect that combining a category refinement model with our TSG model would provide better performance than either approach alone. Our model is similar to the Adaptor Grammar model of Johnson et al. (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar. However, Adaptor Grammars require that each sub-tree expands completely, with only terminal symbols as leaves, while our own model permits non-terminal frontier nodes. In addition, they disallow recursive containment of adapted non-terminals; we impose no such constraint. 3 Model Recall the nature of our task: we are given a corpus of parse trees t and wish to infer a tree-substitution grammar G that we can use to parse new data. Rather than inferring a grammar directly, we go through an intermediate step of inferrin</context>
<context position="11555" citStr="Johnson et al. (2007" startWordPosition="1885" endWordPosition="1888">rated in one of two ways: by drawing from the base distribution, where the probability of any particular tree is proportional to αcP0(ei|c), or by drawing from a cache of previous expansions of c, where the probability of any particular expansion is proportional to the number of times that expansion has been used before. This view makes it clear that the model embodies a “rich-get-richer” dynamic in which a few expansions will occur with high probability, but many will occur only once or twice, as is typical of natural language. Our model is similar in this way to the Adaptor Grammar model of Johnson et al. (2007a). We still need to define P0, the base distribution over tree fragments. We use two such distributions. The first, P0M generates each elementary tree by a series of random decisions: whether to expand a non-terminal, how many children to produce and their identities. The probability of expanding a nonterminal node labelled c is parameterised via a binomial distribution, Bin(Oc), while all other decisions are chosen uniformly at random. The second base distribution, P0C, has a similar generative process but draws non-terminal expansions from a treebanktrained PCFG instead of a uniform distrib</context>
<context position="17232" citStr="Johnson et al., 2007" startWordPosition="2892" endWordPosition="2895">V,0 NP,1 hates broccoli n × 551 5 Parsing We now turn to the problem of using the model to parse novel sentences. This requires finding the maximiser of Jp(t|w, t) = p(t|w, e, α, Q) p(e, α, Q|t) de dα dQ (5) where w is the sequence of words being parsed and t the resulting tree, t are the training trees and e their segmentation into elementary trees. Unfortunately solving for the maximising parse tree in (5) is intractable. However, it can approximated using Monte Carlo techniques. Given a sample of (e, α, Q)5 we can reason over the space of possible trees using a Metropolis-Hastings sampler (Johnson et al., 2007a) coupled with a Monte Carlo integral (Bod, 2003). The first step is to sample from the posterior over derivations, p(d|w, e, α, Q). This is achieved by drawing samples from an approximation grammar, ˜p(d|w), which are then accepted to the true distribution using the Metropolis-Hastings algorithm. The second step records for each sampled derivation the CFG tree. The counts of trees constitute an approximation to p(t|w, e, α, Q), from which we can recover the maximum probability tree. A natural proposal distribution, ˜p(d|w), is the maximum a posterior (MAP) grammar given the elementary tree a</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007b. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. In Advances in Neural Information Processing Systems 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="1352" citStr="Johnson, 1998" startWordPosition="188" endWordPosition="189">ures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary trees), not just immediate children. These large fragments can be used to encode non-local context, such as head</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4), December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>The DOP estimation method is biased and inconsistent.</title>
<date>2002</date>
<journal>Computational Lingusitics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="2413" citStr="Johnson, 2002" startWordPosition="353" endWordPosition="354">ite as entire tree fragments (elementary trees), not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003)), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002), namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004). In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation. The framework allows us to perform inference over an infinite space of grammar productions in an elegant and ef</context>
<context position="6233" citStr="Johnson, 2002" startWordPosition="981" endWordPosition="982"> we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterm</context>
</contexts>
<marker>Johnson, 2002</marker>
<rawString>Mark Johnson. 2002. The DOP estimation method is biased and inconsistent. Computational Lingusitics, 28(1):71–76, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>Tree adjoining grammars.</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics,</booktitle>
<pages>483--501</pages>
<editor>In Ruslan Mikkov, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford, England.</location>
<contexts>
<context position="6064" citStr="Joshi (2003)" startWordPosition="956" endWordPosition="957">bility of a derivation is the product of the probabilities of its component rules, and the probability of a tree is the sum of the probabilities of its derivations. As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (P</context>
</contexts>
<marker>Joshi, 2003</marker>
<rawString>Aravind Joshi. 2003. Tree adjoining grammars. In Ruslan Mikkov, editor, The Oxford Handbook of Computational Linguistics, pages 483–501. Oxford University Press, Oxford, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>688--697</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6702" citStr="Liang et al., 2007" startWordPosition="1053" endWordPosition="1056">tion operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 688–697, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J O’Donnell</author>
<author>Noah D Goodman</author>
<author>Jesse Snedeker</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Computation and reuse in language.</title>
<date>2009</date>
<booktitle>In 31st Annual Conference of the Cognitive Science Society,</booktitle>
<location>Amsterdam, The Netherlands,</location>
<note>To appear.</note>
<marker>O’Donnell, Goodman, Snedeker, Tenenbaum, 2009</marker>
<rawString>Timothy J. O’Donnell, Noah D. Goodman, Jesse Snedeker, and Joshua B. Tenenbaum. 2009a. Computation and reuse in language. In 31st Annual Conference of the Cognitive Science Society, Amsterdam, The Netherlands, July. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J O’Donnell</author>
<author>Noah D Goodman</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Fragment grammar: Exploring reuse in hierarchical generative processes.</title>
<date>2009</date>
<tech>Technical Report MIT-CSAIL-TR-2009-013, MIT.</tech>
<marker>O’Donnell, Goodman, Tenenbaum, 2009</marker>
<rawString>Timothy J. O’Donnell, Noah D. Goodman, and Joshua B. Tenenbaum. 2009b. Fragment grammar: Exploring reuse in hierarchical generative processes. Technical Report MIT-CSAIL-TR-2009-013, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="19659" citStr="Petrov and Klein (2007)" startWordPosition="3304" endWordPosition="3307">e 3: TSG used to generate synthetic data. All production probabilities are uniform. found that using the CYK algorithm (Cocke, 1969) to find the Viterbi derivation for p˜ yielded consistently better results. This algorithm maximises an approximated model, as opposed to approximately optimising the true model. We also present results using the tree with the maximum expected count of CFG rules (MER). This uses counts of the CFG rules applied at each span (compiled from the derivation samples) followed by a maximisation step to find the best tree. This is similar to the MAX-RULE-SUM algorithm of Petrov and Klein (2007) and maximum expected recall parsing (Goodman, 2003). 6 Experiments Synthetic data Before applying the model to natural language, we first create a synthetic problem to confirm that the model is capable of recovering a known tree-substitution grammar. We created 50 random trees from the TSG shown in Figure 3. This produces binary trees with A and B internal nodes and ‘a’ and ‘b’ as terminals, such that the terminals correspond to their grand-parent non-terminal (A and a or B and b). These trees cannot be modelled accurately with a CFG because expanding A and B nodes into terminal strings requi</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6682" citStr="Petrov et al., 2006" startWordPosition="1049" endWordPosition="1052">)) without the adjunction operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs </context>
<context position="21394" citStr="Petrov et al. (2006)" startWordPosition="3602" endWordPosition="3605">d data splits (sections 2–21 for training, 22 for development and 23 for testing). As our model is parameter free (the α and Q parameters are learnt in training), we do not use the development set for pa552 rameter tuning. We expect that fitting these parameters to maximise performance on the development set would lead to a small increase in generalisation performance, but at a significant cost in runtime. We replace tokens with count ≤ 1 in the training sample with one of roughly 50 generic unknown word markers which convey the token’s lexical features and position in the sentence, following Petrov et al. (2006). We also right-binarise the trees to reduce the branching factor in the same manner as Petrov et al. (2006). The predicted trees are evaluated using EVALB6 and we report the F1 score over labelled constituents and exact match accuracy over all sentences in the testing sets. In our experiments, we initialised the sampler by setting all substitution variables to 0, thus treating every full tree in the training set as an elementary tree. Starting with all the variables set to 1 (corresponding to CFG expansions) or a random mix of 0s and 1s considerably increases time until convergence. We hypoth</context>
<context position="24872" citStr="Petrov et al., 2006" startWordPosition="4203" endWordPosition="4206">dditional rules in a larger grammar. Surprisingly, the MPP technique is not systematically better than the MPD approach, with mixed results under the F1 metric. We conjecture that this is due to sampling variance for long sentences, where repeated samples of the same tree are exceedingly rare. The MER technique results in considerably better F1 scores than either MPD or MPP, with a margin of 1.5 to 3 points. This method is less affected by sampling variance due to its use of smaller tree fragments (PCFG productions at each span). For comparison, we trained the Berkeley splitmerge (SM) parser (Petrov et al., 2006) on the same data and decoded using the Viterbi algorithm (MPD) and expected rule count (MER a.k.a. MAX-RULESUM). We ran two iterations of split-merge training, after which the development F1 dropped substantially (in contrast, our model is not fit to the development data). The result is an accuracy slightly below that of our model (SMτ=2). To be fairer to their model, we adjusted the unknown word threshold to their default setting, i.e., to apply to word types oc7The grammar is nevertheless far smaller than the full DOP grammar on this data set, which has 700K rules. 553 0 1 2 3 4 5 6 7 8 cou</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
<author>Remko Scha</author>
<author>Khalil Sima’an</author>
<author>Andreas Zollmann</author>
</authors>
<title>On the statistical consistency of dop estimators.</title>
<date>2004</date>
<booktitle>In Proceedings of the 14th Meeting of Computational Linguistics in the Netherlands,</booktitle>
<location>Antwerp, Belgium.</location>
<marker>Prescher, Scha, Sima’an, Zollmann, 2004</marker>
<rawString>Detlef Prescher, Remko Scha, Khalil Sima’an, and Andreas Zollmann. 2004. On the statistical consistency of dop estimators. In Proceedings of the 14th Meeting of Computational Linguistics in the Netherlands, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Automatic grammar generation from two different perspectives.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6149" citStr="Xia, 2002" startWordPosition="968" endWordPosition="969">the probability of a tree is the sum of the probabilities of its derivations. As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automaticall</context>
</contexts>
<marker>Xia, 2002</marker>
<rawString>Fei Xia. 2002. Automatic grammar generation from two different perspectives. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Khalil Sima’an</author>
</authors>
<title>A consistent and efficient estimator for data-oriented parsing.</title>
<date>2005</date>
<journal>Journal of Automata, Languages and Combinatorics,</journal>
<volume>10</volume>
<issue>2</issue>
<marker>Zollmann, Sima’an, 2005</marker>
<rawString>Andreas Zollmann and Khalil Sima’an. 2005. A consistent and efficient estimator for data-oriented parsing. Journal of Automata, Languages and Combinatorics, 10(2):367–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem Zuidema</author>
</authors>
<title>Parsimonious data-oriented parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>551--560</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6503" citStr="Zuidema, 2007" startWordPosition="1022" endWordPosition="1023"> DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be ex</context>
</contexts>
<marker>Zuidema, 2007</marker>
<rawString>Willem Zuidema. 2007. Parsimonious data-oriented parsing. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 551–560, Prague, Czech Republic, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>