<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995851">
Relatedness Curves for Acquiring Paraphrases
</title>
<author confidence="0.993247">
Georgiana Dinu Grzegorz Chrupała
</author>
<affiliation confidence="0.850372">
Saarland University Saarland University
Saarbruecken, Germany Saarbruecken, Germany
</affiliation>
<email confidence="0.979746">
dinu@coli.uni-sb.de gchrupala@lsv.uni-saarland.de
</email>
<sectionHeader confidence="0.997163" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987424">
In this paper we investigate methods
for computing similarity of two phrases
based on their relatedness scores across
all ranks k in a SVD approximation of
a phrase/term co-occurrence matrix. We
confirm the major observations made in
previous work and our preliminary experi-
ments indicate that these methods can lead
to reliable similarity scores which in turn
can be used for the task of paraphrasing.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999695729166667">
Distributional methods for word similarity use
large amounts of text to acquire similarity judg-
ments based solely on co-occurrence statistics.
Typically each word is assigned a representation
as a point in a high dimensional space, where the
dimensions represent contextual features; follow-
ing this, vector similarity measures are used to
judge the meaning relatedness of words. One way
to make these computations more reliable is to use
Singular Value Decomposition (SVD) in order to
obtain a lower rank approximation of an original
co-occurrence matrix.
SVD is a matrix factorization method which
has applications in a large number of fields such
as signal processing or statistics. In natural lan-
guage processing methods such as Latent Seman-
tic Analysis (LSA) (Deerwester et al., 1990)
use SVD to obtain a factorization of a (typically)
word/document co-occurrence matrix. The under-
lying idea in these models is that the dimension-
ality reduction will produce meaningful dimen-
sions which represent concepts rather than just
terms, rendering similarity measures on these vec-
tors more accurate. Over the years, it has been
shown that these methods can closely match hu-
man similarity judgments and that they can be
used in various applications such as information
retrieval, document classification, essay grading
etc. However it has been noted that the success
of these methods is drastically determined by the
choice of dimension k to which the original space
is reduced.
(Bast and Majumdar, 2005) investigates exactly
this aspect and proves that no fixed choice of di-
mension is appropriate. The authors show that two
terms can be reliably compared only by investigat-
ing the curve of their relatedness scores over all
dimensions k. The authors use a term/document
matrix and analyze relatedness curves for inducing
a hard related/not-related decision and show that
their algorithms significantly improve over previ-
ous methods for information retrieval.
In this paper we investigate: 1) how the findings
of (Bast and Majumdar, 2005) carry over to ac-
quiring paraphrases using SVD on a phrase/term
co-occurrence matrix and 2) if reliable similarity
scores can be obtained from the analysis of relat-
edness curves.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.979954">
2.1 Singular Value Decomposition
</subsectionHeader>
<bodyText confidence="0.999925333333333">
Models such as LSA use Singular Value Decom-
position, in order to obtain term representations
over a space of concepts.
Given a co-occurrence matrix X of size (t, d),
we can compute the singular value decomposition:
UEVT of rank r. Matrices U and V T of sizes
(t, r) and (r, d) are the left and right singular vec-
tors; E is the (r, r) diagonal matrix of singular
values (ordered in descending order)1. Similarity
between terms i and j is computed as the scalar
product between the two vectors associated to the
words in the U matrix:
</bodyText>
<equation confidence="0.465442">
sim(ui, uj) = Ekl�1uilujl
</equation>
<footnote confidence="0.962084">
1Any approximation of rank k &lt; r can simply be ob-
tained from an approximation or rank r by deleting rows and
columns.
</footnote>
<page confidence="0.979938">
27
</page>
<note confidence="0.7671875">
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 27–32,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.996941">
2.2 Relatedness curves
</subsectionHeader>
<bodyText confidence="0.9772420625">
Finding the optimal dimensionality k has proven
to be an extremely important and not trivial step.
(Bast and Majumdar, 2005) show that no single cut
dimension is appropriate to compute the similarity
of two terms but this should be deduced from the
curve of similarity scores over all dimensions k.
The curve of relatedness for two terms ui and uj is
given by their scalar product across all dimensions
k, k smaller than a rank r:
k , E l�1uilujl, for k = 1, ..., r
They show that a smooth curve indicates closely
related terms, while a curve exhibiting many direc-
tion changes indicates unrelated terms; the actual
values of the similarity scores are often mislead-
ing, which explains why a good cut dimension k
is so difficult to find.
</bodyText>
<subsectionHeader confidence="0.989333">
2.3 Vector space representation of phrases
</subsectionHeader>
<bodyText confidence="0.999693652173913">
We choose to apply this to acquiring paraphrases
(or inference rules, i.e. entailments which hold in
just one direction) in the sense of DIRT (Lin and
Pantel, 2001).
In the DIRT algorithm a phrase is a noun-
ending path in a dependency graph and the goal
is to acquire inference rules such as (X solve Y,
X find solution to Y). We will call dependency
paths patterns. The input data consists of large
amounts of parsed text, from which patterns to-
gether with X-filler and Y-filler frequency counts
are extracted.
In this setting, a pattern receives two vector rep-
resentation, one in a X-filler space and one in the
Y-filler space. In order to compute the similarity
between two patterns, these are compared in the
X space and in the Y space, and the two result-
ing scores are multiplied. (The DIRT algorithm
uses Lin measure for computing similarity, which
is given in Section 4). Obtaining these vectors
from the frequency counts is straightforward and
it is exemplified in Table 1 which shows a frag-
ment of a Y-filler DIRT-like vector space.
</bodyText>
<tableCaption confidence="0.9794525">
Table 1: DIRT-like vector representation in the Y-filler
space. The values represent mutual information.
</tableCaption>
<sectionHeader confidence="0.992571" genericHeader="method">
3 Relatedness curves for acquiring
paraphrases
</sectionHeader>
<subsectionHeader confidence="0.999651">
3.1 Setup
</subsectionHeader>
<bodyText confidence="0.988437454545454">
We parsed the XIE fragment of GigaWord (ap-
prox. 100 mil. tokens) with Stanford dependency
parser. From this we built a pattern/word matrix of
size (85000, 3000) containing co-occurrence data
of the most frequent patterns with the most fre-
quent words2. We perform SVD factorization on
this matrix of rank k = 800. For each pair of pat-
terns, we can associate two relatedness curves: a
X curve and Y curve given by the scalar products
of their vectors in the U matrix, across dimensions
k : 1, ..., 800.
</bodyText>
<subsectionHeader confidence="0.978833">
3.2 Evaluating smoothness of the relatedness
curves
</subsectionHeader>
<bodyText confidence="0.999517">
In Figure 1 we plotted the X and Y curves of com-
paring the pattern X����
</bodyText>
<equation confidence="0.8537965">
���� win ����
���� Y with itself.
</equation>
<figureCaption confidence="0.944084">
Figure 1: X-filler and Y-filler relatedness curves
for the identity pair (X subj
</figureCaption>
<equation confidence="0.87213">
���� win dobj
���* Y, X subj
�
dobj
win * Y )
</equation>
<figureCaption confidence="0.940873">
Figure 2: X-filler and Y-filler relatedness curves
</figureCaption>
<bodyText confidence="0.50824">
subj prp pobj pobj 42
</bodyText>
<equation confidence="0.994346666666667">
lead
for (X � leader � o f � Y, X � by �—
lead� Y )
</equation>
<bodyText confidence="0.999646666666667">
Normally, the X and Y curves for the identical
pair are monotonically increasing. However what
can be noticed is that the actual values of these
functions differ by one order of magnitude in the
X and in the Y curves of identical patterns, show-
ing that in themselves they are not a good indica-
</bodyText>
<footnote confidence="0.96260975">
2Even if conceptually we have two semantic spaces (given
by X-fillers and Y-fillers), in reality we can work with a sin-
gle matrix, containing for each pattern also its reverse, both
represented solely in a X-filler space
</footnote>
<figure confidence="0.523022">
.. case problem ..
(X solve Y, Y)
(X settle Y, Y)
.. 6.1 4.4 ..
.. 5.2 5.9 ..
</figure>
<page confidence="0.944646">
28
</page>
<figureCaption confidence="0.998555">
Figure 3: X-filler and Y-filler relatedness curves
</figureCaption>
<figure confidence="0.614235916666667">
for (X subj
←−−− win dobj
−−−→ Y, X subj
←−−− murder dobj −−−→ Y )
tor of similarity. In Figure 2 we investigate a pair
of closely related patterns: (Xsubj
←−−− leader prp
−−→
of pobj
−−−→ Y, X pobj
←−−− by prp
←−− lead subj
</figure>
<bodyText confidence="0.993813058823529">
−−−→ Y ). It can be
noticed that while still not comparable to those of
the identical pair, these curves are much smoother
than the ones associated to the pair of unrelated
patterns in Figure 33.
However, unlike in the information retrieval
scenario in (Bast and Majumdar, 2005), for which
a hard related/not-related assignment works best,
for acquiring paraphrases we need to quantify the
smoothness of the curves. We describe two func-
tions for evaluating curve smoothness which we
will use to compute scores in X-filler and Y-filler
semantic spaces.
Smooth function 1 This function simply com-
putes the number of changes in the direction of the
curve, as the percentage of times the scalar prod-
uct increases or remains equal from step l to step
</bodyText>
<equation confidence="0.773170666666667">
l + 1:
Σuilujl��1
CurveS1(ui, uj) = k , l = 1, ..., k
</equation>
<bodyText confidence="0.992549">
An increasing curve will be assigned the maximal
value 1, while for a curve that is monotonically
decreasing the score will be 0.
Smooth function 2 (Bast and Majumdar, 2005)
The second smooth function is given by:
CurveS2(ui,uj) = k ax (min
Σl=1abs uilujl)
where max and min are the largest and smallest
values in the curves. A curve which is always in-
creasing or always decreasing will get a score of 1.
Unlike the previous method this function is sensi-
tive to the absolute values in the drops of a curve.
3The drop out dimension discussed in (Bast and Majum-
dar, 2005) Section 3, does not seem to exist for our data. This
is to be expected since this result stems from a definition of
perfectly related terms which is adapted to the particularities
of term/document matrices, and not of term/term matrices.
A curve with large drops, irrelevant of their cardi-
nality, will be penalized by being assigned a low
score.
</bodyText>
<sectionHeader confidence="0.99437" genericHeader="evaluation">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.9978996">
In order to compute the similarity score between
two phrases, we follow (Lin and Pantel, 2001)
and compute two similarity scores, corresponding
to the X-fillers and Y-fillers, and multiply them.
Given a similarity function, any pattern encoun-
tered in the corpus can be paraphrased by return-
ing its most similar patterns.
We implement five similarity functions on the
data we have described in the previous section.
The first one is the DIRT algorithm and it is the
only method using the original co-occurrence ma-
trix in which raw counts are replaced by point-
wise mutual information scores.
DIRT method The similarity function for two
vectors pi and pj is:
</bodyText>
<equation confidence="0.921246666666667">
simLin (pi , pj) = 1:lEI(pi)nI(pj)(pil + pjl)
[)
EIEI(pi) pil + 1:lEI(pj) pjl
</equation>
<bodyText confidence="0.999956">
where values in pi and pj are point-wise mu-
tual information, and I(�) gives the indices of non-
negative values in a vector.
Methods on SVD factorization All these meth-
ods perform computations the (85000, 800) U ma-
trix in the SVD factorization. On this we imple-
ment two methods which do an arbitrary dimen-
sion cut of k = 600: 1) SP-600 (scalar product)
and 2) COS-600 (cosine similarity). The other
two algorithms: CurveS1 and CurveS2 use the
two curve smoothness functions in Section 3.2; the
curves plot the scalar product corresponding to the
two patterns, from dimension 1 to 800.
Data In these preliminary experiments we limit
ourselves to paraphrasing a set of patterns ex-
tracted from a subset of the TREC02-TREC06
question answering tracks. From these questions
we extracted and paraphrased the most frequently
occurring 20 patterns. Since judging the cor-
rectness of these paraphrases ”out-of-context” is
rather difficult we limit ourselves to giving exam-
ples and analyzing errors made on this data; im-
portant observations can be clearly made this way,
however in future work we plan to build a proper
evaluation setting (e.g. task-based or instance-
based in the sense of (Szpektor et al., 2007)) for
</bodyText>
<page confidence="0.993674">
29
</page>
<bodyText confidence="0.9721075">
a more detailed analysis of the performance on the
methods discussed.
</bodyText>
<subsectionHeader confidence="0.602737">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.996019163265306">
We list the paraphrases obtained with the different
methods for the pattern X subj
←−−− show dobj
−−−→ Y . This
pattern has been chosen out of the total set due
to its medium difficulty in terms of paraphrasing;
some of the patterns in our list are relatively ac-
curately paraphrased by all methods, such as win,
while others such as marry are almost impossible
to paraphrase, for all methods. In Table 2 we list
the top 10 expansions returned by the four meth-
ods using the SVD factorization. In bold we mark
correct patterns, which we consider to be patterns
for which there is a context in which the entail-
ment holds in at least one direction.
As it is clearly reflected in this example the SP-
600 is much worse than any of the curve analy-
sis methods; however using cosine as similarity
measure at the same arbitrarily chosen dimension
(COS-600) brings major improvements.
The two curve smoothness methods exhibit a
systematic difference between them. In this ex-
ample, and also across all 20 instances we have
considered, CurveS1 ranks as most similar, a large
variety of patterns with the same lexical root (in
which, of course, syntax is often incorrect). Only
following this we can find patterns expressing lex-
ical variations; these again will be present in many
syntactic variations. This sets CurveS1 apart from
both CurveS2 and from COS-600 methods. These
latter two methods, although conceptually differ-
ent seem to exhibit surprisingly similar behavior.
The behavior of CurveS1 smoothing method is
difficult to judge without a proper evaluation; it
can be the case that the errors (mostly in syntac-
tic relations) are indeed errors of the algorithm or
that the parser introduces them already in our input
data.
Table 3 shows the top 10 paraphrases returned
by the DIRT algorithm. The DIRT paraphrases are
rather accurate, however it is interesting to observe
that DIRT and SVD methods can extract differ-
ent paraphrases. Table 4 gives examples of correct
paraphrases which are identified by DIRT but not
CurveS2 and the other way around. This seems to
indicate that these algorithms do capture different
aspects of the data and can be combined for bet-
ter results. An important aspect here is the fact
that obtaining highly accurate paraphrases at the
</bodyText>
<figure confidence="0.922732125">
DIRT
pobj
in
show dobj
←−−−
prp
←−−
−−−→
pobj
to
show dobj
prp
−−−→
subj prp pob
j←−−− show −−→ in
−−−→
subj
dobj
←−−− display −−−→
subj
dobj←−−− bring −−−→
←−−− with prp
pobj ←−− show dobj
−−−→
</figure>
<tableCaption confidence="0.884169">
Table 3: Top 10 paraphrases for X subj
</tableCaption>
<equation confidence="0.4097165">
←−−− show dobj →
Y
</equation>
<bodyText confidence="0.928311714285714">
cost of losing coverage is not particularly difficult4
however not very useful. Previous work such as
(Dinu and Wang, 2009) has shown that for these
resources, the coverage is a rather important as-
pect, since they have to capture the great variety
of ways in which a meaning can be expressed in
different contexts.
</bodyText>
<figure confidence="0.8963388125">
CurveS2 DIRT
subj dobj
←−−− show −−−→
subj dobj
←−−− win −−−→
subj dobj
←−−− enter −−−→
←−−− march prp
subj −−→ into pobj
−−−→
←−−−subj prp pobj
advance −−→ into
−−−→
←−−entry prp
pos−−→ to pobj
−−−→
</figure>
<tableCaption confidence="0.572651">
Table 4: Example of paraphrases (i.e. ranked in
</tableCaption>
<bodyText confidence="0.595436">
the top 30) identified by one method and not the
other
</bodyText>
<subsectionHeader confidence="0.936443">
4.2 Discussion
</subsectionHeader>
<bodyText confidence="0.993482428571429">
In this section we attempt to get more insight into
the way the relatedness curves relate to the intu-
itive notion of similarity, by examining curves of
incorrect paraphrases extracted by our methods.
The first error we consider, is the pattern X pos
←−−
confidence pobj
</bodyText>
<subsectionHeader confidence="0.5276">
←−−− of prp
</subsectionHeader>
<bodyText confidence="0.9995418">
←−− Y which is judged as be-
ing very similar to show by SP-600, COS-600 as
well as CurveS2. Figure 4 shows the relatedness
curves. As it can be noticed, both the X and Y
similarities grow dramatically around dimension
</bodyText>
<figure confidence="0.646104666666667">
←−−− reflect dobj
subj −−−→
←−−−
←−−
←−−−represent dobj
subj −−−→
</figure>
<footnote confidence="0.945909">
4High precision can be very easily achieved simply by in-
tersecting the sets of paraphrases returned by two or more of
the methods implemented
</footnote>
<figure confidence="0.987500595238095">
←−−− in prp
pobj ←−− indicate dobj
−−−→
←−−−in prp
pobj ←−− reflect dobj
−−−→
←−−−interpret prp
dobj −−→ as pobj
−−−→
subj dobj
←−−− display −−−→
subj dobj
←−−− confirm −−−→
←−−−point prp
subj −−→ to pobj
−−−→
−−−−→ appos nn
winner −−→
←−−− vie prp
subj −−→ for pobj −−−→
subjprp pobj
←−−− compete −−→ for −−−→
←−− victory prp
pos −−→ in pobj
−−−→
subj ←−−−win −−−→title nn
dobj −−→
dobj
−−−→
subj
←−−− secure
subj
dobj←−−− indicate −−−→
subj dobj
←−−− demonstrate −−−→
←−−−start prp
subj −−→ in pobj
−−−→
subj prp pobj
←−−− play −−→ in −−−→
subj prp pobj
←−−− join −−→ in −−−→
</figure>
<page confidence="0.928667">
30
</page>
<table confidence="0.9972592">
SP-600 COS-600 CurveS1 CurveS2
pos ←−−− of prp subj → subj −−→ in pobj −−−→ subj →
←−− confidence pobj ←−− ← indicate dobj ←−−−show prp ← indicate dobj
subj subj dobj
←−−− boost dobj ←−−− indicate −−−→
−−−→ rate nn subj −−→ with pobj
−−→ ←−−− show prp −−−→
subj −−→ of pobj pobj prp dobj
←−−−show prp −−−→ ←−−− ←
prp with show −−−→
−−→ to pobj
−−−→ percent nn
−−→
subj −−−→ yuan appos −−−−→
←−−− total dobj
subj −−−→ dollar appos
←−−− hit dobj −−−−→
←−−−subj dobj appos
reach −−−→ dollar−−−−→
subj
←−−−slash dobj
−−−→ rate nn
−−→
nn ←−−− of prp
←−− confidence pobj ←−−
subj
←−−−raise dobj
−−−→ rate nn
−−→
subj ←−−− reflect dobj
−−→ of pobj subj −−−→
←−−− show prp −−−→ subj −−−→
←−−− represent dobj ←−−− represent dobj
subj −−−→ subj dobj −−→
pobj ←−−− bring −−−→ rate nn
←−− show partmod subj −−→ of pobj
←−−− by prp ←−−−−−− ←−−−show prp −−−→
pobj ←−− reflect dobj −−−→ dobj pobj →
←−−− in prp ←−−− interpret prp
pos ←−−− of prp −−→ as
←−− confidence pobj ←−−
pobj prp dobj
←−−− by ←−− reflect
pobj
←−− indicate dobj
←−−− in prp
subj
←−−− reflect dobj
−−−→
subj
−−→ as pobj
←−−− interpret prp −−−→
subj −−−−→
←−−−show tmod
subj −−→ despite pobj
←−−− show prp −−−→
pobj prp dobj
during ←−− show −−−→
pobj
in←−− show prp dobjsubj −−−→
pobj
←−−show partmod
←−−− by prp ←−−−−−−
pobj
← show dobj
← on prp −−−→
pos pobj prp
←−− confidence ←−−− of ←−−
dobj −−→
←−−−show −−−→rate nn
subj −−−→ rate nn
←−−−put dobj −−→
pobj
←−− show partmod
←−−− by prp ←−−−−−−
</table>
<tableCaption confidence="0.998971">
Table 2: Top 10 paraphrases for X subj
</tableCaption>
<bodyText confidence="0.96330827027027">
←−−− show dobj → Y
500. Therefore the scalar product will be very high
at cut point 600, leading to methods’ SP-600 and
COS-600 error. However the two curve methods
are sensitive to the shape of the relatedness curves.
Since CurveS2 is sensitive to actual drop values in
these curves, this pair will still be ranked very sim-
ilar. The curves do decrease by small amounts in
many points which is why method CurveS1 does
score these two patterns as very similar.
An interesting point to be made here is that, this
pair is ranked similar by three methods out of four
because of the dramatic increase in relatedness at
around dimension 500. However, intuitively, such
an increase should be more relevant at earlier di-
mensions, which correspond to the larger eigen-
values, and therefore to the most relevant con-
cepts. Indeed, in the data we have analyzed, highly
similar patterns exhibit large increases at earlier
(first 100-200) dimensions, similarly to the exam-
ples given in Figure 1 and Figure 2. This leads
us to a particular aspect that we would like to in-
vestigate in future work, which is to analyze the
behavior of a relatedness curve in relation to rel-
evance weights obtained from the eigenvalues of
the matrix factorization.
In Figure 5 we plot a second error, the relat-
edness curves of show with X subj
←−−− boost dobj
−−−→
rate nn
−−→ Y which is as error made only by the SP-
600 method. The similarity reflected in curve Y
is relatively high (given by the large overlap of Y-
filler interest), however we obtain a very high X
similarity only due to the peak of the scalar prod-
uct exactly around the cut dimension 600.
</bodyText>
<sectionHeader confidence="0.999719" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.949746">
In this paper we have investigated the relevance of
judging similarity of two phrases across all ranks
k in a SVD approximation of a phrase/term co-
</bodyText>
<figureCaption confidence="0.999451">
Figure 4: X-filler and Y-filler relatedness curves
</figureCaption>
<figure confidence="0.971077666666667">
for (X subj
←−−− show dobj
−−−→ Y, X pos
←−− confidence pobj ←−−−
ofprp
←−− Y )
</figure>
<figureCaption confidence="0.978371">
Figure 5: X-filler and Y-filler relatedness curves
for (X subj
</figureCaption>
<bodyText confidence="0.591617">
←−−− show dobj
</bodyText>
<equation confidence="0.875769">
−−−→ Y, X subj
←−−− boost dobj −−−→
rate −−→ Y )
</equation>
<bodyText confidence="0.96898975">
nn
occurrence matrix. We confirm the major observa-
tions made in previous work and our preliminary
experiments indicate that reliable similarity scores
for paraphrasing can be obtained from the analysis
of relatedness scores across all dimensions.
In the future we plan to 1) use the observations
we have made in Section 4.2 to focus on iden-
tifying good curve-smoothness functions and 2)
build an appropriate evaluation setting in order to
be able to accurately judge the performance of the
methods we propose.
Finally, in this paper we have investigated these
aspects for the task of paraphrasing in a particular
setting, however our findings can be applied to any
vector space method for semantic similarity.
</bodyText>
<page confidence="0.999836">
31
</page>
<sectionHeader confidence="0.998337" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999140545454545">
Scott C. Deerwester and Susan T. Dumais and Thomas
K. Landauer and George W. Furnas and Richard A.
Harshman 1990. Indexing by Latent Semantic Anal-
ysis In JASIS.
Bast, Holger and Majumdar, Debapriyo. 2005. Why
spectral retrieval works. SIGIR ’05: Proceedings of
the 28th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discov-
ery of Inference Rules from Text. In Proceedings of
the ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009).
Idan Szpektor and Eyal Shnarch and Ido Dagan 2007.
Instance-based Evaluation of Entailment Rule Ac-
quisition. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics.
</reference>
<page confidence="0.999275">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949500">
<title confidence="0.999835">Relatedness Curves for Acquiring Paraphrases</title>
<author confidence="0.994534">Georgiana Dinu Grzegorz Chrupała</author>
<affiliation confidence="0.999997">Saarland University Saarland University</affiliation>
<address confidence="0.981761">Saarbruecken, Germany Saarbruecken, Germany</address>
<email confidence="0.979873">dinu@coli.uni-sb.degchrupala@lsv.uni-saarland.de</email>
<abstract confidence="0.999296636363636">In this paper we investigate methods for computing similarity of two phrases based on their relatedness scores across a SVD approximation of a phrase/term co-occurrence matrix. We confirm the major observations made in previous work and our preliminary experiments indicate that these methods can lead to reliable similarity scores which in turn can be used for the task of paraphrasing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis In JASIS.</title>
<date>1990</date>
<contexts>
<context position="1427" citStr="Deerwester et al., 1990" startWordPosition="205" endWordPosition="208">s assigned a representation as a point in a high dimensional space, where the dimensions represent contextual features; following this, vector similarity measures are used to judge the meaning relatedness of words. One way to make these computations more reliable is to use Singular Value Decomposition (SVD) in order to obtain a lower rank approximation of an original co-occurrence matrix. SVD is a matrix factorization method which has applications in a large number of fields such as signal processing or statistics. In natural language processing methods such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990) use SVD to obtain a factorization of a (typically) word/document co-occurrence matrix. The underlying idea in these models is that the dimensionality reduction will produce meaningful dimensions which represent concepts rather than just terms, rendering similarity measures on these vectors more accurate. Over the years, it has been shown that these methods can closely match human similarity judgments and that they can be used in various applications such as information retrieval, document classification, essay grading etc. However it has been noted that the success of these methods is drastic</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester and Susan T. Dumais and Thomas K. Landauer and George W. Furnas and Richard A. Harshman 1990. Indexing by Latent Semantic Analysis In JASIS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Bast</author>
<author>Debapriyo Majumdar</author>
</authors>
<title>Why spectral retrieval works.</title>
<date>2005</date>
<booktitle>SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="2137" citStr="Bast and Majumdar, 2005" startWordPosition="317" endWordPosition="320">. The underlying idea in these models is that the dimensionality reduction will produce meaningful dimensions which represent concepts rather than just terms, rendering similarity measures on these vectors more accurate. Over the years, it has been shown that these methods can closely match human similarity judgments and that they can be used in various applications such as information retrieval, document classification, essay grading etc. However it has been noted that the success of these methods is drastically determined by the choice of dimension k to which the original space is reduced. (Bast and Majumdar, 2005) investigates exactly this aspect and proves that no fixed choice of dimension is appropriate. The authors show that two terms can be reliably compared only by investigating the curve of their relatedness scores over all dimensions k. The authors use a term/document matrix and analyze relatedness curves for inducing a hard related/not-related decision and show that their algorithms significantly improve over previous methods for information retrieval. In this paper we investigate: 1) how the findings of (Bast and Majumdar, 2005) carry over to acquiring paraphrases using SVD on a phrase/term co</context>
<context position="3912" citStr="Bast and Majumdar, 2005" startWordPosition="610" endWordPosition="613">order)1. Similarity between terms i and j is computed as the scalar product between the two vectors associated to the words in the U matrix: sim(ui, uj) = Ekl�1uilujl 1Any approximation of rank k &lt; r can simply be obtained from an approximation or rank r by deleting rows and columns. 27 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 27–32, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2.2 Relatedness curves Finding the optimal dimensionality k has proven to be an extremely important and not trivial step. (Bast and Majumdar, 2005) show that no single cut dimension is appropriate to compute the similarity of two terms but this should be deduced from the curve of similarity scores over all dimensions k. The curve of relatedness for two terms ui and uj is given by their scalar product across all dimensions k, k smaller than a rank r: k , E l�1uilujl, for k = 1, ..., r They show that a smooth curve indicates closely related terms, while a curve exhibiting many direction changes indicates unrelated terms; the actual values of the similarity scores are often misleading, which explains why a good cut dimension k is so difficu</context>
<context position="7831" citStr="Bast and Majumdar, 2005" startWordPosition="1320" endWordPosition="1323">blem .. (X solve Y, Y) (X settle Y, Y) .. 6.1 4.4 .. .. 5.2 5.9 .. 28 Figure 3: X-filler and Y-filler relatedness curves for (X subj ←−−− win dobj −−−→ Y, X subj ←−−− murder dobj −−−→ Y ) tor of similarity. In Figure 2 we investigate a pair of closely related patterns: (Xsubj ←−−− leader prp −−→ of pobj −−−→ Y, X pobj ←−−− by prp ←−− lead subj −−−→ Y ). It can be noticed that while still not comparable to those of the identical pair, these curves are much smoother than the ones associated to the pair of unrelated patterns in Figure 33. However, unlike in the information retrieval scenario in (Bast and Majumdar, 2005), for which a hard related/not-related assignment works best, for acquiring paraphrases we need to quantify the smoothness of the curves. We describe two functions for evaluating curve smoothness which we will use to compute scores in X-filler and Y-filler semantic spaces. Smooth function 1 This function simply computes the number of changes in the direction of the curve, as the percentage of times the scalar product increases or remains equal from step l to step l + 1: Σuilujl��1 CurveS1(ui, uj) = k , l = 1, ..., k An increasing curve will be assigned the maximal value 1, while for a curve th</context>
</contexts>
<marker>Bast, Majumdar, 2005</marker>
<rawString>Bast, Holger and Majumdar, Debapriyo. 2005. Why spectral retrieval works. SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - Discovery of Inference Rules from Text.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="4731" citStr="Lin and Pantel, 2001" startWordPosition="755" endWordPosition="758">s for two terms ui and uj is given by their scalar product across all dimensions k, k smaller than a rank r: k , E l�1uilujl, for k = 1, ..., r They show that a smooth curve indicates closely related terms, while a curve exhibiting many direction changes indicates unrelated terms; the actual values of the similarity scores are often misleading, which explains why a good cut dimension k is so difficult to find. 2.3 Vector space representation of phrases We choose to apply this to acquiring paraphrases (or inference rules, i.e. entailments which hold in just one direction) in the sense of DIRT (Lin and Pantel, 2001). In the DIRT algorithm a phrase is a nounending path in a dependency graph and the goal is to acquire inference rules such as (X solve Y, X find solution to Y). We will call dependency paths patterns. The input data consists of large amounts of parsed text, from which patterns together with X-filler and Y-filler frequency counts are extracted. In this setting, a pattern receives two vector representation, one in a X-filler space and one in the Y-filler space. In order to compute the similarity between two patterns, these are compared in the X space and in the Y space, and the two resulting sc</context>
<context position="9386" citStr="Lin and Pantel, 2001" startWordPosition="1593" endWordPosition="1596">the previous method this function is sensitive to the absolute values in the drops of a curve. 3The drop out dimension discussed in (Bast and Majumdar, 2005) Section 3, does not seem to exist for our data. This is to be expected since this result stems from a definition of perfectly related terms which is adapted to the particularities of term/document matrices, and not of term/term matrices. A curve with large drops, irrelevant of their cardinality, will be penalized by being assigned a low score. 4 Experimental results In order to compute the similarity score between two phrases, we follow (Lin and Pantel, 2001) and compute two similarity scores, corresponding to the X-fillers and Y-fillers, and multiply them. Given a similarity function, any pattern encountered in the corpus can be paraphrased by returning its most similar patterns. We implement five similarity functions on the data we have described in the previous section. The first one is the DIRT algorithm and it is the only method using the original co-occurrence matrix in which raw counts are replaced by pointwise mutual information scores. DIRT method The similarity function for two vectors pi and pj is: simLin (pi , pj) = 1:lEI(pi)nI(pj)(pil</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery of Inference Rules from Text. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Rui Wang</author>
</authors>
<title>Inference rules and their application to recognizing textual entailment.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<contexts>
<context position="13948" citStr="Dinu and Wang, 2009" startWordPosition="2370" endWordPosition="2373">veS2 and the other way around. This seems to indicate that these algorithms do capture different aspects of the data and can be combined for better results. An important aspect here is the fact that obtaining highly accurate paraphrases at the DIRT pobj in show dobj ←−−− prp ←−− −−−→ pobj to show dobj prp −−−→ subj prp pob j←−−− show −−→ in −−−→ subj dobj ←−−− display −−−→ subj dobj←−−− bring −−−→ ←−−− with prp pobj ←−− show dobj −−−→ Table 3: Top 10 paraphrases for X subj ←−−− show dobj → Y cost of losing coverage is not particularly difficult4 however not very useful. Previous work such as (Dinu and Wang, 2009) has shown that for these resources, the coverage is a rather important aspect, since they have to capture the great variety of ways in which a meaning can be expressed in different contexts. CurveS2 DIRT subj dobj ←−−− show −−−→ subj dobj ←−−− win −−−→ subj dobj ←−−− enter −−−→ ←−−− march prp subj −−→ into pobj −−−→ ←−−−subj prp pobj advance −−→ into −−−→ ←−−entry prp pos−−→ to pobj −−−→ Table 4: Example of paraphrases (i.e. ranked in the top 30) identified by one method and not the other 4.2 Discussion In this section we attempt to get more insight into the way the relatedness curves relate </context>
</contexts>
<marker>Dinu, Wang, 2009</marker>
<rawString>Georgiana Dinu and Rui Wang. 2009. Inference rules and their application to recognizing textual entailment. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009).</rawString>
</citation>
<citation valid="true">
<title>Idan Szpektor and Eyal Shnarch and Ido Dagan</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>2007</marker>
<rawString>Idan Szpektor and Eyal Shnarch and Ido Dagan 2007. Instance-based Evaluation of Entailment Rule Acquisition. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>