<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988413">
Automatic learning of textual entailments with cross-pair similarities
</title>
<author confidence="0.931237">
Fabio Massimo Zanzotto
</author>
<affiliation confidence="0.789205333333333">
DISCo
University of Milano-Bicocca
Milan, Italy
</affiliation>
<email confidence="0.98456">
zanzotto@disco.unimib.it
</email>
<author confidence="0.992817">
Alessandro Moschitti
</author>
<affiliation confidence="0.842832333333333">
Department of Computer Science
University of Rome “Tor Vergata”
Rome, Italy
</affiliation>
<email confidence="0.991114">
moschitti@info.uniroma2.it
</email>
<sectionHeader confidence="0.997281" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994138">
In this paper we define a novel similarity
measure between examples of textual en-
tailments and we use it as a kernel func-
tion in Support Vector Machines (SVMs).
This allows us to automatically learn the
rewrite rules that describe a non trivial set
of entailment cases. The experiments with
the data sets of the RTE 2005 challenge
show an improvement of 4.4% over the
state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999157322580645">
Recently, textual entailment recognition has been
receiving a lot of attention. The main reason is
that the understanding of the basic entailment pro-
cesses will allow us to model more accurate se-
mantic theories of natural languages (Chierchia
and McConnell-Ginet, 2001) and design important
applications (Dagan and Glickman, 2004), e.g.,
Question Answering and Information Extraction.
However, previous work (e.g., (Zaenen et al.,
2005)) suggests that determining whether or not
a text T entails a hypothesis H is quite complex
even when all the needed information is explic-
itly asserted. For example, the sentence T1: “At
the end of the year, all solid companies pay divi-
dends.” entails the hypothesis H1: “At the end of
the year, all solid insurance companies pay divi-
dends.” but it does not entail the hypothesis H2:
“At the end of the year, all solid companies pay
cash dividends.”
Although these implications are uncontrover-
sial, their automatic recognition is complex if we
rely on models based on lexical distance (or sim-
ilarity) between hypothesis and text, e.g., (Corley
and Mihalcea, 2005). Indeed, according to such
approaches, the hypotheses H1 and H2 are very
similar and seem to be similarly related to T1. This
suggests that we should study the properties and
differences of such two examples (negative and
positive) to derive more accurate entailment mod-
els. For example, if we consider the following en-
tailment:
</bodyText>
<figure confidence="0.472774714285714">
T3 ⇒ H3?
T3 “All wild animals eat plants that have
scientifically proven medicinal proper-
ties.”
H3 “All wild mountain animals eat plants
that have scientifically proven medici-
nal properties.”
</figure>
<bodyText confidence="0.99969672">
we note that T3 is structurally (and somehow lex-
ically similar) to T1 and H3 is more similar to H1
than to H2. Thus, from T1 ⇒ H1 we may extract
rules to derive that T3 ⇒ H3.
The above example suggests that we should rely
not only on a intra-pair similarity between T and
H but also on a cross-pair similarity between two
pairs (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;). The latter similarity
measure along with a set of annotated examples al-
lows a learning algorithm to automatically derive
syntactic and lexical rules that can solve complex
entailment cases.
In this paper, we define a new cross-pair similar-
ity measure based on text and hypothesis syntactic
trees and we use such similarity with traditional
intra-pair similarities to define a novel semantic
kernel function. We experimented with such ker-
nel using Support Vector Machines (Vapnik, 1995)
on the test tests of the Recognizing Textual En-
tailment (RTE) challenges (Dagan et al., 2005;
Bar Haim et al., 2006). The comparative results
show that (a) we have designed an effective way
to automatically learn entailment rules from ex-
amples and (b) our approach is highly accurate and
exceeds the accuracy of the current state-of-the-art
</bodyText>
<page confidence="0.982852">
401
</page>
<note confidence="0.5371615">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 401–408,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998254375">
models (Glickman et al., 2005; Bayer et al., 2005)
by about 4.4% (i.e. 63% vs. 58.6%) on the RTE 1
test set (Dagan et al., 2005).
In the remainder of this paper, Sec. 2 illustrates
the related work, Sec. 3 introduces the complexity
of learning entailments from examples, Sec. 4 de-
scribes our models, Sec. 6 shows the experimental
results and finally Sec. 7 derives the conclusions.
</bodyText>
<sectionHeader confidence="0.999908" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99972047368421">
Although the textual entailment recognition prob-
lem is not new, most of the automatic approaches
have been proposed only recently. This has been
mainly due to the RTE challenge events (Dagan et
al., 2005; Bar Haim et al., 2006). In the following
we report some of such researches.
A first class of methods defines measures of
the distance or similarity between T and H ei-
ther assuming the independence between words
(Corley and Mihalcea, 2005; Glickman et al.,
2005) in a bag-of-word fashion or exploiting syn-
tactic interpretations (Kouylekov and Magnini,
2005). A pair (T, H) is then in entailment when
sim(T, H) &gt; α. These approaches can hardly
determine whether the entailment holds in the ex-
amples of the previous section. From the point of
view of bag-of-word methods, the pairs (T1, H1)
and (T1, H2) have both the same intra-pair simi-
larity since the sentences of T1 and H1 as well as
those of T1 and H2 differ by a noun, insurance and
cash, respectively. At syntactic level, also, we can-
not capture the required information as such nouns
are both noun modifiers: insurance modifies com-
panies and cash modifies dividends.
A second class of methods can give a solution
to the previous problem. These methods generally
combine a similarity measure with a set of possi-
ble transformations T applied over syntactic and
semantic interpretations. The entailment between
T and H is detected when there is a transformation
r E T so that sim(r(T), H) &gt; α. These trans-
formations are logical rules in (Bos and Markert,
2005) or sequences of allowed rewrite rules in (de
Salvo Braz et al., 2005). The disadvantage is that
such rules have to be manually designed. More-
over, they generally model better positive implica-
tions than negative ones and they do not consider
errors in syntactic parsing and semantic analysis.
</bodyText>
<sectionHeader confidence="0.948242" genericHeader="method">
3 Challenges in learning from examples
</sectionHeader>
<bodyText confidence="0.998479622641509">
In the introductory section, we have shown that,
to carry out automatic learning from examples, we
need to define a cross-pair similarity measure. Its
definition is not straightforward as it should detect
whether two pairs (T0, H0) and (T00, H00) realize
the same rewrite rules. This measure should con-
sider pairs similar when: (1) T0 and H0 are struc-
turally similar to T00 and H00, respectively and (2)
the lexical relations within the pair (T0, H0) are
compatible with those in (T00, H00). Typically, T
and H show a certain degree of overlapping, thus,
lexical relations (e.g., between the same words)
determine word movements from T to H (or vice
versa). This is important to model the syntac-
tic/lexical similarity between example pairs. In-
deed, if we encode such movements in the syntac-
tic parse trees of texts and hypotheses, we can use
interesting similarity measures defined for syntac-
tic parsing, e.g., the tree kernel devised in (Collins
and Duffy, 2002).
To consider structural and lexical relation simi-
larity, we augment syntactic trees with placehold-
ers which identify linked words. More in detail:
- We detect links between words wt in T that are
equal, similar, or semantically dependent on words
wh in H. We call anchors the pairs (wt, wh) and
we associate them with placeholders. For exam-
ple, in Fig. 1, the placeholder 2” indicates the
(companies,companies) anchor between T1 and
H1. This allows us to derive the word movements
between text and hypothesis.
- We align the trees of the two texts T0 and T00 as
well as the tree of the two hypotheses H0 and H00
by considering the word movements. We find a
correct mapping between placeholders of the two
hypothesis H0 and H00 and apply it to the tree of
H00 to substitute its placeholders. The same map-
ping is used to substitute the placeholders in T00.
This mapping should maximize the structural sim-
ilarity between the four trees by considering that
placeholders augment the node labels. Hence, the
cross-pair similarity computation is reduced to the
tree similarity computation.
The above steps define an effective cross-pair
similarity that can be applied to the example in
Fig. 1: T1 and T3 share the subtree in bold start-
ing with S → NP VP. The lexicals in T3 and H3
are quite different from those T1 and H1, but we
can rely on the structural properties expressed by
their bold subtrees. These are more similar to the
subtrees of T1 and H1 than those of T1 and H2,
respectively. Indeed, H1 and H3 share the pro-
duction NP → DT JJ NN NNS while H2 and H3 do
</bodyText>
<page confidence="0.995753">
402
</page>
<figure confidence="0.99989475">
T1 T3
S S
PP
solid companies
2’ 2”
,
NP 2
VP 3
NP a
VP b
DT
JJ a
NNS a
NP 0
IN
, DT
NP c
VBP b
VBP 3
NNS 2
JJ 2
NP 4
NNS 4
All
animals
a”
PP
NP 0
all
At
plants
... properties
c
wild
a’
eat
b
pay
3
NN 0
IN
NP 1
dividends
4
DT
the
end
0
of
DT
NN 1
the
year
1
H3
H1
S
S
NP 2
VP 3
NP a
VP b
PP
,
IN
At
NP 0
NP 0
PP
, DT
all
JJ 2
solid
2’
insurance
NN
companies
2”
NNS 2
VBP 3
pay
3
NP 4
NNS 4
DT
All
JJ a
wild
a’
mountain
NN
NNS a
animals
a”
VBP b
eat
b
NP c
plants
... properties
c
NN 0
IN
NP 1
dividends
4
DT
the
end
0
of
DT
NN 1
the
year
1
H3
H2
S
S
</figure>
<figureCaption confidence="0.903074">
Figure 1: Relations between (T1, H1), (T1, H2), and (T3, H3).
</figureCaption>
<figure confidence="0.997986842105263">
wild
a’
mountain
All
animals
a”
NN
NNS 4
all
plants
... properties
c
eat
b
pay
3
PP
solid companies
2’ 2”
NP 2
VP 3
NP a
VP b
At ... year
DT
JJ 2
NNS 2
VBP 3
NP 4
DT
JJ a
NN
NNS a
VBP b
NP c
cash
dividends
4
</figure>
<bodyText confidence="0.974378814814815">
not. Consequently, to decide if (T3,H3) is a valid
entailment, we should rely on the decision made
for (T1, H1). Note also that the dashed lines con-
necting placeholders of two texts (hypotheses) in-
dicate structurally equivalent nodes. For instance,
the dashed line between 3 and b links the main
verbs both in the texts T1 and T3 and in the hy-
potheses H1 and H3. After substituting 3 with b
and 2 with a, we can detect if T1 and T3 share
the bold subtree S —* NP 2 VP 3. As such subtree
is shared also by H1 and H3, the words within the
pair (T1, H1) are correlated similarly to the words
in (T3, H3).
The above example emphasizes that we need
to derive the best mapping between placeholder
sets. It can be obtained as follows: let A0 and A00
be the placeholders of (T0, H0) and (T00, H00), re-
spectively, without loss of generality, we consider
|A0 |&gt; |A00 |and we align a subset of A0 to A00. The
best alignment is the one that maximizes the syn-
tactic and lexical overlapping of the two subtrees
induced by the aligned set of anchors.
More precisely, let C be the set of all bijective
mappings from a0 C_ A0 : |a0 |= |A00 |to A00, an
element c E C is a substitution function. We
define as the best alignment the one determined
by cmax = argmaxcEC(KT (t(H&apos;, c), t(H&apos;&apos;, i))+
</bodyText>
<equation confidence="0.867952">
KT(t(T&apos;, c), t(T&apos;&apos;, i)) (1)
</equation>
<bodyText confidence="0.999792">
where (a) t(5, c) returns the syntactic tree of the
hypothesis (text) 5 with placeholders replaced by
means of the substitution c, (b) i is the identity
substitution and (c) KT(t1, t2) is a function that
measures the similarity between the two trees t1
and t2 (for more details see Sec. 4.2). For ex-
ample, the cmax between (T1, H1) and (T3, H3)
is {(2’, a’), (2”, a”), (3, b), (4, c)}.
</bodyText>
<sectionHeader confidence="0.976233" genericHeader="method">
4 Similarity Models
</sectionHeader>
<bodyText confidence="0.999714625">
In this section we describe how anchors are found
at the level of a single pair (T, H) (Sec. 4.1). The
anchoring process gives the direct possibility of
implementing an inter-pair similarity that can be
used as a baseline approach or in combination with
the cross-pair similarity. This latter will be imple-
mented with tree kernel functions over syntactic
structures (Sec. 4.2).
</bodyText>
<subsectionHeader confidence="0.998791">
4.1 Anchoring and Lexical Similarity
</subsectionHeader>
<bodyText confidence="0.969008090909091">
The algorithm that we design to find the anchors
is based on similarity functions between words or
more complex expressions. Our approach is in line
with many other researches (e.g., (Corley and Mi-
halcea, 2005; Glickman et al., 2005)).
Given the set of content words (verbs, nouns,
adjectives, and adverbs) WT and WH of the two
sentences T and H, respectively, the set of anchors
A C WT x WH is built using a similarity measure
between two words simw(wt, wh). Each element
wh E WH will be part of a pair (wt, wh) E A if:
</bodyText>
<equation confidence="0.776374">
1) simw(wt, wh) =� 0
2) simw(wt, wh) = maxw&apos; t∈WT simw(w0t, wh)
</equation>
<bodyText confidence="0.962339608695652">
According to these properties, elements in WH
can participate in more than one anchor and con-
versely more than one element in WH can be
linked to a single element w E WT.
The similarity simw(wt, wh) can be defined us-
ing different indicators and resources. First of all,
two words are maximally similar if these have the
same surface form wt = wh. Second, we can use
one of the WordNet (Miller, 1995) similarities in-
dicated with d(lw, lw&apos;) (in line with what was done
in (Corley and Mihalcea, 2005)) and different rela-
tion between words such as the lexical entailment
between verbs (Ent) and derivationally relation
between words (Der). Finally, we use the edit dis-
tance measure lev(wt, wh) to capture the similar-
ity between words that are missed by the previous
analysis for misspelling errors or for the lack of
derivationally forms not coded in WordNet.
As result, given the syntactic category
cw E {noun, verb, adjective, adverbJ and
the lemmatized form lw of a word w, the simi-
larity measure between two words w and w0 is
defined as follows:
</bodyText>
<equation confidence="0.99064525">
1 if w = w0V
lw = lw0 h cw = c 0V
((lw, cw), (lw0 , cwl)) E EntV
((lw, cw), (lw0 , cw0)) E DerV
lev(w, w0) = 1
d(lw, lw0 ) ifcw = cw0 h d(lw, lw0) &gt; 0.2
0 otherwise
(2)
</equation>
<bodyText confidence="0.9997906">
It is worth noticing that, the above measure is not
a pure similarity measure as it includes the entail-
ment relation that does not represent synonymy or
similarity between verbs. To emphasize the contri-
bution of each used resource, in the experimental
section, we will compare Eq. 2 with some versions
that exclude some word relations.
The above word similarity measure can be used
to compute the similarity between T and H. In
line with (Corley and Mihalcea, 2005), we define
</bodyText>
<equation confidence="0.992541">
it as: X simw(wt, wh) x idf(wh)
s1(T,H) = (wt,wh)EA
</equation>
<bodyText confidence="0.999938">
where idf(w) is the inverse document frequency
of the word w. For sake of comparison, we
consider also the corresponding more classical
version that does not apply the inverse document
</bodyText>
<equation confidence="0.995731">
frequency X
s2(T, H) = simw(wt,wh)/|WH |(4)
(wt,wh)EA
</equation>
<bodyText confidence="0.985142">
¿From the above intra-pair similarities, s1
and s2, we can obtain the baseline cross-pair
similarities based on only lexical information:
</bodyText>
<equation confidence="0.658417">
Ki((T&apos;, H&apos;), (T&apos;&apos;, H&apos;&apos;)) = si(T&apos;, H&apos;) x si(T&apos;&apos;, H&apos;&apos;), (5)
</equation>
<bodyText confidence="0.9998865">
where i E {1, 2J. In the next section we define a
novel cross-pair similarity that takes into account
syntactic evidence by means of tree kernel func-
tions.
</bodyText>
<subsectionHeader confidence="0.997191">
4.2 Cross-pair syntactic kernels
</subsectionHeader>
<bodyText confidence="0.9999811">
Section 3 has shown that to measure the syn-
tactic similarity between two pairs, (T0, H0)
and (T00, H00), we should capture the number of
common subtrees between texts and hypotheses
that share the same anchoring scheme. The best
alignment between anchor sets, i.e. the best
substitution cmax, can be found with Eq. 1. As the
corresponding maximum quantifies the alignment
degree, we could define a cross-pair similarity as
follows:
</bodyText>
<equation confidence="0.950359">
Ks ((T&apos;, H&apos;), (T&apos;&apos;, H&apos;&apos;)) = maCx (KT(t(H&apos;, c), t(H&apos;&apos;, i))
cE
�+KT(t(T&apos;, c), t(T&apos;&apos;, i) , (6)
</equation>
<bodyText confidence="0.9841965">
where as KT (t1, t2) we use the tree kernel func-
tion defined in (Collins and Duffy, 2002). This
evaluates the number of subtrees shared by t1 and
t2, thus defining an implicit substructure space.
Formally, given a subtree space F =
{f1, f2, ... , f|F|J, the indicator function Ii(n)
is equal to 1 if the target fi is rooted at
node n and equal to 0 otherwise. A tree-
kernel function over t1 and t2 is KT(t1, t2) =
Pn1∈Nt1 Pn2∈Nt2 A(n1, n2), where Nt1 and Nt2
are the sets of the t1’s and t2’s nodes, respectively.
In turn A(n1, n2) = P|F|
</bodyText>
<equation confidence="0.982362">
i=1 λl(fi)Ii(n1)Ii(n2),
⎧
⎨⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎩
simw(w, w0) =
X idf(wh)
whEWH
(3)
</equation>
<page confidence="0.993734">
404
</page>
<bodyText confidence="0.999891151515152">
where 0 &lt; λ &lt; 1 and l(fi) is the number of lev-
els of the subtree fi. Thus λl(fi) assigns a lower
weight to larger fragments. When λ = 1, A is
equal to the number of common fragments rooted
at nodes n1 and n2. As described in (Collins and
Duffy, 2002), A can be computed in O(JNt1J x
JNt2J).
The KT function has been proven to be a valid
kernel, i.e. its associated Gram matrix is positive-
semidefinite. Some basic operations on kernel
functions, e.g. the sum, are closed with respect
to the set of valid kernels. Thus, if the maximum
held such property, Eq. 6 would be a valid ker-
nel and we could use it in kernel based machines
like SVMs. Unfortunately, a counterexample il-
lustrated in (Boughorbel et al., 2004) shows that
the max function does not produce valid kernels in
general.
However, we observe that: (1)
Ks((T0, H0), (T00, H00)) is a symmetric func-
tion since the set of transformation C are always
computed with respect to the pair that has the
largest anchor set; (2) in (Haasdonk, 2005), it
is shown that when kernel functions are not
positive semidefinite, SVMs still solve a data
separation problem in pseudo Euclidean spaces.
The drawback is that the solution may be only
a local optimum. Therefore, we can experiment
Eq. 6 with SVMs and observe if the empirical
results are satisfactory. Section 6 shows that the
solutions found by Eq. 6 produce accuracy higher
than those evaluated on previous automatic textual
entailment recognition approaches.
</bodyText>
<sectionHeader confidence="0.980848" genericHeader="method">
5 Refining cross-pair syntactic similarity
</sectionHeader>
<bodyText confidence="0.999764222222222">
In the previous section we have defined the intra
and the cross pair similarity. The former does not
show relevant implementation issues whereas the
latter should be optimized to favor its applicability
with SVMs. The Eq. 6 improvement depends on
three factors: (1) its computation complexity; (2)
a correct marking of tree nodes with placeholders;
and, (3) the pruning of irrelevant information in
large syntactic trees.
</bodyText>
<subsectionHeader confidence="0.998112">
5.1 Controlling the computational cost
</subsectionHeader>
<bodyText confidence="0.999982842105263">
The computational cost of cross-pair similarity be-
tween two tree pairs (Eq. 6) depends on the size of
C. This is combinatorial in the size of A0 and A00,
i.e. JCJ = (JA0J − JA00J)!JA00J! if JA0J &gt; JA00J. Thus
we should keep the sizes of A0 and A00 reasonably
small.
To reduce the number of placeholders, we con-
sider the notion of chunk defined in (Abney, 1996),
i.e., not recursive kernels of noun, verb, adjective,
and adverb phrases. When placeholders are in a
single chunk both in the text and hypothesis we
assign them the same name. For example, Fig. 1
shows the placeholders 2’ and 2” that are substi-
tuted by the placeholder 2. The placeholder re-
duction procedure also gives the possibility of re-
solving the ambiguity still present in the anchor
set A (see Sec. 4.1). A way to eliminate the am-
biguous anchors is to select the ones that reduce
the final number of placeholders.
</bodyText>
<subsectionHeader confidence="0.999673">
5.2 Augmenting tree nodes with placeholders
</subsectionHeader>
<bodyText confidence="0.999983260869565">
Anchors are mainly used to extract relevant syn-
tactic subtrees between pairs of text and hypoth-
esis. We also use them to characterize the syn-
tactic information expressed by such subtrees. In-
deed, Eq. 6 depends on the number of common
subtrees between two pairs. Such subtrees are
matched when they have the same node labels.
Thus, to keep track of the argument movements,
we augment the node labels with placeholders.
The larger number of placeholders two hypothe-
ses (texts) match the larger the number of their
common substructures is (i.e. higher similarity).
Thus, it is really important where placeholders are
inserted.
For example, the sentences in the pair (T1, H1)
have related subjects 2 and related main verbs
3. The same occurs in the sentences of the pair
(T3, H3), respectively a and b. To obtain such
node marking, the placeholders are propagated in
the syntactic tree, from the leaves1 to the target
nodes according to the head of constituents. The
example of Fig. 1 shows that the placeholder 0
climbs up to the node governing all the NPs.
</bodyText>
<subsectionHeader confidence="0.9481515">
5.3 Pruning irrelevant information in large
text trees
</subsectionHeader>
<bodyText confidence="0.999175">
Often only a portion of the parse trees is relevant
to detect entailments. For instance, let us consider
the following pair from the RTE 2005 corpus:
</bodyText>
<footnote confidence="0.62645">
1To increase the generalization capacity of the tree ker-
nel function we choose not to assign any placeholder to the
leaves.
</footnote>
<page confidence="0.989976">
405
</page>
<equation confidence="0.300584">
T ⇒ H (id: 929)
</equation>
<construct confidence="0.6957085">
T “Ron Gainsford, chief executive of the
TSI, said: ”It is a major concern to us
thatparents could be unwittingly expos-
ing their children to the risk ofsun dam-
age, thinking they are better protected
than they actually are.”
</construct>
<bodyText confidence="0.955346266666667">
H “Ron Gainsford is the chief executive of
the TSI.”
Only the bold part of T supports the implication;
the rest is useless and also misleading: if we used
it to compute the similarity it would reduce the im-
portance of the relevant part. Moreover, as we nor-
malize the syntactic tree kernel (KT) with respect
to the size of the two trees, we need to focus only
on the part relevant to the implication.
The anchored leaves are good indicators of rel-
evant parts but also some other parts may be very
relevant. For example, the function word not plays
an important role. Another example is given by the
word insurance in H1 and mountain in H3 (see
Fig. 1). They support the implication T1 ⇒ H1
and T1 ⇒ H3 as well as cash supports T1 4�. H2.
By removing these words and the related struc-
tures, we cannot determine the correct implica-
tions of the first two and the incorrect implication
of the second one. Thus, we keep all the words that
are immediately related to relevant constituents.
The reduction procedure can be formally ex-
pressed as follows: given a syntactic tree t, the set
of its nodes N(t), and a set of anchors, we build
a tree t&apos; with all the nodes N&apos; that are anchors or
ancestors of any anchor. Moreover, we add to t&apos;
the leaf nodes of the original tree t that are direct
children of the nodes in N&apos;. We apply such proce-
dure only to the syntactic trees of texts before the
computation of the kernel function.
</bodyText>
<sectionHeader confidence="0.997194" genericHeader="method">
6 Experimental investigation
</sectionHeader>
<bodyText confidence="0.999933857142857">
The aim of the experiments is twofold: we show
that (a) entailment recognition rules can be learned
from examples and (b) our kernel functions over
syntactic structures are effective to derive syntac-
tic properties. The above goals can be achieved by
comparing the different intra and cross pair simi-
larity measures.
</bodyText>
<subsectionHeader confidence="0.997539">
6.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.998697928571429">
For the experiments, we used the Recognizing
Textual Entailment Challenge data sets, which we
name as follows:
- D1, T1 and D2, T2, are the development and
the test sets of the first (Dagan et al., 2005) and
second (Bar Haim et al., 2006) challenges, respec-
tively. D1 contains 567 examples whereas T1,
D2 and T2 have all the same size, i.e. 800 train-
ing/testing instances. The positive examples con-
stitute the 50% of the data.
- ALL is the union of D1, D2, and T1, which we
also split in 70%-30%. This set is useful to test if
we can learn entailments from the data prepared in
the two different challenges.
- D2(50%)&apos; and D2(50%)&apos;&apos; is a random split of
D2. It is possible that the data sets of the two com-
petitions are quite different thus we created this
homogeneous split.
We also used the following resources:
- The Charniak parser (Charniak, 2000) and the
morpha lemmatiser (Minnen et al., 2001) to carry
out the syntactic and morphological analysis.
- WordNet 2.0 (Miller, 1995) to extract both the
verbs in entailment, Ent set, and the derivation-
ally related words, Der set.
- The wn::similarity package (Pedersen et
al., 2004) to compute the Jiang&amp;Conrath (J&amp;C)
distance (Jiang and Conrath, 1997) as in (Corley
and Mihalcea, 2005). This is one of the best fig-
ure method which provides a similarity score in
the [0, 1] interval. We used it to implement the
d(lw,lw,) function.
- A selected portion of the British National Cor-
pus2 to compute the inverse document frequency
(idf). We assigned the maximum idf to words not
found in the BNC.
- SVM-light-TK3 (Moschitti, 2006) which en-
codes the basic tree kernel function, KT, in SVM-
light (Joachims, 1999). We used such software
to implement K3 (Eq. 6), K1, K2 (Eq. 5) and
K3 + Ki kernels. The latter combines our new
kernel with traditional approaches (i E {1, 2}).
</bodyText>
<subsectionHeader confidence="0.994548">
6.2 Results and analysis
</subsectionHeader>
<bodyText confidence="0.999919166666667">
Table 1 reports the results of different similarity
kernels on the different training and test splits de-
scribed in the previous section. The table is orga-
nized as follows:
The first 5 rows (Experiment settings) report the
intra-pair similarity measures defined in Section
4.1, the 6th row refers to only the idf similarity
metric whereas the following two rows report the
cross-pair similarity carried out with Eq. 6 with
(Synt Trees with placeholders) and without (Only
Synt Trees) augmenting the trees with placehold-
ers, respectively. Each column in the Experiment
</bodyText>
<footnote confidence="0.997567666666667">
2http://www.natcorp.ox.ac.uk/
3SVM-light-TK is available at http://ai-nlp.info
.uniroma2.it/moschitti/
</footnote>
<page confidence="0.992428">
406
</page>
<table confidence="0.999574368421053">
Experiment Settings
w = w0 V lw = lw0 h cw = cw0 √ √ √ √ √ √ √ √
cw = cw0 h d(lw, lw0 ) &gt; 0.2 √ √ √ √ √ √
((lw, cw), (lw0, cw0)) E Der √ √ √ √
((lw, cw), (lw0, cw0)) E Ent √ √ √ √
lev(w, w0) = 1 √ √ √
idf √ √ √ √ √ √
Only Synt Trees √
Synt Trees with placeholders √
Datasets
“Train:D1-Test:T1” 0.5388 0.5813 0.5500 0.5788 0.5900 0.5888 0.6213 0.6300
“Train:T1-Test:D1” 0.5714 0.5538 0.5767 0.5450 0.5591 0.5644 0.5732 0.5838
“Train:D2(50%)0-Test:D2(50%)00” 0.6034 0.5961 0.6083 0.6010 0.6083 0.6083 0.6156 0.6350
“Train:D2(50%)00-Test:D2(50%)0” 0.6452 0.6375 0.6427 0.6350 0.6324 0.6272 0.5861 0.6607
“Train:D2-Test:T2” 0.6000 0.5950 0.6025 0.6050 0.6050 0.6038 0.6238 0.6388
Mean 0.5918 0.5927 0.5960 0.5930 0.5990 0.5985 0.6040 0.6297
(f 0.0396 ) (f 0.0303 ) (f 0.0349 ) (f 0.0335 ) (f 0.0270 ) (f 0.0235 ) (f 0.0229 ) (f 0.0282 )
“Train:ALL(70%)-Test:ALL(30%)” 0.5902 0.6024 0.6009 - 0.6131 0.6193 0.6086 0.6376
“Train:ALL-Test:T2” 0.5863 0.5975 0.5975 0.6038 - - 0.6213 0.6250
</table>
<tableCaption confidence="0.999934">
Table 1: Experimental results of the different methods over different test settings
</tableCaption>
<bodyText confidence="0.99983946">
settings indicates a different intra-pair similarity
measure built by means of a combination of basic
similarity approaches. These are specified with the
check sign √. For example, Column 5 refers to a
model using: the surface word form similarity, the
d(l,,,, l,,,t) similarity and the idf.
The next 5 rows show the accuracy on the data
sets and splits used for the experiments and the
next row reports the average and Std. Dev. over
the previous 5 results. Finally, the last two rows
report the accuracy on ALL dataset split in 70/30%
and on the whole ALL dataset used for training
and T2 for testing.
¿From the table we note the following aspects:
- First, the lexical-based distance kernels K1 and
K2 (Eq. 5) show accuracy significantly higher than
the random baseline, i.e. 50%. In all the datasets
(except for the first one), the sim,,,(T, H) simi-
larity based on the lexical overlap (first column)
provides an accuracy essentially similar to the best
lexical-based distance method.
- Second, the dataset “Train:D1-Test:T 1” allows
us to compare our models with the ones of the first
RTE challenge (Dagan et al., 2005). The accuracy
reported for the best systems, i.e. 58.6% (Glick-
man et al., 2005; Bayer et al., 2005), is not signif-
icantly different from the result obtained with K1
that uses the idf.
- Third, the dramatic improvement observed in
(Corley and Mihalcea, 2005) on the dataset
“Train:D1-Test:T 1” is given by the idf rather than
the use of the J&amp;C similarity (second vs. third
columns). The use of J&amp;C with the idf decreases
the accuracy of the idf alone.
- Next, our approach (last column) is significantly
better than all the other methods as it provides the
best result for each combination of training and
test sets. On the “Train:D1-Test:T1” test set, it
exceeds the accuracy of the current state-of-the-
art models (Glickman et al., 2005; Bayer et al.,
2005) by about 4.4 absolute percent points (63%
vs. 58.6%) and 4% over our best lexical simi-
larity measure. By comparing the average on all
datasets, our system improves on all the methods
by at least 3 absolute percent points.
- Finally, the accuracy produced by Synt Trees with
placeholders is higher than the one obtained with
Only Synt Trees. Thus, the use of placeholders
is fundamental to automatically learn entailments
from examples.
</bodyText>
<subsectionHeader confidence="0.988219">
6.2.1 Qualitative analysis
</subsectionHeader>
<bodyText confidence="0.999981333333333">
Hereafter we show some instances selected
from the first experiment “Train:T 1-Test:D1”.
They were correctly classified by our overall
model (last column) and miss-classified by the
models in the seventh and in the eighth columns.
The first is an example in entailment:
</bodyText>
<equation confidence="0.708806">
T ⇒ H (id: 35)
</equation>
<bodyText confidence="0.497247555555555">
T “Saudi Arabia, the biggest oil pro-
ducer in the world, was once a sup-
porter of Osama bin Laden and his
associates who led attacks against the
United States.”
H “Saudi Arabia is the world’s biggest oil
exporter.”
It was correctly classified by exploiting examples
like these two:
</bodyText>
<figure confidence="0.812143166666667">
T ⇒ H (id: 929)
T “Ron Gainsford, chief executive of the
TSI, said: ...”
H “Ron Gainsford is the chief executive of
the TSI.”
T ⇒ H (id: 976)
</figure>
<footnote confidence="0.502378166666667">
T “Harvey Weinstein, the co-chairman of
Miramax, who was instrumental in pop-
ularizing both independent and foreign
films with broad audiences, agrees.”
H “Harvey Weinstein is the co-chairman
ofMiramax.”
</footnote>
<page confidence="0.997112">
407
</page>
<bodyText confidence="0.99976875">
The rewrite rule is: ”X, Y, ...” implies ”X is Y”.
This rule is also described in (Hearst, 1992).
A more interesting rule relates the following
two sentences which are not in entailment:
</bodyText>
<equation confidence="0.768601">
T #&gt; H (id: 2045)
</equation>
<reference confidence="0.5225415">
T “Mrs. Lane, who has been a Director
since 1989, is Special Assistant to the
Board of Trustees and to the President
of Stanford University.”
H “Mrs. Lane is the president of Stanford
University.”
It was correctly classified using instances like the
following:
T #&gt; H (id: 2044)
T “Jacqueline B. Wender is Assistant to
the President of Stanford University.”
H “Jacqueline B. Wender is the President
of Stanford University.”
T #&gt; H (id: 2069)
T “Grieving father Christopher Yavelow
hopes to deliver one million letters to
the queen of Holland to bring his chil-
dren home.”
H “Christopher Yavelow is the queen of
Holland.”
</reference>
<bodyText confidence="0.999007">
Here, the implicit rule is: ”X (VP (V ...) (NP (to Y)
...)” does not imply ”X is Y”.
</bodyText>
<sectionHeader confidence="0.999632" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999996466666667">
We have presented a model for the automatic
learning of rewrite rules for textual entailments
from examples. For this purpose, we devised a
novel powerful kernel based on cross-pair simi-
larities. We experimented with such kernel us-
ing Support Vector Machines on the RTE test
sets. The results show that (1) learning entailments
from positive and negative examples is a viable ap-
proach and (2) our model based on kernel meth-
ods is highly accurate and improves on the current
state-of-the-art entailment systems.
In the future, we would like to study approaches
to improve the computational complexity of our
kernel function and to design approximated ver-
sions that are valid Mercer’s kernels.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828544303797">
Steven Abney. 1996. Part-of-speech tagging and partial pars-
ing. In G.Bloothooft K.Church, S.Young, editor, Corpus-
based methods in language and speech. Kluwer academic
publishers, Dordrecht.
Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-
ampiccolo, Bernardo Magnini, and Idan Szpektor. 2006.
The II PASCAL RTE challenge. In RTE Workshop,
Venice, Italy.
Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and
Alexander Yeh. 2005. MITRE’s submissions to the eu
PASCAL RTE challenge. In Proceedings of the 1st RTE
Workshop, Southampton, UK.
Johan Bos and Katja Markert. 2005. Recognising textual en-
tailment with logical inference. In Proc. of HLT-EMNLP
Conference, Canada.
S. Boughorbel, J-P. Tarel, and F. Fleuret. 2004. Non-mercer
kernel for svm object recognition. In Proceedings of
BMVC 2004.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of the 1st NAACL,Seattle, Washington.
Gennaro Chierchia and Sally McConnell-Ginet. 2001.
Meaning and Grammar: An introduction to Semantics.
MIT press, Cambridge, MA.
Michael Collins and Nigel Duffy. 2002. New ranking al-
gorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of
ACL02.
Courtney Corley and Rada Mihalcea. 2005. Measuring the
semantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and En-
tailment, Ann Arbor, Michigan.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In Proceedings of the Workshop on Learning
Methods for Text Understanding and Mining, Grenoble,
France.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The PASCAL RTE challenge. In RTE Workshop,
Southampton, U.K.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Punyakanok,
Dan Roth, and Mark Sammons. 2005. An inference
model for semantic entailment in natural language. In
Proc. of the RTE Workshop, Southampton, U.K.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web
based probabilistic textual entailment. In Proceedings of
the 1st RTE Workshop, Southampton, UK.
Bernard Haasdonk. 2005. Feature space interpretation of
SVMs with indefinite kernels. IEEE Trans Pattern Anal
Mach Intell.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 15th CoLing,
Nantes, France.
Jay J. Jiang and David W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy. In
Proc. of the 10th ROCLING, Tapei, Taiwan.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In Advances in Kernel Methods-Support Vector
Learning. MIT Press.
Milen Kouylekov and Bernardo Magnini. 2005. Tree edit
distance for textual entailment. In Proc. of the RANLP-
2005, Borovets, Bulgaria.
George A. Miller. 1995. WordNet: A lexical database for
English. Communications of the ACM, November.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Ap-
plied morphological processing of English. Natural Lan-
guage Engineering.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings ofEACL’06,
Trento, Italy.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. Wordnet::similarity - measuring the relatedness of
concepts. In Proc. of 5th NAACL, Boston, MA.
Vladimir Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
Annie Zaenen, Lauri Karttunen, and Richard Crouch. 2005.
Local textual inference: Can it be defined or circum-
scribed? In Proc. of the ACL Workshop on Empirical
Modeling of Semantic Equivalence and Entailment, Ann
Arbor, Michigan.
</reference>
<page confidence="0.997882">
408
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.407754">
<title confidence="0.999895">Automatic learning of textual entailments with cross-pair similarities</title>
<author confidence="0.999828">Fabio Massimo Zanzotto</author>
<email confidence="0.462738">DISCo</email>
<affiliation confidence="0.999891">University of Milano-Bicocca</affiliation>
<address confidence="0.986926">Milan, Italy</address>
<email confidence="0.997292">zanzotto@disco.unimib.it</email>
<author confidence="0.999763">Alessandro Moschitti</author>
<affiliation confidence="0.9995445">Department of Computer Science University of Rome “Tor Vergata”</affiliation>
<address confidence="0.913168">Rome, Italy</address>
<email confidence="0.995549">moschitti@info.uniroma2.it</email>
<abstract confidence="0.998674818181818">In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines (SVMs). This allows us to automatically learn the rules describe a non trivial set of entailment cases. The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4% over the state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Lane</author>
</authors>
<title>who has been a Director since 1989, is Special Assistant to the Board of Trustees and to the President of</title>
<institution>Stanford University.”</institution>
<marker>Lane, </marker>
<rawString>T “Mrs. Lane, who has been a Director since 1989, is Special Assistant to the Board of Trustees and to the President of Stanford University.”</rawString>
</citation>
<citation valid="false">
<authors>
<author>H “Mrs</author>
</authors>
<title>Lane is the president of</title>
<institution>Stanford University.”</institution>
<marker>“Mrs, </marker>
<rawString>H “Mrs. Lane is the president of Stanford University.”</rawString>
</citation>
<citation valid="false">
<title>It was correctly classified using instances like the following:</title>
<marker></marker>
<rawString>It was correctly classified using instances like the following:</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H</author>
</authors>
<title>Wender is Assistant to the President of Stanford University.”</title>
<date></date>
<journal>H “Jacqueline B. Wender is the President of Stanford University.” T #&gt; H</journal>
<marker>H, </marker>
<rawString>T #&gt; H (id: 2044) T “Jacqueline B. Wender is Assistant to the President of Stanford University.” H “Jacqueline B. Wender is the President of Stanford University.” T #&gt; H (id: 2069)</rawString>
</citation>
<citation valid="false">
<authors>
<author>T</author>
</authors>
<title>Grieving father Christopher Yavelow hopes to deliver one million letters to the queen of Holland to bring his children home.”</title>
<marker>T, </marker>
<rawString>T “Grieving father Christopher Yavelow hopes to deliver one million letters to the queen of Holland to bring his children home.”</rawString>
</citation>
<citation valid="false">
<authors>
<author>H “Christopher</author>
</authors>
<title>Yavelow is the queen of Holland.”</title>
<marker>“Christopher, </marker>
<rawString>H “Christopher Yavelow is the queen of Holland.”</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Part-of-speech tagging and partial parsing.</title>
<date>1996</date>
<booktitle>In G.Bloothooft K.Church, S.Young, editor, Corpusbased methods in language and speech. Kluwer academic publishers,</booktitle>
<location>Dordrecht.</location>
<contexts>
<context position="17854" citStr="Abney, 1996" startWordPosition="3175" endWordPosition="3176">VMs. The Eq. 6 improvement depends on three factors: (1) its computation complexity; (2) a correct marking of tree nodes with placeholders; and, (3) the pruning of irrelevant information in large syntactic trees. 5.1 Controlling the computational cost The computational cost of cross-pair similarity between two tree pairs (Eq. 6) depends on the size of C. This is combinatorial in the size of A0 and A00, i.e. JCJ = (JA0J − JA00J)!JA00J! if JA0J &gt; JA00J. Thus we should keep the sizes of A0 and A00 reasonably small. To reduce the number of placeholders, we consider the notion of chunk defined in (Abney, 1996), i.e., not recursive kernels of noun, verb, adjective, and adverb phrases. When placeholders are in a single chunk both in the text and hypothesis we assign them the same name. For example, Fig. 1 shows the placeholders 2’ and 2” that are substituted by the placeholder 2. The placeholder reduction procedure also gives the possibility of resolving the ambiguity still present in the anchor set A (see Sec. 4.1). A way to eliminate the ambiguous anchors is to select the ones that reduce the final number of placeholders. 5.2 Augmenting tree nodes with placeholders Anchors are mainly used to extrac</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Part-of-speech tagging and partial parsing. In G.Bloothooft K.Church, S.Young, editor, Corpusbased methods in language and speech. Kluwer academic publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<date>2006</date>
<booktitle>The II PASCAL RTE challenge. In RTE Workshop,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="3285" citStr="Haim et al., 2006" startWordPosition="523" endWordPosition="526">and (T&apos;&apos;, H&apos;&apos;). The latter similarity measure along with a set of annotated examples allows a learning algorithm to automatically derive syntactic and lexical rules that can solve complex entailment cases. In this paper, we define a new cross-pair similarity measure based on text and hypothesis syntactic trees and we use such similarity with traditional intra-pair similarities to define a novel semantic kernel function. We experimented with such kernel using Support Vector Machines (Vapnik, 1995) on the test tests of the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2005; Bar Haim et al., 2006). The comparative results show that (a) we have designed an effective way to automatically learn entailment rules from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-of-the-art 401 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 401–408, Sydney, July 2006. c�2006 Association for Computational Linguistics models (Glickman et al., 2005; Bayer et al., 2005) by about 4.4% (i.e. 63% vs. 58.6%) on the RTE 1 test set (Dagan et al., 2005). In the remainder of this paper, Sec. 2 illus</context>
<context position="22078" citStr="Haim et al., 2006" startWordPosition="3918" endWordPosition="3921">unction. 6 Experimental investigation The aim of the experiments is twofold: we show that (a) entailment recognition rules can be learned from examples and (b) our kernel functions over syntactic structures are effective to derive syntactic properties. The above goals can be achieved by comparing the different intra and cross pair similarity measures. 6.1 Experimental settings For the experiments, we used the Recognizing Textual Entailment Challenge data sets, which we name as follows: - D1, T1 and D2, T2, are the development and the test sets of the first (Dagan et al., 2005) and second (Bar Haim et al., 2006) challenges, respectively. D1 contains 567 examples whereas T1, D2 and T2 have all the same size, i.e. 800 training/testing instances. The positive examples constitute the 50% of the data. - ALL is the union of D1, D2, and T1, which we also split in 70%-30%. This set is useful to test if we can learn entailments from the data prepared in the two different challenges. - D2(50%)&apos; and D2(50%)&apos;&apos; is a random split of D2. It is possible that the data sets of the two competitions are quite different thus we created this homogeneous split. We also used the following resources: - The Charniak parser (C</context>
</contexts>
<marker>Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The II PASCAL RTE challenge. In RTE Workshop, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Bayer</author>
<author>John Burger</author>
<author>Lisa Ferro</author>
<author>John Henderson</author>
<author>Alexander Yeh</author>
</authors>
<title>MITRE’s submissions to the eu PASCAL RTE challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st RTE Workshop,</booktitle>
<location>Southampton, UK.</location>
<contexts>
<context position="3761" citStr="Bayer et al., 2005" startWordPosition="594" endWordPosition="597">or Machines (Vapnik, 1995) on the test tests of the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2005; Bar Haim et al., 2006). The comparative results show that (a) we have designed an effective way to automatically learn entailment rules from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-of-the-art 401 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 401–408, Sydney, July 2006. c�2006 Association for Computational Linguistics models (Glickman et al., 2005; Bayer et al., 2005) by about 4.4% (i.e. 63% vs. 58.6%) on the RTE 1 test set (Dagan et al., 2005). In the remainder of this paper, Sec. 2 illustrates the related work, Sec. 3 introduces the complexity of learning entailments from examples, Sec. 4 describes our models, Sec. 6 shows the experimental results and finally Sec. 7 derives the conclusions. 2 Related work Although the textual entailment recognition problem is not new, most of the automatic approaches have been proposed only recently. This has been mainly due to the RTE challenge events (Dagan et al., 2005; Bar Haim et al., 2006). In the following we repo</context>
<context position="26646" citStr="Bayer et al., 2005" startWordPosition="4694" endWordPosition="4697">¿From the table we note the following aspects: - First, the lexical-based distance kernels K1 and K2 (Eq. 5) show accuracy significantly higher than the random baseline, i.e. 50%. In all the datasets (except for the first one), the sim,,,(T, H) similarity based on the lexical overlap (first column) provides an accuracy essentially similar to the best lexical-based distance method. - Second, the dataset “Train:D1-Test:T 1” allows us to compare our models with the ones of the first RTE challenge (Dagan et al., 2005). The accuracy reported for the best systems, i.e. 58.6% (Glickman et al., 2005; Bayer et al., 2005), is not significantly different from the result obtained with K1 that uses the idf. - Third, the dramatic improvement observed in (Corley and Mihalcea, 2005) on the dataset “Train:D1-Test:T 1” is given by the idf rather than the use of the J&amp;C similarity (second vs. third columns). The use of J&amp;C with the idf decreases the accuracy of the idf alone. - Next, our approach (last column) is significantly better than all the other methods as it provides the best result for each combination of training and test sets. On the “Train:D1-Test:T1” test set, it exceeds the accuracy of the current state-o</context>
</contexts>
<marker>Bayer, Burger, Ferro, Henderson, Yeh, 2005</marker>
<rawString>Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and Alexander Yeh. 2005. MITRE’s submissions to the eu PASCAL RTE challenge. In Proceedings of the 1st RTE Workshop, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP Conference,</booktitle>
<contexts>
<context position="5626" citStr="Bos and Markert, 2005" startWordPosition="915" endWordPosition="918"> of T1 and H2 differ by a noun, insurance and cash, respectively. At syntactic level, also, we cannot capture the required information as such nouns are both noun modifiers: insurance modifies companies and cash modifies dividends. A second class of methods can give a solution to the previous problem. These methods generally combine a similarity measure with a set of possible transformations T applied over syntactic and semantic interpretations. The entailment between T and H is detected when there is a transformation r E T so that sim(r(T), H) &gt; α. These transformations are logical rules in (Bos and Markert, 2005) or sequences of allowed rewrite rules in (de Salvo Braz et al., 2005). The disadvantage is that such rules have to be manually designed. Moreover, they generally model better positive implications than negative ones and they do not consider errors in syntactic parsing and semantic analysis. 3 Challenges in learning from examples In the introductory section, we have shown that, to carry out automatic learning from examples, we need to define a cross-pair similarity measure. Its definition is not straightforward as it should detect whether two pairs (T0, H0) and (T00, H00) realize the same rewr</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising textual entailment with logical inference. In Proc. of HLT-EMNLP Conference, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boughorbel</author>
<author>J-P Tarel</author>
<author>F Fleuret</author>
</authors>
<title>Non-mercer kernel for svm object recognition.</title>
<date>2004</date>
<booktitle>In Proceedings of BMVC</booktitle>
<contexts>
<context position="16238" citStr="Boughorbel et al., 2004" startWordPosition="2906" endWordPosition="2909">s a lower weight to larger fragments. When λ = 1, A is equal to the number of common fragments rooted at nodes n1 and n2. As described in (Collins and Duffy, 2002), A can be computed in O(JNt1J x JNt2J). The KT function has been proven to be a valid kernel, i.e. its associated Gram matrix is positivesemidefinite. Some basic operations on kernel functions, e.g. the sum, are closed with respect to the set of valid kernels. Thus, if the maximum held such property, Eq. 6 would be a valid kernel and we could use it in kernel based machines like SVMs. Unfortunately, a counterexample illustrated in (Boughorbel et al., 2004) shows that the max function does not produce valid kernels in general. However, we observe that: (1) Ks((T0, H0), (T00, H00)) is a symmetric function since the set of transformation C are always computed with respect to the pair that has the largest anchor set; (2) in (Haasdonk, 2005), it is shown that when kernel functions are not positive semidefinite, SVMs still solve a data separation problem in pseudo Euclidean spaces. The drawback is that the solution may be only a local optimum. Therefore, we can experiment Eq. 6 with SVMs and observe if the empirical results are satisfactory. Section </context>
</contexts>
<marker>Boughorbel, Tarel, Fleuret, 2004</marker>
<rawString>S. Boughorbel, J-P. Tarel, and F. Fleuret. 2004. Non-mercer kernel for svm object recognition. In Proceedings of BMVC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st NAACL,Seattle,</booktitle>
<location>Washington.</location>
<contexts>
<context position="22692" citStr="Charniak, 2000" startWordPosition="4031" endWordPosition="4032">) challenges, respectively. D1 contains 567 examples whereas T1, D2 and T2 have all the same size, i.e. 800 training/testing instances. The positive examples constitute the 50% of the data. - ALL is the union of D1, D2, and T1, which we also split in 70%-30%. This set is useful to test if we can learn entailments from the data prepared in the two different challenges. - D2(50%)&apos; and D2(50%)&apos;&apos; is a random split of D2. It is possible that the data sets of the two competitions are quite different thus we created this homogeneous split. We also used the following resources: - The Charniak parser (Charniak, 2000) and the morpha lemmatiser (Minnen et al., 2001) to carry out the syntactic and morphological analysis. - WordNet 2.0 (Miller, 1995) to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw,lw,) function. - A selected portion of the British National Corpus2 to compute the </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. of the 1st NAACL,Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gennaro Chierchia</author>
<author>Sally McConnell-Ginet</author>
</authors>
<title>Meaning and Grammar: An introduction to Semantics.</title>
<date>2001</date>
<publisher>MIT press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="978" citStr="Chierchia and McConnell-Ginet, 2001" startWordPosition="139" endWordPosition="142"> measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines (SVMs). This allows us to automatically learn the rewrite rules that describe a non trivial set of entailment cases. The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4% over the state-of-the-art methods. 1 Introduction Recently, textual entailment recognition has been receiving a lot of attention. The main reason is that the understanding of the basic entailment processes will allow us to model more accurate semantic theories of natural languages (Chierchia and McConnell-Ginet, 2001) and design important applications (Dagan and Glickman, 2004), e.g., Question Answering and Information Extraction. However, previous work (e.g., (Zaenen et al., 2005)) suggests that determining whether or not a text T entails a hypothesis H is quite complex even when all the needed information is explicitly asserted. For example, the sentence T1: “At the end of the year, all solid companies pay dividends.” entails the hypothesis H1: “At the end of the year, all solid insurance companies pay dividends.” but it does not entail the hypothesis H2: “At the end of the year, all solid companies pay </context>
</contexts>
<marker>Chierchia, McConnell-Ginet, 2001</marker>
<rawString>Gennaro Chierchia and Sally McConnell-Ginet. 2001. Meaning and Grammar: An introduction to Semantics. MIT press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL02.</booktitle>
<contexts>
<context position="6921" citStr="Collins and Duffy, 2002" startWordPosition="1130" endWordPosition="1133">H0 are structurally similar to T00 and H00, respectively and (2) the lexical relations within the pair (T0, H0) are compatible with those in (T00, H00). Typically, T and H show a certain degree of overlapping, thus, lexical relations (e.g., between the same words) determine word movements from T to H (or vice versa). This is important to model the syntactic/lexical similarity between example pairs. Indeed, if we encode such movements in the syntactic parse trees of texts and hypotheses, we can use interesting similarity measures defined for syntactic parsing, e.g., the tree kernel devised in (Collins and Duffy, 2002). To consider structural and lexical relation similarity, we augment syntactic trees with placeholders which identify linked words. More in detail: - We detect links between words wt in T that are equal, similar, or semantically dependent on words wh in H. We call anchors the pairs (wt, wh) and we associate them with placeholders. For example, in Fig. 1, the placeholder 2” indicates the (companies,companies) anchor between T1 and H1. This allows us to derive the word movements between text and hypothesis. - We align the trees of the two texts T0 and T00 as well as the tree of the two hypothese</context>
<context position="14996" citStr="Collins and Duffy, 2002" startWordPosition="2671" endWordPosition="2674">ic kernels Section 3 has shown that to measure the syntactic similarity between two pairs, (T0, H0) and (T00, H00), we should capture the number of common subtrees between texts and hypotheses that share the same anchoring scheme. The best alignment between anchor sets, i.e. the best substitution cmax, can be found with Eq. 1. As the corresponding maximum quantifies the alignment degree, we could define a cross-pair similarity as follows: Ks ((T&apos;, H&apos;), (T&apos;&apos;, H&apos;&apos;)) = maCx (KT(t(H&apos;, c), t(H&apos;&apos;, i)) cE �+KT(t(T&apos;, c), t(T&apos;&apos;, i) , (6) where as KT (t1, t2) we use the tree kernel function defined in (Collins and Duffy, 2002). This evaluates the number of subtrees shared by t1 and t2, thus defining an implicit substructure space. Formally, given a subtree space F = {f1, f2, ... , f|F|J, the indicator function Ii(n) is equal to 1 if the target fi is rooted at node n and equal to 0 otherwise. A treekernel function over t1 and t2 is KT(t1, t2) = Pn1∈Nt1 Pn2∈Nt2 A(n1, n2), where Nt1 and Nt2 are the sets of the t1’s and t2’s nodes, respectively. In turn A(n1, n2) = P|F| i=1 λl(fi)Ii(n1)Ii(n2), ⎧ ⎨⎪⎪⎪⎪⎪⎪⎪ ⎪⎪⎪⎪⎪⎪⎪⎩ simw(w, w0) = X idf(wh) whEWH (3) 404 where 0 &lt; λ &lt; 1 and l(fi) is the number of levels of the subtree fi. </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of ACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1808" citStr="Corley and Mihalcea, 2005" startWordPosition="273" endWordPosition="276">r not a text T entails a hypothesis H is quite complex even when all the needed information is explicitly asserted. For example, the sentence T1: “At the end of the year, all solid companies pay dividends.” entails the hypothesis H1: “At the end of the year, all solid insurance companies pay dividends.” but it does not entail the hypothesis H2: “At the end of the year, all solid companies pay cash dividends.” Although these implications are uncontroversial, their automatic recognition is complex if we rely on models based on lexical distance (or similarity) between hypothesis and text, e.g., (Corley and Mihalcea, 2005). Indeed, according to such approaches, the hypotheses H1 and H2 are very similar and seem to be similarly related to T1. This suggests that we should study the properties and differences of such two examples (negative and positive) to derive more accurate entailment models. For example, if we consider the following entailment: T3 ⇒ H3? T3 “All wild animals eat plants that have scientifically proven medicinal properties.” H3 “All wild mountain animals eat plants that have scientifically proven medicinal properties.” we note that T3 is structurally (and somehow lexically similar) to T1 and H3 i</context>
<context position="4550" citStr="Corley and Mihalcea, 2005" startWordPosition="730" endWordPosition="733">es the complexity of learning entailments from examples, Sec. 4 describes our models, Sec. 6 shows the experimental results and finally Sec. 7 derives the conclusions. 2 Related work Although the textual entailment recognition problem is not new, most of the automatic approaches have been proposed only recently. This has been mainly due to the RTE challenge events (Dagan et al., 2005; Bar Haim et al., 2006). In the following we report some of such researches. A first class of methods defines measures of the distance or similarity between T and H either assuming the independence between words (Corley and Mihalcea, 2005; Glickman et al., 2005) in a bag-of-word fashion or exploiting syntactic interpretations (Kouylekov and Magnini, 2005). A pair (T, H) is then in entailment when sim(T, H) &gt; α. These approaches can hardly determine whether the entailment holds in the examples of the previous section. From the point of view of bag-of-word methods, the pairs (T1, H1) and (T1, H2) have both the same intra-pair similarity since the sentences of T1 and H1 as well as those of T1 and H2 differ by a noun, insurance and cash, respectively. At syntactic level, also, we cannot capture the required information as such nou</context>
<context position="11639" citStr="Corley and Mihalcea, 2005" startWordPosition="2068" endWordPosition="2072"> In this section we describe how anchors are found at the level of a single pair (T, H) (Sec. 4.1). The anchoring process gives the direct possibility of implementing an inter-pair similarity that can be used as a baseline approach or in combination with the cross-pair similarity. This latter will be implemented with tree kernel functions over syntactic structures (Sec. 4.2). 4.1 Anchoring and Lexical Similarity The algorithm that we design to find the anchors is based on similarity functions between words or more complex expressions. Our approach is in line with many other researches (e.g., (Corley and Mihalcea, 2005; Glickman et al., 2005)). Given the set of content words (verbs, nouns, adjectives, and adverbs) WT and WH of the two sentences T and H, respectively, the set of anchors A C WT x WH is built using a similarity measure between two words simw(wt, wh). Each element wh E WH will be part of a pair (wt, wh) E A if: 1) simw(wt, wh) =� 0 2) simw(wt, wh) = maxw&apos; t∈WT simw(w0t, wh) According to these properties, elements in WH can participate in more than one anchor and conversely more than one element in WH can be linked to a single element w E WT. The similarity simw(wt, wh) can be defined using diff</context>
<context position="13695" citStr="Corley and Mihalcea, 2005" startWordPosition="2452" endWordPosition="2455"> w0V lw = lw0 h cw = c 0V ((lw, cw), (lw0 , cwl)) E EntV ((lw, cw), (lw0 , cw0)) E DerV lev(w, w0) = 1 d(lw, lw0 ) ifcw = cw0 h d(lw, lw0) &gt; 0.2 0 otherwise (2) It is worth noticing that, the above measure is not a pure similarity measure as it includes the entailment relation that does not represent synonymy or similarity between verbs. To emphasize the contribution of each used resource, in the experimental section, we will compare Eq. 2 with some versions that exclude some word relations. The above word similarity measure can be used to compute the similarity between T and H. In line with (Corley and Mihalcea, 2005), we define it as: X simw(wt, wh) x idf(wh) s1(T,H) = (wt,wh)EA where idf(w) is the inverse document frequency of the word w. For sake of comparison, we consider also the corresponding more classical version that does not apply the inverse document frequency X s2(T, H) = simw(wt,wh)/|WH |(4) (wt,wh)EA ¿From the above intra-pair similarities, s1 and s2, we can obtain the baseline cross-pair similarities based on only lexical information: Ki((T&apos;, H&apos;), (T&apos;&apos;, H&apos;&apos;)) = si(T&apos;, H&apos;) x si(T&apos;&apos;, H&apos;&apos;), (5) where i E {1, 2J. In the next section we define a novel cross-pair similarity that takes into account</context>
<context position="23078" citStr="Corley and Mihalcea, 2005" startWordPosition="4091" endWordPosition="4094">(50%)&apos; and D2(50%)&apos;&apos; is a random split of D2. It is possible that the data sets of the two competitions are quite different thus we created this homogeneous split. We also used the following resources: - The Charniak parser (Charniak, 2000) and the morpha lemmatiser (Minnen et al., 2001) to carry out the syntactic and morphological analysis. - WordNet 2.0 (Miller, 1995) to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw,lw,) function. - A selected portion of the British National Corpus2 to compute the inverse document frequency (idf). We assigned the maximum idf to words not found in the BNC. - SVM-light-TK3 (Moschitti, 2006) which encodes the basic tree kernel function, KT, in SVMlight (Joachims, 1999). We used such software to implement K3 (Eq. 6), K1, K2 (Eq. 5) and K3 + Ki kernels. The latter combines our new kernel with traditional approaches (i E {1, 2}). 6.2 Results and ana</context>
<context position="26804" citStr="Corley and Mihalcea, 2005" startWordPosition="4720" endWordPosition="4723">e random baseline, i.e. 50%. In all the datasets (except for the first one), the sim,,,(T, H) similarity based on the lexical overlap (first column) provides an accuracy essentially similar to the best lexical-based distance method. - Second, the dataset “Train:D1-Test:T 1” allows us to compare our models with the ones of the first RTE challenge (Dagan et al., 2005). The accuracy reported for the best systems, i.e. 58.6% (Glickman et al., 2005; Bayer et al., 2005), is not significantly different from the result obtained with K1 that uses the idf. - Third, the dramatic improvement observed in (Corley and Mihalcea, 2005) on the dataset “Train:D1-Test:T 1” is given by the idf rather than the use of the J&amp;C similarity (second vs. third columns). The use of J&amp;C with the idf decreases the accuracy of the idf alone. - Next, our approach (last column) is significantly better than all the other methods as it provides the best result for each combination of training and test sets. On the “Train:D1-Test:T1” test set, it exceeds the accuracy of the current state-of-theart models (Glickman et al., 2005; Bayer et al., 2005) by about 4.4 absolute percent points (63% vs. 58.6%) and 4% over our best lexical similarity measu</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
</authors>
<title>Probabilistic textual entailment: Generic applied modeling of language variability.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Learning Methods for Text Understanding and Mining,</booktitle>
<location>Grenoble, France.</location>
<contexts>
<context position="1039" citStr="Dagan and Glickman, 2004" startWordPosition="147" endWordPosition="150">el function in Support Vector Machines (SVMs). This allows us to automatically learn the rewrite rules that describe a non trivial set of entailment cases. The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4% over the state-of-the-art methods. 1 Introduction Recently, textual entailment recognition has been receiving a lot of attention. The main reason is that the understanding of the basic entailment processes will allow us to model more accurate semantic theories of natural languages (Chierchia and McConnell-Ginet, 2001) and design important applications (Dagan and Glickman, 2004), e.g., Question Answering and Information Extraction. However, previous work (e.g., (Zaenen et al., 2005)) suggests that determining whether or not a text T entails a hypothesis H is quite complex even when all the needed information is explicitly asserted. For example, the sentence T1: “At the end of the year, all solid companies pay dividends.” entails the hypothesis H1: “At the end of the year, all solid insurance companies pay dividends.” but it does not entail the hypothesis H2: “At the end of the year, all solid companies pay cash dividends.” Although these implications are uncontrovers</context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>Ido Dagan and Oren Glickman. 2004. Probabilistic textual entailment: Generic applied modeling of language variability. In Proceedings of the Workshop on Learning Methods for Text Understanding and Mining, Grenoble, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<date>2005</date>
<booktitle>The PASCAL RTE challenge. In RTE Workshop,</booktitle>
<location>Southampton, U.K.</location>
<contexts>
<context position="3261" citStr="Dagan et al., 2005" startWordPosition="518" endWordPosition="521">ween two pairs (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;). The latter similarity measure along with a set of annotated examples allows a learning algorithm to automatically derive syntactic and lexical rules that can solve complex entailment cases. In this paper, we define a new cross-pair similarity measure based on text and hypothesis syntactic trees and we use such similarity with traditional intra-pair similarities to define a novel semantic kernel function. We experimented with such kernel using Support Vector Machines (Vapnik, 1995) on the test tests of the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2005; Bar Haim et al., 2006). The comparative results show that (a) we have designed an effective way to automatically learn entailment rules from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-of-the-art 401 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 401–408, Sydney, July 2006. c�2006 Association for Computational Linguistics models (Glickman et al., 2005; Bayer et al., 2005) by about 4.4% (i.e. 63% vs. 58.6%) on the RTE 1 test set (Dagan et al., 2005). In the remainder of </context>
<context position="22043" citStr="Dagan et al., 2005" startWordPosition="3911" endWordPosition="3914">fore the computation of the kernel function. 6 Experimental investigation The aim of the experiments is twofold: we show that (a) entailment recognition rules can be learned from examples and (b) our kernel functions over syntactic structures are effective to derive syntactic properties. The above goals can be achieved by comparing the different intra and cross pair similarity measures. 6.1 Experimental settings For the experiments, we used the Recognizing Textual Entailment Challenge data sets, which we name as follows: - D1, T1 and D2, T2, are the development and the test sets of the first (Dagan et al., 2005) and second (Bar Haim et al., 2006) challenges, respectively. D1 contains 567 examples whereas T1, D2 and T2 have all the same size, i.e. 800 training/testing instances. The positive examples constitute the 50% of the data. - ALL is the union of D1, D2, and T1, which we also split in 70%-30%. This set is useful to test if we can learn entailments from the data prepared in the two different challenges. - D2(50%)&apos; and D2(50%)&apos;&apos; is a random split of D2. It is possible that the data sets of the two competitions are quite different thus we created this homogeneous split. We also used the following </context>
<context position="26546" citStr="Dagan et al., 2005" startWordPosition="4676" endWordPosition="4679">y on ALL dataset split in 70/30% and on the whole ALL dataset used for training and T2 for testing. ¿From the table we note the following aspects: - First, the lexical-based distance kernels K1 and K2 (Eq. 5) show accuracy significantly higher than the random baseline, i.e. 50%. In all the datasets (except for the first one), the sim,,,(T, H) similarity based on the lexical overlap (first column) provides an accuracy essentially similar to the best lexical-based distance method. - Second, the dataset “Train:D1-Test:T 1” allows us to compare our models with the ones of the first RTE challenge (Dagan et al., 2005). The accuracy reported for the best systems, i.e. 58.6% (Glickman et al., 2005; Bayer et al., 2005), is not significantly different from the result obtained with K1 that uses the idf. - Third, the dramatic improvement observed in (Corley and Mihalcea, 2005) on the dataset “Train:D1-Test:T 1” is given by the idf rather than the use of the J&amp;C similarity (second vs. third columns). The use of J&amp;C with the idf decreases the accuracy of the idf alone. - Next, our approach (last column) is significantly better than all the other methods as it provides the best result for each combination of traini</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL RTE challenge. In RTE Workshop, Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodrigo de Salvo Braz</author>
<author>Roxana Girju</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
</authors>
<title>An inference model for semantic entailment in natural language.</title>
<date>2005</date>
<booktitle>In Proc. of the RTE Workshop,</booktitle>
<location>Southampton, U.K.</location>
<contexts>
<context position="5696" citStr="Braz et al., 2005" startWordPosition="928" endWordPosition="931">ctic level, also, we cannot capture the required information as such nouns are both noun modifiers: insurance modifies companies and cash modifies dividends. A second class of methods can give a solution to the previous problem. These methods generally combine a similarity measure with a set of possible transformations T applied over syntactic and semantic interpretations. The entailment between T and H is detected when there is a transformation r E T so that sim(r(T), H) &gt; α. These transformations are logical rules in (Bos and Markert, 2005) or sequences of allowed rewrite rules in (de Salvo Braz et al., 2005). The disadvantage is that such rules have to be manually designed. Moreover, they generally model better positive implications than negative ones and they do not consider errors in syntactic parsing and semantic analysis. 3 Challenges in learning from examples In the introductory section, we have shown that, to carry out automatic learning from examples, we need to define a cross-pair similarity measure. Its definition is not straightforward as it should detect whether two pairs (T0, H0) and (T00, H00) realize the same rewrite rules. This measure should consider pairs similar when: (1) T0 and</context>
</contexts>
<marker>Braz, Girju, Punyakanok, Roth, Sammons, 2005</marker>
<rawString>Rodrigo de Salvo Braz, Roxana Girju, Vasin Punyakanok, Dan Roth, and Mark Sammons. 2005. An inference model for semantic entailment in natural language. In Proc. of the RTE Workshop, Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>Web based probabilistic textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st RTE Workshop,</booktitle>
<location>Southampton, UK.</location>
<contexts>
<context position="3740" citStr="Glickman et al., 2005" startWordPosition="590" endWordPosition="593">rnel using Support Vector Machines (Vapnik, 1995) on the test tests of the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2005; Bar Haim et al., 2006). The comparative results show that (a) we have designed an effective way to automatically learn entailment rules from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-of-the-art 401 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 401–408, Sydney, July 2006. c�2006 Association for Computational Linguistics models (Glickman et al., 2005; Bayer et al., 2005) by about 4.4% (i.e. 63% vs. 58.6%) on the RTE 1 test set (Dagan et al., 2005). In the remainder of this paper, Sec. 2 illustrates the related work, Sec. 3 introduces the complexity of learning entailments from examples, Sec. 4 describes our models, Sec. 6 shows the experimental results and finally Sec. 7 derives the conclusions. 2 Related work Although the textual entailment recognition problem is not new, most of the automatic approaches have been proposed only recently. This has been mainly due to the RTE challenge events (Dagan et al., 2005; Bar Haim et al., 2006). In </context>
<context position="11663" citStr="Glickman et al., 2005" startWordPosition="2073" endWordPosition="2076">e how anchors are found at the level of a single pair (T, H) (Sec. 4.1). The anchoring process gives the direct possibility of implementing an inter-pair similarity that can be used as a baseline approach or in combination with the cross-pair similarity. This latter will be implemented with tree kernel functions over syntactic structures (Sec. 4.2). 4.1 Anchoring and Lexical Similarity The algorithm that we design to find the anchors is based on similarity functions between words or more complex expressions. Our approach is in line with many other researches (e.g., (Corley and Mihalcea, 2005; Glickman et al., 2005)). Given the set of content words (verbs, nouns, adjectives, and adverbs) WT and WH of the two sentences T and H, respectively, the set of anchors A C WT x WH is built using a similarity measure between two words simw(wt, wh). Each element wh E WH will be part of a pair (wt, wh) E A if: 1) simw(wt, wh) =� 0 2) simw(wt, wh) = maxw&apos; t∈WT simw(w0t, wh) According to these properties, elements in WH can participate in more than one anchor and conversely more than one element in WH can be linked to a single element w E WT. The similarity simw(wt, wh) can be defined using different indicators and res</context>
<context position="26625" citStr="Glickman et al., 2005" startWordPosition="4689" endWordPosition="4693">ng and T2 for testing. ¿From the table we note the following aspects: - First, the lexical-based distance kernels K1 and K2 (Eq. 5) show accuracy significantly higher than the random baseline, i.e. 50%. In all the datasets (except for the first one), the sim,,,(T, H) similarity based on the lexical overlap (first column) provides an accuracy essentially similar to the best lexical-based distance method. - Second, the dataset “Train:D1-Test:T 1” allows us to compare our models with the ones of the first RTE challenge (Dagan et al., 2005). The accuracy reported for the best systems, i.e. 58.6% (Glickman et al., 2005; Bayer et al., 2005), is not significantly different from the result obtained with K1 that uses the idf. - Third, the dramatic improvement observed in (Corley and Mihalcea, 2005) on the dataset “Train:D1-Test:T 1” is given by the idf rather than the use of the J&amp;C similarity (second vs. third columns). The use of J&amp;C with the idf decreases the accuracy of the idf alone. - Next, our approach (last column) is significantly better than all the other methods as it provides the best result for each combination of training and test sets. On the “Train:D1-Test:T1” test set, it exceeds the accuracy o</context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web based probabilistic textual entailment. In Proceedings of the 1st RTE Workshop, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Haasdonk</author>
</authors>
<title>Feature space interpretation of SVMs with indefinite kernels.</title>
<date>2005</date>
<journal>IEEE Trans Pattern Anal Mach Intell.</journal>
<contexts>
<context position="16524" citStr="Haasdonk, 2005" startWordPosition="2958" endWordPosition="2959">sitivesemidefinite. Some basic operations on kernel functions, e.g. the sum, are closed with respect to the set of valid kernels. Thus, if the maximum held such property, Eq. 6 would be a valid kernel and we could use it in kernel based machines like SVMs. Unfortunately, a counterexample illustrated in (Boughorbel et al., 2004) shows that the max function does not produce valid kernels in general. However, we observe that: (1) Ks((T0, H0), (T00, H00)) is a symmetric function since the set of transformation C are always computed with respect to the pair that has the largest anchor set; (2) in (Haasdonk, 2005), it is shown that when kernel functions are not positive semidefinite, SVMs still solve a data separation problem in pseudo Euclidean spaces. The drawback is that the solution may be only a local optimum. Therefore, we can experiment Eq. 6 with SVMs and observe if the empirical results are satisfactory. Section 6 shows that the solutions found by Eq. 6 produce accuracy higher than those evaluated on previous automatic textual entailment recognition approaches. 5 Refining cross-pair syntactic similarity In the previous section we have defined the intra and the cross pair similarity. The former</context>
</contexts>
<marker>Haasdonk, 2005</marker>
<rawString>Bernard Haasdonk. 2005. Feature space interpretation of SVMs with indefinite kernels. IEEE Trans Pattern Anal Mach Intell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the 15th CoLing,</booktitle>
<location>Nantes, France.</location>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of the 15th CoLing, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the 10th ROCLING,</booktitle>
<location>Tapei, Taiwan.</location>
<contexts>
<context position="23044" citStr="Jiang and Conrath, 1997" startWordPosition="4085" endWordPosition="4088">e two different challenges. - D2(50%)&apos; and D2(50%)&apos;&apos; is a random split of D2. It is possible that the data sets of the two competitions are quite different thus we created this homogeneous split. We also used the following resources: - The Charniak parser (Charniak, 2000) and the morpha lemmatiser (Minnen et al., 2001) to carry out the syntactic and morphological analysis. - WordNet 2.0 (Miller, 1995) to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw,lw,) function. - A selected portion of the British National Corpus2 to compute the inverse document frequency (idf). We assigned the maximum idf to words not found in the BNC. - SVM-light-TK3 (Moschitti, 2006) which encodes the basic tree kernel function, KT, in SVMlight (Joachims, 1999). We used such software to implement K3 (Eq. 6), K1, K2 (Eq. 5) and K3 + Ki kernels. The latter combines our new kernel with traditional approaches</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the 10th ROCLING, Tapei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods-Support Vector Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23497" citStr="Joachims, 1999" startWordPosition="4167" endWordPosition="4168"> derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw,lw,) function. - A selected portion of the British National Corpus2 to compute the inverse document frequency (idf). We assigned the maximum idf to words not found in the BNC. - SVM-light-TK3 (Moschitti, 2006) which encodes the basic tree kernel function, KT, in SVMlight (Joachims, 1999). We used such software to implement K3 (Eq. 6), K1, K2 (Eq. 5) and K3 + Ki kernels. The latter combines our new kernel with traditional approaches (i E {1, 2}). 6.2 Results and analysis Table 1 reports the results of different similarity kernels on the different training and test splits described in the previous section. The table is organized as follows: The first 5 rows (Experiment settings) report the intra-pair similarity measures defined in Section 4.1, the 6th row refers to only the idf similarity metric whereas the following two rows report the cross-pair similarity carried out with Eq</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale svm learning practical. In Advances in Kernel Methods-Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Bernardo Magnini</author>
</authors>
<title>Tree edit distance for textual entailment.</title>
<date>2005</date>
<booktitle>In Proc. of the RANLP2005,</booktitle>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="4669" citStr="Kouylekov and Magnini, 2005" startWordPosition="747" endWordPosition="750">results and finally Sec. 7 derives the conclusions. 2 Related work Although the textual entailment recognition problem is not new, most of the automatic approaches have been proposed only recently. This has been mainly due to the RTE challenge events (Dagan et al., 2005; Bar Haim et al., 2006). In the following we report some of such researches. A first class of methods defines measures of the distance or similarity between T and H either assuming the independence between words (Corley and Mihalcea, 2005; Glickman et al., 2005) in a bag-of-word fashion or exploiting syntactic interpretations (Kouylekov and Magnini, 2005). A pair (T, H) is then in entailment when sim(T, H) &gt; α. These approaches can hardly determine whether the entailment holds in the examples of the previous section. From the point of view of bag-of-word methods, the pairs (T1, H1) and (T1, H2) have both the same intra-pair similarity since the sentences of T1 and H1 as well as those of T1 and H2 differ by a noun, insurance and cash, respectively. At syntactic level, also, we cannot capture the required information as such nouns are both noun modifiers: insurance modifies companies and cash modifies dividends. A second class of methods can giv</context>
</contexts>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>Milen Kouylekov and Bernardo Magnini. 2005. Tree edit distance for textual entailment. In Proc. of the RANLP2005, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<contexts>
<context position="12414" citStr="Miller, 1995" startWordPosition="2220" endWordPosition="2221"> anchors A C WT x WH is built using a similarity measure between two words simw(wt, wh). Each element wh E WH will be part of a pair (wt, wh) E A if: 1) simw(wt, wh) =� 0 2) simw(wt, wh) = maxw&apos; t∈WT simw(w0t, wh) According to these properties, elements in WH can participate in more than one anchor and conversely more than one element in WH can be linked to a single element w E WT. The similarity simw(wt, wh) can be defined using different indicators and resources. First of all, two words are maximally similar if these have the same surface form wt = wh. Second, we can use one of the WordNet (Miller, 1995) similarities indicated with d(lw, lw&apos;) (in line with what was done in (Corley and Mihalcea, 2005)) and different relation between words such as the lexical entailment between verbs (Ent) and derivationally relation between words (Der). Finally, we use the edit distance measure lev(wt, wh) to capture the similarity between words that are missed by the previous analysis for misspelling errors or for the lack of derivationally forms not coded in WordNet. As result, given the syntactic category cw E {noun, verb, adjective, adverbJ and the lemmatized form lw of a word w, the similarity measure bet</context>
<context position="22824" citStr="Miller, 1995" startWordPosition="4052" endWordPosition="4053">. The positive examples constitute the 50% of the data. - ALL is the union of D1, D2, and T1, which we also split in 70%-30%. This set is useful to test if we can learn entailments from the data prepared in the two different challenges. - D2(50%)&apos; and D2(50%)&apos;&apos; is a random split of D2. It is possible that the data sets of the two competitions are quite different thus we created this homogeneous split. We also used the following resources: - The Charniak parser (Charniak, 2000) and the morpha lemmatiser (Minnen et al., 2001) to carry out the syntactic and morphological analysis. - WordNet 2.0 (Miller, 1995) to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw,lw,) function. - A selected portion of the British National Corpus2 to compute the inverse document frequency (idf). We assigned the maximum idf to words not found in the BNC. - SVM-light-TK3 (Moschitti, 2006) which</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English. Natural Language Engineering.</title>
<date>2001</date>
<contexts>
<context position="22740" citStr="Minnen et al., 2001" startWordPosition="4037" endWordPosition="4040"> examples whereas T1, D2 and T2 have all the same size, i.e. 800 training/testing instances. The positive examples constitute the 50% of the data. - ALL is the union of D1, D2, and T1, which we also split in 70%-30%. This set is useful to test if we can learn entailments from the data prepared in the two different challenges. - D2(50%)&apos; and D2(50%)&apos;&apos; is a random split of D2. It is possible that the data sets of the two competitions are quite different thus we created this homogeneous split. We also used the following resources: - The Charniak parser (Charniak, 2000) and the morpha lemmatiser (Minnen et al., 2001) to carry out the syntactic and morphological analysis. - WordNet 2.0 (Miller, 1995) to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw,lw,) function. - A selected portion of the British National Corpus2 to compute the inverse document frequency (idf). We assigned th</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In Proceedings ofEACL’06,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="23418" citStr="Moschitti, 2006" startWordPosition="4153" endWordPosition="4154">Net 2.0 (Miller, 1995) to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw,lw,) function. - A selected portion of the British National Corpus2 to compute the inverse document frequency (idf). We assigned the maximum idf to words not found in the BNC. - SVM-light-TK3 (Moschitti, 2006) which encodes the basic tree kernel function, KT, in SVMlight (Joachims, 1999). We used such software to implement K3 (Eq. 6), K1, K2 (Eq. 5) and K3 + Ki kernels. The latter combines our new kernel with traditional approaches (i E {1, 2}). 6.2 Results and analysis Table 1 reports the results of different similarity kernels on the different training and test splits described in the previous section. The table is organized as follows: The first 5 rows (Experiment settings) report the intra-pair similarity measures defined in Section 4.1, the 6th row refers to only the idf similarity metric wher</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings ofEACL’06, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proc. of 5th NAACL,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="22974" citStr="Pedersen et al., 2004" startWordPosition="4075" endWordPosition="4078">ful to test if we can learn entailments from the data prepared in the two different challenges. - D2(50%)&apos; and D2(50%)&apos;&apos; is a random split of D2. It is possible that the data sets of the two competitions are quite different thus we created this homogeneous split. We also used the following resources: - The Charniak parser (Charniak, 2000) and the morpha lemmatiser (Minnen et al., 2001) to carry out the syntactic and morphological analysis. - WordNet 2.0 (Miller, 1995) to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw,lw,) function. - A selected portion of the British National Corpus2 to compute the inverse document frequency (idf). We assigned the maximum idf to words not found in the BNC. - SVM-light-TK3 (Moschitti, 2006) which encodes the basic tree kernel function, KT, in SVMlight (Joachims, 1999). We used such software to implement K3 (Eq. 6), K1, K2 (Eq. 5) and K3 + Ki k</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity - measuring the relatedness of concepts. In Proc. of 5th NAACL, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="3168" citStr="Vapnik, 1995" startWordPosition="504" endWordPosition="505"> only on a intra-pair similarity between T and H but also on a cross-pair similarity between two pairs (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;). The latter similarity measure along with a set of annotated examples allows a learning algorithm to automatically derive syntactic and lexical rules that can solve complex entailment cases. In this paper, we define a new cross-pair similarity measure based on text and hypothesis syntactic trees and we use such similarity with traditional intra-pair similarities to define a novel semantic kernel function. We experimented with such kernel using Support Vector Machines (Vapnik, 1995) on the test tests of the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2005; Bar Haim et al., 2006). The comparative results show that (a) we have designed an effective way to automatically learn entailment rules from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-of-the-art 401 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 401–408, Sydney, July 2006. c�2006 Association for Computational Linguistics models (Glickman et al., 2005; Bayer et al., 2005) by abo</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Zaenen</author>
<author>Lauri Karttunen</author>
<author>Richard Crouch</author>
</authors>
<title>Local textual inference: Can it be defined or circumscribed?</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1145" citStr="Zaenen et al., 2005" startWordPosition="161" endWordPosition="164">cribe a non trivial set of entailment cases. The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4% over the state-of-the-art methods. 1 Introduction Recently, textual entailment recognition has been receiving a lot of attention. The main reason is that the understanding of the basic entailment processes will allow us to model more accurate semantic theories of natural languages (Chierchia and McConnell-Ginet, 2001) and design important applications (Dagan and Glickman, 2004), e.g., Question Answering and Information Extraction. However, previous work (e.g., (Zaenen et al., 2005)) suggests that determining whether or not a text T entails a hypothesis H is quite complex even when all the needed information is explicitly asserted. For example, the sentence T1: “At the end of the year, all solid companies pay dividends.” entails the hypothesis H1: “At the end of the year, all solid insurance companies pay dividends.” but it does not entail the hypothesis H2: “At the end of the year, all solid companies pay cash dividends.” Although these implications are uncontroversial, their automatic recognition is complex if we rely on models based on lexical distance (or similarity)</context>
</contexts>
<marker>Zaenen, Karttunen, Crouch, 2005</marker>
<rawString>Annie Zaenen, Lauri Karttunen, and Richard Crouch. 2005. Local textual inference: Can it be defined or circumscribed? In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, Ann Arbor, Michigan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>