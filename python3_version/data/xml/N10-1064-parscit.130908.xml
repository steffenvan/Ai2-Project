<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000300">
<title confidence="0.986165">
Statistical Machine Translation of Texts with Misspelled Words
</title>
<author confidence="0.723179">
Nicola Bertoldi Mauro Cettolo Marcello Federico
</author>
<address confidence="0.4126875">
FBK - Fondazione Bruno Kessler
via Sommarive 18 - 38123 Povo, Trento, Italy
</address>
<email confidence="0.996485">
{bertoldi,cettolo,federico}@fbk.eu
</email>
<sectionHeader confidence="0.998579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99957">
This paper investigates the impact of mis-
spelled words in statistical machine transla-
tion and proposes an extension of the transla-
tion engine for handling misspellings. The en-
hanced system decodes a word-based confu-
sion network representing spelling variations
of the input text.
We present extensive experimental results on
two translation tasks of increasing complex-
ity which show how misspellings of different
types do affect performance of a statistical ma-
chine translation decoder and to what extent
our enhanced system is able to recover from
such errors.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999757960784314">
With the widespread adoption of the Internet, of
modern communication, multimedia and mobile de-
vice technologies, the amount of multilingual in-
formation distributed and available to anyone, any-
where, has exploded. So called social media have
rapidly reshaped information exchange among Inter-
net users, providing new means of communication
(blogs, tweets, etc.), collaboration (e.g. wikis), and
sharing of multimedia content, and entertainment.
In particular, social media have today become also
an important market for advertisement as well as a
global forum for consumer opinions (Kushal et al.,
2003).
The growing spread of user-generated content is
scaling-up the potential demand for on-line machine
translation (MT) but also setting new challenges to
the field of natural language processing (NLP) in
general. The language written and spoken in the
social media presents an impressive variety of con-
tent and styles (Schler et al., 2006), and writing con-
ventions that rapidly evolve over time. Moreover,
much of the content is expressed in informal style,
that more or less violates the standard grammar, con-
tains many abbreviations and acronyms, and finally
many misspelled words. From the point of view of
MT, language of social media is hence very different
from the one represented in the text corpora nowa-
days available to train statistical MT systems.
Facing all these challenges, we pragmatically
scaled down our ambition and decided to investigate
a basic, somehow preliminary, well defined prob-
lem: the impact of misspelled words in statistical
MT. Unintentional typing errors are indeed remark-
ably frequent in online chats, blogs, wikis, reviews,
and hence constitute a major source of noise (Subra-
maniam et al., 2009).
In this paper we aim at studying performance
degradation of statistical MT under different levels
and kinds of noise, and at analyzing to what extent
statistical MT is able to recover from errors by en-
riching its input with spelling variations.
After a brief overview of NLP literature related
to noisy texts, in Section 3 we consider different
types of misspellings and derive simple but realistic
models that are able to reproduce them. Such mod-
els are then used to generate errors in texts passed
to a phrase-based statistical MT system. Next, in
Section 4 we introduce an extension of a statistical
MT system able to handle misspellings by exploiting
confusion network decoding (Bertoldi et al., 2008).
Experiments are reported in Section 5 that in-
</bodyText>
<page confidence="0.971103">
412
</page>
<note confidence="0.7533265">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 412–419,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996675">
vestigate the trade-off between complexity of the
extended MT decoder versus translation accuracy.
Moreover, as the proposed model for handling mis-
spellings embeds specific assumptions on how er-
rors are generated, we also measure the robustness
of the enhanced MT decoder with respect to differ-
ent noise sources. Experiments are reported on two
tasks of different complexity, the translation of Eu-
roparl texts and weather bulletins, involving English
and Italian languages.
</bodyText>
<sectionHeader confidence="0.998448" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999622">
Most contributions addressing NLP of noisy user-
generated content are from the text mining commu-
nity. A survey about the different types of noise that
might affect text mining is in (Subramaniam et al.,
2009), while an analysis of how noise phenomena,
commonly occurring in blogs, affect an opinion min-
ing application is in (Dey and Haque, 2009).
Concerning spelling correction literature, many
works apply the noisy channel model which con-
sists of two components: a source model (prior
of word probabilities) and a channel (error) model,
that accounts for spelling transformations on let-
ter sequences. Several approaches have been
proposed under this framework, that mainly dif-
fer in the employed error model; see for exam-
ple: (Church and Gale, 1991), (Brill and Moore,
2000) and (Toutanova and Moore, 2002).
Comprehensive surveys on methods to model and
recover spelling errors can be found in (Kukich,
1992) and (Pedler, 2007); in particular, the latter
work is specifically centered on methods for cor-
recting so-called real-word errors (cf. Section 3).
The detection of errors and the suggestion of cor-
rections typically rely on the availability of text cor-
pora or human-made lexical resources. Search for
correct alternatives can be based on word similarity
measures, such as the edit distance (Mitton, 1995),
anagram hashing (Reynaert, 2006), and semantic
distance based on WordNet (Hirst and Budanitsky,
2005). More sophisticated approaches have been
proposed by (Fossati and Di Eugenio, 2008), that
mixes surface and Part-Of-Speech Information, and
(Schaback and Li, 2007), which combines similarity
measures at the character, phonetic, word, syntax,
and semantic levels into one global feature-based
framework.
</bodyText>
<figure confidence="0.937471375">
a) *W *w had just come in from Australia [Australia]
b) good service we *staid one week. [Tahiti]
c) The room was *exellent but the hallway was *filty .
[NJ]
d) is a good place to stay, if you are looking for a hotel
*arround LAX airport. [Tahiti]
e) The staff was *freindly ... I was *conerned about
the noise [CT]
</figure>
<tableCaption confidence="0.9908055">
Table 1: Examples of misspellings found in on-line re-
views of an hotel close to Los Angeles Int’l Airport. Cor-
responding corrections are: a) We, e, b) stayed, c) excel-
lent, filthy, d) around, e) friendly, concerned.
</tableCaption>
<bodyText confidence="0.999875647058824">
Concerning the literature of statistical MT, inter-
est in noisy data has been so far considering is-
sues different from misspelled words. For instance,
(Davis et al., 1995) and (Vogel, 2003) address train-
ing methods coping with noisy parallel data, in the
sense that translations do not perfectly match. Work
on speech translation (Casacuberta et al., 2008) fo-
cused instead on efficient methods to couple speech
recognition and MT in order to avoid error propaga-
tion. Very recently, (Carrera et al., 2009) conducted
a qualitative study on the impact of noisy social me-
dia content on statistical and rule-based MT. Unfor-
tunately, this work does not report any quantitative
result, it is only based on a small selection of exam-
ples that are manually evaluated, and finally it does
not address the problem of integrating error correc-
tion with MT.
</bodyText>
<sectionHeader confidence="0.922597" genericHeader="method">
3 Types of Misspellings
</sectionHeader>
<bodyText confidence="0.9997086">
In general, a misspelled word is a sequence of let-
ters that corresponds to no correctly spelled word of
the same language (non-word error), or to a correct
spelling of another word (real-word error). In the
examples shown in Table 1, all marked errors are
non-word errors, but the one in sentence b), which
indeed is likely a misspelling of the word stayed.
Causes of a misspelling may be an unintentional
typing error (e.g. *freindly for friendly), or lack of
knowledge about the proper spelling. Typing errors
can originate from six different typing operations
(Kukich, 1992): substitution, insertion, deletion,
transposition, run-on, and split.1 Lack of knowledge
could be the cause of the misspelled *exellent in sen-
tence c).
</bodyText>
<footnote confidence="0.8880775">
1 Run-on and split are the special cases of deleting and in-
serting blank spaces, respectively.
</footnote>
<page confidence="0.993483">
413
</page>
<listItem confidence="0.9985409">
1. your - you’re
2. then - than
3. its - it’s
4. to - too - two
5. were - where - we’re
6 there - their - they’re
Z a - an - and
8. off - of
9. here - hear
10. lose - loose
</listItem>
<tableCaption confidence="0.9953565">
Table 2: List of frequent real-word errors found in blogs.
Source: http://www.theprobabilist.com.
</tableCaption>
<bodyText confidence="0.999822625">
An interesting combination of cause and effect is
when lack of linguistic competence results in con-
fusing the spelling of a word with the spelling of
another word that sounds similarly (Hirst and Bu-
danitsky, 2005). This could be likely the case of the
Polynesian tourist that authored sentence b).
A short list of words frequently confused in blogs
is reported in Table 2 while a longer list can be found
in the Wikipedia.2 Real-word errors typically fool
spell checkers because their identification requires
analyzing the context in which they occur.
In this paper, we automatically corrupt clean text
with three types of noise described below. This pro-
cedure permits us to analyze the MT performance
against different sources and levels of noise and to
systematically evaluate our error-recovering strat-
egy.
Non-word Noise We randomly replace words in
the text according to a list of 4,100 frequently non-
word errors provided in the Wikipedia. A qualitative
analysis of these errors reveals that all of them origi-
nate by one or two keyboard typing errors of the kind
described beforehand. Practically, non-word noise is
introduced by defining a desired level of corruption
of the source text.
Real-word Noise Similarly to the previous case,
real-word errors are automatically introduced by
another list of frequently misused words in the
Wikipedia. This list contains about 300 pairs of con-
fusable words to which we also added the 10 fre-
quent real-word errors occurring in blogs reported
in Table 2.
</bodyText>
<footnote confidence="0.943353">
2See Wikipedia’s “list of frequently misused English
words”.
</footnote>
<bodyText confidence="0.951521333333333">
Random Noise Finally, we may corrupt the origi-
nal text by randomly replacing, inserting, and delet-
ing characters in it up to a desired percentage.
</bodyText>
<sectionHeader confidence="0.917549" genericHeader="method">
4 Error-recovering Statistical NIT
</sectionHeader>
<bodyText confidence="0.999971">
An enhancement of a statistical MT system is pro-
posed with the goal of improving robustness to mis-
spellings in the input. Rrror recovery is realized
by performing a sequence of actions before the ac-
tual translation, which create reliable spelling alter-
natives of the input and store them into a compact
word-based Confusion Network (CN).
Starting from the possibly noisy input text,
spelling variations are generated by assuming that
each character is a potential typing error, indepen-
dent from other characters.
The variants are represented as a character-based
CN that models possible substitutions, insertion,
deletions of each character, with an empirically de-
termined weight assigned to each alternative. The
network is then searched by a non-monotonic search
process that scores possible character sequences
through a character n-gram language model, and
outputs a set of multiple spelling variants that is fi-
nally converted into a word-based CN. The result-
ing word-based network is finally passed to the MT
engine. In the following, more details are provided
on the augmented MT system with the help of Fig-
ure 1, which shows how the system acts on the cor-
rupted example “all off ame”, supposed to be “hall
of fame”.
Step 1 The input text (a) is split into a sequence
of characters (b) including punctuation marks and
blank spaces ( ), which are here considered as stan-
dard characters. Moreover, single characters inter-
leaved with the conventional empty character E.
Step 2 A CN (c) is built by adding all alternative
characters of the keyboard to each input character,
including the space character and the empty char-
acter. When the string character is , the only ad-
mitted alternative is c. Possible alternative spellings
of the original string correspond to paths in the CN.
Notice that each CN column beginning with a stan-
dard character permits to manage insertion, substi-
tution and split errors, while each column beginning
with the empty character permits to handle deletion
and run-on errors.
</bodyText>
<page confidence="0.996531">
414
</page>
<figure confidence="0.795989">
999 from any other character, respectively.
For the sake of clarity, the probability p(w|a) of
just one entry is reported in Figure 1.
(a)
al l _ o f f _ ame
1
(b) ε a ε l ε l ε _ ε o ε f ε f ε _ ε a ε m ε e ε
</figure>
<bodyText confidence="0.997431307692308">
Step 3 The generation of spelling variations (d) is
operated by means of the same decoder employed
for translation (see below), but in a much simplified
configuration which does not exploit any translation
model. It is designed to search the input character-
based CN for the n-best character sequences which
better “correct” the mistaken input. In Figure 1 the
best sequence is marked by bold boxes (c), and the
empty character c is removed for the sake of clarity
(d). This process relies only on the character-based
6-gram language model trained on monolingual data
in the source language. It is worth noticing that the
generated spelling alternatives may in principle still
contain non-words, just because they are selected by
a character-based language model, which does not
explicitly embed the notion of word.
Transposition errors are modeled both (i) indi-
rectly through consecutive substitutions with appro-
priate characters and (ii) directly by permitting some
re-orderings of adjacent characters. Moreover, pre-
liminary experiments revealed that the explicit han-
dling of deletion and run-on errors by interleaving
input characters with the empty character c (Step 1)
is crucial to achieve good performance. Although
the size of the character-based CN doubles, its de-
coding time increases only by a small factor.
</bodyText>
<figure confidence="0.997138774834437">
2
l
l
ε _
ε
ε
ε
ε
a
ε
ε
a
ε
m
e
o
ε
f
ε
ε
f
ε _
ε
b s
ε
ε
ε a
b
ε
a
a
a
a
a
a
a
ε
a
ε
ε
ε
ε
a
a
ε
a
b
k
b
k
b
j b
b
i b
b
b
c
d
s
r
r
b
b
b
(c)
g
c
c
o
p
...
c
o
c
c
w
c
c
d
p
w
c
y
c
c
c
c
...
d
e
d
d
d
e
d
k d
z
p
z
e
d
d
d
d
...
...
f
g
e
e
e ...
e
e
e
e
e
...
...
...
...
...
h ...
...
...
...
...
...
...
...
...
...
...
i
p(w|a) ∝ 0.91
...
3
hall _ o f _ f ame
al l_ o f _me
ha llo _ f ame
hul l _ o f f _me
all_ o f _ f ame
4
fame
all off
hall of me
hallo ε
...
hull ...
...
5
arca della gloria
</figure>
<figureCaption confidence="0.818357714285714">
Figure 1: The whole process to translate the mistaken
input “all off ame [hall offame]” into “arca della gloria”.
Step 4 The n-best character sequences (d) are
transformed into a word-based CN (e) (Mangu et
al., 2000). First, each character-based sequence is
transformed into a unifilar word-based lattice, whose
edges correspond to words and timestamps to the
</figureCaption>
<bodyText confidence="0.830702166666667">
character positions. Then, the unifilar lattices are put
in parallel to create one lattice with all spelling vari-
ations of the input text (a). Finally, a word-based CN
is generated by means of the lattice-tool available in
the SRILM Toolkit (Stolcke, 2002).
A probability distribution of confusable
keystrokes is generated based on the distance
between the keys on a standard QWERTY key-
board. This distribution is intended to model how a
spelling error is actually produced. Hence, character
alternatives in the CN are associated to a probability
given by:
</bodyText>
<equation confidence="0.996836">
1
p (x|y) a - (1)
k � d(x, y) + 1
</equation>
<bodyText confidence="0.999973666666667">
where d(x, y) is the physical distance between the
key of x and the key of y on the keyboard layout;
for example, the character a has a distance of 3 from
the character c on the considered keyboard layout.
The free parameter k tunes the discriminative power
of the model between correct and wrong typing. In
this paper, k was empirically set to 0.1. The c and
characters are assigned a default distance of 9 and
Step 5 Translation of the CN (e) is performed
with the Moses decoder (Koehn et al., 2007), that
has been successfully applied mainly to text trans-
lation, but also to process multiple input hypothe-
ses (Bertoldi et al., 2008), representing, for exam-
ple, speech transcriptions, word segmentations, texts
with possible punctuation marks, etc. In general,
</bodyText>
<page confidence="0.997936">
415
</page>
<table confidence="0.999161666666667">
set #sent. English Italian
#wrd dict. #wrd dict.
EP train 1.2M 36M 106K 35M 146K
test 2K 60K 6.5K 60K 8.3K
WF train 42K 996K 2641 994K 2843
test 328 8789 606 8704 697
</table>
<tableCaption confidence="0.937557">
Table 3: Statistics of train/test data of the Europarl (EP)
and the Weather Forecast (WF) tasks.
</tableCaption>
<bodyText confidence="0.9997815">
Moses looks for the best translation exploring the
search space defined by a set of feature functions
(models), which are log-linearly interpolated with
weights estimated during a tuning stage.
The rationale of storing the spelling alternatives
into a word-based CN instead of n-best list is two-
fold: (i) the CN contains a significantly larger num-
ber of variations, and (ii) the translation system is
much more efficient to translate CNs instead of n-
best lists.
</bodyText>
<sectionHeader confidence="0.999636" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999833238095238">
Extensive experiments have been conducted on the
Europarl shared task, from English to Italian, as
specified by the Workshop on Statistical Machine
Translation of the ACL 2008.3 Additional experi-
ments were conducted on a smaller task, namely the
translation of weather forecast bulletins between the
same language pair. Statistics on texts employed in
experiments are reported in Table 3.
For both tasks, we created evaluation data by ar-
tificially corrupting input text with the noise sources
described in Section 3. The module for generating
spelling variations (Step 3) was trained on additional
4M and 16M running words in English and Italian,
respectively.
We empirically investigated the following issues:
(a) performance of the standard MT engine versus
nature and level of the input noise; (b) performance
of the error-recovering MT engine versus number of
provided spelling variations; (c) portability of the
approach to another task and translation direction;
(d) computational requirements of the approach.
</bodyText>
<subsectionHeader confidence="0.999159">
5.1 Impact of Noise
</subsectionHeader>
<bodyText confidence="0.999781">
The first set of experiments involved the translation
of corrupted versions of the Europarl test set. Fig-
</bodyText>
<footnote confidence="0.59185">
3http://www.statmt.org/wmt08/
</footnote>
<figure confidence="0.933011">
25
20
15
10
</figure>
<figureCaption confidence="0.9985825">
Figure 2: Translation performance as function of the
noise level (in log-scale) for different types of noise.
</figureCaption>
<bodyText confidence="0.999824178571428">
ure 2 plots three curves of BLEU(%) scores, corre-
sponding to different noise sources and noise ratios,
given in terms of percentage of word error rate. It
also shows the BLEU score on the original clean
text. Notice that this baseline performance (25.16)
represents the state-of-the-art4 for this task.
The major outcome of these experiments is that
the different types of errors seem to affect MT per-
formance in a very similar manner. Quantitatively,
performance degradation begins even for low noise
levels – about 0.5 absolute BLEU loss at 1% of
noise level – and reaches 50% when text corruption
reaches the level of 30%. The similar impact of non-
word and random errors is somehow expected. The
plain reason is that both types of errors very likely5
generate Out-Of-Vocabulary (OOV) words.
We find instead less predictable that the impact of
real-word errors is indistinguishable from that of the
other two noise sources. Notice also that most of the
real-word errors produce indeed words known to the
MT system. Hence, the question regards the behav-
ior of the MT system when the sentence includes on
OOV word or an out-of-context known word. Em-
pirically it seems that in both cases the decoder pro-
duces translations with the same amount of errors.
In some sense, the good news is that real-word er-
rors do not induce more translation errors than OOV
words do.
</bodyText>
<footnote confidence="0.985697">
4http://matrix.statmt.org/matrix
5Modulo noise in the parallel data and the chance that a ran-
dom error generates a true word.
</footnote>
<figure confidence="0.985532636363636">
00.5 1 2 5 10 20
Noise Level (%)
BLEU
25
20
15
10
baseline
random, no-recovery
non-word, no-recovery
real-word, no-recovery
</figure>
<page confidence="0.670269">
416
</page>
<figure confidence="0.99952055">
BLEU
20
baseline
no-recovery
single
multiple, 200
baseline
no-recovery
single
multiple, 200
BLEU 25
20
15
25 25
20
15
25
20
00.5 1 2 5 10 20 50 0 0.5 1 2 5 10
Noise Level (%) Noise Level (%)
</figure>
<figureCaption confidence="0.999923">
Figure 3: Performance of error-recovering method with random (left) and real-word (right) noise.
</figureCaption>
<subsectionHeader confidence="0.999622">
5.2 Impact of Multiple Corrections
</subsectionHeader>
<bodyText confidence="0.999923333333334">
Experiments presented here address evaluation of
our enhanced MT system. In addition to nature and
level of noise, translation performance is also an-
alyzed with respect to the number (1 and 200) of
spelling alternatives generated at Step 3. Figure 3
plots BLEU scores for random (left plot) and real-
word (right plot) noises. For comparison purposes,
the curves with no error recovery are also shown.
Results with non-word noise are not provided since
they are pretty similar to those with random noise.
It is worth noticing that real-word errors are re-
covered in a different way than random errors; in
fact, for the latter a single spelling alternative seems
sufficient to guarantee a substantial error recovery,
whereas for real-word errors this is not the case.
Concerning the use of spelling variations, it is
worth remarking that our system is able to fully re-
cover from both random and non-word errors up to
noise levels of 10%, which remains high even for
noise levels up to 20%, where the BLEU degrada-
tion is limited to around 5% relative.
Real-word errors are optimally recovered in the
case of multiple spelling variations until they do not
exceed 2% of the words in the input text; after that,
the decrement of the MT quality becomes signif-
icant but still limited to about 5% BLEU relative
for a noise level of 10%. So the question arises
about what could be a realistic real-word noise level.
Clearly this question is not easy to address. How-
ever, to get a rough idea we can look at the exam-
ples reported in Table 1. These five sentences were
extracted from a text of about 100 words (of which
Table 1 only shows the sentences containing errors)
that contain in total 8 errors: 7 of which are non-
words and 1 is a real-word. Although from these
figures reliable statistics cannot be estimated, a rea-
sonable assumption could be that a global noise level
of 10%6 might contain a 1/10 ratio for real-word vs.
non-word errors. Thus, looking at the real-word er-
ror curve of Figure 3, the inability to recover errors
for noise levels greater than 2-5% should actually be
acceptable given this empirical observation.
Another relevant remark from Figure 3 is that
for low noise levels (less than 1%) the use of the
error-recovering module is counterproductive, since
it introduces more errors than those actually affect-
ing the original input text, causing a slight degra-
dation of the translation performance. If the com-
putational cost to generate variants, which will be
analyzed in the next paragraph, is also taken into ac-
count, it results evident the importance of design-
ing a good strategy for enabling or disabling on de-
mand the error-recovering stage. A starting point for
defining an effective activation strategy is the esti-
mation of the noise rate. For doing this, non-words
can be counted by exploiting proper dictionaries or
spell checkers; concerning real-word noise, its rate
can be inferred either from the non-word rate, or by
means of the perplexity, which is expected to be-
come higher as the real-word error rate increases
(Subramaniam et al., 2009). Once the noise level
of the input text is known, the decision of activat-
ing the correction module can be easily taken on a
</bodyText>
<footnote confidence="0.996949">
6By the way, at this noise rate, an error-recovering strategy
would be highly recommended.
</footnote>
<page confidence="0.99039">
417
</page>
<figureCaption confidence="0.9884665">
Figure 4: Effects of random noise and noise correction
on translation performance for the WF task.
</figureCaption>
<bodyText confidence="0.992405">
threshold basis. Alternatively, the proper working
point, in terms of precision and recall, of the correc-
tion model could be dynamically chosen as a func-
tion of the actual noise level.
</bodyText>
<subsectionHeader confidence="0.993991">
5.3 Computational Costs
</subsectionHeader>
<bodyText confidence="0.9999814375">
Although our investigation does not address explic-
itly computational aspects of translating noisy in-
put, nevertheless some general considerations can be
drawn.
The effectiveness of our recovering approach re-
lies on the compact representation of many spelling
alternatives in a word-based CN. The CN decod-
ing has been shown to be efficient, just minimally
larger than the single string decoding (Bertoldi et
al., 2008). On the contrary, in the current enhanced
MT setting, the sequence of Steps 1 to 4 for build-
ing the CN from the noisy input text is quite costly.
Rather than to an intrinsic complexity, this is due to
our choice of creating a rich character-based CN in
Step 3 for the sake of flexibility and to a naive im-
plementation of Step 4.
</bodyText>
<subsectionHeader confidence="0.95248">
5.4 Portability
</subsectionHeader>
<bodyText confidence="0.999968851851852">
So far we have analyzed in detail our approach
on the medium-large sized Europarl task, for the
English-to-Italian translation direction. For assess-
ing portability, we also considered a simpler task
–the translation of weather forecast bulletins– where
the translation quality is definitely higher, for the
same language pair but in both translation directions.
The choice of the weather forecast task is not by
chance. In fact, as the automatically translated bul-
letins are published on the Web, a very high transla-
tion quality is required, and then the presence of any
typing error in the original text could be a concern.
(By the way, for this task the presence of real-word
errors is very marginal.)
Figure 4 plots curves of MT performance under
random noise conditions against multiple spelling
variations, for two translation directions. It can
be noticed that the error-recovering system behaves
qualitatively as for the Europarl task but even better
from a quantitative viewpoint. Again, the recovering
model introduces spurious errors which affect trans-
lation quality for low levels of noisy input, but in
this case the break-even point is less than 0.1% noise
level. On the other side, errors corrupting the input
text are fully recovered up to 30-40% of noise lev-
els, for which the BLEU score would be more than
halved for non-corrected texts.
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999987789473684">
There are a number of important issues that this
work has still left open. First of all, we focused
on a specific way of generating spelling varia-
tions, based on single characters, but other possible
choices should be investigated and compared to our
approach, like the use of n-grams of words.
An important open question regards efficiency of
the proposed recovering strategy, since the problem
has been only sketched in Section 5.3. It is our in-
tention to analyze the intrinsic complexity of our
model, possibly discover its bottlenecks and imple-
ment a more efficient solution.
Another topic, mentioned in Section 5.2, is the ac-
tivation strategy of the misspelling recovery. Some
further investigation is required on how its working
point can be effectively selected; in fact, since the
enhanced system necessarily introduces spurious er-
rors, it would be desirable to increase its precision
for low-corrupted input texts.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9997475">
This paper addressed the issue of automatically
translating written texts that are corrupted by mis-
spelling errors. An enhancement of a state-of-the-art
statistical MT system is proposed which efficiently
performs the translation of multiple spelling variants
of noisy input. These alternatives are generated by a
character-based error recovery system under the as-
sumption that misspellings are due to typing errors.
The enhanced MT system has been tested on texts
corrupted with increasing noise levels of three dif-
ferent sources: random, non-word, and real-word er-
rors.
</bodyText>
<figure confidence="0.999617936170213">
English-Italian Italian-English
Noise Level (%) Noise Level (%)
1
1
0
0.
10
50
BLEU
60
50
40
30
20
10
0
baseline
no-recovery
multiple, 200
1
1
0
0.
10
50
baseline
no-recovery
multiple, 200
50
40
30
20
10
0
60
50
40
30
20
10
0
50
40
30
20
10
0
</figure>
<page confidence="0.994971">
418
</page>
<bodyText confidence="0.9714205">
Analysis of experimental results has led us to
draw the following conclusions:
• The impact of misspelling errors on MT perfor-
mance depends on the noise rate, but not on the
noise source.
• The capability of the enhanced MT system to
recover from errors differs according to the
noise source: real-word noise is significantly
harder to remove than random and non-word
noise, which behave substantially the same.
• The exploitation of several spelling alternatives
permits to almost fully recover from errors if
the noise rate does not exceed 10% for non-
word noise and 2% for real-word noise, which
are likely above the corruption level observed
in many social media.
• Finally, performance slightly decreases when
input text is correct or just mistaken at a negli-
gible level, because the error recovery module
rewards recall rather than precision and hence
tends to overgenerate correction alternatives,
even if not needed.
</bodyText>
<sectionHeader confidence="0.999074" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995405">
This work was supported by the EuroMatrixPlus
project (IST-231720), which is funded by the EC un-
der the 7th Framework Programme for Research and
Technological Development.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999761633802817">
N. Bertoldi, et al. 2008. Efficient speech translation
through confusion network decoding. IEEE Trans-
actions on Audio, Speech, and Language Processing,
16(8):1696–1705.
E. Brill and R. C. Moore. 2000. An improved error
model for noisy channel spelling correction. In Pro-
ceedings of ACL. Hong Kong.
J. Carrera, et al. 2009. Machine trans-
lation for cross-language social media.
http://www.promt.com/company/technology/pdf/mach
ine translation for cross language social media.pdf.
F. Casacuberta, et al. 2008. Recent efforts in spoken lan-
guage processing. IEEE Signal Processing Magazine,
25(3):80–88.
K. W. Church and W. A. Gale. 1991. Probability scor-
ing for spelling correction. Statistics and Computing,
1(2):93–103.
M. W. Davis, et al. 1995. Text alignment in the real
world: Improving alignments of noisy translations us-
ing common lexical features, string matching strate-
gies and n-gram comparisons. In Proceedings of
EACL, Dublin, Ireland.
L. Dey and S. M. Haque. 2009. Studying the effects of
noisy text on text mining applications. In Proceedings
ofAND, pages 107–114, Barcelona, Spain.
D. Fossati and B. Di Eugenio. 2008. I saw tree trees in
the park: How to correct real-word spelling mistakes.
In Proceedings of LREC, Marrakech, Morocco.
G. Hirst and A. Budanitsky. 2005. Correcting real-word
spelling errors by restoring lexical cohesion. Natural
Language Engineering, 11(01):87–111.
P. Koehn, et al. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL
- Demo and Poster Sessions, pages 177–180, Prague,
Czech Republic.
K. Kukich. 1992. Spelling correction for the telecom-
munications network for the deaf. Communications of
the ACM, 35(5):80–90.
D. Kushal, et al. 2003. Mining the peanut gallery:
opinion extraction and semantic classification of prod-
uct reviews. In Proceedings of the WWW conference,
pages 519–528, Budapest, Hungary.
L. Mangu, et al. 2000. Finding consensus in speech
recognition: Word error minimization and other appli-
cations of confusion networks. Computer, Speech and
Language, 14(4):373–400.
R. Mitton. 1995. English Spelling and the Computer
(Studies in Language and Linguistics). Addison Wes-
ley Publishing Company.
J. Pedler. 2007. Computer correction of real-word
spelling errors in dyslexic text. Ph.D. thesis, Univer-
sity of London.
M. Reynaert. 2006. Corpus-induced corpus cleanup. In
Proceedings of LREC, Genoa, Italy.
J. Schaback and F. Li. 2007. Multi-level feature extrac-
tion for spelling correction. In IJCAI - Workshop on
Analytics for Noisy Unstructured Text Data, pages 79–
86, Hyderabad, India.
J. Schler, et al. 2006. Effects of age and gender on blog-
ging. In Proceedings of AAAI-CAAW, Palo Alto, CA.
A. Stolcke. 2002. Srilm - an extensible language model-
ing toolkit. In Proceedings of ICSLP, Denver, CO.
L. V. Subramaniam, et al. 2009. A survey of types of text
noise and techniques to handle noisy text. In Proceed-
ings ofAND, pages 115–122, Barcelona, Spain.
K. Toutanova and R. C. Moore. 2002. Pronunciation
modeling for improved spelling correction. In Pro-
ceedings ofACL, pages 144–151, Philadelphia, PA
S. Vogel. 2003. Using noisy biligual data for statisti-
cal machine translation. In Proceedings of EACL, Bu-
dapest, Hungary.
</reference>
<page confidence="0.998669">
419
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762543">
<title confidence="0.999804">Statistical Machine Translation of Texts with Misspelled Words</title>
<author confidence="0.999932">Nicola Bertoldi Mauro Cettolo Marcello Federico</author>
<affiliation confidence="0.810097">FBK - Fondazione Bruno</affiliation>
<address confidence="0.861391">via Sommarive 18 - 38123 Povo, Trento, Italy</address>
<abstract confidence="0.998909466666667">This paper investigates the impact of misspelled words in statistical machine translation and proposes an extension of the translation engine for handling misspellings. The enhanced system decodes a word-based confusion network representing spelling variations of the input text. We present extensive experimental results on two translation tasks of increasing complexity which show how misspellings of different types do affect performance of a statistical machine translation decoder and to what extent our enhanced system is able to recover from such errors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
</authors>
<title>Efficient speech translation through confusion network decoding.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>8</issue>
<marker>Bertoldi, 2008</marker>
<rawString>N. Bertoldi, et al. 2008. Efficient speech translation through confusion network decoding. IEEE Transactions on Audio, Speech, and Language Processing, 16(8):1696–1705.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>R C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL. Hong Kong.</booktitle>
<contexts>
<context position="4761" citStr="Brill and Moore, 2000" startWordPosition="733" endWordPosition="736">ht affect text mining is in (Subramaniam et al., 2009), while an analysis of how noise phenomena, commonly occurring in blogs, affect an opinion mining application is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>E. Brill and R. C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings of ACL. Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carrera</author>
</authors>
<title>Machine translation for cross-language social media. http://www.promt.com/company/technology/pdf/mach ine translation for cross language social media.pdf.</title>
<date>2009</date>
<marker>Carrera, 2009</marker>
<rawString>J. Carrera, et al. 2009. Machine translation for cross-language social media. http://www.promt.com/company/technology/pdf/mach ine translation for cross language social media.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
</authors>
<title>Recent efforts in spoken language processing.</title>
<date>2008</date>
<journal>IEEE Signal Processing Magazine,</journal>
<volume>25</volume>
<issue>3</issue>
<marker>Casacuberta, 2008</marker>
<rawString>F. Casacuberta, et al. 2008. Recent efforts in spoken language processing. IEEE Signal Processing Magazine, 25(3):80–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>Probability scoring for spelling correction.</title>
<date>1991</date>
<journal>Statistics and Computing,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="4736" citStr="Church and Gale, 1991" startWordPosition="729" endWordPosition="732">t types of noise that might affect text mining is in (Subramaniam et al., 2009), while an analysis of how noise phenomena, commonly occurring in blogs, affect an opinion mining application is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), an</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>K. W. Church and W. A. Gale. 1991. Probability scoring for spelling correction. Statistics and Computing, 1(2):93–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Davis</author>
</authors>
<title>Text alignment in the real world: Improving alignments of noisy translations using common lexical features, string matching strategies and n-gram comparisons.</title>
<date>1995</date>
<booktitle>In Proceedings of EACL,</booktitle>
<location>Dublin, Ireland.</location>
<marker>Davis, 1995</marker>
<rawString>M. W. Davis, et al. 1995. Text alignment in the real world: Improving alignments of noisy translations using common lexical features, string matching strategies and n-gram comparisons. In Proceedings of EACL, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Dey</author>
<author>S M Haque</author>
</authors>
<title>Studying the effects of noisy text on text mining applications.</title>
<date>2009</date>
<booktitle>In Proceedings ofAND,</booktitle>
<pages>107--114</pages>
<location>Barcelona,</location>
<contexts>
<context position="4330" citStr="Dey and Haque, 2009" startWordPosition="666" endWordPosition="669">we also measure the robustness of the enhanced MT decoder with respect to different noise sources. Experiments are reported on two tasks of different complexity, the translation of Europarl texts and weather bulletins, involving English and Italian languages. 2 Previous Work Most contributions addressing NLP of noisy usergenerated content are from the text mining community. A survey about the different types of noise that might affect text mining is in (Subramaniam et al., 2009), while an analysis of how noise phenomena, commonly occurring in blogs, affect an opinion mining application is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular,</context>
</contexts>
<marker>Dey, Haque, 2009</marker>
<rawString>L. Dey and S. M. Haque. 2009. Studying the effects of noisy text on text mining applications. In Proceedings ofAND, pages 107–114, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Fossati</author>
<author>B Di Eugenio</author>
</authors>
<title>I saw tree trees in the park: How to correct real-word spelling mistakes.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Fossati, Di Eugenio, 2008</marker>
<rawString>D. Fossati and B. Di Eugenio. 2008. I saw tree trees in the park: How to correct real-word spelling mistakes. In Proceedings of LREC, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>A Budanitsky</author>
</authors>
<title>Correcting real-word spelling errors by restoring lexical cohesion.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>01</issue>
<contexts>
<context position="5401" citStr="Hirst and Budanitsky, 2005" startWordPosition="831" endWordPosition="834">va and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsky, 2005). More sophisticated approaches have been proposed by (Fossati and Di Eugenio, 2008), that mixes surface and Part-Of-Speech Information, and (Schaback and Li, 2007), which combines similarity measures at the character, phonetic, word, syntax, and semantic levels into one global feature-based framework. a) *W *w had just come in from Australia [Australia] b) good service we *staid one week. [Tahiti] c) The room was *exellent but the hallway was *filty . [NJ] d) is a good place to stay, if you are looking for a hotel *arround LAX airport. [Tahiti] e) The staff was *freindly ... I was *conerned a</context>
<context position="8416" citStr="Hirst and Budanitsky, 2005" startWordPosition="1344" endWordPosition="1348">of the misspelled *exellent in sentence c). 1 Run-on and split are the special cases of deleting and inserting blank spaces, respectively. 413 1. your - you’re 2. then - than 3. its - it’s 4. to - too - two 5. were - where - we’re 6 there - their - they’re Z a - an - and 8. off - of 9. here - hear 10. lose - loose Table 2: List of frequent real-word errors found in blogs. Source: http://www.theprobabilist.com. An interesting combination of cause and effect is when lack of linguistic competence results in confusing the spelling of a word with the spelling of another word that sounds similarly (Hirst and Budanitsky, 2005). This could be likely the case of the Polynesian tourist that authored sentence b). A short list of words frequently confused in blogs is reported in Table 2 while a longer list can be found in the Wikipedia.2 Real-word errors typically fool spell checkers because their identification requires analyzing the context in which they occur. In this paper, we automatically corrupt clean text with three types of noise described below. This procedure permits us to analyze the MT performance against different sources and levels of noise and to systematically evaluate our error-recovering strategy. Non</context>
</contexts>
<marker>Hirst, Budanitsky, 2005</marker>
<rawString>G. Hirst and A. Budanitsky. 2005. Correcting real-word spelling errors by restoring lexical cohesion. Natural Language Engineering, 11(01):87–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL - Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<marker>Koehn, 2007</marker>
<rawString>P. Koehn, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL - Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Spelling correction for the telecommunications network for the deaf.</title>
<date>1992</date>
<journal>Communications of the ACM,</journal>
<volume>35</volume>
<issue>5</issue>
<contexts>
<context position="4895" citStr="Kukich, 1992" startWordPosition="756" endWordPosition="757">n mining application is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsky, 2005). More sophisticated approaches have been proposed by (Fossati and Di Eugenio, 2008), that mix</context>
<context position="7680" citStr="Kukich, 1992" startWordPosition="1212" endWordPosition="1213">on with MT. 3 Types of Misspellings In general, a misspelled word is a sequence of letters that corresponds to no correctly spelled word of the same language (non-word error), or to a correct spelling of another word (real-word error). In the examples shown in Table 1, all marked errors are non-word errors, but the one in sentence b), which indeed is likely a misspelling of the word stayed. Causes of a misspelling may be an unintentional typing error (e.g. *freindly for friendly), or lack of knowledge about the proper spelling. Typing errors can originate from six different typing operations (Kukich, 1992): substitution, insertion, deletion, transposition, run-on, and split.1 Lack of knowledge could be the cause of the misspelled *exellent in sentence c). 1 Run-on and split are the special cases of deleting and inserting blank spaces, respectively. 413 1. your - you’re 2. then - than 3. its - it’s 4. to - too - two 5. were - where - we’re 6 there - their - they’re Z a - an - and 8. off - of 9. here - hear 10. lose - loose Table 2: List of frequent real-word errors found in blogs. Source: http://www.theprobabilist.com. An interesting combination of cause and effect is when lack of linguistic com</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>K. Kukich. 1992. Spelling correction for the telecommunications network for the deaf. Communications of the ACM, 35(5):80–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kushal</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the WWW conference,</booktitle>
<pages>519--528</pages>
<location>Budapest, Hungary.</location>
<marker>Kushal, 2003</marker>
<rawString>D. Kushal, et al. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of the WWW conference, pages 519–528, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
</authors>
<title>Finding consensus in speech recognition: Word error minimization and other applications of confusion networks.</title>
<date>2000</date>
<journal>Computer, Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<marker>Mangu, 2000</marker>
<rawString>L. Mangu, et al. 2000. Finding consensus in speech recognition: Word error minimization and other applications of confusion networks. Computer, Speech and Language, 14(4):373–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitton</author>
</authors>
<title>English Spelling and the Computer (Studies in Language and Linguistics).</title>
<date>1995</date>
<publisher>Addison Wesley Publishing Company.</publisher>
<contexts>
<context position="5298" citStr="Mitton, 1995" startWordPosition="819" endWordPosition="820">ror model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsky, 2005). More sophisticated approaches have been proposed by (Fossati and Di Eugenio, 2008), that mixes surface and Part-Of-Speech Information, and (Schaback and Li, 2007), which combines similarity measures at the character, phonetic, word, syntax, and semantic levels into one global feature-based framework. a) *W *w had just come in from Australia [Australia] b) good service we *staid one week. [Tahiti] c) The room was *exellent but the hallway was *filty . [NJ] d) is a good place to stay, if you </context>
</contexts>
<marker>Mitton, 1995</marker>
<rawString>R. Mitton. 1995. English Spelling and the Computer (Studies in Language and Linguistics). Addison Wesley Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pedler</author>
</authors>
<title>Computer correction of real-word spelling errors in dyslexic text.</title>
<date>2007</date>
<booktitle>In Proceedings of LREC,</booktitle>
<tech>Ph.D. thesis,</tech>
<institution>University of</institution>
<location>Genoa, Italy.</location>
<contexts>
<context position="4914" citStr="Pedler, 2007" startWordPosition="759" endWordPosition="760">n is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsky, 2005). More sophisticated approaches have been proposed by (Fossati and Di Eugenio, 2008), that mixes surface and Part</context>
</contexts>
<marker>Pedler, 2007</marker>
<rawString>J. Pedler. 2007. Computer correction of real-word spelling errors in dyslexic text. Ph.D. thesis, University of London. M. Reynaert. 2006. Corpus-induced corpus cleanup. In Proceedings of LREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schaback</author>
<author>F Li</author>
</authors>
<title>Multi-level feature extraction for spelling correction.</title>
<date>2007</date>
<booktitle>In IJCAI - Workshop on Analytics for Noisy Unstructured Text Data,</booktitle>
<pages>79--86</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="5565" citStr="Schaback and Li, 2007" startWordPosition="854" endWordPosition="857">ork is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsky, 2005). More sophisticated approaches have been proposed by (Fossati and Di Eugenio, 2008), that mixes surface and Part-Of-Speech Information, and (Schaback and Li, 2007), which combines similarity measures at the character, phonetic, word, syntax, and semantic levels into one global feature-based framework. a) *W *w had just come in from Australia [Australia] b) good service we *staid one week. [Tahiti] c) The room was *exellent but the hallway was *filty . [NJ] d) is a good place to stay, if you are looking for a hotel *arround LAX airport. [Tahiti] e) The staff was *freindly ... I was *conerned about the noise [CT] Table 1: Examples of misspellings found in on-line reviews of an hotel close to Los Angeles Int’l Airport. Corresponding corrections are: a) We,</context>
</contexts>
<marker>Schaback, Li, 2007</marker>
<rawString>J. Schaback and F. Li. 2007. Multi-level feature extraction for spelling correction. In IJCAI - Workshop on Analytics for Noisy Unstructured Text Data, pages 79– 86, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schler</author>
</authors>
<title>Effects of age and gender on blogging.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-CAAW,</booktitle>
<location>Palo Alto, CA.</location>
<marker>Schler, 2006</marker>
<rawString>J. Schler, et al. 2006. Effects of age and gender on blogging. In Proceedings of AAAI-CAAW, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="14603" citStr="Stolcke, 2002" startWordPosition="2479" endWordPosition="2480">ure 1: The whole process to translate the mistaken input “all off ame [hall offame]” into “arca della gloria”. Step 4 The n-best character sequences (d) are transformed into a word-based CN (e) (Mangu et al., 2000). First, each character-based sequence is transformed into a unifilar word-based lattice, whose edges correspond to words and timestamps to the character positions. Then, the unifilar lattices are put in parallel to create one lattice with all spelling variations of the input text (a). Finally, a word-based CN is generated by means of the lattice-tool available in the SRILM Toolkit (Stolcke, 2002). A probability distribution of confusable keystrokes is generated based on the distance between the keys on a standard QWERTY keyboard. This distribution is intended to model how a spelling error is actually produced. Hence, character alternatives in the CN are associated to a probability given by: 1 p (x|y) a - (1) k � d(x, y) + 1 where d(x, y) is the physical distance between the key of x and the key of y on the keyboard layout; for example, the character a has a distance of 3 from the character c on the considered keyboard layout. The free parameter k tunes the discriminative power of the </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of ICSLP, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L V Subramaniam</author>
</authors>
<title>A survey of types of text noise and techniques to handle noisy text.</title>
<date>2009</date>
<booktitle>In Proceedings ofAND,</booktitle>
<pages>115--122</pages>
<location>Barcelona,</location>
<marker>Subramaniam, 2009</marker>
<rawString>L. V. Subramaniam, et al. 2009. A survey of types of text noise and techniques to handle noisy text. In Proceedings ofAND, pages 115–122, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>R C Moore</author>
</authors>
<title>Pronunciation modeling for improved spelling correction.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>144--151</pages>
<location>Philadelphia, PA</location>
<contexts>
<context position="4793" citStr="Toutanova and Moore, 2002" startWordPosition="738" endWordPosition="741">(Subramaniam et al., 2009), while an analysis of how noise phenomena, commonly occurring in blogs, affect an opinion mining application is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsk</context>
</contexts>
<marker>Toutanova, Moore, 2002</marker>
<rawString>K. Toutanova and R. C. Moore. 2002. Pronunciation modeling for improved spelling correction. In Proceedings ofACL, pages 144–151, Philadelphia, PA</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
</authors>
<title>Using noisy biligual data for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="6425" citStr="Vogel, 2003" startWordPosition="1003" endWordPosition="1004">m was *exellent but the hallway was *filty . [NJ] d) is a good place to stay, if you are looking for a hotel *arround LAX airport. [Tahiti] e) The staff was *freindly ... I was *conerned about the noise [CT] Table 1: Examples of misspellings found in on-line reviews of an hotel close to Los Angeles Int’l Airport. Corresponding corrections are: a) We, e, b) stayed, c) excellent, filthy, d) around, e) friendly, concerned. Concerning the literature of statistical MT, interest in noisy data has been so far considering issues different from misspelled words. For instance, (Davis et al., 1995) and (Vogel, 2003) address training methods coping with noisy parallel data, in the sense that translations do not perfectly match. Work on speech translation (Casacuberta et al., 2008) focused instead on efficient methods to couple speech recognition and MT in order to avoid error propagation. Very recently, (Carrera et al., 2009) conducted a qualitative study on the impact of noisy social media content on statistical and rule-based MT. Unfortunately, this work does not report any quantitative result, it is only based on a small selection of examples that are manually evaluated, and finally it does not address</context>
</contexts>
<marker>Vogel, 2003</marker>
<rawString>S. Vogel. 2003. Using noisy biligual data for statistical machine translation. In Proceedings of EACL, Budapest, Hungary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>