<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020288">
<title confidence="0.995764">
Low-Rank Tensors for Verbs in Compositional Distributional Semantics
</title>
<author confidence="0.999566">
Daniel Fried, Tamara Polajnar, and Stephen Clark
</author>
<affiliation confidence="0.9753775">
University of Cambridge
Computer Laboratory
</affiliation>
<email confidence="0.98827">
{df345,tp366,sc609}@cam.ac.uk
</email>
<sectionHeader confidence="0.997239" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835">
Several compositional distributional se-
mantic methods use tensors to model
multi-way interactions between vectors.
Unfortunately, the size of the tensors can
make their use impractical in large-scale
implementations. In this paper, we inves-
tigate whether we can match the perfor-
mance of full tensors with low-rank ap-
proximations that use a fraction of the
original number of parameters. We in-
vestigate the effect of low-rank tensors on
the transitive verb construction where the
verb is a third-order tensor. The results
show that, while the low-rank tensors re-
quire about two orders of magnitude fewer
parameters per verb, they achieve perfor-
mance comparable to, and occasionally
surpassing, the unconstrained-rank tensors
on sentence similarity and verb disam-
biguation tasks.
</bodyText>
<sectionHeader confidence="0.999487" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999342103448276">
Distributional semantic methods represent word
meanings by their contextual distributions, for ex-
ample by computing word-context co-ocurrence
statistics (Sch¨utze, 1998; Turney and Pantel, 2010)
or by learning vector representations for words
as part of a context prediction model (Bengio et
al., 2003; Collobert et al., 2011; Mikolov et al.,
2013). Recent research has also focused on com-
positional distributional semantics (CDS): com-
bining the distributional representations for words,
often in a syntax-driven fashion, to produce distri-
butional representations of phrases and sentences
(Mitchell and Lapata, 2008; Baroni and Zam-
parelli, 2010; Socher et al., 2012; Zanzotto and
Dell’Arciprete, 2012).
One method for CDS is the Categorial frame-
work (Coecke et al., 2011; Baroni et al., 2014),
where each word is represented by a tensor whose
order is determined by the Categorial Grammar
type of the word. For example, nouns are an
atomic type represented by a vector, and adjec-
tives are matrices that act as functions transform-
ing a noun vector into another noun vector (Baroni
and Zamparelli, 2010). A transitive verb is a third-
order tensor that takes the noun vectors represent-
ing the subject and object and returns a vector in
the sentence space (Polajnar et al., 2014).
However, a concrete implementation of the Cat-
egorial framework requires setting and storing the
values, or parameters, defining these matrices and
tensors. These parameters can be quite numerous
for even low-dimensional sentence spaces. For ex-
ample, a third-order tensor for a given transitive
verb, mapping two 100-dimensional noun spaces
to a 100-dimensional sentence space, would have
1003 parameters in its full form. All of the
more complex types have corresponding tensors of
higher order, and therefore a barrier to the practi-
cal implementation of this framework is the large
number of parameters required to represent an ex-
tended vocabulary and a variety of grammatical
constructions.
We aim to reduce the size of the models by
demonstrating that reduced-rank tensors, which
can be represented in a form requiring fewer pa-
rameters, can capture the semantics of complex
types as well as the full-rank tensors do. We base
our experiments on the transitive verb construction
for which there are established tasks and datasets
(Grefenstette and Sadrzadeh, 2011; Kartsaklis and
Sadrzadeh, 2014).
Previous work on the transitive verb construc-
tion within the Categorial framework includes a
two-step linear-regression method for the con-
struction of the full verb tensors (Grefenstette et
al., 2013) and a multi-linear regression method
combined with a two-dimensional plausibility
space (Polajnar et al., 2014). Polajnar et al. (2014)
</bodyText>
<page confidence="0.908769">
731
</page>
<bodyText confidence="0.962163971428571">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
also introduce several alternative ways of reducing
the number of tensor parameters by using matri-
ces. The best performing method uses two matri-
ces, one representing the subject-verb interactions
and the other the verb-object interactions. Some
interaction between the subject and the object is
re-introduced through a softmax layer. A similar
method is presented in Paperno et al. (2014). Mi-
lajevs et al. (2014) use vectors generated by a neu-
ral language model to construct verb matrices and
several different composition operators to generate
the composed subject-verb-object sentence repre-
sentation.
In this paper, we use tensor rank decomposi-
tion (Kolda and Bader, 2009) to represent each
verb’s tensor as a sum of tensor products of vec-
tors. We learn the component vectors and apply
the composition without ever constructing the full
tensors and thus we are able to improve on both
memory usage and efficiency. This approach fol-
lows recent work on using low-rank tensors to pa-
rameterize models for dependency parsing (Lei et
al., 2014) and semantic role labelling (Lei et al.,
2015). Our work applies the same tensor rank
decompositions, and similar optimization algo-
rithms, to the task of constructing a syntax-driven
model for CDS. Although we focus on the Cat-
egorial framework, the low-rank decomposition
methods are also applicable to other tensor-based
semantic models including Van de Cruys (2010),
Smolensky and Legendre (2006), and Blacoe et al.
(2013).
</bodyText>
<sectionHeader confidence="0.98702" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.997651411764706">
Tensor Models for Verbs We model each tran-
sitive verb as a bilinear function mapping subject
and object noun vectors, each of dimensionality
N, to a single sentence vector of dimensionality 5
(Coecke et al., 2011; Maillard et al., 2014) repre-
senting the composed subject-verb-object (SVO)
triple. Each transitive verb has its own third-
order tensor, which defines this bilinear function.
Consider a verb V with associated tensor V E
RSxNxN, and vectors s E RN, o E RN for
subject and object nouns, respectively. Then the
compositional representation for the subject, verb,
and object is a vector V (s, o) E RS, produced by
applying tensor contraction (the higher-order ana-
logue of matrix multiplication) to the verb tensor
and two noun vectors. The lth component of the
vector for the SVO triple is given by
</bodyText>
<equation confidence="0.9876265">
�V(s,o)l = Vljkoksj (1)
j,k
</equation>
<bodyText confidence="0.988108">
We aim to learn distributional vectors s and o
for subjects and objects, and tensors V for verbs,
such that the output vectors V (s, o) are distri-
butional representations of the entire SVO triple.
While there are several possible definitions of
the sentence space (Clark, 2013; Baroni et al.,
2014), we follow previous work (Grefenstette et
al., 2013) by using a contextual sentence space
consisting of content words that occur within the
same sentences as the SVO triple.
Low-Rank Tensor Representations Following
Lei et al. (2014), we represent each verb’s tensor
using a low-rank canonical polyadic (CP) decom-
position to reduce the numbers of parameters that
must be learned during training. As a higher-order
analogue to singular value decomposition for ma-
trices, CP decomposition factors a tensor into a
sum of R tensor products of vectors.1 Given a
third-order tensor V E RSxNxN, the CP decom-
position of V is:
</bodyText>
<equation confidence="0.998946333333333">
R
V = Pr ® Qr ® Rr (2)
r=1
</equation>
<bodyText confidence="0.999951">
where P E RRxS, Q E RRxN, R E RRxN are
parameter matrices, Pr gives the rth row of matrix
P, and ® is the tensor product.
The smallest R that allows the tensor to be ex-
pressed as this sum of outer products is the rank
of the tensor (Kolda and Bader, 2009). By fixing a
value for R that is sufficiently small compared to
5 and N (forcing the verb tensor to have rank of
at most R), and directly learning the parameters of
the low-rank approximation using gradient-based
optimization, we learn a low-rank tensor requiring
fewer parameters without ever having to store the
full tensor.
In addition to reducing the number of parame-
ters, representing tensors in this form allows us to
formulate the verb tensor’s action on noun vectors
as matrix multiplication. For a tensor in the form
of Eq. (2), the output SVO vector is given by
</bodyText>
<equation confidence="0.976717">
V (s, o) = PT(Qs O Ro) (3)
</equation>
<bodyText confidence="0.8947565">
where O is the elementwise vector product.
1However, unlike matrix singular value decomposition,
the component vectors in the CP decomposition are not nec-
essarily orthonormal.
</bodyText>
<page confidence="0.99364">
732
</page>
<sectionHeader confidence="0.994909" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.999919818181818">
We train the compositional model for verbs in
three steps: extracting transitive verbs and their
subject and object nouns from corpus data, pro-
ducing distributional vectors for the nouns and the
SVO triples, and then learning parameters of the
verb functions, which map the nouns to the SVO
triple vectors.
Corpus Data We extract SVO triples from an
October 2013 download of Wikipedia, tokenized
using Stanford CoreNLP (Manning et al., 2014),
lemmatized with the Morpha lemmatizer (Minnen
et al., 2001), and parsed using the C&amp;C parser
(Curran et al., 2007). We filter the SVO triples
to a set containing 345 distinct verbs: the verbs
from our test datasets, along with some additional
high-frequency verbs included to produce more
representative sentence spaces. For each verb, we
selected up to 600 triples which occurred more
than once and contained subject and object nouns
that occurred at least 100 times (to allow suffi-
cient context to produce a distributional represen-
tation for the triple). This resulted in approxi-
mately 150,000 SVO triples overall.
Distributional Vectors We produce two types
of distributional vectors for nouns and SVO triples
using the Wikipedia corpus. Since these methods
for producing distributional vectors for the SVO
triples require that the triples occur in a corpus of
text, the methods are not a replacement for a com-
positional framework that can produce representa-
tions for previously unseen expressions. However,
they can be used to generate data to train such a
model, as we will describe.
</bodyText>
<listItem confidence="0.9985815">
1) Count vectors (SVD): we count the num-
ber of times each noun or SVO triple co-occurs
with each of the 10,000 most frequent words (ex-
cluding stopwords) in the Wikipedia corpus, using
sentences as context boundaries. If the verb in the
SVO triple is itself a content word, we do not in-
clude it as context for the triple. This produces one
set of context vectors for nouns and another for
SVO triples. We weight entries in these vectors
using the t-test weighting scheme (Curran, 2004;
Polajnar and Clark, 2014), and then reduce the
vectors to 100 dimensions via singular value de-
composition (SVD), decomposing the noun vec-
tors and SVO vectors separately.
2) Prediction vectors (PV): we train vector
embeddings for nouns and SVO triples by adapt-
</listItem>
<bodyText confidence="0.998736657142857">
ing the Paragraph Vector distributed bag of words
method of Le and Mikolov (2014), an extension of
the skip-gram model of Mikolov et al. (2013). In
our experiments, given an SVO triple, the model
must predict contextual words sampled from all
sentences containing that triple. In the process, the
model learns vector embeddings for both the SVO
triples and for the words in the sentences such that
SVO vectors have a high dot product with their
contextual word vectors. While previous work
(Milajevs et al., 2014) has used prediction-based
vectors for words in a tensor-based CDS model,
ours uses prediction-based vectors for both words
and phrases to train a tensor regression model.
We learn 100-dimensional vectors for nouns
and SVO triples with a modified version of
word2vec,2 using the hierarchical sampling
method with the default hyperparameters and 20
iterations through the training data.
Training Methods We learn the tensor V of pa-
rameters for a given verb V using multi-linear re-
gression, treating the noun vectors s and o as in-
put and the composed SVO triple vector V (s, o)
as the regression output. Let MV be the num-
ber of training instances for V , where the ith in-
stance is a triple of vectors (s(i), o(i), t(i)), which
are the distributional vectors for the subject noun,
object noun, and the SVO triple, respectively. We
aim to learn a verb tensor V (either in full or in
decomposed, low-rank form) that minimizes the
mean of the squared residuals between the pre-
dicted SVO vectors V (s(i),o(i)) and those vec-
tors obtained distributionally from the corpus, t(i).
Specifically, we attempt to minimize the following
loss function:
</bodyText>
<equation confidence="0.957575">
— 1 MV
L(V ) — �
||V (s(i), o(i)) − t(i)||2 2 (4)
MV
i=1
V (s, o) is given by Eq. (1) for full tensors, and by
Eq. (3) for tensors represented in low-rank form.
</equation>
<bodyText confidence="0.999927428571428">
In both the low-rank and full-rank tensor learn-
ing, we use mini-batch ADADELTA optimization
(Zeiler, 2012) up to a maximum of 500 iterations
through the training data, which we found to be
sufficient for convergence for every verb. Rather
than placing a regularization penalty on the ten-
sor parameters, we use early stopping if the loss
</bodyText>
<footnote confidence="0.999058333333333">
2https://groups.google.com/d/
msg/word2vec-toolkit/Q49FIrNOQRo/
J6KG8mUj45sJ
</footnote>
<page confidence="0.997166">
733
</page>
<bodyText confidence="0.99978475">
increases on a validation set consisting of 10% of
the available SVO triples for each verb.
For low-rank tensors, we compare seven differ-
ent maximal ranks: R=1, 5, 10, 20, 30, 40 and 50.
To learn the parameters of the low-rank tensors,
we use an alternating optimization method (Kolda
and Bader, 2009; Lei et al., 2014): performing gra-
dient descent on one of the parameter matrices (for
example P) to minimize the loss function while
holding the other two fixed (Q and R), then re-
peating for the other parameter matrices in turn.
The parameter matrices are randomly initialized.3
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999631588235294">
We compare the performance of the low-rank ten-
sors against full tensors on two tasks. Both tasks
require the model to rank pairs of sentences each
consisting of a subject, transitive verb, and object
by the semantic similarity of the sentences in the
pair. The gold standard ranking is given by sim-
ilarity scores provided by human evaluators and
the scores are not averaged among the annotators.
The model ranking is evaluated against the rank-
ing from the gold standard similarity judgements
using Spearman’s ρ.
The verb disambiguation task (GS11) (Grefen-
stette and Sadrzadeh, 2011) involves distinguish-
ing between senses of an ambiguous verb, given
subject and object nouns as context. The dataset
consists of 200 sentence pairs, where the two sen-
tences in each pair have the same subject and ob-
ject but differ in the verb. Each of these pairs was
ranked by human evaluators on a 1-7 similarity
scale so that properly disambiguated pairs (e.g. au-
thor write book – author publish book) have higher
similarity scores than improperly disambiguated
pairs (e.g. author write book – author spell book).
The transitive sentence similarity dataset (Kart-
saklis and Sadrzadeh, 2014) consists of 72 subject-
verb-object sentences arranged into 108 sentence
pairs. As in GS11, each pair has a gold standard
semantic similarity score on a 1-7 scale. For ex-
ample, the pair medication achieve result – drug
produce effect has a high similarity rating, while
author write book – delegate buy land has a low
rating. In this dataset, however, the two sentences
in each pair have no lexical overlap: neither sub-
jects, objects, nor verbs are shared.
</bodyText>
<footnote confidence="0.910582">
3Since the low-rank tensor loss is non-convex, we suspect
that parameter initialization may produce better results.
</footnote>
<table confidence="0.999715416666667">
GS11 KS14 # tensor
SVD PV SVD PV params.
Add. 0.13 0.14 0.55 0.56 –
Mult. 0.13 0.14 0.09 0.27 –
R=1 0.10 0.05 0.18 0.30 300
R=5 0.26 0.30 0.28 0.40 1.5K
R=10 0.29 0.32 0.26 0.45 3K
R=20 0.31 0.34 0.39 0.44 6K
R=30 0.28 0.33 0.32 0.46 9K
R=40 0.32 0.30 0.31 0.52 12K
R=50 0.34 0.32 0.42 0.51 15K
Full 0.29 0.36 0.41 0.52 1M
</table>
<tableCaption confidence="0.998979">
Table 1: Model performance on the verb disam-
</tableCaption>
<bodyText confidence="0.912848166666667">
biguation (GS11) and sentence similarity (KS14)
tasks, given by Spearman’s ρ, and the number of
parameters needed to represent each verb’s tensor.
We show the highest tensor result for each task and
vector set in bold (and also bold the baseline when
it outperforms the tensor method).
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.968096266666666">
Table 1 displays correlations between the systems’
scores and human SVO similarity judgements on
the verb disambiguation (GS11) and sentence sim-
ilarity (KS14) tasks, for both the count (SVD) and
prediction vectors (PV). We also give results for
simple composition of word vectors using elemen-
twise addition and multiplication (Mitchell and
Lapata, 2008) (using verb vectors produced in the
same manner as for nouns). As is consistent with
prior work, the tensor-based models are surpassed
by vector addition on the KS14 dataset (Milajevs
et al., 2014), but perform better than both addition
and multiplication on the GS11 dataset.4
Unsurprisingly, the rank-1 tensor has lowest
performance for both tasks and vector sets, and
performance generally increases as we increase
the maximal rank R. The full tensor achieves
the best, or tied for the best, performance on both
tasks when using the PV vectors. However, for the
SVD vectors, low-rank tensors surpass the perfor-
mance of the full-rank tensor for R=40 and R=50
4The results in this table are not directly comparable with
Milajevs et al. (2014), who compare against averaged annota-
tor scores. Comparing against averaged annotator scores, our
best result on GS11 is 0.47 for the full-rank tensor with PV
vectors, and our best non-addition result on KS14 is 0.68 for
the K=40 tensor with PV vectors (the best result is addition
with PV vectors, which achieves 0.71). These results exceed
the scores reported for tensor-based models by Milajevs et al.
(2014).
</bodyText>
<page confidence="0.995557">
734
</page>
<bodyText confidence="0.9998695">
on GS11, and R=50 on KS14.
On GS11, the SVD and PV vectors have vary-
ing but mostly comparable performance, with PV
having higher performance on 5 out of 8 models.
However, on KS14, the PV vectors have better per-
formance than the SVD vectors for every model
by at least 0.05 points, which is consistent with
prior work comparing count and predict vectors on
these datasets (Milajevs et al., 2014).
The low-rank tensor models are also at least
twice as fast to train as the full tensors: on a single
core, training a rank-1 tensor takes about 5 sec-
onds for each verb on average, ranks 5-50 each
take between 1 and 2 minutes, and the full tensors
each take about 4 minutes. Since a separate tensor
is trained for each verb, this allows a substantial
amount of time to be saved even when using the
constrained vocabulary of 345 verbs.
</bodyText>
<sectionHeader confidence="0.998009" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999883476190476">
We find that low-rank tensors for verbs achieve
comparable or better performance than full-rank
tensors on both verb disambiguation and sentence
similarity tasks, while reducing the number of pa-
rameters that must be learned and stored for each
verb by at least two orders of magnitude, and cut-
ting training time in half.
While in our experiments the prediction-based
vectors outperform the count-based vectors on
both tasks for most models, Levy et al. (2015) in-
dicate that tuning hyperparameters of the count-
based vectors may be able to produce compara-
ble performance. Regardless, we show that the
low-rank tensors are able to achieve performance
comparable to the full rank for both types of vec-
tors. This is important for extending the model
to many more grammatical types (including those
with corresponding tensors of higher order than in-
vestigated here) to build a wide-coverage tensor-
based semantic system using, for example, the
CCG parser of Curran et al. (2007).
</bodyText>
<sectionHeader confidence="0.991722" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999565857142857">
Daniel Fried is supported by a Churchill Schol-
arship. Tamara Polajnar is supported by ERC
Starting Grant DisCoTex (306920). Stephen Clark
is supported by ERC Starting Grant DisCoTex
(306920) and EPSRC grant EP/I037512/1. We
would like to thank Laura Rimell and the anony-
mous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.755148" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.950184134615385">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2010), Cambridge, Massachusetts.
Marco Baroni, Raffaela Bernardi, and Roberto Zam-
parelli. 2014. Frege in space: A program of compo-
sitional distributional semantics. Linguistic Issues
in Language Technology, 9.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, (3):1137–1155.
William Blacoe, Elham Kashefi, and Mirella Lapata.
2013. A quantum-theoretic approach to distribu-
tional semantics. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT 2013), Atlanta,
Georgia.
Stephen Clark. 2013. Type-driven syntax and se-
mantics for composing meaning vectors. Quan-
tum Physics and Linguistics: A Compositional, Dia-
grammatic Discourse, pages 359–377.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2011. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36(1-4):345–384.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493–2537.
James R Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&amp;C
and Boxer. In Proceedings of the Demonstration
Session of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2007),
Prague, Czech Republic.
James R. Curran. 2004. From distributional to seman-
tic similarity. Ph.D. thesis, University of Edinburgh.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimenting with transitive verbs in a DisCoCat.
In Proceedings of the 2011 Workshop on Geometri-
cal Models of Natural Language Semantics (GEMS
2011), Edinburgh, Scotland.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013), Pottsdam, Germany.
</reference>
<page confidence="0.992454">
735
</page>
<reference confidence="0.996252140186916">
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. In Proceedings of the 11th Work-
shop on Quantum Physics and Logic (QPL 2014),
Kyoto, Japan, June.
Tamara G Kolda and Brett W Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455–500.
Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of the 31st International Conference on
Machine Learning (ICML 2014), Beijing, China.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2014), Baltimore, Mary-
land.
Tao Lei, Yuan Zhang, Lluis Marquez, Alessandro Mos-
chitti, and Regina Barzilay. 2015. High-order low-
rank tensors for semantic role labeling. In Proceed-
ings of the 2015 Conference of the North American
Chapter of the Association for Computational Lin-
guistics – Human Language Technologies (NAACL-
HLT 2015), Denver, Colorado.
Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.
Jean Maillard, Stephen Clark, and Edward Grefen-
stette. 2014. A type-driven tensor-based semantics
for CCG. In Proceedings of the EACL 2014 Type
Theory and Natural Language Semantics Workshop
(TTNLS), Gothenburg, Sweden.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2014): System Demonstra-
tions, pages 55–60, Baltimore, Maryland.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Neural Information Processing Systems
(NIPS 2013).
Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh
Sadrzadeh, and Matthew Purver. 2014. Evaluating
neural word representations in tensor-based compo-
sitional settings. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2014).
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(03):207–223.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Assocation for Com-
putational Linguistics: Human Language Technolo-
gies (ACL-08: HLT), Columbus, Ohio.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics.
In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (ACL
2014), Baltimore, Maryland.
Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In Proceedings of the
14th Conference of the European Chapter of the
Association for Computational Linguistics (EACL
2014), Gothenburg, Sweden.
Tamara Polajnar, Luana Fagarasan, and Stephen Clark.
2014. Reducing dimensions of tensors in type-
driven distributional semantics. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2014), Doha,
Qatar.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
124.
Paul Smolensky and Geraldine Legendre. 2006.
The Harmonic Mind: from neural computation to
optimality-theoretic grammar. MIT Press, Cam-
bridge, MA.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Composi-
tionality Through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL 2012), Jeju Island, Korea.
Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188.
Tim Van de Cruys. 2010. A non-negative tensor fac-
torization model for selectional preference induc-
tion. Journal of Natural Language Engineering,
16(4):417–437.
Fabio M Zanzotto and Lorenzo Dell’Arciprete. 2012.
Distributed tree kernels. In Proceedings of the
29th International Conference on Machine Learning
(ICML 2012), Edinburgh, Scotland.
Matthew D Zeiler. 2012. ADADELTA: an
adaptive learning rate method. arXiv preprint
arXiv:1212.5701.
</reference>
<page confidence="0.998541">
736
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.535208">
<title confidence="0.927322">Low-Rank Tensors for Verbs in Compositional Distributional Semantics</title>
<author confidence="0.942887">Tamara Polajnar Fried</author>
<affiliation confidence="0.7789085">University of Computer</affiliation>
<abstract confidence="0.998262333333333">Several compositional distributional semantic methods use tensors to model multi-way interactions between vectors. Unfortunately, the size of the tensors can make their use impractical in large-scale implementations. In this paper, we investigate whether we can match the performance of full tensors with low-rank approximations that use a fraction of the original number of parameters. We investigate the effect of low-rank tensors on the transitive verb construction where the verb is a third-order tensor. The results show that, while the low-rank tensors require about two orders of magnitude fewer parameters per verb, they achieve performance comparable to, and occasionally surpassing, the unconstrained-rank tensors on sentence similarity and verb disambiguation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),</booktitle>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1638" citStr="Baroni and Zamparelli, 2010" startWordPosition="227" endWordPosition="231">mantic methods represent word meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word. For example, nouns are an atomic type represented by a vector, and adjectives are matrices that act as functions transforming a noun vector into another noun vector (Baroni and Zamparelli, 2010). A transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaela Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Frege in space: A program of compositional distributional semantics.</title>
<date>2014</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>9</volume>
<contexts>
<context position="1786" citStr="Baroni et al., 2014" startWordPosition="253" endWordPosition="256">rney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word. For example, nouns are an atomic type represented by a vector, and adjectives are matrices that act as functions transforming a noun vector into another noun vector (Baroni and Zamparelli, 2010). A transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence space (Polajnar et al., 2014). However, a concrete implementation of the Categorial framework requires setting and storing the values, or parameter</context>
<context position="6594" citStr="Baroni et al., 2014" startWordPosition="1017" endWordPosition="1020">en the compositional representation for the subject, verb, and object is a vector V (s, o) E RS, produced by applying tensor contraction (the higher-order analogue of matrix multiplication) to the verb tensor and two noun vectors. The lth component of the vector for the SVO triple is given by �V(s,o)l = Vljkoksj (1) j,k We aim to learn distributional vectors s and o for subjects and objects, and tensors V for verbs, such that the output vectors V (s, o) are distributional representations of the entire SVO triple. While there are several possible definitions of the sentence space (Clark, 2013; Baroni et al., 2014), we follow previous work (Grefenstette et al., 2013) by using a contextual sentence space consisting of content words that occur within the same sentences as the SVO triple. Low-Rank Tensor Representations Following Lei et al. (2014), we represent each verb’s tensor using a low-rank canonical polyadic (CP) decomposition to reduce the numbers of parameters that must be learned during training. As a higher-order analogue to singular value decomposition for matrices, CP decomposition factors a tensor into a sum of R tensor products of vectors.1 Given a third-order tensor V E RSxNxN, the CP decom</context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Baroni, Raffaela Bernardi, and Roberto Zamparelli. 2014. Frege in space: A program of compositional distributional semantics. Linguistic Issues in Language Technology, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1295" citStr="Bengio et al., 2003" startWordPosition="179" endWordPosition="182">the verb is a third-order tensor. The results show that, while the low-rank tensors require about two orders of magnitude fewer parameters per verb, they achieve performance comparable to, and occasionally surpassing, the unconstrained-rank tensors on sentence similarity and verb disambiguation tasks. 1 Introduction Distributional semantic methods represent word meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, (3):1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Elham Kashefi</author>
<author>Mirella Lapata</author>
</authors>
<title>A quantum-theoretic approach to distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013),</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="5451" citStr="Blacoe et al. (2013)" startWordPosition="823" endWordPosition="826">s and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameterize models for dependency parsing (Lei et al., 2014) and semantic role labelling (Lei et al., 2015). Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010), Smolensky and Legendre (2006), and Blacoe et al. (2013). 2 Model Tensor Models for Verbs We model each transitive verb as a bilinear function mapping subject and object noun vectors, each of dimensionality N, to a single sentence vector of dimensionality 5 (Coecke et al., 2011; Maillard et al., 2014) representing the composed subject-verb-object (SVO) triple. Each transitive verb has its own thirdorder tensor, which defines this bilinear function. Consider a verb V with associated tensor V E RSxNxN, and vectors s E RN, o E RN for subject and object nouns, respectively. Then the compositional representation for the subject, verb, and object is a ve</context>
</contexts>
<marker>Blacoe, Kashefi, Lapata, 2013</marker>
<rawString>William Blacoe, Elham Kashefi, and Mirella Lapata. 2013. A quantum-theoretic approach to distributional semantics. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013), Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Type-driven syntax and semantics for composing meaning vectors. Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse,</title>
<date>2013</date>
<pages>359--377</pages>
<contexts>
<context position="6572" citStr="Clark, 2013" startWordPosition="1015" endWordPosition="1016">pectively. Then the compositional representation for the subject, verb, and object is a vector V (s, o) E RS, produced by applying tensor contraction (the higher-order analogue of matrix multiplication) to the verb tensor and two noun vectors. The lth component of the vector for the SVO triple is given by �V(s,o)l = Vljkoksj (1) j,k We aim to learn distributional vectors s and o for subjects and objects, and tensors V for verbs, such that the output vectors V (s, o) are distributional representations of the entire SVO triple. While there are several possible definitions of the sentence space (Clark, 2013; Baroni et al., 2014), we follow previous work (Grefenstette et al., 2013) by using a contextual sentence space consisting of content words that occur within the same sentences as the SVO triple. Low-Rank Tensor Representations Following Lei et al. (2014), we represent each verb’s tensor using a low-rank canonical polyadic (CP) decomposition to reduce the numbers of parameters that must be learned during training. As a higher-order analogue to singular value decomposition for matrices, CP decomposition factors a tensor into a sum of R tensor products of vectors.1 Given a third-order tensor V </context>
</contexts>
<marker>Clark, 2013</marker>
<rawString>Stephen Clark. 2013. Type-driven syntax and semantics for composing meaning vectors. Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse, pages 359–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis,</title>
<date>2011</date>
<pages>36--1</pages>
<contexts>
<context position="1764" citStr="Coecke et al., 2011" startWordPosition="249" endWordPosition="252">s (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word. For example, nouns are an atomic type represented by a vector, and adjectives are matrices that act as functions transforming a noun vector into another noun vector (Baroni and Zamparelli, 2010). A transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence space (Polajnar et al., 2014). However, a concrete implementation of the Categorial framework requires setting and storing th</context>
<context position="5673" citStr="Coecke et al., 2011" startWordPosition="861" endWordPosition="864"> (Lei et al., 2015). Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010), Smolensky and Legendre (2006), and Blacoe et al. (2013). 2 Model Tensor Models for Verbs We model each transitive verb as a bilinear function mapping subject and object noun vectors, each of dimensionality N, to a single sentence vector of dimensionality 5 (Coecke et al., 2011; Maillard et al., 2014) representing the composed subject-verb-object (SVO) triple. Each transitive verb has its own thirdorder tensor, which defines this bilinear function. Consider a verb V with associated tensor V E RSxNxN, and vectors s E RN, o E RN for subject and object nouns, respectively. Then the compositional representation for the subject, verb, and object is a vector V (s, o) E RS, produced by applying tensor contraction (the higher-order analogue of matrix multiplication) to the verb tensor and two noun vectors. The lth component of the vector for the SVO triple is given by �V(s,</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2011</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2011. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis, 36(1-4):345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1319" citStr="Collobert et al., 2011" startWordPosition="183" endWordPosition="186">rder tensor. The results show that, while the low-rank tensors require about two orders of magnitude fewer parameters per verb, they achieve performance comparable to, and occasionally surpassing, the unconstrained-rank tensors on sentence similarity and verb disambiguation tasks. 1 Introduction Distributional semantic methods represent word meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word. For example, nouns</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically motivated large-scale NLP with C&amp;C and Boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the Demonstration Session of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8842" citStr="Curran et al., 2007" startWordPosition="1405" endWordPosition="1408"> decomposition are not necessarily orthonormal. 732 3 Training We train the compositional model for verbs in three steps: extracting transitive verbs and their subject and object nouns from corpus data, producing distributional vectors for the nouns and the SVO triples, and then learning parameters of the verb functions, which map the nouns to the SVO triple vectors. Corpus Data We extract SVO triples from an October 2013 download of Wikipedia, tokenized using Stanford CoreNLP (Manning et al., 2014), lemmatized with the Morpha lemmatizer (Minnen et al., 2001), and parsed using the C&amp;C parser (Curran et al., 2007). We filter the SVO triples to a set containing 345 distinct verbs: the verbs from our test datasets, along with some additional high-frequency verbs included to produce more representative sentence spaces. For each verb, we selected up to 600 triples which occurred more than once and contained subject and object nouns that occurred at least 100 times (to allow sufficient context to produce a distributional representation for the triple). This resulted in approximately 150,000 SVO triples overall. Distributional Vectors We produce two types of distributional vectors for nouns and SVO triples u</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James R Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale NLP with C&amp;C and Boxer. In Proceedings of the Demonstration Session of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From distributional to semantic similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="10300" citStr="Curran, 2004" startWordPosition="1650" endWordPosition="1651">ations for previously unseen expressions. However, they can be used to generate data to train such a model, as we will describe. 1) Count vectors (SVD): we count the number of times each noun or SVO triple co-occurs with each of the 10,000 most frequent words (excluding stopwords) in the Wikipedia corpus, using sentences as context boundaries. If the verb in the SVO triple is itself a content word, we do not include it as context for the triple. This produces one set of context vectors for nouns and another for SVO triples. We weight entries in these vectors using the t-test weighting scheme (Curran, 2004; Polajnar and Clark, 2014), and then reduce the vectors to 100 dimensions via singular value decomposition (SVD), decomposing the noun vectors and SVO vectors separately. 2) Prediction vectors (PV): we train vector embeddings for nouns and SVO triples by adapting the Paragraph Vector distributed bag of words method of Le and Mikolov (2014), an extension of the skip-gram model of Mikolov et al. (2013). In our experiments, given an SVO triple, the model must predict contextual words sampled from all sentences containing that triple. In the process, the model learns vector embeddings for both th</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From distributional to semantic similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimenting with transitive verbs in a DisCoCat.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Workshop on Geometrical Models of Natural Language Semantics (GEMS 2011),</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="3334" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="502" endWordPosition="505"> more complex types have corresponding tensors of higher order, and therefore a barrier to the practical implementation of this framework is the large number of parameters required to represent an extended vocabulary and a variety of grammatical constructions. We aim to reduce the size of the models by demonstrating that reduced-rank tensors, which can be represented in a form requiring fewer parameters, can capture the semantics of complex types as well as the full-rank tensors do. We base our experiments on the transitive verb construction for which there are established tasks and datasets (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014). Previous work on the transitive verb construction within the Categorial framework includes a two-step linear-regression method for the construction of the full verb tensors (Grefenstette et al., 2013) and a multi-linear regression method combined with a two-dimensional plausibility space (Polajnar et al., 2014). Polajnar et al. (2014) 731 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, Beijing, China, July 26-31, 201</context>
<context position="13964" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="2259" endWordPosition="2263">r matrices are randomly initialized.3 4 Evaluation We compare the performance of the low-rank tensors against full tensors on two tasks. Both tasks require the model to rank pairs of sentences each consisting of a subject, transitive verb, and object by the semantic similarity of the sentences in the pair. The gold standard ranking is given by similarity scores provided by human evaluators and the scores are not averaged among the annotators. The model ranking is evaluated against the ranking from the gold standard similarity judgements using Spearman’s ρ. The verb disambiguation task (GS11) (Grefenstette and Sadrzadeh, 2011) involves distinguishing between senses of an ambiguous verb, given subject and object nouns as context. The dataset consists of 200 sentence pairs, where the two sentences in each pair have the same subject and object but differ in the verb. Each of these pairs was ranked by human evaluators on a 1-7 similarity scale so that properly disambiguated pairs (e.g. author write book – author publish book) have higher similarity scores than improperly disambiguated pairs (e.g. author write book – author spell book). The transitive sentence similarity dataset (Kartsaklis and Sadrzadeh, 2014) consists</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimenting with transitive verbs in a DisCoCat. In Proceedings of the 2011 Workshop on Geometrical Models of Natural Language Semantics (GEMS 2011), Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>Yao-Zhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS</booktitle>
<location>Pottsdam, Germany.</location>
<contexts>
<context position="3569" citStr="Grefenstette et al., 2013" startWordPosition="536" endWordPosition="539">ical constructions. We aim to reduce the size of the models by demonstrating that reduced-rank tensors, which can be represented in a form requiring fewer parameters, can capture the semantics of complex types as well as the full-rank tensors do. We base our experiments on the transitive verb construction for which there are established tasks and datasets (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014). Previous work on the transitive verb construction within the Categorial framework includes a two-step linear-regression method for the construction of the full verb tensors (Grefenstette et al., 2013) and a multi-linear regression method combined with a two-dimensional plausibility space (Polajnar et al., 2014). Polajnar et al. (2014) 731 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics also introduce several alternative ways of reducing the number of tensor parameters by using matrices. The best performing method uses two matrices, one representing the subject-verb </context>
<context position="6647" citStr="Grefenstette et al., 2013" startWordPosition="1025" endWordPosition="1028">ject, verb, and object is a vector V (s, o) E RS, produced by applying tensor contraction (the higher-order analogue of matrix multiplication) to the verb tensor and two noun vectors. The lth component of the vector for the SVO triple is given by �V(s,o)l = Vljkoksj (1) j,k We aim to learn distributional vectors s and o for subjects and objects, and tensors V for verbs, such that the output vectors V (s, o) are distributional representations of the entire SVO triple. While there are several possible definitions of the sentence space (Clark, 2013; Baroni et al., 2014), we follow previous work (Grefenstette et al., 2013) by using a contextual sentence space consisting of content words that occur within the same sentences as the SVO triple. Low-Rank Tensor Representations Following Lei et al. (2014), we represent each verb’s tensor using a low-rank canonical polyadic (CP) decomposition to reduce the numbers of parameters that must be learned during training. As a higher-order analogue to singular value decomposition for matrices, CP decomposition factors a tensor into a sum of R tensor products of vectors.1 Given a third-order tensor V E RSxNxN, the CP decomposition of V is: R V = Pr ® Qr ® Rr (2) r=1 where P </context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013), Pottsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A study of entanglement in a categorical framework of natural language.</title>
<date>2014</date>
<booktitle>In Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL 2014),</booktitle>
<location>Kyoto, Japan,</location>
<contexts>
<context position="3367" citStr="Kartsaklis and Sadrzadeh, 2014" startWordPosition="506" endWordPosition="509">ding tensors of higher order, and therefore a barrier to the practical implementation of this framework is the large number of parameters required to represent an extended vocabulary and a variety of grammatical constructions. We aim to reduce the size of the models by demonstrating that reduced-rank tensors, which can be represented in a form requiring fewer parameters, can capture the semantics of complex types as well as the full-rank tensors do. We base our experiments on the transitive verb construction for which there are established tasks and datasets (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014). Previous work on the transitive verb construction within the Categorial framework includes a two-step linear-regression method for the construction of the full verb tensors (Grefenstette et al., 2013) and a multi-linear regression method combined with a two-dimensional plausibility space (Polajnar et al., 2014). Polajnar et al. (2014) 731 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, Beijing, China, July 26-31, 2015. c�2015 Association for Computa</context>
<context position="14555" citStr="Kartsaklis and Sadrzadeh, 2014" startWordPosition="2356" endWordPosition="2360">GS11) (Grefenstette and Sadrzadeh, 2011) involves distinguishing between senses of an ambiguous verb, given subject and object nouns as context. The dataset consists of 200 sentence pairs, where the two sentences in each pair have the same subject and object but differ in the verb. Each of these pairs was ranked by human evaluators on a 1-7 similarity scale so that properly disambiguated pairs (e.g. author write book – author publish book) have higher similarity scores than improperly disambiguated pairs (e.g. author write book – author spell book). The transitive sentence similarity dataset (Kartsaklis and Sadrzadeh, 2014) consists of 72 subjectverb-object sentences arranged into 108 sentence pairs. As in GS11, each pair has a gold standard semantic similarity score on a 1-7 scale. For example, the pair medication achieve result – drug produce effect has a high similarity rating, while author write book – delegate buy land has a low rating. In this dataset, however, the two sentences in each pair have no lexical overlap: neither subjects, objects, nor verbs are shared. 3Since the low-rank tensor loss is non-convex, we suspect that parameter initialization may produce better results. GS11 KS14 # tensor SVD PV SV</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, 2014</marker>
<rawString>Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A study of entanglement in a categorical framework of natural language. In Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL 2014), Kyoto, Japan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara G Kolda</author>
<author>Brett W Bader</author>
</authors>
<title>Tensor decompositions and applications.</title>
<date>2009</date>
<journal>SIAM Review,</journal>
<volume>51</volume>
<issue>3</issue>
<contexts>
<context position="4660" citStr="Kolda and Bader, 2009" startWordPosition="695" endWordPosition="698">the number of tensor parameters by using matrices. The best performing method uses two matrices, one representing the subject-verb interactions and the other the verb-object interactions. Some interaction between the subject and the object is re-introduced through a softmax layer. A similar method is presented in Paperno et al. (2014). Milajevs et al. (2014) use vectors generated by a neural language model to construct verb matrices and several different composition operators to generate the composed subject-verb-object sentence representation. In this paper, we use tensor rank decomposition (Kolda and Bader, 2009) to represent each verb’s tensor as a sum of tensor products of vectors. We learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameterize models for dependency parsing (Lei et al., 2014) and semantic role labelling (Lei et al., 2015). Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial</context>
<context position="7494" citStr="Kolda and Bader, 2009" startWordPosition="1181" endWordPosition="1184">w-rank canonical polyadic (CP) decomposition to reduce the numbers of parameters that must be learned during training. As a higher-order analogue to singular value decomposition for matrices, CP decomposition factors a tensor into a sum of R tensor products of vectors.1 Given a third-order tensor V E RSxNxN, the CP decomposition of V is: R V = Pr ® Qr ® Rr (2) r=1 where P E RRxS, Q E RRxN, R E RRxN are parameter matrices, Pr gives the rth row of matrix P, and ® is the tensor product. The smallest R that allows the tensor to be expressed as this sum of outer products is the rank of the tensor (Kolda and Bader, 2009). By fixing a value for R that is sufficiently small compared to 5 and N (forcing the verb tensor to have rank of at most R), and directly learning the parameters of the low-rank approximation using gradient-based optimization, we learn a low-rank tensor requiring fewer parameters without ever having to store the full tensor. In addition to reducing the number of parameters, representing tensors in this form allows us to formulate the verb tensor’s action on noun vectors as matrix multiplication. For a tensor in the form of Eq. (2), the output SVO vector is given by V (s, o) = PT(Qs O Ro) (3) </context>
<context position="13089" citStr="Kolda and Bader, 2009" startWordPosition="2116" endWordPosition="2119">er, 2012) up to a maximum of 500 iterations through the training data, which we found to be sufficient for convergence for every verb. Rather than placing a regularization penalty on the tensor parameters, we use early stopping if the loss 2https://groups.google.com/d/ msg/word2vec-toolkit/Q49FIrNOQRo/ J6KG8mUj45sJ 733 increases on a validation set consisting of 10% of the available SVO triples for each verb. For low-rank tensors, we compare seven different maximal ranks: R=1, 5, 10, 20, 30, 40 and 50. To learn the parameters of the low-rank tensors, we use an alternating optimization method (Kolda and Bader, 2009; Lei et al., 2014): performing gradient descent on one of the parameter matrices (for example P) to minimize the loss function while holding the other two fixed (Q and R), then repeating for the other parameter matrices in turn. The parameter matrices are randomly initialized.3 4 Evaluation We compare the performance of the low-rank tensors against full tensors on two tasks. Both tasks require the model to rank pairs of sentences each consisting of a subject, transitive verb, and object by the semantic similarity of the sentences in the pair. The gold standard ranking is given by similarity s</context>
</contexts>
<marker>Kolda, Bader, 2009</marker>
<rawString>Tamara G Kolda and Brett W Bader. 2009. Tensor decompositions and applications. SIAM Review, 51(3):455–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML 2014),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="10642" citStr="Mikolov (2014)" startWordPosition="1706" endWordPosition="1707">ries. If the verb in the SVO triple is itself a content word, we do not include it as context for the triple. This produces one set of context vectors for nouns and another for SVO triples. We weight entries in these vectors using the t-test weighting scheme (Curran, 2004; Polajnar and Clark, 2014), and then reduce the vectors to 100 dimensions via singular value decomposition (SVD), decomposing the noun vectors and SVO vectors separately. 2) Prediction vectors (PV): we train vector embeddings for nouns and SVO triples by adapting the Paragraph Vector distributed bag of words method of Le and Mikolov (2014), an extension of the skip-gram model of Mikolov et al. (2013). In our experiments, given an SVO triple, the model must predict contextual words sampled from all sentences containing that triple. In the process, the model learns vector embeddings for both the SVO triples and for the words in the sentences such that SVO vectors have a high dot product with their contextual word vectors. While previous work (Milajevs et al., 2014) has used prediction-based vectors for words in a tensor-based CDS model, ours uses prediction-based vectors for both words and phrases to train a tensor regression mod</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning (ICML 2014), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014),</booktitle>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="5026" citStr="Lei et al., 2014" startWordPosition="758" endWordPosition="761">s generated by a neural language model to construct verb matrices and several different composition operators to generate the composed subject-verb-object sentence representation. In this paper, we use tensor rank decomposition (Kolda and Bader, 2009) to represent each verb’s tensor as a sum of tensor products of vectors. We learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameterize models for dependency parsing (Lei et al., 2014) and semantic role labelling (Lei et al., 2015). Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010), Smolensky and Legendre (2006), and Blacoe et al. (2013). 2 Model Tensor Models for Verbs We model each transitive verb as a bilinear function mapping subject and object noun vectors, each of dimensionality N, to a single sentence </context>
<context position="6828" citStr="Lei et al. (2014)" startWordPosition="1053" endWordPosition="1056">h component of the vector for the SVO triple is given by �V(s,o)l = Vljkoksj (1) j,k We aim to learn distributional vectors s and o for subjects and objects, and tensors V for verbs, such that the output vectors V (s, o) are distributional representations of the entire SVO triple. While there are several possible definitions of the sentence space (Clark, 2013; Baroni et al., 2014), we follow previous work (Grefenstette et al., 2013) by using a contextual sentence space consisting of content words that occur within the same sentences as the SVO triple. Low-Rank Tensor Representations Following Lei et al. (2014), we represent each verb’s tensor using a low-rank canonical polyadic (CP) decomposition to reduce the numbers of parameters that must be learned during training. As a higher-order analogue to singular value decomposition for matrices, CP decomposition factors a tensor into a sum of R tensor products of vectors.1 Given a third-order tensor V E RSxNxN, the CP decomposition of V is: R V = Pr ® Qr ® Rr (2) r=1 where P E RRxS, Q E RRxN, R E RRxN are parameter matrices, Pr gives the rth row of matrix P, and ® is the tensor product. The smallest R that allows the tensor to be expressed as this sum o</context>
<context position="13108" citStr="Lei et al., 2014" startWordPosition="2120" endWordPosition="2123">um of 500 iterations through the training data, which we found to be sufficient for convergence for every verb. Rather than placing a regularization penalty on the tensor parameters, we use early stopping if the loss 2https://groups.google.com/d/ msg/word2vec-toolkit/Q49FIrNOQRo/ J6KG8mUj45sJ 733 increases on a validation set consisting of 10% of the available SVO triples for each verb. For low-rank tensors, we compare seven different maximal ranks: R=1, 5, 10, 20, 30, 40 and 50. To learn the parameters of the low-rank tensors, we use an alternating optimization method (Kolda and Bader, 2009; Lei et al., 2014): performing gradient descent on one of the parameter matrices (for example P) to minimize the loss function while holding the other two fixed (Q and R), then repeating for the other parameter matrices in turn. The parameter matrices are randomly initialized.3 4 Evaluation We compare the performance of the low-rank tensors against full tensors on two tasks. Both tasks require the model to rank pairs of sentences each consisting of a subject, transitive verb, and object by the semantic similarity of the sentences in the pair. The gold standard ranking is given by similarity scores provided by h</context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yuan Zhang</author>
<author>Lluis Marquez</author>
<author>Alessandro Moschitti</author>
<author>Regina Barzilay</author>
</authors>
<title>High-order lowrank tensors for semantic role labeling.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies (NAACLHLT 2015),</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="5073" citStr="Lei et al., 2015" startWordPosition="766" endWordPosition="769">ruct verb matrices and several different composition operators to generate the composed subject-verb-object sentence representation. In this paper, we use tensor rank decomposition (Kolda and Bader, 2009) to represent each verb’s tensor as a sum of tensor products of vectors. We learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameterize models for dependency parsing (Lei et al., 2014) and semantic role labelling (Lei et al., 2015). Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010), Smolensky and Legendre (2006), and Blacoe et al. (2013). 2 Model Tensor Models for Verbs We model each transitive verb as a bilinear function mapping subject and object noun vectors, each of dimensionality N, to a single sentence vector of dimensionality 5 (Coecke et al., 2011</context>
</contexts>
<marker>Lei, Zhang, Marquez, Moschitti, Barzilay, 2015</marker>
<rawString>Tao Lei, Yuan Zhang, Lluis Marquez, Alessandro Moschitti, and Regina Barzilay. 2015. High-order lowrank tensors for semantic role labeling. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies (NAACLHLT 2015), Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving distributional similarity with lessons learned from word embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--211</pages>
<contexts>
<context position="18607" citStr="Levy et al. (2015)" startWordPosition="3048" endWordPosition="3051">te tensor is trained for each verb, this allows a substantial amount of time to be saved even when using the constrained vocabulary of 345 verbs. 6 Conclusion We find that low-rank tensors for verbs achieve comparable or better performance than full-rank tensors on both verb disambiguation and sentence similarity tasks, while reducing the number of parameters that must be learned and stored for each verb by at least two orders of magnitude, and cutting training time in half. While in our experiments the prediction-based vectors outperform the count-based vectors on both tasks for most models, Levy et al. (2015) indicate that tuning hyperparameters of the countbased vectors may be able to produce comparable performance. Regardless, we show that the low-rank tensors are able to achieve performance comparable to the full rank for both types of vectors. This is important for extending the model to many more grammatical types (including those with corresponding tensors of higher order than investigated here) to build a wide-coverage tensorbased semantic system using, for example, the CCG parser of Curran et al. (2007). Acknowledgments Daniel Fried is supported by a Churchill Scholarship. Tamara Polajnar </context>
</contexts>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Maillard</author>
<author>Stephen Clark</author>
<author>Edward Grefenstette</author>
</authors>
<title>A type-driven tensor-based semantics for CCG.</title>
<date>2014</date>
<booktitle>In Proceedings of the EACL 2014 Type Theory and Natural Language Semantics Workshop (TTNLS), Gothenburg,</booktitle>
<contexts>
<context position="5697" citStr="Maillard et al., 2014" startWordPosition="865" endWordPosition="868">Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010), Smolensky and Legendre (2006), and Blacoe et al. (2013). 2 Model Tensor Models for Verbs We model each transitive verb as a bilinear function mapping subject and object noun vectors, each of dimensionality N, to a single sentence vector of dimensionality 5 (Coecke et al., 2011; Maillard et al., 2014) representing the composed subject-verb-object (SVO) triple. Each transitive verb has its own thirdorder tensor, which defines this bilinear function. Consider a verb V with associated tensor V E RSxNxN, and vectors s E RN, o E RN for subject and object nouns, respectively. Then the compositional representation for the subject, verb, and object is a vector V (s, o) E RS, produced by applying tensor contraction (the higher-order analogue of matrix multiplication) to the verb tensor and two noun vectors. The lth component of the vector for the SVO triple is given by �V(s,o)l = Vljkoksj (1) j,k W</context>
</contexts>
<marker>Maillard, Clark, Grefenstette, 2014</marker>
<rawString>Jean Maillard, Stephen Clark, and Edward Grefenstette. 2014. A type-driven tensor-based semantics for CCG. In Proceedings of the EACL 2014 Type Theory and Natural Language Semantics Workshop (TTNLS), Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014): System Demonstrations,</booktitle>
<pages>55--60</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="8726" citStr="Manning et al., 2014" startWordPosition="1386" endWordPosition="1389">the elementwise vector product. 1However, unlike matrix singular value decomposition, the component vectors in the CP decomposition are not necessarily orthonormal. 732 3 Training We train the compositional model for verbs in three steps: extracting transitive verbs and their subject and object nouns from corpus data, producing distributional vectors for the nouns and the SVO triples, and then learning parameters of the verb functions, which map the nouns to the SVO triple vectors. Corpus Data We extract SVO triples from an October 2013 download of Wikipedia, tokenized using Stanford CoreNLP (Manning et al., 2014), lemmatized with the Morpha lemmatizer (Minnen et al., 2001), and parsed using the C&amp;C parser (Curran et al., 2007). We filter the SVO triples to a set containing 345 distinct verbs: the verbs from our test datasets, along with some additional high-frequency verbs included to produce more representative sentence spaces. For each verb, we selected up to 600 triples which occurred more than once and contained subject and object nouns that occurred at least 100 times (to allow sufficient context to produce a distributional representation for the triple). This resulted in approximately 150,000 SV</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014): System Demonstrations, pages 55–60, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Neural Information Processing Systems (NIPS</booktitle>
<contexts>
<context position="1342" citStr="Mikolov et al., 2013" startWordPosition="187" endWordPosition="190"> show that, while the low-rank tensors require about two orders of magnitude fewer parameters per verb, they achieve performance comparable to, and occasionally surpassing, the unconstrained-rank tensors on sentence similarity and verb disambiguation tasks. 1 Introduction Distributional semantic methods represent word meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word. For example, nouns are an atomic type rep</context>
<context position="10704" citStr="Mikolov et al. (2013)" startWordPosition="1715" endWordPosition="1718"> word, we do not include it as context for the triple. This produces one set of context vectors for nouns and another for SVO triples. We weight entries in these vectors using the t-test weighting scheme (Curran, 2004; Polajnar and Clark, 2014), and then reduce the vectors to 100 dimensions via singular value decomposition (SVD), decomposing the noun vectors and SVO vectors separately. 2) Prediction vectors (PV): we train vector embeddings for nouns and SVO triples by adapting the Paragraph Vector distributed bag of words method of Le and Mikolov (2014), an extension of the skip-gram model of Mikolov et al. (2013). In our experiments, given an SVO triple, the model must predict contextual words sampled from all sentences containing that triple. In the process, the model learns vector embeddings for both the SVO triples and for the words in the sentences such that SVO vectors have a high dot product with their contextual word vectors. While previous work (Milajevs et al., 2014) has used prediction-based vectors for words in a tensor-based CDS model, ours uses prediction-based vectors for both words and phrases to train a tensor regression model. We learn 100-dimensional vectors for nouns and SVO triples</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Neural Information Processing Systems (NIPS 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitrijs Milajevs</author>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Matthew Purver</author>
</authors>
<title>Evaluating neural word representations in tensor-based compositional settings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="4398" citStr="Milajevs et al. (2014)" startWordPosition="655" endWordPosition="659">utational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics also introduce several alternative ways of reducing the number of tensor parameters by using matrices. The best performing method uses two matrices, one representing the subject-verb interactions and the other the verb-object interactions. Some interaction between the subject and the object is re-introduced through a softmax layer. A similar method is presented in Paperno et al. (2014). Milajevs et al. (2014) use vectors generated by a neural language model to construct verb matrices and several different composition operators to generate the composed subject-verb-object sentence representation. In this paper, we use tensor rank decomposition (Kolda and Bader, 2009) to represent each verb’s tensor as a sum of tensor products of vectors. We learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameterize models for dependenc</context>
<context position="11074" citStr="Milajevs et al., 2014" startWordPosition="1776" endWordPosition="1779">vectors separately. 2) Prediction vectors (PV): we train vector embeddings for nouns and SVO triples by adapting the Paragraph Vector distributed bag of words method of Le and Mikolov (2014), an extension of the skip-gram model of Mikolov et al. (2013). In our experiments, given an SVO triple, the model must predict contextual words sampled from all sentences containing that triple. In the process, the model learns vector embeddings for both the SVO triples and for the words in the sentences such that SVO vectors have a high dot product with their contextual word vectors. While previous work (Milajevs et al., 2014) has used prediction-based vectors for words in a tensor-based CDS model, ours uses prediction-based vectors for both words and phrases to train a tensor regression model. We learn 100-dimensional vectors for nouns and SVO triples with a modified version of word2vec,2 using the hierarchical sampling method with the default hyperparameters and 20 iterations through the training data. Training Methods We learn the tensor V of parameters for a given verb V using multi-linear regression, treating the noun vectors s and o as input and the composed SVO triple vector V (s, o) as the regression output</context>
<context position="16341" citStr="Milajevs et al., 2014" startWordPosition="2663" endWordPosition="2666">so bold the baseline when it outperforms the tensor method). 5 Results Table 1 displays correlations between the systems’ scores and human SVO similarity judgements on the verb disambiguation (GS11) and sentence similarity (KS14) tasks, for both the count (SVD) and prediction vectors (PV). We also give results for simple composition of word vectors using elementwise addition and multiplication (Mitchell and Lapata, 2008) (using verb vectors produced in the same manner as for nouns). As is consistent with prior work, the tensor-based models are surpassed by vector addition on the KS14 dataset (Milajevs et al., 2014), but perform better than both addition and multiplication on the GS11 dataset.4 Unsurprisingly, the rank-1 tensor has lowest performance for both tasks and vector sets, and performance generally increases as we increase the maximal rank R. The full tensor achieves the best, or tied for the best, performance on both tasks when using the PV vectors. However, for the SVD vectors, low-rank tensors surpass the performance of the full-rank tensor for R=40 and R=50 4The results in this table are not directly comparable with Milajevs et al. (2014), who compare against averaged annotator scores. Compa</context>
<context position="17699" citStr="Milajevs et al., 2014" startWordPosition="2891" endWordPosition="2894">ion result on KS14 is 0.68 for the K=40 tensor with PV vectors (the best result is addition with PV vectors, which achieves 0.71). These results exceed the scores reported for tensor-based models by Milajevs et al. (2014). 734 on GS11, and R=50 on KS14. On GS11, the SVD and PV vectors have varying but mostly comparable performance, with PV having higher performance on 5 out of 8 models. However, on KS14, the PV vectors have better performance than the SVD vectors for every model by at least 0.05 points, which is consistent with prior work comparing count and predict vectors on these datasets (Milajevs et al., 2014). The low-rank tensor models are also at least twice as fast to train as the full tensors: on a single core, training a rank-1 tensor takes about 5 seconds for each verb on average, ranks 5-50 each take between 1 and 2 minutes, and the full tensors each take about 4 minutes. Since a separate tensor is trained for each verb, this allows a substantial amount of time to be saved even when using the constrained vocabulary of 345 verbs. 6 Conclusion We find that low-rank tensors for verbs achieve comparable or better performance than full-rank tensors on both verb disambiguation and sentence simila</context>
</contexts>
<marker>Milajevs, Kartsaklis, Sadrzadeh, Purver, 2014</marker>
<rawString>Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Matthew Purver. 2014. Evaluating neural word representations in tensor-based compositional settings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>03</issue>
<contexts>
<context position="8787" citStr="Minnen et al., 2001" startWordPosition="1395" endWordPosition="1398">ar value decomposition, the component vectors in the CP decomposition are not necessarily orthonormal. 732 3 Training We train the compositional model for verbs in three steps: extracting transitive verbs and their subject and object nouns from corpus data, producing distributional vectors for the nouns and the SVO triples, and then learning parameters of the verb functions, which map the nouns to the SVO triple vectors. Corpus Data We extract SVO triples from an October 2013 download of Wikipedia, tokenized using Stanford CoreNLP (Manning et al., 2014), lemmatized with the Morpha lemmatizer (Minnen et al., 2001), and parsed using the C&amp;C parser (Curran et al., 2007). We filter the SVO triples to a set containing 345 distinct verbs: the verbs from our test datasets, along with some additional high-frequency verbs included to produce more representative sentence spaces. For each verb, we selected up to 600 triples which occurred more than once and contained subject and object nouns that occurred at least 100 times (to allow sufficient context to produce a distributional representation for the triple). This resulted in approximately 150,000 SVO triples overall. Distributional Vectors We produce two type</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(03):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Assocation for Computational Linguistics: Human Language Technologies (ACL-08: HLT),</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1609" citStr="Mitchell and Lapata, 2008" startWordPosition="223" endWordPosition="226">roduction Distributional semantic methods represent word meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word. For example, nouns are an atomic type represented by a vector, and adjectives are matrices that act as functions transforming a noun vector into another noun vector (Baroni and Zamparelli, 2010). A transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and ret</context>
<context position="16143" citStr="Mitchell and Lapata, 2008" startWordPosition="2630" endWordPosition="2633">ntence similarity (KS14) tasks, given by Spearman’s ρ, and the number of parameters needed to represent each verb’s tensor. We show the highest tensor result for each task and vector set in bold (and also bold the baseline when it outperforms the tensor method). 5 Results Table 1 displays correlations between the systems’ scores and human SVO similarity judgements on the verb disambiguation (GS11) and sentence similarity (KS14) tasks, for both the count (SVD) and prediction vectors (PV). We also give results for simple composition of word vectors using elementwise addition and multiplication (Mitchell and Lapata, 2008) (using verb vectors produced in the same manner as for nouns). As is consistent with prior work, the tensor-based models are surpassed by vector addition on the KS14 dataset (Milajevs et al., 2014), but perform better than both addition and multiplication on the GS11 dataset.4 Unsurprisingly, the rank-1 tensor has lowest performance for both tasks and vector sets, and performance generally increases as we increase the maximal rank R. The full tensor achieves the best, or tied for the best, performance on both tasks when using the PV vectors. However, for the SVD vectors, low-rank tensors surp</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of the 46th Annual Meeting of the Assocation for Computational Linguistics: Human Language Technologies (ACL-08: HLT), Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Paperno</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>A practical and linguistically-motivated approach to compositional distributional semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014),</booktitle>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="4374" citStr="Paperno et al. (2014)" startWordPosition="651" endWordPosition="654">he Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics also introduce several alternative ways of reducing the number of tensor parameters by using matrices. The best performing method uses two matrices, one representing the subject-verb interactions and the other the verb-object interactions. Some interaction between the subject and the object is re-introduced through a softmax layer. A similar method is presented in Paperno et al. (2014). Milajevs et al. (2014) use vectors generated by a neural language model to construct verb matrices and several different composition operators to generate the composed subject-verb-object sentence representation. In this paper, we use tensor rank decomposition (Kolda and Bader, 2009) to represent each verb’s tensor as a sum of tensor products of vectors. We learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameter</context>
</contexts>
<marker>Paperno, Pham, Baroni, 2014</marker>
<rawString>Denis Paperno, Nghia The Pham, and Marco Baroni. 2014. A practical and linguistically-motivated approach to compositional distributional semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Polajnar</author>
<author>Stephen Clark</author>
</authors>
<title>Improving distributional semantic vectors through context selection and normalisation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<location>Gothenburg,</location>
<contexts>
<context position="10327" citStr="Polajnar and Clark, 2014" startWordPosition="1652" endWordPosition="1655">viously unseen expressions. However, they can be used to generate data to train such a model, as we will describe. 1) Count vectors (SVD): we count the number of times each noun or SVO triple co-occurs with each of the 10,000 most frequent words (excluding stopwords) in the Wikipedia corpus, using sentences as context boundaries. If the verb in the SVO triple is itself a content word, we do not include it as context for the triple. This produces one set of context vectors for nouns and another for SVO triples. We weight entries in these vectors using the t-test weighting scheme (Curran, 2004; Polajnar and Clark, 2014), and then reduce the vectors to 100 dimensions via singular value decomposition (SVD), decomposing the noun vectors and SVO vectors separately. 2) Prediction vectors (PV): we train vector embeddings for nouns and SVO triples by adapting the Paragraph Vector distributed bag of words method of Le and Mikolov (2014), an extension of the skip-gram model of Mikolov et al. (2013). In our experiments, given an SVO triple, the model must predict contextual words sampled from all sentences containing that triple. In the process, the model learns vector embeddings for both the SVO triples and for the w</context>
</contexts>
<marker>Polajnar, Clark, 2014</marker>
<rawString>Tamara Polajnar and Stephen Clark. 2014. Improving distributional semantic vectors through context selection and normalisation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014), Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Polajnar</author>
<author>Luana Fagarasan</author>
<author>Stephen Clark</author>
</authors>
<title>Reducing dimensions of tensors in typedriven distributional semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<location>Doha, Qatar.</location>
<contexts>
<context position="2268" citStr="Polajnar et al., 2014" startWordPosition="337" endWordPosition="340"> et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word. For example, nouns are an atomic type represented by a vector, and adjectives are matrices that act as functions transforming a noun vector into another noun vector (Baroni and Zamparelli, 2010). A transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence space (Polajnar et al., 2014). However, a concrete implementation of the Categorial framework requires setting and storing the values, or parameters, defining these matrices and tensors. These parameters can be quite numerous for even low-dimensional sentence spaces. For example, a third-order tensor for a given transitive verb, mapping two 100-dimensional noun spaces to a 100-dimensional sentence space, would have 1003 parameters in its full form. All of the more complex types have corresponding tensors of higher order, and therefore a barrier to the practical implementation of this framework is the large number of param</context>
<context position="3681" citStr="Polajnar et al., 2014" startWordPosition="551" endWordPosition="554"> represented in a form requiring fewer parameters, can capture the semantics of complex types as well as the full-rank tensors do. We base our experiments on the transitive verb construction for which there are established tasks and datasets (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014). Previous work on the transitive verb construction within the Categorial framework includes a two-step linear-regression method for the construction of the full verb tensors (Grefenstette et al., 2013) and a multi-linear regression method combined with a two-dimensional plausibility space (Polajnar et al., 2014). Polajnar et al. (2014) 731 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics also introduce several alternative ways of reducing the number of tensor parameters by using matrices. The best performing method uses two matrices, one representing the subject-verb interactions and the other the verb-object interactions. Some interaction between the subject and the object is </context>
</contexts>
<marker>Polajnar, Fagarasan, Clark, 2014</marker>
<rawString>Tamara Polajnar, Luana Fagarasan, and Stephen Clark. 2014. Reducing dimensions of tensors in typedriven distributional semantics. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>124</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
<author>Geraldine Legendre</author>
</authors>
<title>The Harmonic Mind: from neural computation to optimality-theoretic grammar.</title>
<date>2006</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5425" citStr="Smolensky and Legendre (2006)" startWordPosition="818" endWordPosition="821">t ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameterize models for dependency parsing (Lei et al., 2014) and semantic role labelling (Lei et al., 2015). Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010), Smolensky and Legendre (2006), and Blacoe et al. (2013). 2 Model Tensor Models for Verbs We model each transitive verb as a bilinear function mapping subject and object noun vectors, each of dimensionality N, to a single sentence vector of dimensionality 5 (Coecke et al., 2011; Maillard et al., 2014) representing the composed subject-verb-object (SVO) triple. Each transitive verb has its own thirdorder tensor, which defines this bilinear function. Consider a verb V with associated tensor V E RSxNxN, and vectors s E RN, o E RN for subject and object nouns, respectively. Then the compositional representation for the subject</context>
</contexts>
<marker>Smolensky, Legendre, 2006</marker>
<rawString>Paul Smolensky and Geraldine Legendre. 2006. The Harmonic Mind: from neural computation to optimality-theoretic grammar. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality Through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL 2012),</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="1659" citStr="Socher et al., 2012" startWordPosition="232" endWordPosition="235"> meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word. For example, nouns are an atomic type represented by a vector, and adjectives are matrices that act as functions transforming a noun vector into another noun vector (Baroni and Zamparelli, 2010). A transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence space (Polajnar et a</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL 2012), Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1188" citStr="Turney and Pantel, 2010" startWordPosition="161" endWordPosition="164">l number of parameters. We investigate the effect of low-rank tensors on the transitive verb construction where the verb is a third-order tensor. The results show that, while the low-rank tensors require about two orders of magnitude fewer parameters per verb, they achieve performance comparable to, and occasionally surpassing, the unconstrained-rank tensors on sentence similarity and verb disambiguation tasks. 1 Introduction Distributional semantic methods represent word meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A non-negative tensor factorization model for selectional preference induction.</title>
<date>2010</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<marker>Van de Cruys, 2010</marker>
<rawString>Tim Van de Cruys. 2010. A non-negative tensor factorization model for selectional preference induction. Journal of Natural Language Engineering, 16(4):417–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio M Zanzotto</author>
<author>Lorenzo Dell’Arciprete</author>
</authors>
<title>Distributed tree kernels.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning (ICML 2012),</booktitle>
<location>Edinburgh, Scotland.</location>
<marker>Zanzotto, Dell’Arciprete, 2012</marker>
<rawString>Fabio M Zanzotto and Lorenzo Dell’Arciprete. 2012. Distributed tree kernels. In Proceedings of the 29th International Conference on Machine Learning (ICML 2012), Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.</title>
<date>2012</date>
<contexts>
<context position="12477" citStr="Zeiler, 2012" startWordPosition="2022" endWordPosition="2023"> and the SVO triple, respectively. We aim to learn a verb tensor V (either in full or in decomposed, low-rank form) that minimizes the mean of the squared residuals between the predicted SVO vectors V (s(i),o(i)) and those vectors obtained distributionally from the corpus, t(i). Specifically, we attempt to minimize the following loss function: — 1 MV L(V ) — � ||V (s(i), o(i)) − t(i)||2 2 (4) MV i=1 V (s, o) is given by Eq. (1) for full tensors, and by Eq. (3) for tensors represented in low-rank form. In both the low-rank and full-rank tensor learning, we use mini-batch ADADELTA optimization (Zeiler, 2012) up to a maximum of 500 iterations through the training data, which we found to be sufficient for convergence for every verb. Rather than placing a regularization penalty on the tensor parameters, we use early stopping if the loss 2https://groups.google.com/d/ msg/word2vec-toolkit/Q49FIrNOQRo/ J6KG8mUj45sJ 733 increases on a validation set consisting of 10% of the available SVO triples for each verb. For low-rank tensors, we compare seven different maximal ranks: R=1, 5, 10, 20, 30, 40 and 50. To learn the parameters of the low-rank tensors, we use an alternating optimization method (Kolda and</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>