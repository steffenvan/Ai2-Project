<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001073">
<title confidence="0.85603">
Multi-Granular Aspect Aggregation in Aspect-Based Sentiment Analysis
</title>
<author confidence="0.994147">
John Pavlopoulos and Ion Androutsopoulos
</author>
<affiliation confidence="0.998517">
Department of Informatics
Athens University of Economics and Business
</affiliation>
<address confidence="0.920939">
Patission 76, GR-104 34 Athens, Greece
</address>
<email confidence="0.95023">
http://nlp.cs.aueb.gr/
</email>
<sectionHeader confidence="0.996616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998177125">
Aspect-based sentiment analysis estimates
the sentiment expressed for each particu-
lar aspect (e.g., battery, screen) of an en-
tity (e.g., smartphone). Different words
or phrases, however, may be used to re-
fer to the same aspect, and similar as-
pects may need to be aggregated at coarser
or finer granularities to fit the available
space or satisfy user preferences. We in-
troduce the problem of aspect aggrega-
tion at multiple granularities. We decom-
pose it in two processing phases, to al-
low previous work on term similarity and
hierarchical clustering to be reused. We
show that the second phase, where aspects
are clustered, is almost a solved prob-
lem, whereas further research is needed
in the first phase, where semantic simi-
larity measures are employed. We also
introduce a novel sense pruning mecha-
nism for WordNet-based similarity mea-
sures, which improves their performance
in the first phase. Finally, we provide pub-
licly available benchmark datasets.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998672214285714">
Given a set of texts discussing a particular en-
tity (e.g., reviews of a laptop), aspect-based senti-
ment analysis (ABSA) attempts to identify the most
prominent (e.g., frequently discussed) aspects of
the entity (e.g., battery, screen) and the average
sentiment (e.g., 1 to 5 stars) for each aspect or
group of aspects, as in Fig. 1. Most ABSA systems
perform all or some of the following (Liu, 2012):
subjectivity detection to retain only sentences (or
other spans) expressing subjective opinions; as-
pect extraction to extract (and possibly rank) terms
corresponding to aspects (e.g., ‘battery’); aspect
aggregation to group aspect terms that are near-
synonyms (e.g., ‘price’, ‘cost’) or to obtain aspects
</bodyText>
<figureCaption confidence="0.993762">
Figure 1: Aspect groups and scores of an entity.
</figureCaption>
<bodyText confidence="0.978263419354839">
at a coarser granularity (e.g., ‘chicken’,‘steak’,
and ‘fish’ may be replaced by ‘food’ in restaurant
reviews); and aspect sentiment score estimation to
estimate the average sentiment for each aspect or
group of aspects. In this paper, we focus on aspect
aggregation, the least studied stage of the four.
Aspect aggregation is needed to avoid reporting
separate sentiment scores for aspect terms that are
very similar. In Fig. 1, for example, showing sep-
arate lines for ‘money’, ‘price’, and ‘cost’ would
be confusing. The extent to which aspect terms
should be aggregated, however, also depends on
the available space and user preferences. On de-
vices with smaller screens, it may be desirable to
aggregate aspect terms that are similar, though not
necessarily near-synonyms (e.g., ‘design’, ‘color’,
‘feeling’) to show fewer lines (Fig. 1), but finer as-
pects may be preferable on larger screens. Users
may also wish to adjust the granularity of aspects,
e.g., by stretching or narrowing the height of Fig. 1
on a smartphone to view more or fewer lines.
Hence, aspect aggregation should be able to pro-
duce groups of aspect terms for multiple granular-
ities. We assume that the aggregated aspects are
displayed as lists of terms, as in Fig. 1. We make
no effort to order (e.g., by frequency) the terms in
each list, nor do we attempt to produce a single
(more general) term to describe each aggregated
aspect, leaving such tasks for future work.
ABSA systems usually group synonymous (or
near-synonymous) aspect terms (Liu, 2012). Ag-
</bodyText>
<page confidence="0.962072">
78
</page>
<note confidence="0.9930695">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 78–87,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999982979591837">
gregating only synonyms (or near-synonyms),
however, does not allow users to select the desir-
able aspect granularity, and ignores the hierarchi-
cal relations between aspect terms. For example,
‘pizza’ and ‘steak’ are kinds of ‘food’ and, hence,
the three terms can be aggregated to show fewer,
coarser aspects, even though they are not syn-
onyms. Carenini et al. (2005) used a predefined
domain-specific taxonomy to hierarchically aggre-
gate aspect terms, but taxonomies of this kind
are often not available. By contrast, we use only
general-purpose taxonomies (e.g., WordNet), term
similarity measures based on general-purpose tax-
onomies or corpora, and hierarchical clustering.
We define multi-granular aspect aggregation to
be the task of partitioning a given set of aspect
terms (generated by a previous aspect extraction
stage) into k non-overlapping clusters, for multi-
ple values of k. A further constraint is that the
clusters have to be consistent for different k val-
ues, meaning that if two aspect terms t1, t2 are
placed in the same cluster for k = k1, then t1
and t2 must also be grouped together (in the same
cluster) for every k = k2 with k2 &lt; k1, i.e., for
every coarser grouping. For example, if ‘waiter’
and ‘service’ are grouped together for k = 5, they
must also be grouped together for k = 4, 3, 2
and (trivially) k = 1, to allow the user to feel
that selecting a smaller number of aspect groups
(narrowing the height of Fig. 1) has the effect of
zooming out (without aspect terms jumping un-
expectedly to other aspect groups), and similarly
for zooming in.1 This requirement is satisfied by
using agglomerative hierarchical clustering algo-
rithms (Manning and Sch¨utze, 1999; Hastie et al.,
2001), which in our case produce term hierarchies
like the ones of Fig. 2. By using slices (nodes at a
particular depth) of the hierarchies that are closer
to the root or the leaves, we obtain fewer or more
clusters. The vertical dotted lines of Fig. 2 illus-
trate two slices for k = 4. By contrast, flat clus-
tering algorithms (e.g., k-means) do not satisfy the
consistency constraint for different k values.
Agglomerative clustering algorithms require a
measure of the distance between individuals, in
our case a measure of how similar two aspect
terms are, and a linkage criterion to specify which
clusters should be merged to form larger (coarser)
clusters. To experiment with different term sim-
</bodyText>
<footnote confidence="0.999487">
1We also require the clusters to be non-overlapping to
make this zooming in and out metaphor clearer to the user.
</footnote>
<figureCaption confidence="0.9890555">
Figure 2: Example aspect hierarchies produced by
agglomerative hierarchical clustering.
</figureCaption>
<bodyText confidence="0.609858166666667">
food fish sushi dishes wine
food 5 4 4 4 2
fish 4 5 4 2 1
sushi 4 4 5 3 1
dishes 4 2 3 5 2
wine 2 1 1 2 5
</bodyText>
<tableCaption confidence="0.998237">
Table 1: An aspect term similarity matrix.
</tableCaption>
<bodyText confidence="0.999911333333333">
ilarity measures and linkage criteria, we decom-
pose multi-granular aspect aggregation in two pro-
cessing phases. Phase A fills in a symmetric ma-
trix, like the one of Table 1, with scores show-
ing the similarity of each pair of input aspect
terms; the matrix in effect defines the distance
measure to be used by agglomerative clustering.
In Phase B, the aspect terms are grouped into k
non-overlapping clusters, for varying values of k,
given the matrix of Phase A and a linkage crite-
rion; a hierarchy like the ones of Fig. 2 is first
formed via agglomerative clustering, and fewer or
more clusters (for different values of k) are then
obtained by using different slices of the hierarchy,
as already discussed. Our two-phase decomposi-
tion can also accommodate non-hierarchical clus-
tering algorithms, provided that the consistency
constraint is satisfied, but we consider only ag-
glomerative hierarchical clustering in this paper.
The decomposition in two phases has three
main advantages. Firstly, it allows reusing previ-
ous work on term similarity measures (Zhang et
al., 2013), which can be used to fill in the ma-
trix of Phase A. Secondly, the decomposition al-
lows different linkage criteria to be experimen-
tally compared (in Phase B) using the same sim-
ilarity matrix (of Phase A), i.e., the same distance
</bodyText>
<page confidence="0.997356">
79
</page>
<bodyText confidence="0.999982111111111">
measure. Thirdly, the decomposition leads to high
inter-annotator agreement, as we show experimen-
tally. By contrast, in preliminary experiments we
found that asking humans to directly evaluate as-
pect hierarchies produced by hierarchical cluster-
ing, or to manually create gold aspect hierarchies
led to poor inter-annotator agreement.
We show that existing term similarity measures
perform reasonably well in Phase A, especially
when combined, but there is a large scope for im-
provement. We also propose a novel sense pruning
method for WordNet-based similarity measures,
which leads to significant improvements in Phase
A. In Phase B, we experiment with agglomera-
tive clustering using four different linkage criteria,
concluding that they all perform equally well and
that Phase B is almost a solved problem when the
gold similarity matrix of Phase A is used; how-
ever, further improvements are needed in the sim-
ilarity measures of Phase A to produce a suffi-
ciently good similarity matrix. We also make pub-
licly available the datasets of our experiments.
Our main contributions are: (i) to the best
of our knowledge, we are the first to consider
multi-granular aspect aggregation (not just merg-
ing near-synonyms) in ABSA without manually
crafted domain-specific ontologies; (ii) we pro-
pose a two-phase decomposition that allows previ-
ous work on term similarity and hierarchical clus-
tering to be reused and evaluated with high inter-
annotator agreement; (iii) we introduce a novel
sense pruning mechanism that improves WordNet-
based similarity measures; (iv) we provide the first
public datasets for multi-granular aspect aggrega-
tion; (v) we show that the second phase of our de-
composition is almost a solved problem, and that
research should focus on the first phase. Although
we experiment with customer reviews of products
and services, ABSA and the work of this paper in
particular are, at least in principle, also applicable
to texts expressing opinions about other kinds of
entities (e.g., politicians, organizations).
Section 2 below discusses related work. Sec-
tions 3 and 4 present our work for Phase A and B,
respectively. Section 5 concludes.
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999970234042553">
Most existing approaches to aspect aggregation
aim to produce a single, flat partitioning of as-
pect terms into aspect groups, rather than aspect
groups at multiple granularities. The most com-
mon approaches (Liu, 2012) are to aggregate only
synonyms or near-synonyms, using WordNet (Liu
et al., 2005), statistics from corpora (Chen et al.,
2006; Bollegala et al., 2007a; Lin and Wu, 2009),
or semi-supervised learning (Zhai et al., 2010;
Zhai et al., 2011), or to cluster the aspect terms
using (latent) topic models (Titov and McDonald,
2008a; Guo et al., 2009; Brody and Elhadad, 2010;
Jo and Oh, 2011). Topic models do not perform
better than other methods (Zhai et al., 2010), and
their clusters may overlap.2 The topic model of
Titov et al. (2008b) uses two granularity levels;
we consider many more (3–10 levels).
Carenini et al. (2005) used a predefined domain-
specific taxonomy and similarity measures to ag-
gregate related terms. Yu et al. (2011) used a tai-
lored version of an existing taxonomy. By con-
trast, we assume no domain-specific taxonomy.
Kobayashi et al. (2007) proposed methods to ex-
tract aspect terms and relations between them, in-
cluding hierarchical relations. They extract, how-
ever, relations by looking for clues in texts (e.g.,
particular phrases). By contrast, we employ simi-
larity measures and hierarchical clustering, which
allows us to group similar aspect terms even when
they do not cooccur in texts. Also, in contrast
to Kobayashi et al. (2007), we respect the consis-
tency constraint discussed in Section 1.
A similar task is taxonomy induction. Cimi-
ano and Staab (2005) automatically construct tax-
onomies from texts via agglomerative clustering,
much as in our Phase B, but not in the context of
ABSA, and without trying to learn a similarity ma-
trix first. They also label the hierarchy’s concepts,
a task we do not consider. Klapaftis and Manand-
har (2010) show how word sense induction can be
combined with agglomerative clustering to obtain
more accurate taxonomies, again not in the con-
text of ABSA. Our sense pruning method was in-
fluenced by their work, but is much simpler than
their word sense induction. Fountain and Lapata
(2012) study unsupervised methods to induce con-
cept taxonomies, without considering ABSA.
</bodyText>
<sectionHeader confidence="0.982539" genericHeader="method">
3 Phase A
</sectionHeader>
<bodyText confidence="0.872401666666667">
We now discuss our work for Phase A. Recall that
in this phase the input is a set of aspect terms and
2Topic models are typically also used to perform aspect
extraction, apart from aspect aggregation, but simple heuris-
tics (e.g., most frequent nouns) often outperform them in as-
pect extraction (Liu, 2012; Moghaddam and Ester, 2012).
</bodyText>
<page confidence="0.984583">
80
</page>
<bodyText confidence="0.999797">
the goal is to fill in a matrix (Table 1) with scores
showing the similarity of each pair of aspect terms.
</bodyText>
<subsectionHeader confidence="0.999309">
3.1 Datasets used in Phase A
</subsectionHeader>
<bodyText confidence="0.9999792">
We used two benchmark datasets that we had pre-
viously constructed to evaluate ABSA methods for
subjectivity detection, aspect extraction, and as-
pect score estimation, but not aspect aggregation.
We extended them to support aspect aggregation,
and we make them publicly available.3
The two original datasets contain sentences
from customer reviews of restaurants and laptops,
respectively. The reviews are manually split into
sentences, and each sentence is manually anno-
tated as ‘subjective’ (expressing opinion) or ‘ob-
jective’ (not expressing opinion). The restaurants
dataset contains 3,710 English sentences from the
restaurant reviews of Ganu et al. (2009). The lap-
tops dataset contains 3,085 English sentences from
394 customer reviews, collected from sites that
host customer reviews. In the experiments of this
paper, we use only the 3,057 (out of 3,710) sub-
jective restaurant sentences and the 2,631 (out of
3,085) subjective laptop sentences.
For each subjective sentence, our datasets show
the words that human annotators marked as aspect
terms. For example, in “The dessert was divine!”
the aspect term is ‘dessert’, and in “Really bad
waiter.” it is ‘waiter’. Among the 3,057 subjective
restaurant sentences, 1,129 contain exactly one as-
pect term, 829 more than one, and 1,099 no aspect
term; a subjective sentence may express an opin-
ion about the restaurant (or laptop) being reviewed
without mentioning a specific aspect (e.g., “Really
nice restaurant!”), which is why no aspect terms
are present in some subjective sentences. There
are 558 distinct multi-word aspect terms and 431
distinct single-word aspect terms in the subjective
restaurant sentences. Among the 2,631 subjective
sentences of the laptop reviews, 823 contain ex-
actly one aspect term, 389 more than one, and
1,419 no aspect term. There are 273 distinct multi-
word aspect terms and 330 distinct single-word as-
pect terms in the subjective laptop sentences.
From each dataset, we selected the 20 (distinct)
aspect terms that the human annotators had anno-
tated most frequently, taking annotation frequency
to be an indicator of importance; there are only
two multi-word aspect terms (‘hard drive’, ‘bat-
</bodyText>
<footnote confidence="0.660789">
3The datasets are available at http://nlp.cs.
aueb.gr/software.html.
</footnote>
<bodyText confidence="0.999952428571428">
tery life’) among the 20 most frequent ones in the
laptops dataset, and none among the 20 most fre-
quent aspect terms of the restaurants dataset. We
then formed all the 190 possible pairs of the 20
terms and constructed an empty similarity matrix
(Fig. 1), one for each dataset, which was given
to three human judges to fill in (1: strong dis-
similarity, 5: strong similarity).4 For each aspect
term, all the subjective sentences mentioning the
term were also provided, to help the judges un-
derstand how the terms are used in the particu-
lar domains (e.g., ‘window’ and ‘Windows’ have
domain-specific meanings in laptop reviews).
The Pearson correlation coefficient indicated
high inter-annotator agreement (0.81 for restau-
rants, 0.74 for laptops). We also measured the ab-
solute inter-annotator agreement a(l1, l2), defined
below, where l1, l2 are lists containing the scores
(similarity matrix values) of two judges, N is the
length of each list, and vmax, vmin are the largest
and smallest possible scores (5 and 1).
</bodyText>
<equation confidence="0.99639">
N a(l1,l2) = 1 1 − |v (i) = lmax v) I
(Z l
i=1
</equation>
<bodyText confidence="0.999984181818181">
The absolute interannotator agreement was also
high (0.90 for restaurants, 0.91 for laptops).5 With
both measures, we compute the agreement of each
judge with the averaged (for each matrix cell)
scores of the other two judges, and we report the
mean of the three agreement estimates. Finally, we
created the gold similarity matrix of each dataset
by placing in each cell the average scores that the
three judges had provided for that cell.
In preliminary experiments, we gave aspect
terms to human judges, asking them to group any
terms they considered near-synonyms. We then
asked the judges to group the aspect terms into
fewer, coarser groups by grouping terms that could
be viewed as direct hyponyms of the same broader
term (e.g., ‘pizza’ and ‘steak’ are both kinds of
‘food’), or that stood in a hyponym-hypernym re-
lation (e.g., ‘pizza’ and ‘food’). We used the
Dice coefficient to measure inter-annotator agree-
ment, and we obtained reasonably good agreement
for near-synonyms (0.77 for restaurants, 0.81 for
laptops), but poor agreement for the coarser as-
</bodyText>
<footnote confidence="0.9980156">
4The matrix is symmetric; hence, the judges had to fill in
only half of it. The guidelines and an annotation tool that
were given to the judges are available upon request.
5The Pearson correlation ranges from −1 to 1, whereas
the absolute inter-annotator agreement ranges from 0 to 1.
</footnote>
<page confidence="0.99841">
81
</page>
<bodyText confidence="0.999967428571429">
pects (0.25 and 0.11).6 In other preliminary ex-
periments, we asked human judges to rank alter-
native aspect hierarchies that had been produced
by applying agglomerative clustering with differ-
ent linkage criteria to 20 aspect terms, but we ob-
tained very poor inter-annotator agreement (Pear-
son score −0.83 for restaurants and 0 for laptops).
</bodyText>
<subsectionHeader confidence="0.997517">
3.2 Phase A methods
</subsectionHeader>
<bodyText confidence="0.999977888888889">
We employed five term similarity measures. The
first two are WordNet-based (Budanitsky and
Hirst, 2006). The next two combine WordNet with
statistics from corpora. The fifth one is a corpus-
based distributional similarity measure.
The first measure is Wu and Palmer’s (1994). It
is actually a sense similarity measure (a term may
have multiple senses). Given two senses sij, si0j0
of terms ti, ti0, the measure is defined as follows:
</bodyText>
<equation confidence="0.976649">
WP(sij, si0j0) = 2 · ,
depth(sij) + depth(sij)
</equation>
<bodyText confidence="0.999975166666666">
where lcs(sij, si0j0) is the least common sub-
sumer, i.e., the most specific common ancestor of
the two senses in WordNet, and depth(s) is the
depth of sense s in WordNet’s hierarchy.
Most terms have multiple senses, however,
and word sense disambiguation methods (Navigli,
2009) are not yet robust enough. Hence, when
given two aspect terms ti, ti0, rather than particular
senses of the terms, a simplistic greedy approach
is to compute the similarities of all the possible
pairs of senses sij, si0j0 of ti, ti0, and take the sim-
ilarity of ti, ti0 to be the maximum similarity of
the sense pairs (Bollegala et al., 2007b; Zesch and
Gurevych, 2010). We use this greedy approach
with all the WordNet-based measures, but we also
propose a sense pruning mechanism below, which
improves their performance. In all the WordNet-
based measures, if a term is not in WordNet, we
take its similarity to any other term to be zero.7
The second measure, PATH (sij, si0j0), is sim-
ply the inverse of the length (plus one) of the short-
est path connecting the senses sij, si0j0 in WordNet
(Zhang et al., 2013). Again, the greedy approach
can be used with terms having multiple senses.
</bodyText>
<footnote confidence="0.967763875">
6The Dice coefficient ranges from 0 to 1. There was a very
large number of possible responses the judges could provide
and, hence, it would be inappropriate to use Cohen’s K.
7This never happened in the restaurants dataset. In the
laptops dataset, it only happened for ‘hard drive’ and ‘bat-
tery life’. We use the NLTK implementation of the first four
measures (see http://nltk.org/) and our own imple-
mentation of the distributional similarity measure.
</footnote>
<bodyText confidence="0.821434">
The third measure is Lin’s (1998), defined as:
</bodyText>
<equation confidence="0.9984555">
2 · ic(lcs(sij, si0j0))
LIN (sij, si0j0) =ic(sij) + ic(si0j0) ,
</equation>
<bodyText confidence="0.988919375">
where sij, si0j0 are senses of terms ti, ti0,
lcs(sij, si0j0) is the least common subsumer of
sij, si0j0 in WordNet, and ic(s) = − log P(s) is
the information content of sense s (Pedersen et al.,
2004), estimated from a corpus. When the cor-
pus is not sense-tagged, we follow the common
approach of treating each occurrence of a word as
an occurrence of all of its senses, when estimat-
ing ic(s).8 We experimented with two variants of
Lin’s measure, one where the ic(s) scores were
estimated from the Brown corpus (Marcus et al.,
1993), and one where they were estimated from
the (restaurant or laptop) reviews of our datasets.
The fourth measure is Jiang and Conrath’s
(1997), defined below. Again, we experimented
with two variants of ic(s), as above.
</bodyText>
<equation confidence="0.998635">
JCN(sij, si0j0) =
1
ic(sij) + ic(si0j0) − 2 · lcs(sij, si0j0)
</equation>
<bodyText confidence="0.999974222222222">
For all the above WordNet-based measures, we
experimented with a sense pruning mechanism,
which discards some of the senses of the aspect
terms, before applying the greedy approach. For
each aspect term ti, we consider all of its Word-
Net senses sij. For each sij and each other aspect
term ti0, we compute (using PATH) the similar-
ity between sij and each sense si0j0 of ti0, and we
consider the relevance of sij to ti0 to be:9
</bodyText>
<equation confidence="0.887055714285714">
rel(sij, ti0) = max
si0j0 ∈ senses(ti0)
The relevance of sij to all of the N other aspect
terms ti0 is taken to be:
1
rel(sij) = N ·
i0�=i
</equation>
<bodyText confidence="0.9223775">
For each aspect term ti, we retain only its senses
sij with the top rel(sij) scores, which tends to
</bodyText>
<footnote confidence="0.9785275">
8http://www.d.umn.edu/˜tpederse/Data/
README-WN-IC-30.txt. We use the default counting.
9We also experimented with other similarity measures
when computing rel(sij, ti ), instead of PATH, but there
was no significant difference. We use NLTK to tokenize, re-
move punctuation, and stop-words.
</footnote>
<equation confidence="0.980566333333333">
depth(lcs(sij, si0j0))
PATH (sij, si0j0)
rel(sij, ti0)
</equation>
<page confidence="0.98566">
82
</page>
<table confidence="0.999931416666667">
without SP with SP
Method Rest. Lapt. Rest. Lapt.
WP 0.475 0.216 0.502 0.265
PATH 0.524 0.301 0.529 0.332
LIN@domain 0.390 0.256 0.456 0.343
LIN@Brown 0.434 0.329 0.471 0.391
JCN@domain 0.467 0.348 0.509 0.448
JCN@Brown 0.403 0.469 0.419 0.539
DS 0.283 0.517 (0.283) (0.517)
AVG 0.499 0.352 0.537 0.426
WN 0.490 0.328 0.530 0.395
WNDS 0.523 0.453 0.545 0.546
</table>
<tableCaption confidence="0.98571">
Table 2: Phase A results (Pearson correlation to
</tableCaption>
<bodyText confidence="0.959146142857143">
gold similarities) with and without sense pruning.
prune senses that are very irrelevant to the par-
ticular domain (e.g., laptops). This sense prun-
ing mechanism is novel, and we show experimen-
tally that it improves the performance of all the
WordNet-based similarity measures we examined.
We also implemented a distributional simi-
larity measure (Harris, 1968; Pad´o and Lap-
ata, 2007; Cimiano et al., 2009; Zhang et al.,
2013). Following Lin and Wu (2009), for
each aspect term t, we create a vector V(t) =
(PMI (t, wl), ... , PMI (t, wn)). The vector com-
ponents are the Pointwise Mutual Information
scores of t and each word wi of a corpus:
</bodyText>
<equation confidence="0.999802">
P(t, wi)
PMI (t, wi) = − log
P(t) · P(wi)
</equation>
<bodyText confidence="0.999979583333333">
We treat P(t, wi) as the probability of t, wi cooc-
curring in the same sentence, and we use the (lap-
top or restaurant) reviews of our datasets as the
corpus to estimate the probabilities. The distribu-
tional similarity DS(t, t&apos;) of two aspect terms t, t&apos;
is the cosine similarity of v(t),v(t&apos;).10
Finally, we tried combinations of the similarity
measures: AVG is the average of all five; WN is
the average of the first four, which employ Word-
Net; and WNDS is the average of WN and DS;
all the scores range in [0, 1]. We also tried regres-
sion (e.g., SVR), but there was no improvement.
</bodyText>
<subsectionHeader confidence="0.997808">
3.3 Phase A experimental results
</subsectionHeader>
<bodyText confidence="0.978679847457627">
Each similarity measure was evaluated by comput-
ing its Pearson correlation with the scores of the
gold similarity matrix. Table 2 shows the results.
Our sense pruning consistently improves all
four WordNet-based measures. It does not apply to
10We also experimented with Euclidean distance, a nor-
malized PMI (Bouma, 2009), and the Brown corpus, but
there was no improvement.
DS, which is why the DS results are identical with
and without pruning. A paired t test indicates that
the other differences (with and without pruning) of
Table 2 are statistically significant (p &lt; 0.05). We
used the senses with the top five rel(sib) scores for
each aspect term ti during sense pruning. We also
experimented with keeping fewer senses, but the
results were inferior or there was no improvement.
Lin’s measure performed better when infor-
mation content was estimated on the (much
larger, but domain-independent) Brown corpus
(LIN@Brown), as opposed to using the (domain-
specific) reviews of our datasets (LIN@domain),
but we observed no similar consistent pattern for
JCN. Given its simplicity, PATH performed re-
markably well in the restaurants dataset; it was
the best measure (including combinations) without
sense pruning, and the best uncombined measure
with sense pruning. It performed worse, however,
compared to several other measures in the laptops
dataset. Similar comments apply to WP, which is
among the top-performing uncombined measures
in restaurants, both with and without sense prun-
ing, but the worst overall measure in laptops. DS
is the best overall measure in laptops when com-
pared to measures without sense pruning, and the
third best overall when compared to measures that
use sense pruning, but the worst overall in restau-
rants both with and without pruning. LIN and
JCN, which use both WordNet and corpus statis-
tics, have a more balanced performance across the
two datasets, but they are not top-performers in
any of the two. Combinations of similarity mea-
sures seem more stable across domains, as the re-
sults of AVG, WN, and WNDS indicate, though
experiments with more domains are needed to in-
vestigate this issue. WNDS is the best overall
method with sense pruning, and among the best
three methods without pruning in both datasets.
To get a better view of the performance of
WNDS with sense pruning, i.e., the best overall
measure of Table 2, we compared it to two state of
the art semantic similarity systems. First, we ap-
plied the system of Han et al. (2013), one of the
best systems of the recent *Sem 2013 semantic
text similarity competition, to our Phase A data.
The performance (Pearson correlation with gold
similarities) of the same system on the widely used
WordSim353 word similarity dataset (Agirre et al.,
2009) is 0.73, much higher than the same system’s
performance on our Phase A data (see Table 3),
</bodyText>
<page confidence="0.997528">
83
</page>
<table confidence="0.998271857142857">
Method Restaurants Laptops
Han et al. (2013) 0.450 0.471
Word2Vec 0.434 0.485
WNDS with SP 0.545 0.546
Judge 1 0.913 0.875
Judge 2 0.914 0.894
Judge 3 0.888 0.924
</table>
<tableCaption confidence="0.722597666666667">
Table 3: Phase A results (Pearson correlation to
gold similarities) of WNDS with SP against se-
mantic similarity systems and human judges.
</tableCaption>
<bodyText confidence="0.993602789473684">
which suggests that our data are more difficult.11
We also employed the recent Word2Vec sys-
tem, which computes continuous vector space rep-
resentations of words from large corpora and has
been reported to improve results in word similarity
tasks (Mikolov et al., 2013). We used the English
Wikipedia to compute word vectors with 200 fea-
tures.12 The similarity between two aspect terms
was taken to be the cosine similarity of their vec-
tors. This system performed better than Han et
al.’s with laptops, but not with restaurants.
Table 3 shows that WNDS (with sense prun-
ing) performed clearly better than the system of
Han et al. and Word2Vec. Table 3 also shows
the Pearson correlation of each judge’s scores to
the gold similarity scores, as an indication of the
best achievable results. Although WNDS (with
sense pruning) performs reasonably well in both
domains,13 there is large scope for improvement.
</bodyText>
<sectionHeader confidence="0.985568" genericHeader="method">
4 Phase B
</sectionHeader>
<bodyText confidence="0.997637">
In Phase B, the aspect terms are to be grouped
into k non-overlapping clusters, for varying val-
ues of k, given a Phase A similarity matrix. We
experimented with both the gold similarity matrix
of Phase A and similarity matrices produced by
WNDS (with SP), the best Phase A method.
</bodyText>
<subsectionHeader confidence="0.994889">
4.1 Phase B methods
</subsectionHeader>
<bodyText confidence="0.99836225">
We experimented with agglomerative clustering
and four linkage criteria: single, complete, av-
erage, and Ward (Manning and Sch¨utze, 1999;
Hastie et al., 2001). Let d(t1, t2) be the distance of
</bodyText>
<footnote confidence="0.964823111111111">
11The system of Han et al. (2013) is available from
http://semanticwebarchive.cs.umbc.edu/
SimService/; we use the STS similarity.
12Word2Vec is available from https://code.
google.com/p/word2vec/. We used the continuous
bag of words model with default parameters, the first billion
characters of the English Wikipedia, and the preprocessing of
http://mattmahoney.net/dc/textdata.html.
13Recall that the Pearson correlation ranges from −1 to 1.
</footnote>
<bodyText confidence="0.993449375">
two individual instances t1, t2; in our case, the in-
stances are aspect terms and d(t1, t2) is the inverse
of the similarity of t1, t2, defined by the Phase A
similarity matrix (gold or produced by WNDS).
Different linkage criteria define differently the dis-
tance of two clusters D(C1, C2), which affects
the choice of clusters that are merged to produce
coarser (higher-level) clusters:
</bodyText>
<equation confidence="0.994581571428571">
Dsingle(C1, C2) =
min d(t1, t2)
t1∈C1,t2∈C2
Dcompl(C1, C2) = max d(t1, t2)
t1∈C1,t2∈C2
1 E E d(t1, t2)
|C1||C2 |t1∈C1 t2∈C2
</equation>
<bodyText confidence="0.9997052">
Complete linkage tends to produce more compact
clusters, compared to single linkage, with average
linkage being in between. Ward minimizes the to-
tal in-cluster variance; consult Milligan (1980) for
further details.14
</bodyText>
<subsectionHeader confidence="0.992377">
4.2 Phase B experimental results
</subsectionHeader>
<bodyText confidence="0.999900833333333">
To evaluate the k clusters produced at each aspect
granularity by the different linkage criteria, we
used the Silhouette Index (SI) (Rousseeuw, 1987),
a cluster evaluation measure that considers both
inter- and intra-cluster coherence.15 Given a set of
clusters {C1, ... , Ck}, each SI (Ci) is defined as:
</bodyText>
<equation confidence="0.637907">
1 Ci |bj − a
SI (Ci) = |Ci|·� max(bj, aj),
</equation>
<bodyText confidence="0.99951575">
where aj is the mean distance from the j-th in-
stance of Ci to the other instances in Ci, and bj is
the mean distance from the j-th instance of Ci to
the instances in the cluster nearest to Ci. Then:
</bodyText>
<equation confidence="0.986">
Ek
1
SI({C1,...,Ck}) = k ·
i=1
</equation>
<bodyText confidence="0.991653833333333">
We always use the correct (gold) distances of the
instances (terms) when computing the SI scores.
As shown in Fig. 3, no linkage criterion clearly
outperforms the others, when the gold matrix of
Phase A is used; all four criteria perform reason-
ably well. Note that the SI ranges from −1 to
</bodyText>
<footnote confidence="0.992077875">
14We used the SCIPY implementations of agglomera-
tive clustering with the four criteria (see http://www.
scipy.org), relying on maxclust to obtain the slice of the
resulting hierarchy that leads to k (or approx. k) clusters.
15We used the SI implementation of Pedregosa et
al. (2011); see http://scikit-learn.org/. We also
experimented with the Dunn Index (Dunn, 1974) and the
Davies-Bouldin Index (1979), but we obtained similar results.
</footnote>
<equation confidence="0.854181">
Davg(C1, C2) =
SI(Ci)
</equation>
<page confidence="0.91003">
84
</page>
<figure confidence="0.999703">
(a) restaurants (b) laptops
</figure>
<figureCaption confidence="0.9610425">
Figure 3: Silhouette Index (SI) results for Phase
B, using the gold similarity matrix of Phase A.
</figureCaption>
<figure confidence="0.999308">
(a) restaurants (b) laptops
</figure>
<figureCaption confidence="0.801259875">
Figure 4: SI results for Phase B, using the WNDS
(with SP) similarity matrix of Phase A.
1, with higher values indicating better clustering.
Figure 4 shows that when the similarity matrix of
WNDS (with SP) is used, the SI scores deterio-
rate significantly; again, there is no clear winner
among the linkage criteria, but average and Ward
seem to be overall better than the others.
</figureCaption>
<figure confidence="0.99851">
(a) Restaurants (b) Laptops
</figure>
<figureCaption confidence="0.999547">
Figure 5: Human evaluation of aspect groups.
</figureCaption>
<bodyText confidence="0.999956555555556">
In a final experiment, we showed clusterings
of varying granularities (k values) to four human
judges (graduate CS students). The clusterings
were produced by two systems: one that used the
gold similarity matrix of Phase A and agglomer-
ative clustering with average linkage in Phase B,
and one that used the similarity matrix of WNDS
(with SP) and again agglomerative clustering with
average linkage. We showed all the clusterings
to all the judges. Each judge was asked to eval-
uate each clustering on a 1–5 scale. We measured
the absolute inter-annotator agreement, as in Sec-
tion 3.1, and found high agreement in all cases
(0.93 and 0.83 for the two systems, respectively,
in restaurants; 0.85 for both in laptops).16
Figure 5 shows the average human scores of
the two systems for different granularities. The
judges considered the aspect groups always per-
fect or near-perfect when the gold similarity ma-
trix of Phase A was used, but they found the as-
pect groups to be of rather poor quality when
the similarity matrix of the best Phase A mea-
sure was used. These results, along with those of
Fig. 3–4, show that more effort needs to be devoted
to improving the similarity measures of Phase A,
whereas Phase B is in effect an almost solved
problem, if a good similarity matrix is available.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999967">
We considered a new, more demanding form of
aspect aggregation in ABSA, which aims to aggre-
gate aspects at multiple granularities, as opposed
to simply merging near-synonyms, and without as-
suming that manually crafted domain-specific on-
tologies are available. We decomposed the prob-
lem in two processing phases, which allow pre-
vious work on term similarity and hierarchical
clustering to be reused and evaluated appropri-
ately with high inter-annotator agreement. We
showed that the second phase, where we used ag-
glomerative clustering, is an almost solved prob-
lem, whereas further research is needed in the first
phrase, where term similarity measures are em-
ployed. We also introduced a sense pruning mech-
anism that significantly improves WordNet-based
similarity measures, leading to a measure that out-
performs state of the art similarity methods in the
first phase of our decomposition. We also made
publicly available the datasets of our experiments.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99980875">
We thank G. Batistatos, A. Zosakis, and G. Lam-
pouras for their annotations in Phase A. We thank
A. Kosmopoulos, G. Lampouras, P. Malakasiotis,
and I. Lourentzou for their annotations in Phase B.
</bodyText>
<footnote confidence="0.847652">
16The Pearson correlation cannot be computed, as several
judges gave the same rating to the first system, for all k.
</footnote>
<page confidence="0.999566">
85
</page>
<sectionHeader confidence="0.995854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999799867924528">
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pas¸ca, and A. Soroa. 2009. A study on similar-
ity and relatedness using distributional and wordnet-
based approaches. In Proceedings of the Annual
Conference of NAACL, pages 19–27, Boulder, CO,
USA.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007a.
An integrated approach to measuring semantic sim-
ilarity between words using information available
on the web. In Proceedings of HLT-NAACL, pages
340–347, Rochester, NY, USA.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007b.
Measuring semantic similarity between words using
web search engines. In Proceedings of the 16th In-
ternational Conference of WWW, volume 766, pages
757–766, Banff, Alberta, Canada.
G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. Proceedings of
the Biennial Conference of GSCL, pages 31–40.
S. Brody and N. Elhadad. 2010. An unsupervised
aspect-sentiment model for online reviews. In Pro-
ceedings of the Annual Conference of NAACL, pages
804–812, Los Angeles, CA, USA.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13–47.
G. Carenini, R. T. Ng, and E. Zwart. 2005. Extract-
ing knowledge from evaluative text. In Proceedings
of the 3rd International Conference on Knowledge
Capture, pages 11–18, Banff, Alberta, Canada.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking.
In Proceedings of the 21st International Conference
of COLING and the 44th Annual Meeting of ACL,
pages 1009–1016, Sydney, Australia.
P. Cimiano and S. Staab. 2005. Learning concept hier-
archies from text with a guided hierarchical cluster-
ing algorithm. In Proceedings of ICML – Workshop
on Learning and Extending Lexical Ontologies with
Machine Learning Methods, Bonn, Germany.
P. Cimiano, A. M¨adche, S. Staab, and J. V¨olker. 2009.
Ontology learning. In Handbook on Ontologies,
pages 245–267. Springer.
D. L. Davies and D. W. Bouldin. 1979. A cluster sepa-
ration measure. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 1(2):224–227.
J. C. Dunn. 1974. Well-separated clusters and optimal
fuzzy partitions. Journal of Cybernetics, 4(1):95–
104.
T. Fountain and M. Lapata. 2012. Taxonomy induction
using hierarchical random graphs. In Proceedings of
NAACL:HLT, pages 466–476, Montreal, Canada.
G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In Proceedings of the 12th Interna-
tional Workshop on the Web and Databases, Prov-
idence, RI, USA.
H. Guo, H. Zhu, Z. Guo, X. Zhang, and Z. Su. 2009.
Product feature categorization with multilevel latent
semantic association. In Proceedings of the 18th
CIKM, pages 1087–1096.
L. Han, A. Kashyap, T. Finin, J. Mayfield, and
J. Weese. 2013. Umbc ebiquity-core: Semantic tex-
tual similarity systems. In Proceedings of the 2nd
Joint Conference on Lexical and Computational Se-
mantics, pages 44–52, Atlanta, GA, USA.
Z. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley.
T. Hastie, R. Tibshirani, and J. Friedman. 2001. The
Elements of Statistical Learning. Springer.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of ROCLING, pages 19–33, Taiwan,
China.
Y. Jo and A. H. Oh. 2011. Aspect and sentiment unifi-
cation model for online review analysis. In Proceed-
ings of the 4th International Conference of WSDM,
pages 815–824, Hong Kong, China.
I. P. Klapaftis and S. Manandhar. 2010. Taxonomy
learning using word sense induction. In Proceedings
of NAACL, pages 82–90, Los Angeles, CA, USA.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007. Ex-
tracting aspect-evaluation and aspect-of relations in
opinion mining. In Proceedings of the Joint Confer-
ence on EMNLP-CoNLL, pages 1065–1074, Prague,
Czech Republic.
D. Lin and X. Wu. 2009. Phrase clustering for dis-
criminative learning. In Proceedings of ACL, pages
1030–1038, Suntec, Singapore. ACL.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th ICML, pages
296–304, Madison, WI, USA.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer:
analyzing and comparing opinions on the web. In
Proceedings of the 14th International Conference of
WWW, pages 342–351, Chiba, Japan.
B. Liu. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technolo-
gies. Morgan &amp; Claypool.
C. D. Manning and H. Sch¨utze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press, Cambridge, MA, USA.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19(2):313–330.
</reference>
<page confidence="0.969163">
86
</page>
<reference confidence="0.999905285714286">
T. Mikolov, C. Kai, G. Corrado, and J. Dean. 2013.
Efficient estimation of word representations in vec-
tor space. CoRR, abs/1301.3781.
G.W. Milligan. 1980. An examination of the effect of
six types of error perturbation on fifteen clustering
algorithms. Psychometrika, 45(3):325–342.
S. Moghaddam and M. Ester. 2012. On the design of
lda models for aspect-based opinion mining. In Pro-
ceedings of the 21st CIKM, pages 803–812, Maui,
HI, USA.
R. Navigli. 2009. Word sense disambiguation: A sur-
vey. ACM Computing Surveys, 41(2):10:1–10:69.
S. Pad´o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161–199.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity: measuring the relatedness of
concepts. In Proceedings of NAACL:HTL – Demon-
strations, pages 38–41, Boston, MA, USA.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in python. Journal of Machine Learning Re-
search, 12:2825–2830.
P. Rousseeuw. 1987. Silhouettes: a graphical aid to
the interpretation and validation of cluster analysis.
Journal of Computational and Applied Mathemat-
ics, 20(1):53–65.
I. Titov and R. T. McDonald. 2008a. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the 46th Annual Meeting of ACL-
HLT, pages 308–316, Columbus, OH, USA.
I. Titov and R. T. McDonald. 2008b. Modeling online
reviews with multi-grain topic models. In Proceed-
ings of the 17th International Conference of WWW,
pages 111–120, Beijing, China.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexi-
cal selection. In Proceedings of the 32nd ACL, pages
133–138, Las Cruces, NM, USA.
J. Yu, Z. Zha, M. Wang, K. Wang, and T. Chua. 2011.
Domain-assisted product aspect hierarchy genera-
tion: towards hierarchical organization of unstruc-
tured consumer reviews. In Proceedings of EMNLP,
pages 140–150, Edinburgh, UK.
T. Zesch and I. Gurevych. 2010. Wisdom of crowds
versus wisdom of linguists - measuring the semantic
relatedness of words. Natural Language Engineer-
ing, 16(1):25–59.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2010. Group-
ing product features using semi-supervised learning
with soft-constraints. In Proceedings of the 23rd
International Conference of COLING, pages 1272–
1280, Beijing, China.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2011. Clustering
product features for opinion mining. In Proceedings
of the 4th International Conference of WSDM, pages
347–354, Hong Kong, China.
Z. Zhang, A. Gentile, and F. Ciravegna. 2013. Re-
cent advances in methods of lexical semantic relat-
edness - a survey. Natural Language Engineering,
FirstView(1):1–69.
</reference>
<page confidence="0.99947">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.310065">
<title confidence="0.999411">Multi-Granular Aspect Aggregation in Aspect-Based Sentiment Analysis</title>
<author confidence="0.67052">Pavlopoulos</author>
<affiliation confidence="0.9701615">Department of Athens University of Economics and</affiliation>
<note confidence="0.488825">Patission 76, GR-104 34 Athens,</note>
<web confidence="0.991563">http://nlp.cs.aueb.gr/</web>
<abstract confidence="0.99932908">Aspect-based sentiment analysis estimates the sentiment expressed for each particular aspect (e.g., battery, screen) of an entity (e.g., smartphone). Different words or phrases, however, may be used to refer to the same aspect, and similar aspects may need to be aggregated at coarser or finer granularities to fit the available space or satisfy user preferences. We introduce the problem of aspect aggregation at multiple granularities. We decompose it in two processing phases, to allow previous work on term similarity and hierarchical clustering to be reused. We show that the second phase, where aspects are clustered, is almost a solved problem, whereas further research is needed in the first phase, where semantic similarity measures are employed. We also introduce a novel sense pruning mechanism for WordNet-based similarity measures, which improves their performance in the first phase. Finally, we provide publicly available benchmark datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pas¸ca</author>
<author>A Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnetbased approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Conference of NAACL,</booktitle>
<pages>19--27</pages>
<location>Boulder, CO, USA.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas¸ca, and A. Soroa. 2009. A study on similarity and relatedness using distributional and wordnetbased approaches. In Proceedings of the Annual Conference of NAACL, pages 19–27, Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>An integrated approach to measuring semantic similarity between words using information available on the web.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>340--347</pages>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="10312" citStr="Bollegala et al., 2007" startWordPosition="1674" endWordPosition="1677"> also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailore</context>
<context position="18800" citStr="Bollegala et al., 2007" startWordPosition="3067" endWordPosition="3070">j) where lcs(sij, si0j0) is the least common subsumer, i.e., the most specific common ancestor of the two senses in WordNet, and depth(s) is the depth of sense s in WordNet’s hierarchy. Most terms have multiple senses, however, and word sense disambiguation methods (Navigli, 2009) are not yet robust enough. Hence, when given two aspect terms ti, ti0, rather than particular senses of the terms, a simplistic greedy approach is to compute the similarities of all the possible pairs of senses sij, si0j0 of ti, ti0, and take the similarity of ti, ti0 to be the maximum similarity of the sense pairs (Bollegala et al., 2007b; Zesch and Gurevych, 2010). We use this greedy approach with all the WordNet-based measures, but we also propose a sense pruning mechanism below, which improves their performance. In all the WordNetbased measures, if a term is not in WordNet, we take its similarity to any other term to be zero.7 The second measure, PATH (sij, si0j0), is simply the inverse of the length (plus one) of the shortest path connecting the senses sij, si0j0 in WordNet (Zhang et al., 2013). Again, the greedy approach can be used with terms having multiple senses. 6The Dice coefficient ranges from 0 to 1. There was a </context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2007</marker>
<rawString>D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007a. An integrated approach to measuring semantic similarity between words using information available on the web. In Proceedings of HLT-NAACL, pages 340–347, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Measuring semantic similarity between words using web search engines.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference of WWW,</booktitle>
<volume>766</volume>
<pages>757--766</pages>
<location>Banff, Alberta, Canada.</location>
<contexts>
<context position="10312" citStr="Bollegala et al., 2007" startWordPosition="1674" endWordPosition="1677"> also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailore</context>
<context position="18800" citStr="Bollegala et al., 2007" startWordPosition="3067" endWordPosition="3070">j) where lcs(sij, si0j0) is the least common subsumer, i.e., the most specific common ancestor of the two senses in WordNet, and depth(s) is the depth of sense s in WordNet’s hierarchy. Most terms have multiple senses, however, and word sense disambiguation methods (Navigli, 2009) are not yet robust enough. Hence, when given two aspect terms ti, ti0, rather than particular senses of the terms, a simplistic greedy approach is to compute the similarities of all the possible pairs of senses sij, si0j0 of ti, ti0, and take the similarity of ti, ti0 to be the maximum similarity of the sense pairs (Bollegala et al., 2007b; Zesch and Gurevych, 2010). We use this greedy approach with all the WordNet-based measures, but we also propose a sense pruning mechanism below, which improves their performance. In all the WordNetbased measures, if a term is not in WordNet, we take its similarity to any other term to be zero.7 The second measure, PATH (sij, si0j0), is simply the inverse of the length (plus one) of the shortest path connecting the senses sij, si0j0 in WordNet (Zhang et al., 2013). Again, the greedy approach can be used with terms having multiple senses. 6The Dice coefficient ranges from 0 to 1. There was a </context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2007</marker>
<rawString>D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007b. Measuring semantic similarity between words using web search engines. In Proceedings of the 16th International Conference of WWW, volume 766, pages 757–766, Banff, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
</authors>
<title>Normalized (pointwise) mutual information in collocation extraction.</title>
<date>2009</date>
<booktitle>Proceedings of the Biennial Conference of GSCL,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="23764" citStr="Bouma, 2009" startWordPosition="3924" endWordPosition="3925"> of the similarity measures: AVG is the average of all five; WN is the average of the first four, which employ WordNet; and WNDS is the average of WN and DS; all the scores range in [0, 1]. We also tried regression (e.g., SVR), but there was no improvement. 3.3 Phase A experimental results Each similarity measure was evaluated by computing its Pearson correlation with the scores of the gold similarity matrix. Table 2 shows the results. Our sense pruning consistently improves all four WordNet-based measures. It does not apply to 10We also experimented with Euclidean distance, a normalized PMI (Bouma, 2009), and the Brown corpus, but there was no improvement. DS, which is why the DS results are identical with and without pruning. A paired t test indicates that the other differences (with and without pruning) of Table 2 are statistically significant (p &lt; 0.05). We used the senses with the top five rel(sib) scores for each aspect term ti during sense pruning. We also experimented with keeping fewer senses, but the results were inferior or there was no improvement. Lin’s measure performed better when information content was estimated on the (much larger, but domain-independent) Brown corpus (LIN@Br</context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>G. Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. Proceedings of the Biennial Conference of GSCL, pages 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>N Elhadad</author>
</authors>
<title>An unsupervised aspect-sentiment model for online reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Conference of NAACL,</booktitle>
<pages>804--812</pages>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="10530" citStr="Brody and Elhadad, 2010" startWordPosition="1711" endWordPosition="1714">. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hierarchical relations. They</context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>S. Brody and N. Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews. In Proceedings of the Annual Conference of NAACL, pages 804–812, Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="17804" citStr="Budanitsky and Hirst, 2006" startWordPosition="2896" endWordPosition="2899">given to the judges are available upon request. 5The Pearson correlation ranges from −1 to 1, whereas the absolute inter-annotator agreement ranges from 0 to 1. 81 pects (0.25 and 0.11).6 In other preliminary experiments, we asked human judges to rank alternative aspect hierarchies that had been produced by applying agglomerative clustering with different linkage criteria to 20 aspect terms, but we obtained very poor inter-annotator agreement (Pearson score −0.83 for restaurants and 0 for laptops). 3.2 Phase A methods We employed five term similarity measures. The first two are WordNet-based (Budanitsky and Hirst, 2006). The next two combine WordNet with statistics from corpora. The fifth one is a corpusbased distributional similarity measure. The first measure is Wu and Palmer’s (1994). It is actually a sense similarity measure (a term may have multiple senses). Given two senses sij, si0j0 of terms ti, ti0, the measure is defined as follows: WP(sij, si0j0) = 2 · , depth(sij) + depth(sij) where lcs(sij, si0j0) is the least common subsumer, i.e., the most specific common ancestor of the two senses in WordNet, and depth(s) is the depth of sense s in WordNet’s hierarchy. Most terms have multiple senses, however</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>R T Ng</author>
<author>E Zwart</author>
</authors>
<title>Extracting knowledge from evaluative text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Conference on Knowledge Capture,</booktitle>
<pages>11--18</pages>
<location>Banff, Alberta, Canada.</location>
<contexts>
<context position="4087" citStr="Carenini et al. (2005)" startWordPosition="641" endWordPosition="644">ar-synonymous) aspect terms (Liu, 2012). Ag78 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 78–87, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics gregating only synonyms (or near-synonyms), however, does not allow users to select the desirable aspect granularity, and ignores the hierarchical relations between aspect terms. For example, ‘pizza’ and ‘steak’ are kinds of ‘food’ and, hence, the three terms can be aggregated to show fewer, coarser aspects, even though they are not synonyms. Carenini et al. (2005) used a predefined domain-specific taxonomy to hierarchically aggregate aspect terms, but taxonomies of this kind are often not available. By contrast, we use only general-purpose taxonomies (e.g., WordNet), term similarity measures based on general-purpose taxonomies or corpora, and hierarchical clustering. We define multi-granular aspect aggregation to be the task of partitioning a given set of aspect terms (generated by a previous aspect extraction stage) into k non-overlapping clusters, for multiple values of k. A further constraint is that the clusters have to be consistent for different </context>
<context position="10786" citStr="Carenini et al. (2005)" startWordPosition="1755" endWordPosition="1758">re to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hierarchical relations. They extract, however, relations by looking for clues in texts (e.g., particular phrases). By contrast, we employ similarity measures and hierarchical clustering, which allows us to group similar aspect terms even when they do not cooccur in texts. Also, in co</context>
</contexts>
<marker>Carenini, Ng, Zwart, 2005</marker>
<rawString>G. Carenini, R. T. Ng, and E. Zwart. 2005. Extracting knowledge from evaluative text. In Proceedings of the 3rd International Conference on Knowledge Capture, pages 11–18, Banff, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
<author>M Lin</author>
<author>Y Wei</author>
</authors>
<title>Novel association measures using web search with double checking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference of COLING and the 44th Annual Meeting of ACL,</booktitle>
<pages>1009--1016</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="10288" citStr="Chen et al., 2006" startWordPosition="1670" endWordPosition="1673">least in principle, also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et a</context>
</contexts>
<marker>Chen, Lin, Wei, 2006</marker>
<rawString>H. Chen, M. Lin, and Y. Wei. 2006. Novel association measures using web search with double checking. In Proceedings of the 21st International Conference of COLING and the 44th Annual Meeting of ACL, pages 1009–1016, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>S Staab</author>
</authors>
<title>Learning concept hierarchies from text with a guided hierarchical clustering algorithm.</title>
<date>2005</date>
<booktitle>In Proceedings of ICML – Workshop on Learning and Extending Lexical Ontologies with Machine Learning Methods,</booktitle>
<location>Bonn, Germany.</location>
<contexts>
<context position="11545" citStr="Cimiano and Staab (2005)" startWordPosition="1877" endWordPosition="1881"> of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hierarchical relations. They extract, however, relations by looking for clues in texts (e.g., particular phrases). By contrast, we employ similarity measures and hierarchical clustering, which allows us to group similar aspect terms even when they do not cooccur in texts. Also, in contrast to Kobayashi et al. (2007), we respect the consistency constraint discussed in Section 1. A similar task is taxonomy induction. Cimiano and Staab (2005) automatically construct taxonomies from texts via agglomerative clustering, much as in our Phase B, but not in the context of ABSA, and without trying to learn a similarity matrix first. They also label the hierarchy’s concepts, a task we do not consider. Klapaftis and Manandhar (2010) show how word sense induction can be combined with agglomerative clustering to obtain more accurate taxonomies, again not in the context of ABSA. Our sense pruning method was influenced by their work, but is much simpler than their word sense induction. Fountain and Lapata (2012) study unsupervised methods to i</context>
</contexts>
<marker>Cimiano, Staab, 2005</marker>
<rawString>P. Cimiano and S. Staab. 2005. Learning concept hierarchies from text with a guided hierarchical clustering algorithm. In Proceedings of ICML – Workshop on Learning and Extending Lexical Ontologies with Machine Learning Methods, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>A M¨adche</author>
<author>S Staab</author>
<author>J V¨olker</author>
</authors>
<title>Ontology learning.</title>
<date>2009</date>
<booktitle>In Handbook on Ontologies,</booktitle>
<pages>245--267</pages>
<publisher>Springer.</publisher>
<marker>Cimiano, M¨adche, Staab, V¨olker, 2009</marker>
<rawString>P. Cimiano, A. M¨adche, S. Staab, and J. V¨olker. 2009. Ontology learning. In Handbook on Ontologies, pages 245–267. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Davies</author>
<author>D W Bouldin</author>
</authors>
<title>A cluster separation measure.</title>
<date>1979</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>1</volume>
<issue>2</issue>
<marker>Davies, Bouldin, 1979</marker>
<rawString>D. L. Davies and D. W. Bouldin. 1979. A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1(2):224–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Dunn</author>
</authors>
<title>Well-separated clusters and optimal fuzzy partitions.</title>
<date>1974</date>
<journal>Journal of Cybernetics,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>104</pages>
<contexts>
<context position="30427" citStr="Dunn, 1974" startWordPosition="5024" endWordPosition="5025">the instances (terms) when computing the SI scores. As shown in Fig. 3, no linkage criterion clearly outperforms the others, when the gold matrix of Phase A is used; all four criteria perform reasonably well. Note that the SI ranges from −1 to 14We used the SCIPY implementations of agglomerative clustering with the four criteria (see http://www. scipy.org), relying on maxclust to obtain the slice of the resulting hierarchy that leads to k (or approx. k) clusters. 15We used the SI implementation of Pedregosa et al. (2011); see http://scikit-learn.org/. We also experimented with the Dunn Index (Dunn, 1974) and the Davies-Bouldin Index (1979), but we obtained similar results. Davg(C1, C2) = SI(Ci) 84 (a) restaurants (b) laptops Figure 3: Silhouette Index (SI) results for Phase B, using the gold similarity matrix of Phase A. (a) restaurants (b) laptops Figure 4: SI results for Phase B, using the WNDS (with SP) similarity matrix of Phase A. 1, with higher values indicating better clustering. Figure 4 shows that when the similarity matrix of WNDS (with SP) is used, the SI scores deteriorate significantly; again, there is no clear winner among the linkage criteria, but average and Ward seem to be ov</context>
</contexts>
<marker>Dunn, 1974</marker>
<rawString>J. C. Dunn. 1974. Well-separated clusters and optimal fuzzy partitions. Journal of Cybernetics, 4(1):95– 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fountain</author>
<author>M Lapata</author>
</authors>
<title>Taxonomy induction using hierarchical random graphs.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL:HLT,</booktitle>
<pages>466--476</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="12113" citStr="Fountain and Lapata (2012)" startWordPosition="1974" endWordPosition="1977">milar task is taxonomy induction. Cimiano and Staab (2005) automatically construct taxonomies from texts via agglomerative clustering, much as in our Phase B, but not in the context of ABSA, and without trying to learn a similarity matrix first. They also label the hierarchy’s concepts, a task we do not consider. Klapaftis and Manandhar (2010) show how word sense induction can be combined with agglomerative clustering to obtain more accurate taxonomies, again not in the context of ABSA. Our sense pruning method was influenced by their work, but is much simpler than their word sense induction. Fountain and Lapata (2012) study unsupervised methods to induce concept taxonomies, without considering ABSA. 3 Phase A We now discuss our work for Phase A. Recall that in this phase the input is a set of aspect terms and 2Topic models are typically also used to perform aspect extraction, apart from aspect aggregation, but simple heuristics (e.g., most frequent nouns) often outperform them in aspect extraction (Liu, 2012; Moghaddam and Ester, 2012). 80 the goal is to fill in a matrix (Table 1) with scores showing the similarity of each pair of aspect terms. 3.1 Datasets used in Phase A We used two benchmark datasets th</context>
</contexts>
<marker>Fountain, Lapata, 2012</marker>
<rawString>T. Fountain and M. Lapata. 2012. Taxonomy induction using hierarchical random graphs. In Proceedings of NAACL:HLT, pages 466–476, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ganu</author>
<author>N Elhadad</author>
<author>A Marian</author>
</authors>
<title>Beyond the stars: Improving rating predictions using review text content.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th International Workshop on the Web and Databases,</booktitle>
<location>Providence, RI, USA.</location>
<contexts>
<context position="13340" citStr="Ganu et al. (2009)" startWordPosition="2169" endWordPosition="2172"> previously constructed to evaluate ABSA methods for subjectivity detection, aspect extraction, and aspect score estimation, but not aspect aggregation. We extended them to support aspect aggregation, and we make them publicly available.3 The two original datasets contain sentences from customer reviews of restaurants and laptops, respectively. The reviews are manually split into sentences, and each sentence is manually annotated as ‘subjective’ (expressing opinion) or ‘objective’ (not expressing opinion). The restaurants dataset contains 3,710 English sentences from the restaurant reviews of Ganu et al. (2009). The laptops dataset contains 3,085 English sentences from 394 customer reviews, collected from sites that host customer reviews. In the experiments of this paper, we use only the 3,057 (out of 3,710) subjective restaurant sentences and the 2,631 (out of 3,085) subjective laptop sentences. For each subjective sentence, our datasets show the words that human annotators marked as aspect terms. For example, in “The dessert was divine!” the aspect term is ‘dessert’, and in “Really bad waiter.” it is ‘waiter’. Among the 3,057 subjective restaurant sentences, 1,129 contain exactly one aspect term, </context>
</contexts>
<marker>Ganu, Elhadad, Marian, 2009</marker>
<rawString>G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond the stars: Improving rating predictions using review text content. In Proceedings of the 12th International Workshop on the Web and Databases, Providence, RI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Guo</author>
<author>H Zhu</author>
<author>Z Guo</author>
<author>X Zhang</author>
<author>Z Su</author>
</authors>
<title>Product feature categorization with multilevel latent semantic association.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th CIKM,</booktitle>
<pages>1087--1096</pages>
<contexts>
<context position="10505" citStr="Guo et al., 2009" startWordPosition="1707" endWordPosition="1710">nd B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hie</context>
</contexts>
<marker>Guo, Zhu, Guo, Zhang, Su, 2009</marker>
<rawString>H. Guo, H. Zhu, Z. Guo, X. Zhang, and Z. Su. 2009. Product feature categorization with multilevel latent semantic association. In Proceedings of the 18th CIKM, pages 1087–1096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Han</author>
<author>A Kashyap</author>
<author>T Finin</author>
<author>J Mayfield</author>
<author>J Weese</author>
</authors>
<title>Umbc ebiquity-core: Semantic textual similarity systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>44--52</pages>
<location>Atlanta, GA, USA.</location>
<contexts>
<context position="25917" citStr="Han et al. (2013)" startWordPosition="4279" endWordPosition="4282">nce across the two datasets, but they are not top-performers in any of the two. Combinations of similarity measures seem more stable across domains, as the results of AVG, WN, and WNDS indicate, though experiments with more domains are needed to investigate this issue. WNDS is the best overall method with sense pruning, and among the best three methods without pruning in both datasets. To get a better view of the performance of WNDS with sense pruning, i.e., the best overall measure of Table 2, we compared it to two state of the art semantic similarity systems. First, we applied the system of Han et al. (2013), one of the best systems of the recent *Sem 2013 semantic text similarity competition, to our Phase A data. The performance (Pearson correlation with gold similarities) of the same system on the widely used WordSim353 word similarity dataset (Agirre et al., 2009) is 0.73, much higher than the same system’s performance on our Phase A data (see Table 3), 83 Method Restaurants Laptops Han et al. (2013) 0.450 0.471 Word2Vec 0.434 0.485 WNDS with SP 0.545 0.546 Judge 1 0.913 0.875 Judge 2 0.914 0.894 Judge 3 0.888 0.924 Table 3: Phase A results (Pearson correlation to gold similarities) of WNDS wi</context>
<context position="28018" citStr="Han et al. (2013)" startWordPosition="4633" endWordPosition="4636">rforms reasonably well in both domains,13 there is large scope for improvement. 4 Phase B In Phase B, the aspect terms are to be grouped into k non-overlapping clusters, for varying values of k, given a Phase A similarity matrix. We experimented with both the gold similarity matrix of Phase A and similarity matrices produced by WNDS (with SP), the best Phase A method. 4.1 Phase B methods We experimented with agglomerative clustering and four linkage criteria: single, complete, average, and Ward (Manning and Sch¨utze, 1999; Hastie et al., 2001). Let d(t1, t2) be the distance of 11The system of Han et al. (2013) is available from http://semanticwebarchive.cs.umbc.edu/ SimService/; we use the STS similarity. 12Word2Vec is available from https://code. google.com/p/word2vec/. We used the continuous bag of words model with default parameters, the first billion characters of the English Wikipedia, and the preprocessing of http://mattmahoney.net/dc/textdata.html. 13Recall that the Pearson correlation ranges from −1 to 1. two individual instances t1, t2; in our case, the instances are aspect terms and d(t1, t2) is the inverse of the similarity of t1, t2, defined by the Phase A similarity matrix (gold or pro</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>L. Han, A. Kashyap, T. Finin, J. Mayfield, and J. Weese. 2013. Umbc ebiquity-core: Semantic textual similarity systems. In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics, pages 44–52, Atlanta, GA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley.</publisher>
<contexts>
<context position="22502" citStr="Harris, 1968" startWordPosition="3694" endWordPosition="3695">434 0.329 0.471 0.391 JCN@domain 0.467 0.348 0.509 0.448 JCN@Brown 0.403 0.469 0.419 0.539 DS 0.283 0.517 (0.283) (0.517) AVG 0.499 0.352 0.537 0.426 WN 0.490 0.328 0.530 0.395 WNDS 0.523 0.453 0.545 0.546 Table 2: Phase A results (Pearson correlation to gold similarities) with and without sense pruning. prune senses that are very irrelevant to the particular domain (e.g., laptops). This sense pruning mechanism is novel, and we show experimentally that it improves the performance of all the WordNet-based similarity measures we examined. We also implemented a distributional similarity measure (Harris, 1968; Pad´o and Lapata, 2007; Cimiano et al., 2009; Zhang et al., 2013). Following Lin and Wu (2009), for each aspect term t, we create a vector V(t) = (PMI (t, wl), ... , PMI (t, wn)). The vector components are the Pointwise Mutual Information scores of t and each word wi of a corpus: P(t, wi) PMI (t, wi) = − log P(t) · P(wi) We treat P(t, wi) as the probability of t, wi cooccurring in the same sentence, and we use the (laptop or restaurant) reviews of our datasets as the corpus to estimate the probabilities. The distributional similarity DS(t, t&apos;) of two aspect terms t, t&apos; is the cosine similari</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Z. Harris. 1968. Mathematical Structures of Language. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
<author>J Friedman</author>
</authors>
<title>The Elements of Statistical Learning.</title>
<date>2001</date>
<publisher>Springer.</publisher>
<contexts>
<context position="5431" citStr="Hastie et al., 2001" startWordPosition="868" endWordPosition="871">uped together (in the same cluster) for every k = k2 with k2 &lt; k1, i.e., for every coarser grouping. For example, if ‘waiter’ and ‘service’ are grouped together for k = 5, they must also be grouped together for k = 4, 3, 2 and (trivially) k = 1, to allow the user to feel that selecting a smaller number of aspect groups (narrowing the height of Fig. 1) has the effect of zooming out (without aspect terms jumping unexpectedly to other aspect groups), and similarly for zooming in.1 This requirement is satisfied by using agglomerative hierarchical clustering algorithms (Manning and Sch¨utze, 1999; Hastie et al., 2001), which in our case produce term hierarchies like the ones of Fig. 2. By using slices (nodes at a particular depth) of the hierarchies that are closer to the root or the leaves, we obtain fewer or more clusters. The vertical dotted lines of Fig. 2 illustrate two slices for k = 4. By contrast, flat clustering algorithms (e.g., k-means) do not satisfy the consistency constraint for different k values. Agglomerative clustering algorithms require a measure of the distance between individuals, in our case a measure of how similar two aspect terms are, and a linkage criterion to specify which cluste</context>
<context position="27950" citStr="Hastie et al., 2001" startWordPosition="4619" endWordPosition="4622">n of the best achievable results. Although WNDS (with sense pruning) performs reasonably well in both domains,13 there is large scope for improvement. 4 Phase B In Phase B, the aspect terms are to be grouped into k non-overlapping clusters, for varying values of k, given a Phase A similarity matrix. We experimented with both the gold similarity matrix of Phase A and similarity matrices produced by WNDS (with SP), the best Phase A method. 4.1 Phase B methods We experimented with agglomerative clustering and four linkage criteria: single, complete, average, and Ward (Manning and Sch¨utze, 1999; Hastie et al., 2001). Let d(t1, t2) be the distance of 11The system of Han et al. (2013) is available from http://semanticwebarchive.cs.umbc.edu/ SimService/; we use the STS similarity. 12Word2Vec is available from https://code. google.com/p/word2vec/. We used the continuous bag of words model with default parameters, the first billion characters of the English Wikipedia, and the preprocessing of http://mattmahoney.net/dc/textdata.html. 13Recall that the Pearson correlation ranges from −1 to 1. two individual instances t1, t2; in our case, the instances are aspect terms and d(t1, t2) is the inverse of the similar</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2001</marker>
<rawString>T. Hastie, R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of ROCLING,</booktitle>
<pages>19--33</pages>
<location>Taiwan, China.</location>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. J. Jiang and D. W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of ROCLING, pages 19–33, Taiwan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Jo</author>
<author>A H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th International Conference of WSDM,</booktitle>
<pages>815--824</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="10548" citStr="Jo and Oh, 2011" startWordPosition="1715" endWordPosition="1718">Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hierarchical relations. They extract, however,</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Y. Jo and A. H. Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of the 4th International Conference of WSDM, pages 815–824, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I P Klapaftis</author>
<author>S Manandhar</author>
</authors>
<title>Taxonomy learning using word sense induction.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>82--90</pages>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="11832" citStr="Klapaftis and Manandhar (2010)" startWordPosition="1926" endWordPosition="1930">ular phrases). By contrast, we employ similarity measures and hierarchical clustering, which allows us to group similar aspect terms even when they do not cooccur in texts. Also, in contrast to Kobayashi et al. (2007), we respect the consistency constraint discussed in Section 1. A similar task is taxonomy induction. Cimiano and Staab (2005) automatically construct taxonomies from texts via agglomerative clustering, much as in our Phase B, but not in the context of ABSA, and without trying to learn a similarity matrix first. They also label the hierarchy’s concepts, a task we do not consider. Klapaftis and Manandhar (2010) show how word sense induction can be combined with agglomerative clustering to obtain more accurate taxonomies, again not in the context of ABSA. Our sense pruning method was influenced by their work, but is much simpler than their word sense induction. Fountain and Lapata (2012) study unsupervised methods to induce concept taxonomies, without considering ABSA. 3 Phase A We now discuss our work for Phase A. Recall that in this phase the input is a set of aspect terms and 2Topic models are typically also used to perform aspect extraction, apart from aspect aggregation, but simple heuristics (e</context>
</contexts>
<marker>Klapaftis, Manandhar, 2010</marker>
<rawString>I. P. Klapaftis and S. Manandhar. 2010. Taxonomy learning using word sense induction. In Proceedings of NAACL, pages 82–90, Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kobayashi</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on EMNLP-CoNLL,</booktitle>
<pages>1065--1074</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11022" citStr="Kobayashi et al. (2007)" startWordPosition="1794" endWordPosition="1797">), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hierarchical relations. They extract, however, relations by looking for clues in texts (e.g., particular phrases). By contrast, we employ similarity measures and hierarchical clustering, which allows us to group similar aspect terms even when they do not cooccur in texts. Also, in contrast to Kobayashi et al. (2007), we respect the consistency constraint discussed in Section 1. A similar task is taxonomy induction. Cimiano and Staab (2005) automatically construct taxonomies from texts via agglomerative clustering, </context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>N. Kobayashi, K. Inui, and Y. Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of the Joint Conference on EMNLP-CoNLL, pages 1065–1074, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>X Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1030--1038</pages>
<publisher>ACL.</publisher>
<location>Suntec, Singapore.</location>
<contexts>
<context position="10332" citStr="Lin and Wu, 2009" startWordPosition="1678" endWordPosition="1681"> expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an exis</context>
<context position="22598" citStr="Lin and Wu (2009)" startWordPosition="3710" endWordPosition="3713">DS 0.283 0.517 (0.283) (0.517) AVG 0.499 0.352 0.537 0.426 WN 0.490 0.328 0.530 0.395 WNDS 0.523 0.453 0.545 0.546 Table 2: Phase A results (Pearson correlation to gold similarities) with and without sense pruning. prune senses that are very irrelevant to the particular domain (e.g., laptops). This sense pruning mechanism is novel, and we show experimentally that it improves the performance of all the WordNet-based similarity measures we examined. We also implemented a distributional similarity measure (Harris, 1968; Pad´o and Lapata, 2007; Cimiano et al., 2009; Zhang et al., 2013). Following Lin and Wu (2009), for each aspect term t, we create a vector V(t) = (PMI (t, wl), ... , PMI (t, wn)). The vector components are the Pointwise Mutual Information scores of t and each word wi of a corpus: P(t, wi) PMI (t, wi) = − log P(t) · P(wi) We treat P(t, wi) as the probability of t, wi cooccurring in the same sentence, and we use the (laptop or restaurant) reviews of our datasets as the corpus to estimate the probabilities. The distributional similarity DS(t, t&apos;) of two aspect terms t, t&apos; is the cosine similarity of v(t),v(t&apos;).10 Finally, we tried combinations of the similarity measures: AVG is the averag</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>D. Lin and X. Wu. 2009. Phrase clustering for discriminative learning. In Proceedings of ACL, pages 1030–1038, Suntec, Singapore. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th ICML,</booktitle>
<pages>296--304</pages>
<location>Madison, WI, USA.</location>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th ICML, pages 296–304, Madison, WI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>M Hu</author>
<author>J Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th International Conference of WWW,</booktitle>
<pages>342--351</pages>
<location>Chiba, Japan.</location>
<contexts>
<context position="10244" citStr="Liu et al., 2005" startWordPosition="1663" endWordPosition="1666">he work of this paper in particular are, at least in principle, also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity </context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In Proceedings of the 14th International Conference of WWW, pages 342–351, Chiba, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool.</publisher>
<contexts>
<context position="1622" citStr="Liu, 2012" startWordPosition="252" endWordPosition="253">so introduce a novel sense pruning mechanism for WordNet-based similarity measures, which improves their performance in the first phase. Finally, we provide publicly available benchmark datasets. 1 Introduction Given a set of texts discussing a particular entity (e.g., reviews of a laptop), aspect-based sentiment analysis (ABSA) attempts to identify the most prominent (e.g., frequently discussed) aspects of the entity (e.g., battery, screen) and the average sentiment (e.g., 1 to 5 stars) for each aspect or group of aspects, as in Fig. 1. Most ABSA systems perform all or some of the following (Liu, 2012): subjectivity detection to retain only sentences (or other spans) expressing subjective opinions; aspect extraction to extract (and possibly rank) terms corresponding to aspects (e.g., ‘battery’); aspect aggregation to group aspect terms that are nearsynonyms (e.g., ‘price’, ‘cost’) or to obtain aspects Figure 1: Aspect groups and scores of an entity. at a coarser granularity (e.g., ‘chicken’,‘steak’, and ‘fish’ may be replaced by ‘food’ in restaurant reviews); and aspect sentiment score estimation to estimate the average sentiment for each aspect or group of aspects. In this paper, we focus </context>
<context position="3504" citStr="Liu, 2012" startWordPosition="555" endWordPosition="556">to adjust the granularity of aspects, e.g., by stretching or narrowing the height of Fig. 1 on a smartphone to view more or fewer lines. Hence, aspect aggregation should be able to produce groups of aspect terms for multiple granularities. We assume that the aggregated aspects are displayed as lists of terms, as in Fig. 1. We make no effort to order (e.g., by frequency) the terms in each list, nor do we attempt to produce a single (more general) term to describe each aggregated aspect, leaving such tasks for future work. ABSA systems usually group synonymous (or near-synonymous) aspect terms (Liu, 2012). Ag78 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 78–87, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics gregating only synonyms (or near-synonyms), however, does not allow users to select the desirable aspect granularity, and ignores the hierarchical relations between aspect terms. For example, ‘pizza’ and ‘steak’ are kinds of ‘food’ and, hence, the three terms can be aggregated to show fewer, coarser aspects, even though they are not synonyms. Carenini et al. (2005) used a predefine</context>
<context position="10162" citStr="Liu, 2012" startWordPosition="1652" endWordPosition="1653">gh we experiment with customer reviews of products and services, ABSA and the work of this paper in particular are, at least in principle, also applicable to texts expressing opinions about other kinds of entities (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels)</context>
<context position="12511" citStr="Liu, 2012" startWordPosition="2043" endWordPosition="2044">tering to obtain more accurate taxonomies, again not in the context of ABSA. Our sense pruning method was influenced by their work, but is much simpler than their word sense induction. Fountain and Lapata (2012) study unsupervised methods to induce concept taxonomies, without considering ABSA. 3 Phase A We now discuss our work for Phase A. Recall that in this phase the input is a set of aspect terms and 2Topic models are typically also used to perform aspect extraction, apart from aspect aggregation, but simple heuristics (e.g., most frequent nouns) often outperform them in aspect extraction (Liu, 2012; Moghaddam and Ester, 2012). 80 the goal is to fill in a matrix (Table 1) with scores showing the similarity of each pair of aspect terms. 3.1 Datasets used in Phase A We used two benchmark datasets that we had previously constructed to evaluate ABSA methods for subjectivity detection, aspect extraction, and aspect score estimation, but not aspect aggregation. We extended them to support aspect aggregation, and we make them publicly available.3 The two original datasets contain sentences from customer reviews of restaurants and laptops, respectively. The reviews are manually split into senten</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>B. Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C. D. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="20442" citStr="Marcus et al., 1993" startWordPosition="3351" endWordPosition="3354">’s (1998), defined as: 2 · ic(lcs(sij, si0j0)) LIN (sij, si0j0) =ic(sij) + ic(si0j0) , where sij, si0j0 are senses of terms ti, ti0, lcs(sij, si0j0) is the least common subsumer of sij, si0j0 in WordNet, and ic(s) = − log P(s) is the information content of sense s (Pedersen et al., 2004), estimated from a corpus. When the corpus is not sense-tagged, we follow the common approach of treating each occurrence of a word as an occurrence of all of its senses, when estimating ic(s).8 We experimented with two variants of Lin’s measure, one where the ic(s) scores were estimated from the Brown corpus (Marcus et al., 1993), and one where they were estimated from the (restaurant or laptop) reviews of our datasets. The fourth measure is Jiang and Conrath’s (1997), defined below. Again, we experimented with two variants of ic(s), as above. JCN(sij, si0j0) = 1 ic(sij) + ic(si0j0) − 2 · lcs(sij, si0j0) For all the above WordNet-based measures, we experimented with a sense pruning mechanism, which discards some of the senses of the aspect terms, before applying the greedy approach. For each aspect term ti, we consider all of its WordNet senses sij. For each sij and each other aspect term ti0, we compute (using PATH) </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>C Kai</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="26844" citStr="Mikolov et al., 2013" startWordPosition="4433" endWordPosition="4436">’s performance on our Phase A data (see Table 3), 83 Method Restaurants Laptops Han et al. (2013) 0.450 0.471 Word2Vec 0.434 0.485 WNDS with SP 0.545 0.546 Judge 1 0.913 0.875 Judge 2 0.914 0.894 Judge 3 0.888 0.924 Table 3: Phase A results (Pearson correlation to gold similarities) of WNDS with SP against semantic similarity systems and human judges. which suggests that our data are more difficult.11 We also employed the recent Word2Vec system, which computes continuous vector space representations of words from large corpora and has been reported to improve results in word similarity tasks (Mikolov et al., 2013). We used the English Wikipedia to compute word vectors with 200 features.12 The similarity between two aspect terms was taken to be the cosine similarity of their vectors. This system performed better than Han et al.’s with laptops, but not with restaurants. Table 3 shows that WNDS (with sense pruning) performed clearly better than the system of Han et al. and Word2Vec. Table 3 also shows the Pearson correlation of each judge’s scores to the gold similarity scores, as an indication of the best achievable results. Although WNDS (with sense pruning) performs reasonably well in both domains,13 t</context>
</contexts>
<marker>Mikolov, Kai, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, C. Kai, G. Corrado, and J. Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Milligan</author>
</authors>
<title>An examination of the effect of six types of error perturbation on fifteen clustering algorithms.</title>
<date>1980</date>
<journal>Psychometrika,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="29134" citStr="Milligan (1980)" startWordPosition="4800" endWordPosition="4801">) is the inverse of the similarity of t1, t2, defined by the Phase A similarity matrix (gold or produced by WNDS). Different linkage criteria define differently the distance of two clusters D(C1, C2), which affects the choice of clusters that are merged to produce coarser (higher-level) clusters: Dsingle(C1, C2) = min d(t1, t2) t1∈C1,t2∈C2 Dcompl(C1, C2) = max d(t1, t2) t1∈C1,t2∈C2 1 E E d(t1, t2) |C1||C2 |t1∈C1 t2∈C2 Complete linkage tends to produce more compact clusters, compared to single linkage, with average linkage being in between. Ward minimizes the total in-cluster variance; consult Milligan (1980) for further details.14 4.2 Phase B experimental results To evaluate the k clusters produced at each aspect granularity by the different linkage criteria, we used the Silhouette Index (SI) (Rousseeuw, 1987), a cluster evaluation measure that considers both inter- and intra-cluster coherence.15 Given a set of clusters {C1, ... , Ck}, each SI (Ci) is defined as: 1 Ci |bj − a SI (Ci) = |Ci|·� max(bj, aj), where aj is the mean distance from the j-th instance of Ci to the other instances in Ci, and bj is the mean distance from the j-th instance of Ci to the instances in the cluster nearest to Ci. T</context>
</contexts>
<marker>Milligan, 1980</marker>
<rawString>G.W. Milligan. 1980. An examination of the effect of six types of error perturbation on fifteen clustering algorithms. Psychometrika, 45(3):325–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Moghaddam</author>
<author>M Ester</author>
</authors>
<title>On the design of lda models for aspect-based opinion mining.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st CIKM,</booktitle>
<pages>803--812</pages>
<location>Maui, HI, USA.</location>
<contexts>
<context position="12539" citStr="Moghaddam and Ester, 2012" startWordPosition="2045" endWordPosition="2048">btain more accurate taxonomies, again not in the context of ABSA. Our sense pruning method was influenced by their work, but is much simpler than their word sense induction. Fountain and Lapata (2012) study unsupervised methods to induce concept taxonomies, without considering ABSA. 3 Phase A We now discuss our work for Phase A. Recall that in this phase the input is a set of aspect terms and 2Topic models are typically also used to perform aspect extraction, apart from aspect aggregation, but simple heuristics (e.g., most frequent nouns) often outperform them in aspect extraction (Liu, 2012; Moghaddam and Ester, 2012). 80 the goal is to fill in a matrix (Table 1) with scores showing the similarity of each pair of aspect terms. 3.1 Datasets used in Phase A We used two benchmark datasets that we had previously constructed to evaluate ABSA methods for subjectivity detection, aspect extraction, and aspect score estimation, but not aspect aggregation. We extended them to support aspect aggregation, and we make them publicly available.3 The two original datasets contain sentences from customer reviews of restaurants and laptops, respectively. The reviews are manually split into sentences, and each sentence is ma</context>
</contexts>
<marker>Moghaddam, Ester, 2012</marker>
<rawString>S. Moghaddam and M. Ester. 2012. On the design of lda models for aspect-based opinion mining. In Proceedings of the 21st CIKM, pages 803–812, Maui, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="18459" citStr="Navigli, 2009" startWordPosition="3007" endWordPosition="3008">stics from corpora. The fifth one is a corpusbased distributional similarity measure. The first measure is Wu and Palmer’s (1994). It is actually a sense similarity measure (a term may have multiple senses). Given two senses sij, si0j0 of terms ti, ti0, the measure is defined as follows: WP(sij, si0j0) = 2 · , depth(sij) + depth(sij) where lcs(sij, si0j0) is the least common subsumer, i.e., the most specific common ancestor of the two senses in WordNet, and depth(s) is the depth of sense s in WordNet’s hierarchy. Most terms have multiple senses, however, and word sense disambiguation methods (Navigli, 2009) are not yet robust enough. Hence, when given two aspect terms ti, ti0, rather than particular senses of the terms, a simplistic greedy approach is to compute the similarities of all the possible pairs of senses sij, si0j0 of ti, ti0, and take the similarity of ti, ti0 to be the maximum similarity of the sense pairs (Bollegala et al., 2007b; Zesch and Gurevych, 2010). We use this greedy approach with all the WordNet-based measures, but we also propose a sense pruning mechanism below, which improves their performance. In all the WordNetbased measures, if a term is not in WordNet, we take its si</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>R. Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys, 41(2):10:1–10:69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>S. Pad´o and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>Wordnet::similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL:HTL – Demonstrations,</booktitle>
<pages>38--41</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="20110" citStr="Pedersen et al., 2004" startWordPosition="3293" endWordPosition="3296"> inappropriate to use Cohen’s K. 7This never happened in the restaurants dataset. In the laptops dataset, it only happened for ‘hard drive’ and ‘battery life’. We use the NLTK implementation of the first four measures (see http://nltk.org/) and our own implementation of the distributional similarity measure. The third measure is Lin’s (1998), defined as: 2 · ic(lcs(sij, si0j0)) LIN (sij, si0j0) =ic(sij) + ic(si0j0) , where sij, si0j0 are senses of terms ti, ti0, lcs(sij, si0j0) is the least common subsumer of sij, si0j0 in WordNet, and ic(s) = − log P(s) is the information content of sense s (Pedersen et al., 2004), estimated from a corpus. When the corpus is not sense-tagged, we follow the common approach of treating each occurrence of a word as an occurrence of all of its senses, when estimating ic(s).8 We experimented with two variants of Lin’s measure, one where the ic(s) scores were estimated from the Brown corpus (Marcus et al., 1993), and one where they were estimated from the (restaurant or laptop) reviews of our datasets. The fourth measure is Jiang and Conrath’s (1997), defined below. Again, we experimented with two variants of ic(s), as above. JCN(sij, si0j0) = 1 ic(sij) + ic(si0j0) − 2 · lcs</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. Wordnet::similarity: measuring the relatedness of concepts. In Proceedings of NAACL:HTL – Demonstrations, pages 38–41, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="30342" citStr="Pedregosa et al. (2011)" startWordPosition="5011" endWordPosition="5014">earest to Ci. Then: Ek 1 SI({C1,...,Ck}) = k · i=1 We always use the correct (gold) distances of the instances (terms) when computing the SI scores. As shown in Fig. 3, no linkage criterion clearly outperforms the others, when the gold matrix of Phase A is used; all four criteria perform reasonably well. Note that the SI ranges from −1 to 14We used the SCIPY implementations of agglomerative clustering with the four criteria (see http://www. scipy.org), relying on maxclust to obtain the slice of the resulting hierarchy that leads to k (or approx. k) clusters. 15We used the SI implementation of Pedregosa et al. (2011); see http://scikit-learn.org/. We also experimented with the Dunn Index (Dunn, 1974) and the Davies-Bouldin Index (1979), but we obtained similar results. Davg(C1, C2) = SI(Ci) 84 (a) restaurants (b) laptops Figure 3: Silhouette Index (SI) results for Phase B, using the gold similarity matrix of Phase A. (a) restaurants (b) laptops Figure 4: SI results for Phase B, using the WNDS (with SP) similarity matrix of Phase A. 1, with higher values indicating better clustering. Figure 4 shows that when the similarity matrix of WNDS (with SP) is used, the SI scores deteriorate significantly; again, th</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rousseeuw</author>
</authors>
<title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis.</title>
<date>1987</date>
<journal>Journal of Computational and Applied Mathematics,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="29340" citStr="Rousseeuw, 1987" startWordPosition="4831" endWordPosition="4832">h affects the choice of clusters that are merged to produce coarser (higher-level) clusters: Dsingle(C1, C2) = min d(t1, t2) t1∈C1,t2∈C2 Dcompl(C1, C2) = max d(t1, t2) t1∈C1,t2∈C2 1 E E d(t1, t2) |C1||C2 |t1∈C1 t2∈C2 Complete linkage tends to produce more compact clusters, compared to single linkage, with average linkage being in between. Ward minimizes the total in-cluster variance; consult Milligan (1980) for further details.14 4.2 Phase B experimental results To evaluate the k clusters produced at each aspect granularity by the different linkage criteria, we used the Silhouette Index (SI) (Rousseeuw, 1987), a cluster evaluation measure that considers both inter- and intra-cluster coherence.15 Given a set of clusters {C1, ... , Ck}, each SI (Ci) is defined as: 1 Ci |bj − a SI (Ci) = |Ci|·� max(bj, aj), where aj is the mean distance from the j-th instance of Ci to the other instances in Ci, and bj is the mean distance from the j-th instance of Ci to the instances in the cluster nearest to Ci. Then: Ek 1 SI({C1,...,Ck}) = k · i=1 We always use the correct (gold) distances of the instances (terms) when computing the SI scores. As shown in Fig. 3, no linkage criterion clearly outperforms the others,</context>
</contexts>
<marker>Rousseeuw, 1987</marker>
<rawString>P. Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20(1):53–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R T McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of ACLHLT,</booktitle>
<pages>308--316</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="10486" citStr="Titov and McDonald, 2008" startWordPosition="1703" endWordPosition="1706">sent our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between </context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>I. Titov and R. T. McDonald. 2008a. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of the 46th Annual Meeting of ACLHLT, pages 308–316, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R T McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference of WWW,</booktitle>
<pages>111--120</pages>
<location>Beijing, China.</location>
<contexts>
<context position="10486" citStr="Titov and McDonald, 2008" startWordPosition="1703" endWordPosition="1706">sent our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between </context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>I. Titov and R. T. McDonald. 2008b. Modeling online reviews with multi-grain topic models. In Proceedings of the 17th International Conference of WWW, pages 111–120, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd ACL,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, NM, USA.</location>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z. Wu and M. Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd ACL, pages 133–138, Las Cruces, NM, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yu</author>
<author>Z Zha</author>
<author>M Wang</author>
<author>K Wang</author>
<author>T Chua</author>
</authors>
<title>Domain-assisted product aspect hierarchy generation: towards hierarchical organization of unstructured consumer reviews.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>140--150</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="10897" citStr="Yu et al. (2011)" startWordPosition="1773" endWordPosition="1776">., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. Kobayashi et al. (2007) proposed methods to extract aspect terms and relations between them, including hierarchical relations. They extract, however, relations by looking for clues in texts (e.g., particular phrases). By contrast, we employ similarity measures and hierarchical clustering, which allows us to group similar aspect terms even when they do not cooccur in texts. Also, in contrast to Kobayashi et al. (2007), we respect the consistency constraint discussed in Section 1. A similar task</context>
</contexts>
<marker>Yu, Zha, Wang, Wang, Chua, 2011</marker>
<rawString>J. Yu, Z. Zha, M. Wang, K. Wang, and T. Chua. 2011. Domain-assisted product aspect hierarchy generation: towards hierarchical organization of unstructured consumer reviews. In Proceedings of EMNLP, pages 140–150, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zesch</author>
<author>I Gurevych</author>
</authors>
<title>Wisdom of crowds versus wisdom of linguists - measuring the semantic relatedness of words.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="18828" citStr="Zesch and Gurevych, 2010" startWordPosition="3071" endWordPosition="3074">is the least common subsumer, i.e., the most specific common ancestor of the two senses in WordNet, and depth(s) is the depth of sense s in WordNet’s hierarchy. Most terms have multiple senses, however, and word sense disambiguation methods (Navigli, 2009) are not yet robust enough. Hence, when given two aspect terms ti, ti0, rather than particular senses of the terms, a simplistic greedy approach is to compute the similarities of all the possible pairs of senses sij, si0j0 of ti, ti0, and take the similarity of ti, ti0 to be the maximum similarity of the sense pairs (Bollegala et al., 2007b; Zesch and Gurevych, 2010). We use this greedy approach with all the WordNet-based measures, but we also propose a sense pruning mechanism below, which improves their performance. In all the WordNetbased measures, if a term is not in WordNet, we take its similarity to any other term to be zero.7 The second measure, PATH (sij, si0j0), is simply the inverse of the length (plus one) of the shortest path connecting the senses sij, si0j0 in WordNet (Zhang et al., 2013). Again, the greedy approach can be used with terms having multiple senses. 6The Dice coefficient ranges from 0 to 1. There was a very large number of possibl</context>
</contexts>
<marker>Zesch, Gurevych, 2010</marker>
<rawString>T. Zesch and I. Gurevych. 2010. Wisdom of crowds versus wisdom of linguists - measuring the semantic relatedness of words. Natural Language Engineering, 16(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhai</author>
<author>B Liu</author>
<author>H Xu</author>
<author>P Jia</author>
</authors>
<title>Grouping product features using semi-supervised learning with soft-constraints.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference of COLING,</booktitle>
<pages>1272--1280</pages>
<location>Beijing, China.</location>
<contexts>
<context position="10380" citStr="Zhai et al., 2010" startWordPosition="1685" endWordPosition="1688">es (e.g., politicians, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-</context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2010</marker>
<rawString>Z. Zhai, B. Liu, H. Xu, and P. Jia. 2010. Grouping product features using semi-supervised learning with soft-constraints. In Proceedings of the 23rd International Conference of COLING, pages 1272– 1280, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhai</author>
<author>B Liu</author>
<author>H Xu</author>
<author>P Jia</author>
</authors>
<title>Clustering product features for opinion mining.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th International Conference of WSDM,</booktitle>
<pages>347--354</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="10400" citStr="Zhai et al., 2011" startWordPosition="1689" endWordPosition="1692">ns, organizations). Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B, respectively. Section 5 concludes. 2 Related work Most existing approaches to aspect aggregation aim to produce a single, flat partitioning of aspect terms into aspect groups, rather than aspect groups at multiple granularities. The most common approaches (Liu, 2012) are to aggregate only synonyms or near-synonyms, using WordNet (Liu et al., 2005), statistics from corpora (Chen et al., 2006; Bollegala et al., 2007a; Lin and Wu, 2009), or semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), or to cluster the aspect terms using (latent) topic models (Titov and McDonald, 2008a; Guo et al., 2009; Brody and Elhadad, 2010; Jo and Oh, 2011). Topic models do not perform better than other methods (Zhai et al., 2010), and their clusters may overlap.2 The topic model of Titov et al. (2008b) uses two granularity levels; we consider many more (3–10 levels). Carenini et al. (2005) used a predefined domainspecific taxonomy and similarity measures to aggregate related terms. Yu et al. (2011) used a tailored version of an existing taxonomy. By contrast, we assume no domain-specific taxonomy. K</context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2011</marker>
<rawString>Z. Zhai, B. Liu, H. Xu, and P. Jia. 2011. Clustering product features for opinion mining. In Proceedings of the 4th International Conference of WSDM, pages 347–354, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhang</author>
<author>A Gentile</author>
<author>F Ciravegna</author>
</authors>
<title>Recent advances in methods of lexical semantic relatedness - a survey. Natural Language Engineering,</title>
<date>2013</date>
<location>FirstView(1):1–69.</location>
<contexts>
<context position="7548" citStr="Zhang et al., 2013" startWordPosition="1232" endWordPosition="1235">hase A and a linkage criterion; a hierarchy like the ones of Fig. 2 is first formed via agglomerative clustering, and fewer or more clusters (for different values of k) are then obtained by using different slices of the hierarchy, as already discussed. Our two-phase decomposition can also accommodate non-hierarchical clustering algorithms, provided that the consistency constraint is satisfied, but we consider only agglomerative hierarchical clustering in this paper. The decomposition in two phases has three main advantages. Firstly, it allows reusing previous work on term similarity measures (Zhang et al., 2013), which can be used to fill in the matrix of Phase A. Secondly, the decomposition allows different linkage criteria to be experimentally compared (in Phase B) using the same similarity matrix (of Phase A), i.e., the same distance 79 measure. Thirdly, the decomposition leads to high inter-annotator agreement, as we show experimentally. By contrast, in preliminary experiments we found that asking humans to directly evaluate aspect hierarchies produced by hierarchical clustering, or to manually create gold aspect hierarchies led to poor inter-annotator agreement. We show that existing term simila</context>
<context position="19270" citStr="Zhang et al., 2013" startWordPosition="3150" endWordPosition="3153">ble pairs of senses sij, si0j0 of ti, ti0, and take the similarity of ti, ti0 to be the maximum similarity of the sense pairs (Bollegala et al., 2007b; Zesch and Gurevych, 2010). We use this greedy approach with all the WordNet-based measures, but we also propose a sense pruning mechanism below, which improves their performance. In all the WordNetbased measures, if a term is not in WordNet, we take its similarity to any other term to be zero.7 The second measure, PATH (sij, si0j0), is simply the inverse of the length (plus one) of the shortest path connecting the senses sij, si0j0 in WordNet (Zhang et al., 2013). Again, the greedy approach can be used with terms having multiple senses. 6The Dice coefficient ranges from 0 to 1. There was a very large number of possible responses the judges could provide and, hence, it would be inappropriate to use Cohen’s K. 7This never happened in the restaurants dataset. In the laptops dataset, it only happened for ‘hard drive’ and ‘battery life’. We use the NLTK implementation of the first four measures (see http://nltk.org/) and our own implementation of the distributional similarity measure. The third measure is Lin’s (1998), defined as: 2 · ic(lcs(sij, si0j0)) L</context>
<context position="22569" citStr="Zhang et al., 2013" startWordPosition="3705" endWordPosition="3708">@Brown 0.403 0.469 0.419 0.539 DS 0.283 0.517 (0.283) (0.517) AVG 0.499 0.352 0.537 0.426 WN 0.490 0.328 0.530 0.395 WNDS 0.523 0.453 0.545 0.546 Table 2: Phase A results (Pearson correlation to gold similarities) with and without sense pruning. prune senses that are very irrelevant to the particular domain (e.g., laptops). This sense pruning mechanism is novel, and we show experimentally that it improves the performance of all the WordNet-based similarity measures we examined. We also implemented a distributional similarity measure (Harris, 1968; Pad´o and Lapata, 2007; Cimiano et al., 2009; Zhang et al., 2013). Following Lin and Wu (2009), for each aspect term t, we create a vector V(t) = (PMI (t, wl), ... , PMI (t, wn)). The vector components are the Pointwise Mutual Information scores of t and each word wi of a corpus: P(t, wi) PMI (t, wi) = − log P(t) · P(wi) We treat P(t, wi) as the probability of t, wi cooccurring in the same sentence, and we use the (laptop or restaurant) reviews of our datasets as the corpus to estimate the probabilities. The distributional similarity DS(t, t&apos;) of two aspect terms t, t&apos; is the cosine similarity of v(t),v(t&apos;).10 Finally, we tried combinations of the similarit</context>
</contexts>
<marker>Zhang, Gentile, Ciravegna, 2013</marker>
<rawString>Z. Zhang, A. Gentile, and F. Ciravegna. 2013. Recent advances in methods of lexical semantic relatedness - a survey. Natural Language Engineering, FirstView(1):1–69.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>