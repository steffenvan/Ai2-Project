<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000065">
<title confidence="0.974626">
Multi-Document Summarization of Evaluative Text
</title>
<author confidence="0.992647">
Giuseppe Carenini, Raymond Ng, and Adam Pauls
</author>
<affiliation confidence="0.9974895">
Deptartment of Computer Science
University of British Columbia Vancouver, Canada
</affiliation>
<email confidence="0.99399">
carenini,rng,adpauls @cs.ubc.ca
</email>
<sectionHeader confidence="0.998552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991692307692">
We present and compare two approaches
to the task of summarizing evaluative ar-
guments. The first is a sentence extraction-
based approach while the second is a lan-
guage generation-based approach. We
evaluate these approaches in a user study
and find that they quantitatively perform
equally well. Qualitatively, however, we
find that they perform well for different but
complementary reasons. We conclude that
an effective method for summarizing eval-
uative arguments must effectively synthe-
size the two approaches.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971644067797">
Many organizations are faced with the challenge
of summarizing large corpora of text data. One im-
portant application is evaluative text, i.e. any doc-
ument expressing an evaluation of an entity as ei-
ther positive or negative. For example, many web-
sites collect large quantities of online customer re-
views of consumer electronics. Summaries of this
literature could be of great strategic value to prod-
uct designers, planners and manufacturers. There
are other equally important commercial applica-
tions, such as the summarization of travel logs, and
non-commercial applications, such as the summa-
rization of candidate reviews.
The general problem we consider in this paper
is how to effectively summarize a large corpora of
evaluative text about a single entity (e.g., a prod-
uct). In contrast, most previous work on multi-
document summarization has focused on factual
text (e.g., news (McKeown et al., 2002), biogra-
phies (Zhou et al., 2004)). For factual documents,
the goal of a summarizer is to select the most im-
portant facts and present them in a sensible or-
dering while avoiding repetition. Previous work
has shown that this can be effectively achieved by
carefully extracting and ordering the most infor-
mative sentences from the original documents in
a domain-independent way. Notice however that
when the source documents are assumed to con-
tain inconsistent information (e.g., conflicting re-
ports of a natural disaster (White et al., 2002)),
a different approach is needed. The summarizer
needs first to extract the information from the doc-
uments, then process such information to identify
overlaps and inconsistencies between the different
sources and finally produce a summary that points
out and explain those inconsistencies.
A corpus of evaluative text typically contains a
large number of possibly inconsistent ‘facts’ (i.e.
opinions), as opinions on the same entity feature
may be uniform or varied. Thus, summarizing a
corpus of evaluative text is much more similar to
summarizing conflicting reports than a consistent
set of factual documents. When there are diverse
opinions on the same issue, the different perspec-
tives need to be included in the summary.
Based on this observation, we argue that any
strategy to effectively summarize evaluative text
about a single entity should rely on a preliminary
phase of information extraction from the target
corpus. In particular, the summarizer should at
least know for each document: what features of
the entity were evaluated, the polarity of the eval-
uations and their strengths.
In this paper, we explore this hypothesis by con-
sidering two alternative approaches. First, we de-
veloped a sentence-extraction based summarizer
that uses the information extracted from the cor-
pus to select and rank sentences from the corpus.
We implemented this system, called MEAD*, by
</bodyText>
<page confidence="0.997929">
305
</page>
<bodyText confidence="0.99467572">
adapting MEAD (Radev et al., 2003), an open-
source framework for multi-document summariza-
tion. Second, we developed a summarizer that
produces summaries primarily by generating lan-
guage from the information extracted from the
corpus. We implemented this system, called the
Summarizer of Evaluative Arguments (SEA), by
adapting the Generator of Evaluative Arguments
(GEA) (Carenini and Moore, expected 2006) a
framework for generating user tailored evaluative
arguments.
We have performed an empirical formative eval-
uation of MEAD* and SEA in a user study. In
this evaluation, we also tested the effectiveness of
human generated summaries (HGS) as a topline
and of summaries generated by MEAD without
access to the extracted information as a baseline.
The results indicate that SEA and MEAD* quan-
titatively perform equally well above MEAD and
below HGS. Qualitatively, we find that they per-
form well for different but complementary rea-
sons. While SEA appears to provide a more gen-
eral overview of the source text, MEAD* seems to
provide a more varied language and detail about
customer opinions.
</bodyText>
<sectionHeader confidence="0.977667" genericHeader="method">
2 Information Extraction from
Evaluative Text
</sectionHeader>
<subsectionHeader confidence="0.998592">
2.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.994782173913044">
Knowledge extraction from evaluative text about
a single entity is typically decomposed into three
distinct phases: the determination of features of
the entity evaluated in the text, the strength of
each evaluation, and the polarity of each evalua-
tion. For instance, the information extracted from
the sentence “The menus are very easy to navi-
gate but the user preference dialog is somewhat
difficult to locate.” should be that the “menus”
and the “user preference dialog” features are eval-
uated, and that the “menus” receive a very posi-
tive evaluation while the “user preference dialog”
is evaluated rather negatively.
For these tasks, we adopt the approach de-
scribed in detail in (Carenini et al., 2005). This ap-
proach relies on the work of (Hu and Liu, 2004a)
for the tasks of strength and polarity determina-
tion. For the task of feature extraction, it en-
hances earlier work (Hu and Liu, 2004c) by map-
ping the extracted features into a hierarchy of fea-
tures which describes the entity of interest. The re-
sulting mapping reduces redundancy and provides
conceptual organization of the extracted features.
</bodyText>
<figureCaption confidence="0.9939315">
Figure 1: Partial view of UDF taxonomies for a
digital camera.
</figureCaption>
<bodyText confidence="0.9995958">
Before continuing, we shall describe the ter-
minology we use when discussing the extracted
knowledge. The features evaluated in a corpus of
reviews and extracted by following Hu and Liu’s
approach are called Crude Features.
</bodyText>
<equation confidence="0.976636">
CF cfj j 1 n
</equation>
<bodyText confidence="0.981422346153846">
For example, crude features for a digital cam-
era might include “picture quality”, “viewfinder”,
and “lens”. Each sentence sk in the corpus con-
tains a set of evaluations (of crude features) called
evalsk . Each evaluation contains both a polar-
ity and a strength represented as an integer in the
range 3 2 1 1 2 3 where 3 is the
most positive possible evaluation and 3 is the
most negative possible evaluation.
There is also a hierarchical set of possibly more
abstract user-defined features 1
UDF ud fi i 1 m
See Figure 1 for a sample UDF. The process of hi-
erarchically organizing the extracted features pro-
duces a mapping from CF to UDF features (see
(Carenini et al., 2005) for details). We call the set
of crude features mapped to the user-defined fea-
ture ud fi map ud fi . For example, the crude fea-
tures “unresponsiveness”, “delay”, and “lag time”
would all be mapped to the ud f “delay between
shots”.
For each cfj, there is a set of polarity and
strength evaluations ps cfj corresponding to
each evaluation of cfj in the corpus. We call the
set of polarity/strength evaluations directly associ-
ated with ud fi
</bodyText>
<equation confidence="0.669408">
PSi ps cfj
cfjεmapudfi
</equation>
<bodyText confidence="0.974283">
The total set of polarity/strength evaluations as-
sociated with ud fi, including its descendants, is
</bodyText>
<footnote confidence="0.539522333333333">
1We call them here user-deft ned features for consistency
with (Carenini et al., 2005). In this paper, they are not as-
sumed to be and are not in practice deft ned by the user.
</footnote>
<figure confidence="0.998891157894737">
Camera
Lens
Digital Zoom
Optical Zoom
...
Editing/Viewing
Viewft nder
...
Flash
...
Image
Image Type
TIFF
JPEG
...
Resolution
Effective Pixels
Aspect Ratio
...
</figure>
<page confidence="0.740494">
306
</page>
<equation confidence="0.477324666666667">
TPSi PSi
udfkεdescudfi
PSk
</equation>
<bodyText confidence="0.995015">
where desc ud fi refers to all descendants of ud fi.
</bodyText>
<sectionHeader confidence="0.981508" genericHeader="method">
3 MEAD*: Sentence Extraction
</sectionHeader>
<bodyText confidence="0.999880571428572">
Most modern summarization systems use sen-
tences extracted from the source text as the ba-
sis for summarization (see (Nat, 2005b) for a rep-
resentative sample). Extraction-based approaches
have the advantage of avoiding the difficult task
of natural language generation, thus maintaining
domain-independence because the system need
not be aware of specialized vocabulary for its tar-
get domain. The main disadvantage of extraction-
based approaches is the poor linguistic coherence
of the extracted summaries.
Because of the widespread and well-developed
use of sentence extractors in summarization, we
chose to develop our own sentence extractor as
a first attempt at summarizing evaluative argu-
ments. To do this, we adapted MEAD (Radev et
al., 2003), an open-source framework for multi-
document summarization, to suit our purposes.
We refer to our adapted version of MEAD as
MEAD*. The MEAD framework decomposes
sentence extraction into three steps: (i) Feature
Calculation: Some numerical feature(s) are cal-
culated for each sentence, for example, a score
based on document position and a score based on
the TF*IDF of a sentence. (ii) Classification: The
features calculated during step (i) are combined
into a single numerical score for each sentence.
(iii) Reranking: The numerical score for each sen-
tence is adjusted relative to other sentences. This
allows the system to avoid redundancy in the final
set of sentences by lowering the score of sentences
which are similar to already selected sentences.
We found from early experimentation that
the most informative sentences could be accu-
rately determined by examining the extracted CFs.
Thus, we created our own sentence-level feature
based on the number, strength, and polarity of CFs
extracted for each sentence.
During system development, we found this
measure to be effective because it was sensitive
to the number of CFs mentioned in a given sen-
tence as well as to the strength of the evaluation for
each CF. However, many sentences may have the
same CF sum score (especially sentences which
contain an evaluation for only one CF). In such
cases, we used the MEAD 3.072 centroid feature
as a ‘tie-breaker’. The centroid is a common fea-
ture in multidocument summarization (cf. (Radev
et al., 2003), (Saggion and Gaizauskas, 2004)).
At the reranking stage, we adopted a different
algorithm than the default in MEAD. We placed
each sentence which contained an evaluation of a
given CF into a ‘bucket’ for that CF. Because a
sentence could contain more than one CF, a sen-
tence could be placed in multiple buckets. We
then selected the top-ranked sentence from each
bucket, starting with the bucket containing the
most sentences (largestps cfj), never selecting
the same sentence twice. Once one sentence had
been selected from each bucket, the process was
repeated3. This selection algorithm accomplishes
two important tasks: firstly, it avoids redundancy
by only selecting one sentence to represent each
CF (unless all other CFs have already been rep-
resented), and secondly, it gives priority to CFs
which are mentioned more frequently in the text.
The sentence selection algorithm permits us to
select an arbitrary number of sentences to fit a de-
sired word length. We then ordered the sentences
according to a primitive discourse planning strat-
egy in which the most general CF (i.e. the CF
mapped to the topmost node in the UDF) is dis-
cussed first. The remaining sentences were then
ordered according to a depth-first traversal of the
UDF hierarchy. In this way, general features are
followed immediately by their more specific chil-
dren in the hierarchy.
</bodyText>
<sectionHeader confidence="0.979246" genericHeader="method">
4 SEA: Natural Language Generation
</sectionHeader>
<bodyText confidence="0.999919555555556">
The extraction-based approach described in the
previous section has several disadvantages. We al-
ready discussed problems with the linguistic co-
herence of the summary, but more specific prob-
lems arise in our particular task of summarizing
a corpus of evaluative text. Firstly, sentence ex-
traction does not give the reader any explicit infor-
mation about of the distribution of evaluations, for
example, how many users mentioned a given fea-
</bodyText>
<footnote confidence="0.995603">
2The centroid calculation requires an IDF database. We
constructed an IDF database from several corpora of reviews
and a set of stop words.
3In practice the process would only be repeated in sum-
maries long enough to contain sentences for each CF, which
is very rare.
</footnote>
<figure confidence="0.793392">
CF sum sk
∑
psiε evalsk
psi
</figure>
<page confidence="0.984146">
307
</page>
<bodyText confidence="0.999606933333333">
ture and whether user opinions were uniform or
varied. It also does not give an aggregate view of
user evaluations because typically it only presents
one evaluation for each CF. It may be that a very
positive evaluation for one CF was selected for ex-
traction, even though most evaluations were only
somewhat positive and some were even negative.
We thus also developed a system, SEA, that
presents such information in generated natural lan-
guage. This system calculates several important
characteristics of the source corpus by aggregat-
ing the extracted information including the CF to
UDF mapping. We first describe these character-
istics and then discuss their presentation in natural
language.
</bodyText>
<subsectionHeader confidence="0.999283">
4.1 Aggregation of Extracted Information
</subsectionHeader>
<bodyText confidence="0.999733333333333">
In order to provide an aggregate view of the eval-
uation expressed in a corpus of evaluative text a
summarizer should at least determine: (i) which
features of the evaluated entity were most ‘impor-
tant’ to the users (ii) some aggregate of the user
opinions for the important features (iii) the distri-
bution of those opinions and (iv) the reasons be-
hind each user opinion. We now discuss each of
these aspects in detail.
</bodyText>
<subsubsectionHeader confidence="0.623486">
4.1.1 Feature Selection
</subsubsectionHeader>
<bodyText confidence="0.9980654">
We approach the task of selecting the most ‘im-
portant’ features by defining a ‘measure of impor-
tance’ for each feature of the evaluated entity. We
define the ‘direct importance’ of a feature in the
UDF as
</bodyText>
<equation confidence="0.464331">
psk2
</equation>
<bodyText confidence="0.987020538461539">
where by ‘direct’ we mean the importance de-
rived only from that feature and not from its chil-
dren. This metric produces high scores for fea-
tures which either occur frequently in the corpus
or have strong evaluations (or both). This ‘direct’
measure of importance, however, is incomplete, as
each non-leaf node in the UDF effectively serves
a dual purpose. It is both a feature upon which
a user might comment and a category for group-
ing its sub-features. Thus, a non-leaf node should
be important if either its children are important or
the node itself is important (or both). To this end,
we have defined the total measure of importance
moi udfi as
dir moi ud fi ch udfi 0
a dir moi ud fi
1 a Eudfkechudfi moi ud fk otherwise
where ch udfi refers to the children of udfi in
the hierarchy and a is some real parameter in the
range 05 1. In this measure, the importance of a
node is a combination of its direct importance and
of the importance of its children. The parameter
a may be adjusted to vary the relative weight of
the parent and children. We used a 09 for our
experiments. This setting resulted in more infor-
mative summaries during system development.
In order to perform feature selection using this
metric, we must also define a selection procedure.
The most obvious is a simple greedy selection –
sort the nodes in the UDF by the measure of im-
portance and select the most important node until
a desired number of features is included. How-
ever, because a node derives part of its ‘impor-
tance’ from its children, it is possible for a node’s
importance to be dominated by one or more of its
children. Including both the child and parent node
would be redundant because most of the informa-
tion is contained in the child. We thus choose a
dynamic greedy selection algorithm in which we
recalculate the importance of each node after each
round of selection, with all previously selected
nodes removed from the tree. In this way, if a
node that dominates its parent’s importance is se-
lected, its parent’s importance will be reduced dur-
ing later rounds of selection. This approach mim-
ics the behaviour of several sentence extraction-
based summarizers (e.g. (Schiffman et al., 2002;
Saggion and Gaizauskas, 2004)) which define a
metric for sentence importance and then greed-
ily select the sentence which minimizes similarity
with already selected sentences and maximizes in-
formativeness.
</bodyText>
<subsubsectionHeader confidence="0.898645">
4.1.2 Opinion Aggregation
</subsubsectionHeader>
<bodyText confidence="0.988908888888889">
We approach the task of aggregating opinions
from the source text in a similar fashion to de-
termining the measure of importance. We cal-
culate an ‘orientation’ for each UDF by aggre-
gating the polarity/strength evaluations of all re-
lated CFs into a single value. We define the ‘di-
rect orientation’ of a UDF as the average of the
strength/polarity evaluations of all related CFs
dir ori ud fi avg
</bodyText>
<equation confidence="0.8808932">
pskePSi
dir moi ud fi
E
pskePSi
psk
</equation>
<page confidence="0.995626">
308
</page>
<bodyText confidence="0.993335">
As with our measure of importance, we must
also include the orientation of a feature’s children
in its orientation. Because a feature in the UDF
conceptually groups its children, the orientation of
a feature should include some information about
the orientation of its children. We thus define the
total orientation oriud fi as
dir ori ud fi chud fi 0
</bodyText>
<figure confidence="0.273475">
a dir ori ud fi
1 a avgudfkEchudfi ori udfk otherwise
</figure>
<bodyText confidence="0.99953075">
This metric produces a real number between 3
and 3 which serves as an aggregate of user opin-
ions for a feature. We use the same value of a as
in moi ud fi .
</bodyText>
<subsectionHeader confidence="0.897182">
4.1.3 Distribution of Opinions
</subsectionHeader>
<bodyText confidence="0.999948454545455">
Communicating user opinions to the reader is
not simply a matter of classifying each feature
as being evaluated negatively or positively – the
reader may also want to know if all users evalu-
ated a feature in a similar way or if evaluations
were varied. We thus also need a method of de-
termining the modality of the distribution of user
opinions. We calculate the sum of positive polar-
ity/strength evaluations (or negative if ori ud fi is
negative) for a node and its children as a fraction
of all polarity/strength evaluations
</bodyText>
<equation confidence="0.916888666666667">
EviE pskETPSisignumpsk vi
signumoriudfi
EviETPSivi
</equation>
<bodyText confidence="0.999015571428571">
If this fraction is very close to 0.5, this indicates
an almost perfect split of user opinions on that
features. So we classify the feature as ‘bimodal’
and we report this fact to the user. Otherwise, the
feature is classified as ‘unimodal’, i.e. we need
only to communicate one aggregate opinion to the
reader.
</bodyText>
<subsectionHeader confidence="0.7883185">
4.2 Generating Language: Adapting the
Generator of Evaluative Arguments
</subsectionHeader>
<bodyText confidence="0.99834256">
(GEA)
The first task in generating a natural language
summary from the information extracted from the
corpus is content selection. This task is accom-
plished in SEA by the feature selection strategy
described in Section 4.1.1. After content selection,
the automatic generation of a natural language
summary involves the following additional tasks
(Reiter and Dale, 2000): (i) structuring the content
by ordering and grouping the selected content ele-
ments as well as by specifying discourse relations
(e.g., supporting vs. opposing evidence) between
the resulting groups; (ii) microplanning, which
involves lexical selection and sentence planning;
and (iii) sentence realization, which produces En-
glish text from the output of the microplanner. For
most of these tasks, we have adapted the Genera-
tor of Evaluative Arguments (GEA) (Carenini and
Moore, expected 2006), a framework for generat-
ing user tailored evaluative arguments. For lack of
space we cannot discuss the details here. These
are provided on the online version of this paper,
which is available at the first author’s Web page.
That version also includes a detailed discussion of
related and future work.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9998166">
We evaluated our two summarizers by performing
a user study in which four treatments were consid-
ered: SEA, MEAD*, human-written summaries
as a topline and summaries generated by MEAD
(with all options set to default) as a baseline.
</bodyText>
<subsectionHeader confidence="0.983664">
5.1 The Experiment
</subsectionHeader>
<bodyText confidence="0.999780142857143">
Twenty-eight undergraduate students participated
in our experiment, seven for each treatment. Each
participant was given a set of 20 customer reviews
randomly selected from a corpus of reviews. In
each treatment three participants received reviews
from a corpus of 46 reviews of the Canon G3 dig-
ital camera and four received them from a cor-
pus of 101 reviews of the Apex 2600 Progressive
Scan DVD player, both obtained from Hu and Liu
(2004b). The reviews from these corpora which
serve as input to our systems have been manually
annotated with crude features, strength, and polar-
ity. We used this ‘gold standard’ for crude fea-
ture, strength, and polarity extraction because we
wanted our experiments to focus on our summary
and not be confounded by errors in the knowledge
extraction phase.
The participant was told to pretend that they
work for the manufacturer of the product (either
Canon or Apex). They were told that they would
have to provide a 100 word summary of the re-
views to the quality assurance department. The
purpose of these instructions was to prime the user
to the task of looking for information worthy of
summarization. They were then given 20 minutes
to explore the set of reviews.
After 20 minutes, the participant was asked to
stop. The participant was then given a set of in-
</bodyText>
<page confidence="0.99819">
309
</page>
<bodyText confidence="0.9966035625">
structions which explained that the company was
testing a computer-based system for automatically
generating a summary of the reviews s/he has
been reading. S/he was then shown a 100 word
summary of the 20 reviews generated either by
MEAD, MEAD*, SEA, or written by a human 4.
Figure 2 shows four summaries of the same 20 re-
views, one of each type.
In order to facilitate their analysis, summaries
were displayed in a web browser. The upper por-
tion of the browser contained the text of the sum-
mary with ‘footnotes’ linking to reviews on which
the summary was based. For MEAD and MEAD*,
for each sentence the footnote pointed to the re-
view from which the sentence had been extracted.
For SEA and human-generated summaries, for
each aggregate evaluation the footnote pointed to
the review containing a sample sentence on which
that evaluation was based. In all summaries, click-
ing on one of the footnotes caused the correspond-
ing review to be displayed in which the appropri-
ate sentence was highlighted.
Once finished, the participant was asked to fill
out a questionnaire assessing the summary along
several dimensions related to its effectiveness. The
participant could still access the summary while
s/he worked on the questionnaire.
Our questionnaire consisted of nine questions.
The first five questions were the SEE linguistic
well-formedness questions used at the 2005 Doc-
ument Understanding Conference (DUC) (Nat,
2005a). The next three questions were designed to
assess the content of the summary. We based our
questions on the Responsive evaluation at DUC
2005; however, we were interested in a more spe-
cific evaluation of the content that one overall
rank. As such, we split the content into the fol-
lowing three separate questions:
(Recall) The summary contains all of the information
you would have included from the source text.
(Precision) The summary contains no information you
would NOT have included from the source text.
(Accuracy) All information expressed in the summary
accurately reflects the information contained in the
source text.
The final question in the questionnaire asked the
participant to rank the overall quality of the sum-
mary holistically.
</bodyText>
<footnote confidence="0.724082">
4For automatically generated summaries, we generated
the longest possible summary with less than 100 words.
</footnote>
<subsectionHeader confidence="0.998767">
5.2 Quantitative Results
</subsectionHeader>
<bodyText confidence="0.99901136">
Table 1 consists of two parts. The first top half fo-
cuses on linguistic questions while the second bot-
tom half focuses on content issues. We performed
a two-way ANOVA test with summary type as
rows and the question sets as columns. Overall,
it is easy to conclude that MEAD* and SEA per-
formed at a roughly equal level, while the baseline
MEAD performed significantly lower and the Hu-
man summarizer significantly higher (p 001).
When individual questions/categories are consid-
ered, there are few questions that differentiate be-
tween MEAD* and SEA with a p-value below
0.05. The primary reason is our small sample size.
Nonetheless, if we relax the p-value threshold, we
can make the following observations/hypotheses.
To validate some of these hypotheses, we would
conduct a larger user study in future work.
On the linguistic side, the average
score suggests the ordering of: Human
MEAD SEA MEAD. Both MEAD* and
SEA are also on par with the median DUC score
(Nat, 2005b). On the focus question, in fact,
SEA’s score is tied with the Human’s score, which
may be a beneficial effect of the UDF guiding
content structuring in a top-down fashion. It
is also interesting to see that SEA outperforms
MEAD* on grammaticality, showing that the
generative text approach may be more effective
than simply extracting sentences on this aspect of
grammaticality. On the other hand, MEAD* out-
performs SEA on non-redundancy, and structure
and coherence. SEA’s disappointing performance
on structure and coherence was among the most
surprising finding. One possibility is that our
adaptation of GEA content structuring strategy
was suboptimal or even inappropriate. We plan to
investigate possible causes in the future.
On the content side, the average score sug-
gests the ordering of: Human SEA MEAD
MEAD. As for the three individual content ques-
tions, on the recall one, both SEA and MEAD*
were dominated by the Human summarizer. This
indicates that both SEA and MEAD* omit some
features considered important. We feel that if a
longer summary was allowed, the gap between the
two and the Human summarizer would be nar-
rower. The precision question is somewhat sur-
prising in that SEA actually performs better than
the Human summarizer. In general this indicates
that the feature selection strategy was quite suc-
</bodyText>
<page confidence="0.992808">
310
</page>
<bodyText confidence="0.986641">
MEAD*: Bottom line , well made camera , easy to use , very flexible and powerful features to include the ability to use external flash and lense / fi lters
choices . 1It has a beautiful design , lots of features , very easy to use , very confi gurable and customizable , and the battery duration is amazing ! Great
colors, pictures and white balance. The camera is a dream to operate in automode , but also gives tremendous flexibility in aperture priority, shutter priority
, and manual modes . I ’d highly recommend this camera for anyone who is looking for excellent quality pictures and a combination of ease of use and the
flexibility to get advanced with many options to adjust if you like.
SEA: Almost all users loved the Canon G3 possibly because some users thought the physical appearance was very good. Furthermore, several users found
the manual features and the special features to be very good. Also, some users liked the convenience because some users thought the battery was excellent.
Finally, some users found the editing/viewing interface to be good despite the fact that several customers really disliked the viewfi nder . However, there
were some negative evaluations. Some customers thought the lens was poor even though some customers found the optical zoom capability to be excellent.
Most customers thought the quality of the images was very good.
MEAD: I am a software engineer and am very keen into technical details of everything i buy , i spend around 3 months before buying the digital camera ;
and i must say , g3 worth every single cent i spent on it . I do n’t write many reviews but i ’m compelled to do so with this camera . I spent a lot of time
comparing different cameras , and i realized that there is not such thing as the best digital camera . I bought my canon g3 about a month ago and i have to
say i am very satisfi ed .
Human: The Canon G3 was received exceedingly well. Consumer reviews from novice photographers to semi-professional all listed an impressive number
of attributes, they claim makes this camera superior in the market. Customers are pleased with the many features the camera offers, and state that the camera
is easy to use and universally accessible. Picture quality, long lasting battery life, size and style were all highlighted in glowing reviews. One flaw in the
camera frequently mentioned was the lens which partially obsructs the view through the view fi nder, however most claimed it was only a minor annoyance
since they used the LCD sceen.
</bodyText>
<figureCaption confidence="0.963675">
Figure 2: Sample automatically generated summaries.
</figureCaption>
<table confidence="0.997579">
SEA MEAD* MEAD Human DUC
Question Avg. Dev. Avg. Dev. Avg. Dev. Avg. Dev. Med. Min. Max.
Grammaticality 3.43 1.13 2.71 0.76 3.14 0.90 4.29 0.76 3.86 2.60 4.34
Non-redundancy 3.14 1.57 3.86 0.90 3.57 0.98 4.43 1.13 4.44 3.96 4.74
Referential clarity 3.86 0.69 4.00 1.15 3.00 1.15 4.71 0.49 2.98 2.16 4.14
Focus 4.14 0.69 3.71 1.60 2.29 1.60 4.14 0.69 3.16 2.38 3.94
Structure and Coherence 2.29 0.95 3.00 1.41 1.86 0.90 4.43 0.53 2.10 1.60 3.24
Linguistic Average 3.37 1.19 3.46 1.24 2.77 1.24 4.4 0.74 3.31 2.54 4.08
Recall 2.33 1.03 2.57 0.98 1.57 0.53 3.57 1.27 – – –
Precision 4.17 1.17 3.50 1.38 2.17 1.17 3.86 1.07 – – –
Accuracy 4.00 0.82 3.57 1.13 2.57 1.4 4.29 0.76 – – –
Content Average 3.5 1.26 3.21 1.2 2.1 1.12 3.9 1.04 – – –
Overall 3.14 0.69 3.14 1.21 2.14 1.21 4.43 0.79 – – –
Macro Average 3.39 0.73 3.34 0.51 2.48 0.65 4.24 0.34 – – –
</table>
<tableCaption confidence="0.8597405">
Table 1: Quantative results of user responses to our questionnaire on a scale from 1 (Strongly Disagree)
to 5 (Strongly Agree).
</tableCaption>
<bodyText confidence="0.999722181818182">
cessful. Finally, for the accuracy question, SEA is
closer to the Human summarizer than MEAD*. In
sum, recall that for evaluative text, it is very pos-
sible that different reviews express different opin-
ions on the same question. Thus, for the summa-
rization of evaluative text, when there is a differ-
ence in opinions, it is desirable that the summary
accurately covers both angles or conveys the dis-
agreement. On this count, according to the scores
on the precision and accuracy questions, SEA ap-
pears to outperform MEAD*.
</bodyText>
<subsectionHeader confidence="0.996177">
5.3 Qualitative Results
</subsectionHeader>
<bodyText confidence="0.991890514285714">
MEAD*: The most interesting aspect of the
comments made by participants who evaluated
MEAD*-based summaries was that they rarely
criticized the summary for being nothing more
than a set of extracted sentences. For example,
one user claimed that the summary had a “simple
sentence first, then ideas are fleshed out, and ends
with a fun impact statement”. Other users, while
noticing that the summary was solely quotation,
still felt the summary was adequate (“Shouldn’t
just copy consumers ... However, it summarized
various aspects of the consumer’s opinions ... ”).
With regard to content, two main complaints by
participants were: (i) the summary did not reflect
overall opinions (e.g., included positive evalua-
tions of the DVD player even though most eval-
uations were negative), and (ii) the evaluations
of some features were repeated. The first com-
plaint is consistent with the relatively low score of
MEAD* on the accuracy question.
We could address this complaint by only includ-
ing sentences whose CF evaluations have polari-
ties matching the majority polarity for each CF.
The second complaint could be avoided by not
selecting sentences which contain evaluations of
CFs already in the summary.
SEA: Comments about the structure of the sum-
maries generated by SEA mentioned the “coherent
but robotic” feel of the summaries, the repetition
of “users/customers” and lack of pronoun use, the
lack of flow between sentences, and the repeated
use of generic terms such as “good”. These prob-
lems are largely a result of simplistic microplan-
ning and seems to contradict SEA’s disappointing
performance on the structure and coherence ques-
</bodyText>
<page confidence="0.99596">
311
</page>
<bodyText confidence="0.99942024">
tion.
In terms of content, there were two main sets of
complaints. Firstly, participants wanted more “de-
tails” in the summary, for instance, they wanted
examples of the “manual features” mentioned by
SEA. Note that this is one complaint absent from
the MEAD* summaries. That is, where the
MEAD* summaries lack structure but contain de-
tail, SEA summaries provide a general, structured
overview while lacking in specifics.
The other set of complaints related to the prob-
lem that participants disagreed with the choice of
features in the summary. We note that this is actu-
ally a problem common to MEAD* and even the
Human summarizer. The best example to illus-
trate this point is on the “physical appearance” of
the digital camera. One reason participants may
have disagreed with the summarizer’s decision to
include the physical appearance in the summary
is that some evaluations of the physical appear-
ance were quite subtle. For example, the sentence
“This camera has a design flaw” was annotated in
our corpus as evaluating the physical appearance,
although not all readers would agree with that an-
notation.
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999934652173913">
We have presented and compared a sentence
extraction- and language generation based ap-
proach to summarizing evaluative text. A forma-
tive user study of our MEAD* and SEA summa-
rizers found that, quantitatively, they performed
equally well relative to each other, while signifi-
cantly outperforming a baseline standard approach
to multidocument summarization. Trends that we
identified in the results as well as qualitative com-
ments from participants in the user study indicate
that the summarizers have different strengths and
weaknesses. On the one hand, though providing
varied language and detail about customer opin-
ions, MEAD* summaries lack in accuracy and
precision, failing to give and overview of the opin-
ions expressed in the evaluative text. On the other,
SEA summaries provide a general overview of the
source text, while sounding ‘robotic’, repetitive,
and surprisingly rather incoherent.
Some of these differences are, fortunately, quite
complimentary. We plan in the future to investi-
gate how SEA and MEAD* can be integrated and
improved.
</bodyText>
<sectionHeader confidence="0.998392" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999608363636364">
G. Carenini and J. D. Moore. expected 2006. Generat-
ing and evaluating evaluative arguments. AI Journal
(accepted for publication, contact fi rst author for a
draft).
G. Carenini, R.T Ng, and E. Zwart. 2005. Extracting
knowlede from evaluative text. In Proc. Third Inter-
national Conference on Knowledge Capture.
M. Hu and B. Liu. 2004a. Mining and summariz-
ing customer reviews. In Proc. of the 10th ACM
SIGKDD Conf., pages 168–177, New York, NY,
USA. ACM Press.
Minqing Hu and Bing Liu. 2004b. Fea-
ture based summary of customer reviews dataset.
http://www.cs.uic.edu/ liub/FBS/FBS.html.
Minqing Hu and Bing Liu. 2004c. Mining opinion
features in customer reviews. In Proc. AAAI.
K. R. McKeown, R. Barzilay, D. Evans, V. Hatzi-
vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,
B. Schiffman, and S. Sigelman. 2002. Tracking and
summarizing news on a daily basis with Columbia’s
Newsblaster. In Proceedings of the HLT Conf.
2005a. Linguistic quality questions from the
2005 DUC. http://duc.nist.gov/duc2005/quality-
questions.txt.
2005b. Proc. of DUC 2005.
D. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer,
H. Qi, A. elebi, D. Liu, and E. Drabek. 2003. Eval-
uation challenges in large-scale document summa-
rization. In Proc. of the 41stACL, pages 375–382.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Studies in Natural
Language Processing. Cambridge University Press.
H. Saggion and R. Gaizauskas. 2004. Multi-document
summarization by cluster/profi le relevance and re-
dundancy removal. In Proc. of DUC04.
B. Schiffman, A. Nenkova, and K. McKeown. 2002.
Experiments in multidocument summarization. In
Proc. of HLT02, San Diego, Ca.
M. White, C. Cardie, and V. Ng. 2002. Detecting
discrepancies in numeric estimates using multidoc-
ument hypertext summaries. In Proc of HLT02.
L. Zhou, M. Ticrea, and E. Hovy. 2004. Multi-
document biography summarization. In Proceed-
ings of EMNLP.
</reference>
<page confidence="0.998666">
312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823540">
<title confidence="0.998582">Multi-Document Summarization of Evaluative Text</title>
<author confidence="0.998479">Giuseppe Carenini</author>
<author confidence="0.998479">Raymond Ng</author>
<author confidence="0.998479">Adam Pauls</author>
<affiliation confidence="0.9658575">Deptartment of Computer Science University of British Columbia Vancouver, Canada</affiliation>
<email confidence="0.895191">carenini,rng,adpauls@cs.ubc.ca</email>
<abstract confidence="0.999629142857143">We present and compare two approaches to the task of summarizing evaluative arguments. The first is a sentence extractionbased approach while the second is a language generation-based approach. We evaluate these approaches in a user study and find that they quantitatively perform equally well. Qualitatively, however, we find that they perform well for different but complementary reasons. We conclude that an effective method for summarizing evaluative arguments must effectively synthesize the two approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>G Carenini</author>
<author>J D Moore</author>
</authors>
<title>expected 2006. Generating and evaluating evaluative arguments. AI Journal (accepted for publication, contact fi rst author for a draft).</title>
<marker>Carenini, Moore, </marker>
<rawString>G. Carenini and J. D. Moore. expected 2006. Generating and evaluating evaluative arguments. AI Journal (accepted for publication, contact fi rst author for a draft).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>R T Ng</author>
<author>E Zwart</author>
</authors>
<title>Extracting knowlede from evaluative text.</title>
<date>2005</date>
<booktitle>In Proc. Third International Conference on Knowledge Capture.</booktitle>
<contexts>
<context position="5442" citStr="Carenini et al., 2005" startWordPosition="841" endWordPosition="844">osed into three distinct phases: the determination of features of the entity evaluated in the text, the strength of each evaluation, and the polarity of each evaluation. For instance, the information extracted from the sentence “The menus are very easy to navigate but the user preference dialog is somewhat difficult to locate.” should be that the “menus” and the “user preference dialog” features are evaluated, and that the “menus” receive a very positive evaluation while the “user preference dialog” is evaluated rather negatively. For these tasks, we adopt the approach described in detail in (Carenini et al., 2005). This approach relies on the work of (Hu and Liu, 2004a) for the tasks of strength and polarity determination. For the task of feature extraction, it enhances earlier work (Hu and Liu, 2004c) by mapping the extracted features into a hierarchy of features which describes the entity of interest. The resulting mapping reduces redundancy and provides conceptual organization of the extracted features. Figure 1: Partial view of UDF taxonomies for a digital camera. Before continuing, we shall describe the terminology we use when discussing the extracted knowledge. The features evaluated in a corpus </context>
<context position="6816" citStr="Carenini et al., 2005" startWordPosition="1082" endWordPosition="1085">ht include “picture quality”, “viewfinder”, and “lens”. Each sentence sk in the corpus contains a set of evaluations (of crude features) called evalsk . Each evaluation contains both a polarity and a strength represented as an integer in the range 3 2 1 1 2 3 where 3 is the most positive possible evaluation and 3 is the most negative possible evaluation. There is also a hierarchical set of possibly more abstract user-defined features 1 UDF ud fi i 1 m See Figure 1 for a sample UDF. The process of hierarchically organizing the extracted features produces a mapping from CF to UDF features (see (Carenini et al., 2005) for details). We call the set of crude features mapped to the user-defined feature ud fi map ud fi . For example, the crude features “unresponsiveness”, “delay”, and “lag time” would all be mapped to the ud f “delay between shots”. For each cfj, there is a set of polarity and strength evaluations ps cfj corresponding to each evaluation of cfj in the corpus. We call the set of polarity/strength evaluations directly associated with ud fi PSi ps cfj cfjεmapudfi The total set of polarity/strength evaluations associated with ud fi, including its descendants, is 1We call them here user-deft ned fea</context>
</contexts>
<marker>Carenini, Ng, Zwart, 2005</marker>
<rawString>G. Carenini, R.T Ng, and E. Zwart. 2005. Extracting knowlede from evaluative text. In Proc. Third International Conference on Knowledge Capture.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proc. of the 10th ACM SIGKDD Conf.,</booktitle>
<pages>168--177</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5497" citStr="Hu and Liu, 2004" startWordPosition="853" endWordPosition="856">s of the entity evaluated in the text, the strength of each evaluation, and the polarity of each evaluation. For instance, the information extracted from the sentence “The menus are very easy to navigate but the user preference dialog is somewhat difficult to locate.” should be that the “menus” and the “user preference dialog” features are evaluated, and that the “menus” receive a very positive evaluation while the “user preference dialog” is evaluated rather negatively. For these tasks, we adopt the approach described in detail in (Carenini et al., 2005). This approach relies on the work of (Hu and Liu, 2004a) for the tasks of strength and polarity determination. For the task of feature extraction, it enhances earlier work (Hu and Liu, 2004c) by mapping the extracted features into a hierarchy of features which describes the entity of interest. The resulting mapping reduces redundancy and provides conceptual organization of the extracted features. Figure 1: Partial view of UDF taxonomies for a digital camera. Before continuing, we shall describe the terminology we use when discussing the extracted knowledge. The features evaluated in a corpus of reviews and extracted by following Hu and Liu’s appr</context>
<context position="19872" citStr="Hu and Liu (2004" startWordPosition="3263" endWordPosition="3266">reatments were considered: SEA, MEAD*, human-written summaries as a topline and summaries generated by MEAD (with all options set to default) as a baseline. 5.1 The Experiment Twenty-eight undergraduate students participated in our experiment, seven for each treatment. Each participant was given a set of 20 customer reviews randomly selected from a corpus of reviews. In each treatment three participants received reviews from a corpus of 46 reviews of the Canon G3 digital camera and four received them from a corpus of 101 reviews of the Apex 2600 Progressive Scan DVD player, both obtained from Hu and Liu (2004b). The reviews from these corpora which serve as input to our systems have been manually annotated with crude features, strength, and polarity. We used this ‘gold standard’ for crude feature, strength, and polarity extraction because we wanted our experiments to focus on our summary and not be confounded by errors in the knowledge extraction phase. The participant was told to pretend that they work for the manufacturer of the product (either Canon or Apex). They were told that they would have to provide a 100 word summary of the reviews to the quality assurance department. The purpose of thes</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004a. Mining and summarizing customer reviews. In Proc. of the 10th ACM SIGKDD Conf., pages 168–177, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Feature based summary of customer reviews dataset.</title>
<date>2004</date>
<note>http://www.cs.uic.edu/ liub/FBS/FBS.html.</note>
<contexts>
<context position="5497" citStr="Hu and Liu, 2004" startWordPosition="853" endWordPosition="856">s of the entity evaluated in the text, the strength of each evaluation, and the polarity of each evaluation. For instance, the information extracted from the sentence “The menus are very easy to navigate but the user preference dialog is somewhat difficult to locate.” should be that the “menus” and the “user preference dialog” features are evaluated, and that the “menus” receive a very positive evaluation while the “user preference dialog” is evaluated rather negatively. For these tasks, we adopt the approach described in detail in (Carenini et al., 2005). This approach relies on the work of (Hu and Liu, 2004a) for the tasks of strength and polarity determination. For the task of feature extraction, it enhances earlier work (Hu and Liu, 2004c) by mapping the extracted features into a hierarchy of features which describes the entity of interest. The resulting mapping reduces redundancy and provides conceptual organization of the extracted features. Figure 1: Partial view of UDF taxonomies for a digital camera. Before continuing, we shall describe the terminology we use when discussing the extracted knowledge. The features evaluated in a corpus of reviews and extracted by following Hu and Liu’s appr</context>
<context position="19872" citStr="Hu and Liu (2004" startWordPosition="3263" endWordPosition="3266">reatments were considered: SEA, MEAD*, human-written summaries as a topline and summaries generated by MEAD (with all options set to default) as a baseline. 5.1 The Experiment Twenty-eight undergraduate students participated in our experiment, seven for each treatment. Each participant was given a set of 20 customer reviews randomly selected from a corpus of reviews. In each treatment three participants received reviews from a corpus of 46 reviews of the Canon G3 digital camera and four received them from a corpus of 101 reviews of the Apex 2600 Progressive Scan DVD player, both obtained from Hu and Liu (2004b). The reviews from these corpora which serve as input to our systems have been manually annotated with crude features, strength, and polarity. We used this ‘gold standard’ for crude feature, strength, and polarity extraction because we wanted our experiments to focus on our summary and not be confounded by errors in the knowledge extraction phase. The participant was told to pretend that they work for the manufacturer of the product (either Canon or Apex). They were told that they would have to provide a 100 word summary of the reviews to the quality assurance department. The purpose of thes</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004b. Feature based summary of customer reviews dataset. http://www.cs.uic.edu/ liub/FBS/FBS.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proc. AAAI.</booktitle>
<contexts>
<context position="5497" citStr="Hu and Liu, 2004" startWordPosition="853" endWordPosition="856">s of the entity evaluated in the text, the strength of each evaluation, and the polarity of each evaluation. For instance, the information extracted from the sentence “The menus are very easy to navigate but the user preference dialog is somewhat difficult to locate.” should be that the “menus” and the “user preference dialog” features are evaluated, and that the “menus” receive a very positive evaluation while the “user preference dialog” is evaluated rather negatively. For these tasks, we adopt the approach described in detail in (Carenini et al., 2005). This approach relies on the work of (Hu and Liu, 2004a) for the tasks of strength and polarity determination. For the task of feature extraction, it enhances earlier work (Hu and Liu, 2004c) by mapping the extracted features into a hierarchy of features which describes the entity of interest. The resulting mapping reduces redundancy and provides conceptual organization of the extracted features. Figure 1: Partial view of UDF taxonomies for a digital camera. Before continuing, we shall describe the terminology we use when discussing the extracted knowledge. The features evaluated in a corpus of reviews and extracted by following Hu and Liu’s appr</context>
<context position="19872" citStr="Hu and Liu (2004" startWordPosition="3263" endWordPosition="3266">reatments were considered: SEA, MEAD*, human-written summaries as a topline and summaries generated by MEAD (with all options set to default) as a baseline. 5.1 The Experiment Twenty-eight undergraduate students participated in our experiment, seven for each treatment. Each participant was given a set of 20 customer reviews randomly selected from a corpus of reviews. In each treatment three participants received reviews from a corpus of 46 reviews of the Canon G3 digital camera and four received them from a corpus of 101 reviews of the Apex 2600 Progressive Scan DVD player, both obtained from Hu and Liu (2004b). The reviews from these corpora which serve as input to our systems have been manually annotated with crude features, strength, and polarity. We used this ‘gold standard’ for crude feature, strength, and polarity extraction because we wanted our experiments to focus on our summary and not be confounded by errors in the knowledge extraction phase. The participant was told to pretend that they work for the manufacturer of the product (either Canon or Apex). They were told that they would have to provide a 100 word summary of the reviews to the quality assurance department. The purpose of thes</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004c. Mining opinion features in customer reviews. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
<author>R Barzilay</author>
<author>D Evans</author>
<author>V Hatzivassiloglou</author>
<author>J L Klavans</author>
<author>A Nenkova</author>
<author>C Sable</author>
<author>B Schiffman</author>
<author>S Sigelman</author>
</authors>
<title>Tracking and summarizing news on a daily basis with Columbia’s Newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of the HLT Conf.</booktitle>
<contexts>
<context position="1647" citStr="McKeown et al., 2002" startWordPosition="244" endWordPosition="247">line customer reviews of consumer electronics. Summaries of this literature could be of great strategic value to product designers, planners and manufacturers. There are other equally important commercial applications, such as the summarization of travel logs, and non-commercial applications, such as the summarization of candidate reviews. The general problem we consider in this paper is how to effectively summarize a large corpora of evaluative text about a single entity (e.g., a product). In contrast, most previous work on multidocument summarization has focused on factual text (e.g., news (McKeown et al., 2002), biographies (Zhou et al., 2004)). For factual documents, the goal of a summarizer is to select the most important facts and present them in a sensible ordering while avoiding repetition. Previous work has shown that this can be effectively achieved by carefully extracting and ordering the most informative sentences from the original documents in a domain-independent way. Notice however that when the source documents are assumed to contain inconsistent information (e.g., conflicting reports of a natural disaster (White et al., 2002)), a different approach is needed. The summarizer needs first</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>K. R. McKeown, R. Barzilay, D. Evans, V. Hatzivassiloglou, J. L. Klavans, A. Nenkova, C. Sable, B. Schiffman, and S. Sigelman. 2002. Tracking and summarizing news on a daily basis with Columbia’s Newsblaster. In Proceedings of the HLT Conf.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>2005a. Linguistic quality questions from the 2005 DUC. http://duc.nist.gov/duc2005/qualityquestions.txt. 2005b. Proc. of DUC</booktitle>
<marker>2005</marker>
<rawString>2005a. Linguistic quality questions from the 2005 DUC. http://duc.nist.gov/duc2005/qualityquestions.txt. 2005b. Proc. of DUC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>S Teufel</author>
<author>H Saggion</author>
<author>W Lam</author>
<author>J Blitzer</author>
<author>H Qi</author>
<author>A elebi</author>
<author>D Liu</author>
<author>E Drabek</author>
</authors>
<title>Evaluation challenges in large-scale document summarization.</title>
<date>2003</date>
<booktitle>In Proc. of the 41stACL,</booktitle>
<pages>375--382</pages>
<contexts>
<context position="3608" citStr="Radev et al., 2003" startWordPosition="553" endWordPosition="556">ize evaluative text about a single entity should rely on a preliminary phase of information extraction from the target corpus. In particular, the summarizer should at least know for each document: what features of the entity were evaluated, the polarity of the evaluations and their strengths. In this paper, we explore this hypothesis by considering two alternative approaches. First, we developed a sentence-extraction based summarizer that uses the information extracted from the corpus to select and rank sentences from the corpus. We implemented this system, called MEAD*, by 305 adapting MEAD (Radev et al., 2003), an opensource framework for multi-document summarization. Second, we developed a summarizer that produces summaries primarily by generating language from the information extracted from the corpus. We implemented this system, called the Summarizer of Evaluative Arguments (SEA), by adapting the Generator of Evaluative Arguments (GEA) (Carenini and Moore, expected 2006) a framework for generating user tailored evaluative arguments. We have performed an empirical formative evaluation of MEAD* and SEA in a user study. In this evaluation, we also tested the effectiveness of human generated summari</context>
<context position="8574" citStr="Radev et al., 2003" startWordPosition="1372" endWordPosition="1375">) for a representative sample). Extraction-based approaches have the advantage of avoiding the difficult task of natural language generation, thus maintaining domain-independence because the system need not be aware of specialized vocabulary for its target domain. The main disadvantage of extractionbased approaches is the poor linguistic coherence of the extracted summaries. Because of the widespread and well-developed use of sentence extractors in summarization, we chose to develop our own sentence extractor as a first attempt at summarizing evaluative arguments. To do this, we adapted MEAD (Radev et al., 2003), an open-source framework for multidocument summarization, to suit our purposes. We refer to our adapted version of MEAD as MEAD*. The MEAD framework decomposes sentence extraction into three steps: (i) Feature Calculation: Some numerical feature(s) are calculated for each sentence, for example, a score based on document position and a score based on the TF*IDF of a sentence. (ii) Classification: The features calculated during step (i) are combined into a single numerical score for each sentence. (iii) Reranking: The numerical score for each sentence is adjusted relative to other sentences. T</context>
<context position="10079" citStr="Radev et al., 2003" startWordPosition="1616" endWordPosition="1619">CFs. Thus, we created our own sentence-level feature based on the number, strength, and polarity of CFs extracted for each sentence. During system development, we found this measure to be effective because it was sensitive to the number of CFs mentioned in a given sentence as well as to the strength of the evaluation for each CF. However, many sentences may have the same CF sum score (especially sentences which contain an evaluation for only one CF). In such cases, we used the MEAD 3.072 centroid feature as a ‘tie-breaker’. The centroid is a common feature in multidocument summarization (cf. (Radev et al., 2003), (Saggion and Gaizauskas, 2004)). At the reranking stage, we adopted a different algorithm than the default in MEAD. We placed each sentence which contained an evaluation of a given CF into a ‘bucket’ for that CF. Because a sentence could contain more than one CF, a sentence could be placed in multiple buckets. We then selected the top-ranked sentence from each bucket, starting with the bucket containing the most sentences (largestps cfj), never selecting the same sentence twice. Once one sentence had been selected from each bucket, the process was repeated3. This selection algorithm accompli</context>
</contexts>
<marker>Radev, Teufel, Saggion, Lam, Blitzer, Qi, elebi, Liu, Drabek, 2003</marker>
<rawString>D. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. elebi, D. Liu, and E. Drabek. 2003. Evaluation challenges in large-scale document summarization. In Proc. of the 41stACL, pages 375–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems. Studies in Natural Language Processing.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18371" citStr="Reiter and Dale, 2000" startWordPosition="3023" endWordPosition="3026"> feature as ‘bimodal’ and we report this fact to the user. Otherwise, the feature is classified as ‘unimodal’, i.e. we need only to communicate one aggregate opinion to the reader. 4.2 Generating Language: Adapting the Generator of Evaluative Arguments (GEA) The first task in generating a natural language summary from the information extracted from the corpus is content selection. This task is accomplished in SEA by the feature selection strategy described in Section 4.1.1. After content selection, the automatic generation of a natural language summary involves the following additional tasks (Reiter and Dale, 2000): (i) structuring the content by ordering and grouping the selected content elements as well as by specifying discourse relations (e.g., supporting vs. opposing evidence) between the resulting groups; (ii) microplanning, which involves lexical selection and sentence planning; and (iii) sentence realization, which produces English text from the output of the microplanner. For most of these tasks, we have adapted the Generator of Evaluative Arguments (GEA) (Carenini and Moore, expected 2006), a framework for generating user tailored evaluative arguments. For lack of space we cannot discuss the d</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Studies in Natural Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>R Gaizauskas</author>
</authors>
<title>Multi-document summarization by cluster/profi le relevance and redundancy removal.</title>
<date>2004</date>
<booktitle>In Proc. of DUC04.</booktitle>
<contexts>
<context position="10111" citStr="Saggion and Gaizauskas, 2004" startWordPosition="1620" endWordPosition="1623">our own sentence-level feature based on the number, strength, and polarity of CFs extracted for each sentence. During system development, we found this measure to be effective because it was sensitive to the number of CFs mentioned in a given sentence as well as to the strength of the evaluation for each CF. However, many sentences may have the same CF sum score (especially sentences which contain an evaluation for only one CF). In such cases, we used the MEAD 3.072 centroid feature as a ‘tie-breaker’. The centroid is a common feature in multidocument summarization (cf. (Radev et al., 2003), (Saggion and Gaizauskas, 2004)). At the reranking stage, we adopted a different algorithm than the default in MEAD. We placed each sentence which contained an evaluation of a given CF into a ‘bucket’ for that CF. Because a sentence could contain more than one CF, a sentence could be placed in multiple buckets. We then selected the top-ranked sentence from each bucket, starting with the bucket containing the most sentences (largestps cfj), never selecting the same sentence twice. Once one sentence had been selected from each bucket, the process was repeated3. This selection algorithm accomplishes two important tasks: firstl</context>
<context position="15810" citStr="Saggion and Gaizauskas, 2004" startWordPosition="2590" endWordPosition="2593">or more of its children. Including both the child and parent node would be redundant because most of the information is contained in the child. We thus choose a dynamic greedy selection algorithm in which we recalculate the importance of each node after each round of selection, with all previously selected nodes removed from the tree. In this way, if a node that dominates its parent’s importance is selected, its parent’s importance will be reduced during later rounds of selection. This approach mimics the behaviour of several sentence extractionbased summarizers (e.g. (Schiffman et al., 2002; Saggion and Gaizauskas, 2004)) which define a metric for sentence importance and then greedily select the sentence which minimizes similarity with already selected sentences and maximizes informativeness. 4.1.2 Opinion Aggregation We approach the task of aggregating opinions from the source text in a similar fashion to determining the measure of importance. We calculate an ‘orientation’ for each UDF by aggregating the polarity/strength evaluations of all related CFs into a single value. We define the ‘direct orientation’ of a UDF as the average of the strength/polarity evaluations of all related CFs dir ori ud fi avg pske</context>
</contexts>
<marker>Saggion, Gaizauskas, 2004</marker>
<rawString>H. Saggion and R. Gaizauskas. 2004. Multi-document summarization by cluster/profi le relevance and redundancy removal. In Proc. of DUC04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Schiffman</author>
<author>A Nenkova</author>
<author>K McKeown</author>
</authors>
<title>Experiments in multidocument summarization.</title>
<date>2002</date>
<booktitle>In Proc. of HLT02,</booktitle>
<location>San Diego, Ca.</location>
<contexts>
<context position="15779" citStr="Schiffman et al., 2002" startWordPosition="2586" endWordPosition="2589"> to be dominated by one or more of its children. Including both the child and parent node would be redundant because most of the information is contained in the child. We thus choose a dynamic greedy selection algorithm in which we recalculate the importance of each node after each round of selection, with all previously selected nodes removed from the tree. In this way, if a node that dominates its parent’s importance is selected, its parent’s importance will be reduced during later rounds of selection. This approach mimics the behaviour of several sentence extractionbased summarizers (e.g. (Schiffman et al., 2002; Saggion and Gaizauskas, 2004)) which define a metric for sentence importance and then greedily select the sentence which minimizes similarity with already selected sentences and maximizes informativeness. 4.1.2 Opinion Aggregation We approach the task of aggregating opinions from the source text in a similar fashion to determining the measure of importance. We calculate an ‘orientation’ for each UDF by aggregating the polarity/strength evaluations of all related CFs into a single value. We define the ‘direct orientation’ of a UDF as the average of the strength/polarity evaluations of all rel</context>
</contexts>
<marker>Schiffman, Nenkova, McKeown, 2002</marker>
<rawString>B. Schiffman, A. Nenkova, and K. McKeown. 2002. Experiments in multidocument summarization. In Proc. of HLT02, San Diego, Ca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>C Cardie</author>
<author>V Ng</author>
</authors>
<title>Detecting discrepancies in numeric estimates using multidocument hypertext summaries.</title>
<date>2002</date>
<booktitle>In Proc of HLT02.</booktitle>
<contexts>
<context position="2186" citStr="White et al., 2002" startWordPosition="331" endWordPosition="334">ent summarization has focused on factual text (e.g., news (McKeown et al., 2002), biographies (Zhou et al., 2004)). For factual documents, the goal of a summarizer is to select the most important facts and present them in a sensible ordering while avoiding repetition. Previous work has shown that this can be effectively achieved by carefully extracting and ordering the most informative sentences from the original documents in a domain-independent way. Notice however that when the source documents are assumed to contain inconsistent information (e.g., conflicting reports of a natural disaster (White et al., 2002)), a different approach is needed. The summarizer needs first to extract the information from the documents, then process such information to identify overlaps and inconsistencies between the different sources and finally produce a summary that points out and explain those inconsistencies. A corpus of evaluative text typically contains a large number of possibly inconsistent ‘facts’ (i.e. opinions), as opinions on the same entity feature may be uniform or varied. Thus, summarizing a corpus of evaluative text is much more similar to summarizing conflicting reports than a consistent set of factu</context>
</contexts>
<marker>White, Cardie, Ng, 2002</marker>
<rawString>M. White, C. Cardie, and V. Ng. 2002. Detecting discrepancies in numeric estimates using multidocument hypertext summaries. In Proc of HLT02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>M Ticrea</author>
<author>E Hovy</author>
</authors>
<title>Multidocument biography summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1680" citStr="Zhou et al., 2004" startWordPosition="250" endWordPosition="253">ectronics. Summaries of this literature could be of great strategic value to product designers, planners and manufacturers. There are other equally important commercial applications, such as the summarization of travel logs, and non-commercial applications, such as the summarization of candidate reviews. The general problem we consider in this paper is how to effectively summarize a large corpora of evaluative text about a single entity (e.g., a product). In contrast, most previous work on multidocument summarization has focused on factual text (e.g., news (McKeown et al., 2002), biographies (Zhou et al., 2004)). For factual documents, the goal of a summarizer is to select the most important facts and present them in a sensible ordering while avoiding repetition. Previous work has shown that this can be effectively achieved by carefully extracting and ordering the most informative sentences from the original documents in a domain-independent way. Notice however that when the source documents are assumed to contain inconsistent information (e.g., conflicting reports of a natural disaster (White et al., 2002)), a different approach is needed. The summarizer needs first to extract the information from </context>
</contexts>
<marker>Zhou, Ticrea, Hovy, 2004</marker>
<rawString>L. Zhou, M. Ticrea, and E. Hovy. 2004. Multidocument biography summarization. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>