<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000055">
<title confidence="0.961906">
Enlarged Search Space for SITG Parsing
</title>
<author confidence="0.85918">
Guillem Gascó, Joan-Andreu Sánchez, José-Miguel Benedí
</author>
<affiliation confidence="0.72734">
Institut Tecnològic d’Informàtica, Universitat Politècnica de València
Camí de Vera s/n, València, 46022, Spain
</affiliation>
<email confidence="0.99042">
ggasco@iti.upv.es,{jandreu,jbenedi}@dsic.upv.es
</email>
<sectionHeader confidence="0.997263" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.982076111111111">
Stochastic Inversion Transduction Grammars
constitute a powerful formalism in Machine
Translation for which an efficient Dynamic
Programming parsing algorithm exists. In this
work, we review this parsing algorithm and
propose important modifications that enlarge
the search space. These modifications allow
the parsing algorithm to search for more and
better solutions.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867652173913">
Syntax Machine Translation has received great at-
tention in the last few years, especially for pairs of
languages that are sufficiently non-monotonic. Sev-
eral works have explored the use of syntax for Ma-
chine Translation (Wu, 1997; Chiang, 2007). In
(Wu, 1997), Stochastic Inverse Transduction Gram-
mars (SITGs) were introduced for describing struc-
turally correlated pairs of languages. SITGs can be
used to simultaneously analyze two strings from dif-
ferent languages and to correlate them. An efficient
Dynamic Programming parsing algorithm for SITGs
was presented in (Wu, 1997). This algorithm is sim-
ilar to the CKY algorithm for Probabilistic Context
Free Grammars. The parsing algorithm does not al-
low the association of two items that have the empty
string in one of their sides. This limitation restricts
the search space and prevents the algorithm from ex-
ploring some valid parse trees.
In this paper, we review Wu’s parsing algorithm
for SITGs (referred to as the original algorithm) and
propose some modifications to increase the search
space in order to make it possible to find these valid
parse trees.
</bodyText>
<sectionHeader confidence="0.997011" genericHeader="method">
2 SITG Parsing
</sectionHeader>
<bodyText confidence="0.999362857142857">
SITGs (Wu, 1997) can be viewed as a restricted
subset of Stochastic Syntax-Directed Transduction
Grammars (Maryanski and Thomason, 1979). For-
mally, a SITG in Chomsky Normal Form can be
defined as a set of lexical rules that are noted as
A → x/c, A → c/y, A → x/y; direct syntac-
tic rules that are noted as A → [BC]; and inverse
syntactic rules that are noted as A → hBCi, where
A, B, C are non-terminal symbols, x, y are terminal
symbols, c is the empty string, and each rule has a
probability value p attached. The sum of the proba-
bilities of the rules with the same non-terminal in the
left side must be equal to 1. When a direct syntactic
rule is used in parsing, both strings are parsed with
the syntactic rule A → BC. When an inverse rule is
used in parsing, one string is parsed with the syntac-
tic rule A → BC, and the other string is parsed with
the syntactic rule A → CB.
An efficient Viterbi-like parsing algorithm that is
based on a Dynamic Programming Scheme was pro-
posed in (Wu, 1997). It allows us to obtain the most
probable parse tree that simultaneously analyzes two
strings, X = x1...x|X |and Y = y1...x|Y |, i.e. the
bilingual string X/Y . It has a time complexity of
O(|X|3|Y |3|R|), where |R |is the number of rules
of the grammar.
The parsing algorithm is based on the definition
of:
</bodyText>
<equation confidence="0.965571">
Sijkl(A) = Pr(A ⇒~ xi+1 ··· xj/yk+1 ··· yl)
</equation>
<bodyText confidence="0.9964306">
as the maximum probability of any parsing tree that
simultaneously generates the substrings xi+1 · · · xj
and yk+1 ··· yl from the non-terminal symbol A .
In (Wu, 1997), the parsing algorithm was defined
as follows:
</bodyText>
<page confidence="0.984555">
653
</page>
<footnote confidence="0.1494295">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 653–656,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</footnote>
<equation confidence="0.856292941176471">
S
a/b
1. Initialization
δi−1,i,k−1,k(A) = p(A —* xi/yk)
1 &lt; i &lt; JXJ,1 &lt; k &lt; JY J,
δi−1,i,k,k(A) = p(A —* xi/ǫ)
1 &lt; i &lt; JXJ,0 &lt; k &lt; JY J,
δi,i,k−1,k(A) = p(A —* ǫ/yk)
0 &lt; i &lt; JXJ,1 &lt; k &lt; JY J,
2. Recursion
For all A E N and
i, j, k, l such that { 0 &lt; i &lt; j &lt; JXJ,
j − i + l − k &gt; 2,
0&lt;k&lt;l&lt;JYJ, (1)
δijkl(A) = max(δ��
ijkl(A), δ��
ijkl(A))
</equation>
<bodyText confidence="0.705141">
where
</bodyText>
<equation confidence="0.936385">
δ��
ijkl(A)
p(A —* [BC])δiIkK(B)δIjKl(C) (2)
= max
</equation>
<figure confidence="0.833265333333333">
B,CEN
i&lt;I&lt;j,k&lt;K&lt;l
(I−i)(j−I)+(K−k)(l−K)&gt;0
(a) S (b)
S S
a/ǫ ǫ/b
</figure>
<figureCaption confidence="0.896165">
Figure 2: Parse tree (a) can be obtained with Wu’s algo-
rithm for a/b, but parse tree (b) cannot be obtained.
</figureCaption>
<equation confidence="0.947351">
(I−i)(j−I)+(K−k)(l−K) =� 0 in expression (2)
is not accomplished given that I = i or I = j, and
K = k or l = K (similarly in expression (3)).
</equation>
<bodyText confidence="0.999476818181818">
From now on, we will use the term non-explored
trees to denote the set of trees that are possible when
rules of the grammar are applied but cannot be ex-
plored with Wu’s algorithm. In fact, this situation
appears for other paired strings (see Fig. 3) in which
a string in one side is associated with the empty
string in the other side through rules that are not lexi-
cal rules. For example, in Fig. 3b, substring aa could
be associated with ǫ. However,this parse tree cannot
be obtained with the algorithm due to the search re-
strictions described above.
</bodyText>
<figure confidence="0.973408307692308">
a/ǫ
S
S
a/b
(a)
S
S
S S
S
ǫ/b
S
(b)
a/ǫ a/ǫ
</figure>
<bodyText confidence="0.949614333333333">
This algorithm cannot provide the correct parsing
tree in some situations. For example, consider the
SITG shown in Fig. 1. If the input pair is a/b,
</bodyText>
<equation confidence="0.756668666666667">
p S —* [SS] p S —* (SS)
q S —* ǫ/b q S —* a/ǫ
1 − 2p − 2q S — *a/b
</equation>
<figureCaption confidence="0.999653">
Figure 1: Example SITG.
</figureCaption>
<bodyText confidence="0.989799909090909">
this SITG provides the parse tree (a) that is shown in
Fig. 2 with probability 1 − 2p − 2q. However, the
parse tree (b) is more likely if 1 − 2p − 2q &lt; 2pq.
The above parsing algorithm is not able to obtain
this parse tree due to the restriction j − i+ l − k &gt; 2
in (1). This restriction does not allow the algo-
rithm to consider two subproblems in which each
substring has length 1 which have not been previ-
ously considered in the initialization step. Chang-
ing this restriction to j − i + l − k &gt; 2 is not
enough to tackle this situation since the restriction
</bodyText>
<figureCaption confidence="0.748302666666667">
Figure 3: Parse tree (a) can be obtained with Wu’s algo-
rithm for aa/b, but parse tree (b) would be more probable
if pq2 &gt; 1 − 2p − 2q.
</figureCaption>
<bodyText confidence="0.999399666666667">
The changes needed in the algorithm to be able to
find the sort of parsing trees described above are the
following:
</bodyText>
<listItem confidence="0.978562375">
• Changing restriction j − i + l − k &gt; 2 in (1) to
j − i + l − k &gt; 2. Note that this new restriction
is redundant and could be removed.
• Changing restriction (I −i)(j −I)+(K −k)(l −
K) =� 0 to ((j−I)+(l−K))*((I−i)+(K−k)) =�
0 in (2) and to ((j − I) + (K − k)) * ((I − i) +
(l − K)) =� 0 in (3) in order to guarantee the
algorithm’s termination.
</listItem>
<sectionHeader confidence="0.939693" genericHeader="method">
3 Search under SITG Constraints
</sectionHeader>
<bodyText confidence="0.988511">
The modifications that have been introduced in Sec-
tion 2 enlarge the search space and allow the parsing
</bodyText>
<page confidence="0.983838">
654
</page>
<figure confidence="0.801118571428571">
δ��
ijkl(A)
= max
B,CEN
i&lt;I&lt;j,k&lt;K&lt;l
(I−i)(j−I)+(K−k)(l−K)&gt;0
p(A —* (BC))δiIKl(B)δIjkK(C) (3)
</figure>
<bodyText confidence="0.999387833333333">
algorithm to explore a greater number of possible so-
lutions. We illustrate this situation with an example.
Consider the SITG introduced in Figure 1. Fig. 4
shows the possible complete matched trees for the
input pair a/b that are considered in the search pro-
cess with the modifications introduced.
</bodyText>
<figureCaption confidence="0.9898815">
Figure 4: Parse trees for input pair a/b that are taken into
account in the search process with the modifications.
</figureCaption>
<bodyText confidence="0.999498833333333">
Without these modifications, the parsing algo-
rithm only takes into account tree (a) of Fig. 4. For
this grammar, we have computed the growth in num-
ber of complete matched trees. Table 1 shows how
the search space grows notably with the modifica-
tions introduced.
</bodyText>
<table confidence="0.854923285714286">
n Wu’s alg. Modified alg. ratio
1 1 5 0.200
2 34 290 0.117
3 1,928 34,088 0.057
4 131,880 5,152,040 0.026
5 10,071,264 890,510,432 0.011
6 827,969,856 167,399,588,160 0.005
</table>
<tableCaption confidence="0.98663075">
Table 1: Growth in number of explored trees for the orig-
inal and modified parsing algorithms (n is the length of
the input pair strings and the last column represents the
ratio between columns two and three).
</tableCaption>
<bodyText confidence="0.999978111111111">
As a preliminary experiment and in order to eval-
uate empirically the Wu’s parsing algorithm versus
the modified algorithm, we parsed first 100K sen-
tence of German-English Europarl corpus. The lex-
ical rules in the Bracketing SITG used for pars-
ing were obtained from a probabilistic dictionary
by aligning with IBM3 model (NULL aligments
were also included). In this experiment, the modi-
fied algorithm obtained a more probable parse tree
for 6% of the sentences. If we added brackets to
the sentences separately with monolingual parsers,
we could use a parsing algorithm similar to the al-
gorithm that is described in (Sánchez and Benedí,
2006). The monolingual brackets restricted the
parse tree to those that were compatible with the
brackets. In that case the modified algorithm ob-
tained a more probable parse tree for 14% of the
sentences.
</bodyText>
<sectionHeader confidence="0.993541" genericHeader="method">
4 Inside Probability
</sectionHeader>
<bodyText confidence="0.983725033333333">
The parsing algorithm described above computes
the most likely parse tree for a given paired string
X/Y . However, in some cases (Wu, 1995; Huang
and Zhou, 2009), we need the inside probability
(Q0�jXj�0�jY j(S)), i.e., the probability that the gram-
mar assigns to the whole set of parse trees that yield
X/Y . If the maximizations are replaced by sums,
the algorithm can be used to compute the inside
probability. However, as stated above, the origi-
nal algorithm cannot find the whole set of trees for
a given paired string in some cases. These non-
explored trees have a probability greater than 0.
As an example, we computed the amount of prob-
ability lost in the inside computation using the origi-
nal algorithm with the grammar shown in Fig. 1. Let
F be the amount of probability of the non-explored
trees (the lost probability). It must be noted that
since height 1 trees are all reachable, we must accu-
mulate lost probability for trees of height 2 or more.
Hence, let -y be the amount of lost probability for
trees of height 2 or more. Note that all such trees
must have initially used the production S → SS in-
versely or directly. Thus, F = 2p · -y. Fig. 5 shows
the kinds of non-explored trees. Then -y is:
-y = 4·q2+2·2p·(1−2p)·-y+(2p)2·(2-y(1−-y)+-y2)
The first addend is the probability of the non-
explored trees of height 2 (Fig. 5a). The second ad-
dend is the probability that one of the subtrees uses
a syntactic production, this new subtree produces
a non-explored tree (2p · -y) and the other subtree
</bodyText>
<figureCaption confidence="0.967926125">
Figure 5: Partial representation of non-explored parse
trees from the non-terminal string SS introduced after
the first derivation step: (a) both non-terminals yield a
terminal in one side and the empty string in the other;
(b) one of the non-terminals uses a lexical production
and the other non-terminal yields a non-explored tree; (c)
both non-terminals use a syntactic production and one (or
both) yields a non-explored tree.
</figureCaption>
<figure confidence="0.99992">
b b
E
E
(a) (b)
S S (c) S(d) S (e)
S
a/b
S S
S S S S S
S
a E E a a
E E
a
b E E b
S (a) S (b) S(c)
S S S S S
S
</figure>
<page confidence="0.849439">
655
</page>
<figureCaption confidence="0.999888">
Figure 6: Amount of lost probability for values of p and q.
</figureCaption>
<bodyText confidence="0.998749142857143">
rewrites itself using a lexical production (1 − 2p).
Note that the non-explored tree can be yielded from
either the left or the right non-terminal, (Fig. 5b).
The third addend is the probability that both non-
terminals use a syntactic production (2p)2 and ei-
ther one (2(γ)(1−γ)) or both (γ2) subtrees are non-
explored trees (Fig. 5c). If we isolate F, we get
</bodyText>
<equation confidence="0.977085333333333">
V/
1 − 4p f 16p2 − 8p + 1 + 64p2q2
4p2
</equation>
<bodyText confidence="0.99995785">
Since the solution with the positive square root
takes values greater than 1, we can discard it.
Fig. 6 shows the probability accumulated in the
non-explored trees for values of p and q between
0 and 0.25 (higher values of p produce inconsistent
SITGs). That is the amount of probability lost in the
inside parsing for the whole language generated by
the grammar shown in Fig. 1.
In order to prove the loss of probability produced
by the original algorithm, we use the grammar in
Fig. 1 with p = q = 0.2. We parse all the paired
strings X/Y such that JXJ + JY J G l, where l is a
fixed maximum length. We repeat the same exper-
iment using the modified algorithm. Fig. 7 shows
the accumulated inside probabilities for both origi-
nal and modified algorithms and the theoretical max-
imums (1−F for the original algorithm and 1 for the
modified algorithm). Note that the computed results
approach the theoretical maximums and the modi-
fied algorithm covers the whole search space.
</bodyText>
<sectionHeader confidence="0.999829" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9998414">
SITGs have proven to be a powerful tool in Syntax
Machine Translation. However, the algorithms have
been proposed do not explore all the possible parse
trees. This work proposes modifications of the algo-
rithms to be able to explore the whole search space.
</bodyText>
<figureCaption confidence="0.9992455">
Figure 7: Accumulated inside probability for the original
and modified algorithms.
</figureCaption>
<bodyText confidence="0.999944666666667">
Using an example, we have shown that the modifi-
cations allow a complete search. As future work, we
plan to proove the correctness of the modified algo-
rithm and to study the impact of these modifications
on the use of SITGs for Machine Translation, and
the estimation of SITGs.
</bodyText>
<sectionHeader confidence="0.997786" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.983085">
Work supported by the EC (FSE), the Spanish Gov-
ernment (MICINN, &amp;quot;Plan E&amp;quot;) under grants MIPRCV
&amp;quot;Consolider Ingenio 2010&amp;quot; CSD2007-00018, iTrans2
TIN2009-14511 and the Generalitat Valenciana grant
Prometeo/2009/014 and BFPI/2007/117.
</bodyText>
<sectionHeader confidence="0.999058" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998372809523809">
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
S. Huang and B. Zhou. 2009. An em algorithm for scfg
in formal syntax-based translation. In ICASSP, pages
4813–4816, Taiwan, China, April.
F.J. Maryanski and M.T. Thomason. 1979. Properties of
stochastic syntax-directed tranlation schemata. Jour-
nal of Computer and Information Sciences, 8(2):89–
110.
J.A. Sánchez and J.M. Benedí. 2006. Stochastic in-
version transduction grammars for obtaining word
phrases for phrase-based statistical machine transla-
tion. In Proc. of Workshop on Statistical Machine
Translation. HLT-NAACL 06, pages 130–133.
D. Wu. 1995. Trainable coarse bilingual grammars for
parallel text bracketing. In Proceedings of the Third
Annual Workshop on Very Large Corpora, pages 69–
81.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putationalLinguistics, 23(3):377–404.
</reference>
<equation confidence="0.983076">
F = 2p ·
</equation>
<page confidence="0.996961">
656
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.755518">
<title confidence="0.954963">Enlarged Search Space for SITG Parsing</title>
<author confidence="0.80167">Guillem Gascó</author>
<author confidence="0.80167">Joan-Andreu Sánchez</author>
<author confidence="0.80167">José-Miguel</author>
<affiliation confidence="0.92793">Institut Tecnològic d’Informàtica, Universitat Politècnica de</affiliation>
<address confidence="0.934312">Camí de Vera s/n, València, 46022,</address>
<abstract confidence="0.9974111">Stochastic Inversion Transduction Grammars constitute a powerful formalism in Machine Translation for which an efficient Dynamic Programming parsing algorithm exists. In this work, we review this parsing algorithm and propose important modifications that enlarge the search space. These modifications allow the parsing algorithm to search for more and better solutions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="892" citStr="Chiang, 2007" startWordPosition="117" endWordPosition="118">sion Transduction Grammars constitute a powerful formalism in Machine Translation for which an efficient Dynamic Programming parsing algorithm exists. In this work, we review this parsing algorithm and propose important modifications that enlarge the search space. These modifications allow the parsing algorithm to search for more and better solutions. 1 Introduction Syntax Machine Translation has received great attention in the last few years, especially for pairs of languages that are sufficiently non-monotonic. Several works have explored the use of syntax for Machine Translation (Wu, 1997; Chiang, 2007). In (Wu, 1997), Stochastic Inverse Transduction Grammars (SITGs) were introduced for describing structurally correlated pairs of languages. SITGs can be used to simultaneously analyze two strings from different languages and to correlate them. An efficient Dynamic Programming parsing algorithm for SITGs was presented in (Wu, 1997). This algorithm is similar to the CKY algorithm for Probabilistic Context Free Grammars. The parsing algorithm does not allow the association of two items that have the empty string in one of their sides. This limitation restricts the search space and prevents the a</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Huang</author>
<author>B Zhou</author>
</authors>
<title>An em algorithm for scfg in formal syntax-based translation.</title>
<date>2009</date>
<booktitle>In ICASSP,</booktitle>
<pages>4813--4816</pages>
<location>Taiwan, China,</location>
<contexts>
<context position="8557" citStr="Huang and Zhou, 2009" startWordPosition="1538" endWordPosition="1541">tained a more probable parse tree for 6% of the sentences. If we added brackets to the sentences separately with monolingual parsers, we could use a parsing algorithm similar to the algorithm that is described in (Sánchez and Benedí, 2006). The monolingual brackets restricted the parse tree to those that were compatible with the brackets. In that case the modified algorithm obtained a more probable parse tree for 14% of the sentences. 4 Inside Probability The parsing algorithm described above computes the most likely parse tree for a given paired string X/Y . However, in some cases (Wu, 1995; Huang and Zhou, 2009), we need the inside probability (Q0�jXj�0�jY j(S)), i.e., the probability that the grammar assigns to the whole set of parse trees that yield X/Y . If the maximizations are replaced by sums, the algorithm can be used to compute the inside probability. However, as stated above, the original algorithm cannot find the whole set of trees for a given paired string in some cases. These nonexplored trees have a probability greater than 0. As an example, we computed the amount of probability lost in the inside computation using the original algorithm with the grammar shown in Fig. 1. Let F be the amo</context>
</contexts>
<marker>Huang, Zhou, 2009</marker>
<rawString>S. Huang and B. Zhou. 2009. An em algorithm for scfg in formal syntax-based translation. In ICASSP, pages 4813–4816, Taiwan, China, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Maryanski</author>
<author>M T Thomason</author>
</authors>
<title>Properties of stochastic syntax-directed tranlation schemata.</title>
<date>1979</date>
<journal>Journal of Computer and Information Sciences,</journal>
<volume>8</volume>
<issue>2</issue>
<pages>110</pages>
<contexts>
<context position="1911" citStr="Maryanski and Thomason, 1979" startWordPosition="276" endWordPosition="279">abilistic Context Free Grammars. The parsing algorithm does not allow the association of two items that have the empty string in one of their sides. This limitation restricts the search space and prevents the algorithm from exploring some valid parse trees. In this paper, we review Wu’s parsing algorithm for SITGs (referred to as the original algorithm) and propose some modifications to increase the search space in order to make it possible to find these valid parse trees. 2 SITG Parsing SITGs (Wu, 1997) can be viewed as a restricted subset of Stochastic Syntax-Directed Transduction Grammars (Maryanski and Thomason, 1979). Formally, a SITG in Chomsky Normal Form can be defined as a set of lexical rules that are noted as A → x/c, A → c/y, A → x/y; direct syntactic rules that are noted as A → [BC]; and inverse syntactic rules that are noted as A → hBCi, where A, B, C are non-terminal symbols, x, y are terminal symbols, c is the empty string, and each rule has a probability value p attached. The sum of the probabilities of the rules with the same non-terminal in the left side must be equal to 1. When a direct syntactic rule is used in parsing, both strings are parsed with the syntactic rule A → BC. When an invers</context>
</contexts>
<marker>Maryanski, Thomason, 1979</marker>
<rawString>F.J. Maryanski and M.T. Thomason. 1979. Properties of stochastic syntax-directed tranlation schemata. Journal of Computer and Information Sciences, 8(2):89– 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Sánchez</author>
<author>J M Benedí</author>
</authors>
<title>Stochastic inversion transduction grammars for obtaining word phrases for phrase-based statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of Workshop on Statistical Machine Translation. HLT-NAACL 06,</booktitle>
<pages>130--133</pages>
<contexts>
<context position="8175" citStr="Sánchez and Benedí, 2006" startWordPosition="1474" endWordPosition="1477"> experiment and in order to evaluate empirically the Wu’s parsing algorithm versus the modified algorithm, we parsed first 100K sentence of German-English Europarl corpus. The lexical rules in the Bracketing SITG used for parsing were obtained from a probabilistic dictionary by aligning with IBM3 model (NULL aligments were also included). In this experiment, the modified algorithm obtained a more probable parse tree for 6% of the sentences. If we added brackets to the sentences separately with monolingual parsers, we could use a parsing algorithm similar to the algorithm that is described in (Sánchez and Benedí, 2006). The monolingual brackets restricted the parse tree to those that were compatible with the brackets. In that case the modified algorithm obtained a more probable parse tree for 14% of the sentences. 4 Inside Probability The parsing algorithm described above computes the most likely parse tree for a given paired string X/Y . However, in some cases (Wu, 1995; Huang and Zhou, 2009), we need the inside probability (Q0�jXj�0�jY j(S)), i.e., the probability that the grammar assigns to the whole set of parse trees that yield X/Y . If the maximizations are replaced by sums, the algorithm can be used </context>
</contexts>
<marker>Sánchez, Benedí, 2006</marker>
<rawString>J.A. Sánchez and J.M. Benedí. 2006. Stochastic inversion transduction grammars for obtaining word phrases for phrase-based statistical machine translation. In Proc. of Workshop on Statistical Machine Translation. HLT-NAACL 06, pages 130–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Trainable coarse bilingual grammars for parallel text bracketing.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Annual Workshop on Very Large Corpora,</booktitle>
<pages>69--81</pages>
<contexts>
<context position="8534" citStr="Wu, 1995" startWordPosition="1536" endWordPosition="1537">gorithm obtained a more probable parse tree for 6% of the sentences. If we added brackets to the sentences separately with monolingual parsers, we could use a parsing algorithm similar to the algorithm that is described in (Sánchez and Benedí, 2006). The monolingual brackets restricted the parse tree to those that were compatible with the brackets. In that case the modified algorithm obtained a more probable parse tree for 14% of the sentences. 4 Inside Probability The parsing algorithm described above computes the most likely parse tree for a given paired string X/Y . However, in some cases (Wu, 1995; Huang and Zhou, 2009), we need the inside probability (Q0�jXj�0�jY j(S)), i.e., the probability that the grammar assigns to the whole set of parse trees that yield X/Y . If the maximizations are replaced by sums, the algorithm can be used to compute the inside probability. However, as stated above, the original algorithm cannot find the whole set of trees for a given paired string in some cases. These nonexplored trees have a probability greater than 0. As an example, we computed the amount of probability lost in the inside computation using the original algorithm with the grammar shown in F</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>D. Wu. 1995. Trainable coarse bilingual grammars for parallel text bracketing. In Proceedings of the Third Annual Workshop on Very Large Corpora, pages 69– 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>ComputationalLinguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="877" citStr="Wu, 1997" startWordPosition="115" endWordPosition="116">stic Inversion Transduction Grammars constitute a powerful formalism in Machine Translation for which an efficient Dynamic Programming parsing algorithm exists. In this work, we review this parsing algorithm and propose important modifications that enlarge the search space. These modifications allow the parsing algorithm to search for more and better solutions. 1 Introduction Syntax Machine Translation has received great attention in the last few years, especially for pairs of languages that are sufficiently non-monotonic. Several works have explored the use of syntax for Machine Translation (Wu, 1997; Chiang, 2007). In (Wu, 1997), Stochastic Inverse Transduction Grammars (SITGs) were introduced for describing structurally correlated pairs of languages. SITGs can be used to simultaneously analyze two strings from different languages and to correlate them. An efficient Dynamic Programming parsing algorithm for SITGs was presented in (Wu, 1997). This algorithm is similar to the CKY algorithm for Probabilistic Context Free Grammars. The parsing algorithm does not allow the association of two items that have the empty string in one of their sides. This limitation restricts the search space and</context>
<context position="2770" citStr="Wu, 1997" startWordPosition="450" endWordPosition="451">re non-terminal symbols, x, y are terminal symbols, c is the empty string, and each rule has a probability value p attached. The sum of the probabilities of the rules with the same non-terminal in the left side must be equal to 1. When a direct syntactic rule is used in parsing, both strings are parsed with the syntactic rule A → BC. When an inverse rule is used in parsing, one string is parsed with the syntactic rule A → BC, and the other string is parsed with the syntactic rule A → CB. An efficient Viterbi-like parsing algorithm that is based on a Dynamic Programming Scheme was proposed in (Wu, 1997). It allows us to obtain the most probable parse tree that simultaneously analyzes two strings, X = x1...x|X |and Y = y1...x|Y |, i.e. the bilingual string X/Y . It has a time complexity of O(|X|3|Y |3|R|), where |R |is the number of rules of the grammar. The parsing algorithm is based on the definition of: Sijkl(A) = Pr(A ⇒~ xi+1 ··· xj/yk+1 ··· yl) as the maximum probability of any parsing tree that simultaneously generates the substrings xi+1 · · · xj and yk+1 ··· yl from the non-terminal symbol A . In (Wu, 1997), the parsing algorithm was defined as follows: 653 Human Language Technologies</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. ComputationalLinguistics, 23(3):377–404.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>