<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.092851">
<title confidence="0.99898">
Improved Reordering for Shallow-n Grammar based Hierarchical
Phrase-based Translation
</title>
<author confidence="0.982071">
Baskaran Sankaran and Anoop Sarkar
</author>
<affiliation confidence="0.899446666666667">
School of Computing Science
Simon Fraser University
Burnaby BC. Canada
</affiliation>
<email confidence="0.992698">
{baskaran, anoop}@cs.sfu.ca
</email>
<sectionHeader confidence="0.993292" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999209047619048">
Shallow-n grammars (de Gispert et al., 2010)
were introduced to reduce over-generation in
the Hiero translation model (Chiang, 2005) re-
sulting in much faster decoding and restricting
reordering to a desired level for specific lan-
guage pairs. However, Shallow-n grammars
require parameters which cannot be directly
optimized using minimum error-rate tuning
by the decoder. This paper introduces some
novel improvements to the translation model
for Shallow-n grammars. We introduce two
rules: a BITG-style reordering glue rule and a
simpler monotonic concatenation rule. We use
separate features for the new rules in our log-
linear model allowing the decoder to directly
optimize the feature weights. We show this
formulation of Shallow-n hierarchical phrase-
based translation is comparable in translation
quality to full Hiero-style decoding (without
shallow rules) while at the same time being
considerably faster.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933675675675">
Hierarchical phrase-based translation (Chiang,
2005; Chiang, 2007) extends the highly lexicalized
models from phrase-based translation systems in
order to model lexicalized reordering and discon-
tiguous phrases. However, a major drawback in this
approach, when compared to phrase-based systems,
is the total number of rules that are learnt are several
orders of magnitude larger than standard phrase
tables, which leads to over-generation and search
errors and contribute to much longer decoding
times. Several approaches have been proposed to
address these issues: from filtering the extracted
synchronous grammar (Zollmann et al., 2008; He
et al., 2009; Iglesias et al., 2009) to alternative
Bayesian approaches for learning minimal gram-
mars (Blunsom et al., 2008; Blunsom et al., 2009;
Sankaran et al., 2011). The idea of Shallow-n gram-
mars (de Gispert et al., 2010) takes an orthogonal
direction for controlling the over-generation and
search space in Hiero decoder by restricting the
degree of nesting allowed for Hierarchical rules.
We propose an novel statistical model for
Shallow-n grammars which does not require addi-
tional non-terminals for monotonic re-ordering and
also eliminates hand-tuned parameters and instead
introduces an automatically tunable alternative. We
introduce a BITG-style (Saers et al., 2009) reorder-
ing glue rule (§ 3) and a monotonic X-glue rule
(§ 4). Our experiments show the resulting Shallow-n
decoding is comparable in translation quality to full
Hiero-style decoding while at the same time being
considerably faster.
All the experiments in this paper were done using
Kriya (Sankaran et al., 2012) hierarchical phrase-
based system which also supports decoding with
Shallow-n grammars. We extended Kriya to addi-
tionally support reordering glue rules as well.
</bodyText>
<sectionHeader confidence="0.98836" genericHeader="method">
2 Shallow-n Grammars
</sectionHeader>
<bodyText confidence="0.9299442">
Formally a Shallow-n grammar G is defined as a 5-
tuple: G = (N, T, R, Ry, S), such that T is a set of
finite terminals and N a set of finite non-terminals
{X°, ... , XN}. Ry refers to the glue rules that
rewrite the start symbol S:
</bodyText>
<equation confidence="0.996526">
S → &lt;X, X&gt; (1)
S → &lt;SX, SX&gt; (2)
</equation>
<bodyText confidence="0.9937285">
R is the set of finite production rules in G and has
two types, viz. hierarchical (3) and terminal (4). The
hierarchical rules at each level n are additionally
conditioned to have at least one Xn−1 non-terminal
</bodyText>
<page confidence="0.910642">
533
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 533–537,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.992595333333333">
in them. — represents the indices for aligning non-
terminals where co-indexed non-terminal pairs are
rewritten synchronously.
</bodyText>
<equation confidence="0.999329">
Xn --+ &lt;7, α, — &gt;, -/,α E {{Xn−1} U T+} (3)
X° --+ &lt;ry, α&gt;, -y, α E T+ (4)
</equation>
<bodyText confidence="0.99926297826087">
de Gispert et al. (2010) also proposed additional
non-terminals Mk to enable reordering over longer
spans by concatenating the hierarchical rules within
the span. It also uses additional parameters such
as monotonicity level (K1 and K2), maximum and
minimum rule spans allowed for the non-terminals
(�3.1 and 3.2 in de Gispert et al. (2010)). The mono-
tonicity level parameters determine the number of
non-terminals that are combined in monotonic or-
der at the N — 1 level and can be adapted to the
reordering requirements of specific language pairs.
The maximum and minimum rule spans further con-
trol the usage of hierarchical rule in a derivation by
stipulating the underlying span to be within a range
of values. Intuitively, this avoids hierarchical rules
being used for a source phrase that is either too short
or too long. While these parameters offer flexibility
for adapting the translation system to specific lan-
guage pairs, they have to be manually tuned which
is tedious and error-prone.
We propose an elegant and automatically tun-
able alternative for the Shallow-n grammars setting.
Specifically, we introduce a BITG-style reordering
glue rule (� 3) and a monotonic X-glue rule (� 4).
Our experiments show the resulting Shallow-n de-
coding to perform to the same level as full-Hiero de-
coding at the same time being faster.
In addition, our implementation of Shallow-n
grammar differs from (de Gispert et al., 2010) in
at least two other aspects. First, their formula-
tion constrains the X in the glue rules to be at the
top-level and specifically they define them to be:
S --+ &lt;SXN, SXN&gt; and S --+ &lt;XN, XN&gt;,
where XN is the non-terminal corresponding to the
top-most level. Interestingly, this resulted in poor
BLEU scores and we found the more generic glue
rules (as in (1) and (2)) to perform significantly bet-
ter, as we show later.
Secondly, they also employ pattern-based filter-
ing (Iglesias et al., 2009) in order to reducing redun-
dancies in the Hiero grammar by filtering it based on
certain rule patterns. However in our limited experi-
ments, we observed the filtered grammar to perform
worse than the full grammar, as also noted by (Zoll-
mann et al., 2008). Hence, we do not employ any
grammar filtering in our experiments.
</bodyText>
<sectionHeader confidence="0.954514" genericHeader="method">
3 Reordering Glue Rule
</sectionHeader>
<bodyText confidence="0.999819666666667">
In this paper, we propose an additional BITG-style
glue rule (called R-glue) as in (5) for reordering the
phrases along the left-branch of the derivation.
</bodyText>
<equation confidence="0.90381">
S --+ &lt;SX, XS&gt; (5)
</equation>
<bodyText confidence="0.9229">
In order to use this rule sparsely in the derivation,
we use a separate feature for this rule and apply a
penalty of 1. Similar to the case of regular glue
rules, we experimented with a variant of the reorder-
ing glue rule, where X is restricted to the top-level:
S --+ &lt;SXN, XNS&gt; and S --+ &lt;XN, XN&gt;.
</bodyText>
<subsectionHeader confidence="0.988031">
3.1 Language Model Integration
</subsectionHeader>
<bodyText confidence="0.999974">
The traditional phrase-based decoders using beam
search generate the target hypotheses in the left-to-
right order. In contrast, Hiero-style systems typ-
ically use CKY chart-parsing decoders which can
freely combine target hypotheses generated in inter-
mediate cells with hierarchical rules in the higher
cells. Thus the generation of the target hypotheses
are fragmented and out of order compared to the left
to right order preferred by n-gram language models.
This leads to challenges in the estimation of lan-
guage model scores for partial target hypothesis,
which is being addressed in different ways in the
existing Hiero-style systems. Some systems add a
sentence initial marker (&lt;s&gt;) to the beginning of
each path and some other systems have this implic-
itly in the derivation through the translation mod-
els. Thus the language model scores for the hypoth-
esis in the intermediate cell are approximated, with
the true language model score (taking into account
sentence boundaries) being computed in the last cell
that spans the entire source sentence.
We introduce a novel improvement in computing
the language model scores: for each of the target
hypothesis fragment, our approach finds the best po-
sition for the fragment in the final sentence and uses
the corresponding score. We compute three different
scores corresponding to the three positions where
the fragment can end up in the final sentence, viz.
</bodyText>
<page confidence="0.990985">
534
</page>
<bodyText confidence="0.999855368421053">
sentence initial, middle and final: and choose the
best score. As an example for fragment tf consist-
ing of a sequence of target tokens, we compute LM
scores for i) &lt;s&gt; tf, ii) tf and iii) tf &lt;/s&gt; and use
the best score for pruning alone1.
This improvement significantly reduces the
search errors while performing cube pruning (Chi-
ang, 2007) at the cost of additional language model
queries. While this approach works well for the
usual glue rules, it is particularly effective in the case
of reordering glue rules. For example, a partial can-
didate covering a non-final source span might trans-
late to the final position in the target sentence. If we
just compute the LM score for the target fragment
as is done normally, this might get pruned early on
before being reordered by the new glue rule. Our ap-
proach instead computes the three LM scores and it
would correctly use the last LM score which is likely
to be the best, for pruning.
</bodyText>
<sectionHeader confidence="0.998038" genericHeader="method">
4 Monotonic Concatenation Glue rule
</sectionHeader>
<bodyText confidence="0.999979636363636">
The reordering glue rule facilitates reordering at the
top-level. However, this is still not sufficient to allow
long-distance reordering as the shallow-decoding re-
stricts the depth of the derivation. Consider the Chi-
nese example in Table 1, in which translation of the
Chinese word corresponding to the English phrase
the delegates involves a long distance reordering to
the beginning of the sentence. Note that, three of the
four human references prefer this long distance re-
ordering, while the fourth one avoids the movement
by using a complex construction with relative clause
and a sentence initial prepositional phrase.
Such long distance reordering is very difficult in
conventional Hiero decoding and more so with the
Shallow-n grammars. While the R-glue rule per-
mit such long distance movements, it also requires
a long phrase generated by a series of rules to be
moved as a block. We address this issue, by adding
a monotonic concatenation (called X-glue) rule that
concatenates a series of hierarchical rules. In order
to control overgeneration, we apply this rule only at
the N — 1 level similar to de Gispert et al. (2010).
</bodyText>
<equation confidence="0.83855">
XN−1 ___� &lt;XN−1XN−1, XN−1XN−1&gt; (6)
</equation>
<bodyText confidence="0.936478846153846">
1This ensures the the LM score estimates are never underes-
timated for pruning. We retain the LM score for fragment (case
ii) for estimating the score for the full candidate sentence later.
However unlike their approach, we use this rule as
a feature in the log-linear model so that its weight
can be optimized in the tuning step. Also, our ap-
proach removes the need for additional parameters
K1 and K2 for controlling monotonicity, which was
being tuned manually in their work. For the Chinese
example above, shallow-1 decoding using R and X-
glue rules achieve the complex movement resulting
in a significantly better translation than full-Hiero
decoding as shown in the last two lines in Table 1.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="conclusions">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999423794117647">
We present results for Chinese-English translation
as it often requires heavy reordering. We use the
HK parallel text and GALE phase-1 corpus consist-
ing of —2.3M sentence pairs for training. For tuning
and testing, we use the MTC parts 1 and 3 (1928
sentences) and MTC part 4 (919 sentences) respec-
tively. We used the usual pre-processing pipeline
and an additional segmentation step for the Chinese
side of the bitext using the LDC segmenter2.
Our log-linear model uses the standard features
conditional (p(e|f) and p(f|e)) and lexical (pl(e|f)
and pl(f|e)) probabilities, phrase (pp) and word
(wp) penalties, language model and regular glue
penalty (my) apart from two additional features for
R—glue (ry) and X—glue (xy).
Table 2 shows the BLEU scores and decoding
time for the MTC test-set. We provide the IBM
BLEU (Papineni et al., 2002) scores for the Shallow-
n grammars for order: n = 1, 2,3 and compare it to
the full-Hiero baseline. Finally, we experiment with
two variants of the S glue rules, i) a restricted ver-
sion where the glue rules combine only X at level
N, (column ’Glue: XN’ in table), ii) more free vari-
ant where they are allowed to use any X freely (col-
umn ’Glue: X’ in table).
As it can be seen, the unrestricted glue rules vari-
ant (column ’Glue: X’) consistently outperforms
the glue rules restricted to the top-level non-terminal
XN, achieving a maximum BLEU score of 26.24,
which is about 1.4 BLEU points higher than the lat-
ter and is also marginally higher than full Hiero. The
decoding speeds for free-Glue and restricted-Glue
variants were mostly identical and so we only pro-
vide the decoding time for the latter. Shallow-2 and
</bodyText>
<footnote confidence="0.989907">
2We slightly modified the LDC segmenter, in order to cor-
rectly handle non-Chinese characters in ASCII and UTF8.
</footnote>
<page confidence="0.989457">
535
</page>
<table confidence="0.999382266666667">
Source 在阿根廷首都布宜诺斯艾利斯参加联合国全球气候大会的代表们继续进行工作。
Gloss in argentine capital beunos aires participate united nations global climate conference delegates continue
to work.
Ref 0 delegates attending the un conference on world climate continue their work in the argentine capital of
buenos aires.
Ref 1 the delegates to the un global climate conference held in Buenos aires, capital city of argentina, go on with
their work.
Ref 2 the delegates continue their works at the united nations global climate talks in buenos aires, capital of
argentina
Ref 3 in buenos aires, the capital of argentina, the representatives attending un global climate meeting continued
their work.
Full-Hiero: in the argentine capital of buenos aires to attend the un conference on global climate of representatives
Baseline continue to work.
Sh-1 Hiero: R- the representatives were in the argentine capital of beunos aires to attend the un conference on global climate
glue &amp; X-glue continues to work.
</table>
<tableCaption confidence="0.999037">
Table 1: An example for the level of reordering in Chinese-English translation
</tableCaption>
<table confidence="0.999945454545454">
Grammar Glue: XN Glue: X Time
Full Hiero 25.96 0.71
Shallow-1 23.54 24.04 0.24
+ R-Glue 23.41 24.15 0.25
+ X-Glue 23.75 24.74 0.72
Shallow-2 24.54 25.12 0.55
+ R-Glue 24.75 25.60 0.57
+ X-Glue 24.33 25.43 0.69
Shallow-3 24.88 25.89 0.62
+ R-Glue 24.77 26.24 0.63
+ X-Glue 24.75 25.83 0.69
</table>
<tableCaption confidence="0.994635">
Table 2: Results for Chinese-English. The decoding time
is in secs/word on the Test set for column ’Glue: X’.
Bold font indicate best BLEU for each shallow-order.
</tableCaption>
<bodyText confidence="0.998306125">
shallow-3 free glue variants achieve BLEU scores
comparable to full-Hiero and at the same time being
12 − 20% faster.
R-glue (ry) appears to contribute more than
the X-glue (xy) as can be seen in shallow-2 and
shallow-3 cases. Interestingly, xy is more helpful for
the shallow-1 case specifically when the glue rules
are restricted. As the glue rules are restricted, the
X-glue rules concatenates other lower-order rules
before being folded into the glue rules. Both ry and
xy improve the BLEU scores by 0.58 over the plain
shallow case for shallow orders 1 and 2 and performs
comparably for shallow-3 case. We have also con-
ducted experiments for Arabic-English (Table 3) and
we notice that X-glue is more effective and that R-
glue is helpful for higher shallow orders.
</bodyText>
<table confidence="0.999966636363636">
Grammar Glue: X Time
Full Hiero 37.54 0.67
Shallow-1 36.90 0.40
+ R-Glue 36.98 0.43
+ X-Glue 37.21 0.57
Shallow-2 36.97 0.57
+ R-Glue 36.80 0.58
+ X-Glue 37.36 0.61
Shallow-3 36.88 0.61
+ R-Glue 37.18 0.63
+ X-Glue 37.31 0.64
</table>
<tableCaption confidence="0.9879195">
Table 3: Results for Arabic-English. The decoding time
is in secs/word on the Test set.
</tableCaption>
<subsectionHeader confidence="0.990125">
5.1 Effect of our novel LM integration
</subsectionHeader>
<bodyText confidence="0.999972625">
Here we analyze the effect of our novel LM integra-
tion approach in terms of BLEU score and search er-
rors comparing it to the naive method used in typical
Hiero systems. In shallow setting, our method im-
proved the BLEU scores by 0.4 for both Ar-En and
Cn-En. In order to quantify the change in the search
errors, we compare the model scores of the (corre-
sponding) candidates in the N-best lists obtained by
the two methods and compute the % of high scor-
ing candidates in each. Our approach was clearly
superior with 94.6% and 77.3% of candidates hav-
ing better scores respectively for Cn-En and Ar-En.
In full decoding setting the margin of improvements
were reduced slightly- BLEU improved by 0.3 and
about 57−69% of target candidates had better model
scores for the two language pairs.
</bodyText>
<page confidence="0.997329">
536
</page>
<sectionHeader confidence="0.99015" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999848150943396">
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of Neural Information Processing Systems.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of Association of
Computational Linguistics, pages 782–790.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
Association of Computational Linguistics, pages 263–
270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite-
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Discarding
monotone composed rule for hierarchical phrase-based
statistical machine translation. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 25–29.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the 12th Conference of the European Chapter of the
ACL, pages 380–388.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of Association of Computational Lin-
guistics, pages 311–318.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proceedings of the 11th International Conference on
Parsing Technologies, pages 29–32. Association for
Computational Linguistics.
Baskaran Sankaran, Gholamreza Haffari, and Anoop
Sarkar. 2011. Bayesian extraction of minimal scfg
rules for hierarchical phrase-based translation. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation, pages 533–541.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya – an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, (97):83–98, April.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics, pages 1145–1152.
</reference>
<page confidence="0.997386">
537
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.552157">
<title confidence="0.7975245">Reordering for based Hierarchical Phrase-based Translation Baskaran Sankaran and Anoop School of Computing</title>
<author confidence="0.9185665">Simon Fraser Burnaby BC</author>
<abstract confidence="0.999302590909091">de Gispert et al., 2010) were introduced to reduce over-generation in the Hiero translation model (Chiang, 2005) resulting in much faster decoding and restricting reordering to a desired level for specific lanpairs. However, require parameters which cannot be directly optimized using minimum error-rate tuning by the decoder. This paper introduces some novel improvements to the translation model We introduce two a BITG-style reordering and a simpler monotonic concatenation rule. We use separate features for the new rules in our loglinear model allowing the decoder to directly optimize the feature weights. We show this of phrasebased translation is comparable in translation quality to full Hiero-style decoding (without shallow rules) while at the same time being considerably faster.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2008</date>
<booktitle>In Proceedings of Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1921" citStr="Blunsom et al., 2008" startWordPosition="270" endWordPosition="273">n order to model lexicalized reordering and discontiguous phrases. However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times. Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar (Zollmann et al., 2008; He et al., 2009; Iglesias et al., 2009) to alternative Bayesian approaches for learning minimal grammars (Blunsom et al., 2008; Blunsom et al., 2009; Sankaran et al., 2011). The idea of Shallow-n grammars (de Gispert et al., 2010) takes an orthogonal direction for controlling the over-generation and search space in Hiero decoder by restricting the degree of nesting allowed for Hierarchical rules. We propose an novel statistical model for Shallow-n grammars which does not require additional non-terminals for monotonic re-ordering and also eliminates hand-tuned parameters and instead introduces an automatically tunable alternative. We introduce a BITG-style (Saers et al., 2009) reordering glue rule (§ 3) and a monotoni</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. Bayesian synchronous grammar induction. In Proceedings of Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of Association of Computational Linguistics,</booktitle>
<pages>782--790</pages>
<contexts>
<context position="1943" citStr="Blunsom et al., 2009" startWordPosition="274" endWordPosition="277">alized reordering and discontiguous phrases. However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times. Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar (Zollmann et al., 2008; He et al., 2009; Iglesias et al., 2009) to alternative Bayesian approaches for learning minimal grammars (Blunsom et al., 2008; Blunsom et al., 2009; Sankaran et al., 2011). The idea of Shallow-n grammars (de Gispert et al., 2010) takes an orthogonal direction for controlling the over-generation and search space in Hiero decoder by restricting the degree of nesting allowed for Hierarchical rules. We propose an novel statistical model for Shallow-n grammars which does not require additional non-terminals for monotonic re-ordering and also eliminates hand-tuned parameters and instead introduces an automatically tunable alternative. We introduce a BITG-style (Saers et al., 2009) reordering glue rule (§ 3) and a monotonic X-glue rule (§ 4). O</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proceedings of Association of Computational Linguistics, pages 782–790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Association of Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1208" citStr="Chiang, 2005" startWordPosition="166" endWordPosition="167">der. This paper introduces some novel improvements to the translation model for Shallow-n grammars. We introduce two rules: a BITG-style reordering glue rule and a simpler monotonic concatenation rule. We use separate features for the new rules in our loglinear model allowing the decoder to directly optimize the feature weights. We show this formulation of Shallow-n hierarchical phrasebased translation is comparable in translation quality to full Hiero-style decoding (without shallow rules) while at the same time being considerably faster. 1 Introduction Hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007) extends the highly lexicalized models from phrase-based translation systems in order to model lexicalized reordering and discontiguous phrases. However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times. Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar (Zollmann et al., 2008; He et al., 20</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of Association of Computational Linguistics, pages 263– 270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<contexts>
<context position="1223" citStr="Chiang, 2007" startWordPosition="168" endWordPosition="169">r introduces some novel improvements to the translation model for Shallow-n grammars. We introduce two rules: a BITG-style reordering glue rule and a simpler monotonic concatenation rule. We use separate features for the new rules in our loglinear model allowing the decoder to directly optimize the feature weights. We show this formulation of Shallow-n hierarchical phrasebased translation is comparable in translation quality to full Hiero-style decoding (without shallow rules) while at the same time being considerably faster. 1 Introduction Hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007) extends the highly lexicalized models from phrase-based translation systems in order to model lexicalized reordering and discontiguous phrases. However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times. Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar (Zollmann et al., 2008; He et al., 2009; Iglesias et</context>
<context position="8393" citStr="Chiang, 2007" startWordPosition="1351" endWordPosition="1353">e target hypothesis fragment, our approach finds the best position for the fragment in the final sentence and uses the corresponding score. We compute three different scores corresponding to the three positions where the fragment can end up in the final sentence, viz. 534 sentence initial, middle and final: and choose the best score. As an example for fragment tf consisting of a sequence of target tokens, we compute LM scores for i) &lt;s&gt; tf, ii) tf and iii) tf &lt;/s&gt; and use the best score for pruning alone1. This improvement significantly reduces the search errors while performing cube pruning (Chiang, 2007) at the cost of additional language model queries. While this approach works well for the usual glue rules, it is particularly effective in the case of reordering glue rules. For example, a partial candidate covering a non-final source span might translate to the final position in the target sentence. If we just compute the LM score for the target fragment as is done normally, this might get pruned early on before being reordered by the new glue rule. Our approach instead computes the three LM scores and it would correctly use the last LM score which is likely to be the best, for pruning. 4 Mo</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Gonzalo Iglesias</author>
<author>Graeme Blackwood</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrase-based translation with weighted finitestate transducers and shallow-n grammars.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<marker>de Gispert, Iglesias, Blackwood, Banga, Byrne, 2010</marker>
<rawString>Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne. 2010. Hierarchical phrase-based translation with weighted finitestate transducers and shallow-n grammars. Computational Linguistics, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Yao Meng</author>
<author>Hao Yu</author>
</authors>
<title>Discarding monotone composed rule for hierarchical phrase-based statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd International Universal Communication Symposium,</booktitle>
<pages>25--29</pages>
<contexts>
<context position="1810" citStr="He et al., 2009" startWordPosition="253" endWordPosition="256">(Chiang, 2005; Chiang, 2007) extends the highly lexicalized models from phrase-based translation systems in order to model lexicalized reordering and discontiguous phrases. However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times. Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar (Zollmann et al., 2008; He et al., 2009; Iglesias et al., 2009) to alternative Bayesian approaches for learning minimal grammars (Blunsom et al., 2008; Blunsom et al., 2009; Sankaran et al., 2011). The idea of Shallow-n grammars (de Gispert et al., 2010) takes an orthogonal direction for controlling the over-generation and search space in Hiero decoder by restricting the degree of nesting allowed for Hierarchical rules. We propose an novel statistical model for Shallow-n grammars which does not require additional non-terminals for monotonic re-ordering and also eliminates hand-tuned parameters and instead introduces an automaticall</context>
</contexts>
<marker>He, Meng, Yu, 2009</marker>
<rawString>Zhongjun He, Yao Meng, and Hao Yu. 2009. Discarding monotone composed rule for hierarchical phrase-based statistical machine translation. In Proceedings of the 3rd International Universal Communication Symposium, pages 25–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Rule filtering by pattern for efficient hierarchical translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL,</booktitle>
<pages>380--388</pages>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule filtering by pattern for efficient hierarchical translation. In Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of Association of Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="11749" citStr="Papineni et al., 2002" startWordPosition="1909" endWordPosition="1912">se the MTC parts 1 and 3 (1928 sentences) and MTC part 4 (919 sentences) respectively. We used the usual pre-processing pipeline and an additional segmentation step for the Chinese side of the bitext using the LDC segmenter2. Our log-linear model uses the standard features conditional (p(e|f) and p(f|e)) and lexical (pl(e|f) and pl(f|e)) probabilities, phrase (pp) and word (wp) penalties, language model and regular glue penalty (my) apart from two additional features for R—glue (ry) and X—glue (xy). Table 2 shows the BLEU scores and decoding time for the MTC test-set. We provide the IBM BLEU (Papineni et al., 2002) scores for the Shallown grammars for order: n = 1, 2,3 and compare it to the full-Hiero baseline. Finally, we experiment with two variants of the S glue rules, i) a restricted version where the glue rules combine only X at level N, (column ’Glue: XN’ in table), ii) more free variant where they are allowed to use any X freely (column ’Glue: X’ in table). As it can be seen, the unrestricted glue rules variant (column ’Glue: X’) consistently outperforms the glue rules restricted to the top-level non-terminal XN, achieving a maximum BLEU score of 26.24, which is about 1.4 BLEU points higher than </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of Association of Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>29--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2479" citStr="Saers et al., 2009" startWordPosition="352" endWordPosition="355">pproaches for learning minimal grammars (Blunsom et al., 2008; Blunsom et al., 2009; Sankaran et al., 2011). The idea of Shallow-n grammars (de Gispert et al., 2010) takes an orthogonal direction for controlling the over-generation and search space in Hiero decoder by restricting the degree of nesting allowed for Hierarchical rules. We propose an novel statistical model for Shallow-n grammars which does not require additional non-terminals for monotonic re-ordering and also eliminates hand-tuned parameters and instead introduces an automatically tunable alternative. We introduce a BITG-style (Saers et al., 2009) reordering glue rule (§ 3) and a monotonic X-glue rule (§ 4). Our experiments show the resulting Shallow-n decoding is comparable in translation quality to full Hiero-style decoding while at the same time being considerably faster. All the experiments in this paper were done using Kriya (Sankaran et al., 2012) hierarchical phrasebased system which also supports decoding with Shallow-n grammars. We extended Kriya to additionally support reordering glue rules as well. 2 Shallow-n Grammars Formally a Shallow-n grammar G is defined as a 5- tuple: G = (N, T, R, Ry, S), such that T is a set of fini</context>
</contexts>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. 2009. Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm. In Proceedings of the 11th International Conference on Parsing Technologies, pages 29–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baskaran Sankaran</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Bayesian extraction of minimal scfg rules for hierarchical phrase-based translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>533--541</pages>
<contexts>
<context position="1967" citStr="Sankaran et al., 2011" startWordPosition="278" endWordPosition="281">discontiguous phrases. However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times. Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar (Zollmann et al., 2008; He et al., 2009; Iglesias et al., 2009) to alternative Bayesian approaches for learning minimal grammars (Blunsom et al., 2008; Blunsom et al., 2009; Sankaran et al., 2011). The idea of Shallow-n grammars (de Gispert et al., 2010) takes an orthogonal direction for controlling the over-generation and search space in Hiero decoder by restricting the degree of nesting allowed for Hierarchical rules. We propose an novel statistical model for Shallow-n grammars which does not require additional non-terminals for monotonic re-ordering and also eliminates hand-tuned parameters and instead introduces an automatically tunable alternative. We introduce a BITG-style (Saers et al., 2009) reordering glue rule (§ 3) and a monotonic X-glue rule (§ 4). Our experiments show the </context>
</contexts>
<marker>Sankaran, Haffari, Sarkar, 2011</marker>
<rawString>Baskaran Sankaran, Gholamreza Haffari, and Anoop Sarkar. 2011. Bayesian extraction of minimal scfg rules for hierarchical phrase-based translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 533–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baskaran Sankaran</author>
<author>Majid Razmara</author>
<author>Anoop Sarkar</author>
</authors>
<title>Kriya – an end-to-end hierarchical phrase-based mt system. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2012</date>
<contexts>
<context position="2791" citStr="Sankaran et al., 2012" startWordPosition="403" endWordPosition="406">d for Hierarchical rules. We propose an novel statistical model for Shallow-n grammars which does not require additional non-terminals for monotonic re-ordering and also eliminates hand-tuned parameters and instead introduces an automatically tunable alternative. We introduce a BITG-style (Saers et al., 2009) reordering glue rule (§ 3) and a monotonic X-glue rule (§ 4). Our experiments show the resulting Shallow-n decoding is comparable in translation quality to full Hiero-style decoding while at the same time being considerably faster. All the experiments in this paper were done using Kriya (Sankaran et al., 2012) hierarchical phrasebased system which also supports decoding with Shallow-n grammars. We extended Kriya to additionally support reordering glue rules as well. 2 Shallow-n Grammars Formally a Shallow-n grammar G is defined as a 5- tuple: G = (N, T, R, Ry, S), such that T is a set of finite terminals and N a set of finite non-terminals {X°, ... , XN}. Ry refers to the glue rules that rewrite the start symbol S: S → &lt;X, X&gt; (1) S → &lt;SX, SX&gt; (2) R is the set of finite production rules in G and has two types, viz. hierarchical (3) and terminal (4). The hierarchical rules at each level n are additio</context>
</contexts>
<marker>Sankaran, Razmara, Sarkar, 2012</marker>
<rawString>Baskaran Sankaran, Majid Razmara, and Anoop Sarkar. 2012. Kriya – an end-to-end hierarchical phrase-based mt system. The Prague Bulletin of Mathematical Linguistics, (97):83–98, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Franz Och</author>
<author>Jay Ponte</author>
</authors>
<title>A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical mt.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>1145--1152</pages>
<contexts>
<context position="1793" citStr="Zollmann et al., 2008" startWordPosition="249" endWordPosition="252">rase-based translation (Chiang, 2005; Chiang, 2007) extends the highly lexicalized models from phrase-based translation systems in order to model lexicalized reordering and discontiguous phrases. However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times. Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar (Zollmann et al., 2008; He et al., 2009; Iglesias et al., 2009) to alternative Bayesian approaches for learning minimal grammars (Blunsom et al., 2008; Blunsom et al., 2009; Sankaran et al., 2011). The idea of Shallow-n grammars (de Gispert et al., 2010) takes an orthogonal direction for controlling the over-generation and search space in Hiero decoder by restricting the degree of nesting allowed for Hierarchical rules. We propose an novel statistical model for Shallow-n grammars which does not require additional non-terminals for monotonic re-ordering and also eliminates hand-tuned parameters and instead introduce</context>
<context position="6046" citStr="Zollmann et al., 2008" startWordPosition="957" endWordPosition="961">ically they define them to be: S --+ &lt;SXN, SXN&gt; and S --+ &lt;XN, XN&gt;, where XN is the non-terminal corresponding to the top-most level. Interestingly, this resulted in poor BLEU scores and we found the more generic glue rules (as in (1) and (2)) to perform significantly better, as we show later. Secondly, they also employ pattern-based filtering (Iglesias et al., 2009) in order to reducing redundancies in the Hiero grammar by filtering it based on certain rule patterns. However in our limited experiments, we observed the filtered grammar to perform worse than the full grammar, as also noted by (Zollmann et al., 2008). Hence, we do not employ any grammar filtering in our experiments. 3 Reordering Glue Rule In this paper, we propose an additional BITG-style glue rule (called R-glue) as in (5) for reordering the phrases along the left-branch of the derivation. S --+ &lt;SX, XS&gt; (5) In order to use this rule sparsely in the derivation, we use a separate feature for this rule and apply a penalty of 1. Similar to the case of regular glue rules, we experimented with a variant of the reordering glue rule, where X is restricted to the top-level: S --+ &lt;SXN, XNS&gt; and S --+ &lt;XN, XN&gt;. 3.1 Language Model Integration The </context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Franz Och, and Jay Ponte. 2008. A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical mt. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 1145–1152.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>