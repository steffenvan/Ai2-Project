<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<note confidence="0.938865">
CompuTerm 2004 - 3rd International Workshop on Computational Terminology 47
</note>
<title confidence="0.993838">
Automatically Inducing Ontologies from Corpora
</title>
<author confidence="0.62826">
Inderjeet Mani
</author>
<affiliation confidence="0.5052705">
Department of Linguistics
Georgetown University, ICC 452
</affiliation>
<address confidence="0.510793">
37th and O Sts, NW
Washington, DC 20057, USA
</address>
<email confidence="0.988878">
im5@georgetown.edu
</email>
<author confidence="0.760643">
Ken Samuel, Kris Concepcion and
David Vogel
</author>
<affiliation confidence="0.363587">
The MITRE Corporation
</affiliation>
<address confidence="0.507018">
7515 Colshire Drive
McLean, VA 22102, USA
</address>
<email confidence="0.964376">
{samuel, kjc9, dvogel}@mitre.org
</email>
<sectionHeader confidence="0.993666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960625">
The emergence of vast quantities of on-line
information has raised the importance of methods
for automatic cataloguing of information in a
variety of domains, including electronic commerce
and bioinformatics. Ontologies can play a critical
role in such cataloguing. In this paper, we describe
a system that automatically induces an ontology
from any large on-line text collection in a specific
domain. The ontology that is induced consists of
domain concepts, related by kind-of and part-of
links. To achieve domain-independence, we use a
combination of relatively shallow methods along
with any available repositories of applicable
background knowledge. We describe our
evaluation experiences using these methods, and
provide examples of induced structures.
</bodyText>
<sectionHeader confidence="0.998693" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997952631579">
The emergence of vast quantities of on-line
information has raised the importance of methods
for automatic cataloguing of information in a
variety of domains, including electronic commerce
and bioinformatics. Ontologies1 can play a critical
role in such cataloguing. In bioinformatics, for
example, there is growing recognition that
common ontologies, e.g., the Gene Ontology2, are
critical to interoperation and integration of
biological data, including both structured data as
found in protein databases, as well as unstructured
data, as found in on-line biomedical literature.
Constructing an ontology is an extremely
laborious effort. Even with some reuse of “core”
knowledge from an Upper Model (Cohen et al.
1999), the task of creating an ontology for a
particular domain and task has a high cost,
incurred for each new domain. Tools that could
automate, or semi-automate, the construction of
</bodyText>
<footnote confidence="0.991201333333333">
1 This research was supported by the National Science
Foundation (ITR-0205470).
2 www.geneontology.org
</footnote>
<bodyText confidence="0.999935651162791">
ontologies for different domains could
dramatically reduce the knowledge creation cost.
One approach to developing such tools is to rely
on information implicit in collections of on-line
text in a particular domain. If it were possible to
automatically extract terms and their semantic
relations from the text corpus, the ontology
developer could build on that knowledge, revising
it, as needed, etc. This would be more cost-
effective than having a human develop the
ontology from scratch.
Our approach is inspired by research on topic-
focused multi-document summarization of large
text collections, where there is a need to
characterize the collection content succinctly in a
hierarchy of topic terms and their relationships.
Current approaches to multi-document
summarization combine linguistic analysis, corpus
statistics, and the use of background semantic
knowledge from generic thesauri such as WordNet
to infer semantic information about a person. In
extending such approaches to ontology induction,
the hypothesis is that similar hybrid approaches
can be used to identify technical terms in a
domain-specific corpus and infer semantic
relationships among them.
In this paper, we describe a system that
automatically induces an ontology from any large
on-line text collection in a specific domain, to
support cataloguing in information access and data
integration tasks. The induced ontology consists of
domain concepts related by kind-of and part-of
links, but does not include more specialized
relations or axioms. The structure of the ontology
is a directed acyclic graph (DAG). To achieve
domain-independence, we use a combination of
relatively shallow methods along with existing
repositories of applicable background knowledge.
These are described in Section 2. In Section 3, we
also introduce a new metric Relation Precision for
evaluating induced ontologies in comparison with
reference ontologies. We have applied our system
to produce ontologies in numerous domains:
</bodyText>
<note confidence="0.449083">
48 CompuTerm 2004 - 3rd International Workshop on Computational Terminology
</note>
<figureCaption confidence="0.997982">
Figure 1: System Architecture
</figureCaption>
<table confidence="0.999468666666667">
IRS 285 0 285
Publication 17
k1 n1
Reuters 9 19,024 19,043
Corpus k2 n2
Total 294 19,024 19,328
</table>
<tableCaption confidence="0.999675">
Table 1: Distribution of ‘income tax’ in domain and background corpora
</tableCaption>
<bodyText confidence="0.999651782608696">
(i) newswire from the TREC collection (ii)
taxation information from the IRS (Publication 17,
from (IRS 2001)), (iii) epidemiological newsgroup
messages from the Program for Monitoring
Emerging Diseases (PROMED) from the
Federation of American Scientists3, (iv) the text of
a book by the first author called Automatic
Summarization, and (v) MEDLINE biomedical
abstracts retrieved from the National Library of
Medicine’s PubMed system4. In the latter domain,
we have begun building a large ontology using the
ontology induction methods along with post-
editing by domain experts in molecular biology at
Georgetown University 5 . This ontology, called
PRONTO, involves hundreds of thousands of
protein names found in MEDLINE abstracts and in
UNIPROT, the world’s largest protein database6. It
is therefore infeasible to construct PRONTO by
hand from scratch. PRONTO is also much larger
than other ontologies in the biology area; for
example, the Gene Ontology is rather high-level,
and contains (as of March 2004) only about 17,000
terms.
</bodyText>
<footnote confidence="0.9941655">
3 www.fas.org/promed/
4www4.ncbi.nlm.nih.gov/PubMed/
5 complingone.georgetown.edu/~prot/
6pir.georgetown.edu
</footnote>
<sectionHeader confidence="0.957367" genericHeader="introduction">
2 Approach
</sectionHeader>
<subsectionHeader confidence="0.988276">
2.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.999969904761905">
An overall architecture for domain-independent
ontology induction is shown in Figure 1. The
documents are preprocessed to separate out
headers. Next, terms are extracted using finite-state
syntactic parsing and scored to discover domain-
relevant terms. The subsequent processing infers
semantic relations between pairs of terms using the
‘weak’ knowledge sources run in the order
described below. Evidence from multiple
knowledge sources is then combined to infer the
resulting relations. The resulting ontologies are
written out in a standard XML-based format (e.g.,
XOL, RDF, OWL), for use in various information
access applications.
While the ontology induction procedure does not
involve human labor, except for writing the
preprocessing and term tokenization program for
specialized technical domains, the human may edit
the resulting ontology for use in a given
application. An ontology editor has been
developed, discussed briefly in Section 3.1.
</bodyText>
<subsectionHeader confidence="0.998584">
2.2 Term Discovery
</subsectionHeader>
<bodyText confidence="0.991751509803921">
The system takes a collection of documents in a
subject area, and identifies terms characteristic of
the domain. In a given domain such as
CompuTerm 2004 - 3rd International Workshop on Computational Terminology 49
bioinformatics, specialized term tokenization (into
single- and multi-word terms) is required. The
protein names can be long, e.g.,
“steroid/thyroid/retinoic nuclear hormone receptor
homolog nhr-35”, and involve specialized patterns.
In constructing PRONTO, we have used a protein
name tagger based on an ensemble of statistical
classifiers to tag protein names in collections of
MEDLINE abstracts (Anon 2004). Thus, in such a
domain, a specialized tagger replaces the
components in the dotted box in Figure 1.
In other domains, we adopt a generic term-
discovery approach. Here the text is tagged for
part-of-speech, and single- and multi-word terms
consisting of minimal NPs are extracted using
finite-state parsing with CASS (Abney 1996). All
punctuation except for hyphens are removed from
the terms, which are then lower-cased. Each word
in each term is stemmed, with statistics (see below)
being gathered for each stemmed term. Multi-word
terms are clustered so that open, closed and
hyphenated compounds are treated as equivalent,
with the most frequent term in the collection being
used as the cluster representative.
The terms are scored for domain-relevance based
on the assumption that if a term occurs
significantly more in a domain corpus than in a
more diffuse background corpus, then the term is
clearly domain relevant.
As an illustration, in Table 1 we compare the
number of documents containing the term ‘income
tax’ (or ‘income taxes’) in a long (2.18 Mb) IRS
publication, Publication 17, from an IRS web site
(IRS 2001) compared to a larger (27.63 Mb subset
of the) Reuters 21578 news corpus7. One would
expect that ‘income tax’ is much more a
characteristic of the IRS publication, and this is
borne out by the document frequencies in the table.
We use the log likelihood ratio (LLR) (Dunning
1993) given by
-2log2(Ho(p;k1,n1,k2,n2)/Ha(p1,p2;n1,k1,n2,k2))
LLR measures the extent to which a
hypothesized model of the distribution of cell
counts, Ha, differs from the null hypothesis, Ho
(namely, that the percentage of documents
containing this term is the same in both corpora).
We used a binomial model for Ho and Ha8.
</bodyText>
<subsectionHeader confidence="0.997069">
2.3 Relationship Discovery
</subsectionHeader>
<bodyText confidence="0.999028">
The main innovation in our approach is to fuse
together information from multiple knowledge
</bodyText>
<footnote confidence="0.785072666666667">
7 In Publication 17, each “chapter” is a document.
8From Table 1, p=294/19238=.015, p1=285/285=1.0,
p2=9/19043=4.72, k1=285, n1=285, k2=9, n2=19043.
</footnote>
<bodyText confidence="0.999589">
sources as evidence for particular semantic
relationships between terms. To infer semantic
relations such as kind-of and part-of, the system
uses a bottom-up data-driven approach using a
combination of evidence from shallow methods.
</bodyText>
<subsectionHeader confidence="0.94161">
2.3.1 Subphrase Relations
</subsectionHeader>
<bodyText confidence="0.999945">
These are based on the presence of common
syntactic heads, and allow us to infer, for example,
that ‘p68 protein’ is a kind-of ‘protein’. Likewise,
in the TREC domain, subphrase analysis tells us
that ‘electric car’ is a kind of ‘car’, and in the IRS
domain, that ‘federal income tax’ is a kind of
‘income tax’.
</bodyText>
<subsectionHeader confidence="0.874587">
2.3.2 Existing Ontology Relations
</subsectionHeader>
<bodyText confidence="0.972085875">
These are obtained from a thesaurus. For
example, the Gene Ontology can be used to infer
that ‘ATP-dependent RNA helicase’ is a kind of
‘RNA-helicase’. Likewise, in the TREC domain,
using WordNet tells us that ‘tailpipe’ is part of
‘automobile’, and in the IRS domain, that ‘spouse’
is a kind of ‘person’. Synonyms are also merged
together at this stage.
</bodyText>
<subsectionHeader confidence="0.73388">
2.3.3 Contextual Subsumption Relations
</subsectionHeader>
<bodyText confidence="0.9775924375">
We also infer hierarchical relations between
terms, by top-down clustering using a context-
based subsumption (CBS) algorithm. The
algorithm uses a probabilistic measure of set
covering to find subsumption relations. For each
term in the corpus, we note the set of contexts in
which the term appears. Term1 is said to subsume
term2 when the conditional probability of term1
appearing in a context given the presence of term2,
i.e., P(term1|term2), is greater than some threshold.
CBS is based on the algorithm of (Lawrie et al.
2001), which used a greedy approximation of the
Domination Set Problem for graphs to discover
subsumption relations among terms. Unlike their
work, we did not seek to minimize the set of
covering terms; therefore, a subsumed term may
have multiple parents. The conditional probability
threshold (0.8) we use to determine subsumption is
much higher than in their approach. We also
restrict the height of the hierarchies we build to
three tiers. Tightening these latter two constraints
appears to notably improve the quality of our
subsumption relations.
The largest corpus against which CBS has run is
the ProMed corpus where, considering each
paragraph a distinct context, there were 117,690
contexts in the 11,198 documents. Here is an
example from ProMed of a transitive relation that
spans three tiers: ‘mosquito’ is a hypernym of
‘mosquito pool’, and ‘mosquito’ is also a
hypernym of ‘standing water’.
50 CompuTerm 2004 - 3rd International Workshop on Computational Terminology
</bodyText>
<subsectionHeader confidence="0.476845">
2.3.4 Explicit Patterns Relations
</subsectionHeader>
<bodyText confidence="0.99992275">
This knowledge source infers specific relations
between terms based on characteristic cue-phrases
which relate them. For example, the cue-phrase
“such as” (Hearst 1992) (Caraballo 1999) suggest a
kind-of relation, e.g., ‘a ligand such as
triethylphosphine’ tells us that ‘triethylphosphene’
is a kind of ‘ligand’. Likewise, in the TREC
domain, ‘air toxics such as benzene’ can suggest
that ‘benzene’ is a kind of ‘air toxic’. However,
since such cue-phrase patterns tend to be sparse in
occurrence, we do not use them in the evaluations
described below.
</bodyText>
<subsectionHeader confidence="0.597092">
2.3.5 Domain-Specific Knowledge Sources
</subsectionHeader>
<bodyText confidence="0.999932555555555">
Although our approach is domain-independent, it
is possible to factor in domain knowledge sources
for a given domain. For example, in biology, ‘ase’
is usually a suffix indicating an enzyme.
Postmodifying PPs (found using a CASS grammar)
can also be useful in some domains, as shown in
‘tax on investment income of child’ in Figure 2.
We have so far, however, not investigated other
domain-specific knowledge sources.
</bodyText>
<subsectionHeader confidence="0.978775">
2.4 Evidence Combination
</subsectionHeader>
<bodyText confidence="0.999990941176471">
The main point about these and other knowledge
sources is that each may provide only partial
information. Combining these knowledge sources
together, we expect, will lead to superior
performance compared to just any one of them.
Not only do inferences from different knowledge
sources support each other, but they are also
combined to produce new inferences by transitivity
relations. For example, since phrase analysis tells
us that ‘pyridine metabolism’ is a kind-of
‘metabolism’, and Gene Ontology tells us that
‘metabolism’ is a kind-of ‘biological process’, it
follows that ‘pyridine metabolism’ is a kind-of
‘biological process’. The evidence combination, in
addition to computing transitive closure of these
relations, also detects inconsistencies, querying the
user to resolve them when detected.
</bodyText>
<sectionHeader confidence="0.999658" genericHeader="background">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.871133">
3.1 Informal Assessment
</subsectionHeader>
<bodyText confidence="0.999805928571429">
Subphrase Relations is a relatively high-
precision knowledge source compared to the
others, producing many linked chains. Its
performance can be improved by flagging and
excluding proper names and idioms from its input
(e..g, so that ‘palm pilot’ doesn’t show up as a
kind-of ‘pilot’). However, a chain of such relations
can be interrupted by terms that aren’t lexically
similar, but that are nevertheless in a kind-of
relation. Some of these gaps are filled by
transitivity relations involving other knowledge
sources, especially Existing Ontologies, which is
especially useful in filling gaps in some of the
upper levels of the ontology. While Contextual
Subsumption is good at discovering associations
between ‘leaves’ in the DAG and other concepts,
the method cannot reliably infer the label of the
relation. For example, in the IRS domain, we
obtain ‘divorce’ as more general than ‘decree of
divorce’ and ‘separate maintenance’, but we don’t
know the nature of the relations. Contextual
Subsumption-inferred links are directed edges with
label ‘unknown’.
Overall, the ontologies produced are noisy and
require human correction, and the methods can
produce many fragments that need to be linked by
hand. While the system can detect cycles that need
resolution by the human, these rarely arise
</bodyText>
<figureCaption confidence="0.948443">
Figure 2: An IRS Ontology viewed in the Ontology Editor
</figureCaption>
<table confidence="0.99928725">
CompuTerm 2004 - 3rd International Workshop on Computational Terminology 51
Term Target Back- LLR IG NI DF TF TF *
DF ground IDF
DF
electric 80 61 99.9 99.9 81.3 99.9 99.9 27.8
car 77 56 99.6 99.3 81.5 99.8 99.9 79.4
battery 54 16 99.0 98.2 86.9 98.7 99.9 94.9
emission 15 0 96.5 96.8 99.2 79.1 96.6 64.8
year 58 505 67.9 67.6 25.0 99.2 99.7 65.7
informal 10 29 66.2 66.3 0.2 48.6 99.7 99.2
record 8 138 15.2 15.7 4.4 50.2 99.9 99.9
osha 1 0 0.0 0.0 0.0 0.0 99.9 0.0
</table>
<tableCaption confidence="0.999101">
Table 2: Comparing Topic 230 Term Percentile Rankings
</tableCaption>
<figureCaption confidence="0.688391">
For a flavor of the kind of results we get, see
Figure 2, which displays an ontology induced
</figureCaption>
<bodyText confidence="0.96919725">
without any human intervention from IRS
Publication 17. Here the DAG is displayed as a
tree. The immediate children of ‘person’, a node
high in the ontology, is shown in the left part of the
window. Selecting ‘child’ brings up its kinds as
well as some other children linked by “unknown”
label via Contextual Subsumption, e.g., ‘full-time
student’. A list of orphaned terms that aren’t
related to any others are shown on the far right.
The terms with checkboxes are those that occur in
the corpus; the others are those that are found
exclusively by Existing Ontology Relations.
Checking a term allows it to be inspected in its
occurrence context in the corpus. The editor comes
with a variety of tools to help integrate ontology
fragments.
</bodyText>
<subsectionHeader confidence="0.953528">
3.2 Human Evaluation
3.2.1 Term Scoring
</subsectionHeader>
<bodyText confidence="0.999973466666667">
To evaluate term scoring, we used a corpus of
news articles about automobiles that consisted of
85 documents relevant to the TREC Topic 230
query: “Is the automobile industry making an
honest effort to develop and produce an electric-
powered automobile?” In Table 2, we provide
some examples of how the LLR term scoring
statistic performed with respect to five others on
selected unigrams in the Topic 230 domain: term
frequency, document frequency, term frequency
times inverse document frequency (TF*IDF),
pointwise mutual information (MI), and
information gain (IG). Terms in bold are ones we
judged important in the Topic 230 domain, the
others are deemed unimportant. The numbers are
percentile rankings. LLR and IG do equally well,
outperforming the others.
We carried out other comparisons for two other
domains. In the income-tax domain, a hand-built
term list from the IRS contained 82 terms which
occurred in IRS Publication 17, of which the
system discovered 77 (94% recall). In the ProMed
domain, a pre-existing hand-built taxonomy
produced by a bioterrorism analyst had 1048 terms
which occurred in the ProMed message corpus, of
which 607 were discovered by the system (58%
recall). However, the hand-built taxonomy, which
was built without consulting a corpus, wasn’t a
full-fledged ontology, for example, there was no
label for the parent-child relation.
</bodyText>
<subsectionHeader confidence="0.619557">
3.2.2 Term Relationships
</subsectionHeader>
<bodyText confidence="0.999988125">
We also carried out an evaluation experiment to
determine if the relations being discovered by the
machine were in keeping with human judgments.
We focused here on an evaluation of pairs of
knowledge sources. Our experiment examined the
case where the system discovered a kind-of
relation. Here each subject was first asked to read
four newspaper articles from the TREC topic-230
sub-collection. The articles were then kept
accessible to the subject in a browser window for
the subject to consult if needed in answering
subsequent questions. The subject was asked to
judge, based on the documents read, whether term
X was a kind of term Y, term Y was a kind of term
X, or neither; e.g., “Is acid a kind of pollutant, or is
pollutant a kind of acid, or neither?”. The subject
had one of three mutually exclusive choices; the
first two choices were presented in randomized
order.
The subjects were 16 native speakers of English
unconnected with the project. Each subject was
given ten questions to answer in each of the
experiments. For each set of ten questions, five
were chosen at random from pairs of terms related
</bodyText>
<note confidence="0.386728">
52 CompuTerm 2004 - 3rd International Workshop on Computational Terminology
</note>
<bodyText confidence="0.998633">
by (immediate) kind-of relations. The remaining
five questions were chosen at random from pairs of
terms between which the system found no relation
whatsoever.
</bodyText>
<table confidence="0.996952833333333">
Human
System kind- not kind-
of(A, B) of(A,B)
kind-of(A, B) 56 18
not kind- 6 74
of(A,B)
</table>
<tableCaption confidence="0.999905">
Table 3: Is X a kind-of Y?
</tableCaption>
<bodyText confidence="0.999986363636364">
We first discuss inter-subject agreement. Three
subjects given the same relation to judge agreed
75% of the time, leading to a Kappa score of 0.72,
indicating a good level of agreement. This means
that subjects were able to reliably make judgments
as to whether A is a kind of B in some document.
The results for the 16 subjects are shown in
Table 3. When the system is compared to the
human as ground truth, this gives a Precision of
.90, a Recall of .75, and an F-measure of .82. This
performance is also significantly better than
random assignment: with chi-square=74.29, with p
&lt; 0.0019. The substantial effect sizes of the chi-
square indicates a very solid result. There were 62
decisions involving Subphrase Relations (with 44
True Positives and 18 False Negatives), and 10
decisions involving WordNet (with 12 True
Positives). This shows that there is solid agreement
between the human subjects and the system on the
kind-of relations. However, these 154 decisions
involved only four newspaper articles, so clearly
more data would be helpful.
</bodyText>
<subsectionHeader confidence="0.999617">
3.3 Automatic Evaluation
</subsectionHeader>
<bodyText confidence="0.999466705882353">
While evaluation by humans is valuable, it is
expensive to carry out, and this expense must be
incurred each time one wants to do an evaluation.
Automatic comparison of a machine-generated
ontology against reference ontologies constructed
by humans, e.g., (Zhang et al. 1996) (Sekine et al.
1999) (Daude et al. 2001), is therefore desirable,
provided suitable reference ontologies are
available. In this evaluation, the human-generated
taxonomy for ProMed described in Section 3.2.1
was used as a reference ontology, with its
unlabeled parent-child relation treated as a kind-of
link. However, the human ‘ontology’ was created
without looking at a corpus, and was developed for
use with a different set of goals in mind. Although
this involves comparing ‘apples’ and ‘oranges’, a
comparison is nevertheless illustrative, and can in
</bodyText>
<footnote confidence="0.7272844">
9 The chi-square for Subphrase Relations is 61.68,
and the chi-square for WordNet is 56.73, with p &lt; 0.001
in all cases.
addition be useful when comparing mutiple
ontologies created under similar conditions.
</footnote>
<figureCaption confidence="0.9851985">
Figure 3: Automatically Induced Fragment
from ProMed
</figureCaption>
<bodyText confidence="0.995021688888889">
To set aside the problem of differences in
terminology involved in the comparison, we
decided to restrict our attention to the set of terms
TH (of cardinality 3025) in the human ontology
(H), and have our system induce relations between
them using the ProMed corpus. Relations were
induced automatically in the machine ontology (M)
for just 761 of those terms, yielding a set TH1. The
structure of TH1 is shown in a fragment in Figure
3. Here A is a kind-of B if it is printed under B
without a label; A is a part-of B if it is printed
under B with a “p” label.
We then automatically computed, for each pair
of terms t1 and t2 in TH1 that were linked distance 1
apart in M, the distance between those terms in H.
Likewise, we also computed, for each pair of terms
t1 and t2 in TH1 distance 1 apart in H, the distance
between those terms in M.
The results of this comparison are as follows.
The number of relations where the two ontologies
agree exactly is 63 (i.e., the terms are distance 1
apart in both ontologies). Since, given a set of
terms, there are many different ways to construct
an ontology, this is encouraging.
The number of relations that our system found
which were ‘missed’, i.e., more than distance 1
away, in H is 1203. Given the previous experiment
where the human subjects agreed with the system&apos;s
relations, these 1203 relations are likely to contain
many that the human probably missed. For
example, the relations in the machine ontology
between ‘eye’ and ‘farsightedness’, and ‘medicine’
CompuTerm 2004 - 3rd International Workshop on Computational Terminology 53
The number of relations in H that our system
missed (relations that were more than distance 1
away in the system ontology), is 3493. However,
of these 3493 relations, 2955 involved at least 1
term that was not included in M, leaving 538
relations that we could calculate the distance for in
M. These 538 relations in H include relations
between ‘acid indigestion medicine’ and ‘maalox’,
and ‘alternative medicine’ and ‘acupuncture’ (a
majority of the misses involved relations between a
disease and the name of a specific drug for it,
which aren’t part-of or kind-of relations).
</bodyText>
<figureCaption confidence="0.998311">
Figure 4: Relation Precision
</figureCaption>
<bodyText confidence="0.999032666666667">
These observations lead to a metric for
comparing one ontology with another one serving
as a reference ontology. Given two ontologies A
and B, define Relation Precision (A, B, D) as the
proportion of the distance 1 relations in A that are
at most a distance D apart in B. This measure can
be plotted for different values of D. In Figure 4, we
show the Relation Precision(H, M, D), and
Relation Precision(M, H, D), for our machine
ontology M and human ontology H. Both curves
show Relation Precision(H, M, D) growing faster
than Relation Precision(M, H, D), with 70% of the
area being below the former curve and 54% being
below the latter curve. The graph shows that while
22% of distance 1 relations in M are at most 3
apart in H (but keep in mind the errors of omission
in H), 40% of distance 1 relations in H are at most
3 apart in M10.
</bodyText>
<page confidence="0.424371">
10 The mean distance in H between terms that are
</page>
<figureCaption confidence="0.5557495">
distance 1 apart in M is 5.17, with a standard deviation
of 2.12. The mean distance in M between terms which
are distance 1 apart in H is 3.85, with a standard
deviation of 1.69.
</figureCaption>
<sectionHeader confidence="0.999762" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999970818181818">
merging ontologies or database schemas (Doan et
al. 2002). Other approaches use natural language
data, sometimes just by analyzing the corpus
(Sanderson and Croft 1999), (Caraballo 1999) or
by learning to expand WordNet with clusters of
terms from a corpus, e.g., (Girju et al. 2003).
Information extraction approaches that infer
labeled relations either require substantial hand-
created linguistic or domain knowledge, e.g.,
(Craven and Kumlien 1999) (Hull and Gomez
1993), or require human-annotated training data
with relation information for each domain (Craven
et al. 1998).
Many, though not all, domain-independent
approaches (Evans et al. 1991) (Grefenstette 1997)
have restricted themselves to discovering term-
associations, rather than labeled relations. A
notable exception is (Sanderson and Croft 1995),
which (unlike our approach) assumes the existence
of a query that was used to originally retrieve the
documents (so that terms can be extracted from the
query and then expanded to generate additional
terms for the ontology). Their approach also is
restricted to one method to discover relations,
while we use several.
Our approach is complementary to approaches
aimed at automatically enhancing existing
resources for a particular domain, e.g. (Moldovan
et al. 2000). Finally, the prior methods, while they
often carry out evaluation, lack standard criteria for
ontology evaluation. Although ontology evaluation
remains challenging, we have discussed several
evaluation methods in this paper.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999920388888889">
The evidence combination described above is
based on transitivity and union. Since the above
evaluations, we have been experimenting with an
ad hoc weighted evidence combination scheme,
based on each knowledge source expressing a
strength for a posited relation. In future, we will
also investigate using an initial seed ontology to
provide a better ‘backbone’ for induction, and then
using a spreading activation method to activate
nodes related by existing knowledge sources to
seed nodes. Corpus statistics can be used to weight
the links. For example, based on (Caraballo 1999),
each parent of a leaf node could be viewed as a
cluster label for its children, with the weight of a
parent-child link being determined based on how
strongly the child is associated with the cluster.
The ontology induction methods described here
can allow for considerable savings in time in
</bodyText>
<figure confidence="0.991368434782609">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
1 2 3 4 5 6 7 8 9
D
Relation-
Prec.(H,M,D)
Relation-
Prec.(M,H,D)
and ‘chiropractic medicine’ are missed by H. This
highlights a problem with human-generated
o The existing approaches to ontology induction
ontologies: substantial errors of omission.
include those that start from structured data,
54 CompuTerm 2004 - 3rd International Workshop on Computational Terminology
</figure>
<bodyText confidence="0.999833466666667">
constructing ontologies. The evaluations we have
carried out are suggestive, but many issues remain
open. There are many unanswered questions about
human-created reference ontologies, including lack
of inter-annotator agreement studies. Indeed,
experience shows that without guidelines for
ontology construction, humans are prone to come
up with very different ontologies for a domain.
Comparing a machine-induced ontology against an
ideal human reference ontology, were one to be
available, is also fraught with problems. Our
experience with using an implementation of the
(Daude et al. 2001) constraint relaxation algorithm
for ontology comparison suggests that much work
is needed on distance metrics which are not over-
sensitive to small differences in structure.
Our interest, therefore, is focused more towards
an extrinsic evaluation. PRONTO, which is due to
be released in 2004, offers the opportunity to
measure costs of ontology induction and post-
editing on a large-scale problem of value to the
biology community. We also plan to measure the
effectiveness of PRONTO in query expansion for
information access to MEDLINE and protein
databases. Finally, we will investigate more
sophisticated evidence combination methods, and
compare against other automatic methods for
ontology induction.
The ontology induction tools are available for
free distribution for research purposes.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999932586666666">
Abney, S. 1996. Partial parsing Via Finite-State
Cascades. Proceedings of the ESSLLI &apos;96 Robust
Parsing Workshop.
Caraballo, S. A. 1999. Automatic Construction of a
hypernym-labeled noun hierarchy from text. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL&apos;1999), 120-122.
Cohen, P. R., Chaudhri, V., Pease, A. and Schrag,
R. 1999. Does Prior Knowledge Facilitate the
Development of Knowledge-based Systems? The
Sixteenth National Conference on Artificial
Intelligence (AAAI-99).
Craven, M. and Kumlien, J. 1999. Constructing
biological knowledge bases by extracting
information from text sources. Proc Int Conf
Intell Syst Mol Biol., 77-86.
Craven, M., DiPasquo, D., Freitag, D., McCallum,
A., Mitchell, T., Nigam, K., and Slattery, S..
1998. Learning to Extract Symbolic Knowledge
from the World Wide Web. Proceedings of
AAAI-98, 509-516.
Daude, J., Padro, L. and Rigau, G. 2001 A
Complete WN1.5 to WN1.6 Mapping. NAACL-
2001 Workshop on WordNet and Other Lexical
Resources: Applications, Extension, and
Customization, 83-88.
Doan, A., Madhavan, J. , Domings, P. and Halevy,
A. 2002. Learning to Map between Ontologies
on the Semantic Web. WWW’2002.
Dunning, T. 1993. Accurate Methods for the
Statistics of Surprise and Coincidence,”
Computational Linguistics, 19(1):61-74, 1993.
Girju, R., Badulescu, A., and Moldovan, D. 2003.
Learning Semantic Constraints for the Automatic
Discovery of Part-Whole Relations. Proceedings
of HLT’2003, Edmonton.
Grefenstette, G. 1997. Explorations in Automatic
Thesaurus Discovery. Kluwer International
Series in Engineering and Computer Science,
Vol 278.
Hearst, M. 1992. Automatic Acquisition of
Hyponyms from Large Text Corpora.
Proceedings of the fourteenth International
Conference on Computational Linguistics,
Nantes, France, July 1992.
Hull, R. and Gomez, F. 1993. Inferring Heuristic
Classification Hierarchies from Natural
Language Input. Telematics and Informatics,
9(3/4), pp. 265-281.
IRS (Internal Revenue Service). 2001. Tax Guide
2001. Publication 17. http://www.irs.gov/pub/irs-
pdf/p17.pdf
Lawrie, D., Croft, W. B., and Rosenberg, A. 2001.
Finding topic words for hierarchical
summarization. 24th ACM Intl. Conf. on
Research and Development in Information
Retrieval, 349-357, 2001.
Miller, G. (1995). WordNet: A Lexical Database
for English. Communications Of the Association
For Computing Machinery (CACM) 38, 39-41.
Sanderson, M. and Croft, B. 1995. Deriving
concept hierarchies from text. Proceedings of the
22nd Annual Internationaql ACM SIGIR
Conference on Research and Development in
Information Retrieval, 160-170.
Sekine, S., Sudo, K. and Ogino, T. 1999. Statistical
Matching of Two Ontologies. Proceedings of
ACL SIGLEX99 Workshop: Standardizing
Lexical Resources.
Zhang, K., Wang, J. T. L. and Shasha, D. 1996.
On the Editing Distance between Undirected
Acyclic Graphs and Related Problems.
International Journal of Foundations of
Computer Science 7, 43-58.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.452012">
<note confidence="0.641827">CompuTerm 2004 - 3rd International Workshop on Computational Terminology 47</note>
<title confidence="0.975885">Automatically Inducing Ontologies from Corpora</title>
<author confidence="0.998833">Inderjeet Mani</author>
<affiliation confidence="0.996948">Department of Georgetown University, ICC and O Sts,</affiliation>
<address confidence="0.942059">Washington, DC 20057,</address>
<email confidence="0.998677">im5@georgetown.edu</email>
<author confidence="0.978587">Ken Samuel</author>
<author confidence="0.978587">Kris Concepcion</author>
<author confidence="0.978587">David Vogel</author>
<affiliation confidence="0.969129">The MITRE</affiliation>
<address confidence="0.9717505">7515 Colshire McLean, VA 22102,</address>
<email confidence="0.890233">samuel@mitre.org</email>
<email confidence="0.890233">kjc9@mitre.org</email>
<email confidence="0.890233">dvogel@mitre.org</email>
<abstract confidence="0.999462176470588">The emergence of vast quantities of on-line information has raised the importance of methods for automatic cataloguing of information in a variety of domains, including electronic commerce and bioinformatics. Ontologies can play a critical role in such cataloguing. In this paper, we describe a system that automatically induces an ontology from any large on-line text collection in a specific domain. The ontology that is induced consists of concepts, related by links. To achieve domain-independence, we use a combination of relatively shallow methods along with any available repositories of applicable background knowledge. We describe our evaluation experiences using these methods, and provide examples of induced structures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Partial parsing Via Finite-State Cascades.</title>
<date>1996</date>
<booktitle>Proceedings of the ESSLLI &apos;96 Robust Parsing Workshop.</booktitle>
<contexts>
<context position="7544" citStr="Abney 1996" startWordPosition="1101" endWordPosition="1102">.g., “steroid/thyroid/retinoic nuclear hormone receptor homolog nhr-35”, and involve specialized patterns. In constructing PRONTO, we have used a protein name tagger based on an ensemble of statistical classifiers to tag protein names in collections of MEDLINE abstracts (Anon 2004). Thus, in such a domain, a specialized tagger replaces the components in the dotted box in Figure 1. In other domains, we adopt a generic termdiscovery approach. Here the text is tagged for part-of-speech, and single- and multi-word terms consisting of minimal NPs are extracted using finite-state parsing with CASS (Abney 1996). All punctuation except for hyphens are removed from the terms, which are then lower-cased. Each word in each term is stemmed, with statistics (see below) being gathered for each stemmed term. Multi-word terms are clustered so that open, closed and hyphenated compounds are treated as equivalent, with the most frequent term in the collection being used as the cluster representative. The terms are scored for domain-relevance based on the assumption that if a term occurs significantly more in a domain corpus than in a more diffuse background corpus, then the term is clearly domain relevant. As a</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Abney, S. 1996. Partial parsing Via Finite-State Cascades. Proceedings of the ESSLLI &apos;96 Robust Parsing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Caraballo</author>
</authors>
<title>Automatic Construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL&apos;1999),</booktitle>
<pages>120--122</pages>
<contexts>
<context position="11940" citStr="Caraballo 1999" startWordPosition="1793" endWordPosition="1794">CBS has run is the ProMed corpus where, considering each paragraph a distinct context, there were 117,690 contexts in the 11,198 documents. Here is an example from ProMed of a transitive relation that spans three tiers: ‘mosquito’ is a hypernym of ‘mosquito pool’, and ‘mosquito’ is also a hypernym of ‘standing water’. 50 CompuTerm 2004 - 3rd International Workshop on Computational Terminology 2.3.4 Explicit Patterns Relations This knowledge source infers specific relations between terms based on characteristic cue-phrases which relate them. For example, the cue-phrase “such as” (Hearst 1992) (Caraballo 1999) suggest a kind-of relation, e.g., ‘a ligand such as triethylphosphine’ tells us that ‘triethylphosphene’ is a kind of ‘ligand’. Likewise, in the TREC domain, ‘air toxics such as benzene’ can suggest that ‘benzene’ is a kind of ‘air toxic’. However, since such cue-phrase patterns tend to be sparse in occurrence, we do not use them in the evaluations described below. 2.3.5 Domain-Specific Knowledge Sources Although our approach is domain-independent, it is possible to factor in domain knowledge sources for a given domain. For example, in biology, ‘ase’ is usually a suffix indicating an enzyme. </context>
<context position="24871" citStr="Caraballo 1999" startWordPosition="3942" endWordPosition="3943">h shows that while 22% of distance 1 relations in M are at most 3 apart in H (but keep in mind the errors of omission in H), 40% of distance 1 relations in H are at most 3 apart in M10. 10 The mean distance in H between terms that are distance 1 apart in M is 5.17, with a standard deviation of 2.12. The mean distance in M between terms which are distance 1 apart in H is 3.85, with a standard deviation of 1.69. 4 Related Work merging ontologies or database schemas (Doan et al. 2002). Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999), (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003). Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993), or require human-annotated training data with relation information for each domain (Craven et al. 1998). Many, though not all, domain-independent approaches (Evans et al. 1991) (Grefenstette 1997) have restricted themselves to discovering termassociations, rather than labeled relations. A notable exception is (</context>
<context position="26785" citStr="Caraballo 1999" startWordPosition="4226" endWordPosition="4227">thods in this paper. 5 Conclusion The evidence combination described above is based on transitivity and union. Since the above evaluations, we have been experimenting with an ad hoc weighted evidence combination scheme, based on each knowledge source expressing a strength for a posited relation. In future, we will also investigate using an initial seed ontology to provide a better ‘backbone’ for induction, and then using a spreading activation method to activate nodes related by existing knowledge sources to seed nodes. Corpus statistics can be used to weight the links. For example, based on (Caraballo 1999), each parent of a leaf node could be viewed as a cluster label for its children, with the weight of a parent-child link being determined based on how strongly the child is associated with the cluster. The ontology induction methods described here can allow for considerable savings in time in 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1 1 2 3 4 5 6 7 8 9 D RelationPrec.(H,M,D) RelationPrec.(M,H,D) and ‘chiropractic medicine’ are missed by H. This highlights a problem with human-generated o The existing approaches to ontology induction ontologies: substantial errors of omission. include those that s</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Caraballo, S. A. 1999. Automatic Construction of a hypernym-labeled noun hierarchy from text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL&apos;1999), 120-122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>V Chaudhri</author>
<author>A Pease</author>
<author>R Schrag</author>
</authors>
<date>1999</date>
<booktitle>Does Prior Knowledge Facilitate the Development of Knowledge-based Systems? The Sixteenth National Conference on Artificial Intelligence (AAAI-99).</booktitle>
<contexts>
<context position="1902" citStr="Cohen et al. 1999" startWordPosition="267" endWordPosition="270">matic cataloguing of information in a variety of domains, including electronic commerce and bioinformatics. Ontologies1 can play a critical role in such cataloguing. In bioinformatics, for example, there is growing recognition that common ontologies, e.g., the Gene Ontology2, are critical to interoperation and integration of biological data, including both structured data as found in protein databases, as well as unstructured data, as found in on-line biomedical literature. Constructing an ontology is an extremely laborious effort. Even with some reuse of “core” knowledge from an Upper Model (Cohen et al. 1999), the task of creating an ontology for a particular domain and task has a high cost, incurred for each new domain. Tools that could automate, or semi-automate, the construction of 1 This research was supported by the National Science Foundation (ITR-0205470). 2 www.geneontology.org ontologies for different domains could dramatically reduce the knowledge creation cost. One approach to developing such tools is to rely on information implicit in collections of on-line text in a particular domain. If it were possible to automatically extract terms and their semantic relations from the text corpus,</context>
</contexts>
<marker>Cohen, Chaudhri, Pease, Schrag, 1999</marker>
<rawString>Cohen, P. R., Chaudhri, V., Pease, A. and Schrag, R. 1999. Does Prior Knowledge Facilitate the Development of Knowledge-based Systems? The Sixteenth National Conference on Artificial Intelligence (AAAI-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
<author>J Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>Proc Int Conf Intell Syst Mol Biol.,</booktitle>
<pages>77--86</pages>
<contexts>
<context position="25135" citStr="Craven and Kumlien 1999" startWordPosition="3979" endWordPosition="3982">M is 5.17, with a standard deviation of 2.12. The mean distance in M between terms which are distance 1 apart in H is 3.85, with a standard deviation of 1.69. 4 Related Work merging ontologies or database schemas (Doan et al. 2002). Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999), (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003). Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993), or require human-annotated training data with relation information for each domain (Craven et al. 1998). Many, though not all, domain-independent approaches (Evans et al. 1991) (Grefenstette 1997) have restricted themselves to discovering termassociations, rather than labeled relations. A notable exception is (Sanderson and Croft 1995), which (unlike our approach) assumes the existence of a query that was used to originally retrieve the documents (so that terms can be extracted from the query and then expanded to generate additional terms for the ontology). Their approa</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Craven, M. and Kumlien, J. 1999. Constructing biological knowledge bases by extracting information from text sources. Proc Int Conf Intell Syst Mol Biol., 77-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
<author>D DiPasquo</author>
<author>D Freitag</author>
<author>A McCallum</author>
<author>T Mitchell</author>
<author>K Nigam</author>
<author>S Slattery</author>
</authors>
<title>Learning to Extract Symbolic Knowledge from the World Wide Web.</title>
<date>1998</date>
<booktitle>Proceedings of AAAI-98,</booktitle>
<pages>509--516</pages>
<contexts>
<context position="25262" citStr="Craven et al. 1998" startWordPosition="3998" endWordPosition="4001">tandard deviation of 1.69. 4 Related Work merging ontologies or database schemas (Doan et al. 2002). Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999), (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003). Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993), or require human-annotated training data with relation information for each domain (Craven et al. 1998). Many, though not all, domain-independent approaches (Evans et al. 1991) (Grefenstette 1997) have restricted themselves to discovering termassociations, rather than labeled relations. A notable exception is (Sanderson and Croft 1995), which (unlike our approach) assumes the existence of a query that was used to originally retrieve the documents (so that terms can be extracted from the query and then expanded to generate additional terms for the ontology). Their approach also is restricted to one method to discover relations, while we use several. Our approach is complementary to approaches ai</context>
</contexts>
<marker>Craven, DiPasquo, Freitag, McCallum, Mitchell, Nigam, Slattery, 1998</marker>
<rawString>Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., and Slattery, S.. 1998. Learning to Extract Symbolic Knowledge from the World Wide Web. Proceedings of AAAI-98, 509-516.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Daude</author>
<author>L Padro</author>
<author>G Rigau</author>
</authors>
<date>2001</date>
<booktitle>A Complete WN1.5 to WN1.6 Mapping. NAACL2001 Workshop on WordNet and Other Lexical Resources: Applications, Extension, and Customization,</booktitle>
<pages>83--88</pages>
<contexts>
<context position="20631" citStr="Daude et al. 2001" startWordPosition="3211" endWordPosition="3214">and 10 decisions involving WordNet (with 12 True Positives). This shows that there is solid agreement between the human subjects and the system on the kind-of relations. However, these 154 decisions involved only four newspaper articles, so clearly more data would be helpful. 3.3 Automatic Evaluation While evaluation by humans is valuable, it is expensive to carry out, and this expense must be incurred each time one wants to do an evaluation. Automatic comparison of a machine-generated ontology against reference ontologies constructed by humans, e.g., (Zhang et al. 1996) (Sekine et al. 1999) (Daude et al. 2001), is therefore desirable, provided suitable reference ontologies are available. In this evaluation, the human-generated taxonomy for ProMed described in Section 3.2.1 was used as a reference ontology, with its unlabeled parent-child relation treated as a kind-of link. However, the human ‘ontology’ was created without looking at a corpus, and was developed for use with a different set of goals in mind. Although this involves comparing ‘apples’ and ‘oranges’, a comparison is nevertheless illustrative, and can in 9 The chi-square for Subphrase Relations is 61.68, and the chi-square for WordNet is</context>
<context position="28081" citStr="Daude et al. 2001" startWordPosition="4425" endWordPosition="4428">omputational Terminology constructing ontologies. The evaluations we have carried out are suggestive, but many issues remain open. There are many unanswered questions about human-created reference ontologies, including lack of inter-annotator agreement studies. Indeed, experience shows that without guidelines for ontology construction, humans are prone to come up with very different ontologies for a domain. Comparing a machine-induced ontology against an ideal human reference ontology, were one to be available, is also fraught with problems. Our experience with using an implementation of the (Daude et al. 2001) constraint relaxation algorithm for ontology comparison suggests that much work is needed on distance metrics which are not oversensitive to small differences in structure. Our interest, therefore, is focused more towards an extrinsic evaluation. PRONTO, which is due to be released in 2004, offers the opportunity to measure costs of ontology induction and postediting on a large-scale problem of value to the biology community. We also plan to measure the effectiveness of PRONTO in query expansion for information access to MEDLINE and protein databases. Finally, we will investigate more sophist</context>
</contexts>
<marker>Daude, Padro, Rigau, 2001</marker>
<rawString>Daude, J., Padro, L. and Rigau, G. 2001 A Complete WN1.5 to WN1.6 Mapping. NAACL2001 Workshop on WordNet and Other Lexical Resources: Applications, Extension, and Customization, 83-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domings</author>
<author>A Halevy</author>
</authors>
<date>2002</date>
<booktitle>Learning to Map between Ontologies on the Semantic Web. WWW’2002.</booktitle>
<marker>Domings, Halevy, 2002</marker>
<rawString>Doan, A., Madhavan, J. , Domings, P. and Halevy, A. 2002. Learning to Map between Ontologies on the Semantic Web. WWW’2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and</title>
<date>1993</date>
<journal>Coincidence,” Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="8623" citStr="Dunning 1993" startWordPosition="1281" endWordPosition="1282"> occurs significantly more in a domain corpus than in a more diffuse background corpus, then the term is clearly domain relevant. As an illustration, in Table 1 we compare the number of documents containing the term ‘income tax’ (or ‘income taxes’) in a long (2.18 Mb) IRS publication, Publication 17, from an IRS web site (IRS 2001) compared to a larger (27.63 Mb subset of the) Reuters 21578 news corpus7. One would expect that ‘income tax’ is much more a characteristic of the IRS publication, and this is borne out by the document frequencies in the table. We use the log likelihood ratio (LLR) (Dunning 1993) given by -2log2(Ho(p;k1,n1,k2,n2)/Ha(p1,p2;n1,k1,n2,k2)) LLR measures the extent to which a hypothesized model of the distribution of cell counts, Ha, differs from the null hypothesis, Ho (namely, that the percentage of documents containing this term is the same in both corpora). We used a binomial model for Ho and Ha8. 2.3 Relationship Discovery The main innovation in our approach is to fuse together information from multiple knowledge 7 In Publication 17, each “chapter” is a document. 8From Table 1, p=294/19238=.015, p1=285/285=1.0, p2=9/19043=4.72, k1=285, n1=285, k2=9, n2=19043. sources a</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, T. 1993. Accurate Methods for the Statistics of Surprise and Coincidence,” Computational Linguistics, 19(1):61-74, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations.</title>
<date>2003</date>
<booktitle>Proceedings of HLT’2003,</booktitle>
<location>Edmonton.</location>
<contexts>
<context position="24968" citStr="Girju et al. 2003" startWordPosition="3958" endWordPosition="3961"> the errors of omission in H), 40% of distance 1 relations in H are at most 3 apart in M10. 10 The mean distance in H between terms that are distance 1 apart in M is 5.17, with a standard deviation of 2.12. The mean distance in M between terms which are distance 1 apart in H is 3.85, with a standard deviation of 1.69. 4 Related Work merging ontologies or database schemas (Doan et al. 2002). Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999), (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003). Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993), or require human-annotated training data with relation information for each domain (Craven et al. 1998). Many, though not all, domain-independent approaches (Evans et al. 1991) (Grefenstette 1997) have restricted themselves to discovering termassociations, rather than labeled relations. A notable exception is (Sanderson and Croft 1995), which (unlike our approach) assumes the existence of a query that was </context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Girju, R., Badulescu, A., and Moldovan, D. 2003. Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations. Proceedings of HLT’2003, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1997</date>
<booktitle>International Series in Engineering and Computer Science, Vol 278.</booktitle>
<publisher>Kluwer</publisher>
<contexts>
<context position="25355" citStr="Grefenstette 1997" startWordPosition="4012" endWordPosition="4013">2002). Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999), (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003). Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993), or require human-annotated training data with relation information for each domain (Craven et al. 1998). Many, though not all, domain-independent approaches (Evans et al. 1991) (Grefenstette 1997) have restricted themselves to discovering termassociations, rather than labeled relations. A notable exception is (Sanderson and Croft 1995), which (unlike our approach) assumes the existence of a query that was used to originally retrieve the documents (so that terms can be extracted from the query and then expanded to generate additional terms for the ontology). Their approach also is restricted to one method to discover relations, while we use several. Our approach is complementary to approaches aimed at automatically enhancing existing resources for a particular domain, e.g. (Moldovan et </context>
</contexts>
<marker>Grefenstette, 1997</marker>
<rawString>Grefenstette, G. 1997. Explorations in Automatic Thesaurus Discovery. Kluwer International Series in Engineering and Computer Science, Vol 278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>Proceedings of the fourteenth International Conference on Computational Linguistics,</booktitle>
<location>Nantes, France,</location>
<contexts>
<context position="11923" citStr="Hearst 1992" startWordPosition="1791" endWordPosition="1792">against which CBS has run is the ProMed corpus where, considering each paragraph a distinct context, there were 117,690 contexts in the 11,198 documents. Here is an example from ProMed of a transitive relation that spans three tiers: ‘mosquito’ is a hypernym of ‘mosquito pool’, and ‘mosquito’ is also a hypernym of ‘standing water’. 50 CompuTerm 2004 - 3rd International Workshop on Computational Terminology 2.3.4 Explicit Patterns Relations This knowledge source infers specific relations between terms based on characteristic cue-phrases which relate them. For example, the cue-phrase “such as” (Hearst 1992) (Caraballo 1999) suggest a kind-of relation, e.g., ‘a ligand such as triethylphosphine’ tells us that ‘triethylphosphene’ is a kind of ‘ligand’. Likewise, in the TREC domain, ‘air toxics such as benzene’ can suggest that ‘benzene’ is a kind of ‘air toxic’. However, since such cue-phrase patterns tend to be sparse in occurrence, we do not use them in the evaluations described below. 2.3.5 Domain-Specific Knowledge Sources Although our approach is domain-independent, it is possible to factor in domain knowledge sources for a given domain. For example, in biology, ‘ase’ is usually a suffix indic</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. Proceedings of the fourteenth International Conference on Computational Linguistics, Nantes, France, July 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hull</author>
<author>F Gomez</author>
</authors>
<title>Inferring Heuristic Classification Hierarchies from Natural Language Input.</title>
<date>1993</date>
<journal>Telematics and Informatics,</journal>
<volume>9</volume>
<issue>3</issue>
<pages>265--281</pages>
<contexts>
<context position="25157" citStr="Hull and Gomez 1993" startWordPosition="3983" endWordPosition="3986"> deviation of 2.12. The mean distance in M between terms which are distance 1 apart in H is 3.85, with a standard deviation of 1.69. 4 Related Work merging ontologies or database schemas (Doan et al. 2002). Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999), (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003). Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993), or require human-annotated training data with relation information for each domain (Craven et al. 1998). Many, though not all, domain-independent approaches (Evans et al. 1991) (Grefenstette 1997) have restricted themselves to discovering termassociations, rather than labeled relations. A notable exception is (Sanderson and Croft 1995), which (unlike our approach) assumes the existence of a query that was used to originally retrieve the documents (so that terms can be extracted from the query and then expanded to generate additional terms for the ontology). Their approach also is restricted </context>
</contexts>
<marker>Hull, Gomez, 1993</marker>
<rawString>Hull, R. and Gomez, F. 1993. Inferring Heuristic Classification Hierarchies from Natural Language Input. Telematics and Informatics, 9(3/4), pp. 265-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IRS</author>
</authors>
<title>Tax Guide</title>
<date>2001</date>
<note>Publication 17. http://www.irs.gov/pub/irspdf/p17.pdf</note>
<contexts>
<context position="4549" citStr="IRS 2001" startWordPosition="666" endWordPosition="667">bed in Section 2. In Section 3, we also introduce a new metric Relation Precision for evaluating induced ontologies in comparison with reference ontologies. We have applied our system to produce ontologies in numerous domains: 48 CompuTerm 2004 - 3rd International Workshop on Computational Terminology Figure 1: System Architecture IRS 285 0 285 Publication 17 k1 n1 Reuters 9 19,024 19,043 Corpus k2 n2 Total 294 19,024 19,328 Table 1: Distribution of ‘income tax’ in domain and background corpora (i) newswire from the TREC collection (ii) taxation information from the IRS (Publication 17, from (IRS 2001)), (iii) epidemiological newsgroup messages from the Program for Monitoring Emerging Diseases (PROMED) from the Federation of American Scientists3, (iv) the text of a book by the first author called Automatic Summarization, and (v) MEDLINE biomedical abstracts retrieved from the National Library of Medicine’s PubMed system4. In the latter domain, we have begun building a large ontology using the ontology induction methods along with postediting by domain experts in molecular biology at Georgetown University 5 . This ontology, called PRONTO, involves hundreds of thousands of protein names found</context>
<context position="8343" citStr="IRS 2001" startWordPosition="1232" endWordPosition="1233">rm. Multi-word terms are clustered so that open, closed and hyphenated compounds are treated as equivalent, with the most frequent term in the collection being used as the cluster representative. The terms are scored for domain-relevance based on the assumption that if a term occurs significantly more in a domain corpus than in a more diffuse background corpus, then the term is clearly domain relevant. As an illustration, in Table 1 we compare the number of documents containing the term ‘income tax’ (or ‘income taxes’) in a long (2.18 Mb) IRS publication, Publication 17, from an IRS web site (IRS 2001) compared to a larger (27.63 Mb subset of the) Reuters 21578 news corpus7. One would expect that ‘income tax’ is much more a characteristic of the IRS publication, and this is borne out by the document frequencies in the table. We use the log likelihood ratio (LLR) (Dunning 1993) given by -2log2(Ho(p;k1,n1,k2,n2)/Ha(p1,p2;n1,k1,n2,k2)) LLR measures the extent to which a hypothesized model of the distribution of cell counts, Ha, differs from the null hypothesis, Ho (namely, that the percentage of documents containing this term is the same in both corpora). We used a binomial model for Ho and Ha</context>
</contexts>
<marker>IRS, 2001</marker>
<rawString>IRS (Internal Revenue Service). 2001. Tax Guide 2001. Publication 17. http://www.irs.gov/pub/irspdf/p17.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lawrie</author>
<author>W B Croft</author>
<author>A Rosenberg</author>
</authors>
<title>Finding topic words for hierarchical summarization.</title>
<date>2001</date>
<booktitle>24th ACM Intl. Conf. on Research and Development in Information Retrieval,</booktitle>
<pages>349--357</pages>
<contexts>
<context position="10744" citStr="Lawrie et al. 2001" startWordPosition="1610" endWordPosition="1613">Synonyms are also merged together at this stage. 2.3.3 Contextual Subsumption Relations We also infer hierarchical relations between terms, by top-down clustering using a contextbased subsumption (CBS) algorithm. The algorithm uses a probabilistic measure of set covering to find subsumption relations. For each term in the corpus, we note the set of contexts in which the term appears. Term1 is said to subsume term2 when the conditional probability of term1 appearing in a context given the presence of term2, i.e., P(term1|term2), is greater than some threshold. CBS is based on the algorithm of (Lawrie et al. 2001), which used a greedy approximation of the Domination Set Problem for graphs to discover subsumption relations among terms. Unlike their work, we did not seek to minimize the set of covering terms; therefore, a subsumed term may have multiple parents. The conditional probability threshold (0.8) we use to determine subsumption is much higher than in their approach. We also restrict the height of the hierarchies we build to three tiers. Tightening these latter two constraints appears to notably improve the quality of our subsumption relations. The largest corpus against which CBS has run is the </context>
</contexts>
<marker>Lawrie, Croft, Rosenberg, 2001</marker>
<rawString>Lawrie, D., Croft, W. B., and Rosenberg, A. 2001. Finding topic words for hierarchical summarization. 24th ACM Intl. Conf. on Research and Development in Information Retrieval, 349-357, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications Of the Association For Computing Machinery (CACM)</journal>
<volume>38</volume>
<pages>39--41</pages>
<marker>Miller, 1995</marker>
<rawString>Miller, G. (1995). WordNet: A Lexical Database for English. Communications Of the Association For Computing Machinery (CACM) 38, 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sanderson</author>
<author>B Croft</author>
</authors>
<title>Deriving concept hierarchies from text.</title>
<date>1995</date>
<booktitle>Proceedings of the 22nd Annual Internationaql ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>160--170</pages>
<contexts>
<context position="25496" citStr="Sanderson and Croft 1995" startWordPosition="4029" endWordPosition="4032"> or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003). Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993), or require human-annotated training data with relation information for each domain (Craven et al. 1998). Many, though not all, domain-independent approaches (Evans et al. 1991) (Grefenstette 1997) have restricted themselves to discovering termassociations, rather than labeled relations. A notable exception is (Sanderson and Croft 1995), which (unlike our approach) assumes the existence of a query that was used to originally retrieve the documents (so that terms can be extracted from the query and then expanded to generate additional terms for the ontology). Their approach also is restricted to one method to discover relations, while we use several. Our approach is complementary to approaches aimed at automatically enhancing existing resources for a particular domain, e.g. (Moldovan et al. 2000). Finally, the prior methods, while they often carry out evaluation, lack standard criteria for ontology evaluation. Although ontolo</context>
</contexts>
<marker>Sanderson, Croft, 1995</marker>
<rawString>Sanderson, M. and Croft, B. 1995. Deriving concept hierarchies from text. Proceedings of the 22nd Annual Internationaql ACM SIGIR Conference on Research and Development in Information Retrieval, 160-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>K Sudo</author>
<author>T Ogino</author>
</authors>
<title>Statistical Matching of Two Ontologies.</title>
<date>1999</date>
<booktitle>Proceedings of ACL SIGLEX99 Workshop: Standardizing Lexical Resources.</booktitle>
<contexts>
<context position="20611" citStr="Sekine et al. 1999" startWordPosition="3207" endWordPosition="3210">18 False Negatives), and 10 decisions involving WordNet (with 12 True Positives). This shows that there is solid agreement between the human subjects and the system on the kind-of relations. However, these 154 decisions involved only four newspaper articles, so clearly more data would be helpful. 3.3 Automatic Evaluation While evaluation by humans is valuable, it is expensive to carry out, and this expense must be incurred each time one wants to do an evaluation. Automatic comparison of a machine-generated ontology against reference ontologies constructed by humans, e.g., (Zhang et al. 1996) (Sekine et al. 1999) (Daude et al. 2001), is therefore desirable, provided suitable reference ontologies are available. In this evaluation, the human-generated taxonomy for ProMed described in Section 3.2.1 was used as a reference ontology, with its unlabeled parent-child relation treated as a kind-of link. However, the human ‘ontology’ was created without looking at a corpus, and was developed for use with a different set of goals in mind. Although this involves comparing ‘apples’ and ‘oranges’, a comparison is nevertheless illustrative, and can in 9 The chi-square for Subphrase Relations is 61.68, and the chi-s</context>
</contexts>
<marker>Sekine, Sudo, Ogino, 1999</marker>
<rawString>Sekine, S., Sudo, K. and Ogino, T. 1999. Statistical Matching of Two Ontologies. Proceedings of ACL SIGLEX99 Workshop: Standardizing Lexical Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
<author>J T L Wang</author>
<author>D Shasha</author>
</authors>
<title>On the Editing Distance between Undirected Acyclic Graphs and Related Problems.</title>
<date>1996</date>
<journal>International Journal of Foundations of Computer Science</journal>
<volume>7</volume>
<pages>43--58</pages>
<contexts>
<context position="20590" citStr="Zhang et al. 1996" startWordPosition="3203" endWordPosition="3206"> True Positives and 18 False Negatives), and 10 decisions involving WordNet (with 12 True Positives). This shows that there is solid agreement between the human subjects and the system on the kind-of relations. However, these 154 decisions involved only four newspaper articles, so clearly more data would be helpful. 3.3 Automatic Evaluation While evaluation by humans is valuable, it is expensive to carry out, and this expense must be incurred each time one wants to do an evaluation. Automatic comparison of a machine-generated ontology against reference ontologies constructed by humans, e.g., (Zhang et al. 1996) (Sekine et al. 1999) (Daude et al. 2001), is therefore desirable, provided suitable reference ontologies are available. In this evaluation, the human-generated taxonomy for ProMed described in Section 3.2.1 was used as a reference ontology, with its unlabeled parent-child relation treated as a kind-of link. However, the human ‘ontology’ was created without looking at a corpus, and was developed for use with a different set of goals in mind. Although this involves comparing ‘apples’ and ‘oranges’, a comparison is nevertheless illustrative, and can in 9 The chi-square for Subphrase Relations is</context>
</contexts>
<marker>Zhang, Wang, Shasha, 1996</marker>
<rawString>Zhang, K., Wang, J. T. L. and Shasha, D. 1996. On the Editing Distance between Undirected Acyclic Graphs and Related Problems. International Journal of Foundations of Computer Science 7, 43-58.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>