<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.9980075">
Empirically-motivated Generalizations of CCG Semantic Parsing
Learning Algorithms
</title>
<author confidence="0.996586">
Jesse Glass
</author>
<affiliation confidence="0.996909">
Temple University
</affiliation>
<address confidence="0.542059">
1801 N Broad Street
Philadelphia, PA 19122, USA
</address>
<email confidence="0.987736">
jglassemc2@gmail.com
</email>
<author confidence="0.980591">
Alexander Yates
</author>
<affiliation confidence="0.988003">
Temple University
</affiliation>
<address confidence="0.5430095">
1801 N Broad Street
Philadelphia, PA 19122, USA
</address>
<email confidence="0.996206">
ayates@gmail.com
</email>
<sectionHeader confidence="0.994738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999805666666667">
Learning algorithms for semantic parsing
have improved drastically over the past
decade, as steady improvements on bench-
mark datasets have shown. In this pa-
per we investigate whether they can gen-
eralize to a novel biomedical dataset that
differs in important respects from the tra-
ditional geography and air travel bench-
mark datasets. Empirical results for two
state-of-the-art PCCG semantic parsers in-
dicates that learning algorithms are sensi-
tive to the kinds of semantic and syntac-
tic constructions used in a domain. In re-
sponse, we develop a novel learning algo-
rithm that can produce an effective seman-
tic parser for geography, as well as a much
better semantic parser for the biomedical
dataset.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970234042553">
Semantic parsing is the task of converting nat-
ural language utterances into formal representa-
tions of their meaning. In this paper, we consider
in particular a grounded form of semantic pars-
ing, in which the meaning representation language
takes its logical constants from a given, fixed on-
tology. Several recent systems have demonstrated
the ability to learn semantic parsers for domains
like the GeoQuery database containing geography
relations, or the ATIS database of air travel infor-
mation. In these settings, existing systems can
produce correct meaning representations with F1
scores approaching 0.9 (Wong and Mooney, 2007;
Kwiatkowski et al., 2011).
These benchmark datasets have supported a di-
verse and influential line of research into semantic
parsing learning algorithms for sophisticated se-
mantic constructions, with continuing advances in
accuracy. However, the focus on these datasets
leads to a natural question — do other natural
datasets have similar syntax and semantics, and if
not, can existing algorithms handle the variability
in syntax and semantics?
In an effort to investigate and improve the
generalization capacity of existing learning algo-
rithms for semantic parsing, we develop a novel,
natural experimental setting, and we test whether
current semantic parsers generalize to the new set-
ting. For our datset, we use descriptions of clin-
ical trials of experimental drugs in the United
States, available from the U.S. National Insti-
tutes of Health1. Much of the text in the de-
scription of these clinical trials can be mapped
neatly onto biomedical ontologies, thus permitting
grounded semantic analysis. Crucially, the dataset
was not designed specifically with semantic pars-
ing or question-answering in mind, and as a re-
sult, it provides a natural source for the variety
and complexity of utterances that humans use in
this domain. As an added benefit, a successful
semantic parser in this domain could yield a va-
riety of useful bioinformatics applications by per-
mitting comparisons between and across clinical
trials using structured representations of the data,
rather than unstructured text.
In this initial investigation of semantic parsing
in this context, we ask:
</bodyText>
<listItem confidence="0.997247625">
• Can existing semantic parsing learning al-
gorithms handle the variety and complexity
of the clinical trials dataset? We show that
two representative learning algorithms fare
poorly on the clinical trials data: the best one
achieves a 0.41 F1 in our tests.
• What types of constructions are the major
cause of errors on the clinical trials dataset,
</listItem>
<page confidence="0.6434035">
1clinicaltrials.gov
348
</page>
<note confidence="0.993541">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 348–357,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.997020380952381">
and can semantic parsers be extended to han-
dle them? While this initial investigation
does not cover all types of constructions, we
identify three important types of construc-
tions that existing learning algorithms do not
handle. We propose a new learning algorithm
that can handle these types of constructions,
and we demonstrate empirically that the new
algorithm produces a semantic parser that im-
proves by over 23 points in F1 on the clinical
trials dataset compared with existing parsers.
The rest of this paper is organized as follows.
The next section provides background information
on CCG and semantic parsing. Section 3 describes
the text and ontology that form the new clinical
trials dataset for semantic parsing, as well as some
of the problems that exising approaches have on
this dataset. Sections 4 describes our semantic
parsing model, and learning and inference algo-
rithms. Section 5 presents our experiments and re-
sults, and Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.9508425" genericHeader="introduction">
2 Background on Semantic Parsing with
CCG
</sectionHeader>
<bodyText confidence="0.998731625">
Our approach to learning a semantic parser
falls into the general framework of context-free
Probabilistic Combinatory Categorial Grammars
(PCCG) (Zettlemoyer and Collins, 2005) with
typed lambda calculus expressions for the seman-
tics. PCCG grammars involve lexical entries,
which are weighted unary rewrite rules of the form
Syntax: Semantics —* Phrase. For example:
</bodyText>
<equation confidence="0.860314">
Example Lexical Entries
NP : melanoma —* skin cancer
S\NP : ApAd.has condition(p, d) —*
patients with
</equation>
<bodyText confidence="0.999970948275862">
In addition to lexical rules, PCCG grammars in-
volve weighted binary rewrite rules like the fol-
lowing:
These rules apply for any syntactic categories X
and Y , and any logical forms f and g. The rules
specify mechanisms for deducing syntactic cate-
gories for whole phrases based on their constituent
parts. They also specify mechanisms for identify-
ing semantics (logical forms) for phrases and sen-
tences based on combinations of the semantics for
the constituent parts. Besides function application,
other ways to combine the semantics of children
typically include conjunction, disjunction, func-
tion composition, and substitution, among others.
Inference algorithms for PCCG can identify the
best parse and logical form for a given sentence us-
ing standard dynamic programming algorithms for
context-free grammars (Clark and Curran, 2007).
As a baseline in our experiments, we use a
learning algorithm for semantic parsing known as
Unification Based Learning (UBL) (Kwiatkowski
et al., 2010). Source code for UBL is freely
available. Its authors found that the semantic
parsers it learns achieve results competitive with
the state-of-the-art on a variety of standard se-
mantic parsing data sets, including GeoQuery
(0.882 F1). UBL uses a log-linear probabilis-
tic model P(L, T S) over logical forms L and
parse tree derivations T, given sentences S. Dur-
ing training, only S and L are observed, and
UBL’s gradient-based parameter estimation algo-
rithm tries to maximize ET P(L, T S) over the
training dataset. To learn lexicon entries, it adopts
a search procedure that involves unification in
higher-order logic. The objective of the search
procedure is to identify lexical entries for the
words in a sentence that, when combined with the
lexical entries for other words in the sentence, will
produce the observed logical form in the training
data. For each training sentence, UBL heuristi-
cally explores the space of all possible lexical en-
tries to produce a set of promising candidates, and
adds them to the lexicon.
Our second baseline is an extension of this
work, called Factored Unification Based Learning
(FUBL) (Kwiatkowski et al., 2011). Again, source
code is freely available. FUBL factors the lexicon
into a set of base lexical entries, and a set of tem-
plates that can construct more complex lexical en-
tries from the base entries. This allows for a signif-
icantly more compact lexicon, as well as the abil-
ity to handle certain linguistic constructions, like
ellipsis, that appear frequently in the ATIS dataset
and which UBL struggles with. FUBL achieves an
F1 of 0.82 on ATIS (compared with 66.3 for UBL),
and an F1 of 0.886 on GeoQuery; both results are
at or very near the best-reported results for those
datasets.
</bodyText>
<subsectionHeader confidence="0.923113">
2.1 Previous Work
</subsectionHeader>
<bodyText confidence="0.9996524">
Many supervised learning frameworks have been
applied to the task of learning a semantic parser,
including inductive logic programming (Zelle and
Mooney, 1996; Thompson and Mooney, 1999;
Thompson and Mooney, 2003), support vec-
</bodyText>
<equation confidence="0.972581333333333">
Example CCG Grammar Rules
X : f(g) —* X/Y : f Y : g (function application)
X : f(g) —* Y : g X\Y : f (backward application)
</equation>
<page confidence="0.989518">
349
</page>
<bodyText confidence="0.999951880952381">
tor machine-based kernel approaches (Kate et
al., 2005; Kate and Mooney, 2006; Kate and
Mooney, 2007), machine translation-style syn-
chronous grammars (Wong and Mooney, 2007),
and context-free grammar-based approaches like
probabilistic Combinatory Categorial Grammar
(Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007; Zettlemoyer and Collins, 2009;
Kwiatkowski et al., 2010; Kwiatkowski et al.,
2011; Lu et al., 2008) and discriminative reranking
(Ge and Mooney, 2006; Ge and Mooney, 2009).
These approaches have yielded steady improve-
ments on standard test sets like GeoQuery. As far
as we are aware, such systems have not been tested
on domains besides ATIS and GeoQuery.
Because of the complexity involved in build-
ing a training dataset for a supervised semantic
parser, there has been a recent push towards de-
veloping techniques which reduce the annotation
cost or the data complexity of the models. Mod-
els have been developed which can handle some
ambiguity in terms of which logical form is the
correct label for each training sentence (Chen et
al., 2010; Liang et al., 2009). Another set of ap-
proaches have investigated the case where no log-
ical forms are provided, but instead some form of
feedback or response from the world is used as ev-
idence for what the correct logical form must have
been (Clarke et al., 2010; Liang et al., 2011; Artzi
and Zettlemoyer, 2011). Several projects have in-
vestigated unsupervised (Goldwasser et al., 2011;
Poon, 2013; Krishnamurthy and Mitchell, 2012)
and semi-supervised (Yahya et al., 2012; Cai and
Yates, 2013) approaches. These techniques tend
to handle either the same benchmark domains, or
simpler questions over larger ontologies. While
such techniques are important, their (unlabeled
and labeled) sample complexity is higher than it
could be, because the underlying grammars in-
volved are not as general as they could be. Our
work investigates techniques that will reduce this
sample complexity.
</bodyText>
<sectionHeader confidence="0.975303" genericHeader="method">
3 The Clinical Trials Dataset
</sectionHeader>
<bodyText confidence="0.999552666666667">
Clinical trials are scientific experiments that mea-
sure the effects of a medical procedure, instru-
ment, or product on humans. Since September
2009 in the United States, any clinical trial that
is funded by the federal government must make
its results publicly available online at clinicaltri-
als.gov. This site provides a wealth of biomedical
text and structured data, which we use to produce
a novel test set for semantic parsing.
</bodyText>
<subsectionHeader confidence="0.999752">
3.1 The text and ontology
</subsectionHeader>
<bodyText confidence="0.999993978723404">
We collected our utterances from a set of 47 ran-
dom documents from clinicaltrials.gov. Many as-
pects of each study are reported in structured for-
mat; for example, the number of participants who
were given a placebo and the number of partici-
pants who were given the intervention under con-
sideration are both reported in a table in a stan-
dard format. However, certain crucial aspects of
each study are reported only in text. Perhaps the
most critical aspect of each study that is described
only in text is the set of criteria for deciding who
will be admitted to the study and who cannot be;
these criteria are called inclusion criteria and ex-
clusion criteria. We focus our semantic parsing
tests on these criteria because they often form the
longest portion of unstructured text for a given
clinical trial report; because their meaning can be
represented using a concise set of logical constants
from a biomedical ontology; and because the cri-
teria have a great deal of significance in the clin-
ical trials domain. For example, these criteria are
crucial for understanding why the results of two
related studies about the same intervention might
differ.
The criteria for a study can be logically repre-
sented as a function of candidate test subjects that
returns true if they match the study criteria, and
false otherwise. We use a variant of lambda calcu-
lus over a typed ontology to represent each inclu-
sion and exclusion criterion in our dataset. We ran-
domly collected 803 utterances and manually la-
beled each using our representation language. 401
were used for training, 109 for development, and
293 for our final tests.
To keep our semantic parsing study simple, we
eschewed existing ontologies like UMLS (Boden-
reider, 2004) that are large and overly-complex for
this problem. We instead developed an ontology
of 10 types, 38 relations and functions, and a dic-
tionary of 591 named-entities to build the logical
forms. The five most common types and relations
in our dataset are listed in Table 1. On average,
the logical forms in our dataset involved 3.7 rela-
tions per logical form, typically joined with con-
junction, implication, or disjunction. If accepted,
both the full ontology and dataset will be made
publicly available.
</bodyText>
<subsectionHeader confidence="0.9797955">
3.2 Problems with semantic parsing the
clinical trials data
</subsectionHeader>
<bodyText confidence="0.999909666666667">
We applied two state-of-the-art learning algo-
rithms for learning PCCG semantic parsers —
UBL and its extension, FUBL— to our training
</bodyText>
<page confidence="0.994422">
350
</page>
<tableCaption confidence="0.8479515">
Example Types Example Functions
Table 1: Common types and functions in our on-
</tableCaption>
<bodyText confidence="0.94474012244898">
tology. In the example functions, t indicates
boolean type, i indicates real values, p indicates
person, d disease, and so on.
patients with acute lymphoma
Ap . has condition(p, acute(lymphoma))
hypertension
(i.e., include patients with hypertension)
Ap . has condition(p, hypertension)
AST &gt; 3 mg
(i.e., include patients with a level of the AST en-
zyme in the blood of greater than 3 mg)
Ap . &gt; (result(p,AST),unit(3,mg))
Table 2: Example utterances from the clinical tri-
als dataset, and their logical forms. Paraphrases in
parentheses do not appear in the actual data.
data and tested the resulting parsers on develop-
ment data. Results indicate that both systems have
difficulty with the clinical trials datasets: FUBL
achieves an F1 of 0.413, and UBL of just 0.281.
To help understand why state-of-the-art sys-
tems’ performance differs so much from perfor-
mance on benchmark datasets like GeoQuery, we
performed an error analysis. Table 3 describes the
most common errors we observed. The most com-
mon errors occurred on sentences containing co-
ordination constructions, nested function applica-
tions, and for UBL, ellipsis, although a long tail
of less common errors exists. FUBL manages to
handle the elliptical constructions in Clinical Tri-
als well, but not coordination or nested functions.
Both systems tend to learn many, overly-specific
lexical entries that include too much of the logical
form in one lexical entry. For instance, from the
coordination example in Table 3, UBL learns a lex-
ical entry for the word “or” that includes the log-
ical form ApAd1Ad2 . or(has condition(p, d1),
has condition(p, d2)). While this entry works
well when coordinating two diseases or conditions
that patients must have, it will not work for coor-
dinations between treatments or dates, or coordi-
nations between diseases that patients should not
have. UBL learns over 250 lexical entries for the
word “or” from our training dataset of 401 sen-
tences, each one with limited applicability to de-
velopment sentences.
Based on these observed error types, we next
develop novel learning procedures that properly
handle coordination, nested function construc-
tions, and ellipsis.
</bodyText>
<sectionHeader confidence="0.946716" genericHeader="method">
4 Learning to Handle Complex
Constructions in Clinical Trials Data
</sectionHeader>
<subsectionHeader confidence="0.99977">
4.1 Model and Inference
</subsectionHeader>
<bodyText confidence="0.999943631578947">
We introduce the GLL system for learning a se-
mantic parser that generalizes to both GeoQuery
and Clinical Trials data. The semantic parsing
model involves a grammar that consists of a fixed
set of binary CCG rewrite rules, a learned lexicon
A, and a new set T of learned templates for con-
structing unary type-raising rules. We call these
templates for type-raising rules T-rules; these are
described below in Section 4.4.
Following Kwiatkowsi et al.(2010), we as-
sign a probability P(L, T |S, B, G) to a logical
form and parse tree for a sentence licensed by
grammar G using a log-linear model with pa-
rameters B. We use a set of feature functions
F�(L,T,S) = (fi(L,T,S),..., fK(L,T,S)),
where each fi counts the number of times that the
ith grammar rule is used in the derivation of T
and S. The probability of a particular logical form
given a sentence and θ� is given by:
</bodyText>
<equation confidence="0.996459333333333">
P(L|S, B, G) = Y-T exp( T
exp(B · F�(L&apos;, T&apos;, S))
(1)
</equation>
<bodyText confidence="0.9998688">
where the trees T (and T&apos;) are restricted to those
that are licensed by G and which produce L (L’) as
the logical form for the parent node of the tree. In-
ference is performed using standard dynamic pro-
gramming algorithms for context-free parsing.
</bodyText>
<subsectionHeader confidence="0.990897">
4.2 Learning
</subsectionHeader>
<bodyText confidence="0.999842428571429">
The input for the task of learning a semantic parser
is a set of sentences 5, where each Si E S� has
been labeled with a logical form L(Si). We as-
sume a fixed set of binary grammar productions,
and use the training data to learn lexical entries,
T-rules, and parameters. The training objective is
to maximize the likelihood of the observed logical
</bodyText>
<figure confidence="0.9872575">
(p)erson t has condition(p, d)
(d)isease t complication(d, d)
(t)est i result(p, t)
(tr)eatment t treated with(p, tr, date)
(bo)dy-part t located(d, bo)
B · F�(L, T, S))
</figure>
<page confidence="0.967304">
351
</page>
<table confidence="0.940205181818182">
Error Type Freq. Example Description
Nested Funcs. 27% patients &gt; 18 years of age Many logical forms involve functions as
λp . &gt; (result(age, p), unit(18, year)) arguments to other functions or relations.
Ellipsis 26% diabetes Many examples in the inclusion (exclu-
λp . has condition(p, diabetes) sion) criteria simply list a disease or treat-
ment, with the understanding that a patient
p should be included (excluded) if p has
the disease or is undergoing the treatment.
Coordination 16% patient is pregnant or lactating Clinical trials data has more coordina-
λp . or(has condition(p, pregnant), tion, especially noun phrase and adjective
has condition(p, lactating)) phrase coordination, than GeoQuery.
</table>
<tableCaption confidence="0.979610333333333">
Table 3: Three common kinds of utterances in the clinical trials development set that caused UBL and
FUBL to make errors. Frequency indicates the percentage of all development examples that exhibited
that type of construction.
</tableCaption>
<bodyText confidence="0.973668333333333">
Input: set of labeled sentences {(Si, L(Si))}, ini-
tial grammar G0, number of iterations MAX,
learning rate α
</bodyText>
<equation confidence="0.8728899">
A ← ∅
∀i : A ← A ∪ {S : L(Si) → Si}
G ← G0 ∪ A
θ~← 0~
For iteration := 1 to MAX:
TR ← TRLEARN(G)
Add dimension θt to θ~ for t ∈ TR − G
G ← G ∪ TR
For each sentence Si :
A ← LEXENTLEARN(Si, L(Si), G)
</equation>
<bodyText confidence="0.537459">
Add dimension θλ to θ~ for all λ ∈ A−G
</bodyText>
<equation confidence="0.843736">
G ← G ∪ A
θ~← θ~+ α∇iCLL
Return G, θ~
</equation>
<figureCaption confidence="0.999915666666667">
Figure 1: The GLL Learning Algorithm. ∇iCLL
indicates the local gradient of the conditional log
likelihood at sentence Si.
</figureCaption>
<bodyText confidence="0.989326">
forms, or to find G∗ and ~θ∗ such that:
</bodyText>
<equation confidence="0.901511666666667">
G∗, ~θ∗ = arg max
G,~θ rl P(L(Si)|Si, ~θ, G)
i
</equation>
<bodyText confidence="0.999528">
This is a non-convex optimization problem. We
use a greedy optimization procedure that iter-
atively updates G and ~θ. Figure 1 shows an
overview of the full algorithm.
We use stochastic gradient updates to estimate
parameters (LeCun et al., 1998). For each exam-
ple sentence Si in training, we compute the local
gradient of the conditional log likelihood function
CLL = log P(L(Si)|Si, ~θ, G), and update θ~ by
a step in the direction of this local gradient. The
partial derivatives for this local gradient are:
</bodyText>
<equation confidence="0.927775333333333">
∂CLL = ~ (L (Sz )�7� SZ )−
∂θj EP(T|L(Si),Si,θ,G) fj
EP(T |Si,~θ,G) fj(L(Si), T, Si)
</equation>
<subsectionHeader confidence="0.999021">
4.3 Learning Lexical Entries with Inverse
Function Composition
</subsectionHeader>
<bodyText confidence="0.99998584">
We adopt a greedy approach to learning new lex-
ical entries. We first identify in our current parse
any high-scoring lexical entries that cover multiple
words, and then look for new lexical rules for the
sub-phrases covered by these lexical entries that
could combine to create the current parse chart en-
try using the existing grammar rules. This requires
searching through the grammar rules to find chil-
dren nodes that the nonterminal could be the par-
ent of. In general, this produces an intractably
large set, because it requires taking the inverse
of function application and function composition
for forming the semantics of the nonterminal, and
those inverses are intractably large.
Figure 2 shows our algorithm for learning lex-
ical entries, and Figure 3 shows the details of the
critical component that generates the semantics of
new potential lexical entries. For brevity, we omit
the details of how we learn the syntax and map-
pings from semantics to words or phrases for new
lexical entries, but these are borrowed from the ex-
isting techniques in UBL. The crucial difference
from existing techniques is that the SPLITLEARN
algorithm focuses on inverse function composi-
tion, while existing techniques focus on inverse
</bodyText>
<page confidence="0.995746">
352
</page>
<bodyText confidence="0.864001">
Input: training sentence Sent, its logical form L,
current grammar G
</bodyText>
<equation confidence="0.844157272727273">
Initialize:
PC +— parse chart from parsing Sent with G
splits +— 0
For len := length(Sent) to 1:
For pos := 0 to length(Sent) — len:
e = argmaxentryEPC[len][pos] entry.score
if e’s only derivation is a lexical rule in G:
(score, A) +— SPLITLEARN(e, PC)
splits +— splits U {(score, A)}
split* +— argmaxsplitEsplits split.score
Return split*.A
</equation>
<figureCaption confidence="0.9628435">
Figure 2: LEXENTLEARN Algorithm for learning
lexical entries
</figureCaption>
<bodyText confidence="0.995989882352941">
function application. While a priori both tech-
niques are reasonable choices (and both work well
on GeoQuery), our empirical results show that in-
verse function composition can learn the same se-
mantic forms as inverse function application, but
in addition can handle nested functions (which
are function compositions) and coordination — a
form of function composition if one views logical
connectives like or as boolean functions.
The SPLITLEARN algorithm uses a GET-
SUBEXPR subroutine to heuristically select only
certain subexpressions of the input logical form
for computing inverse composition. This is to
avoid a combinatorial explosion in the number
of learned splits of the input semantics. Mostly
we consider any subexpression that forms an ar-
gument to some function in le.sem, but we take
care to also include abstracted versions of these
subexpressions, in which some of their arguments
are in turn replaced by variables. The subrou-
tine FREEVARS identifies all variables in a logical
form that have no quantifier; REPEATVARS iden-
tifies all variables that appear at least twice. PC-
SCORE looks for any entry in the parse chart that
has a matching semantics and returns the score of
that entry, or 0 if no matches are found. We use
PCSCORE to measure the improvement (delta) in
the score of the parse if it uses the two new lexical
entries, rather than the previous single lexical en-
try. SPLITLEARN returns the set of lexical entries
that tie for the largest improvement in the score of
the parse.
Figures 4 and 5 illustrate the difference be-
Input: lexical entry le, parse chart PC
</bodyText>
<equation confidence="0.8386238">
Entries +— 0
For s EGETSUBEXPR(le.sem):
t +— copy of s
sem&apos; +— copy of le.sem
Apply[t] +— 0
</equation>
<figure confidence="0.919707625">
For v E FREEVARS(s)n REPEATVARS(sem&apos;):
Create variable v&apos;, t +— tv&apos; sub for v
Concatenate “Av&apos;” onto front of t
Apply[t] +— Apply[t] U {v}
For v E FREEVARS(t):
Remove “Av” from front of sem&apos;
Concatenate “Av” onto front of t
Create new variable w
sub +— “(w” + each a E Apply[t] + “)”
sem&apos; +— sem&apos;sub sub for s
Concatenate “Aw” onto front of sem&apos;
Entries +— Entries U It, sem&apos;}
delta[t], delta[sem&apos;] +— PCSCORE(t) +
PCSCORE(sem&apos;) - PCSCORE(le)
max +— maxx delta[x]
Return max, Is E Entries I delta[s] = max}
</figure>
<figureCaption confidence="0.995137">
Figure 3: SPLITLEARN Algorithm for generating
(the semantics of) new lexical entries.
</figureCaption>
<bodyText confidence="0.9995944">
tween SPLITLEARN and lexical entry learning for
UBL and FUBL. For both example sentences,
there is a point in the learning process where
a logical form must be split using inverse func-
tion composition in order for useful lexical en-
tries to be learned. At those points, UBL and
FUBL split the logical forms using inverse func-
tion application, resulting in splits where the se-
mantics of different lexemes are mixed together
in the two resulting subexpressions. In Figure 4,
all three systems take the logical form Au.Ap. &gt;
(result(p, bilirubin), unit(1.5, u)) and split
it by removing some aspect of the final argu-
ment, unit(1.5, u), from the full expression. In
UBL and FUBL, the term that is left behind in
the full expression is something that unifies with
Au.unit(1.5, u). In GLL, however, only a vari-
able is left behind, since that variable can be re-
placed by Au. unit(1.5, u) through function com-
position to obtain the original expression. Thus
GLL’s split yields one significantly simpler subex-
pression, which in the end yields simpler lexical
entries. In both figures, and in general for most
parses we have observed, inverse function compo-
sition yields simpler and cleaner subexpressions.
</bodyText>
<page confidence="0.995909">
353
</page>
<figure confidence="0.999744757575758">
λp.&gt;(result(p,bilirubin),unit(1.5,mg/dl))
λu.λp.&gt;(result(p,bilirubin),unit(1.5,u))
F: u → i
G: p → i
1.5:i
mg/dl:u
λu.unit(1.5,u)
λi.λu.λp.&gt;(result(p,bilirubin),unit(i,u))
λF.λu.λp.&gt;(result(p,bilirubin),F(u))
λi.λp.&gt;(result(p,bilirubin),i)
λp.result(p,bilirubin)
UBL
FUBL
GLL
λq.λi.λu.λp.&gt;(result(p,q),unit(i,u))
λG.λF.λu.λp.&gt;(G(p),F(u))
λi&apos;.λi.&gt;(i&apos;,i)
bilirubin:q
total bilirubin &gt; 1.5 mg/dL
λp.or(has condition(p,pregnant), has condition(p,lactating))
λd.λp.has condition(p,d)
λd&apos;.λF.λd.λp.or(has condition(p,d&apos;),F(p,d))
λF.λd.λp.or(has condition(p,pregnant),F(p,d))
F: (p,d) → t
G: d → t
λG.λd.or(G(pregnant),G(d))
pregnant:d lactating:d
λd.λp.has condition(p,d)
λd&apos;.λG.λd.or(G(d&apos;),G(d))
patient is pregnant or lactating
λd.λp.or(has condition(p,pregnant), has condition(p,d))
(F)UBL
GLL
</figure>
<figureCaption confidence="0.864292857142857">
Figure 4: An example of a sentence with nested-
function semantics. GLL’s lexical entry learning
procedure correctly identifies the most general se-
mantics for the lexeme &gt;, while UBL and FUBL
learn more specific and complex semantics.
Figure 5: An example of a sentence with coor-
dination semantics. GLL’s lexical entry learning
</figureCaption>
<bodyText confidence="0.807487666666667">
procedure correctly identifies the semantics for the
lexeme or, while UBL and FUBL learn incorrect
semantics.
</bodyText>
<subsectionHeader confidence="0.999275">
4.4 Learning T-rules
</subsectionHeader>
<bodyText confidence="0.994869589285715">
We use T-rules to handle elliptical constructions.
They are essentially a simplification of the fac-
tored lexicon used in FUBL that yields very sim-
ilar results. Each T-rule T ∈ T is a function of the
form Ae . if type(e) then return Syn: f(e) →
Syn&apos; : e, where type is a type from our ontology,
Syn and Syn&apos; are two syntactic CCG categories
or variables, and f is an arbitrary lambda calcu-
lus expression. For example, consider the T-rule
T = (Ae . if disease(e) then return S\N :
Ap . has condition(p, e) → N : e).
When applied to the entity diabetes, this T-
rule results in an ordinary CCG rule: S\N :
Ap . has condition(p, diabetes) → N :
diabetes. Thus each T-rule is a template for con-
structing unary (type-raising) CCG grammar rules
from an entity of the appropriate type.
TRLEARN works by first identifying a set
of entity symbols E that appear in multiple
lexical entries in the input grammar G. Let
the lexical entries for entity e ∈ E be denoted
by A(e); thus, E consists of all entities where
|A(e) |≥ 2. TRLEARN then looks for patterns
in each of these sets of lexical entries. If one
of the lexical entries in A(e) has a semantics
that consists of just e (for example, the lexical
entry N : diabetes → diabetes), we create
candidate T-rules from every other lexical entry
l&apos; ∈ A(e) that has the same child, such as
S\N : Ap . has condition(p, diabetes) →
diabetes. From this lexical entry,
we create the candidate T-rule T =
(Ax . if disease(x) then return S\N :
Ap . has condition(p, x) → N : x). In general,
the test in the if statement in the T-rule contains
a check for the type of entity e. The right-hand
side of the implication contains a unary grammar
rule whose parent matches the parent of the rule
in l&apos;, except that entity e has been replaced by a
variable x. The child of the grammar rule matches
the parent of the basic lexical entry N : e, except
again that the entity e has been replaced by the
variable x.
Having constructed a set of candidate T-rules
from this process, TRLEARN must select the ones
that will actually be added to the grammar. We
use a test of selecting T-rules that cover at least
MIN existing grammar rules in the input gram-
mar G. In our implementation, we set MIN = 2.
When parsing a sentence, the parser checks any
parse chart entry for semantics that consist solely
of an entity; for any such entry, it looks in a hash-
based index for applicable T-rules, applies them to
the entity to construct new unary grammar rules,
and then applies the unary grammar rules to the
parse chart entry to create new nonterminal nodes.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999956666666667">
In our experiments, we test the generality of our
learning algorithm by testing its ability to handle
both GeoQuery and the Clinical Trials datasets.
</bodyText>
<subsectionHeader confidence="0.989237">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9902825">
The clinical trials dataset is described above in
Section 3. GeoQuery consists of a database of
</bodyText>
<page confidence="0.996181">
354
</page>
<table confidence="0.99969525">
System Precision Recall F1
UBL 87.9 88.5 88.2
FUBL 88.6 88.6 88.6
GLL 84.6 86.1 85.5
</table>
<tableCaption confidence="0.769724666666667">
Table 4: GLL performs comparably to two state-
of-the-art learning algorithms for PCCG semantic
parsing on the benchmark GeoQuery dataset.
</tableCaption>
<table confidence="0.999801">
System Precision Recall F1
UBL 20.3 19.9 20.1
FUBL 42.3 39.7 40.8
GLL 65.3 63.2 64.1
</table>
<tableCaption confidence="0.902239">
Table 5: On the clinical trials dataset, GLL outper-
</tableCaption>
<bodyText confidence="0.983411388888889">
forms UBL and FUBL by more than 23 points in
F1, for a reduction in error (i.e., 1-F1) of nearly
40% over FUBL.
2400 geographical entities, such as nations, rivers,
and mountains, as well as 8 geography relations,
such as the location of a mountain, and whether
one state borders another. The text for semantic
parsing consists of a set of 880 geography ques-
tions, labeled with a lambda-calculus representa-
tion of the sentence’s meaning. We follow the pro-
cedure described by Kwiatkowski et al.. (2010)
in splitting these sentences into training, develop-
ment, and test sentences. This dataset allows us to
provide a comparison with other semantic parsers
on a well-known dataset. We measured perfor-
mance based on exact-match of the full logical
form, modulo re-ordering of arguments to sym-
metric relations (like conjunction and disjunction).
</bodyText>
<subsectionHeader confidence="0.880307">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9998576875">
Tables 4 and 5 show the results of semantic parsers
learned by the UBL, FUBL, and GLL learning
algorithms on the GeoQuery and clinical trials
datasets, respectively. On the GeoQuery dataset,
all three parsers perform very similarly, although
GLL’s performance is slightly worse. However, on
the clinical trials dataset, GLL significantly out-
performs both UBL and FUBL in terms of preci-
sion, recall, and F1. Of course, there clearly re-
main many syntactic and semantic constructions
that none of these algorithms can currently han-
dle, as all systems perform significantly worse on
clinical trials than on GeoQuery.
Tables 6 shows the overall size of UBL’s and
GLL’s learned lexicons, and Table 7 shows the
number of learned entries for selected lexical
</bodyText>
<table confidence="0.888881">
Lexicon Size
System GeoQuery Clinical Trials
UBL 5,149 49,635
GLL 4,528 36,112
</table>
<tableCaption confidence="0.991666">
Table 6: GLL learns a lexicon that is 27% smaller
than UBL’s lexicon on clinical trials data.
</tableCaption>
<table confidence="0.965853166666667">
Lexeme UBL meanings GLL meanings
&gt; 36 2
&lt; 28 2
= 35 2
and 6 4
or 254 9
</table>
<tableCaption confidence="0.984221">
Table 7: For certain common and critical lexical
</tableCaption>
<bodyText confidence="0.98055">
items in the clinical trials dataset, GLL learns far
fewer (but more general) lexical entries; for the
word “or”, GLL learns only 3.5% of the entries
that UBL learns.
items that appear frequently in the clinical trials
corpus. FUBL uses a factored lexicon in which
the semantics of a logical form is split across two
data structures. As a result, FUBL’s lexicon is not
directly comparable to the other systems, so for
these comparisons we restrict our attention to UBL
and GLL. UBL tends to learn far more lexical en-
tries than GLL, particularly for words that appear
in multiple sentences. Yet the poorer performance
of UBL on clinical trials is an indication that these
lexical entries are overly specific.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977133333333">
We have introduced the clinical trials dataset,
a naturally-occurring set of text where existing
learning algorithms for semantic parsing struggle.
Our new GLL algorithm uses a novel inverse func-
tion composition algorithm to handle coordina-
tion and nested function constructions, and pattern
learning to handle elliptical constructions. These
innovations allow GLL to handle GeoQuery and
improve on clinical trials. Many sources of er-
ror on clinical trials remain for future research,
including long-distance dependencies, attachment
ambiguities, and coreference. In addition, further
investigation is necessary to test how these algo-
rithms handle additional domains and other types
of natural linguistic constructions.
</bodyText>
<page confidence="0.998392">
355
</page>
<sectionHeader confidence="0.998182" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997775">
This work was supported by National Science
Foundation grant 1218692. The authors appreciate
the help that Anjan Nepal, Qingqing Cai, Avirup
Sil, and Fei Huang provided.
</bodyText>
<sectionHeader confidence="0.99757" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999288198019802">
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping Semantic Parsers from Conversations. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research, 32:D267–D270.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexi-
con Extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a Multilingual
Sportscaster: Using Perceptual Context to Learn
Language. Journal of Artificial Intelligence Re-
search, 37:397–435.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and
log-linear models. Computational Linguistics,
33(4):493–552.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL).
Ruifang Ge and Raymond J. Mooney. 2006. Discrim-
inative Reranking for Semantic Parsing. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING/ACL-06).
Ruifang Ge and Raymond J. Mooney. 2009. Learning
a Compositional Semantic Parser using an Existing
Syntactic Parser. In Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP 2009).
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic
parsing. In Association for Computational Linguis-
tics (ACL).
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing String-Kernels for Learning Semantic Parsers.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the ACL.
Rohit J. Kate and Raymond J. Mooney. 2007. Semi-
Supervised Learning for Semantic Parsing using
Support Vector Machines. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, Short Papers (NAACL/HLT-
2007).
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to Transform Natural to
Formal Languages. In Proceedings of the Twen-
tieth National Conference on Artificial Intelligence
(AAAI-05).
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly Supervised Training of Semantic Parsers. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing Probabilis-
tic CCG Grammars from Logical Form with Higher-
order Unification. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical Gen-
eralization in CCG Grammar Induction for Seman-
tic Parsing. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recog-
nition. In Proceedings of the IEEE.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and In-
ternational Joint Conference on Natural Language
Processing (ACL-IJCNLP).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A Generative Model for Parsing
Natural Language to Meaning Representations. In
Proceedings of The Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Hoifung Poon. 2013. Grounded Unsupervised Se-
mantic Parsing. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
C.A. Thompson and R.J. Mooney. 1999. Automatic
construction of semantic lexicons for learning natu-
ral language interfaces. In Proc. 16th National Con-
ference on Artificial Intelligence (AAAI-99), pages
487–493.
</reference>
<page confidence="0.986644">
356
</page>
<reference confidence="0.999605027777778">
Cynthia A. Thompson and Raymond J. Mooney. 2003.
Acquiring Word-Meaning Mappings for Natural
Language Interfaces. Journal of Artificial Intelli-
gence Research (JAIR), 18:1–44.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning Synchronous Grammars for Semantic
Parsing with Lambda Calculus. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2007).
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural Language Questions for the
Web of Data. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to Parse Database Queries using Inductive Logic
Programming. In AAAI/IAAI, pages 1050–1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to Map Sentences to Logical Form: Struc-
tured Classification with Probabilistic Categorial
Grammars. In Proceedings of the Twenty First
Conference on Uncertainty in Artificial Intelligence
(UAI).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line Learning of Relaxed CCG Grammars for Pars-
ing to Logical Form. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
Luke S. Zettlemoyer and Michael Collins. 2009.
Learning Context-dependent Mappings from Sen-
tences to Logical Form. In Proceedings of the Joint
Conference of the Association for Computational
Linguistics and International Joint Conference on
Natural Language Processing (ACL-IJCNLP).
</reference>
<page confidence="0.998191">
357
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.215399">
<title confidence="0.9985605">Empirically-motivated Generalizations of CCG Semantic Learning Algorithms</title>
<author confidence="0.971784">Jesse</author>
<affiliation confidence="0.448054">Temple</affiliation>
<address confidence="0.931715">1801 N Broad Philadelphia, PA 19122,</address>
<email confidence="0.99817">jglassemc2@gmail.com</email>
<author confidence="0.885515">Alexander</author>
<affiliation confidence="0.615807">Temple</affiliation>
<address confidence="0.9650805">1801 N Broad Philadelphia, PA 19122,</address>
<email confidence="0.999536">ayates@gmail.com</email>
<abstract confidence="0.995236789473684">Learning algorithms for semantic parsing have improved drastically over the past decade, as steady improvements on benchmark datasets have shown. In this paper we investigate whether they can generalize to a novel biomedical dataset that differs in important respects from the traditional geography and air travel benchmark datasets. Empirical results for two state-of-the-art PCCG semantic parsers indicates that learning algorithms are sensitive to the kinds of semantic and syntactic constructions used in a domain. In response, we develop a novel learning algorithm that can produce an effective semantic parser for geography, as well as a much better semantic parser for the biomedical dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Bootstrapping Semantic Parsers from Conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="9741" citStr="Artzi and Zettlemoyer, 2011" startWordPosition="1534" endWordPosition="1537"> supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions over larger ontologies. While such techniques are important, their (unlabeled and labeled) sample complexity is higher than it could be, because the underlying grammars involved are not as general as they could be. Our work investigates techniques that will reduce this sample complexity. 3 The Clinical Trials Datas</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping Semantic Parsers from Conversations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
</authors>
<title>The Unified Medical Language System (UMLS): integrating biomedical terminology.</title>
<date>2004</date>
<journal>Nucleic Acids Research,</journal>
<pages>32--267</pages>
<contexts>
<context position="12526" citStr="Bodenreider, 2004" startWordPosition="1992" endWordPosition="1994">s about the same intervention might differ. The criteria for a study can be logically represented as a function of candidate test subjects that returns true if they match the study criteria, and false otherwise. We use a variant of lambda calculus over a typed ontology to represent each inclusion and exclusion criterion in our dataset. We randomly collected 803 utterances and manually labeled each using our representation language. 401 were used for training, 109 for development, and 293 for our final tests. To keep our semantic parsing study simple, we eschewed existing ontologies like UMLS (Bodenreider, 2004) that are large and overly-complex for this problem. We instead developed an ontology of 10 types, 38 relations and functions, and a dictionary of 591 named-entities to build the logical forms. The five most common types and relations in our dataset are listed in Table 1. On average, the logical forms in our dataset involved 3.7 relations per logical form, typically joined with conjunction, implication, or disjunction. If accepted, both the full ontology and dataset will be made publicly available. 3.2 Problems with semantic parsing the clinical trials data We applied two state-of-the-art lear</context>
</contexts>
<marker>Bodenreider, 2004</marker>
<rawString>Olivier Bodenreider. 2004. The Unified Medical Language System (UMLS): integrating biomedical terminology. Nucleic Acids Research, 32:D267–D270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale Semantic Parsing via Schema Matching and Lexicon Extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="9924" citStr="Cai and Yates, 2013" startWordPosition="1560" endWordPosition="1563"> can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions over larger ontologies. While such techniques are important, their (unlabeled and labeled) sample complexity is higher than it could be, because the underlying grammars involved are not as general as they could be. Our work investigates techniques that will reduce this sample complexity. 3 The Clinical Trials Dataset Clinical trials are scientific experiments that measure the effects of a medical procedure, instrument, or product on humans. Since September 2009 in the United States, any clinica</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale Semantic Parsing via Schema Matching and Lexicon Extension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--397</pages>
<contexts>
<context position="9428" citStr="Chen et al., 2010" startWordPosition="1477" endWordPosition="1480">ing (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions ove</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language. Journal of Artificial Intelligence Research, 37:397–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with ccg and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="6098" citStr="Clark and Curran, 2007" startWordPosition="932" endWordPosition="935">nisms for deducing syntactic categories for whole phrases based on their constituent parts. They also specify mechanisms for identifying semantics (logical forms) for phrases and sentences based on combinations of the semantics for the constituent parts. Besides function application, other ways to combine the semantics of children typically include conjunction, disjunction, function composition, and substitution, among others. Inference algorithms for PCCG can identify the best parse and logical form for a given sentence using standard dynamic programming algorithms for context-free grammars (Clark and Curran, 2007). As a baseline in our experiments, we use a learning algorithm for semantic parsing known as Unification Based Learning (UBL) (Kwiatkowski et al., 2010). Source code for UBL is freely available. Its authors found that the semantic parsers it learns achieve results competitive with the state-of-the-art on a variety of standard semantic parsing data sets, including GeoQuery (0.882 F1). UBL uses a log-linear probabilistic model P(L, T S) over logical forms L and parse tree derivations T, given sentences S. During training, only S and L are observed, and UBL’s gradient-based parameter estimation </context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with ccg and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>M Chang</author>
<author>D Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<journal>In Computational Natural Language Learning (CoNLL).</journal>
<contexts>
<context position="9691" citStr="Clarke et al., 2010" startWordPosition="1526" endWordPosition="1529">lved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions over larger ontologies. While such techniques are important, their (unlabeled and labeled) sample complexity is higher than it could be, because the underlying grammars involved are not as general as they could be. Our work investigates techniques that will reduce t</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010. Driving semantic parsing from the world’s response. In Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>Discriminative Reranking for Semantic Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06).</booktitle>
<contexts>
<context position="8835" citStr="Ge and Mooney, 2006" startWordPosition="1376" endWordPosition="1379"> support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang</context>
</contexts>
<marker>Ge, Mooney, 2006</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2006. Discriminative Reranking for Semantic Parsing. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning a Compositional Semantic Parser using an Existing Syntactic Parser.</title>
<date>2009</date>
<booktitle>In Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACLIJCNLP</booktitle>
<contexts>
<context position="8857" citStr="Ge and Mooney, 2009" startWordPosition="1380" endWordPosition="1383">CG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Anothe</context>
</contexts>
<marker>Ge, Mooney, 2009</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2009. Learning a Compositional Semantic Parser using an Existing Syntactic Parser. In Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACLIJCNLP 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>R Reichart</author>
<author>J Clarke</author>
<author>D Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="9815" citStr="Goldwasser et al., 2011" startWordPosition="1544" endWordPosition="1547">chniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions over larger ontologies. While such techniques are important, their (unlabeled and labeled) sample complexity is higher than it could be, because the underlying grammars involved are not as general as they could be. Our work investigates techniques that will reduce this sample complexity. 3 The Clinical Trials Dataset Clinical trials are scientific experiments that measure the effects of </context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>D. Goldwasser, R. Reichart, J. Clarke, and D. Roth. 2011. Confidence driven unsupervised semantic parsing. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using String-Kernels for Learning Semantic Parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL.</booktitle>
<contexts>
<context position="8433" citStr="Kate and Mooney, 2006" startWordPosition="1323" endWordPosition="1326">achieves an F1 of 0.82 on ATIS (compared with 66.3 for UBL), and an F1 of 0.886 on GeoQuery; both results are at or very near the best-reported results for those datasets. 2.1 Previous Work Many supervised learning frameworks have been applied to the task of learning a semantic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and G</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using String-Kernels for Learning Semantic Parsers. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>SemiSupervised Learning for Semantic Parsing using Support Vector Machines.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Short Papers (NAACL/HLT2007).</booktitle>
<contexts>
<context position="8457" citStr="Kate and Mooney, 2007" startWordPosition="1327" endWordPosition="1330">on ATIS (compared with 66.3 for UBL), and an F1 of 0.886 on GeoQuery; both results are at or very near the best-reported results for those datasets. 2.1 Previous Work Many supervised learning frameworks have been applied to the task of learning a semantic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the </context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2007. SemiSupervised Learning for Semantic Parsing using Support Vector Machines. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Short Papers (NAACL/HLT2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to Transform Natural to Formal Languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI-05).</booktitle>
<contexts>
<context position="8410" citStr="Kate et al., 2005" startWordPosition="1319" endWordPosition="1322">ruggles with. FUBL achieves an F1 of 0.82 on ATIS (compared with 66.3 for UBL), and an F1 of 0.886 on GeoQuery; both results are at or very near the best-reported results for those datasets. 2.1 Previous Work Many supervised learning frameworks have been applied to the task of learning a semantic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on dom</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to Transform Natural to Formal Languages. In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom Mitchell</author>
</authors>
<title>Weakly Supervised Training of Semantic Parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="9862" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="1550" endWordPosition="1553">cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions over larger ontologies. While such techniques are important, their (unlabeled and labeled) sample complexity is higher than it could be, because the underlying grammars involved are not as general as they could be. Our work investigates techniques that will reduce this sample complexity. 3 The Clinical Trials Dataset Clinical trials are scientific experiments that measure the effects of a medical procedure, instrument, or product on </context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom Mitchell. 2012. Weakly Supervised Training of Semantic Parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing Probabilistic CCG Grammars from Logical Form with Higherorder Unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6251" citStr="Kwiatkowski et al., 2010" startWordPosition="956" endWordPosition="959">ical forms) for phrases and sentences based on combinations of the semantics for the constituent parts. Besides function application, other ways to combine the semantics of children typically include conjunction, disjunction, function composition, and substitution, among others. Inference algorithms for PCCG can identify the best parse and logical form for a given sentence using standard dynamic programming algorithms for context-free grammars (Clark and Curran, 2007). As a baseline in our experiments, we use a learning algorithm for semantic parsing known as Unification Based Learning (UBL) (Kwiatkowski et al., 2010). Source code for UBL is freely available. Its authors found that the semantic parsers it learns achieve results competitive with the state-of-the-art on a variety of standard semantic parsing data sets, including GeoQuery (0.882 F1). UBL uses a log-linear probabilistic model P(L, T S) over logical forms L and parse tree derivations T, given sentences S. During training, only S and L are observed, and UBL’s gradient-based parameter estimation algorithm tries to maximize ET P(L, T S) over the training dataset. To learn lexicon entries, it adopts a search procedure that involves unification in h</context>
<context position="8741" citStr="Kwiatkowski et al., 2010" startWordPosition="1361" endWordPosition="1364">e logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms o</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing Probabilistic CCG Grammars from Logical Form with Higherorder Unification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical Generalization in CCG Grammar Induction for Semantic Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1660" citStr="Kwiatkowski et al., 2011" startWordPosition="247" endWordPosition="250"> converting natural language utterances into formal representations of their meaning. In this paper, we consider in particular a grounded form of semantic parsing, in which the meaning representation language takes its logical constants from a given, fixed ontology. Several recent systems have demonstrated the ability to learn semantic parsers for domains like the GeoQuery database containing geography relations, or the ATIS database of air travel information. In these settings, existing systems can produce correct meaning representations with F1 scores approaching 0.9 (Wong and Mooney, 2007; Kwiatkowski et al., 2011). These benchmark datasets have supported a diverse and influential line of research into semantic parsing learning algorithms for sophisticated semantic constructions, with continuing advances in accuracy. However, the focus on these datasets leads to a natural question — do other natural datasets have similar syntax and semantics, and if not, can existing algorithms handle the variability in syntax and semantics? In an effort to investigate and improve the generalization capacity of existing learning algorithms for semantic parsing, we develop a novel, natural experimental setting, and we te</context>
<context position="7401" citStr="Kwiatkowski et al., 2011" startWordPosition="1144" endWordPosition="1147">exicon entries, it adopts a search procedure that involves unification in higher-order logic. The objective of the search procedure is to identify lexical entries for the words in a sentence that, when combined with the lexical entries for other words in the sentence, will produce the observed logical form in the training data. For each training sentence, UBL heuristically explores the space of all possible lexical entries to produce a set of promising candidates, and adds them to the lexicon. Our second baseline is an extension of this work, called Factored Unification Based Learning (FUBL) (Kwiatkowski et al., 2011). Again, source code is freely available. FUBL factors the lexicon into a set of base lexical entries, and a set of templates that can construct more complex lexical entries from the base entries. This allows for a significantly more compact lexicon, as well as the ability to handle certain linguistic constructions, like ellipsis, that appear frequently in the ATIS dataset and which UBL struggles with. FUBL achieves an F1 of 0.82 on ATIS (compared with 66.3 for UBL), and an F1 of 0.886 on GeoQuery; both results are at or very near the best-reported results for those datasets. 2.1 Previous Work</context>
<context position="8767" citStr="Kwiatkowski et al., 2011" startWordPosition="1365" endWordPosition="1368"> and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is th</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical Generalization in CCG Grammar Induction for Semantic Parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y LeCun</author>
<author>L Bottou</author>
<author>Y Bengio</author>
<author>P Haffner</author>
</authors>
<title>Gradient-based learning applied to document recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the IEEE.</booktitle>
<contexts>
<context position="19031" citStr="LeCun et al., 1998" startWordPosition="3099" endWordPosition="3102">t to θ~ for t ∈ TR − G G ← G ∪ TR For each sentence Si : A ← LEXENTLEARN(Si, L(Si), G) Add dimension θλ to θ~ for all λ ∈ A−G G ← G ∪ A θ~← θ~+ α∇iCLL Return G, θ~ Figure 1: The GLL Learning Algorithm. ∇iCLL indicates the local gradient of the conditional log likelihood at sentence Si. forms, or to find G∗ and ~θ∗ such that: G∗, ~θ∗ = arg max G,~θ rl P(L(Si)|Si, ~θ, G) i This is a non-convex optimization problem. We use a greedy optimization procedure that iteratively updates G and ~θ. Figure 1 shows an overview of the full algorithm. We use stochastic gradient updates to estimate parameters (LeCun et al., 1998). For each example sentence Si in training, we compute the local gradient of the conditional log likelihood function CLL = log P(L(Si)|Si, ~θ, G), and update θ~ by a step in the direction of this local gradient. The partial derivatives for this local gradient are: ∂CLL = ~ (L (Sz )�7� SZ )− ∂θj EP(T|L(Si),Si,θ,G) fj EP(T |Si,~θ,G) fj(L(Si), T, Si) 4.3 Learning Lexical Entries with Inverse Function Composition We adopt a greedy approach to learning new lexical entries. We first identify in our current parse any high-scoring lexical entries that cover multiple words, and then look for new lexica</context>
</contexts>
<marker>LeCun, Bottou, Bengio, Haffner, 1998</marker>
<rawString>Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning applied to document recognition. In Proceedings of the IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</booktitle>
<contexts>
<context position="9449" citStr="Liang et al., 2009" startWordPosition="1481" endWordPosition="1484"> 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions over larger ontologies. </context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="9711" citStr="Liang et al., 2011" startWordPosition="1530" endWordPosition="1533">aining dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions over larger ontologies. While such techniques are important, their (unlabeled and labeled) sample complexity is higher than it could be, because the underlying grammars involved are not as general as they could be. Our work investigates techniques that will reduce this sample complexit</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A Generative Model for Parsing Natural Language to Meaning Representations.</title>
<date>2008</date>
<booktitle>In Proceedings of The Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="8785" citStr="Lu et al., 2008" startWordPosition="1369" endWordPosition="1372">n and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label fo</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A Generative Model for Parsing Natural Language to Meaning Representations. In Proceedings of The Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
</authors>
<title>Grounded Unsupervised Semantic Parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="9827" citStr="Poon, 2013" startWordPosition="1548" endWordPosition="1549"> annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions over larger ontologies. While such techniques are important, their (unlabeled and labeled) sample complexity is higher than it could be, because the underlying grammars involved are not as general as they could be. Our work investigates techniques that will reduce this sample complexity. 3 The Clinical Trials Dataset Clinical trials are scientific experiments that measure the effects of a medical pr</context>
</contexts>
<marker>Poon, 2013</marker>
<rawString>Hoifung Poon. 2013. Grounded Unsupervised Semantic Parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Thompson</author>
<author>R J Mooney</author>
</authors>
<title>Automatic construction of semantic lexicons for learning natural language interfaces.</title>
<date>1999</date>
<booktitle>In Proc. 16th National Conference on Artificial Intelligence (AAAI-99),</booktitle>
<pages>487--493</pages>
<contexts>
<context position="8187" citStr="Thompson and Mooney, 1999" startWordPosition="1276" endWordPosition="1279">ical entries from the base entries. This allows for a significantly more compact lexicon, as well as the ability to handle certain linguistic constructions, like ellipsis, that appear frequently in the ATIS dataset and which UBL struggles with. FUBL achieves an F1 of 0.82 on ATIS (compared with 66.3 for UBL), and an F1 of 0.886 on GeoQuery; both results are at or very near the best-reported results for those datasets. 2.1 Previous Work Many supervised learning frameworks have been applied to the task of learning a semantic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) a</context>
</contexts>
<marker>Thompson, Mooney, 1999</marker>
<rawString>C.A. Thompson and R.J. Mooney. 1999. Automatic construction of semantic lexicons for learning natural language interfaces. In Proc. 16th National Conference on Artificial Intelligence (AAAI-99), pages 487–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Raymond J Mooney</author>
</authors>
<title>Acquiring Word-Meaning Mappings for Natural Language Interfaces.</title>
<date>2003</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>18--1</pages>
<contexts>
<context position="8215" citStr="Thompson and Mooney, 2003" startWordPosition="1280" endWordPosition="1283">entries. This allows for a significantly more compact lexicon, as well as the ability to handle certain linguistic constructions, like ellipsis, that appear frequently in the ATIS dataset and which UBL struggles with. FUBL achieves an F1 of 0.82 on ATIS (compared with 66.3 for UBL), and an F1 of 0.886 on GeoQuery; both results are at or very near the best-reported results for those datasets. 2.1 Previous Work Many supervised learning frameworks have been applied to the task of learning a semantic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking </context>
</contexts>
<marker>Thompson, Mooney, 2003</marker>
<rawString>Cynthia A. Thompson and Raymond J. Mooney. 2003. Acquiring Word-Meaning Mappings for Natural Language Interfaces. Journal of Artificial Intelligence Research (JAIR), 18:1–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-2007).</booktitle>
<contexts>
<context position="1633" citStr="Wong and Mooney, 2007" startWordPosition="243" endWordPosition="246"> parsing is the task of converting natural language utterances into formal representations of their meaning. In this paper, we consider in particular a grounded form of semantic parsing, in which the meaning representation language takes its logical constants from a given, fixed ontology. Several recent systems have demonstrated the ability to learn semantic parsers for domains like the GeoQuery database containing geography relations, or the ATIS database of air travel information. In these settings, existing systems can produce correct meaning representations with F1 scores approaching 0.9 (Wong and Mooney, 2007; Kwiatkowski et al., 2011). These benchmark datasets have supported a diverse and influential line of research into semantic parsing learning algorithms for sophisticated semantic constructions, with continuing advances in accuracy. However, the focus on these datasets leads to a natural question — do other natural datasets have similar syntax and semantics, and if not, can existing algorithms handle the variability in syntax and semantics? In an effort to investigate and improve the generalization capacity of existing learning algorithms for semantic parsing, we develop a novel, natural expe</context>
<context position="8529" citStr="Wong and Mooney, 2007" startWordPosition="1336" endWordPosition="1339">th results are at or very near the best-reported results for those datasets. 2.1 Previous Work Many supervised learning frameworks have been applied to the task of learning a semantic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised sema</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2007. Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Klaus Berberich</author>
<author>Shady Elbassuoni</author>
<author>Maya Ramanath</author>
<author>Volker Tresp</author>
<author>Gerhard Weikum</author>
</authors>
<title>Natural Language Questions for the Web of Data.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="9902" citStr="Yahya et al., 2012" startWordPosition="1556" endWordPosition="1559">been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence (Chen et al., 2010; Liang et al., 2009). Another set of approaches have investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011). Several projects have investigated unsupervised (Goldwasser et al., 2011; Poon, 2013; Krishnamurthy and Mitchell, 2012) and semi-supervised (Yahya et al., 2012; Cai and Yates, 2013) approaches. These techniques tend to handle either the same benchmark domains, or simpler questions over larger ontologies. While such techniques are important, their (unlabeled and labeled) sample complexity is higher than it could be, because the underlying grammars involved are not as general as they could be. Our work investigates techniques that will reduce this sample complexity. 3 The Clinical Trials Dataset Clinical trials are scientific experiments that measure the effects of a medical procedure, instrument, or product on humans. Since September 2009 in the Unit</context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Ramanath, Tresp, Weikum, 2012</marker>
<rawString>Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural Language Questions for the Web of Data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to Parse Database Queries using Inductive Logic Programming. In</title>
<date>1996</date>
<booktitle>AAAI/IAAI,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="8160" citStr="Zelle and Mooney, 1996" startWordPosition="1272" endWordPosition="1275">nstruct more complex lexical entries from the base entries. This allows for a significantly more compact lexicon, as well as the ability to handle certain linguistic constructions, like ellipsis, that appear frequently in the ATIS dataset and which UBL struggles with. FUBL achieves an F1 of 0.82 on ATIS (compared with 66.3 for UBL), and an F1 of 0.886 on GeoQuery; both results are at or very near the best-reported results for those datasets. 2.1 Previous Work Many supervised learning frameworks have been applied to the task of learning a semantic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to Parse Database Queries using Inductive Logic Programming. In AAAI/IAAI, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twenty First Conference on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="4969" citStr="Zettlemoyer and Collins, 2005" startWordPosition="758" endWordPosition="761">ction provides background information on CCG and semantic parsing. Section 3 describes the text and ontology that form the new clinical trials dataset for semantic parsing, as well as some of the problems that exising approaches have on this dataset. Sections 4 describes our semantic parsing model, and learning and inference algorithms. Section 5 presents our experiments and results, and Section 6 concludes. 2 Background on Semantic Parsing with CCG Our approach to learning a semantic parser falls into the general framework of context-free Probabilistic Combinatory Categorial Grammars (PCCG) (Zettlemoyer and Collins, 2005) with typed lambda calculus expressions for the semantics. PCCG grammars involve lexical entries, which are weighted unary rewrite rules of the form Syntax: Semantics —* Phrase. For example: Example Lexical Entries NP : melanoma —* skin cancer S\NP : ApAd.has condition(p, d) —* patients with In addition to lexical rules, PCCG grammars involve weighted binary rewrite rules like the following: These rules apply for any syntactic categories X and Y , and any logical forms f and g. The rules specify mechanisms for deducing syntactic categories for whole phrases based on their constituent parts. Th</context>
<context position="8653" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1349" endWordPosition="1352">ng frameworks have been applied to the task of learning a semantic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complex</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars. In Proceedings of the Twenty First Conference on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online Learning of Relaxed CCG Grammars for Parsing to Logical Form.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="8684" citStr="Zettlemoyer and Collins, 2007" startWordPosition="1353" endWordPosition="1356"> to the task of learning a semantic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have </context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online Learning of Relaxed CCG Grammars for Parsing to Logical Form. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning Context-dependent Mappings from Sentences to Logical Form.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</booktitle>
<contexts>
<context position="8715" citStr="Zettlemoyer and Collins, 2009" startWordPosition="1357" endWordPosition="1360">ntic parser, including inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 1999; Thompson and Mooney, 2003), support vecExample CCG Grammar Rules X : f(g) —* X/Y : f Y : g (function application) X : f(g) —* Y : g X\Y : f (backward application) 349 tor machine-based kernel approaches (Kate et al., 2005; Kate and Mooney, 2006; Kate and Mooney, 2007), machine translation-style synchronous grammars (Wong and Mooney, 2007), and context-free grammar-based approaches like probabilistic Combinatory Categorial Grammar (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Lu et al., 2008) and discriminative reranking (Ge and Mooney, 2006; Ge and Mooney, 2009). These approaches have yielded steady improvements on standard test sets like GeoQuery. As far as we are aware, such systems have not been tested on domains besides ATIS and GeoQuery. Because of the complexity involved in building a training dataset for a supervised semantic parser, there has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle</context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2009. Learning Context-dependent Mappings from Sentences to Logical Form. In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>