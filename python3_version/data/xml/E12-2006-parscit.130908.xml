<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.114540">
<title confidence="0.941312">
ONTS: “Optima” News Translation System
</title>
<author confidence="0.94023">
Marco Turchi*, Martin Atkinson*, Alastair Wilcox+, Brett Crawley,
Stefano Bucci+, Ralf Steinberger* and Erik Van der Goot*
</author>
<affiliation confidence="0.604233">
European Commission - Joint Research Centre (JRC), IPSC - GlobeSec
</affiliation>
<address confidence="0.661288">
Via Fermi 2749, 21020 Ispra (VA) - Italy
</address>
<email confidence="0.979988333333333">
*[name].[surname]@jrc.ec.europa.eu
+[name].[surname]@ext.jrc.ec.europa.eu
brettcrawley@gmail.com
</email>
<sectionHeader confidence="0.993804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999769714285714">
We propose a real-time machine translation
system that allows users to select a news
category and to translate the related live
news articles from Arabic, Czech, Danish,
Farsi, French, German, Italian, Polish, Por-
tuguese, Spanish and Turkish into English.
The Moses-based system was optimised for
the news domain and differs from other
available systems in four ways: (1) News
items are automatically categorised on the
source side, before translation; (2) Named
entity translation is optimised by recog-
nising and extracting them on the source
side and by re-inserting their translation in
the target language, making use of a sep-
arate entity repository; (3) News titles are
translated with a separate translation sys-
tem which is optimised for the specific style
of news titles; (4) The system was opti-
mised for speed in order to cope with the
large volume of daily news articles.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999749666666667">
Being able to read news from other countries and
written in other languages allows readers to be
better informed. It allows them to detect national
news bias and thus improves transparency and
democracy. Existing online translation systems
such as Google Translate and Bing Translator1
are thus a great service, but the number of docu-
ments that can be submitted is restricted (Google
will even entirely stop their service in 2012) and
submitting documents means disclosing the users’
interests and their (possibly sensitive) data to the
service-providing company.
</bodyText>
<footnote confidence="0.9933745">
1http://translate.google.com/ and http:
//www.microsofttranslator.com/
</footnote>
<bodyText confidence="0.999851615384615">
For these reasons, we have developed our
in-house machine translation system ONTS. Its
translation results will be publicly accessible as
part of the Europe Media Monitor family of ap-
plications, (Steinberger et al., 2009), which gather
and process about 100,000 news articles per day
in about fifty languages. ONTS is based on
the open source phrase-based statistical machine
translation toolkit Moses (Koehn et al., 2007),
trained mostly on freely available parallel cor-
pora and optimised for the news domain, as stated
above. The main objective of developing our in-
house system is thus not to improve translation
quality over the existing services (this would be
beyond our possibilities), but to offer our users a
rough translation (a “gist”) that allows them to get
an idea of the main contents of the article and to
determine whether the news item at hand is rele-
vant for their field of interest or not.
A similar news-focused translation service is
“Found in Translation” (Turchi et al., 2009),
which gathers articles in 23 languages and trans-
lates them into English. “Found in Translation” is
also based on Moses, but it categorises the news
after translation and the translation process is not
optimised for the news domain.
</bodyText>
<sectionHeader confidence="0.963633" genericHeader="method">
2 Europe Media Monitor
</sectionHeader>
<bodyText confidence="0.999682">
Europe Media Monitor (EMM)2 gathers a daily
average of 100,000 news articles in approximately
50 languages, from about 3,400 hand-selected
web news sources, from a couple of hundred spe-
cialist and government websites, as well as from
about twenty commercial news providers. It vis-
its the news web sites up to every five minutes to
</bodyText>
<footnote confidence="0.967568">
2http://emm.newsbrief.eu/overview.html
</footnote>
<page confidence="0.987518">
25
</page>
<bodyText confidence="0.954353058823529">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 25–30,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
search for the latest articles. When news sites of-
fer RSS feeds, it makes use of these, otherwise
it extracts the news text from the often complex
HTML pages. All news items are converted to
Unicode. They are processed in a pipeline struc-
ture, where each module adds additional informa-
tion. Independently of how files are written, the
system uses UTF-8-encoded RSS format.
Inside the pipeline, different algorithms are im-
plemented to produce monolingual and multilin-
gual clusters and to extract various types of in-
formation such as named entities, quotations, cat-
egories and more. ONTS uses two modules of
EMM: the named entity recognition and the cate-
gorization parts.
</bodyText>
<subsectionHeader confidence="0.598813">
2.1 Named Entity Recognition and Variant
Matching.
</subsectionHeader>
<bodyText confidence="0.999846444444444">
Named Entity Recognition (NER) is per-
formed using manually constructed language-
independent rules that make use of language-
specific lists of trigger words such as titles
(president), professions or occupations (tennis
player, playboy), references to countries, regions,
ethnic or religious groups (French, Bavarian,
Berber, Muslim), age expressions (57-year-old),
verbal phrases (deceased), modifiers (former)
and more. These patterns can also occur in
combination and patterns can be nested to capture
more complex titles, (Steinberger and Pouliquen,
2007). In order to be able to cover many different
languages, no other dictionaries and no parsers or
part-of-speech taggers are used.
To identify which of the names newly found
every day are new entities and which ones are
merely variant spellings of entities already con-
tained in the database, we apply a language-
independent name similarity measure to decide
which name variants should be automatically
merged, for details see (Pouliquen and Stein-
berger, 2009). This allows us to maintain a
database containing over 1,15 million named en-
tities and 200,000 variants. The major part of
this resource can be downloaded from http:
//langtech.jrc.it/JRC-Names.html
</bodyText>
<subsectionHeader confidence="0.943599">
2.2 Category Classification across
Languages.
</subsectionHeader>
<bodyText confidence="0.9998121">
All news items are categorized into hundreds of
categories. Category definitions are multilingual,
created by humans and they include geographic
regions such as each country of the world, organi-
zations, themes such as natural disasters or secu-
rity, and more specific classes such as earthquake,
terrorism or tuberculosis,
Articles fall into a given category if they sat-
isfy the category definition, which consists of
Boolean operators with optional vicinity opera-
tors and wild cards. Alternatively, cumulative
positive or negative weights and a threshold can
be used. Uppercase letters in the category defi-
nition only match uppercase words, while lower-
case words in the definition match both uppercase
and lowercase words. Many categories are de-
fined with input from the users themselves. This
method to categorize the articles is rather sim-
ple and user-friendly, and it lends itself to dealing
with many languages, (Steinberger et al., 2009).
</bodyText>
<sectionHeader confidence="0.986006" genericHeader="method">
3 News Translation System
</sectionHeader>
<bodyText confidence="0.999888375">
In this section, we describe our statistical machine
translation (SMT) service based on the open-
source toolkit Moses (Koehn et al., 2007) and its
adaptation to translation of news items.
Which is the most suitable SMT system for
our requirements? The main goal of our system
is to help the user understand the content of an ar-
ticle. This means that a translated article is evalu-
ated positively even if it is not perfect in the target
language. Dealing with such a large number of
source languages and articles per day, our system
should take into account the translation speed, and
try to avoid using language-dependent tools such
as part-of-speech taggers.
Inside the Moses toolkit, three different
statistical approaches have been implemented:
phrase based statistical machine translation (PB-
SMT) (Koehn et al., 2003), hierarchical phrase
based statistical machine translation (Chiang,
2007) and syntax-based statistical machine trans-
lation (Marcu et al., 2006). To identify the
most suitable system for our requirements, we
run a set of experiments training the three mod-
els with Europarl V4 German-English (Koehn,
2005) and optimizing and testing on the News
corpus (Callison-Burch et al., 2009). For all of
them, we use their default configurations and they
are run under the same condition on the same ma-
chine to better evaluate translation time. For the
syntax model we use linguistic information only
on the target side. According to our experiments,
in terms of performance the hierarchical model
</bodyText>
<page confidence="0.972528">
26
</page>
<bodyText confidence="0.999974875">
performs better than PBSMT and syntax (18.31,
18.09, 17.62 Bleu points), but in terms of transla-
tion speed PBSMT is better than hierarchical and
syntax (1.02, 4.5, 49 second per sentence). Al-
though, the hierarchical model has the best Bleu
score, we prefer to use the PBSMT system in our
translation service, because it is four times faster.
Which training data can we use? It is known
in statistical machine translation that more train-
ing data implies better translation. Although, the
number of parallel corpora has been is growing
in the last years, the amounts of training data
vary from language pair to language pair. To
train our models we use the freely available cor-
pora (when possible): Europarl (Koehn, 2005),
JRC-Acquis (Steinberger et al., 2006), DGT-
TM3, Opus (Tiedemann, 2009), SE-Times (Ty-
ers and Alperen, 2010), Tehran English-Persian
Parallel Corpus (Pilevar et al., 2011), News
Corpus (Callison-Burch et al., 2009), UN Cor-
pus (Rafalovitch and Dale, 2009), CzEng0.9 (Bo-
jar and ˇZabokrtsk´y, 2009), English-Persian paral-
lel corpus distributed by ELRA4 and two Arabic-
English datasets distributed by LDC5. This re-
sults in some language pairs with a large cover-
age, (more than 4 million sentences), and other
with a very small coverage, (less than 1 million).
The language models are trained using 12 model
sentences for the content model and 4.7 million
for the title model. Both sets are extracted from
English news.
For less resourced languages such as Farsi and
Turkish, we tried to extend the available corpora.
For Farsi, we applied the methodology proposed
by (Lambert et al., 2011), where we used a large
language model and an English-Farsi SMT model
to produce new sentence pairs. For Turkish we
added the Movie Subtitles corpus (Tiedemann,
2009), which allowed the SMT system to in-
crease its translation capability, but included sev-
eral slang words and spoken phrases.
How to deal with Named Entities in transla-
tion? News articles are related to the most impor-
tant events. These names need to be efficiently
translated to correctly understand the content of
an article. From an SMT point of view, two main
issues are related to Named Entity translation: (1)
such a name is not in the training data or (2) part
</bodyText>
<footnote confidence="0.999938333333333">
3http://langtech.jrc.it/DGT-TM.html
4http://catalog.elra.info/
5http://www.ldc.upenn.edu/
</footnote>
<bodyText confidence="0.999915510204082">
of the name is a common word in the target lan-
guage and it is wrongly translated, e.g. the French
name “Bruno Le Maire” which risks to be trans-
lated into English as “Bruno Mayor”. To mitigate
both the effects we use our multilingual named
entity database. In the source language, each news
item is analysed to identify possible entities; if
an entity is recognised, its correct translation into
English is retrieved from the database, and sug-
gested to the SMT system enriching the source
sentence using the xml markup option 6 in Moses.
This approach allows us to complement the train-
ing data increasing the translation capability of
our system.
How to deal with different language styles
in the news? News title writing style contains
more gerund verbs, no or few linking verbs,
prepositions and adverbs than normal sentences,
while content sentences include more preposi-
tion, adverbs and different verbal tenses. Starting
from this assumption, we investigated if this phe-
nomenon can affect the translation performance
of our system.
We trained two SMT systems, SMTcontent
and SMTtitle, using the Europarl V4 German-
English data as training corpus, and two dif-
ferent development sets: one made of content
sentences, News Commentaries (Callison-Burch
et al., 2009), and the other made of news ti-
tles in the source language which were trans-
lated into English using a commercial transla-
tion system. With the same strategy we gener-
ated also a Title test set. The SMTtitle used a
language model created using only English news
titles. The News and Title test sets were trans-
lated by both the systems. Although the perfor-
mance obtained translating the News and Title
corpora are not comparable, we were interested
in analysing how the same test set is translated
by the two systems. We noticed that translat-
ing a test set with a system that was optimized
with the same type of data resulted in almost 2
Blue score improvements: Title-TestSet: 0.3706
(SMTtitle), 0.3511 (SMTcontent); News-TestSet:
0.1768 (SMTtitle), 0.1945 (SMTcontent). This
behaviour was present also in different language
pairs. According to these results we decided
to use two different translation systems for each
language pair, one optimized using title data
</bodyText>
<footnote confidence="0.975628">
6http://www.statmt.org/moses/?n=Moses.
AdvancedFeatures#ntoc4
</footnote>
<page confidence="0.999131">
27
</page>
<bodyText confidence="0.999971857142857">
and the other using normal content sentences.
Even though this implementation choice requires
more computational power to run in memory two
Moses servers, it allows us to mitigate the work-
load of each single instance reducing translation
time of each single article and to improve transla-
tion quality.
</bodyText>
<subsectionHeader confidence="0.999343">
3.1 Translation Quality
</subsectionHeader>
<bodyText confidence="0.999834256410256">
To evaluate the translation performance of ONTS,
we run a set of experiments where we translate a
test set for each language pair using our system
and Google Translate. Lack of human translated
parallel titles obliges us to test only the content
based model. For German, Spanish and Czech we
use the news test sets proposed in (Callison-Burch
et al., 2010), for French and Italian the news test
sets presented in (Callison-Burch et al., 2008),
for Arabic, Farsi and Turkish, sets of 2,000 news
sentences extracted from the Arabic-English and
English-Persian datasets and the SE-Times cor-
pus. For the other languages we use 2,000 sen-
tences which are not news but a mixture of JRC-
Acquis, Europarl and DGT-TM data. It is not
guarantee that our test sets are not part of the train-
ing data of Google Translate.
Each test set is translated by Google Translate
- Translator Toolkit, and by our system. Bleu
score is used to evaluate the performance of both
systems. Results, see Table 1, show that Google
Translate produces better translation for those lan-
guages for which large amounts of data are avail-
able such as French, German, Italian and Spanish.
Surprisingly, for Danish, Portuguese and Polish,
ONTS has better performance, this depends on
the choice of the test sets which are not made of
news data but of data that is fairly homogeneous
in terms of style and genre with the training sets.
The impact of the named entity module is ev-
ident for Arabic and Farsi, where each English
suggested entity results in a larger coverage of
the source language and better translations. For
highly inflected and agglutinative languages such
as Turkish, the output proposed by ONTS is poor.
We are working on gathering more training data
coming from the news domain and on the pos-
sibility of applying a linguistic pre-processing of
the documents.
</bodyText>
<table confidence="0.999554333333333">
Source L. ONTS Google T.
Arabic 0.318 0.255
Czech 0.218 0.226
Danish 0.324 0.296
Farsi 0.245 0.197
French 0.26 0.286
German 0.205 0.25
Italian 0.234 0.31
Polish 0.568 0.511
Portuguese 0.579 0.424
Spanish 0.283 0.334
Turkish 0.238 0.395
</table>
<tableCaption confidence="0.999899">
Table 1: Automatic evaluation.
</tableCaption>
<sectionHeader confidence="0.990254" genericHeader="method">
4 Technical Implementation
</sectionHeader>
<bodyText confidence="0.9995684375">
The translation service is made of two compo-
nents: the connection module and the Moses
server. The connection module is a servlet im-
plemented in Java. It receives the RSS files,
isolates each single news article, identifies each
source language and pre-processes it. Each news
item is split into sentences, each sentence is to-
kenized, lowercased, passed through a statisti-
cal compound word splitter, (Koehn and Knight,
2003), and the named entity annotator module.
For language modelling we use the KenLM im-
plementation, (Heafield, 2011).
According to the language, the correct Moses
servers, title and content, are fed in a multi-
thread manner. We use the multi-thread version
of Moses (Haddow, 2010). When all the sentences
of each article are translated, the inverse process
is run: they are detokenized, recased, and untrans-
lated/unknown words are listed. The translated ti-
tle and content of each article are uploaded into
the RSS file and it is passed to the next modules.
The full system including the translation mod-
ules is running in a 2xQuad-Core with In-
tel Hyper-threading Technology processors with
48GB of memory. It is our intention to locate
the Moses servers on different machines. This is
possible thanks to the high modularity and cus-
tomization of the connection module. At the mo-
ment, the translation models are available for the
following source languages: Arabic, Czech, Dan-
ish, Farsi, French, German, Italian, Polish, Por-
tuguese, Spanish and Turkish.
</bodyText>
<page confidence="0.998334">
28
</page>
<figureCaption confidence="0.999342">
Figure 1: Demo Web site.
</figureCaption>
<subsectionHeader confidence="0.983582">
4.1 Demo
</subsectionHeader>
<bodyText confidence="0.9999814">
Our translation service is currently presented on
a demo web site, see Figure 1, which is available
athttp://optima.jrc.it/Translate/.
News articles can be retrieved selecting one of the
topics and the language. All the topics are as-
signed to each article using the methodology de-
scribed in 2.2. These articles are shown in the left
column of the interface. When the button “Trans-
late” is pressed, the translation process starts and
the translated articles appear in the right column
of the page.
The translation system can be customized from
the interface enabling or disabling the named
entity, compound, recaser, detokenizer and un-
known word modules. Each translated article is
enriched showing the translation time in millisec-
onds per character and, if enabled, the list of un-
known words. The interface is linked to the con-
nection module and data is transferred using RSS
structure.
</bodyText>
<sectionHeader confidence="0.998166" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.998949310344828">
In this paper we present the Optima News Trans-
lation System and how it is connected to Eu-
rope Media Monitor application. Different strate-
gies are applied to increase the translation perfor-
mance taking advantage of the document struc-
ture and other resources available in our research
group. We believe that the experiments described
in this work can result very useful for the develop-
ment of other similar systems. Translations pro-
duced by our system will soon be available as part
of the main EMM applications.
The performance of our system is encouraging,
but not as good as the performance of web ser-
vices such as Google Translate, mostly because
we use less training data and we have reduced
computational power. On the other hand, our in-
house system can be fed with a large number of
articles per day and sensitive data without includ-
ing third parties in the translation process. Per-
formance and translation time vary according to
the number and complexity of sentences and lan-
guage pairs.
The domain of news articles dynamically
changes according to the main events in the world,
while existing parallel data is static and usually
associated to governmental domains. It is our in-
tention to investigate how to adapt our translation
system updating the language model with the En-
glish articles of the day.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999505">
The authors thank the JRC’s OPTIMA team for
its support during the development of ONTS.
</bodyText>
<sectionHeader confidence="0.998053" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998930833333333">
O. Bojar and Z. ˇZabokrtsk´y. 2009. CzEng0.9: Large
Parallel Treebank with Rich Annotation. Prague
Bulletin of Mathematical Linguistics, 92.
C. Callison-Burch and C. Fordyce and P. Koehn and
C. Monz and J. Schroeder. 2008. Further Meta-
Evaluation of Machine Translation. Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, pages 70–106. Columbus, US.
C. Callison-Burch, and P. Koehn and C. Monz and J.
Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 1–28. Athens, Greece.
C. Callison-Burch, and P. Koehn and C. Monz and K.
Peterson and M. Przybocki and O. Zaidan. 2009.
Findings of the 2010 Joint Workshop on Statisti-
cal Machine Translation and Metrics for Machine
Translation. Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 17–53. Uppsala, Sweden.
D. Chiang. 2005. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2): pages 201–
228. MIT Press.
B. Haddow. 2010. Adding multi-threaded decoding to
moses. The Prague Bulletin of Mathematical Lin-
guistics, 93(1): pages 57–66. Versita.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. Proceedings of the Sixth
Workshop on Statistical Machine Translation, Ed-
inburgh, UK.
</reference>
<page confidence="0.989715">
29
</page>
<reference confidence="0.998944695121951">
P. Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of
the Machine Translation Summit X, pages 79-86.
Phuket, Thailand.
P. Koehn and F. J. Och and D. Marcu. 2003. Statistical
phrase-based translation. Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology, pages 48–54. Edmon-
ton, Canada.
P. Koehn and K. Knight. 2003. Empirical methods
for compound splitting. Proceedings of the tenth
conference on European chapter of the Association
for Computational Linguistics, pages 187–193. Bu-
dapest, Hungary.
P. Koehn and H. Hoang and A. Birch and C. Callison-
Burch and M. Federico and N. Bertoldi and B.
Cowan and W. Shen and C. Moran and R. Zens
and C. Dyer and O. Bojar and A. Constantin and E.
Herbst 2007. Moses: Open source toolkit for sta-
tistical machine translation. Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics, demonstration session, pages 177–180.
Columbus, Oh, USA.
P. Lambert and H. Schwenk and C. Servan and S.
Abdul-Rauf. 2011. SPMT: Investigations on Trans-
lation Model Adaptation Using Monolingual Data.
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 284–293. Edinburgh,
Scotland.
D. Marcu and W. Wang and A. Echihabi and K.
Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language phrases.
Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
48–54. Edmonton, Canada.
M. Pilevar and H. Faili and A. Pilevar. 2011. TEP:
Tehran English-Persian Parallel Corpus. Compu-
tational Linguistics and Intelligent Text Processing,
pages 68–79. Springer.
B. Pouliquen and R. Steinberger. 2009. Auto-
matic construction of multilingual name dictionar-
ies. Learning Machine Translation, pages 59–78.
MIT Press - Advances in Neural Information Pro-
cessing Systems Series (NIPS).
A. Rafalovitch and R. Dale. 2009. United nations
general assembly resolutions: A six-language par-
allel corpus. Proceedings of the MT Summit XIII,
pages 292–299. Ottawa, Canada.
R. Steinberger and B. Pouliquen. 2007. Cross-lingual
named entity recognition. Lingvisticæ Investiga-
tiones, 30(1) pages 135–162. John Benjamins Pub-
lishing Company.
R. Steinberger and B. Pouliquen and A. Widiger and
C. Ignat and T. Erjavec and D. Tufis¸ and D. Varga.
2006. The JRC-Acquis: A multilingual aligned par-
allel corpus with 20+ languages. Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 2142–2147. Genova,
Italy.
R. Steinberger and B. Pouliquen and E. van der Goot.
2009. An Introduction to the Europe Media Monitor
Family of Applications. Proceedings of the Infor-
mation Access in a Multilingual World-Proceedings
of the SIGIR 2009 Workshop, pages 1–8. Boston,
USA.
J. Tiedemann. 2009. News from OPUS-A Collection
of Multilingual Parallel Corpora with Tools and
Interfaces. Recent advances in natural language
processing V: selected papers from RANLP 2007,
pages 309:237.
M. Turchi and I. Flaounas and O. Ali and T. DeBie
and T. Snowsill and N. Cristianini. 2009. Found in
translation. Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discov-
ery in Databases, pages 746–749. Bled, Slovenia.
F. Tyers and M.S. Alperen. 2010. South-East Euro-
pean Times: A parallel corpus of Balkan languages.
Proceedings of the LREC workshop on Exploita-
tion of multilingual resources and tools for Central
and (South) Eastern European Languages, Valletta,
Malta.
</reference>
<page confidence="0.998794">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.283537">
<title confidence="0.999787">ONTS: “Optima” News Translation System</title>
<author confidence="0.7454115">Joint Research Centre</author>
<email confidence="0.999677">brettcrawley@gmail.com</email>
<abstract confidence="0.993175681818182">We propose a real-time machine translation system that allows users to select a news category and to translate the related live news articles from Arabic, Czech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and Turkish into English. The Moses-based system was optimised for the news domain and differs from other available systems in four ways: (1) News items are automatically categorised on the source side, before translation; (2) Named entity translation is optimised by recognising and extracting them on the source side and by re-inserting their translation in the target language, making use of a separate entity repository; (3) News titles are translated with a separate translation system which is optimised for the specific style of news titles; (4) The system was optimised for speed in order to cope with the large volume of daily news articles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>Z ˇZabokrtsk´y</author>
</authors>
<title>CzEng0.9: Large Parallel Treebank with Rich Annotation.</title>
<date>2009</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>92</volume>
<marker>Bojar, ˇZabokrtsk´y, 2009</marker>
<rawString>O. Bojar and Z. ˇZabokrtsk´y. 2009. CzEng0.9: Large Parallel Treebank with Rich Annotation. Prague Bulletin of Mathematical Linguistics, 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Fordyce</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<date>2008</date>
<booktitle>Further MetaEvaluation of Machine Translation. Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<location>Columbus, US.</location>
<contexts>
<context position="13585" citStr="Callison-Burch et al., 2008" startWordPosition="2126" endWordPosition="2129">Moses servers, it allows us to mitigate the workload of each single instance reducing translation time of each single article and to improve translation quality. 3.1 Translation Quality To evaluate the translation performance of ONTS, we run a set of experiments where we translate a test set for each language pair using our system and Google Translate. Lack of human translated parallel titles obliges us to test only the content based model. For German, Spanish and Czech we use the news test sets proposed in (Callison-Burch et al., 2010), for French and Italian the news test sets presented in (Callison-Burch et al., 2008), for Arabic, Farsi and Turkish, sets of 2,000 news sentences extracted from the Arabic-English and English-Persian datasets and the SE-Times corpus. For the other languages we use 2,000 sentences which are not news but a mixture of JRCAcquis, Europarl and DGT-TM data. It is not guarantee that our test sets are not part of the training data of Google Translate. Each test set is translated by Google Translate - Translator Toolkit, and by our system. Bleu score is used to evaluate the performance of both systems. Results, see Table 1, show that Google Translate produces better translation for th</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>C. Callison-Burch and C. Fordyce and P. Koehn and C. Monz and J. Schroeder. 2008. Further MetaEvaluation of Machine Translation. Proceedings of the Third Workshop on Statistical Machine Translation, pages 70–106. Columbus, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--28</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="7893" citStr="Callison-Burch et al., 2009" startWordPosition="1208" endWordPosition="1211">speed, and try to avoid using language-dependent tools such as part-of-speech taggers. Inside the Moses toolkit, three different statistical approaches have been implemented: phrase based statistical machine translation (PBSMT) (Koehn et al., 2003), hierarchical phrase based statistical machine translation (Chiang, 2007) and syntax-based statistical machine translation (Marcu et al., 2006). To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al., 2009). For all of them, we use their default configurations and they are run under the same condition on the same machine to better evaluate translation time. For the syntax model we use linguistic information only on the target side. According to our experiments, in terms of performance the hierarchical model 26 performs better than PBSMT and syntax (18.31, 18.09, 17.62 Bleu points), but in terms of translation speed PBSMT is better than hierarchical and syntax (1.02, 4.5, 49 second per sentence). Although, the hierarchical model has the best Bleu score, we prefer to use the PBSMT system in our tr</context>
<context position="9135" citStr="Callison-Burch et al., 2009" startWordPosition="1411" endWordPosition="1414">ice, because it is four times faster. Which training data can we use? It is known in statistical machine translation that more training data implies better translation. Although, the number of parallel corpora has been is growing in the last years, the amounts of training data vary from language pair to language pair. To train our models we use the freely available corpora (when possible): Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), DGTTM3, Opus (Tiedemann, 2009), SE-Times (Tyers and Alperen, 2010), Tehran English-Persian Parallel Corpus (Pilevar et al., 2011), News Corpus (Callison-Burch et al., 2009), UN Corpus (Rafalovitch and Dale, 2009), CzEng0.9 (Bojar and ˇZabokrtsk´y, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5. This results in some language pairs with a large coverage, (more than 4 million sentences), and other with a very small coverage, (less than 1 million). The language models are trained using 12 model sentences for the content model and 4.7 million for the title model. Both sets are extracted from English news. For less resourced languages such as Farsi and Turkish, we tried to extend the available corpora. Fo</context>
<context position="11789" citStr="Callison-Burch et al., 2009" startWordPosition="1839" endWordPosition="1842">ur system. How to deal with different language styles in the news? News title writing style contains more gerund verbs, no or few linking verbs, prepositions and adverbs than normal sentences, while content sentences include more preposition, adverbs and different verbal tenses. Starting from this assumption, we investigated if this phenomenon can affect the translation performance of our system. We trained two SMT systems, SMTcontent and SMTtitle, using the Europarl V4 GermanEnglish data as training corpus, and two different development sets: one made of content sentences, News Commentaries (Callison-Burch et al., 2009), and the other made of news titles in the source language which were translated into English using a commercial translation system. With the same strategy we generated also a Title test set. The SMTtitle used a language model created using only English news titles. The News and Title test sets were translated by both the systems. Although the performance obtained translating the News and Title corpora are not comparable, we were interested in analysing how the same test set is translated by the two systems. We noticed that translating a test set with a system that was optimized with the same </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>C. Callison-Burch, and P. Koehn and C. Monz and J. Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 1–28. Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>K Peterson</author>
<author>M Przybocki</author>
<author>O Zaidan</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation. Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>17--53</pages>
<location>Uppsala,</location>
<contexts>
<context position="7893" citStr="Callison-Burch et al., 2009" startWordPosition="1208" endWordPosition="1211">speed, and try to avoid using language-dependent tools such as part-of-speech taggers. Inside the Moses toolkit, three different statistical approaches have been implemented: phrase based statistical machine translation (PBSMT) (Koehn et al., 2003), hierarchical phrase based statistical machine translation (Chiang, 2007) and syntax-based statistical machine translation (Marcu et al., 2006). To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al., 2009). For all of them, we use their default configurations and they are run under the same condition on the same machine to better evaluate translation time. For the syntax model we use linguistic information only on the target side. According to our experiments, in terms of performance the hierarchical model 26 performs better than PBSMT and syntax (18.31, 18.09, 17.62 Bleu points), but in terms of translation speed PBSMT is better than hierarchical and syntax (1.02, 4.5, 49 second per sentence). Although, the hierarchical model has the best Bleu score, we prefer to use the PBSMT system in our tr</context>
<context position="9135" citStr="Callison-Burch et al., 2009" startWordPosition="1411" endWordPosition="1414">ice, because it is four times faster. Which training data can we use? It is known in statistical machine translation that more training data implies better translation. Although, the number of parallel corpora has been is growing in the last years, the amounts of training data vary from language pair to language pair. To train our models we use the freely available corpora (when possible): Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), DGTTM3, Opus (Tiedemann, 2009), SE-Times (Tyers and Alperen, 2010), Tehran English-Persian Parallel Corpus (Pilevar et al., 2011), News Corpus (Callison-Burch et al., 2009), UN Corpus (Rafalovitch and Dale, 2009), CzEng0.9 (Bojar and ˇZabokrtsk´y, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5. This results in some language pairs with a large coverage, (more than 4 million sentences), and other with a very small coverage, (less than 1 million). The language models are trained using 12 model sentences for the content model and 4.7 million for the title model. Both sets are extracted from English news. For less resourced languages such as Farsi and Turkish, we tried to extend the available corpora. Fo</context>
<context position="11789" citStr="Callison-Burch et al., 2009" startWordPosition="1839" endWordPosition="1842">ur system. How to deal with different language styles in the news? News title writing style contains more gerund verbs, no or few linking verbs, prepositions and adverbs than normal sentences, while content sentences include more preposition, adverbs and different verbal tenses. Starting from this assumption, we investigated if this phenomenon can affect the translation performance of our system. We trained two SMT systems, SMTcontent and SMTtitle, using the Europarl V4 GermanEnglish data as training corpus, and two different development sets: one made of content sentences, News Commentaries (Callison-Burch et al., 2009), and the other made of news titles in the source language which were translated into English using a commercial translation system. With the same strategy we generated also a Title test set. The SMTtitle used a language model created using only English news titles. The News and Title test sets were translated by both the systems. Although the performance obtained translating the News and Title corpora are not comparable, we were interested in analysing how the same test set is translated by the two systems. We noticed that translating a test set with a system that was optimized with the same </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2009</marker>
<rawString>C. Callison-Burch, and P. Koehn and C. Monz and K. Peterson and M. Przybocki and O. Zaidan. 2009. Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation. Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53. Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>201--228</pages>
<publisher>MIT Press.</publisher>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. Hierarchical phrase-based translation. Computational Linguistics, 33(2): pages 201– 228. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Haddow</author>
</authors>
<title>Adding multi-threaded decoding to moses. The Prague</title>
<date>2010</date>
<journal>Bulletin of Mathematical Linguistics,</journal>
<volume>93</volume>
<issue>1</issue>
<pages>57--66</pages>
<publisher>Versita.</publisher>
<contexts>
<context position="15972" citStr="Haddow, 2010" startWordPosition="2522" endWordPosition="2523">server. The connection module is a servlet implemented in Java. It receives the RSS files, isolates each single news article, identifies each source language and pre-processes it. Each news item is split into sentences, each sentence is tokenized, lowercased, passed through a statistical compound word splitter, (Koehn and Knight, 2003), and the named entity annotator module. For language modelling we use the KenLM implementation, (Heafield, 2011). According to the language, the correct Moses servers, title and content, are fed in a multithread manner. We use the multi-thread version of Moses (Haddow, 2010). When all the sentences of each article are translated, the inverse process is run: they are detokenized, recased, and untranslated/unknown words are listed. The translated title and content of each article are uploaded into the RSS file and it is passed to the next modules. The full system including the translation modules is running in a 2xQuad-Core with Intel Hyper-threading Technology processors with 48GB of memory. It is our intention to locate the Moses servers on different machines. This is possible thanks to the high modularity and customization of the connection module. At the moment</context>
</contexts>
<marker>Haddow, 2010</marker>
<rawString>B. Haddow. 2010. Adding multi-threaded decoding to moses. The Prague Bulletin of Mathematical Linguistics, 93(1): pages 57–66. Versita.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="15809" citStr="Heafield, 2011" startWordPosition="2495" endWordPosition="2496">Turkish 0.238 0.395 Table 1: Automatic evaluation. 4 Technical Implementation The translation service is made of two components: the connection module and the Moses server. The connection module is a servlet implemented in Java. It receives the RSS files, isolates each single news article, identifies each source language and pre-processes it. Each news item is split into sentences, each sentence is tokenized, lowercased, passed through a statistical compound word splitter, (Koehn and Knight, 2003), and the named entity annotator module. For language modelling we use the KenLM implementation, (Heafield, 2011). According to the language, the correct Moses servers, title and content, are fed in a multithread manner. We use the multi-thread version of Moses (Haddow, 2010). When all the sentences of each article are translated, the inverse process is run: they are detokenized, recased, and untranslated/unknown words are listed. The translated title and content of each article are uploaded into the RSS file and it is passed to the next modules. The full system including the translation modules is running in a 2xQuad-Core with Intel Hyper-threading Technology processors with 48GB of memory. It is our in</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>K. Heafield. 2011. KenLM: Faster and smaller language model queries. Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>Proceedings of the Machine Translation Summit X,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="7817" citStr="Koehn, 2005" startWordPosition="1198" endWordPosition="1199">er day, our system should take into account the translation speed, and try to avoid using language-dependent tools such as part-of-speech taggers. Inside the Moses toolkit, three different statistical approaches have been implemented: phrase based statistical machine translation (PBSMT) (Koehn et al., 2003), hierarchical phrase based statistical machine translation (Chiang, 2007) and syntax-based statistical machine translation (Marcu et al., 2006). To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al., 2009). For all of them, we use their default configurations and they are run under the same condition on the same machine to better evaluate translation time. For the syntax model we use linguistic information only on the target side. According to our experiments, in terms of performance the hierarchical model 26 performs better than PBSMT and syntax (18.31, 18.09, 17.62 Bleu points), but in terms of translation speed PBSMT is better than hierarchical and syntax (1.02, 4.5, 49 second per sentence). Although, the hierarchica</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. Proceedings of the Machine Translation Summit X, pages 79-86. Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="7513" citStr="Koehn et al., 2003" startWordPosition="1152" endWordPosition="1155">most suitable SMT system for our requirements? The main goal of our system is to help the user understand the content of an article. This means that a translated article is evaluated positively even if it is not perfect in the target language. Dealing with such a large number of source languages and articles per day, our system should take into account the translation speed, and try to avoid using language-dependent tools such as part-of-speech taggers. Inside the Moses toolkit, three different statistical approaches have been implemented: phrase based statistical machine translation (PBSMT) (Koehn et al., 2003), hierarchical phrase based statistical machine translation (Chiang, 2007) and syntax-based statistical machine translation (Marcu et al., 2006). To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al., 2009). For all of them, we use their default configurations and they are run under the same condition on the same machine to better evaluate translation time. For the syntax model we use linguistic information only on the tar</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn and F. J. Och and D. Marcu. 2003. Statistical phrase-based translation. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>187--193</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="15696" citStr="Koehn and Knight, 2003" startWordPosition="2476" endWordPosition="2479">197 French 0.26 0.286 German 0.205 0.25 Italian 0.234 0.31 Polish 0.568 0.511 Portuguese 0.579 0.424 Spanish 0.283 0.334 Turkish 0.238 0.395 Table 1: Automatic evaluation. 4 Technical Implementation The translation service is made of two components: the connection module and the Moses server. The connection module is a servlet implemented in Java. It receives the RSS files, isolates each single news article, identifies each source language and pre-processes it. Each news item is split into sentences, each sentence is tokenized, lowercased, passed through a statistical compound word splitter, (Koehn and Knight, 2003), and the named entity annotator module. For language modelling we use the KenLM implementation, (Heafield, 2011). According to the language, the correct Moses servers, title and content, are fed in a multithread manner. We use the multi-thread version of Moses (Haddow, 2010). When all the sentences of each article are translated, the inverse process is run: they are detokenized, recased, and untranslated/unknown words are listed. The translated title and content of each article are uploaded into the RSS file and it is passed to the next modules. The full system including the translation modul</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>P. Koehn and K. Knight. 2003. Empirical methods for compound splitting. Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics, pages 187–193. Budapest, Hungary.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C CallisonBurch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of the Annual Meeting of the Association for Computational Linguistics, demonstration session,</booktitle>
<pages>177--180</pages>
<location>Columbus, Oh, USA.</location>
<contexts>
<context position="2329" citStr="Koehn et al., 2007" startWordPosition="341" endWordPosition="344">mitting documents means disclosing the users’ interests and their (possibly sensitive) data to the service-providing company. 1http://translate.google.com/ and http: //www.microsofttranslator.com/ For these reasons, we have developed our in-house machine translation system ONTS. Its translation results will be publicly accessible as part of the Europe Media Monitor family of applications, (Steinberger et al., 2009), which gather and process about 100,000 news articles per day in about fifty languages. ONTS is based on the open source phrase-based statistical machine translation toolkit Moses (Koehn et al., 2007), trained mostly on freely available parallel corpora and optimised for the news domain, as stated above. The main objective of developing our inhouse system is thus not to improve translation quality over the existing services (this would be beyond our possibilities), but to offer our users a rough translation (a “gist”) that allows them to get an idea of the main contents of the article and to determine whether the news item at hand is relevant for their field of interest or not. A similar news-focused translation service is “Found in Translation” (Turchi et al., 2009), which gathers article</context>
<context position="6831" citStr="Koehn et al., 2007" startWordPosition="1041" endWordPosition="1044">rnatively, cumulative positive or negative weights and a threshold can be used. Uppercase letters in the category definition only match uppercase words, while lowercase words in the definition match both uppercase and lowercase words. Many categories are defined with input from the users themselves. This method to categorize the articles is rather simple and user-friendly, and it lends itself to dealing with many languages, (Steinberger et al., 2009). 3 News Translation System In this section, we describe our statistical machine translation (SMT) service based on the opensource toolkit Moses (Koehn et al., 2007) and its adaptation to translation of news items. Which is the most suitable SMT system for our requirements? The main goal of our system is to help the user understand the content of an article. This means that a translated article is evaluated positively even if it is not perfect in the target language. Dealing with such a large number of source languages and articles per day, our system should take into account the translation speed, and try to avoid using language-dependent tools such as part-of-speech taggers. Inside the Moses toolkit, three different statistical approaches have been impl</context>
</contexts>
<marker>Koehn, Hoang, Birch, CallisonBurch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn and H. Hoang and A. Birch and C. CallisonBurch and M. Federico and N. Bertoldi and B. Cowan and W. Shen and C. Moran and R. Zens and C. Dyer and O. Bojar and A. Constantin and E. Herbst 2007. Moses: Open source toolkit for statistical machine translation. Proceedings of the Annual Meeting of the Association for Computational Linguistics, demonstration session, pages 177–180. Columbus, Oh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lambert</author>
<author>H Schwenk</author>
<author>C Servan</author>
<author>S Abdul-Rauf</author>
</authors>
<title>SPMT: Investigations on Translation Model Adaptation Using Monolingual Data.</title>
<date>2011</date>
<booktitle>Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>284--293</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="9805" citStr="Lambert et al., 2011" startWordPosition="1522" endWordPosition="1525">9 (Bojar and ˇZabokrtsk´y, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5. This results in some language pairs with a large coverage, (more than 4 million sentences), and other with a very small coverage, (less than 1 million). The language models are trained using 12 model sentences for the content model and 4.7 million for the title model. Both sets are extracted from English news. For less resourced languages such as Farsi and Turkish, we tried to extend the available corpora. For Farsi, we applied the methodology proposed by (Lambert et al., 2011), where we used a large language model and an English-Farsi SMT model to produce new sentence pairs. For Turkish we added the Movie Subtitles corpus (Tiedemann, 2009), which allowed the SMT system to increase its translation capability, but included several slang words and spoken phrases. How to deal with Named Entities in translation? News articles are related to the most important events. These names need to be efficiently translated to correctly understand the content of an article. From an SMT point of view, two main issues are related to Named Entity translation: (1) such a name is not in</context>
</contexts>
<marker>Lambert, Schwenk, Servan, Abdul-Rauf, 2011</marker>
<rawString>P. Lambert and H. Schwenk and C. Servan and S. Abdul-Rauf. 2011. SPMT: Investigations on Translation Model Adaptation Using Monolingual Data. Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 284–293. Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="7657" citStr="Marcu et al., 2006" startWordPosition="1170" endWordPosition="1173">that a translated article is evaluated positively even if it is not perfect in the target language. Dealing with such a large number of source languages and articles per day, our system should take into account the translation speed, and try to avoid using language-dependent tools such as part-of-speech taggers. Inside the Moses toolkit, three different statistical approaches have been implemented: phrase based statistical machine translation (PBSMT) (Koehn et al., 2003), hierarchical phrase based statistical machine translation (Chiang, 2007) and syntax-based statistical machine translation (Marcu et al., 2006). To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al., 2009). For all of them, we use their default configurations and they are run under the same condition on the same machine to better evaluate translation time. For the syntax model we use linguistic information only on the target side. According to our experiments, in terms of performance the hierarchical model 26 performs better than PBSMT and syntax (18.31, 18.09, 1</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>D. Marcu and W. Wang and A. Echihabi and K. Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 48–54. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pilevar</author>
<author>H Faili</author>
<author>A Pilevar</author>
</authors>
<title>TEP: Tehran English-Persian Parallel Corpus.</title>
<date>2011</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>68--79</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9092" citStr="Pilevar et al., 2011" startWordPosition="1405" endWordPosition="1408">PBSMT system in our translation service, because it is four times faster. Which training data can we use? It is known in statistical machine translation that more training data implies better translation. Although, the number of parallel corpora has been is growing in the last years, the amounts of training data vary from language pair to language pair. To train our models we use the freely available corpora (when possible): Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), DGTTM3, Opus (Tiedemann, 2009), SE-Times (Tyers and Alperen, 2010), Tehran English-Persian Parallel Corpus (Pilevar et al., 2011), News Corpus (Callison-Burch et al., 2009), UN Corpus (Rafalovitch and Dale, 2009), CzEng0.9 (Bojar and ˇZabokrtsk´y, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5. This results in some language pairs with a large coverage, (more than 4 million sentences), and other with a very small coverage, (less than 1 million). The language models are trained using 12 model sentences for the content model and 4.7 million for the title model. Both sets are extracted from English news. For less resourced languages such as Farsi and Turkish, w</context>
</contexts>
<marker>Pilevar, Faili, Pilevar, 2011</marker>
<rawString>M. Pilevar and H. Faili and A. Pilevar. 2011. TEP: Tehran English-Persian Parallel Corpus. Computational Linguistics and Intelligent Text Processing, pages 68–79. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pouliquen</author>
<author>R Steinberger</author>
</authors>
<title>Automatic construction of multilingual name dictionaries. Learning Machine Translation,</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems Series (NIPS).</booktitle>
<pages>59--78</pages>
<publisher>MIT Press</publisher>
<contexts>
<context position="5478" citStr="Pouliquen and Steinberger, 2009" startWordPosition="834" endWordPosition="838">ceased), modifiers (former) and more. These patterns can also occur in combination and patterns can be nested to capture more complex titles, (Steinberger and Pouliquen, 2007). In order to be able to cover many different languages, no other dictionaries and no parsers or part-of-speech taggers are used. To identify which of the names newly found every day are new entities and which ones are merely variant spellings of entities already contained in the database, we apply a languageindependent name similarity measure to decide which name variants should be automatically merged, for details see (Pouliquen and Steinberger, 2009). This allows us to maintain a database containing over 1,15 million named entities and 200,000 variants. The major part of this resource can be downloaded from http: //langtech.jrc.it/JRC-Names.html 2.2 Category Classification across Languages. All news items are categorized into hundreds of categories. Category definitions are multilingual, created by humans and they include geographic regions such as each country of the world, organizations, themes such as natural disasters or security, and more specific classes such as earthquake, terrorism or tuberculosis, Articles fall into a given categ</context>
</contexts>
<marker>Pouliquen, Steinberger, 2009</marker>
<rawString>B. Pouliquen and R. Steinberger. 2009. Automatic construction of multilingual name dictionaries. Learning Machine Translation, pages 59–78. MIT Press - Advances in Neural Information Processing Systems Series (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rafalovitch</author>
<author>R Dale</author>
</authors>
<title>United nations general assembly resolutions: A six-language parallel corpus.</title>
<date>2009</date>
<booktitle>Proceedings of the MT Summit XIII,</booktitle>
<pages>292--299</pages>
<location>Ottawa, Canada.</location>
<contexts>
<context position="9175" citStr="Rafalovitch and Dale, 2009" startWordPosition="1418" endWordPosition="1421">ch training data can we use? It is known in statistical machine translation that more training data implies better translation. Although, the number of parallel corpora has been is growing in the last years, the amounts of training data vary from language pair to language pair. To train our models we use the freely available corpora (when possible): Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), DGTTM3, Opus (Tiedemann, 2009), SE-Times (Tyers and Alperen, 2010), Tehran English-Persian Parallel Corpus (Pilevar et al., 2011), News Corpus (Callison-Burch et al., 2009), UN Corpus (Rafalovitch and Dale, 2009), CzEng0.9 (Bojar and ˇZabokrtsk´y, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5. This results in some language pairs with a large coverage, (more than 4 million sentences), and other with a very small coverage, (less than 1 million). The language models are trained using 12 model sentences for the content model and 4.7 million for the title model. Both sets are extracted from English news. For less resourced languages such as Farsi and Turkish, we tried to extend the available corpora. For Farsi, we applied the methodology prop</context>
</contexts>
<marker>Rafalovitch, Dale, 2009</marker>
<rawString>A. Rafalovitch and R. Dale. 2009. United nations general assembly resolutions: A six-language parallel corpus. Proceedings of the MT Summit XIII, pages 292–299. Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>B Pouliquen</author>
</authors>
<title>Cross-lingual named entity recognition.</title>
<date>2007</date>
<journal>Lingvisticæ Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<pages>135--162</pages>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="5021" citStr="Steinberger and Pouliquen, 2007" startWordPosition="761" endWordPosition="764">he categorization parts. 2.1 Named Entity Recognition and Variant Matching. Named Entity Recognition (NER) is performed using manually constructed languageindependent rules that make use of languagespecific lists of trigger words such as titles (president), professions or occupations (tennis player, playboy), references to countries, regions, ethnic or religious groups (French, Bavarian, Berber, Muslim), age expressions (57-year-old), verbal phrases (deceased), modifiers (former) and more. These patterns can also occur in combination and patterns can be nested to capture more complex titles, (Steinberger and Pouliquen, 2007). In order to be able to cover many different languages, no other dictionaries and no parsers or part-of-speech taggers are used. To identify which of the names newly found every day are new entities and which ones are merely variant spellings of entities already contained in the database, we apply a languageindependent name similarity measure to decide which name variants should be automatically merged, for details see (Pouliquen and Steinberger, 2009). This allows us to maintain a database containing over 1,15 million named entities and 200,000 variants. The major part of this resource can b</context>
</contexts>
<marker>Steinberger, Pouliquen, 2007</marker>
<rawString>R. Steinberger and B. Pouliquen. 2007. Cross-lingual named entity recognition. Lingvisticæ Investigationes, 30(1) pages 135–162. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>B Pouliquen</author>
<author>A Widiger</author>
<author>C Ignat</author>
<author>T Erjavec</author>
<author>D Tufis¸</author>
<author>D Varga</author>
</authors>
<title>The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages.</title>
<date>2006</date>
<booktitle>Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>2142--2147</pages>
<location>Genova, Italy.</location>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis¸, Varga, 2006</marker>
<rawString>R. Steinberger and B. Pouliquen and A. Widiger and C. Ignat and T. Erjavec and D. Tufis¸ and D. Varga. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 2142–2147. Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>B Pouliquen</author>
<author>E van der Goot</author>
</authors>
<title>An Introduction to the Europe Media Monitor Family of Applications.</title>
<date>2009</date>
<booktitle>Proceedings of the Information Access in a Multilingual World-Proceedings of the SIGIR 2009 Workshop,</booktitle>
<pages>1--8</pages>
<location>Boston, USA.</location>
<marker>Steinberger, Pouliquen, van der Goot, 2009</marker>
<rawString>R. Steinberger and B. Pouliquen and E. van der Goot. 2009. An Introduction to the Europe Media Monitor Family of Applications. Proceedings of the Information Access in a Multilingual World-Proceedings of the SIGIR 2009 Workshop, pages 1–8. Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>News from OPUS-A Collection of Multilingual Parallel Corpora with Tools and Interfaces. Recent advances in natural language processing V: selected papers from RANLP</title>
<date>2009</date>
<pages>309--237</pages>
<contexts>
<context position="8993" citStr="Tiedemann, 2009" startWordPosition="1393" endWordPosition="1394">per sentence). Although, the hierarchical model has the best Bleu score, we prefer to use the PBSMT system in our translation service, because it is four times faster. Which training data can we use? It is known in statistical machine translation that more training data implies better translation. Although, the number of parallel corpora has been is growing in the last years, the amounts of training data vary from language pair to language pair. To train our models we use the freely available corpora (when possible): Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), DGTTM3, Opus (Tiedemann, 2009), SE-Times (Tyers and Alperen, 2010), Tehran English-Persian Parallel Corpus (Pilevar et al., 2011), News Corpus (Callison-Burch et al., 2009), UN Corpus (Rafalovitch and Dale, 2009), CzEng0.9 (Bojar and ˇZabokrtsk´y, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5. This results in some language pairs with a large coverage, (more than 4 million sentences), and other with a very small coverage, (less than 1 million). The language models are trained using 12 model sentences for the content model and 4.7 million for the title model. B</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J. Tiedemann. 2009. News from OPUS-A Collection of Multilingual Parallel Corpora with Tools and Interfaces. Recent advances in natural language processing V: selected papers from RANLP 2007, pages 309:237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turchi</author>
<author>I Flaounas</author>
<author>O Ali</author>
<author>T DeBie</author>
<author>T Snowsill</author>
<author>N Cristianini</author>
</authors>
<title>Found in translation.</title>
<date>2009</date>
<booktitle>Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>746--749</pages>
<location>Bled, Slovenia.</location>
<contexts>
<context position="2906" citStr="Turchi et al., 2009" startWordPosition="440" endWordPosition="443">nslation toolkit Moses (Koehn et al., 2007), trained mostly on freely available parallel corpora and optimised for the news domain, as stated above. The main objective of developing our inhouse system is thus not to improve translation quality over the existing services (this would be beyond our possibilities), but to offer our users a rough translation (a “gist”) that allows them to get an idea of the main contents of the article and to determine whether the news item at hand is relevant for their field of interest or not. A similar news-focused translation service is “Found in Translation” (Turchi et al., 2009), which gathers articles in 23 languages and translates them into English. “Found in Translation” is also based on Moses, but it categorises the news after translation and the translation process is not optimised for the news domain. 2 Europe Media Monitor Europe Media Monitor (EMM)2 gathers a daily average of 100,000 news articles in approximately 50 languages, from about 3,400 hand-selected web news sources, from a couple of hundred specialist and government websites, as well as from about twenty commercial news providers. It visits the news web sites up to every five minutes to 2http://emm.</context>
</contexts>
<marker>Turchi, Flaounas, Ali, DeBie, Snowsill, Cristianini, 2009</marker>
<rawString>M. Turchi and I. Flaounas and O. Ali and T. DeBie and T. Snowsill and N. Cristianini. 2009. Found in translation. Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, pages 746–749. Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Tyers</author>
<author>M S Alperen</author>
</authors>
<title>South-East European Times: A parallel corpus of Balkan languages.</title>
<date>2010</date>
<booktitle>Proceedings of the LREC workshop on Exploitation of</booktitle>
<location>Valletta,</location>
<contexts>
<context position="9029" citStr="Tyers and Alperen, 2010" startWordPosition="1396" endWordPosition="1400"> hierarchical model has the best Bleu score, we prefer to use the PBSMT system in our translation service, because it is four times faster. Which training data can we use? It is known in statistical machine translation that more training data implies better translation. Although, the number of parallel corpora has been is growing in the last years, the amounts of training data vary from language pair to language pair. To train our models we use the freely available corpora (when possible): Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), DGTTM3, Opus (Tiedemann, 2009), SE-Times (Tyers and Alperen, 2010), Tehran English-Persian Parallel Corpus (Pilevar et al., 2011), News Corpus (Callison-Burch et al., 2009), UN Corpus (Rafalovitch and Dale, 2009), CzEng0.9 (Bojar and ˇZabokrtsk´y, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5. This results in some language pairs with a large coverage, (more than 4 million sentences), and other with a very small coverage, (less than 1 million). The language models are trained using 12 model sentences for the content model and 4.7 million for the title model. Both sets are extracted from English </context>
</contexts>
<marker>Tyers, Alperen, 2010</marker>
<rawString>F. Tyers and M.S. Alperen. 2010. South-East European Times: A parallel corpus of Balkan languages. Proceedings of the LREC workshop on Exploitation of multilingual resources and tools for Central and (South) Eastern European Languages, Valletta, Malta.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>