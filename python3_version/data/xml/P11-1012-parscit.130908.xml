<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.984268">
Query Weighting for Ranking Model Adaptation
</title>
<author confidence="0.900606">
Peng Cai&apos;, Wei Gao2, Aoying Zhou&apos;, and Kam-Fai Wong2,3
</author>
<affiliation confidence="0.783307666666667">
&apos;East China Normal University, Shanghai, China
pengcai2010@gmail.com, ayzhou@sei.ecnu.edu.cn
2The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
</affiliation>
<email confidence="0.883612">
{wgao, kfwong}@se.cuhk.edu.hk
</email>
<note confidence="0.536274">
3Key Laboratory of High Confidence Software Technologies, Ministry of Education, China
</note>
<sectionHeader confidence="0.993398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999753115384616">
We propose to directly measure the impor-
tance of queries in the source domain to the
target domain where no rank labels of doc-
uments are available, which is referred to
as query weighting. Query weighting is a
key step in ranking model adaptation. As
the learning object of ranking algorithms is
divided by query instances, we argue that
it’s more reasonable to conduct importance
weighting at query level than document level.
We present two query weighting schemes.
The first compresses the query into a query
feature vector, which aggregates all document
instances in the same query, and then con-
ducts query weighting based on the query fea-
ture vector. This method can efficiently esti-
mate query importance by compressing query
data, but the potential risk is information loss
resulted from the compression. The second
measures the similarity between the source
query and each target query, and then com-
bines these fine-grained similarity values for
its importance estimation. Adaptation exper-
iments on LETOR3.0 data set demonstrate
that query weighting significantly outperforms
document instance weighting methods.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999881170731707">
Learning to rank, which aims at ranking documents
in terms of their relevance to user’s query, has been
widely studied in machine learning and information
retrieval communities (Herbrich et al., 2000; Fre-
und et al., 2004; Burges et al., 2005; Yue et al.,
2007; Cao et al., 2007; Liu, 2009). In general,
large amount of training data need to be annotated
by domain experts for achieving better ranking per-
formance. In real applications, however, it is time
consuming and expensive to annotate training data
for each search domain. To alleviate the lack of
training data in the target domain, many researchers
have proposed to transfer ranking knowledge from
the source domain with plenty of labeled data to the
target domain where only a few or no labeled data is
available, which is known as ranking model adapta-
tion (Chen et al., 2008a; Chen et al., 2010; Chen et
al., 2008b; Geng et al., 2009; Gao et al., 2009).
Intuitively, the more similar an source instance
is to the target instances, it is expected to be more
useful for cross-domain knowledge transfer. This
motivated the popular domain adaptation solution
based on instance weighting, which assigns larger
weights to those transferable instances so that the
model trained on the source domain can adapt more
effectively to the target domain (Jiang and Zhai,
2007). Existing instance weighting schemes mainly
focus on the adaptation problem for classification
(Zadrozny, 2004; Huang et al., 2007; Jiang and Zhai,
2007; Sugiyama et al., 2008).
Although instance weighting scheme may be ap-
plied to documents for ranking model adaptation,
the difference between classification and learning to
rank should be highlighted to take careful consider-
ation. Compared to classification, the learning ob-
ject for ranking is essentially a query, which con-
tains a list of document instances each with a rel-
evance judgement. Recently, researchers proposed
listwise ranking algorithms (Yue et al., 2007; Cao
et al., 2007) to take the whole query as a learning
object. The benchmark evaluation showed that list-
</bodyText>
<page confidence="0.97046">
112
</page>
<note confidence="0.9872575">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 112–122,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.99011318367347">
Source Domain Target domain Source Domain Target domain
(s1)
d1 (s1) d2
d(s1)
3
d(s2)
1
d(s2)
2
d(s2)
3
d(t1)
1
d(t2)
2
d(t1)
3
d(t2)
3
2
1
qs1
qs2
d(s1)
1
d(s1)
2
d(s1)
3
d1(s2)
d(s2)
2
d(s2)
3
d(t1)
1
d(t1)
3
d(t1)
2
d(t2)
1
d(t2)
2
d(t2)
3
qt1
qt2
(a) Instance based weighting (b) Query based weighting
</figure>
<figureCaption confidence="0.998264333333333">
Figure 1: The information about which document instances belong to the same query is lost in document instance
weighting scheme. To avoid losing this information, query weighting takes the query as a whole and directly measures
its importance.
</figureCaption>
<bodyText confidence="0.9995064375">
wise approach significantly outperformed pointwise
approach, which takes each document instance as in-
dependent learning object, as well as pairwise ap-
proach, which concentrates learning on the order of
a pair of documents (Liu, 2009). Inspired by the
principle of listwise approach, we hypothesize that
the importance weighting for ranking model adapta-
tion could be done better at query level rather than
document level.
Figure 1 demonstrates the difference between in-
stance weighting and query weighting, where there
are two queries qs1 and qs2 in the source domain
and qt1 and qt2 in the target domain, respectively,
and each query has three retrieved documents. In
Figure 1(a), source and target domains are repre-
sented as a bag of document instances. It is worth
noting that the information about which document
instances belong to the same query is lost. To
avoid this information loss, query weighting scheme
shown as Figure 1(b) directly measures importance
weight at query level.
Instance weighting makes the importance estima-
tion of document instances inaccurate when docu-
ments of the same source query are similar to the
documents from different target queries. Take Fig-
ure 2 as a toy example, where the document in-
stance is represented as a feature vector with four
features. No matter what weighting schemes are
used, it makes sense to assign high weights to source
queries qs1 and qs2 because they are similar to tar-
get queries qt1 and qt2, respectively. Meanwhile, the
source query qs3 should be weighted lower because
</bodyText>
<figureCaption confidence="0.999469">
Figure 2: A toy example showing the problem of docu-
ment instance weighting scheme.
</figureCaption>
<bodyText confidence="0.999848071428572">
it’s not quite similar to any of qt1 and qt2 at query
level, meaning that the ranking knowledge from qs3
is different from that of qt1 and qt2 and thus less
useful for the transfer to the target domain. Unfor-
tunately, the three source queries qs1, qs2 and qs3
would be weighted equally by document instance
weighting scheme. The reason is that all of their
documents are similar to the two document instances
in target domain despite the fact that the documents
of qs3 correspond to their counterparts from different
target queries.
Therefore, we should consider the source query
as a whole and directly measure the query impor-
tance. However, it’s not trivial to directly estimate
</bodyText>
<equation confidence="0.9964856">
qt1
qs1
&lt;d1s1&gt;=( 5, 1, 0 ,0 )
&lt;d2s1&gt;=( 6, 2, 0 ,0 )
&lt;d1 t1&gt;=(5, 1, 0 ,0 )
&lt;d2 t1&gt;=(6, 2, 0 ,0 )
&lt;d1 s2&gt;=( 0, 0, 5, 1)
&lt;d2 s2&gt;=( 0, 0, 6, 2)
&lt;d1 s3&gt;=( 5, 1, 0, 0)
&lt;d2 s3&gt;=( 0, 0, 6, 2)
&lt;d1 t2&gt;=( 0, 0, 5, 1)
&lt;d2 t2&gt;=( 0, 0, 6, 2)
qs3
qs2
qt2
</equation>
<page confidence="0.990886">
113
</page>
<bodyText confidence="0.999934166666667">
a query’s weight because a query is essentially pro-
vided as a matrix where each row represents a vector
of document features. In this work, we present two
simple but very effective approaches attempting to
resolve the problem from distinct perspectives: (1)
we compress each query into a query feature vec-
tor by aggregating all of its document instances, and
then conduct query weighting on these query feature
vectors; (2) we measure the similarity between the
source query and each target query one by one, and
then combine these fine-grained similarity values to
calculate its importance to the target domain.
</bodyText>
<sectionHeader confidence="0.990343" genericHeader="method">
2 Instance Weighting Scheme Review
</sectionHeader>
<bodyText confidence="0.999000846153846">
The basic idea of instance weighting is to put larger
weights on source instances which are more simi-
lar to target domain. As a result, the key problem
is how to accurately estimate the instance’s weight
indicating its importance to target domain. (Jiang
and Zhai, 2007) used a small number of labeled data
from target domain to weight source instances. Re-
cently, some researchers proposed to weight source
instance only using unlabeled target instances (Shi-
modaira, 2000; Sugiyama et al., 2008; Huang et al.,
2007; Zadrozny, 2004; Gao et al., 2010). In this
work, we also focus on weighting source queries
only using unlabeled target queries.
(Gao et al., 2010; Ben-David et al., 2010) pro-
posed to use a classification hyperplane to separate
source instances from target instances. With the do-
main separator, the probability that a source instance
is classified to target domain can be used as the im-
portance weight. Other instance weighting methods
were proposed for the sample selection bias or co-
variate shift in the more general setting of classifier
learning (Shimodaira, 2000; Sugiyama et al., 2008;
Huang et al., 2007; Zadrozny, 2004). (Sugiyama et
al., 2008) used a natural model selection procedure,
referred to as Kullback-Leibler divergence Impor-
tance Estimation Procedure (KLIEP), for automat-
ically tuning parameters, and showed that its impor-
tance estimation was more accurate. The main idea
is to directly estimate the density function ratio of
target distribution pt(x) to source distribution ps(x),
i.e. w(x) = pt(x)Then model w(x) can be used to
p.(x) .
estimate the importance of source instances. Model
parameters were computed with a linear model by
minimizing the KL-divergence from pt(x) to its esti-
mator Pt(x). Since Pt(x) = w(x)ps(x), the ultimate
objective only contains model w(x).
For using instance weighting in pairwise rank-
ing algorithms, the weights of document instances
should be transformed into those of document
pairs (Gao et al., 2010). Given a pair of documents
(xi, xj) and their weights wi and wj, the pairwise
weight wij could be estimated probabilistically as
wi * wj. To consider query factor, query weight was
further estimated as the average value of the weights
over all the pairs, i.e., wq = M Ei,j wij, where M
is the number of pairs in query q. Additionally, to
take the advantage of both query and document in-
formation, a probabilistic weighting for (xi, xj) was
modeled by wq * wij. Through the transformation,
instance weighting schemes for classification can be
applied to ranking model adaptation.
</bodyText>
<sectionHeader confidence="0.978558" genericHeader="method">
3 Query Weighting
</sectionHeader>
<bodyText confidence="0.999960875">
In this section, we extend instance weighting to di-
rectly estimate query importance for more effec-
tive ranking model adaptation. We present two
query weighting methods from different perspec-
tives. Note that although our methods are based on
domain separator scheme, other instance weighting
schemes such as KLIEP (Sugiyama et al., 2008) can
also be extended similarly.
</bodyText>
<subsectionHeader confidence="0.9532145">
3.1 Query Weighting by Document Feature
Aggregation
</subsectionHeader>
<bodyText confidence="0.985975944444444">
Our first query weighting method is inspired by the
recent work on local learning for ranking (Geng et
al., 2008; Banerjee et al., 2009). The query can be
compressed into a query feature vector, where each
feature value is obtained by the aggregate of its cor-
responding features of all documents in the query.
We concatenate two types of aggregates to construct
the query feature vector: the mean µ⃗ = |e1 E�e |1 ⃗fi
�|q|
and the variance σ⃗ = 1 i=1(⃗fi − ⃗µ)2, where ⃗fi
|q|
is the feature vector of document i and |q |denotes
the number of documents in q . Based on the ag-
gregation of documents within each query, we can
use a domain separator to directly weight the source
queries with the set of queries from both domains.
Given query data sets Ds = {qisJm i=1 and Dt =
{qjt Jnj=1 respectively from the source and target do-
</bodyText>
<page confidence="0.992114">
114
</page>
<figure confidence="0.327411">
Algorithm 1 Query Weighting Based on Document Feature Aggregation in the Query
Input:
Queries in the source domain, Ds = {qi s}m i=1;
Queries in the target domain, Dt = {qjt }nj=1;
Output:
Importance weights of queries in the source domain, IWs = {Wi}mi=1;
</figure>
<listItem confidence="0.992991647058823">
1: ys = −1, yt = +1;
2: for i = 1; i ≤ m; i + + do
3: Calculate the mean vector ⃗µi and variance vector ⃗σi for qis;
4: Add query feature vector ⃗qis = (⃗µi, ⃗σi, ys) to D′ s ;
5: end for
6: for j = 1; j ≤ n; j + + do
7: Calculate the mean vector ⃗µj and variance vector ⃗σj for qjt ;
8: Add query feature vector ⃗qjt = (⃗µj, ⃗σj, yt) to D′t;
9: end for
10: Find classification hyperplane Hst which separates D′s from D′t;
11: for i = 1; i ≤ m; i + + do
12: Calculate the distance of ⃗qis to Hst, denoted as L(⃗qis);
13: Wi = P(qi 1
s ∈ Dt) = 1+exp(α∗L(⃗qis)+β)
14: Add Wi to IWs;
15: end for
16: return IWs;
</listItem>
<bodyText confidence="0.999872">
mains, we use algorithm 1 to estimate the proba-
bility that the query qis can be classified to Dt, i.e.
P(qis ∈ Dt), which can be used as the importance of
qis relative to the target domain. From step 1 to 9, D′s
and D′t are constructed using query feature vectors
from source and target domains. Then, a classifi-
cation hyperplane Hst is used to separate D′s from
D′t in step 10. The distance of the query feature
vector ⃗qis from Hst are transformed to the probabil-
ity P(qis ∈ Dt) using a sigmoid function (Platt and
Platt, 1999).
</bodyText>
<subsectionHeader confidence="0.9993715">
3.2 Query Weighting by Comparing Queries
across Domains
</subsectionHeader>
<bodyText confidence="0.990530714285714">
Although the query feature vector in algorithm 1 can
approximate a query by aggregating its documents’
features, it potentially fails to capture important fea-
ture information due to the averaging effect during
the aggregation. For example, the merit of features
in some influential documents may be canceled out
in the mean-variance calculation, resulting in many
distorted feature values in the query feature vector
that hurts the accuracy of query classification hy-
perplane. This urges us to propose another query
weighting method from a different perspective of
query similarity.
Intuitively, the importance of a source query to
the target domain is determined by its overall sim-
ilarity to every target query. Based on this intu-
ition, we leverage domain separator to measure the
similarity between a source query and each one of
the target queries, where an individual domain sep-
arator is created for each pair of queries. We esti-
mate the weight of a source query using algorithm 2.
Note that we assume document instances in the same
query are conditionally independent and all queries
are independent of each other. In step 3, D′ i is con-
qs
structed by all the document instances {⃗xk} in query
qis with the domain label ys. For each target query
qt , we use the classification hyperplane Hij to es-
timate P(⃗xk ∈ D′j), i.e. the probability that each
qt
document ⃗xk of qis is classified into the document set
of qjt (step 8). Then the similarity between qis and qjt
is measured by the probability P(qis ∼ qjt) at step 9.
Finally, the probability of qis belonging to the target
domain P(qis ∈ Dt) is calculated at step 11.
It can be expected that algorithm 2 will generate
</bodyText>
<page confidence="0.987108">
115
</page>
<figure confidence="0.724029333333333">
Algorithm 2 Query Weighting by Comparing Source and Target Queries
Input:
Queries in source domain, Ds = {qi s}m i=1;
Queries in target domain, Dt = {qjt }n j=1;
Output:
Importance weights of queries in source domain, IWs = {Wi}mi=1;
1: ys = −1, yt = +1;
2: for i = 1; i ≤ m; i + + do
3: Set D′qis={⃗xk, ys)}|qis|
</figure>
<equation confidence="0.9834398">
k=1;
4: for j = 1; j ≤ n; j + + do
={⃗xk′, yt)}|qj t |
qj k′=1;
t
</equation>
<listItem confidence="0.9627935">
6: Find a classification hyperplane Hij which separates D′qis from D′ qj
t ;
7: For each k, calculate the distance of ⃗xk to Hij, denoted as L(⃗xk);
8: For each k, calculate P(
</listItem>
<equation confidence="0.95422225">
k
qjt) 1+exp(α∗L(⃗xk)+β);
Px ED
9: Calculate P(qis ∼ qjt) = |qis|EIk s|1 (k et );
⃗x
∈
′
11: Add
= P(
Dt) = n En 1 P(q
i
qis∈
is∼
jt
IWs;
5: Set D′
</equation>
<bodyText confidence="0.944120142857143">
es of two domains. ows:
116 be constructed based on the ground truth rank labels.
Given ranking function f, the objective of RSVM is
presented as foll
where
and
are two documents with dif-
</bodyText>
<figure confidence="0.398124172413793">
ferent rank label, and
+1 if
more relevant than
or
otherwise.
Let
= 21C and replace
with Hinge Loss func-
tion (.)+, Equation 1 can be turn
⃗xj(1)
qi
⃗xj(2)
qi
zij=
⃗xj(1)
qi is labeled
⃗xj(2)
qi ;
zij =−1
λ
ξij
ed to the following
form:
−zij∗f(⃗w, ⃗xj(1) qi − ⃗xj(2)
qi ))+
10: end for
) to IWs;
12: end for
13: return
</figure>
<bodyText confidence="0.947656555555556">
more precise measures of query similarity by utiliz-
ing the more fine-grained classification hyperplane
for separating the queri
4 Ranking Model Adaptation via Query
Weighting
To adapt the source ranking model to the target do-
main, we need to incorporate query weights into ex-
isting ranking algorithms. Note that query weights
can be integrated with either pairwise or listwise al-
gorithms. For pairwise algorithms, a straightforward
way is to assign the query weight to all the document
pairs associated with this query. However, document
instance weighting cannot be appropriately utilized
in listwise approach. In order to compare query
weighting with document instance weighting, we
need to fairly apply them for the same approach of
ranking. Therefore, we choose pairwise approach to
incorporate query weighting. In this section, we ex-
tend Ranking SVM (RSVM) (Herbrich et al., 2000;
Joachims, 2002) —one of the typical pairwise algo-
rithms for this.
assume there are m queries in the data set
of source domain, and for each query
there are
number of meaningful document pairs that can
Let’s
qi
</bodyText>
<equation confidence="0.8826542">
ℓ(qi)
ξij (1)
subject to
zij∗f(⃗w,⃗xj(1) qi − ⃗xj(2)
qi ) ≥ 1 − ξij
ξij ≥ 0, i = 1, . . . , m; j = 1, . . . , ℓ(qi)
ℓ(qi) �
j=1
(1
(2)
</equation>
<bodyText confidence="0.908947625">
Let
represent the importance weight of
source query
Equation 2 is extended for inte-
grating the query
IW(qi)
qi.
weight into the loss function in a
</bodyText>
<equation confidence="0.987283">
min2||⃗w||2 + C �m ℓ(qi) �
1 i=1 j=1
min λ||⃗w||2+
�m
i=1
straightforward way:
min λ||⃗V||2+
(1 − zij ∗ f(⃗V, ⃗xj(1) — xj(2)))+
qz qz
</equation>
<bodyText confidence="0.999926">
where IW(.) takes any one of the weighting
schemes given by algorithm 1 and algorithm 2.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999975071428571">
We evaluated the proposed two query weighting
methods on TREC-2003 and TREC-2004 web track
datasets, which were released through LETOR3.0 as
a benchmark collection for learning to rank by (Qin
et al., 2010). Originally, different query tasks were
defined on different parts of data in the collection,
which can be considered as different domains for us.
Adaptation takes place when ranking tasks are per-
formed by using the models trained on the domains
in which they were originally defined to rank the
documents in other domains. Our goal is to demon-
strate that query weighting can be more effective
than the state-of-the-art document instance weight-
ing.
</bodyText>
<subsectionHeader confidence="0.98957">
5.1 Datasets and Setup
</subsectionHeader>
<bodyText confidence="0.999983666666667">
Three query tasks were defined in TREC-2003 and
TREC-2004 web track, which are home page finding
(HP), named page finding (NP) and topic distilla-
tion (TD) (Voorhees, 2003; Voorhees, 2004). In this
dataset, each document instance is represented by 64
features, including low-level features such as term
frequency, inverse document frequency and docu-
ment length, and high-level features such as BM25,
language-modeling, PageRank and HITS. The num-
ber of queries of each task is given in Table 1.
The baseline ranking model is an RSVM directly
trained on the source domain without using any
weighting methods, denoted as no-weight. We im-
plemented two weighting measures based on do-
main separator and Kullback-Leibler divergence, re-
ferred to DS and KL, respectively. In DS measure,
three document instance weighting methods based
on probability principle (Gao et al., 2010) were
implemented for comparison, denoted as doc-pair,
doc-avg and doc-comb (see Section 2). In KL mea-
sure, there is no probabilistic meaning for KL weight
</bodyText>
<table confidence="0.99744525">
Query Task TREC 2003 TREC 2004
Topic Distillation 50 75
Home Page finding 150 75
Named Page finding 150 75
</table>
<tableCaption confidence="0.9945415">
Table 1: The number of queries in TREC-2003 and
TREC-2004 web track
</tableCaption>
<bodyText confidence="0.999929785714286">
and the doc-comb based on KL is not interpretable,
and we only present the results of doc-pair and doc-
avg for KL measure. Our proposed query weight-
ing methods are denoted by query-aggr and query-
comp, corresponding to document feature aggrega-
tion in query and query comparison across domains,
respectively. All ranking models above were trained
only on source domain training data and the labeled
data of target domain was just used for testing.
For training the models efficiently, we imple-
mented RSVM with Stochastic Gradient Descent
(SGD) optimizer (Shalev-Shwartz et al., 2007). The
reported performance is obtained by five-fold cross
validation.
</bodyText>
<subsectionHeader confidence="0.998277">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999979142857143">
The task of HP and NP are more similar to
each other whereas HP/NP is rather different from
TD (Voorhees, 2003; Voorhees, 2004). Thus,
we carried out HP/NP to TD and TD to HP/NP
ranking adaptation tasks. Mean Average Precision
(MAP) (Baeza-Yates and Ribeiro-Neto, 1999) is
used as the ranking performance measure.
</bodyText>
<subsectionHeader confidence="0.704489">
5.2.1 Adaptation from HP/NP to TD
</subsectionHeader>
<bodyText confidence="0.999972333333333">
The first set of experiments performed adaptation
from HP to TD and NP to TD. The results of MAP
are shown in Table 2.
For the DS-based measure, as shown in the table,
query-aggr works mostly better than no-weight,doc-
pair, doc-avg and doc-comb, and query-comp per-
forms the best among the five weighting methods.
T-test on MAP indicates that the improvement of
query-aggr over no-weight is statistically significant
on two adaptation tasks while the improvement of
document instance weighting over no-weight is sta-
tistically significant only on one task. All of the
improvement of query-comp over no-weight, doc-
pair,doc-avg and doc-comb are statistically signifi-
cant. This demonstrates the effectiveness of query
</bodyText>
<equation confidence="0.988006333333333">
m
IW (qi) ∗ ℓ(qz) �
i=1 j=1
</equation>
<page confidence="0.996831">
117
</page>
<table confidence="0.99979475">
Model Weighting method HP03 to TD03 HP04 to TD04 NP03 to TD03 NP04 to TD04
no-weight 0.2508 0.2086 0.1936 0.1756
doc-pair 0.2505 0.2042 0.19821 0.1708
doc-avg 0.2514 0.2019 0.21221$ 0.1716
DS doc-comb 0.2562 0.2051 0.22241$0 0.1793
query-aggr 0.2573 0.21061$0 0.2088 0.18081$0
query-comp 0.28161$0 0.21471$0 0.23921$0 0.18611$0
doc-pair 0.2521 0.2048 0.1901 0.1761
doc-avg 0.2534 0.21271 0.1904 0.1777
KL doc-comb - - - -
query-aggr 0.1890 0.1901 0.1870 0.1643
query-comp 0.25481 0.21421 0.23131$0 0.18071
</table>
<tableCaption confidence="0.9956105">
Table 2: Results of MAP for HP/NP to TD adaptation. †, ‡, ♯ and boldface indicate significantly better than no-weight,
doc-pair, doc-avg and doc-comb, respectively. Confidence level is set at 95%
</tableCaption>
<bodyText confidence="0.99943624">
weighting compared to document instance weight-
ing.
Furthermore, query-comp can perform better than
query-aggr. The reason is that although document
feature aggregation might be a reasonable represen-
tation for a set of document instances, it is possible
that some information could be lost or distorted in
the process of compression. By contrast, more ac-
curate query weights can be achieved by the more
fine-grained similarity measure between the source
query and all target queries in algorithm 2.
For the KL-based measure, similar observation
can be obtained. However, it’s obvious that DS-
based models can work better than the KL-based.
The reason is that KL conducts weighting by density
function ratio which is sensitive to the data scale.
Specifically, after document feature aggregation, the
number of query feature vectors in all adaptation
tasks is no more than 150 in source and target do-
mains. It renders the density estimation in query-
aggr is very inaccurate since the set of samples is
too small. As each query contains 1000 documents,
they seemed to provide query-comp enough samples
for achieving reasonable estimation of the density
functions in both domains.
</bodyText>
<subsectionHeader confidence="0.69798">
5.2.2 Adaptation from TD to HP/NP
</subsectionHeader>
<bodyText confidence="0.999836909090909">
To further validate the effectiveness of query
weighting, we also conducted adaptation from TD
to HP and TD to NP . MAP results with significant
test are shown in Table 3.
We can see that document instance weighting
schemes including doc-pair, doc-avg and doc-comb
can not outperform no-weight based on MAP mea-
sure. The reason is that each query in TD has 1000
retrieved documents in which 10-15 documents are
relevant whereas each query in HP or NP only con-
sists 1-2 relevant documents. Thus, when TD serves
as the source domain, it leads to the problem that
too many document pairs were generated for train-
ing the RSVM model. In this case, a small number
of documents that were weighted inaccurately can
make significant impact on many number of docu-
ment pairs. Since query weighting method directly
estimates the query importance instead of document
instance importance, both query-aggr and query-
comp can avoid such kind of negative influence that
is inevitable in the three document instance weight-
ing methods.
</bodyText>
<subsectionHeader confidence="0.857568">
5.2.3 The Analysis on Source Query Weights
</subsectionHeader>
<bodyText confidence="0.999970846153846">
An interesting problem is which queries in the
source domain are assigned high weights and why
it’s the case. Query weighting assigns each source
query with a weight value. Note that it’s not mean-
ingful to directly compare absolute weight values
between query-aggr and query-comp because source
query weights from distinct weighting methods have
different range and scale. However, it is feasible
to compare the weights with the same weighting
method. Intuitively, if the ranking model learned
from a source query can work well in target do-
main, it should get high weight. According to this
intuition, if ranking models fq1 and fq2 are learned
</bodyText>
<page confidence="0.993653">
118
</page>
<table confidence="0.995978083333333">
model weighting scheme TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04
no-weight 0.6986 0.6158 0.5053 0.5427
doc-pair 0.6588 0.6235† 0.4878 0.5212
doc-avg 0.6654 0.6200 0.4736 0.5035
DS doc-comb 0.6932 0.6214† 0.4974 0.5077
query-aggr 0.7179†‡♯ 0.6292†‡♯ 0.5198†‡♯ 0.5551†‡♯
query-comp 0.7297†‡♯ 0.6499†‡♯ 0.5203†‡♯ 0.6541†‡♯
doc-pair 0.6480 0.6107 0.4633 0.5413
doc-avg 0.6472 0.6132 0.4626 0.5406
KL doc-comb – – – –
query-aggr 0.6263 0.5929 0.4597 0.4673
query-comp 0.6530‡♯ 0.6358†‡♯ 0.4726 0.5559†‡♯
</table>
<tableCaption confidence="0.995637">
Table 3: Results of MAP for TD to HP/NP adaptation. †, ‡, ♯ and boldface indicate significantly better than no-weight,
doc-pair, doc-avg and doc-comb, respectively. Confidence level is set as 95%.
</tableCaption>
<bodyText confidence="0.997736388888889">
from queries qs and qs respectively, and fq1 per-
forms better than fq2, then the source query weight
of qs should be higher than that of qs.
For further analysis, we compare the weight val-
ues between each source query pair, for which we
trained RSVM on each source query and evaluated
the learned model on test data from target domain.
Then, the source queries are ranked according to the
MAP values obtained by their corresponding rank-
ing models. The order is denoted as Rmap. Mean-
while, the source queries are also ranked with re-
spect to their weights estimated by DS-based mea-
sure, and the order is denoted as Rweight. We hope
Rweight is correlated as positively as possible with
Rmap. For comparison, we also ranked these queries
according to randomly generated query weights,
which is denoted as query-rand in addition to query-
aggr and query-comp. The Kendall’s T = P −Q
</bodyText>
<equation confidence="0.846024">
P �Q
</equation>
<bodyText confidence="0.999180769230769">
is used to measure the correlation (Kendall, 1970),
where P is the number of concordant query pairs
and Q is the number of discordant pairs. It’s
noted that T’s range is from -1 to 1, and the larger
value means the two ranking is better correlated.
The Kendall’s T by different weighting methods are
given in Table 4 and 5.
We find that Rweight produced by query-aggr and
query-comp are all positively correlated with Rmap
and clearly the orders generated by query-comp are
more positive than those by query-aggr. This is
another explanation why query-comp outperforms
query-aggr. Furthermore, both are far better than
</bodyText>
<table confidence="0.8807435">
weighting TD03 to HP03 TD04 to HP04
doc-pair 28,835 secs 21,640 secs
query-aggr 182 secs 123 secs
query-comp 15,056 secs 10,081 secs
</table>
<tableCaption confidence="0.996476">
Table 6: The efficiency of weighting in seconds.
</tableCaption>
<bodyText confidence="0.94831">
query-rand because the Rweight by query-rand is ac-
tually independent of Rmap.
</bodyText>
<subsectionHeader confidence="0.701279">
5.2.4 Efficiency
</subsectionHeader>
<bodyText confidence="0.999968428571429">
In the situation where there are large scale data in
source and target domains, how to efficiently weight
a source query is another interesting problem. With-
out the loss of generality, we reported the weighting
time of doc-pair, query-aggr and query-comp from
adaptation from TD to HP using DS measure. As
doc-avg and doc-comb are derived from doc-pair,
their efficiency is equivalent to doc-pair.
As shown in table 6, query-aggr can efficiently
weight query using query feature vector. The reason
is two-fold: one is the operation of query document
aggregation can be done very fast, and the other is
there are 1000 documents in each query of TD or HP,
which means that the compression ratio is 1000:1.
Thus, the domain separator can be found quickly. In
addition, query-comp is more efficient than doc-pair
because doc-pair needs too much time to find the
separator using all instances from source and target
domain. And query-comp uses a divide-and-conquer
method to measure the similarity of source query to
each target query, and then efficiently combine these
</bodyText>
<page confidence="0.98116">
119
</page>
<bodyText confidence="0.6098375">
Weighting method HP03 to TD03 HP04 to TD04 NP03 to TD03 NP04 to TD04
query-aggr 0.0906 0.0280 0.0247 0.0525
query-comp 0.1001 0.0804 0.0711 0.1737
query-rand 0.0041 0.0008 -0.0127 0.0163
</bodyText>
<tableCaption confidence="0.996403">
Table 4: The Kendall’s T of Rf11,ight and R,,,,ap in HP/NP to TD adaptation.
</tableCaption>
<table confidence="0.884038">
Weighting method TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04
query-aggr 0.1172 0.0121 0.0574 0.0464
query-comp 0.1304 0.1393 0.1586 0.0545
query-rand −0.0291 0.0022 0.0161 -0.0262
</table>
<tableCaption confidence="0.998715">
Table 5: The Kendall’s T of RU1,ight and R„Lap in TD to HP/NP adaptation.
</tableCaption>
<bodyText confidence="0.460684">
fine-grained similarity values.
</bodyText>
<sectionHeader confidence="0.999916" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999921315789474">
Cross-domain knowledge transfer has became an
important topic in machine learning and natural lan-
guage processing (Ben-David et al., 2010; Jiang
and Zhai, 2007; Blitzer et al., 2006; Daum´e III
and Marcu, 2006). (Blitzer et al., 2006) pro-
posed model adaptation using pivot features to build
structural feature correspondence in two domains.
(Pan et al., 2009) proposed to seek a common fea-
tures space to reduce the distribution difference be-
tween the source and target domain. (Daum´e III and
Marcu, 2006) assumed training instances were gen-
erated from source domain, target domain and cross-
domain distributions, and estimated the parameter
for the mixture distribution.
Recently, domain adaptation in learning to rank
received more and more attentions due to the lack
of training data in new search domains. Existing
ranking adaptation approaches can be grouped into
feature-based (Geng et al., 2009; Chen et al., 2008b;
Wang et al., 2009; Gao et al., 2009) and instance-
based (Chen et al., 2010; Chen et al., 2008a; Gao et
al., 2010) approaches. In (Geng et al., 2009; Chen et
al., 2008b), the parameters of ranking model trained
on the source domain was adjusted with the small
set of labeled data in the target domain. (Wang et al.,
2009) aimed at ranking adaptation in heterogeneous
domains. (Gao et al., 2009) learned ranking mod-
els on the source and target domains independently,
and then constructed a stronger model by interpo-
lating the two models. (Chen et al., 2010; Chen et
al., 2008a) weighted source instances by using small
amount of labeled data in the target domain. (Gao et
al., 2010) studied instance weighting based on do-
main separator for learning to rank by only using
training data from source domain. In this work, we
propose to directly measure the query importance in-
stead of document instance importance by consider-
ing information at both levels.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999975625">
We introduced two simple yet effective query
weighting methods for ranking model adaptation.
The first represents a set of document instances
within the same query as a query feature vector,
and then directly measure the source query impor-
tance to the target domain. The second measures
the similarity between a source query and each tar-
get query, and then combine the fine-grained simi-
larity values to estimate its importance to target do-
main. We evaluated our approaches on LETOR3.0
dataset for ranking adaptation and found that: (1)
the first method efficiently estimate query weights,
and can outperform the document instance weight-
ing but some information is lost during the aggrega-
tion; (2) the second method consistently and signifi-
cantly outperforms document instance weighting.
</bodyText>
<sectionHeader confidence="0.997847" genericHeader="conclusions">
8 Acknowledgement
</sectionHeader>
<bodyText confidence="0.971991">
P. Cai and A. Zhou are supported by NSFC (No.
60925008) and 973 program (No. 2010CB731402).
W. Gao and K.-F. Wong are supported by national
863 program (No. 2009AA01Z150). We also thank
anonymous reviewers for their helpful comments.
</bodyText>
<page confidence="0.993566">
120
</page>
<sectionHeader confidence="0.996242" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889822429906">
Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval.
Somnath Banerjee, Avinava Dubey, Jinesh Machchhar,
and Soumen Chakrabarti. 2009. Efficient and accu-
rate local learning for ranking. In SIGIR workshop :
Learning to rank for information retrieval, pages 1–8.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79(1-2):151–175.
John Blitzer, Ryan Mcdonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
rank using gradient descent. In Proceedings of ICML,
pages 89–96.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise ap-
proach to listwise approach. In Proceedings of ICML,
pages 129 – 136.
Depin Chen, Jun Yan, Gang Wang, Yan Xiong, Weiguo
Fan, and Zheng Chen. 2008a. Transrank: A novel
algorithm for transfer of rank learning. In Proceedings
of ICDM Workshops, pages 106–115.
Keke Chen, Rongqing Lu, C.K. Wong, Gordon Sun,
Larry Heck, and Belle Tseng. 2008b. Trada: Tree
based ranking function adaptation. In Proceedings of
CIKM.
Depin Chen, Yan Xiong, Jun Yan, Gui-Rong Xue, Gang
Wang, and Zheng Chen. 2010. Knowledge transfer
for cross domain learning to rank. Information Re-
trieval, 13(3):236–253.
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101–126.
Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2004.
An efficient boosting algorithm for combining prefer-
ences. Journal of Machine Learning Research, 4:933–
969.
Jianfeng Gao, Qiang Wu, Chris Burges, Krysta Svore,
Yi Su, Nazan Khan, Shalin Shah, and Hongyan Zhou.
2009. Model adaptation via model interpolation and
boosting for web search ranking. In Proceedings of
EMNLP.
Wei Gao, Peng Cai, Kam Fai Wong, and Aoying Zhou.
2010. Learning to rank only using training data from
related domain. In Proceedings of SIGIR, pages 162–
169.
Xiubo Geng, Tie-Yan Liu, Tao Qin, Andrew Arnold,
Hang Li, and Heung-Yeung Shum. 2008. Query de-
pendent ranking using k-nearest neighbor. In Proceed-
ings of SIGIR, pages 115–122.
Bo Geng, Linjun Yang, Chao Xu, and Xian-Sheng Hua.
2009. Ranking model adaptation for domain-specific
search. In Proceedings of CIKM.
R. Herbrich, T. Graepel, and K. Obermayer. 2000.
Large Margin Rank Boundaries for Ordinal Regres-
sion. MIT Press, Cambridge.
Jiayuan Huang, Alexander J. Smola, Arthur Gretton,
Karsten M. Borgwardt, and Bernhard Sch¨olkopf.
2007. Correcting sample selection bias by unlabeled
data. In Proceedings of NIPS, pages 601–608.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
ACL.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of SIGKDD,
pages 133–142.
Maurice Kendall. 1970. Rank Correlation Methods.
Griffin.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information Re-
trieval, 3(3):225–331.
Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and
Qiang Yang. 2009. Domain adaptation via transfer
component analysis. In Proceedings of IJCAI, pages
1187–1192.
John C. Platt and John C. Platt. 1999. Probabilistic out-
puts for support vector machines and comparisons to
regularized likelihood methods. In Advances in Large
Margin Classifiers, pages 61–74. MIT Press.
Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. Letor:
A benchmark collection for research on learning to
rank for information retrieval. Information Retrieval,
13(4):346–374.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: Primal estimated sub-gradient solver for svm.
In Proceedings of the 24th International Conference
on Machine Learning, pages 807–814.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227–244.
Masashi Sugiyama, Shinichi Nakajima, Hisashi
Kashima, Paul von B¨unau, and Motoaki Kawan-
abe. 2008. Direct importance estimation with
model selection and its application to covariate
shift adaptation. In Proceedings of NIPS, pages
1433–1440.
Ellen M. Voorhees. 2003. Overview of trec 2003. In
Proceedings of TREC-2003, pages 1–13.
Ellen M. Voorhees. 2004. Overview of trec 2004. In
Proceedings of TREC-2004, pages 1–12.
Bo Wang, Jie Tang, Wei Fan, Songcan Chen, Zi Yang,
and Yanzhu Liu. 2009. Heterogeneous cross domain
ranking in latent space. In Proceedings of CIKM.
</reference>
<page confidence="0.97295">
121
</page>
<reference confidence="0.992451333333333">
Y. Yue, T. Finley, F. Radlinski, and T. Joachims. 2007.
A support vector method for optimizing average preci-
sion. In Proceedings of SIGIR, pages 271–278.
Bianca Zadrozny Zadrozny. 2004. Learning and evalu-
ating classifiers under sample selection bias. In Pro-
ceedings of ICML, pages 325–332.
</reference>
<page confidence="0.997481">
122
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.745830">
<title confidence="0.999516">Query Weighting for Ranking Model Adaptation</title>
<author confidence="0.990826">Wei Aoying</author>
<author confidence="0.990826">Kam-Fai</author>
<affiliation confidence="0.990373">China Normal University, Shanghai,</affiliation>
<email confidence="0.849292">pengcai2010@gmail.com,</email>
<affiliation confidence="0.9334145">Chinese University of Hong Kong, Shatin, N.T., Hong Laboratory of High Confidence Software Technologies, Ministry of Education, China</affiliation>
<abstract confidence="0.999683407407407">We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. Query weighting is a key step in ranking model adaptation. As the learning object of ranking algorithms is divided by query instances, we argue that it’s more reasonable to conduct importance weighting at query level than document level. We present two query weighting schemes. first compresses the query into a which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. This method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then comthese values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo A Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<date>1999</date>
<journal>Modern Information Retrieval.</journal>
<contexts>
<context position="20100" citStr="Baeza-Yates and Ribeiro-Neto, 1999" startWordPosition="3440" endWordPosition="3443"> ranking models above were trained only on source domain training data and the labeled data of target domain was just used for testing. For training the models efficiently, we implemented RSVM with Stochastic Gradient Descent (SGD) optimizer (Shalev-Shwartz et al., 2007). The reported performance is obtained by five-fold cross validation. 5.2 Experimental Results The task of HP and NP are more similar to each other whereas HP/NP is rather different from TD (Voorhees, 2003; Voorhees, 2004). Thus, we carried out HP/NP to TD and TD to HP/NP ranking adaptation tasks. Mean Average Precision (MAP) (Baeza-Yates and Ribeiro-Neto, 1999) is used as the ranking performance measure. 5.2.1 Adaptation from HP/NP to TD The first set of experiments performed adaptation from HP to TD and NP to TD. The results of MAP are shown in Table 2. For the DS-based measure, as shown in the table, query-aggr works mostly better than no-weight,docpair, doc-avg and doc-comb, and query-comp performs the best among the five weighting methods. T-test on MAP indicates that the improvement of query-aggr over no-weight is statistically significant on two adaptation tasks while the improvement of document instance weighting over no-weight is statistical</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somnath Banerjee</author>
<author>Avinava Dubey</author>
<author>Jinesh Machchhar</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Efficient and accurate local learning for ranking. In SIGIR workshop : Learning to rank for information retrieval,</title>
<date>2009</date>
<pages>1--8</pages>
<contexts>
<context position="10626" citStr="Banerjee et al., 2009" startWordPosition="1737" endWordPosition="1740">on can be applied to ranking model adaptation. 3 Query Weighting In this section, we extend instance weighting to directly estimate query importance for more effective ranking model adaptation. We present two query weighting methods from different perspectives. Note that although our methods are based on domain separator scheme, other instance weighting schemes such as KLIEP (Sugiyama et al., 2008) can also be extended similarly. 3.1 Query Weighting by Document Feature Aggregation Our first query weighting method is inspired by the recent work on local learning for ranking (Geng et al., 2008; Banerjee et al., 2009). The query can be compressed into a query feature vector, where each feature value is obtained by the aggregate of its corresponding features of all documents in the query. We concatenate two types of aggregates to construct the query feature vector: the mean µ⃗ = |e1 E�e |1 ⃗fi �|q| and the variance σ⃗ = 1 i=1(⃗fi − ⃗µ)2, where ⃗fi |q| is the feature vector of document i and |q |denotes the number of documents in q . Based on the aggregation of documents within each query, we can use a domain separator to directly weight the source queries with the set of queries from both domains. Given que</context>
</contexts>
<marker>Banerjee, Dubey, Machchhar, Chakrabarti, 2009</marker>
<rawString>Somnath Banerjee, Avinava Dubey, Jinesh Machchhar, and Soumen Chakrabarti. 2009. Efficient and accurate local learning for ranking. In SIGIR workshop : Learning to rank for information retrieval, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--1</pages>
<contexts>
<context position="8184" citStr="Ben-David et al., 2010" startWordPosition="1345" endWordPosition="1348">n source instances which are more similar to target domain. As a result, the key problem is how to accurately estimate the instance’s weight indicating its importance to target domain. (Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. Recently, some researchers proposed to weight source instance only using unlabeled target instances (Shimodaira, 2000; Sugiyama et al., 2008; Huang et al., 2007; Zadrozny, 2004; Gao et al., 2010). In this work, we also focus on weighting source queries only using unlabeled target queries. (Gao et al., 2010; Ben-David et al., 2010) proposed to use a classification hyperplane to separate source instances from target instances. With the domain separator, the probability that a source instance is classified to target domain can be used as the importance weight. Other instance weighting methods were proposed for the sample selection bias or covariate shift in the more general setting of classifier learning (Shimodaira, 2000; Sugiyama et al., 2008; Huang et al., 2007; Zadrozny, 2004). (Sugiyama et al., 2008) used a natural model selection procedure, referred to as Kullback-Leibler divergence Importance Estimation Procedure (</context>
<context position="28801" citStr="Ben-David et al., 2010" startWordPosition="4843" endWordPosition="4846">.0280 0.0247 0.0525 query-comp 0.1001 0.0804 0.0711 0.1737 query-rand 0.0041 0.0008 -0.0127 0.0163 Table 4: The Kendall’s T of Rf11,ight and R,,,,ap in HP/NP to TD adaptation. Weighting method TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04 query-aggr 0.1172 0.0121 0.0574 0.0464 query-comp 0.1304 0.1393 0.1586 0.0545 query-rand −0.0291 0.0022 0.0161 -0.0262 Table 5: The Kendall’s T of RU1,ight and R„Lap in TD to HP/NP adaptation. fine-grained similarity values. 6 Related Work Cross-domain knowledge transfer has became an important topic in machine learning and natural language processing (Ben-David et al., 2010; Jiang and Zhai, 2007; Blitzer et al., 2006; Daum´e III and Marcu, 2006). (Blitzer et al., 2006) proposed model adaptation using pivot features to build structural feature correspondence in two domains. (Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and </context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2010</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from different domains. Machine Learning, 79(1-2):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan Mcdonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="28845" citStr="Blitzer et al., 2006" startWordPosition="4851" endWordPosition="4854">.0711 0.1737 query-rand 0.0041 0.0008 -0.0127 0.0163 Table 4: The Kendall’s T of Rf11,ight and R,,,,ap in HP/NP to TD adaptation. Weighting method TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04 query-aggr 0.1172 0.0121 0.0574 0.0464 query-comp 0.1304 0.1393 0.1586 0.0545 query-rand −0.0291 0.0022 0.0161 -0.0262 Table 5: The Kendall’s T of RU1,ight and R„Lap in TD to HP/NP adaptation. fine-grained similarity values. 6 Related Work Cross-domain knowledge transfer has became an important topic in machine learning and natural language processing (Ben-David et al., 2010; Jiang and Zhai, 2007; Blitzer et al., 2006; Daum´e III and Marcu, 2006). (Blitzer et al., 2006) proposed model adaptation using pivot features to build structural feature correspondence in two domains. (Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training </context>
</contexts>
<marker>Blitzer, Mcdonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan Mcdonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burges</author>
<author>T Shaked</author>
<author>E Renshaw</author>
<author>A Lazier</author>
<author>M Deeds</author>
<author>N Hamilton</author>
<author>G Hullender</author>
</authors>
<title>Learning to rank using gradient descent.</title>
<date>2005</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="1756" citStr="Burges et al., 2005" startWordPosition="260" endWordPosition="263">sk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods. 1 Introduction Learning to rank, which aims at ranking documents in terms of their relevance to user’s query, has been widely studied in machine learning and information retrieval communities (Herbrich et al., 2000; Freund et al., 2004; Burges et al., 2005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Che</context>
</contexts>
<marker>Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton, Hullender, 2005</marker>
<rawString>C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. 2005. Learning to rank using gradient descent. In Proceedings of ICML, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhe Cao</author>
<author>Tao Qin</author>
<author>Tie-Yan Liu</author>
<author>Ming-Feng Tsai</author>
<author>Hang Li</author>
</authors>
<title>Learning to rank: from pairwise approach to listwise approach.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="1792" citStr="Cao et al., 2007" startWordPosition="268" endWordPosition="271">e compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods. 1 Introduction Learning to rank, which aims at ranking documents in terms of their relevance to user’s query, has been widely studied in machine learning and information retrieval communities (Herbrich et al., 2000; Freund et al., 2004; Burges et al., 2005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Chen et al., 2008b; </context>
<context position="3478" citStr="Cao et al., 2007" startWordPosition="538" endWordPosition="541">ghting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004; Huang et al., 2007; Jiang and Zhai, 2007; Sugiyama et al., 2008). Although instance weighting scheme may be applied to documents for ranking model adaptation, the difference between classification and learning to rank should be highlighted to take careful consideration. Compared to classification, the learning object for ranking is essentially a query, which contains a list of document instances each with a relevance judgement. Recently, researchers proposed listwise ranking algorithms (Yue et al., 2007; Cao et al., 2007) to take the whole query as a learning object. The benchmark evaluation showed that list112 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 112–122, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Source Domain Target domain Source Domain Target domain (s1) d1 (s1) d2 d(s1) 3 d(s2) 1 d(s2) 2 d(s2) 3 d(t1) 1 d(t2) 2 d(t1) 3 d(t2) 3 2 1 qs1 qs2 d(s1) 1 d(s1) 2 d(s1) 3 d1(s2) d(s2) 2 d(s2) 3 d(t1) 1 d(t1) 3 d(t1) 2 d(t2) 1 d(t2) 2 d(t2) 3 qt1 qt2 (a) Instance based weighting (b) Query based weighting Figure 1: The</context>
</contexts>
<marker>Cao, Qin, Liu, Tsai, Li, 2007</marker>
<rawString>Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In Proceedings of ICML, pages 129 – 136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Depin Chen</author>
<author>Jun Yan</author>
<author>Gang Wang</author>
<author>Yan Xiong</author>
<author>Weiguo Fan</author>
<author>Zheng Chen</author>
</authors>
<title>Transrank: A novel algorithm for transfer of rank learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICDM Workshops,</booktitle>
<pages>106--115</pages>
<contexts>
<context position="2350" citStr="Chen et al., 2008" startWordPosition="362" endWordPosition="365">04; Burges et al., 2005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Chen et al., 2008b; Geng et al., 2009; Gao et al., 2009). Intuitively, the more similar an source instance is to the target instances, it is expected to be more useful for cross-domain knowledge transfer. This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). Existing instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004;</context>
<context position="29583" citStr="Chen et al., 2008" startWordPosition="4967" endWordPosition="4970">feature correspondence in two domains. (Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training data in new search domains. Existing ranking adaptation approaches can be grouped into feature-based (Geng et al., 2009; Chen et al., 2008b; Wang et al., 2009; Gao et al., 2009) and instancebased (Chen et al., 2010; Chen et al., 2008a; Gao et al., 2010) approaches. In (Geng et al., 2009; Chen et al., 2008b), the parameters of ranking model trained on the source domain was adjusted with the small set of labeled data in the target domain. (Wang et al., 2009) aimed at ranking adaptation in heterogeneous domains. (Gao et al., 2009) learned ranking models on the source and target domains independently, and then constructed a stronger model by interpolating the two models. (Chen et al., 2010; Chen et al., 2008a) weighted source instan</context>
</contexts>
<marker>Chen, Yan, Wang, Xiong, Fan, Chen, 2008</marker>
<rawString>Depin Chen, Jun Yan, Gang Wang, Yan Xiong, Weiguo Fan, and Zheng Chen. 2008a. Transrank: A novel algorithm for transfer of rank learning. In Proceedings of ICDM Workshops, pages 106–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keke Chen</author>
<author>Rongqing Lu</author>
<author>C K Wong</author>
<author>Gordon Sun</author>
<author>Larry Heck</author>
<author>Belle Tseng</author>
</authors>
<title>Trada: Tree based ranking function adaptation.</title>
<date>2008</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="2350" citStr="Chen et al., 2008" startWordPosition="362" endWordPosition="365">04; Burges et al., 2005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Chen et al., 2008b; Geng et al., 2009; Gao et al., 2009). Intuitively, the more similar an source instance is to the target instances, it is expected to be more useful for cross-domain knowledge transfer. This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). Existing instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004;</context>
<context position="29583" citStr="Chen et al., 2008" startWordPosition="4967" endWordPosition="4970">feature correspondence in two domains. (Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training data in new search domains. Existing ranking adaptation approaches can be grouped into feature-based (Geng et al., 2009; Chen et al., 2008b; Wang et al., 2009; Gao et al., 2009) and instancebased (Chen et al., 2010; Chen et al., 2008a; Gao et al., 2010) approaches. In (Geng et al., 2009; Chen et al., 2008b), the parameters of ranking model trained on the source domain was adjusted with the small set of labeled data in the target domain. (Wang et al., 2009) aimed at ranking adaptation in heterogeneous domains. (Gao et al., 2009) learned ranking models on the source and target domains independently, and then constructed a stronger model by interpolating the two models. (Chen et al., 2010; Chen et al., 2008a) weighted source instan</context>
</contexts>
<marker>Chen, Lu, Wong, Sun, Heck, Tseng, 2008</marker>
<rawString>Keke Chen, Rongqing Lu, C.K. Wong, Gordon Sun, Larry Heck, and Belle Tseng. 2008b. Trada: Tree based ranking function adaptation. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Depin Chen</author>
<author>Yan Xiong</author>
<author>Jun Yan</author>
<author>Gui-Rong Xue</author>
<author>Gang Wang</author>
<author>Zheng Chen</author>
</authors>
<title>Knowledge transfer for cross domain learning to rank.</title>
<date>2010</date>
<journal>Information Retrieval,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="2370" citStr="Chen et al., 2010" startWordPosition="366" endWordPosition="369">005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Chen et al., 2008b; Geng et al., 2009; Gao et al., 2009). Intuitively, the more similar an source instance is to the target instances, it is expected to be more useful for cross-domain knowledge transfer. This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). Existing instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004; Huang et al., 2007;</context>
<context position="29659" citStr="Chen et al., 2010" startWordPosition="4982" endWordPosition="4985"> common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training data in new search domains. Existing ranking adaptation approaches can be grouped into feature-based (Geng et al., 2009; Chen et al., 2008b; Wang et al., 2009; Gao et al., 2009) and instancebased (Chen et al., 2010; Chen et al., 2008a; Gao et al., 2010) approaches. In (Geng et al., 2009; Chen et al., 2008b), the parameters of ranking model trained on the source domain was adjusted with the small set of labeled data in the target domain. (Wang et al., 2009) aimed at ranking adaptation in heterogeneous domains. (Gao et al., 2009) learned ranking models on the source and target domains independently, and then constructed a stronger model by interpolating the two models. (Chen et al., 2010; Chen et al., 2008a) weighted source instances by using small amount of labeled data in the target domain. (Gao et al.,</context>
</contexts>
<marker>Chen, Xiong, Yan, Xue, Wang, Chen, 2010</marker>
<rawString>Depin Chen, Yan Xiong, Jun Yan, Gui-Rong Xue, Gang Wang, and Zheng Chen. 2010. Knowledge transfer for cross domain learning to rank. Information Retrieval, 13(3):236–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>26</volume>
<issue>1</issue>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26(1):101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Iyer</author>
<author>R Schapire</author>
<author>Y Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>4</volume>
<pages>969</pages>
<contexts>
<context position="1735" citStr="Freund et al., 2004" startWordPosition="255" endWordPosition="259"> but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods. 1 Introduction Learning to rank, which aims at ranking documents in terms of their relevance to user’s query, has been widely studied in machine learning and information retrieval communities (Herbrich et al., 2000; Freund et al., 2004; Burges et al., 2005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Ch</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 2004</marker>
<rawString>Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2004. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933– 969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Qiang Wu</author>
<author>Chris Burges</author>
<author>Krysta Svore</author>
<author>Yi Su</author>
<author>Nazan Khan</author>
<author>Shalin Shah</author>
<author>Hongyan Zhou</author>
</authors>
<title>Model adaptation via model interpolation and boosting for web search ranking.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2428" citStr="Gao et al., 2009" startWordPosition="378" endWordPosition="381">neral, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Chen et al., 2008b; Geng et al., 2009; Gao et al., 2009). Intuitively, the more similar an source instance is to the target instances, it is expected to be more useful for cross-domain knowledge transfer. This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). Existing instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004; Huang et al., 2007; Jiang and Zhai, 2007; Sugiyama et al., 2008). Although in</context>
<context position="29622" citStr="Gao et al., 2009" startWordPosition="4975" endWordPosition="4978">(Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training data in new search domains. Existing ranking adaptation approaches can be grouped into feature-based (Geng et al., 2009; Chen et al., 2008b; Wang et al., 2009; Gao et al., 2009) and instancebased (Chen et al., 2010; Chen et al., 2008a; Gao et al., 2010) approaches. In (Geng et al., 2009; Chen et al., 2008b), the parameters of ranking model trained on the source domain was adjusted with the small set of labeled data in the target domain. (Wang et al., 2009) aimed at ranking adaptation in heterogeneous domains. (Gao et al., 2009) learned ranking models on the source and target domains independently, and then constructed a stronger model by interpolating the two models. (Chen et al., 2010; Chen et al., 2008a) weighted source instances by using small amount of labeled da</context>
</contexts>
<marker>Gao, Wu, Burges, Svore, Su, Khan, Shah, Zhou, 2009</marker>
<rawString>Jianfeng Gao, Qiang Wu, Chris Burges, Krysta Svore, Yi Su, Nazan Khan, Shalin Shah, and Hongyan Zhou. 2009. Model adaptation via model interpolation and boosting for web search ranking. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Gao</author>
<author>Peng Cai</author>
<author>Kam Fai Wong</author>
<author>Aoying Zhou</author>
</authors>
<title>Learning to rank only using training data from related domain.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>162--169</pages>
<contexts>
<context position="8047" citStr="Gao et al., 2010" startWordPosition="1322" endWordPosition="1325">importance to the target domain. 2 Instance Weighting Scheme Review The basic idea of instance weighting is to put larger weights on source instances which are more similar to target domain. As a result, the key problem is how to accurately estimate the instance’s weight indicating its importance to target domain. (Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. Recently, some researchers proposed to weight source instance only using unlabeled target instances (Shimodaira, 2000; Sugiyama et al., 2008; Huang et al., 2007; Zadrozny, 2004; Gao et al., 2010). In this work, we also focus on weighting source queries only using unlabeled target queries. (Gao et al., 2010; Ben-David et al., 2010) proposed to use a classification hyperplane to separate source instances from target instances. With the domain separator, the probability that a source instance is classified to target domain can be used as the importance weight. Other instance weighting methods were proposed for the sample selection bias or covariate shift in the more general setting of classifier learning (Shimodaira, 2000; Sugiyama et al., 2008; Huang et al., 2007; Zadrozny, 2004). (Sugi</context>
<context position="9471" citStr="Gao et al., 2010" startWordPosition="1546" endWordPosition="1549">tance estimation was more accurate. The main idea is to directly estimate the density function ratio of target distribution pt(x) to source distribution ps(x), i.e. w(x) = pt(x)Then model w(x) can be used to p.(x) . estimate the importance of source instances. Model parameters were computed with a linear model by minimizing the KL-divergence from pt(x) to its estimator Pt(x). Since Pt(x) = w(x)ps(x), the ultimate objective only contains model w(x). For using instance weighting in pairwise ranking algorithms, the weights of document instances should be transformed into those of document pairs (Gao et al., 2010). Given a pair of documents (xi, xj) and their weights wi and wj, the pairwise weight wij could be estimated probabilistically as wi * wj. To consider query factor, query weight was further estimated as the average value of the weights over all the pairs, i.e., wq = M Ei,j wij, where M is the number of pairs in query q. Additionally, to take the advantage of both query and document information, a probabilistic weighting for (xi, xj) was modeled by wq * wij. Through the transformation, instance weighting schemes for classification can be applied to ranking model adaptation. 3 Query Weighting In</context>
<context position="18825" citStr="Gao et al., 2010" startWordPosition="3233" endWordPosition="3236">eatures, including low-level features such as term frequency, inverse document frequency and document length, and high-level features such as BM25, language-modeling, PageRank and HITS. The number of queries of each task is given in Table 1. The baseline ranking model is an RSVM directly trained on the source domain without using any weighting methods, denoted as no-weight. We implemented two weighting measures based on domain separator and Kullback-Leibler divergence, referred to DS and KL, respectively. In DS measure, three document instance weighting methods based on probability principle (Gao et al., 2010) were implemented for comparison, denoted as doc-pair, doc-avg and doc-comb (see Section 2). In KL measure, there is no probabilistic meaning for KL weight Query Task TREC 2003 TREC 2004 Topic Distillation 50 75 Home Page finding 150 75 Named Page finding 150 75 Table 1: The number of queries in TREC-2003 and TREC-2004 web track and the doc-comb based on KL is not interpretable, and we only present the results of doc-pair and docavg for KL measure. Our proposed query weighting methods are denoted by query-aggr and querycomp, corresponding to document feature aggregation in query and query comp</context>
<context position="29698" citStr="Gao et al., 2010" startWordPosition="4990" endWordPosition="4993">stribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training data in new search domains. Existing ranking adaptation approaches can be grouped into feature-based (Geng et al., 2009; Chen et al., 2008b; Wang et al., 2009; Gao et al., 2009) and instancebased (Chen et al., 2010; Chen et al., 2008a; Gao et al., 2010) approaches. In (Geng et al., 2009; Chen et al., 2008b), the parameters of ranking model trained on the source domain was adjusted with the small set of labeled data in the target domain. (Wang et al., 2009) aimed at ranking adaptation in heterogeneous domains. (Gao et al., 2009) learned ranking models on the source and target domains independently, and then constructed a stronger model by interpolating the two models. (Chen et al., 2010; Chen et al., 2008a) weighted source instances by using small amount of labeled data in the target domain. (Gao et al., 2010) studied instance weighting based</context>
</contexts>
<marker>Gao, Cai, Wong, Zhou, 2010</marker>
<rawString>Wei Gao, Peng Cai, Kam Fai Wong, and Aoying Zhou. 2010. Learning to rank only using training data from related domain. In Proceedings of SIGIR, pages 162– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiubo Geng</author>
<author>Tie-Yan Liu</author>
<author>Tao Qin</author>
<author>Andrew Arnold</author>
<author>Hang Li</author>
<author>Heung-Yeung Shum</author>
</authors>
<title>Query dependent ranking using k-nearest neighbor.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>115--122</pages>
<contexts>
<context position="10602" citStr="Geng et al., 2008" startWordPosition="1733" endWordPosition="1736">es for classification can be applied to ranking model adaptation. 3 Query Weighting In this section, we extend instance weighting to directly estimate query importance for more effective ranking model adaptation. We present two query weighting methods from different perspectives. Note that although our methods are based on domain separator scheme, other instance weighting schemes such as KLIEP (Sugiyama et al., 2008) can also be extended similarly. 3.1 Query Weighting by Document Feature Aggregation Our first query weighting method is inspired by the recent work on local learning for ranking (Geng et al., 2008; Banerjee et al., 2009). The query can be compressed into a query feature vector, where each feature value is obtained by the aggregate of its corresponding features of all documents in the query. We concatenate two types of aggregates to construct the query feature vector: the mean µ⃗ = |e1 E�e |1 ⃗fi �|q| and the variance σ⃗ = 1 i=1(⃗fi − ⃗µ)2, where ⃗fi |q| is the feature vector of document i and |q |denotes the number of documents in q . Based on the aggregation of documents within each query, we can use a domain separator to directly weight the source queries with the set of queries from</context>
</contexts>
<marker>Geng, Liu, Qin, Arnold, Li, Shum, 2008</marker>
<rawString>Xiubo Geng, Tie-Yan Liu, Tao Qin, Andrew Arnold, Hang Li, and Heung-Yeung Shum. 2008. Query dependent ranking using k-nearest neighbor. In Proceedings of SIGIR, pages 115–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Geng</author>
<author>Linjun Yang</author>
<author>Chao Xu</author>
<author>Xian-Sheng Hua</author>
</authors>
<title>Ranking model adaptation for domain-specific search.</title>
<date>2009</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="2409" citStr="Geng et al., 2009" startWordPosition="374" endWordPosition="377">; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Chen et al., 2008b; Geng et al., 2009; Gao et al., 2009). Intuitively, the more similar an source instance is to the target instances, it is expected to be more useful for cross-domain knowledge transfer. This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). Existing instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004; Huang et al., 2007; Jiang and Zhai, 2007; Sugiyama et al.,</context>
<context position="29564" citStr="Geng et al., 2009" startWordPosition="4963" endWordPosition="4966">o build structural feature correspondence in two domains. (Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training data in new search domains. Existing ranking adaptation approaches can be grouped into feature-based (Geng et al., 2009; Chen et al., 2008b; Wang et al., 2009; Gao et al., 2009) and instancebased (Chen et al., 2010; Chen et al., 2008a; Gao et al., 2010) approaches. In (Geng et al., 2009; Chen et al., 2008b), the parameters of ranking model trained on the source domain was adjusted with the small set of labeled data in the target domain. (Wang et al., 2009) aimed at ranking adaptation in heterogeneous domains. (Gao et al., 2009) learned ranking models on the source and target domains independently, and then constructed a stronger model by interpolating the two models. (Chen et al., 2010; Chen et al., 2008a) wei</context>
</contexts>
<marker>Geng, Yang, Xu, Hua, 2009</marker>
<rawString>Bo Geng, Linjun Yang, Chao Xu, and Xian-Sheng Hua. 2009. Ranking model adaptation for domain-specific search. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Herbrich</author>
<author>T Graepel</author>
<author>K Obermayer</author>
</authors>
<title>Large Margin Rank Boundaries for Ordinal Regression.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1714" citStr="Herbrich et al., 2000" startWordPosition="251" endWordPosition="254">compressing query data, but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods. 1 Introduction Learning to rank, which aims at ranking documents in terms of their relevance to user’s query, has been widely studied in machine learning and information retrieval communities (Herbrich et al., 2000; Freund et al., 2004; Burges et al., 2005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking</context>
<context position="16548" citStr="Herbrich et al., 2000" startWordPosition="2839" endWordPosition="2842">ghts into existing ranking algorithms. Note that query weights can be integrated with either pairwise or listwise algorithms. For pairwise algorithms, a straightforward way is to assign the query weight to all the document pairs associated with this query. However, document instance weighting cannot be appropriately utilized in listwise approach. In order to compare query weighting with document instance weighting, we need to fairly apply them for the same approach of ranking. Therefore, we choose pairwise approach to incorporate query weighting. In this section, we extend Ranking SVM (RSVM) (Herbrich et al., 2000; Joachims, 2002) —one of the typical pairwise algorithms for this. assume there are m queries in the data set of source domain, and for each query there are number of meaningful document pairs that can Let’s qi ℓ(qi) ξij (1) subject to zij∗f(⃗w,⃗xj(1) qi − ⃗xj(2) qi ) ≥ 1 − ξij ξij ≥ 0, i = 1, . . . , m; j = 1, . . . , ℓ(qi) ℓ(qi) � j=1 (1 (2) Let represent the importance weight of source query Equation 2 is extended for integrating the query IW(qi) qi. weight into the loss function in a min2||⃗w||2 + C �m ℓ(qi) � 1 i=1 j=1 min λ||⃗w||2+ �m i=1 straightforward way: min λ||⃗V||2+ (1 − zij ∗ f(</context>
</contexts>
<marker>Herbrich, Graepel, Obermayer, 2000</marker>
<rawString>R. Herbrich, T. Graepel, and K. Obermayer. 2000. Large Margin Rank Boundaries for Ordinal Regression. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiayuan Huang</author>
<author>Alexander J Smola</author>
<author>Arthur Gretton</author>
<author>Karsten M Borgwardt</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>Correcting sample selection bias by unlabeled data.</title>
<date>2007</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>601--608</pages>
<marker>Huang, Smola, Gretton, Borgwardt, Sch¨olkopf, 2007</marker>
<rawString>Jiayuan Huang, Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard Sch¨olkopf. 2007. Correcting sample selection bias by unlabeled data. In Proceedings of NIPS, pages 601–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2838" citStr="Jiang and Zhai, 2007" startWordPosition="441" endWordPosition="444"> to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Chen et al., 2008b; Geng et al., 2009; Gao et al., 2009). Intuitively, the more similar an source instance is to the target instances, it is expected to be more useful for cross-domain knowledge transfer. This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). Existing instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004; Huang et al., 2007; Jiang and Zhai, 2007; Sugiyama et al., 2008). Although instance weighting scheme may be applied to documents for ranking model adaptation, the difference between classification and learning to rank should be highlighted to take careful consideration. Compared to classification, the learning object for ranking is essentially a query, which contains a list of document instances each with a relevance judgement. Recently, researchers proposed listwise ranking algorit</context>
<context position="7768" citStr="Jiang and Zhai, 2007" startWordPosition="1277" endWordPosition="1280">ture vector by aggregating all of its document instances, and then conduct query weighting on these query feature vectors; (2) we measure the similarity between the source query and each target query one by one, and then combine these fine-grained similarity values to calculate its importance to the target domain. 2 Instance Weighting Scheme Review The basic idea of instance weighting is to put larger weights on source instances which are more similar to target domain. As a result, the key problem is how to accurately estimate the instance’s weight indicating its importance to target domain. (Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. Recently, some researchers proposed to weight source instance only using unlabeled target instances (Shimodaira, 2000; Sugiyama et al., 2008; Huang et al., 2007; Zadrozny, 2004; Gao et al., 2010). In this work, we also focus on weighting source queries only using unlabeled target queries. (Gao et al., 2010; Ben-David et al., 2010) proposed to use a classification hyperplane to separate source instances from target instances. With the domain separator, the probability that a source instance is classified to targ</context>
<context position="28823" citStr="Jiang and Zhai, 2007" startWordPosition="4847" endWordPosition="4850">y-comp 0.1001 0.0804 0.0711 0.1737 query-rand 0.0041 0.0008 -0.0127 0.0163 Table 4: The Kendall’s T of Rf11,ight and R,,,,ap in HP/NP to TD adaptation. Weighting method TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04 query-aggr 0.1172 0.0121 0.0574 0.0464 query-comp 0.1304 0.1393 0.1586 0.0545 query-rand −0.0291 0.0022 0.0161 -0.0262 Table 5: The Kendall’s T of RU1,ight and R„Lap in TD to HP/NP adaptation. fine-grained similarity values. 6 Related Work Cross-domain knowledge transfer has became an important topic in machine learning and natural language processing (Ben-David et al., 2010; Jiang and Zhai, 2007; Blitzer et al., 2006; Daum´e III and Marcu, 2006). (Blitzer et al., 2006) proposed model adaptation using pivot features to build structural feature correspondence in two domains. (Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGKDD,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="16565" citStr="Joachims, 2002" startWordPosition="2843" endWordPosition="2844">ing algorithms. Note that query weights can be integrated with either pairwise or listwise algorithms. For pairwise algorithms, a straightforward way is to assign the query weight to all the document pairs associated with this query. However, document instance weighting cannot be appropriately utilized in listwise approach. In order to compare query weighting with document instance weighting, we need to fairly apply them for the same approach of ranking. Therefore, we choose pairwise approach to incorporate query weighting. In this section, we extend Ranking SVM (RSVM) (Herbrich et al., 2000; Joachims, 2002) —one of the typical pairwise algorithms for this. assume there are m queries in the data set of source domain, and for each query there are number of meaningful document pairs that can Let’s qi ℓ(qi) ξij (1) subject to zij∗f(⃗w,⃗xj(1) qi − ⃗xj(2) qi ) ≥ 1 − ξij ξij ≥ 0, i = 1, . . . , m; j = 1, . . . , ℓ(qi) ℓ(qi) � j=1 (1 (2) Let represent the importance weight of source query Equation 2 is extended for integrating the query IW(qi) qi. weight into the loss function in a min2||⃗w||2 + C �m ℓ(qi) � 1 i=1 j=1 min λ||⃗w||2+ �m i=1 straightforward way: min λ||⃗V||2+ (1 − zij ∗ f(⃗V, ⃗xj(1) — xj(2</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of SIGKDD, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Kendall</author>
</authors>
<title>Rank Correlation Methods.</title>
<date>1970</date>
<location>Griffin.</location>
<contexts>
<context position="26175" citStr="Kendall, 1970" startWordPosition="4419" endWordPosition="4420">rom target domain. Then, the source queries are ranked according to the MAP values obtained by their corresponding ranking models. The order is denoted as Rmap. Meanwhile, the source queries are also ranked with respect to their weights estimated by DS-based measure, and the order is denoted as Rweight. We hope Rweight is correlated as positively as possible with Rmap. For comparison, we also ranked these queries according to randomly generated query weights, which is denoted as query-rand in addition to queryaggr and query-comp. The Kendall’s T = P −Q P �Q is used to measure the correlation (Kendall, 1970), where P is the number of concordant query pairs and Q is the number of discordant pairs. It’s noted that T’s range is from -1 to 1, and the larger value means the two ranking is better correlated. The Kendall’s T by different weighting methods are given in Table 4 and 5. We find that Rweight produced by query-aggr and query-comp are all positively correlated with Rmap and clearly the orders generated by query-comp are more positive than those by query-aggr. This is another explanation why query-comp outperforms query-aggr. Furthermore, both are far better than weighting TD03 to HP03 TD04 to </context>
</contexts>
<marker>Kendall, 1970</marker>
<rawString>Maurice Kendall. 1970. Rank Correlation Methods. Griffin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tie-Yan Liu</author>
</authors>
<title>Learning to rank for information retrieval.</title>
<date>2009</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="1804" citStr="Liu, 2009" startWordPosition="272" endWordPosition="273"> second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods. 1 Introduction Learning to rank, which aims at ranking documents in terms of their relevance to user’s query, has been widely studied in machine learning and information retrieval communities (Herbrich et al., 2000; Freund et al., 2004; Burges et al., 2005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Chen et al., 2008b; Geng et al.,</context>
<context position="4542" citStr="Liu, 2009" startWordPosition="718" endWordPosition="719">1(s2) d(s2) 2 d(s2) 3 d(t1) 1 d(t1) 3 d(t1) 2 d(t2) 1 d(t2) 2 d(t2) 3 qt1 qt2 (a) Instance based weighting (b) Query based weighting Figure 1: The information about which document instances belong to the same query is lost in document instance weighting scheme. To avoid losing this information, query weighting takes the query as a whole and directly measures its importance. wise approach significantly outperformed pointwise approach, which takes each document instance as independent learning object, as well as pairwise approach, which concentrates learning on the order of a pair of documents (Liu, 2009). Inspired by the principle of listwise approach, we hypothesize that the importance weighting for ranking model adaptation could be done better at query level rather than document level. Figure 1 demonstrates the difference between instance weighting and query weighting, where there are two queries qs1 and qs2 in the source domain and qt1 and qt2 in the target domain, respectively, and each query has three retrieved documents. In Figure 1(a), source and target domains are represented as a bag of document instances. It is worth noting that the information about which document instances belong </context>
</contexts>
<marker>Liu, 2009</marker>
<rawString>Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Ivor W Tsang</author>
<author>James T Kwok</author>
<author>Qiang Yang</author>
</authors>
<title>Domain adaptation via transfer component analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1187--1192</pages>
<contexts>
<context position="29023" citStr="Pan et al., 2009" startWordPosition="4879" endWordPosition="4882">NP03 TD04 to NP04 query-aggr 0.1172 0.0121 0.0574 0.0464 query-comp 0.1304 0.1393 0.1586 0.0545 query-rand −0.0291 0.0022 0.0161 -0.0262 Table 5: The Kendall’s T of RU1,ight and R„Lap in TD to HP/NP adaptation. fine-grained similarity values. 6 Related Work Cross-domain knowledge transfer has became an important topic in machine learning and natural language processing (Ben-David et al., 2010; Jiang and Zhai, 2007; Blitzer et al., 2006; Daum´e III and Marcu, 2006). (Blitzer et al., 2006) proposed model adaptation using pivot features to build structural feature correspondence in two domains. (Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training data in new search domains. Existing ranking adaptation approaches can be grouped into feature-based (Geng et al., 2009; Chen et al., 2008b; Wang et al., 2009; Gao et al., 2009) </context>
</contexts>
<marker>Pan, Tsang, Kwok, Yang, 2009</marker>
<rawString>Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. 2009. Domain adaptation via transfer component analysis. In Proceedings of IJCAI, pages 1187–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
<author>John C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>In Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12716" citStr="Platt and Platt, 1999" startWordPosition="2153" endWordPosition="2156"> 13: Wi = P(qi 1 s ∈ Dt) = 1+exp(α∗L(⃗qis)+β) 14: Add Wi to IWs; 15: end for 16: return IWs; mains, we use algorithm 1 to estimate the probability that the query qis can be classified to Dt, i.e. P(qis ∈ Dt), which can be used as the importance of qis relative to the target domain. From step 1 to 9, D′s and D′t are constructed using query feature vectors from source and target domains. Then, a classification hyperplane Hst is used to separate D′s from D′t in step 10. The distance of the query feature vector ⃗qis from Hst are transformed to the probability P(qis ∈ Dt) using a sigmoid function (Platt and Platt, 1999). 3.2 Query Weighting by Comparing Queries across Domains Although the query feature vector in algorithm 1 can approximate a query by aggregating its documents’ features, it potentially fails to capture important feature information due to the averaging effect during the aggregation. For example, the merit of features in some influential documents may be canceled out in the mean-variance calculation, resulting in many distorted feature values in the query feature vector that hurts the accuracy of query classification hyperplane. This urges us to propose another query weighting method from a di</context>
</contexts>
<marker>Platt, Platt, 1999</marker>
<rawString>John C. Platt and John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, pages 61–74. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Qin</author>
<author>Tie-Yan Liu</author>
<author>Jun Xu</author>
<author>Hang Li</author>
</authors>
<title>Letor: A benchmark collection for research on learning to rank for information retrieval.</title>
<date>2010</date>
<journal>Information Retrieval,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="17484" citStr="Qin et al., 2010" startWordPosition="3020" endWordPosition="3023"> , ℓ(qi) ℓ(qi) � j=1 (1 (2) Let represent the importance weight of source query Equation 2 is extended for integrating the query IW(qi) qi. weight into the loss function in a min2||⃗w||2 + C �m ℓ(qi) � 1 i=1 j=1 min λ||⃗w||2+ �m i=1 straightforward way: min λ||⃗V||2+ (1 − zij ∗ f(⃗V, ⃗xj(1) — xj(2)))+ qz qz where IW(.) takes any one of the weighting schemes given by algorithm 1 and algorithm 2. 5 Evaluation We evaluated the proposed two query weighting methods on TREC-2003 and TREC-2004 web track datasets, which were released through LETOR3.0 as a benchmark collection for learning to rank by (Qin et al., 2010). Originally, different query tasks were defined on different parts of data in the collection, which can be considered as different domains for us. Adaptation takes place when ranking tasks are performed by using the models trained on the domains in which they were originally defined to rank the documents in other domains. Our goal is to demonstrate that query weighting can be more effective than the state-of-the-art document instance weighting. 5.1 Datasets and Setup Three query tasks were defined in TREC-2003 and TREC-2004 web track, which are home page finding (HP), named page finding (NP) </context>
</contexts>
<marker>Qin, Liu, Xu, Li, 2010</marker>
<rawString>Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. Letor: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval, 13(4):346–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>N Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for svm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="19736" citStr="Shalev-Shwartz et al., 2007" startWordPosition="3383" endWordPosition="3386">of queries in TREC-2003 and TREC-2004 web track and the doc-comb based on KL is not interpretable, and we only present the results of doc-pair and docavg for KL measure. Our proposed query weighting methods are denoted by query-aggr and querycomp, corresponding to document feature aggregation in query and query comparison across domains, respectively. All ranking models above were trained only on source domain training data and the labeled data of target domain was just used for testing. For training the models efficiently, we implemented RSVM with Stochastic Gradient Descent (SGD) optimizer (Shalev-Shwartz et al., 2007). The reported performance is obtained by five-fold cross validation. 5.2 Experimental Results The task of HP and NP are more similar to each other whereas HP/NP is rather different from TD (Voorhees, 2003; Voorhees, 2004). Thus, we carried out HP/NP to TD and TD to HP/NP ranking adaptation tasks. Mean Average Precision (MAP) (Baeza-Yates and Ribeiro-Neto, 1999) is used as the ranking performance measure. 5.2.1 Adaptation from HP/NP to TD The first set of experiments performed adaptation from HP to TD and NP to TD. The results of MAP are shown in Table 2. For the DS-based measure, as shown in </context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for svm. In Proceedings of the 24th International Conference on Machine Learning, pages 807–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetoshi Shimodaira</author>
</authors>
<title>Improving predictive inference under covariate shift by weighting the loglikelihood function.</title>
<date>2000</date>
<journal>Journal of Statistical Planning and Inference,</journal>
<pages>90--227</pages>
<contexts>
<context position="7969" citStr="Shimodaira, 2000" startWordPosition="1309" endWordPosition="1311"> one, and then combine these fine-grained similarity values to calculate its importance to the target domain. 2 Instance Weighting Scheme Review The basic idea of instance weighting is to put larger weights on source instances which are more similar to target domain. As a result, the key problem is how to accurately estimate the instance’s weight indicating its importance to target domain. (Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. Recently, some researchers proposed to weight source instance only using unlabeled target instances (Shimodaira, 2000; Sugiyama et al., 2008; Huang et al., 2007; Zadrozny, 2004; Gao et al., 2010). In this work, we also focus on weighting source queries only using unlabeled target queries. (Gao et al., 2010; Ben-David et al., 2010) proposed to use a classification hyperplane to separate source instances from target instances. With the domain separator, the probability that a source instance is classified to target domain can be used as the importance weight. Other instance weighting methods were proposed for the sample selection bias or covariate shift in the more general setting of classifier learning (Shimo</context>
</contexts>
<marker>Shimodaira, 2000</marker>
<rawString>Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90:227–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masashi Sugiyama</author>
<author>Shinichi Nakajima</author>
<author>Hisashi Kashima</author>
<author>Paul von B¨unau</author>
<author>Motoaki Kawanabe</author>
</authors>
<title>Direct importance estimation with model selection and its application to covariate shift adaptation.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1433--1440</pages>
<marker>Sugiyama, Nakajima, Kashima, von B¨unau, Kawanabe, 2008</marker>
<rawString>Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul von B¨unau, and Motoaki Kawanabe. 2008. Direct importance estimation with model selection and its application to covariate shift adaptation. In Proceedings of NIPS, pages 1433–1440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of trec 2003.</title>
<date>2003</date>
<booktitle>In Proceedings of TREC-2003,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="18127" citStr="Voorhees, 2003" startWordPosition="3127" endWordPosition="3128"> tasks were defined on different parts of data in the collection, which can be considered as different domains for us. Adaptation takes place when ranking tasks are performed by using the models trained on the domains in which they were originally defined to rank the documents in other domains. Our goal is to demonstrate that query weighting can be more effective than the state-of-the-art document instance weighting. 5.1 Datasets and Setup Three query tasks were defined in TREC-2003 and TREC-2004 web track, which are home page finding (HP), named page finding (NP) and topic distillation (TD) (Voorhees, 2003; Voorhees, 2004). In this dataset, each document instance is represented by 64 features, including low-level features such as term frequency, inverse document frequency and document length, and high-level features such as BM25, language-modeling, PageRank and HITS. The number of queries of each task is given in Table 1. The baseline ranking model is an RSVM directly trained on the source domain without using any weighting methods, denoted as no-weight. We implemented two weighting measures based on domain separator and Kullback-Leibler divergence, referred to DS and KL, respectively. In DS me</context>
<context position="19941" citStr="Voorhees, 2003" startWordPosition="3418" endWordPosition="3419">by query-aggr and querycomp, corresponding to document feature aggregation in query and query comparison across domains, respectively. All ranking models above were trained only on source domain training data and the labeled data of target domain was just used for testing. For training the models efficiently, we implemented RSVM with Stochastic Gradient Descent (SGD) optimizer (Shalev-Shwartz et al., 2007). The reported performance is obtained by five-fold cross validation. 5.2 Experimental Results The task of HP and NP are more similar to each other whereas HP/NP is rather different from TD (Voorhees, 2003; Voorhees, 2004). Thus, we carried out HP/NP to TD and TD to HP/NP ranking adaptation tasks. Mean Average Precision (MAP) (Baeza-Yates and Ribeiro-Neto, 1999) is used as the ranking performance measure. 5.2.1 Adaptation from HP/NP to TD The first set of experiments performed adaptation from HP to TD and NP to TD. The results of MAP are shown in Table 2. For the DS-based measure, as shown in the table, query-aggr works mostly better than no-weight,docpair, doc-avg and doc-comb, and query-comp performs the best among the five weighting methods. T-test on MAP indicates that the improvement of qu</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Ellen M. Voorhees. 2003. Overview of trec 2003. In Proceedings of TREC-2003, pages 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of trec</title>
<date>2004</date>
<booktitle>In Proceedings of TREC-2004,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="18144" citStr="Voorhees, 2004" startWordPosition="3129" endWordPosition="3130">ned on different parts of data in the collection, which can be considered as different domains for us. Adaptation takes place when ranking tasks are performed by using the models trained on the domains in which they were originally defined to rank the documents in other domains. Our goal is to demonstrate that query weighting can be more effective than the state-of-the-art document instance weighting. 5.1 Datasets and Setup Three query tasks were defined in TREC-2003 and TREC-2004 web track, which are home page finding (HP), named page finding (NP) and topic distillation (TD) (Voorhees, 2003; Voorhees, 2004). In this dataset, each document instance is represented by 64 features, including low-level features such as term frequency, inverse document frequency and document length, and high-level features such as BM25, language-modeling, PageRank and HITS. The number of queries of each task is given in Table 1. The baseline ranking model is an RSVM directly trained on the source domain without using any weighting methods, denoted as no-weight. We implemented two weighting measures based on domain separator and Kullback-Leibler divergence, referred to DS and KL, respectively. In DS measure, three docu</context>
<context position="19958" citStr="Voorhees, 2004" startWordPosition="3420" endWordPosition="3421">d querycomp, corresponding to document feature aggregation in query and query comparison across domains, respectively. All ranking models above were trained only on source domain training data and the labeled data of target domain was just used for testing. For training the models efficiently, we implemented RSVM with Stochastic Gradient Descent (SGD) optimizer (Shalev-Shwartz et al., 2007). The reported performance is obtained by five-fold cross validation. 5.2 Experimental Results The task of HP and NP are more similar to each other whereas HP/NP is rather different from TD (Voorhees, 2003; Voorhees, 2004). Thus, we carried out HP/NP to TD and TD to HP/NP ranking adaptation tasks. Mean Average Precision (MAP) (Baeza-Yates and Ribeiro-Neto, 1999) is used as the ranking performance measure. 5.2.1 Adaptation from HP/NP to TD The first set of experiments performed adaptation from HP to TD and NP to TD. The results of MAP are shown in Table 2. For the DS-based measure, as shown in the table, query-aggr works mostly better than no-weight,docpair, doc-avg and doc-comb, and query-comp performs the best among the five weighting methods. T-test on MAP indicates that the improvement of query-aggr over no-</context>
</contexts>
<marker>Voorhees, 2004</marker>
<rawString>Ellen M. Voorhees. 2004. Overview of trec 2004. In Proceedings of TREC-2004, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Wang</author>
<author>Jie Tang</author>
<author>Wei Fan</author>
<author>Songcan Chen</author>
<author>Zi Yang</author>
<author>Yanzhu Liu</author>
</authors>
<title>Heterogeneous cross domain ranking in latent space.</title>
<date>2009</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="29603" citStr="Wang et al., 2009" startWordPosition="4971" endWordPosition="4974">ce in two domains. (Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training data in new search domains. Existing ranking adaptation approaches can be grouped into feature-based (Geng et al., 2009; Chen et al., 2008b; Wang et al., 2009; Gao et al., 2009) and instancebased (Chen et al., 2010; Chen et al., 2008a; Gao et al., 2010) approaches. In (Geng et al., 2009; Chen et al., 2008b), the parameters of ranking model trained on the source domain was adjusted with the small set of labeled data in the target domain. (Wang et al., 2009) aimed at ranking adaptation in heterogeneous domains. (Gao et al., 2009) learned ranking models on the source and target domains independently, and then constructed a stronger model by interpolating the two models. (Chen et al., 2010; Chen et al., 2008a) weighted source instances by using small a</context>
</contexts>
<marker>Wang, Tang, Fan, Chen, Yang, Liu, 2009</marker>
<rawString>Bo Wang, Jie Tang, Wei Fan, Songcan Chen, Zi Yang, and Yanzhu Liu. 2009. Heterogeneous cross domain ranking in latent space. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yue</author>
<author>T Finley</author>
<author>F Radlinski</author>
<author>T Joachims</author>
</authors>
<title>A support vector method for optimizing average precision.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="1774" citStr="Yue et al., 2007" startWordPosition="264" endWordPosition="267">s resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods. 1 Introduction Learning to rank, which aims at ranking documents in terms of their relevance to user’s query, has been widely studied in machine learning and information retrieval communities (Herbrich et al., 2000; Freund et al., 2004; Burges et al., 2005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Ch</context>
<context position="3459" citStr="Yue et al., 2007" startWordPosition="534" endWordPosition="537">sting instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004; Huang et al., 2007; Jiang and Zhai, 2007; Sugiyama et al., 2008). Although instance weighting scheme may be applied to documents for ranking model adaptation, the difference between classification and learning to rank should be highlighted to take careful consideration. Compared to classification, the learning object for ranking is essentially a query, which contains a list of document instances each with a relevance judgement. Recently, researchers proposed listwise ranking algorithms (Yue et al., 2007; Cao et al., 2007) to take the whole query as a learning object. The benchmark evaluation showed that list112 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 112–122, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Source Domain Target domain Source Domain Target domain (s1) d1 (s1) d2 d(s1) 3 d(s2) 1 d(s2) 2 d(s2) 3 d(t1) 1 d(t2) 2 d(t1) 3 d(t2) 3 2 1 qs1 qs2 d(s1) 1 d(s1) 2 d(s1) 3 d1(s2) d(s2) 2 d(s2) 3 d(t1) 1 d(t1) 3 d(t1) 2 d(t2) 1 d(t2) 2 d(t2) 3 qt1 qt2 (a) Instance based weighting (b) Query based weig</context>
</contexts>
<marker>Yue, Finley, Radlinski, Joachims, 2007</marker>
<rawString>Y. Yue, T. Finley, F. Radlinski, and T. Joachims. 2007. A support vector method for optimizing average precision. In Proceedings of SIGIR, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bianca Zadrozny Zadrozny</author>
</authors>
<title>Learning and evaluating classifiers under sample selection bias.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>325--332</pages>
<contexts>
<context position="2949" citStr="Zadrozny, 2004" startWordPosition="457" endWordPosition="458">en et al., 2008a; Chen et al., 2010; Chen et al., 2008b; Geng et al., 2009; Gao et al., 2009). Intuitively, the more similar an source instance is to the target instances, it is expected to be more useful for cross-domain knowledge transfer. This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). Existing instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004; Huang et al., 2007; Jiang and Zhai, 2007; Sugiyama et al., 2008). Although instance weighting scheme may be applied to documents for ranking model adaptation, the difference between classification and learning to rank should be highlighted to take careful consideration. Compared to classification, the learning object for ranking is essentially a query, which contains a list of document instances each with a relevance judgement. Recently, researchers proposed listwise ranking algorithms (Yue et al., 2007; Cao et al., 2007) to take the whole query as a learning object. The benchmark evaluation</context>
<context position="8028" citStr="Zadrozny, 2004" startWordPosition="1320" endWordPosition="1321">o calculate its importance to the target domain. 2 Instance Weighting Scheme Review The basic idea of instance weighting is to put larger weights on source instances which are more similar to target domain. As a result, the key problem is how to accurately estimate the instance’s weight indicating its importance to target domain. (Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. Recently, some researchers proposed to weight source instance only using unlabeled target instances (Shimodaira, 2000; Sugiyama et al., 2008; Huang et al., 2007; Zadrozny, 2004; Gao et al., 2010). In this work, we also focus on weighting source queries only using unlabeled target queries. (Gao et al., 2010; Ben-David et al., 2010) proposed to use a classification hyperplane to separate source instances from target instances. With the domain separator, the probability that a source instance is classified to target domain can be used as the importance weight. Other instance weighting methods were proposed for the sample selection bias or covariate shift in the more general setting of classifier learning (Shimodaira, 2000; Sugiyama et al., 2008; Huang et al., 2007; Zad</context>
</contexts>
<marker>Zadrozny, 2004</marker>
<rawString>Bianca Zadrozny Zadrozny. 2004. Learning and evaluating classifiers under sample selection bias. In Proceedings of ICML, pages 325–332.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>