<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001802">
<title confidence="0.481367">
ABSTRACTS OF CURRENT LITERATURE
</title>
<figure confidence="0.807020166666667">
Semantically Guided Hierarchical
Deduction and Equality Conditional
Resolution
DAI V47(12), SecB, pp4965
Wang, Tie-Cheng
The University of Texas at Austin Ph.D.
1986, 154 pages
Computer Science
University Microfilms International
ADG87-06I24
Updating Databases With Incomplete
Information
DAI V47(12), SecB, pp4966
Winslett, Marianne Southall
Stanford University Ph.D. 1987, 174 pages
Computer Science
University Microfilms International
ADG87-07765
</figure>
<bodyText confidence="0.999902298245614">
This dissertation is concerned with research in designing
computer programs which are effective in finding, or helping to
find, proofs of theorems of first order logic. Discussed herein
are three interrelated topics. The first topic is the design of a
new, goal-oriented proof procedure—hierarchical deduction. The
second topic is semantically guided hierarchical deduction,
which concerns issues related to designing models for guiding
the search of a particular version of this procedure. The third
topic is equality conditional resolution, which provides a control
strategy useful for further development of hierarchical
deduction.
The hierarchical deduction procedure (HD) proves a theorem
by searching for a proof acceptable to a hierarchical deduction
structure; those derivations which are irrelevant to this proof
are limited by means of a set of completeness-preserving
refinements. In addition to the basic algorithm, there is a partial
set of support strategy which provides a simple but effective
way to incorporate semantics and human factors.
Semantically guided hierarchical deduction (SHD) is defined
from HD for incorporating domain-dependent knowledge
presented in models. A set of rules by which a user designs
models for SHD is investigated and applied to design models
for helping a SHD-prover prove efficiently a series of non-trivial
theorems. In order to include type information and other human
factors into models, a three-value interpretation is introduced.
An ECR strategy is described through studying an equality
conditional resolution proof procedure (ECR). ECR incorporates
a user&apos;s knowledge concerning the different roles of input
equations and other hypotheses in a proof. The input equations
are transformed into different classes of rules according to the
roles they will play. The conditions on the application of these
rules control their inference and prevent inappropriate use of
them. An ECR-prover, which is a modified SHD-prover
enhanced by the ECR strategy, is implemented and proves
efficiently a number of theorems from mathematics.
For each of these topics, a summary of results obtained by
the computer implementations is presented, and a concluding
remark is made in comparison with the performance of others.
Issues related to the extension and further development of SHD
and ECR proof procedures are discussed in the final chapter.
The proofs concerning the completeness of the basic algorithm
of the hierarchical deduction are included in an appendix.
Suppose one wishes to construct, use, and maintain a database
of facts about the real world, even though the state of that
world is only partially known. In the Al domain, this problem
arises when an agent has a base set of beliefs that reflect
partial knowledge about the world, and then tries to incorporate
new, possibly contradictory knowledge into this set of beliefs.
In the database domain, one facet of this situation is the well-
known null values problem. We choose to represent such a
database as a logical theory, and view the models of the theory
as representing possible states of the world that are consistent
with all known information.
How can new information be incorporated into the database?
For example, given the new information that &amp;quot;b or c is true,&amp;quot;
how can one get rid of all outdated information about b and c,
add the new information, and yet in the process not disturb
</bodyText>
<figure confidence="0.847104545454545">
Computational Linguistics, Volume 14, Number 2, June 1988 87
Abstracts of Current Literature
An Experimental Comparison of
Uncertain Inference Systems
DAI V47(12), SecB, pp4966
Wise, Ben Paul
Carnegie-Mellon University Ph.D. 1986,
260 pages
Computer Science
University Microfilms International
ADG87-02918
</figure>
<bodyText confidence="0.999721964912281">
any other information in the database? In current-day database
management systems, the burden of determining exactly what to
add and remove from the database is placed on the user.
Our research has produced a formal method of specifying the
desired change intentionally, by stating a well-formed formula
that the state of the world is now known to satisfy. The
database update algorithms we provide will automatically
accomplish that change. Our approach embeds the incomplete
database and the incoming information in the language of
mathematical logic, and gives formal definitions of the semantics
of our update operators, along with proofs of correctness for
their associated algorithms. We assess the computational
complexity of the algorithms, and propose a means of lazy
evaluation to avoid undesirable expense during execution of
updates. We also examine means of enforcing integrity
constraints as the database is updated.
This thesis also examines the question of choices of
semantics for update operators for databases with incomplete
information, and proposes a framework for evaluation of
competing candidate semantics. Several candidate semantics are
evaluated with respect to that framework.
An experimental implementation of our method has been
constructed, and we include the results of test runs on a range
of patterns of queries and updates.
Uncertainty is a pervasive feature of the domains in which
expert systems are supposed to function. There are several
mechanisms for handling uncertainty, of which the oldest and
most widely used is probability theory. It is the only one which
is derived from a formal description of rational behavior. For
use in patten-directed inference systems, or rule-based inference
engines, artificial intelligence researchers have favored others
largely for reasons of simplicity and speed. We have developed
techniques which measure how these alternatives approximate
the results of probability theory, assess how well they perform
by those measures, and find out what underlying features of a
problem affect performance.
Because the amount of data required to fully specify a
probability distribution is enormous, some technique must be
used to estimate a distribution when only partial information is
given. We give intuitive and axiomatic arguments, algebraic
analysis, and numerical examples, that fitting maximum entropy
priors and using minimum cross entropy updating are the most
appropriate ways to do so.
For several uncertain inference systems, detailed analysis of
operations have been performed to elucidate which basic
problem-features bias the answers and what are the directions
of the biases. We present and discuss both the motivation and
design of our analysis techniques, and the specific structures
which were found to have strong effects on perf ormance. The
techniques have also been tried on several variations of a
fragment from a real expert system, with qualitatively similar
results.
We have found that the newer uncertain inference systems
often re-incorporated features of general probability theory
which have been eliminated in earlier systems. Moreover, we
found that newer systems sometimes continued exactly the
features which they were supposed to eliminate, albeit in
</bodyText>
<page confidence="0.967897">
88 Computational Linguistics, Volume 14, Number 2, June 1988
</page>
<figure confidence="0.896515619047619">
Abstracts of Current Literature
Assessing and Forecasting the
Implications of Artificial Intelligence
Systems on Pedagogy in The Public
Sector
DAI V48(02), SecA, pp371
Skolyan, Kenneth Stephen
United States International University
Ed.D. 1987, 199 pages
Education, Technology
University Microfilms International
ADG87-11457
A Speech Act Theory of Metadiscourse
DAI V47(12), SecA, pp4374
Beauvais, Paul Jude
Michigan State University Ph.D. 1986,
157 pages
Language, Linguistics. Education, Teacher
Training
University Microfilms International
ADG87-07094
</figure>
<bodyText confidence="0.999792053571429">
different notation. For every simple uncertain inference system,
we found not only structures for which the performance was
very good, but also structures for which performance was
worse than random guessing, systematic bias was present, or
data was interpreted as having the opposite of its true import.
The Problem. The purpose of this study was to assess and
forecast the implications artificial intelligence systems would
have on teaching methods, teacher training, curriculum and
teacher-student roles in public education.
Method. A three-round Delphi Study was conducted. The
Delphi Panel included 23 participants from the areas of A. I.
research, computer-using teachers from universities and high
schools, and writers in the field.
Results. (I) Teacher Training and Reaction. (1) Resistance to
the introduction of artificial intelligence systems will occur. (2)
Additional training for teachers in computers is likely. (3) The
structure of teacher training will change from informational
transmittal to learning how people learn and the structure of
what they know. (II) Curriculum. (1) Schools will not be
radically changed. The goals of education and the importance of
reading and writing would remain. (2) Students would not be
learning a large portion of their lessons at home. (3) Elitist
separation by subject matter or social adjustment would not
occur. (4) The classroom would change. Multi-leveled, multi-
topical learning centers would be developed where students
could learn at their own pace. Changes in testing would occur
along with deeper student involvement with subject matter. (5)
New subjects and increases in strategy development and
problem solving would occur as a result of A. I. systems in the
classroom. (6) The A. I. system would become an indispensable
tool with frequent upgrade of use skills and development of
new subjects occurring. (III) Teacher/Student Roles. (1) Teacher
roles would not change from deliverance of knowledge and
skills to parent, counselor, or psychologist. (2) A. I. systems
would not fill the role of parent, counselor, or psychologist nor
transmit values. (3) The role of teacher would change from
learning director to co-problem solver. (4) A. I. systems would
greatly assist teachers in improving learning experiences,
diagnosing problems, and assisting students with learning
handicaps.
Metadiscourse commonly is defined as &amp;quot;discourse about
discoursing&amp;quot;. In its brief history, the term has appeared in
several models of text structure; however, theorists disagree
concerning the range of metadiscursive structures and the role
of metadiscourse in a larger theory of text linguistics. This
dissertation provides a detailed history and a critical analysis of
the existing metadiscourse theories, and it offers an alternative
theory that defines metadiscourse as a component of speech act
theory.
The first chapter surveys the history of metadiscourse from
Zellig S. Harris&apos; early use of the term to recent studies by
Joseph M. Williams, Avon Crismore, and William J. Vande
Kopple.
The second chapter introduces four criteria for evaluating the
utility of theoretical models. The existing metadiscourse models
are analyzed in light of these criteria and are found to contain
</bodyText>
<table confidence="0.599452363636364">
Computational Linguistics, Volume 14, Number 2, June 1988 89
Abstracts of Current Literature
Towards Computational Discrimination
of English Word Senses
DAI V4747(12), SecA, pp4376
Finn, Kathleen Ellen
Georgetown University Ph.D. 1986,
261 pages
Language, Linguistics
University Microfilms International
ADG87-07343
</table>
<bodyText confidence="0.999906842105263">
imprecise definitions of key terms. The models also are found
to be collections of disparate structures instead of principled
systems.
The third chapter provides an overview of important works
on speech act theory by J. L. Austin and John R. Searle.
Particular attention is devoted to the distinction between
illocutionary acts and propositions, the differences between
explicit performative structures and implicit expressions of
illocutionary intent, and the types of illocutionary acts that are
possible.
In the fourth chapter, metadiscourse is defined as those
illocutionary force indicators that identify expositive illocutionary
acts. A taxonomy of metadiscourse types is provided, and
canonical forms using performative or near-performative
structures are identified for each type. Partially explicit forms of
metadiscourse that do not provide an attributive subject are also
identified.
The dissertation concludes with suggestions for experimental
studies using the proposed metadiscourse model.
Although acoustically based automatic speech recognition (ASR)
systems have witnessed enormous improvements over the past
ten years, they still experience difficulties in several areas,
including operation in noisy environments.
Research with human subjects has shown that visible
information about the talker provides extensive benefit to
speech recognition in difficult listening situations. The primary
purpose of this study was to demonstrate the feasibility of
performing automatic speech recognition based only on optical
information. Other purposes included characterizing the nature
of the optical recognition process, comparing it to human visual
speech perception, and estimating its potential value to acoustic-
based ASR.
An optically-based algorithm was developed to recognize a
set of 23 English consonant phonemes. Distance measurements
were derived from a set of 12 dots placed on or near the
speaker&apos;s mouth. The dots offered a computationally simple
means of tracking lip movements.
After data reduction and selective weighting of the variables,
five distance variables were found capable of identifying 74% of
the phonemes, with no acoustic information whatsoever. The
same variables correctly identified 87% of the phonemes by
viseme groups. The machine&apos;s viseme set was very similar to
the human set.
The preponderance of the variables measured vertical
distances, suggesting that vertical opening, as opposed to
horizontal movement or area of mouth opening, is a critical cue
to optical speech recognition. The optical recognition process
was subjected to the effects of random visual noise at various
levels, and was found to be fairly robust.
The results of the optical recognition algorithm were
compared against the results of an acoustic algorithm operating
over the same speech tokens. The acoustic algorithm&apos;s
performance, subjected to signal-to-noise ratios ranging from
+25 dB to +65 dB, measured 64% correct phoneme
recognition. It was estimated that an effective combination of
the optical and acoustic recognition systems could result in 95%
recognition.
</bodyText>
<page confidence="0.957362">
90 Computational Linguistics, Volume 14, Number 2, June 1988
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.930517">ABSTRACTS OF CURRENT LITERATURE Semantically Guided Hierarchical Deduction and Equality Conditional Resolution</title>
<note confidence="0.807955">DAI V47(12), SecB, pp4965</note>
<author confidence="0.911179">Tie-Cheng Wang</author>
<affiliation confidence="0.954478">The University of Texas at Austin Ph.D.</affiliation>
<address confidence="0.786406">1986, 154 pages</address>
<affiliation confidence="0.996722">Computer Science University Microfilms International</affiliation>
<address confidence="0.551304">ADG87-06I24</address>
<title confidence="0.9306435">Updating Databases With Incomplete Information</title>
<note confidence="0.718102">DAI V47(12), SecB, pp4966</note>
<author confidence="0.632081">Marianne Southall Winslett</author>
<affiliation confidence="0.992933">Stanford University Ph.D. 1987, 174 pages Computer Science University Microfilms International</affiliation>
<abstract confidence="0.98930475862069">ADG87-07765 This dissertation is concerned with research in designing computer programs which are effective in finding, or helping to find, proofs of theorems of first order logic. Discussed herein are three interrelated topics. The first topic is the design of a new, goal-oriented proof procedure—hierarchical deduction. The second topic is semantically guided hierarchical deduction, which concerns issues related to designing models for guiding the search of a particular version of this procedure. The third topic is equality conditional resolution, which provides a control strategy useful for further development of hierarchical deduction. The hierarchical deduction procedure (HD) proves a theorem by searching for a proof acceptable to a hierarchical deduction structure; those derivations which are irrelevant to this proof are limited by means of a set of completeness-preserving refinements. In addition to the basic algorithm, there is a partial set of support strategy which provides a simple but effective way to incorporate semantics and human factors. Semantically guided hierarchical deduction (SHD) is defined from HD for incorporating domain-dependent knowledge presented in models. A set of rules by which a user designs models for SHD is investigated and applied to design models for helping a SHD-prover prove efficiently a series of non-trivial theorems. In order to include type information and other human factors into models, a three-value interpretation is introduced. An ECR strategy is described through studying an equality conditional resolution proof procedure (ECR). ECR incorporates a user&apos;s knowledge concerning the different roles of input equations and other hypotheses in a proof. The input equations are transformed into different classes of rules according to the roles they will play. The conditions on the application of these rules control their inference and prevent inappropriate use of them. An ECR-prover, which is a modified SHD-prover enhanced by the ECR strategy, is implemented and proves efficiently a number of theorems from mathematics. For each of these topics, a summary of results obtained by the computer implementations is presented, and a concluding remark is made in comparison with the performance of others. Issues related to the extension and further development of SHD and ECR proof procedures are discussed in the final chapter. The proofs concerning the completeness of the basic algorithm of the hierarchical deduction are included in an appendix. Suppose one wishes to construct, use, and maintain a database of facts about the real world, even though the state of that world is only partially known. In the Al domain, this problem arises when an agent has a base set of beliefs that reflect partial knowledge about the world, and then tries to incorporate new, possibly contradictory knowledge into this set of beliefs. In the database domain, one facet of this situation is the wellknown null values problem. We choose to represent such a database as a logical theory, and view the models of the theory as representing possible states of the world that are consistent with all known information. How can new information be incorporated into the database? For example, given the new information that &amp;quot;b or c is true,&amp;quot; how can one get rid of all outdated information about b and c, add the new information, and yet in the process not disturb</abstract>
<note confidence="0.899293">Computational Linguistics, Volume 14, Number 2, June 1988 87</note>
<title confidence="0.974359666666667">Abstracts of Current Literature An Experimental Comparison of Uncertain Inference Systems</title>
<note confidence="0.530832">DAI V47(12), SecB, pp4966</note>
<author confidence="0.975517">Ben Paul Wise</author>
<affiliation confidence="0.946339">Carnegie-Mellon University Ph.D. 1986,</affiliation>
<address confidence="0.935647">260 pages</address>
<affiliation confidence="0.9973215">Computer Science University Microfilms International</affiliation>
<address confidence="0.528835">ADG87-02918</address>
<abstract confidence="0.997719701754386">any other information in the database? In current-day database management systems, the burden of determining exactly what to add and remove from the database is placed on the user. Our research has produced a formal method of specifying the desired change intentionally, by stating a well-formed formula that the state of the world is now known to satisfy. The database update algorithms we provide will automatically accomplish that change. Our approach embeds the incomplete database and the incoming information in the language of mathematical logic, and gives formal definitions of the semantics of our update operators, along with proofs of correctness for their associated algorithms. We assess the computational complexity of the algorithms, and propose a means of lazy evaluation to avoid undesirable expense during execution of updates. We also examine means of enforcing integrity constraints as the database is updated. This thesis also examines the question of choices of semantics for update operators for databases with incomplete information, and proposes a framework for evaluation of competing candidate semantics. Several candidate semantics are evaluated with respect to that framework. An experimental implementation of our method has been constructed, and we include the results of test runs on a range of patterns of queries and updates. Uncertainty is a pervasive feature of the domains in which expert systems are supposed to function. There are several mechanisms for handling uncertainty, of which the oldest and most widely used is probability theory. It is the only one which is derived from a formal description of rational behavior. For use in patten-directed inference systems, or rule-based inference engines, artificial intelligence researchers have favored others largely for reasons of simplicity and speed. We have developed techniques which measure how these alternatives approximate the results of probability theory, assess how well they perform by those measures, and find out what underlying features of a problem affect performance. Because the amount of data required to fully specify a probability distribution is enormous, some technique must be used to estimate a distribution when only partial information is given. We give intuitive and axiomatic arguments, algebraic analysis, and numerical examples, that fitting maximum entropy priors and using minimum cross entropy updating are the most appropriate ways to do so. For several uncertain inference systems, detailed analysis of operations have been performed to elucidate which basic problem-features bias the answers and what are the directions of the biases. We present and discuss both the motivation and design of our analysis techniques, and the specific structures which were found to have strong effects on perf ormance. The techniques have also been tried on several variations of a fragment from a real expert system, with qualitatively similar results. We have found that the newer uncertain inference systems often re-incorporated features of general probability theory which have been eliminated in earlier systems. Moreover, we found that newer systems sometimes continued exactly the features which they were supposed to eliminate, albeit in</abstract>
<note confidence="0.453451142857143">88 Computational Linguistics, Volume 14, Number 2, June 1988 Abstracts of Current Literature Assessing and Forecasting the Implications of Artificial Intelligence Systems on Pedagogy in The Public Sector DAI V48(02), SecA, pp371</note>
<author confidence="0.976105">Kenneth Stephen Skolyan</author>
<affiliation confidence="0.99363">United States International University</affiliation>
<address confidence="0.469217">Ed.D. 1987, 199 pages</address>
<affiliation confidence="0.968049">Education, Technology University Microfilms International</affiliation>
<address confidence="0.57795">ADG87-11457</address>
<note confidence="0.5877366">A Speech Act Theory of Metadiscourse DAI V47(12), SecA, pp4374 Beauvais, Paul Jude Michigan State University Ph.D. 1986, 157 pages</note>
<author confidence="0.468102">Teacher Education</author>
<affiliation confidence="0.704586">Training University Microfilms International</affiliation>
<address confidence="0.7617">ADG87-07094</address>
<abstract confidence="0.995194307692308">different notation. For every simple uncertain inference system, we found not only structures for which the performance was very good, but also structures for which performance was worse than random guessing, systematic bias was present, or data was interpreted as having the opposite of its true import. Problem. purpose of this study was to assess and forecast the implications artificial intelligence systems would have on teaching methods, teacher training, curriculum and teacher-student roles in public education. three-round Delphi Study was conducted. The Delphi Panel included 23 participants from the areas of A. I. research, computer-using teachers from universities and high schools, and writers in the field.</abstract>
<intro confidence="0.884836">Teacher Training and Reaction. (1) Resistance to</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>