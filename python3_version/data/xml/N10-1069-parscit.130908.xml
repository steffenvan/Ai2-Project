<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.951158">
Distributed Training Strategies for the Structured Perceptron
</title>
<author confidence="0.953827">
Ryan McDonald Keith Hall Gideon Mann
</author>
<affiliation confidence="0.920945">
Google, Inc., New York / Zurich
</affiliation>
<email confidence="0.998544">
{ryanmcd|kbhall|gmann}@google.com
</email>
<sectionHeader confidence="0.994776" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999746631578947">
Perceptron training is widely applied in the
natural language processing community for
learning complex structured models. Like all
structured prediction learning frameworks, the
structured perceptron can be costly to train
as training complexity is proportional to in-
ference, which is frequently non-linear in ex-
ample sequence length. In this paper we
investigate distributed training strategies for
the structured perceptron as a means to re-
duce training times when computing clusters
are available. We look at two strategies and
provide convergence bounds for a particu-
lar mode of distributed structured perceptron
training based on iterative parameter mixing
(or averaging). We present experiments on
two structured prediction problems – named-
entity recognition and dependency parsing –
to highlight the efficiency of this method.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995368">
One of the most popular training algorithms for
structured prediction problems in natural language
processing is the perceptron (Rosenblatt, 1958;
Collins, 2002). The structured perceptron has many
desirable properties, most notably that there is no
need to calculate a partition function, which is
necessary for other structured prediction paradigms
such as CRFs (Lafferty et al., 2001). Furthermore,
it is robust to approximate inference, which is of-
ten required for problems where the search space
is too large and where strong structural indepen-
dence assumptions are insufficient, such as parsing
(Collins and Roark, 2004; McDonald and Pereira,
2006; Zhang and Clark, 2008) and machine trans-
lation (Liang et al., 2006). However, like all struc-
tured prediction learning frameworks, the structure
perceptron can still be cumbersome to train. This
is both due to the increasing size of available train-
ing sets as well as the fact that training complexity
is proportional to inference, which is frequently non-
linear in sequence length, even with strong structural
independence assumptions.
In this paper we investigate distributed training
strategies for the structured perceptron as a means
of reducing training times when large computing
clusters are available. Traditional machine learning
algorithms are typically designed for a single ma-
chine, and designing an efficient training mechanism
for analogous algorithms on a computing cluster –
often via a map-reduce framework (Dean and Ghe-
mawat, 2004) – is an active area of research (Chu
et al., 2007). However, unlike many batch learning
algorithms that can easily be distributed through the
gradient calculation, a distributed training analog for
the perceptron is less clear cut. It employs online up-
dates and its loss function is technically non-convex.
A recent study by Mann et al. (2009) has shown
that distributed training through parameter mixing
(or averaging) for maximum entropy models can
be empirically powerful and has strong theoretical
guarantees. A parameter mixing strategy, which can
be applied to any parameterized learning algorithm,
trains separate models in parallel, each on a disjoint
subset of the training data, and then takes an average
of all the parameters as the final model. In this paper,
we provide results which suggest that the percep-
tron is ill-suited for straight-forward parameter mix-
ing, even though it is commonly used for large-scale
structured learning, e.g., Whitelaw et al. (2008) for
named-entity recognition. However, a slight mod-
</bodyText>
<page confidence="0.986175">
456
</page>
<note confidence="0.752594">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 456–464,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999941571428571">
ification we call iterative parameter mixing can be
shown to: 1) have similar convergence properties to
the standard perceptron algorithm, 2) find a sepa-
rating hyperplane if the training set is separable, 3)
reduce training times significantly, and 4) produce
models with comparable (or superior) accuracies to
those trained serially on all the data.
</bodyText>
<sectionHeader confidence="0.997791" genericHeader="related work">
2 Related Work
</sectionHeader>
<equation confidence="0.993292">
Perceptron(T = {(xt, yt)}|T |
t�1)
</equation>
<listItem confidence="0.8963965">
1. w(°) = 0; k = 0
2. for n : 1..N
3. fort : 1..T
4. Let y0 = arg maxy, w(k) · f(xt, y0)
5. if y0 =� yt
6. w(k+1) = w(k) + f(xt,yt) − f(xt,y0)
7. k = k + 1
8. return w(k)
</listItem>
<figureCaption confidence="0.999861">
Figure 1: The perceptron algorithm.
</figureCaption>
<bodyText confidence="0.99966180952381">
Distributed cluster computation for many batch
training algorithms has previously been examined
by Chu et al. (2007), among others. Much of the
relevant prior work on online (or sub-gradient) dis-
tributed training has been focused on asynchronous
optimization via gradient descent. In this sce-
nario, multiple machines run stochastic gradient de-
scent simultaneously as they update and read from
a shared parameter vector asynchronously. Early
work by Tsitsiklis et al. (1986) demonstrated that
if the delay between model updates and reads is
bounded, then asynchronous optimization is guaran-
teed to converge. Recently, Zinkevich et al. (2009)
performed a similar type of analysis for online learn-
ers with asynchronous updates via stochastic gra-
dient descent. The asynchronous algorithms in
these studies require shared memory between the
distributed computations and are less suitable to
the more common cluster computing environment,
which is what we study here.
While we focus on the perceptron algorithm, there
is a large body of work on training structured pre-
diction classifiers. For batch training the most com-
mon is conditional random fields (CRFs) (Lafferty
et al., 2001), which is the structured analog of maxi-
mum entropy. As such, its training can easily be dis-
tributed through the gradient or sub-gradient com-
putations (Finkel et al., 2008). However, unlike per-
ceptron, CRFs require the computation of a partition
function, which is often expensive and sometimes
intractable. Other batch learning algorithms include
M3Ns (Taskar et al., 2004) and Structured SVMs
(Tsochantaridis et al., 2004). Due to their efficiency,
online learning algorithms have gained attention, es-
pecially for structured prediction tasks in NLP. In
addition to the perceptron (Collins, 2002), others
have looked at stochastic gradient descent (Zhang,
2004), passive aggressive algorithms (McDonald et
al., 2005; Crammer et al., 2006), the recently intro-
duced confidence weighted learning (Dredze et al.,
2008) and coordinate descent algorithms (Duchi and
Singer, 2009).
</bodyText>
<sectionHeader confidence="0.984762" genericHeader="method">
3 Structured Perceptron
</sectionHeader>
<bodyText confidence="0.978764379310345">
The structured perceptron was introduced by Collins
(2002) and we adopt much of the notation and pre-
sentation of that study. The structured percetron al-
gorithm – which is identical to the multi-class per-
ceptron – is shown in Figure 1. The perceptron is an
online learning algorithm and processes training in-
stances one at a time during each epoch of training.
Lines 4-6 are the core of the algorithm. For a input-
output training instance pair (xt, yt) E T, the algo-
rithm predicts a structured output y&apos; E Yt, where Yt
is the space of permissible structured outputs for in-
put xt, e.g., parse trees for an input sentence. This
prediction is determined by a linear classifier based
on the dot product between a high-dimensional fea-
ture representation of a candidate input-output pair
f(x, y) E RM and a corresponding weight vector
w E RM, which are the parameters of the model1.
If this prediction is incorrect, then the parameters
are updated to add weight to features for the cor-
responding correct output yt and take weight away
from features for the incorrect output y&apos;. For struc-
tured prediction, the inference step in line 4 is prob-
lem dependent, e.g., CKY for context-free parsing.
A training set T is separable with margin -y &gt;
0 if there exists a vector u E RM with I IuI I = 1
such that u · f(xt, yt) − u · f(xt, y&apos;) &gt; -y, for all
(xt, yt) E T, and for all y&apos; E Yt such that y&apos; =� yt.
Furthermore, let R &gt; ||f(xt, yt)−f(xt, y&apos;)||, for all
(xt, yt) E T and y&apos; E Yt. A fundamental theorem
</bodyText>
<footnote confidence="0.971914">
1The perceptron can be kernalized for non-linearity.
</footnote>
<page confidence="0.997831">
457
</page>
<bodyText confidence="0.9562284">
of the perceptron is as follows:
Theorem 1 (Novikoff (1962)). Assume training set
T is separable by margin &apos;y. Let k be the number of
mistakes made training the perceptron (Figure 1) on
T. If training is run indefinitely, then k &lt; R2
</bodyText>
<equation confidence="0.895414">
�2 .
Proof. See Collins (2002) Theorem 1.
</equation>
<bodyText confidence="0.998781333333334">
Theorem 1 implies that if T is separable then 1) the
perceptron will converge in a finite amount of time,
and 2) will produce a w that separates T . Collins
also proposed a variant of the structured perceptron
where the final weight vector is a weighted average
of all parameters that occur during training, which
he called the averaged perceptron and can be viewed
as an approximation to the voted perceptron algo-
rithm (Freund and Schapire, 1999).
</bodyText>
<sectionHeader confidence="0.990902" genericHeader="method">
4 Distributed Structured Perceptron
</sectionHeader>
<bodyText confidence="0.999677333333333">
In this section we examine two distributed training
strategies for the perceptron algorithm based on pa-
rameter mixing.
</bodyText>
<subsectionHeader confidence="0.99808">
4.1 Parameter Mixing
</subsectionHeader>
<bodyText confidence="0.999930625">
Distributed training through parameter mixing is a
straight-forward way of training classifiers in paral-
lel. The algorithm is given in Figure 2. The idea is
simple: divide the training data T into S disjoint
shards such that T = {T1, ... , TS}. Next, train
perceptron models (or any learning algorithm) on
each shard in parallel. After training, set the final
parameters to a weighted mixture of the parameters
of each model using mixture coefficients µ. Note
that we call this strategy parameter mixing as op-
posed to parameter averaging to distinguish it from
the averaged perceptron (see previous section). It is
easy to see how this can be implemented on a cluster
through a map-reduce framework, i.e., the map step
trains the individual models in parallel and the re-
duce step mixes their parameters. The advantages of
parameter mixing are: 1) that it is parallel, making
it possibly to scale to extremely large data sets, and
2) it is resource efficient, in particular with respect
to network usage as parameters are not repeatedly
passed across the network as is often the case for
exact distributed training strategies.
For maximum entropy models, Mann et al. (2009)
show it is possible to bound the norm of the dif-
</bodyText>
<equation confidence="0.997279">
PerceptronParamMix(T = {(xt, yt)}|� |
t�1)
</equation>
<listItem confidence="0.96830825">
1. Shard T into S pieces T = {T1, ... , TS}
2. w(i) = Perceptron(Ti) †
3. w = Ei µiw(i) $
4. return w
</listItem>
<figureCaption confidence="0.89108">
Figure 2: Distributed perceptron using a parameter mix-
ing strategy. † Each w(i) is computed in parallel. $ µ =
{µ1, ... , µS}, Vµi E µ : µi &gt; 0 and Ei µi = 1.
</figureCaption>
<bodyText confidence="0.99932225">
ference between parameters trained on all the data
serially versus parameters trained with parameter
mixing. However, their analysis requires a stabil-
ity bound on the parameters of a regularized max-
imum entropy model, which is not known to hold
for the perceptron. In Section 5, we present empir-
ical results showing that parameter mixing for dis-
tributed perceptron can be sub-optimal. Addition-
ally, Dredze et al. (2008) present negative parame-
ter mixing results for confidence weighted learning,
which is another online learning algorithm. The fol-
lowing theorem may help explain this behavior.
</bodyText>
<construct confidence="0.9197395">
Theorem 2. For a any training set T separable by
margin &apos;y, the perceptron algorithm trained through
a parameter mixing strategy (Figure 2) does not nec-
essarily return a separating weight vector w.
</construct>
<bodyText confidence="0.850719523809524">
Proof. Consider a binary classification setting
where Y = {0, 1} and T has 4 instances.
We distribute the training set into two shards,
T1 = {(x1,1, y1,1), (x1,2, y1,2)} and T2 =
{(x2,1, y2,1), (x2,2, y2,2)}. Let y1,1 = y2,1 = 0 and
y1,2 = y2,2 = 1. Now, let w, f E R6 and using
block features, define the feature space as,
f(x1,1, 0) _ [1 1 0 0 0 0] f(x1,1, 1) _ [0 0 0 1 1 0]
f(x1,2, 0) _ [0 0 1 0 0 0] f(x1,2, 1) _ [0 0 0 0 0 1]
f(x2,1, 0) _ [0 1 1 0 0 0] f(x2,1, 1) _ [0 0 0 0 1 1]
f(x2,2, 0) _ [1 0 0 0 0 0] f(x2,2, 1) _ [0 0 0 1 0 0]
Assuming label 1 tie-breaking, parameter mixing re-
turns w1=[1 1 0 -1 -1 0] and w2=[0 1 1 0 -1 -1]. For
any µ, the mixed weight vector w will not separate
all the points. If both µ1/µ2 are non-zero, then all
examples will be classified 0. If µ1=1 and µ2=0,
then (x2,2, y2,2) will be incorrectly classified as 0
and (x1,2, y1,2) when µ1=0 and µ2=1. But there is a
separating weight vector w = [-1 2 -1 1 -2 1].
This counter example does not say that a parameter
mixing strategy will not converge. On the contrary,
</bodyText>
<page confidence="0.99405">
458
</page>
<bodyText confidence="0.9986972">
if T is separable, then each of its subsets is separa-
ble and converge via Theorem 1. What it does say
is that, independent of µ, the mixed weight vector
produced after convergence will not necessarily sep-
arate the entire data, even when T is separable.
</bodyText>
<subsectionHeader confidence="0.989508">
4.2 Iterative Parameter Mixing
</subsectionHeader>
<bodyText confidence="0.914530948717949">
Consider a slight augmentation to the parameter
mixing strategy. Previously, each parallel percep-
tron was trained to convergence before the parame-
ter mixing step. Instead, shard the data as before, but
train a single epoch of the perceptron algorithm for
each shard (in parallel) and mix the model weights.
This mixed weight vector is then re-sent to each
shard and the perceptrons on those shards reset their
weights to the new mixed weights. Another single
epoch of training is then run (again in parallel over
the shards) and the process repeats. This iterative
parameter mixing algorithm is given in Figure 3.
Again, it is easy to see how this can be imple-
mented as map-reduce, where the map computes the
parameters for each shard for one epoch and the re-
duce mixes and re-sends them. This is analogous
to batch distributed gradient descent methods where
the gradient for each shard is computed in parallel in
the map step and the reduce step sums the gradients
and updates the weight vector. The disadvantage of
iterative parameter mixing, relative to simple param-
eter mixing, is that the amount of information sent
across the network will increase. Thus, if network
latency is a bottleneck, this can become problematic.
However, for many parallel computing frameworks,
including both multi-core computing as well as clus-
ter computing with high rates of connectivity, this is
less of an issue.
Theorem 3. Assume a training set T is separable
by margin γ. Let ki,n be the number of mistakes that
occurred on shard i during the nth epoch of train-
ing. For any N, when training the perceptron with
iterative parameter mixing (Figure 3),
µi,nki,n ≤R2
γ
Proof. Let w(i,n) to be the weight vector for the
ith shard after the nth epoch of the main loop and
let w([i,n]−k) be the weight vector that existed on
shard i in the nth epoch k errors before w(i,n). Let
</bodyText>
<equation confidence="0.9955675">
PerceptronIterParamMix(T = {(xt, yt)}�� �
t=1)
</equation>
<listItem confidence="0.9812095">
1. Shard T into S pieces T = {T1, ... , TS}
2. w = 0
3. for n : 1..N
4. w(i,n) = OneEpochPerceptron(Ti, w) †
</listItem>
<equation confidence="0.60797275">
w = E
5. i µi,nw(i,n) ‡
6. return w
OneEpochPerceptron(T, w*)
</equation>
<listItem confidence="0.965087142857143">
1. w(0) = w*; k = 0
2. fort: 1..T
3. Let y&apos; = arg max,, w(k) · f(xt, y&apos;)
4. if y&apos; =6 yt
5. w(k+1) = w(k) + f(xt,yt) − f(xt,y&apos;)
6. k = k + 1
7. return w(k)
</listItem>
<figureCaption confidence="0.988818">
Figure 3: Distributed perceptron using an iterative param-
</figureCaption>
<bodyText confidence="0.6660654">
eter mixing strategy. † Each w(i,n) is computed in paral-
lel. ‡ µn = {µ1,n, ... , µS,n}, ∀µi,n ∈ µn: µi,n ≥ 0 and
∀n: Ei µi,n = 1.
w(avg,n) be the mixed vector from the weight vec-
tors returned after the nth epoch, i.e.,
</bodyText>
<equation confidence="0.997284333333333">
S
w(avg,n) = µi,nw(i,n)
i=1
</equation>
<bodyText confidence="0.787466">
Following the analysis from Collins (2002) Theorem
1, by examining line 5 of OneEpochPerceptron in
Figure 3 and the fact that u separates the data by γ:
</bodyText>
<equation confidence="0.945683166666667">
u · w(i,n) = u · w([i,n]−1)
+ u · (f(xt,yt) − f(xt,y&apos;))
≥ u · w([i,n]−1) + γ
≥ u · w([i,n]−2) + 2γ
... ≥ u · w(avg,n−1) + ki,nγ (A1)
That is, u · w(i,n) is bounded below by the average
</equation>
<bodyText confidence="0.9600984">
weight vector for the n-1st epoch plus the number
of mistakes made on shard i during the nth epoch
times the margin γ. Next, by OneEpochPerceptron
line 5, the definition of R, and w([i,n]−1)(f(xt, yt)−
f(xt, y&apos;)) ≤ 0 when line 5 is called:
</bodyText>
<equation confidence="0.9920501">
kw(i,n)k2 = kw([i,n]−1)k2
+kf(xt,yt) − f(xt,y&apos;)k2
+ 2w([i,n]−1)(f(xt, yt) − f(xt, y&apos;))
≤ kw([i,n]−1)k2 + R2
≤ kw([i,n]−2)k2 + 2R2
. . .≤ kw(avg,n−1)k2 + ki,nR2 (A2)
S
i=1
N
n=1
</equation>
<page confidence="0.978791">
459
460
</page>
<equation confidence="0.994929232558139">
XS
i=1
kw(avg,N)k2 ≤
µi,Nkw(i,N)k2
XS
≤
i=1
µi,N(kw(avg,N−1)k2 + ki,NR2)
XS
i=1
XN
n=1
XS
i=1
µi,Nki,NR2
u · w(avg,N) ≥
µi,nki,nγ (IH1)
= kw(avg,N−1)k2 +
XS
i=1
µi,1ki,1γ
u · wavg,1 = XS µi,1u · w(i,1) ≥
i=1
XS
i=1
u · w(avg,N) =
XS
i=1
≥
µi,N(u · w(i,N))
µi,N(u · w(avg,N−1) + ki,Nγ)
XS
i=1
µi,Nki,Nγ
= u · w(avg,N−1) +
S
X
i=1
S
X
i=1
µi,1kw(i,1)k2 ≤
µi,1ki,1R2
</equation>
<bodyText confidence="0.992323833333333">
That is, the squared L2-norm of a shards weight vec-
tor is bounded above by the same value for the aver-
age weight vector of the n-1st epoch and the number
of mistakes made on that shard during the nth epoch
times R2.
Using A1/A2 we prove two inductive hypotheses:
</bodyText>
<equation confidence="0.7782514">
we can write:
µi,nki,nR2 (IH2)
IH1 implies kw(avg,N)k ≥ PN PS i=1 µi,nki,nγ
n=1
since u · w ≤ kukkwk and kuk = 1.
</equation>
<bodyText confidence="0.822092">
The base case is w(avg,1), where we can observe:
using A1 and the fact that w(avg,0) = 0 for the sec-
ond step. For the IH2 base case we can write:
</bodyText>
<equation confidence="0.8537855">
kw(avg,1)k2 =
≤
</equation>
<bodyText confidence="0.85925">
The first inequality is Jensen’s inequality, and the
second is true by A2 and kw(avg,0)k2 = 0.
Proceeding to the general case, w(avg,N):
</bodyText>
<equation confidence="0.835887090909091">
N−1 S S
≥ X X µi,nki,nγ + X µi,Nki,N
n=1 i=1 i=1
=
µi,nki,nγ
N
X
n=1
S
X
i=1
</equation>
<bodyText confidence="0.60575625">
The first inequality uses A1, the second step
P
i µi,N = 1 and the second inequality the induc-
tive hypothesis IH1. For IH2, in the general case,
</bodyText>
<equation confidence="0.9950264">
&amp;quot;N−1X #
XS XS
µi,nki,nR2 +
n=1 i=1
µi,nki,nR2
</equation>
<bodyText confidence="0.999204">
The first inequality is Jensen’s, the second A2, and
the third the inductive hypothesis IH2. Putting to-
gether IH1, IH2 and kw(avg,N)k ≥ u · w(avg,N):
</bodyText>
<equation confidence="0.883101">
which yields: PN PS i=1 µi,nki,n ≤ R2
n=1 γ2
4.3 Analysis
</equation>
<bodyText confidence="0.964366863636364">
If we set each µn to be the uniform mixture, µi,n =
1/S, then Theorem 3 guarantees convergence to
a separating hyperplane. If PSi=1 µi,nki,n = 0,
then the previous weight vector already separated
the data. Otherwise, PN PS i=1 µi,nki,n is still in-
n=1
creasing, but is bounded and cannot increase indefi-
nitely. Also note that if S = 1, then µ1,n must equal
1 for all n and this bound is identical to Theorem 1.
However, we are mainly concerned with how fast
convergence occurs, which is directly related to the
number of training epochs each algorithm must run,
i.e., N in Figure 1 and Figure 3. For the non-
distributed variant of the perceptron we can say that
Nnon dist ≤ R2/γ2 since in the worst case a single
mistake happens on each epoch.2 For the distributed
case, consider setting µi,n = ki,n/kn, where kn =
P
i ki,n. That is, we mix parameters proportional to
the number of errors each made during the previous
epoch. Theorem 3 still implies convergence to a sep-
arating hyperplane with this choice. Further, we can
</bodyText>
<equation confidence="0.980679878787879">
,Nki,NR2
2It is not hard to derive such degenerate cases.
µi
i=1
&amp;quot;XN #2 &amp;quot; XN #
XS XS
µi,nki,n γ2 ≤ µi,nki,n R2
n=1 i=1 n=1
i=1
S
X
i=1
N
X
n=1
kw(avg,N)k2 ≤
S
X
i=1
N
X
n=1
II � � � �
2
� � � � �
µi,1w(i,1)
S
X
i=1
bound the required number of epochs Ndigt:
ki,n�2
ki,n ≤
kn �2
</equation>
<bodyText confidence="0.999153846153846">
Ignoring when all ki,n are zero (since the algorithm
will have converged), the first inequality is true since
either ki,n ≥ 1, implying that [ki,n]ki,n/kn ≥ 1, or
ki,n = 0 and [ki,n]ki,n/kn = 1. The second inequal-
ity is true by the generalized arithmetic-geometric
mean inequality and the final inequality is Theo-
rem 3. Thus, the worst-case number of epochs is
identical for both the regular and distributed percep-
tron – but the distributed perceptron can theoreti-
cally process each epoch 5 times faster. This ob-
servation holds only for cases where Ai,n &gt; 0 when
ki,n ≥ 1 and Ai,n = 0 when ki,n = 0, which does
not include uniform mixing.
</bodyText>
<sectionHeader confidence="0.999543" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99881875">
To investigate the distributed perceptron strategies
discussed in Section 4 we look at two structured pre-
diction tasks – named entity recognition and depen-
dency parsing. We compare up to four systems:
</bodyText>
<listItem confidence="0.987763444444444">
1. Serial (All Data): This is the classifier returned
if trained serially on all the available data.
2. Serial (Sub Sampling): Shard the data, select
one shard randomly and train serially.
3. Parallel (Parameter Mix): Parallel strategy
discussed in Section 4.1 with uniform mixing.
4. Parallel (Iterative Parameter Mix): Parallel
strategy discussed in Section 4.2 with uniform
mixing (Section 5.1 looks at mixing strategies).
</listItem>
<bodyText confidence="0.995919032786885">
For all four systems we compare results for both the
standard perceptron algorithm as well as the aver-
aged perceptron algorithm (Collins, 2002).
We report the final test set metrics of the con-
verged classifiers to determine whether any loss in
accuracy is observed as a consequence of distributed
training strategies. We define convergence as ei-
ther: 1) the training set is separated, or 2) the train-
ing set performance measure (accuracy, f-measure,
etc.) does not change by more than some pre-defined
threshold on three consecutive epochs. As with most
real world data sets, convergence by training set sep-
aration was rarely observed, though in both cases
training set accuracies approached 100%. For both
tasks we also plot test set metrics relative to the user
wall-clock taken to obtain the classifier. The results
were computed by collecting the metrics at the end
of each epoch for every classifier. All experiments
used 10 shards (Section 5.1 looks at convergence rel-
ative to different shard size).
Our first experiment is a named-entity recogni-
tion task using the English data from the CoNLL
2003 shared-task (Tjong Kim Sang and De Meul-
der, 2003). The task is to detect entities in sentences
and label them as one of four types: people, organi-
zations, locations or miscellaneous. For our exper-
iments we used the entire training set (14041 sen-
tences) and evaluated on the official development
set (3250 sentences). We used a straight-forward
IOB label encoding with a 1st order Markov fac-
torization. Our feature set consisted of predicates
extracted over word identities, word affixes, orthog-
raphy, part-of-speech tags and corresponding con-
catenations. The evaluation metric used was micro
f-measure over the four entity class types.
Results are given in Figure 4. There are a num-
ber of things to observe here: 1) training on a single
shard clearly provides inferior performance to train-
ing on all data, 2) the simple parameter mixing strat-
egy improves upon a single shard, but does not meet
the performance of training on all data, 3) iterative
parameter mixing achieves performance as good as
or better than training serially on all the data, and
4) the distributed algorithms return better classifiers
much quicker than training serially on all the data.
This is true regardless of whether the underlying al-
gorithm is the regular or the averaged perceptron.
Point 3 deserves more discussion. In particular, the
iterative parameter mixing strategy has a higher final
f-measure than training on all the data serially than
the standard perceptron (f-measure of 87.9 vs. 85.8).
We suspect this happens for two reasons. First, the
parameter mixing has a bagging like effect which
helps to reduce the variance of the per-shard classi-
fiers (Breiman, 1996). Second, the fact that parame-
ter mixing is just a form of parameter averaging per-
haps has the same effect as the averaged perceptron.
Our second set of experiments looked at the much
more computationally intensive task of dependency
parsing. We used the Prague Dependency Tree-
bank (PDT) (Hajiˇc et al., 2001), which is a Czech
</bodyText>
<figure confidence="0.99889075">
S
i��
k% n
kn ≤
[ki,n]
Ndi.t ≤
NdietE
n��
NdietE
n��
S
i��
</figure>
<page confidence="0.956685">
461
</page>
<figure confidence="0.581065">
Wall Clock
Serial (All Data)
Serial (Sub Sampling)
Parallel (Parameter Mix)
Parallel (Iterative Parameter Mix)
Wall Clock
Reg. Perceptron Avg. Perceptron
F-measure F-measure
85.8 88.2
75.3 76.6
81.5 81.6
87.9 88.1
Test Data F-measure
0.85
0.75
0.65
0.8
0.7
Perceptron -- Serial (All Data)
Perceptron -- Serial (Sub Sampling)
Perceptron -- Parallel (Parameter Mix)
Perceptron -- Parallel (Iterative Parameter Mix)
Averaged Perceptron -- Serial (All Data)
Averaged Perceptron -- Serial (Sub Sampling)
Averaged Perceptron -- Parallel (Parameter Mix)
Averaged Perceptron -- Parallel (Iterative Parameter Mix)
Test Data F-measure 0.85
0.8
0.75
0.7
</figure>
<figureCaption confidence="0.99646">
Figure 4: NER experiments. Upper figures plot test data f-measure versus wall clock for both regular perceptron (left)
and averaged perceptron (right). Lower table is f-measure for converged models.
</figureCaption>
<bodyText confidence="0.999650214285714">
language treebank and currently one of the largest
dependency treebanks in existence. We used the
CoNLL-X training (72703 sentences) and testing
splits (365 sentences) of this data (Buchholz and
Marsi, 2006) and dependency parsing models based
on McDonald and Pereira (2006) which factors fea-
tures over pairs of dependency arcs in a tree. To
parse all the sentences in the PDT, one must use a
non-projective parsing algorithm, which is a known
NP-complete inference problem when not assuming
strong independence assumptions. Thus, the use of
approximate inference techniques is common in or-
der to find the highest weighted tree for a sentence.
We use the approximate parsing algorithm given in
McDonald and Pereira (2006), which runs in time
roughly cubic in sentence length. To train such a
model is computationally expensive and can take on
the order of days to train on a single machine.
Unlabeled attachment scores (Buchholz and
Marsi, 2006) are given in Figure 5. The same trends
are seen for dependency parsing that are seen for
named-entity recognition. That is, iterative param-
eter mixing learns classifiers faster and has a final
accuracy as good as or better than training serially
on all data. Again we see that the iterative parame-
ter mixing model returns a more accurate classifier
than the regular perceptron, but at about the same
level as the averaged perceptron.
</bodyText>
<subsectionHeader confidence="0.998765">
5.1 Convergence Properties
</subsectionHeader>
<bodyText confidence="0.999865541666667">
Section 4.3 suggests that different weighting strate-
gies can lead to different convergence properties,
in particular with respect to the number of epochs.
For the named-entity recognition task we ran four
experiments comparing two different mixing strate-
gies – uniform mixing (µi,n=1/5) and error mix-
ing (µi,n=ki,n/kn) – each with two shard sizes –
5 = 10 and 5 = 100. Figure 6 plots the number
of training errors per epoch for each strategy.
We can make a couple observations. First, the
mixing strategy makes little difference. The rea-
son being that the number of observed errors per
epoch is roughly uniform across shards, making
both strategies ultimately equivalent. The other ob-
servation is that increasing the number of shards
can slow down convergence when viewed relative to
epochs3. Again, this appears in contradiction to the
analysis in Section 4.3, which, at least for the case
of error weighted mixtures, implied that the num-
ber of epochs to convergence was independent of
the number of shards. But that analysis was based
on worst-case scenarios where a single error occurs
on a single shard at each epoch, which is unlikely to
occur in real world data. Instead, consider the uni-
</bodyText>
<footnote confidence="0.9337815">
3As opposed to raw wall-clock/CPU time, which benefits
from faster epochs the more shards there are.
</footnote>
<page confidence="0.998082">
462
</page>
<figure confidence="0.954033966666667">
Wall Clock
Wall Clock
Perceptron -- Serial (All Data)
Perceptron -- Serial (Sub Sampling)
Perceptron -- Parallel (Iterative Parameter Mix)
Averaged Perceptron -- Serial (All Data)
Averaged Perceptron -- Serial (Sub Sampling)
Averaged Perceptron -- (Iterative Parameter Mix)
Unlabeled Attachment Score 0.84
0.82
0.8
0.78
0.76
0.74
Unlabeled Attachment Score 0.85
0.84
0.83
0.82
0.81
0.8
0.79
0.78
Serial (All Data)
Serial (Sub Sampling)
Parallel (Iterative Parameter Mix)
Reg. Perceptron Avg. Perceptron
Unlabeled Attachment Score Unlabeled Attachment Score
81.3 84.7
77.2 80.1
83.5 84.5
</figure>
<figureCaption confidence="0.9957114">
Figure 5: Dependency Parsing experiments. Upper figures plot test data unlabeled attachment score versus wall clock
for both regular perceptron (left) and averaged perceptron (right). Lower table is unlabeled attachment score for
converged models.
Figure 6: Training errors per epoch for different shard
size and parameter mixing strategies.
</figureCaption>
<bodyText confidence="0.923764">
form mixture case. Theorem 3 implies:
</bodyText>
<equation confidence="0.683397666666667">
2
ki,n &lt; S xR
γ
</equation>
<bodyText confidence="0.999988238095238">
Thus, for cases where training errors are uniformly
distributed across shards, it is possible that, in the
worst-case, convergence may slow proportional the
the number of shards. This implies a trade-off be-
tween slower convergence and quicker epochs when
selecting a large number of shards. In fact, we ob-
served a tipping point for our experiments in which
increasing the number of shards began to have an ad-
verse effect on training times, which for the named-
entity experiments occurred around 25-50 shards.
This is both due to reasons described in this section
as well as the added overhead of maintaining and
summing multiple high-dimensional weight vectors
after each distributed epoch.
It is worth pointing out that a linear term S in
the convergence bound above is similar to conver-
gence/regret bounds for asynchronous distributed
online learning, which typically have bounds lin-
ear in the asynchronous delay (Mesterharm, 2005;
Zinkevich et al., 2009). This delay will be on aver-
age roughly equal to the number of shards S.
</bodyText>
<sectionHeader confidence="0.999488" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999612066666667">
In this paper we have investigated distributing the
structured perceptron via simple parameter mixing
strategies. Our analysis shows that an iterative pa-
rameter mixing strategy is both guaranteed to sepa-
rate the data (if possible) and significantly reduces
the time required to train high accuracy classifiers.
However, there is a trade-off between increasing
training times through distributed computation and
slower convergence relative to the number of shards.
Finally, we note that using similar proofs to those
given in this paper, it is possible to provide theoreti-
cal guarantees for distributed online passive aggres-
sive learning (Crammer et al., 2006), which is a form
of large-margin perceptron learning. Unfortunately
space limitations prevent exploration here.
</bodyText>
<footnote confidence="0.584118">
Acknowledgements: We thank Mehryar Mohri, Fer-
nando Periera, Mark Dredze and the three anonymous re-
views for their helpful comments on this work.
</footnote>
<figure confidence="0.997863857142857">
0 10 20 30 40 50
Training Epochs
# Training Mistakes
10000
8000
6000
4000
2000
0
Error mixing (10 shards)
Uniform mixing (10 shards)
Error mixing (100 shards)
Uniform mixing (100 shards)
R2
ki,n &lt;
S
γ2
��
S
i=1
N
E
n=1
N
E
n=1
S
i=1
</figure>
<page confidence="0.999644">
463
</page>
<sectionHeader confidence="0.993884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999916209523809">
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123–140.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning.
C.T. Chu, S.K. Kim, Y.A. Lin, Y.Y. Yu, G. Bradski, A.Y.
Ng, and K. Olukotun. 2007. Map-Reduce for ma-
chine learning on multicore. In Advances in Neural
Information Processing Systems.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithm. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. The Journal of Machine Learning Research,
7:551–585.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In Sixth Sym-
posium on Operating System Design and Implementa-
tion.
M. Dredze, K. Crammer, and F. Pereira. 2008.
Confidence-weighted linear classification. In Pro-
ceedings of the International Conference on Machine
learning.
J. Duchi and Y. Singer. 2009. Efficient learning using
forward-backward splitting. In Advances in Neural In-
formation Processing Systems.
J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In Proceedings of the Conference of the Association
for Computational Linguistics.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277–296.
J. Hajiˇc, B. Vidova Hladka, J. Panevov´a, E. Hajiˇcov´a,
P. Sgall, and P. Pajas. 2001. Prague Dependency Tree-
bank 1.0. LDC, 2001T10.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning.
P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proceedings of the Conference of
the Association for Computational Linguistics.
G. Mann, R. McDonald, M. Mohri, N. Silberman, and
D. Walker. 2009. Efficient large-scale distributed
training of conditional maximum entropy models. In
Advances in Neural Information Processing Systems.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of the Conference of the European Chapter
of the Association for Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the Conference of the Association for
Computational Linguistics.
C. Mesterharm. 2005. Online learning with delayed la-
bel feedback. In Proceedings ofAlgorithmic Learning
Theory.
A.B. Novikoff. 1962. On convergence proofs on percep-
trons. In Symposium on the Mathematical Theory of
Automata.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386–408.
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning.
J. N. Tsitsiklis, D. P. Bertsekas, and M. Athans. 1986.
Distributed asynchronous deterministic and stochastic
gradient optimization algorithms. IEEE Transactions
on Automatic Control, 31(9):803–812.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the International Conference on Machine learning.
C. Whitelaw, A. Kehlenbeck, N. Petrovic, and L. Ungar.
2008. Web-scale named entity recognition. In Pro-
ceedings of the International Conference on Informa-
tion and Knowledge Management.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
T. Zhang. 2004. Solving large scale linear prediction
problems using stochastic gradient descent algorithms.
In Proceedings of the International Conference on Ma-
chine Learning.
M. Zinkevich, A. Smola, and J. Langford. 2009. Slow
learners are fast. In Advances in Neural Information
Processing Systems.
</reference>
<page confidence="0.999518">
464
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.852910">
<title confidence="0.999227">Distributed Training Strategies for the Structured Perceptron</title>
<author confidence="0.999903">Ryan McDonald Keith Hall Gideon Mann</author>
<affiliation confidence="0.90473">Google, Inc., New York / Zurich</affiliation>
<abstract confidence="0.997098">Perceptron training is widely applied in the natural language processing community for learning complex structured models. Like all structured prediction learning frameworks, the structured perceptron can be costly to train as training complexity is proportional to inference, which is frequently non-linear in example sequence length. In this paper we investigate distributed training strategies for the structured perceptron as a means to reduce training times when computing clusters are available. We look at two strategies and provide convergence bounds for a particular mode of distributed structured perceptron training based on iterative parameter mixing (or averaging). We present experiments on two structured prediction problems – namedentity recognition and dependency parsing – to highlight the efficiency of this method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="22701" citStr="Breiman, 1996" startWordPosition="3945" endWordPosition="3946">e data, and 4) the distributed algorithms return better classifiers much quicker than training serially on all the data. This is true regardless of whether the underlying algorithm is the regular or the averaged perceptron. Point 3 deserves more discussion. In particular, the iterative parameter mixing strategy has a higher final f-measure than training on all the data serially than the standard perceptron (f-measure of 87.9 vs. 85.8). We suspect this happens for two reasons. First, the parameter mixing has a bagging like effect which helps to reduce the variance of the per-shard classifiers (Breiman, 1996). Second, the fact that parameter mixing is just a form of parameter averaging perhaps has the same effect as the averaged perceptron. Our second set of experiments looked at the much more computationally intensive task of dependency parsing. We used the Prague Dependency Treebank (PDT) (Hajiˇc et al., 2001), which is a Czech S i�� k% n kn ≤ [ki,n] Ndi.t ≤ NdietE n�� NdietE n�� S i�� 461 Wall Clock Serial (All Data) Serial (Sub Sampling) Parallel (Parameter Mix) Parallel (Iterative Parameter Mix) Wall Clock Reg. Perceptron Avg. Perceptron F-measure F-measure 85.8 88.2 75.3 76.6 81.5 81.6 87.9 </context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="24141" citStr="Buchholz and Marsi, 2006" startWordPosition="4167" endWordPosition="4170">rceptron -- Serial (All Data) Averaged Perceptron -- Serial (Sub Sampling) Averaged Perceptron -- Parallel (Parameter Mix) Averaged Perceptron -- Parallel (Iterative Parameter Mix) Test Data F-measure 0.85 0.8 0.75 0.7 Figure 4: NER experiments. Upper figures plot test data f-measure versus wall clock for both regular perceptron (left) and averaged perceptron (right). Lower table is f-measure for converged models. language treebank and currently one of the largest dependency treebanks in existence. We used the CoNLL-X training (72703 sentences) and testing splits (365 sentences) of this data (Buchholz and Marsi, 2006) and dependency parsing models based on McDonald and Pereira (2006) which factors features over pairs of dependency arcs in a tree. To parse all the sentences in the PDT, one must use a non-projective parsing algorithm, which is a known NP-complete inference problem when not assuming strong independence assumptions. Thus, the use of approximate inference techniques is common in order to find the highest weighted tree for a sentence. We use the approximate parsing algorithm given in McDonald and Pereira (2006), which runs in time roughly cubic in sentence length. To train such a model is comput</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Chu</author>
<author>S K Kim</author>
<author>Y A Lin</author>
<author>Y Y Yu</author>
<author>G Bradski</author>
<author>A Y Ng</author>
<author>K Olukotun</author>
</authors>
<title>Map-Reduce for machine learning on multicore.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2578" citStr="Chu et al., 2007" startWordPosition="377" endWordPosition="380">raining complexity is proportional to inference, which is frequently nonlinear in sequence length, even with strong structural independence assumptions. In this paper we investigate distributed training strategies for the structured perceptron as a means of reducing training times when large computing clusters are available. Traditional machine learning algorithms are typically designed for a single machine, and designing an efficient training mechanism for analogous algorithms on a computing cluster – often via a map-reduce framework (Dean and Ghemawat, 2004) – is an active area of research (Chu et al., 2007). However, unlike many batch learning algorithms that can easily be distributed through the gradient calculation, a distributed training analog for the perceptron is less clear cut. It employs online updates and its loss function is technically non-convex. A recent study by Mann et al. (2009) has shown that distributed training through parameter mixing (or averaging) for maximum entropy models can be empirically powerful and has strong theoretical guarantees. A parameter mixing strategy, which can be applied to any parameterized learning algorithm, trains separate models in parallel, each on a</context>
<context position="4481" citStr="Chu et al. (2007)" startWordPosition="688" endWordPosition="691"> standard perceptron algorithm, 2) find a separating hyperplane if the training set is separable, 3) reduce training times significantly, and 4) produce models with comparable (or superior) accuracies to those trained serially on all the data. 2 Related Work Perceptron(T = {(xt, yt)}|T | t�1) 1. w(°) = 0; k = 0 2. for n : 1..N 3. fort : 1..T 4. Let y0 = arg maxy, w(k) · f(xt, y0) 5. if y0 =� yt 6. w(k+1) = w(k) + f(xt,yt) − f(xt,y0) 7. k = k + 1 8. return w(k) Figure 1: The perceptron algorithm. Distributed cluster computation for many batch training algorithms has previously been examined by Chu et al. (2007), among others. Much of the relevant prior work on online (or sub-gradient) distributed training has been focused on asynchronous optimization via gradient descent. In this scenario, multiple machines run stochastic gradient descent simultaneously as they update and read from a shared parameter vector asynchronously. Early work by Tsitsiklis et al. (1986) demonstrated that if the delay between model updates and reads is bounded, then asynchronous optimization is guaranteed to converge. Recently, Zinkevich et al. (2009) performed a similar type of analysis for online learners with asynchronous </context>
</contexts>
<marker>Chu, Kim, Lin, Yu, Bradski, Ng, Olukotun, 2007</marker>
<rawString>C.T. Chu, S.K. Kim, Y.A. Lin, Y.Y. Yu, G. Bradski, A.Y. Ng, and K. Olukotun. 2007. Map-Reduce for machine learning on multicore. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1649" citStr="Collins and Roark, 2004" startWordPosition="233" endWordPosition="236">e of the most popular training algorithms for structured prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all structured prediction learning frameworks, the structure perceptron can still be cumbersome to train. This is both due to the increasing size of available training sets as well as the fact that training complexity is proportional to inference, which is frequently nonlinear in sequence length, even with strong structural independence assumptions. In this paper we investigate distributed training strategies for the structured perceptron as a means of reducing training times when l</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>M. Collins and B. Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithm.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1185" citStr="Collins, 2002" startWordPosition="164" endWordPosition="165">for the structured perceptron as a means to reduce training times when computing clusters are available. We look at two strategies and provide convergence bounds for a particular mode of distributed structured perceptron training based on iterative parameter mixing (or averaging). We present experiments on two structured prediction problems – namedentity recognition and dependency parsing – to highlight the efficiency of this method. 1 Introduction One of the most popular training algorithms for structured prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all structured predicti</context>
<context position="6139" citStr="Collins, 2002" startWordPosition="943" endWordPosition="944">rty et al., 2001), which is the structured analog of maximum entropy. As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al., 2008). However, unlike perceptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et al., 2005; Crammer et al., 2006), the recently introduced confidence weighted learning (Dredze et al., 2008) and coordinate descent algorithms (Duchi and Singer, 2009). 3 Structured Perceptron The structured perceptron was introduced by Collins (2002) and we adopt much of the notation and presentation of that study. The structured percetron algorithm – which is identical to the multi-class perceptron – is shown in Figure 1. The perceptron is an online learning algorithm and processes t</context>
<context position="8255" citStr="Collins (2002)" startWordPosition="1323" endWordPosition="1324">T is separable with margin -y &gt; 0 if there exists a vector u E RM with I IuI I = 1 such that u · f(xt, yt) − u · f(xt, y&apos;) &gt; -y, for all (xt, yt) E T, and for all y&apos; E Yt such that y&apos; =� yt. Furthermore, let R &gt; ||f(xt, yt)−f(xt, y&apos;)||, for all (xt, yt) E T and y&apos; E Yt. A fundamental theorem 1The perceptron can be kernalized for non-linearity. 457 of the perceptron is as follows: Theorem 1 (Novikoff (1962)). Assume training set T is separable by margin &apos;y. Let k be the number of mistakes made training the perceptron (Figure 1) on T. If training is run indefinitely, then k &lt; R2 �2 . Proof. See Collins (2002) Theorem 1. Theorem 1 implies that if T is separable then 1) the perceptron will converge in a finite amount of time, and 2) will produce a w that separates T . Collins also proposed a variant of the structured perceptron where the final weight vector is a weighted average of all parameters that occur during training, which he called the averaged perceptron and can be viewed as an approximation to the voted perceptron algorithm (Freund and Schapire, 1999). 4 Distributed Structured Perceptron In this section we examine two distributed training strategies for the perceptron algorithm based on pa</context>
<context position="15127" citStr="Collins (2002)" startWordPosition="2586" endWordPosition="2587">r n : 1..N 4. w(i,n) = OneEpochPerceptron(Ti, w) † w = E 5. i µi,nw(i,n) ‡ 6. return w OneEpochPerceptron(T, w*) 1. w(0) = w*; k = 0 2. fort: 1..T 3. Let y&apos; = arg max,, w(k) · f(xt, y&apos;) 4. if y&apos; =6 yt 5. w(k+1) = w(k) + f(xt,yt) − f(xt,y&apos;) 6. k = k + 1 7. return w(k) Figure 3: Distributed perceptron using an iterative parameter mixing strategy. † Each w(i,n) is computed in parallel. ‡ µn = {µ1,n, ... , µS,n}, ∀µi,n ∈ µn: µi,n ≥ 0 and ∀n: Ei µi,n = 1. w(avg,n) be the mixed vector from the weight vectors returned after the nth epoch, i.e., S w(avg,n) = µi,nw(i,n) i=1 Following the analysis from Collins (2002) Theorem 1, by examining line 5 of OneEpochPerceptron in Figure 3 and the fact that u separates the data by γ: u · w(i,n) = u · w([i,n]−1) + u · (f(xt,yt) − f(xt,y&apos;)) ≥ u · w([i,n]−1) + γ ≥ u · w([i,n]−2) + 2γ ... ≥ u · w(avg,n−1) + ki,nγ (A1) That is, u · w(i,n) is bounded below by the average weight vector for the n-1st epoch plus the number of mistakes made on shard i during the nth epoch times the margin γ. Next, by OneEpochPerceptron line 5, the definition of R, and w([i,n]−1)(f(xt, yt)− f(xt, y&apos;)) ≤ 0 when line 5 is called: kw(i,n)k2 = kw([i,n]−1)k2 +kf(xt,yt) − f(xt,y&apos;)k2 + 2w([i,n]−1)(</context>
<context position="20086" citStr="Collins, 2002" startWordPosition="3518" endWordPosition="3519">ency parsing. We compare up to four systems: 1. Serial (All Data): This is the classifier returned if trained serially on all the available data. 2. Serial (Sub Sampling): Shard the data, select one shard randomly and train serially. 3. Parallel (Parameter Mix): Parallel strategy discussed in Section 4.1 with uniform mixing. 4. Parallel (Iterative Parameter Mix): Parallel strategy discussed in Section 4.2 with uniform mixing (Section 5.1 looks at mixing strategies). For all four systems we compare results for both the standard perceptron algorithm as well as the averaged perceptron algorithm (Collins, 2002). We report the final test set metrics of the converged classifiers to determine whether any loss in accuracy is observed as a consequence of distributed training strategies. We define convergence as either: 1) the training set is separated, or 2) the training set performance measure (accuracy, f-measure, etc.) does not change by more than some pre-defined threshold on three consecutive epochs. As with most real world data sets, convergence by training set separation was rarely observed, though in both cases training set accuracies approached 100%. For both tasks we also plot test set metrics </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithm. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="6281" citStr="Crammer et al., 2006" startWordPosition="961" endWordPosition="964">ent or sub-gradient computations (Finkel et al., 2008). However, unlike perceptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et al., 2005; Crammer et al., 2006), the recently introduced confidence weighted learning (Dredze et al., 2008) and coordinate descent algorithms (Duchi and Singer, 2009). 3 Structured Perceptron The structured perceptron was introduced by Collins (2002) and we adopt much of the notation and presentation of that study. The structured percetron algorithm – which is identical to the multi-class perceptron – is shown in Figure 1. The perceptron is an online learning algorithm and processes training instances one at a time during each epoch of training. Lines 4-6 are the core of the algorithm. For a inputoutput training instance pa</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. The Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dean</author>
<author>S Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In Sixth Symposium on Operating System Design and Implementation.</booktitle>
<contexts>
<context position="2527" citStr="Dean and Ghemawat, 2004" startWordPosition="365" endWordPosition="369">size of available training sets as well as the fact that training complexity is proportional to inference, which is frequently nonlinear in sequence length, even with strong structural independence assumptions. In this paper we investigate distributed training strategies for the structured perceptron as a means of reducing training times when large computing clusters are available. Traditional machine learning algorithms are typically designed for a single machine, and designing an efficient training mechanism for analogous algorithms on a computing cluster – often via a map-reduce framework (Dean and Ghemawat, 2004) – is an active area of research (Chu et al., 2007). However, unlike many batch learning algorithms that can easily be distributed through the gradient calculation, a distributed training analog for the perceptron is less clear cut. It employs online updates and its loss function is technically non-convex. A recent study by Mann et al. (2009) has shown that distributed training through parameter mixing (or averaging) for maximum entropy models can be empirically powerful and has strong theoretical guarantees. A parameter mixing strategy, which can be applied to any parameterized learning algor</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>J. Dean and S. Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. In Sixth Symposium on Operating System Design and Implementation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dredze</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Confidence-weighted linear classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Machine learning.</booktitle>
<contexts>
<context position="6357" citStr="Dredze et al., 2008" startWordPosition="972" endWordPosition="975">ptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et al., 2005; Crammer et al., 2006), the recently introduced confidence weighted learning (Dredze et al., 2008) and coordinate descent algorithms (Duchi and Singer, 2009). 3 Structured Perceptron The structured perceptron was introduced by Collins (2002) and we adopt much of the notation and presentation of that study. The structured percetron algorithm – which is identical to the multi-class perceptron – is shown in Figure 1. The perceptron is an online learning algorithm and processes training instances one at a time during each epoch of training. Lines 4-6 are the core of the algorithm. For a inputoutput training instance pair (xt, yt) E T, the algorithm predicts a structured output y&apos; E Yt, where Y</context>
<context position="10836" citStr="Dredze et al. (2008)" startWordPosition="1768" endWordPosition="1771">tron(Ti) † 3. w = Ei µiw(i) $ 4. return w Figure 2: Distributed perceptron using a parameter mixing strategy. † Each w(i) is computed in parallel. $ µ = {µ1, ... , µS}, Vµi E µ : µi &gt; 0 and Ei µi = 1. ference between parameters trained on all the data serially versus parameters trained with parameter mixing. However, their analysis requires a stability bound on the parameters of a regularized maximum entropy model, which is not known to hold for the perceptron. In Section 5, we present empirical results showing that parameter mixing for distributed perceptron can be sub-optimal. Additionally, Dredze et al. (2008) present negative parameter mixing results for confidence weighted learning, which is another online learning algorithm. The following theorem may help explain this behavior. Theorem 2. For a any training set T separable by margin &apos;y, the perceptron algorithm trained through a parameter mixing strategy (Figure 2) does not necessarily return a separating weight vector w. Proof. Consider a binary classification setting where Y = {0, 1} and T has 4 instances. We distribute the training set into two shards, T1 = {(x1,1, y1,1), (x1,2, y1,2)} and T2 = {(x2,1, y2,1), (x2,2, y2,2)}. Let y1,1 = y2,1 = </context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>M. Dredze, K. Crammer, and F. Pereira. 2008. Confidence-weighted linear classification. In Proceedings of the International Conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>Y Singer</author>
</authors>
<title>Efficient learning using forward-backward splitting.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="6416" citStr="Duchi and Singer, 2009" startWordPosition="980" endWordPosition="983">ion, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et al., 2005; Crammer et al., 2006), the recently introduced confidence weighted learning (Dredze et al., 2008) and coordinate descent algorithms (Duchi and Singer, 2009). 3 Structured Perceptron The structured perceptron was introduced by Collins (2002) and we adopt much of the notation and presentation of that study. The structured percetron algorithm – which is identical to the multi-class perceptron – is shown in Figure 1. The perceptron is an online learning algorithm and processes training instances one at a time during each epoch of training. Lines 4-6 are the core of the algorithm. For a inputoutput training instance pair (xt, yt) E T, the algorithm predicts a structured output y&apos; E Yt, where Yt is the space of permissible structured outputs for input </context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>J. Duchi and Y. Singer. 2009. Efficient learning using forward-backward splitting. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5714" citStr="Finkel et al., 2008" startWordPosition="880" endWordPosition="883">tochastic gradient descent. The asynchronous algorithms in these studies require shared memory between the distributed computations and are less suitable to the more common cluster computing environment, which is what we study here. While we focus on the perceptron algorithm, there is a large body of work on training structured prediction classifiers. For batch training the most common is conditional random fields (CRFs) (Lafferty et al., 2001), which is the structured analog of maximum entropy. As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al., 2008). However, unlike perceptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et al., 2005; Crammer et al., 2006), the recently introduced confide</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="8714" citStr="Freund and Schapire, 1999" startWordPosition="1401" endWordPosition="1404">by margin &apos;y. Let k be the number of mistakes made training the perceptron (Figure 1) on T. If training is run indefinitely, then k &lt; R2 �2 . Proof. See Collins (2002) Theorem 1. Theorem 1 implies that if T is separable then 1) the perceptron will converge in a finite amount of time, and 2) will produce a w that separates T . Collins also proposed a variant of the structured perceptron where the final weight vector is a weighted average of all parameters that occur during training, which he called the averaged perceptron and can be viewed as an approximation to the voted perceptron algorithm (Freund and Schapire, 1999). 4 Distributed Structured Perceptron In this section we examine two distributed training strategies for the perceptron algorithm based on parameter mixing. 4.1 Parameter Mixing Distributed training through parameter mixing is a straight-forward way of training classifiers in parallel. The algorithm is given in Figure 2. The idea is simple: divide the training data T into S disjoint shards such that T = {T1, ... , TS}. Next, train perceptron models (or any learning algorithm) on each shard in parallel. After training, set the final parameters to a weighted mixture of the parameters of each mod</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R.E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>B Vidova Hladka</author>
<author>J Panevov´a</author>
<author>E Hajiˇcov´a</author>
<author>P Sgall</author>
<author>P Pajas</author>
</authors>
<title>Prague Dependency Treebank 1.0. LDC,</title>
<date>2001</date>
<marker>Hajiˇc, Hladka, Panevov´a, Hajiˇcov´a, Sgall, Pajas, 2001</marker>
<rawString>J. Hajiˇc, B. Vidova Hladka, J. Panevov´a, E. Hajiˇcov´a, P. Sgall, and P. Pajas. 2001. Prague Dependency Treebank 1.0. LDC, 2001T10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1411" citStr="Lafferty et al., 2001" startWordPosition="196" endWordPosition="199">on training based on iterative parameter mixing (or averaging). We present experiments on two structured prediction problems – namedentity recognition and dependency parsing – to highlight the efficiency of this method. 1 Introduction One of the most popular training algorithms for structured prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all structured prediction learning frameworks, the structure perceptron can still be cumbersome to train. This is both due to the increasing size of available training sets as well as the fact that training complexity is proportional to inference, w</context>
<context position="5542" citStr="Lafferty et al., 2001" startWordPosition="851" endWordPosition="854">synchronous optimization is guaranteed to converge. Recently, Zinkevich et al. (2009) performed a similar type of analysis for online learners with asynchronous updates via stochastic gradient descent. The asynchronous algorithms in these studies require shared memory between the distributed computations and are less suitable to the more common cluster computing environment, which is what we study here. While we focus on the perceptron algorithm, there is a large body of work on training structured prediction classifiers. For batch training the most common is conditional random fields (CRFs) (Lafferty et al., 2001), which is the structured analog of maximum entropy. As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al., 2008). However, unlike perceptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), o</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>R McDonald</author>
<author>M Mohri</author>
<author>N Silberman</author>
<author>D Walker</author>
</authors>
<title>Efficient large-scale distributed training of conditional maximum entropy models.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2871" citStr="Mann et al. (2009)" startWordPosition="423" endWordPosition="426"> computing clusters are available. Traditional machine learning algorithms are typically designed for a single machine, and designing an efficient training mechanism for analogous algorithms on a computing cluster – often via a map-reduce framework (Dean and Ghemawat, 2004) – is an active area of research (Chu et al., 2007). However, unlike many batch learning algorithms that can easily be distributed through the gradient calculation, a distributed training analog for the perceptron is less clear cut. It employs online updates and its loss function is technically non-convex. A recent study by Mann et al. (2009) has shown that distributed training through parameter mixing (or averaging) for maximum entropy models can be empirically powerful and has strong theoretical guarantees. A parameter mixing strategy, which can be applied to any parameterized learning algorithm, trains separate models in parallel, each on a disjoint subset of the training data, and then takes an average of all the parameters as the final model. In this paper, we provide results which suggest that the perceptron is ill-suited for straight-forward parameter mixing, even though it is commonly used for large-scale structured learni</context>
<context position="10064" citStr="Mann et al. (2009)" startWordPosition="1622" endWordPosition="1625">t from the averaged perceptron (see previous section). It is easy to see how this can be implemented on a cluster through a map-reduce framework, i.e., the map step trains the individual models in parallel and the reduce step mixes their parameters. The advantages of parameter mixing are: 1) that it is parallel, making it possibly to scale to extremely large data sets, and 2) it is resource efficient, in particular with respect to network usage as parameters are not repeatedly passed across the network as is often the case for exact distributed training strategies. For maximum entropy models, Mann et al. (2009) show it is possible to bound the norm of the difPerceptronParamMix(T = {(xt, yt)}|� | t�1) 1. Shard T into S pieces T = {T1, ... , TS} 2. w(i) = Perceptron(Ti) † 3. w = Ei µiw(i) $ 4. return w Figure 2: Distributed perceptron using a parameter mixing strategy. † Each w(i) is computed in parallel. $ µ = {µ1, ... , µS}, Vµi E µ : µi &gt; 0 and Ei µi = 1. ference between parameters trained on all the data serially versus parameters trained with parameter mixing. However, their analysis requires a stability bound on the parameters of a regularized maximum entropy model, which is not known to hold fo</context>
</contexts>
<marker>Mann, McDonald, Mohri, Silberman, Walker, 2009</marker>
<rawString>G. Mann, R. McDonald, M. Mohri, N. Silberman, and D. Walker. 2009. Efficient large-scale distributed training of conditional maximum entropy models. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1677" citStr="McDonald and Pereira, 2006" startWordPosition="237" endWordPosition="240">ining algorithms for structured prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all structured prediction learning frameworks, the structure perceptron can still be cumbersome to train. This is both due to the increasing size of available training sets as well as the fact that training complexity is proportional to inference, which is frequently nonlinear in sequence length, even with strong structural independence assumptions. In this paper we investigate distributed training strategies for the structured perceptron as a means of reducing training times when large computing clusters are </context>
<context position="24208" citStr="McDonald and Pereira (2006)" startWordPosition="4177" endWordPosition="4180">Sampling) Averaged Perceptron -- Parallel (Parameter Mix) Averaged Perceptron -- Parallel (Iterative Parameter Mix) Test Data F-measure 0.85 0.8 0.75 0.7 Figure 4: NER experiments. Upper figures plot test data f-measure versus wall clock for both regular perceptron (left) and averaged perceptron (right). Lower table is f-measure for converged models. language treebank and currently one of the largest dependency treebanks in existence. We used the CoNLL-X training (72703 sentences) and testing splits (365 sentences) of this data (Buchholz and Marsi, 2006) and dependency parsing models based on McDonald and Pereira (2006) which factors features over pairs of dependency arcs in a tree. To parse all the sentences in the PDT, one must use a non-projective parsing algorithm, which is a known NP-complete inference problem when not assuming strong independence assumptions. Thus, the use of approximate inference techniques is common in order to find the highest weighted tree for a sentence. We use the approximate parsing algorithm given in McDonald and Pereira (2006), which runs in time roughly cubic in sentence length. To train such a model is computationally expensive and can take on the order of days to train on a</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6258" citStr="McDonald et al., 2005" startWordPosition="957" endWordPosition="960">buted through the gradient or sub-gradient computations (Finkel et al., 2008). However, unlike perceptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et al., 2005; Crammer et al., 2006), the recently introduced confidence weighted learning (Dredze et al., 2008) and coordinate descent algorithms (Duchi and Singer, 2009). 3 Structured Perceptron The structured perceptron was introduced by Collins (2002) and we adopt much of the notation and presentation of that study. The structured percetron algorithm – which is identical to the multi-class perceptron – is shown in Figure 1. The perceptron is an online learning algorithm and processes training instances one at a time during each epoch of training. Lines 4-6 are the core of the algorithm. For a inputoutp</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mesterharm</author>
</authors>
<title>Online learning with delayed label feedback.</title>
<date>2005</date>
<booktitle>In Proceedings ofAlgorithmic Learning Theory.</booktitle>
<contexts>
<context position="28554" citStr="Mesterharm, 2005" startWordPosition="4875" endWordPosition="4876">ipping point for our experiments in which increasing the number of shards began to have an adverse effect on training times, which for the namedentity experiments occurred around 25-50 shards. This is both due to reasons described in this section as well as the added overhead of maintaining and summing multiple high-dimensional weight vectors after each distributed epoch. It is worth pointing out that a linear term S in the convergence bound above is similar to convergence/regret bounds for asynchronous distributed online learning, which typically have bounds linear in the asynchronous delay (Mesterharm, 2005; Zinkevich et al., 2009). This delay will be on average roughly equal to the number of shards S. 6 Conclusions In this paper we have investigated distributing the structured perceptron via simple parameter mixing strategies. Our analysis shows that an iterative parameter mixing strategy is both guaranteed to separate the data (if possible) and significantly reduces the time required to train high accuracy classifiers. However, there is a trade-off between increasing training times through distributed computation and slower convergence relative to the number of shards. Finally, we note that us</context>
</contexts>
<marker>Mesterharm, 2005</marker>
<rawString>C. Mesterharm. 2005. Online learning with delayed label feedback. In Proceedings ofAlgorithmic Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Novikoff</author>
</authors>
<title>On convergence proofs on perceptrons.</title>
<date>1962</date>
<booktitle>In Symposium on the Mathematical Theory of Automata.</booktitle>
<contexts>
<context position="8050" citStr="Novikoff (1962)" startWordPosition="1284" endWordPosition="1285">ct output yt and take weight away from features for the incorrect output y&apos;. For structured prediction, the inference step in line 4 is problem dependent, e.g., CKY for context-free parsing. A training set T is separable with margin -y &gt; 0 if there exists a vector u E RM with I IuI I = 1 such that u · f(xt, yt) − u · f(xt, y&apos;) &gt; -y, for all (xt, yt) E T, and for all y&apos; E Yt such that y&apos; =� yt. Furthermore, let R &gt; ||f(xt, yt)−f(xt, y&apos;)||, for all (xt, yt) E T and y&apos; E Yt. A fundamental theorem 1The perceptron can be kernalized for non-linearity. 457 of the perceptron is as follows: Theorem 1 (Novikoff (1962)). Assume training set T is separable by margin &apos;y. Let k be the number of mistakes made training the perceptron (Figure 1) on T. If training is run indefinitely, then k &lt; R2 �2 . Proof. See Collins (2002) Theorem 1. Theorem 1 implies that if T is separable then 1) the perceptron will converge in a finite amount of time, and 2) will produce a w that separates T . Collins also proposed a variant of the structured perceptron where the final weight vector is a weighted average of all parameters that occur during training, which he called the averaged perceptron and can be viewed as an approximati</context>
</contexts>
<marker>Novikoff, 1962</marker>
<rawString>A.B. Novikoff. 1962. On convergence proofs on perceptrons. In Symposium on the Mathematical Theory of Automata.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rosenblatt</author>
</authors>
<title>The perceptron: A probabilistic model for information storage and organization in the brain.</title>
<date>1958</date>
<journal>Psychological Review,</journal>
<volume>65</volume>
<issue>6</issue>
<contexts>
<context position="1169" citStr="Rosenblatt, 1958" startWordPosition="162" endWordPosition="163">aining strategies for the structured perceptron as a means to reduce training times when computing clusters are available. We look at two strategies and provide convergence bounds for a particular mode of distributed structured perceptron training based on iterative parameter mixing (or averaging). We present experiments on two structured prediction problems – namedentity recognition and dependency parsing – to highlight the efficiency of this method. 1 Introduction One of the most popular training algorithms for structured prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all str</context>
</contexts>
<marker>Rosenblatt, 1958</marker>
<rawString>F. Rosenblatt. 1958. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5916" citStr="Taskar et al., 2004" startWordPosition="909" endWordPosition="912">, which is what we study here. While we focus on the perceptron algorithm, there is a large body of work on training structured prediction classifiers. For batch training the most common is conditional random fields (CRFs) (Lafferty et al., 2001), which is the structured analog of maximum entropy. As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al., 2008). However, unlike perceptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et al., 2005; Crammer et al., 2006), the recently introduced confidence weighted learning (Dredze et al., 2008) and coordinate descent algorithms (Duchi and Singer, 2009). 3 Structured Perceptron The structured perceptron was introduced by Collins (2002) and we adopt mu</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2004</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2004. Max-margin Markov networks. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning.</booktitle>
<marker>Sang, De Meulder, 2003</marker>
<rawString>E. F. Tjong Kim Sang and F. De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition. In Proceedings of the Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Tsitsiklis</author>
<author>D P Bertsekas</author>
<author>M Athans</author>
</authors>
<title>Distributed asynchronous deterministic and stochastic gradient optimization algorithms.</title>
<date>1986</date>
<journal>IEEE Transactions on Automatic Control,</journal>
<volume>31</volume>
<issue>9</issue>
<contexts>
<context position="4838" citStr="Tsitsiklis et al. (1986)" startWordPosition="742" endWordPosition="745"> y0 = arg maxy, w(k) · f(xt, y0) 5. if y0 =� yt 6. w(k+1) = w(k) + f(xt,yt) − f(xt,y0) 7. k = k + 1 8. return w(k) Figure 1: The perceptron algorithm. Distributed cluster computation for many batch training algorithms has previously been examined by Chu et al. (2007), among others. Much of the relevant prior work on online (or sub-gradient) distributed training has been focused on asynchronous optimization via gradient descent. In this scenario, multiple machines run stochastic gradient descent simultaneously as they update and read from a shared parameter vector asynchronously. Early work by Tsitsiklis et al. (1986) demonstrated that if the delay between model updates and reads is bounded, then asynchronous optimization is guaranteed to converge. Recently, Zinkevich et al. (2009) performed a similar type of analysis for online learners with asynchronous updates via stochastic gradient descent. The asynchronous algorithms in these studies require shared memory between the distributed computations and are less suitable to the more common cluster computing environment, which is what we study here. While we focus on the perceptron algorithm, there is a large body of work on training structured prediction cla</context>
</contexts>
<marker>Tsitsiklis, Bertsekas, Athans, 1986</marker>
<rawString>J. N. Tsitsiklis, D. P. Bertsekas, and M. Athans. 1986. Distributed asynchronous deterministic and stochastic gradient optimization algorithms. IEEE Transactions on Automatic Control, 31(9):803–812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Machine learning.</booktitle>
<contexts>
<context position="5966" citStr="Tsochantaridis et al., 2004" startWordPosition="916" endWordPosition="919">cus on the perceptron algorithm, there is a large body of work on training structured prediction classifiers. For batch training the most common is conditional random fields (CRFs) (Lafferty et al., 2001), which is the structured analog of maximum entropy. As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al., 2008). However, unlike perceptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et al., 2005; Crammer et al., 2006), the recently introduced confidence weighted learning (Dredze et al., 2008) and coordinate descent algorithms (Duchi and Singer, 2009). 3 Structured Perceptron The structured perceptron was introduced by Collins (2002) and we adopt much of the notation and presentation of that study.</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the International Conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Whitelaw</author>
<author>A Kehlenbeck</author>
<author>N Petrovic</author>
<author>L Ungar</author>
</authors>
<title>Web-scale named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="3503" citStr="Whitelaw et al. (2008)" startWordPosition="520" endWordPosition="523"> that distributed training through parameter mixing (or averaging) for maximum entropy models can be empirically powerful and has strong theoretical guarantees. A parameter mixing strategy, which can be applied to any parameterized learning algorithm, trains separate models in parallel, each on a disjoint subset of the training data, and then takes an average of all the parameters as the final model. In this paper, we provide results which suggest that the perceptron is ill-suited for straight-forward parameter mixing, even though it is commonly used for large-scale structured learning, e.g., Whitelaw et al. (2008) for named-entity recognition. However, a slight mod456 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 456–464, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics ification we call iterative parameter mixing can be shown to: 1) have similar convergence properties to the standard perceptron algorithm, 2) find a separating hyperplane if the training set is separable, 3) reduce training times significantly, and 4) produce models with comparable (or superior) accuracies to those trained serially on all the d</context>
</contexts>
<marker>Whitelaw, Kehlenbeck, Petrovic, Ungar, 2008</marker>
<rawString>C. Whitelaw, A. Kehlenbeck, N. Petrovic, and L. Ungar. 2008. Web-scale named entity recognition. In Proceedings of the International Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transitionbased dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1701" citStr="Zhang and Clark, 2008" startWordPosition="241" endWordPosition="244">red prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all structured prediction learning frameworks, the structure perceptron can still be cumbersome to train. This is both due to the increasing size of available training sets as well as the fact that training complexity is proportional to inference, which is frequently nonlinear in sequence length, even with strong structural independence assumptions. In this paper we investigate distributed training strategies for the structured perceptron as a means of reducing training times when large computing clusters are available. Traditional m</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Y. Zhang and S. Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transitionbased dependency parsing using beam-search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
</authors>
<title>Solving large scale linear prediction problems using stochastic gradient descent algorithms.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6204" citStr="Zhang, 2004" startWordPosition="952" endWordPosition="953">y. As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al., 2008). However, unlike perceptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et al., 2005; Crammer et al., 2006), the recently introduced confidence weighted learning (Dredze et al., 2008) and coordinate descent algorithms (Duchi and Singer, 2009). 3 Structured Perceptron The structured perceptron was introduced by Collins (2002) and we adopt much of the notation and presentation of that study. The structured percetron algorithm – which is identical to the multi-class perceptron – is shown in Figure 1. The perceptron is an online learning algorithm and processes training instances one at a time during each epoch of training. Li</context>
</contexts>
<marker>Zhang, 2004</marker>
<rawString>T. Zhang. 2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zinkevich</author>
<author>A Smola</author>
<author>J Langford</author>
</authors>
<title>Slow learners are fast.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5005" citStr="Zinkevich et al. (2009)" startWordPosition="767" endWordPosition="770">er computation for many batch training algorithms has previously been examined by Chu et al. (2007), among others. Much of the relevant prior work on online (or sub-gradient) distributed training has been focused on asynchronous optimization via gradient descent. In this scenario, multiple machines run stochastic gradient descent simultaneously as they update and read from a shared parameter vector asynchronously. Early work by Tsitsiklis et al. (1986) demonstrated that if the delay between model updates and reads is bounded, then asynchronous optimization is guaranteed to converge. Recently, Zinkevich et al. (2009) performed a similar type of analysis for online learners with asynchronous updates via stochastic gradient descent. The asynchronous algorithms in these studies require shared memory between the distributed computations and are less suitable to the more common cluster computing environment, which is what we study here. While we focus on the perceptron algorithm, there is a large body of work on training structured prediction classifiers. For batch training the most common is conditional random fields (CRFs) (Lafferty et al., 2001), which is the structured analog of maximum entropy. As such, i</context>
<context position="28579" citStr="Zinkevich et al., 2009" startWordPosition="4877" endWordPosition="4880">ur experiments in which increasing the number of shards began to have an adverse effect on training times, which for the namedentity experiments occurred around 25-50 shards. This is both due to reasons described in this section as well as the added overhead of maintaining and summing multiple high-dimensional weight vectors after each distributed epoch. It is worth pointing out that a linear term S in the convergence bound above is similar to convergence/regret bounds for asynchronous distributed online learning, which typically have bounds linear in the asynchronous delay (Mesterharm, 2005; Zinkevich et al., 2009). This delay will be on average roughly equal to the number of shards S. 6 Conclusions In this paper we have investigated distributing the structured perceptron via simple parameter mixing strategies. Our analysis shows that an iterative parameter mixing strategy is both guaranteed to separate the data (if possible) and significantly reduces the time required to train high accuracy classifiers. However, there is a trade-off between increasing training times through distributed computation and slower convergence relative to the number of shards. Finally, we note that using similar proofs to tho</context>
</contexts>
<marker>Zinkevich, Smola, Langford, 2009</marker>
<rawString>M. Zinkevich, A. Smola, and J. Langford. 2009. Slow learners are fast. In Advances in Neural Information Processing Systems.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>