<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.331944">
<note confidence="0.812681">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 151-153, Lisbon, Portugal, 2000.
</note>
<title confidence="0.971529">
Text Chunking by System Combination
</title>
<author confidence="0.980323">
Erik F. Tjong Kim Sang
</author>
<affiliation confidence="0.977022">
CNTS - Language Technology Group
University of Antwerp
</affiliation>
<email confidence="0.992015">
erikt@uia.ua.ac.be
</email>
<sectionHeader confidence="0.998411" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999802625">
We will apply a system-internal combination
of memory-based learning classifiers to the
CoNLL-2000 shared task: finding base chunks.
Apart from testing different combination meth-
ods, we will also examine if dividing the chunk-
ing process in a boundary recognition phase and
a type identification phase would aid perfor-
mance.
</bodyText>
<sectionHeader confidence="0.965123" genericHeader="keywords">
2 Approach
</sectionHeader>
<bodyText confidence="0.999938376811594">
Tjong Kim Sang (2000) describes how a system-
internal combination of memory-based learners
can be used for base noun phrase (baseNP)
recognition. The idea is to generate different
chunking models by using different chunk rep-
resentations. Chunks can be represented with
bracket structures but alternatively one can use
a tagging representation which classifies words
as being inside a chunk (I), outside a chunk
(0) or at a chunk boundary (B) (Ramshaw and
Marcus, 1995). There are four variants of this
representation. The B tags can be used for the
first word of chunks that immediately follow an-
other chunk (the IOB1 representation) or they
can be used for every chunk-initial word (I0B2).
Alternatively an E tag can be used for labeling
the final word of a chunk immediately preced-
ing another chunk (IOE1) or it can be used for
every chunk-final word (10E2). Bracket struc-
tures can also be represented as tagging struc-
tures by using two streams of tags which de-
fine whether words start a chunk or not (0)
or whether words are at the end of a chunk or
not (C). We need both for encoding the phrase
structure and hence we will treat the two tag
streams as a single representation (0+C). A
combination of baseNP classifiers that use the
five representation performs better than any of
the included systems (Tjong Kim Sang, 2000).
We will apply such a classifier combination to
the CoNLL-2000 shared task.
The individual classifiers will use the
memory-based learning algorithm IB1-IG
(Daelemans et al., 1999) for determining
the most probable tag for each word. In
memory-based learning the training data is
stored and a new item is classified by the most
frequent classification among training items
which are closest to this new item. Data items
are represented as sets of feature-value pairs.
Features receive weights which are based on
the amount of information they provide for
classifying the training data (Daelemans et al.,
1999).
We will evaluate nine different methods for
combining the output of our five chunkers (Van
Halteren et al., 1998). Five are so-called voting
methods. They assign weights to the output of
the individual systems and use these weights to
determine the most probable output tag. Since
the classifiers generate different output formats,
all classifier output has been converted to the
0 and the C representations. The most sim-
ple voting method assigns uniform weights and
picks the tag that occurs most often (Majority).
A more advanced method is to use as a weight
the accuracy of the classifier on some held-out
part of the training data, the tuning data (Tot-
Precision). One can also use the precision ob-
tained by a classifier for a specific output value
as a weight (TagPrecision). Alternatively, we
use as a weight a combination of the precision
score for the output tag in combination with
the recall score for competing tags (Precision-
Recall). The most advanced voting method ex-
amines output values of pairs of classifiers and
assigns weights to tags based on how often they
appear with this pair in the tuning data (Tag-
Pair, Van Halteren et al., (1998)).
</bodyText>
<page confidence="0.992192">
151
</page>
<bodyText confidence="0.998435076923077">
Apart from these voting methods we have also
applied two memory-based learners to the out-
put of the five chunkers: ml-IG and IGTREE, a
decision tree variant of ml-IG (Daelemans et
al., 1999). This approach is called classifier
stacking. Like with the voting algorithms, we
have tested these meta-classifiers with the out-
put of the first classification stage. Unlike the
voting algorithms, the classifiers do not require
a uniform input. Therefore we have tested if
their performance can be improved by supply-
ing them with information about the input of
the first classification stage. For this purpose
we have used the part-of-speech tag of the cur-
rent word as compressed representation of the
first stage input (Van Halteren et al., 1998).
The combination methods will generate a list
of open brackets and a list of close brackets. We
have converted these to phrases by only using
brackets which could be matched with the clos-
est matching candidate and ignoring the others.
For example, in the structure [NP a [NP b byp
[vp c ]pp d ]vp, we would accept [NP b ]/â– [/..
as a noun phrase and ignore all other brackets
since they cannot be matched with their clos-
est candidate for a pair, either because of type
inconsistencies or because there was some other
bracket in between them.
We will examine three processing strategies
in order to test our hypothesis that chunking
performance can be increased by making a dis-
tinction between finding chunk boundaries and
identifying chunk types. The first is the single-
pass method. Here each individual classifier at-
tempts to find the correct chunk tag for each
word in one step. A variant of this is the double-
pass method. It processes the data twice: first
it searches for chunks boundaries and then it
attempts to identify the types of the chunks
found. The third processing method is the n-
pass method. It contains as many passes as
there are different chunk types. In each pass,
it attempts to find chunks of a single type. In
case a word is classified as belonging to more
than one chunk type, preference will be given
to the chunk type that occurs most often in the
training data. We expect the n-pass method to
outperform the other two methods. However,
we are not sure if the performance difference
will be large enough to compensate for the extra
computation that is required for this processing
method.
</bodyText>
<sectionHeader confidence="0.999226" genericHeader="introduction">
3 Results
</sectionHeader>
<bodyText confidence="0.999367458333333">
In order to find out which of the three process-
ing methods and which of the nine combination
methods performs best, we have applied them
to the training data of the CoNLL-2000 shared
task (Tjong Kim Sang and Buchholz, 2000) in a
10-fold cross-validation experiment (Weiss and
Kulikowski, 1991). For the single-pass method,
we trained ml-IG classifiers to produce the most
likely output tags for the five data representa-
tions. In the input of the classifiers a word was
represented as itself, its part-of-speech tag and
a context of four left and four right word/part-
of-speech tag pairs. For the four JO represen-
tations we used a second phase with a lim-
ited input context (3) but with additionally the
two previous and the two next chunk tags pre-
dicted by the first phase. The classifier out-
put was converted to the 0 representation (open
brackets) and the C representation (close brack-
ets) and the results were combined with the
nine combination methods. In the double-pass
method finding the most likely tag for each word
was split in finding chunk boundaries and as-
signing types to the chunks. The n-pass method
divided this process into eleven passes each of
which recognized one chunk type.
For each processing strategy, all combination
results were better than those obtained with the
five individual classifiers. The differences be-
tween combination results within each process-
ing strategy were small and between the three
strategies the best results were not far apart:
the best Fo=1 rates were 92.40 (single-pass),
92.35 (double-pass) and 92.75 (n-pass).
Since the three processing methods reach a
similar performances, we can choose any of
them for our remaining experiments. The n-
pass method performed best but it has the
disadvantage of needing as many passes as
there are chunk types. This will require a
lot of computation. The single-pass method
was second-best but in order to obtain good
results with this method, we would need to
use a stacked classifier because those performed
better (F0=1.92.40) than the voting methods
(F0=1=91.98). This stacked classifier requires
preprocessed combinator training data which
can be obtained by processing the original train-
</bodyText>
<page confidence="0.996215">
152
</page>
<bodyText confidence="0.999892666666667">
ing data with 10-fold cross-validation. Again
this will require a lot of work for new data sets.
We have chosen for the double-pass method
because in this processing strategy it is possi-
ble to obtain good results with majority vot-
ing. The advantage of using majority voting is
that it does not require extra preprocessed com-
binator training data so by using it we avoid
the extra computation required for generating
this data. We have applied the double-pass
method with majority voting to the CoNLL-
2000 test data while using the complete train-
ing data. The results can be found in table 1.
The recognition method performs well for the
most frequently occurring chunk types (NP, VP
and PP) and worse for the other seven (the test
data did not contain UCP chunks). The recog-
nition rate for NP chunks (F0=1=93.23) is close
to the result for a related standard baseNP data
set obtained by Tjong Kim Sang (2000) (93.26).
Our method outperforms the results mentioned
in Buchholz et al. (1999) in four of the five
cases (ADJP, NP, PP and VP); only for ADVP
chunks it performs slightly worse. This is sur-
prising given that Buchholz et al. (1999) used
956696 tokens of training data and we have used
only 211727 (78% less).
</bodyText>
<sectionHeader confidence="0.973635" genericHeader="method">
4 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999897263157895">
We have evaluated three methods for recogniz-
ing non-recursive non-overlapping text chunks
of arbitrary syntactical categories. In each
method a memory-based learner was trained
to recognize chunks represented in five differ-
ent ways. We have examined nine different
methods for combining the five results. A 10-
fold cross-validation experiment on the train-
ing data of the CoNLL-2000 shared task re-
vealed that (1) the combined results were better
than the individual results, (2) the combination
methods perform equally well and (3) the best
performances of the three processing methods
were similar. We have selected the double-pass
method with majority voting for processing the
CoNLL-2000 shared task data. This method
outperformed an earlier text chunking study for
most chunk types, despite the fact that it used
about 80% less training data.
</bodyText>
<sectionHeader confidence="0.995189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.848524">
Sabine Buchholz, Jorn Veenstra, and Walter Daele-
mans. 1999. Cascaded grammatical relation as-
</reference>
<table confidence="0.9998235">
test data precision recall F0=1
ADJP 85.25% 59.36% 69.99
ADVP 85.03% 71.48% 77.67
CONJP 42.86% 33.33% 37.50
INTJ 100.00% 50.00% 66.67
LST 0.00% 0.00% 0.00
NP 94.14% 92.34% 93.23
PP 96.45% 96.59% 96.52
PRT 79.49% 58.49% 67.39
SBAR 89.81% 72.52% 80.25
VP 93.97% 91.35% 92.64
all 94.04% 91.00% 92.50
</table>
<tableCaption confidence="0.998129">
Table 1: The results per chunk type of process-
</tableCaption>
<bodyText confidence="0.897421">
ing the test data with the double-pass method
and majority voting. Our method outper-
forms most chunk type results mention in Buch-
holz et al. (1999) (FADJP=66.7, FADvP=77.9
FNp=92.3, Fpp=96.8, FNp=91.8) despite the
fact that we have used about 80% less training
data.
</bodyText>
<reference confidence="0.999194466666667">
signment. In Proceedings of EMNLP/VLC-99.
Association for Computational Linguistics.
Walter Daelemans, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 1999. TiMBL:
Tilburg Memory Based Learner, version 2.0,
Reference Guide. ILK Technical Report 99-01.
http://ilk.kub.n1/.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learn-
ing. In Proceedings of the Third ACL Workshop
on Very Large Corpora. Association for Compu-
tational Linguistics.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared
task: Chunking. In Proceedings of the CoNLL-
2000. Association for Computational Linguistics.
Erik F. Tjong Kim Sang. 2000. Noun phrase recog-
nition by system combination. In Proceedings of
the ANLP-NAACL 2000. Seattle, Washington,
USA. Morgan Kaufman Publishers.
Hans van Halteren, Jakub Zavrel, and Walter Daele-
mans. 1998. Improving data driven wordclass
tagging by system combination. In Proceedings
of COLING-ACL &apos;98. Association for Computa-
tional Linguistics.
Sholom M. Weiss and Casimir A. Kulikowski. 1991.
Computer Systems That Learn: Classification and
Prediction Methods from Statistics, Neural Nets,
Machine Learning and Expert Systems. Morgan
Kaufmann.
</reference>
<page confidence="0.999218">
153
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.467645">
<note confidence="0.928926">of CoNLL-2000 and LLL-2000, 151-153, Lisbon, Portugal, 2000.</note>
<title confidence="0.995619">Text Chunking by System Combination</title>
<author confidence="0.999347">Erik F Tjong Kim</author>
<affiliation confidence="0.9272185">CNTS - Language Technology University of</affiliation>
<intro confidence="0.591479">erikt@uia.ua.ac.be</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Jorn Veenstra</author>
<author>Walter Daelemans</author>
</authors>
<title>Cascaded grammatical relation assignment.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC-99. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9196" citStr="Buchholz et al. (1999)" startWordPosition="1534" endWordPosition="1537">void the extra computation required for generating this data. We have applied the double-pass method with majority voting to the CoNLL2000 test data while using the complete training data. The results can be found in table 1. The recognition method performs well for the most frequently occurring chunk types (NP, VP and PP) and worse for the other seven (the test data did not contain UCP chunks). The recognition rate for NP chunks (F0=1=93.23) is close to the result for a related standard baseNP data set obtained by Tjong Kim Sang (2000) (93.26). Our method outperforms the results mentioned in Buchholz et al. (1999) in four of the five cases (ADJP, NP, PP and VP); only for ADVP chunks it performs slightly worse. This is surprising given that Buchholz et al. (1999) used 956696 tokens of training data and we have used only 211727 (78% less). 4 Concluding Remarks We have evaluated three methods for recognizing non-recursive non-overlapping text chunks of arbitrary syntactical categories. In each method a memory-based learner was trained to recognize chunks represented in five different ways. We have examined nine different methods for combining the five results. A 10- fold cross-validation experiment on the</context>
</contexts>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>Sabine Buchholz, Jorn Veenstra, and Walter Daelemans. 1999. Cascaded grammatical relation assignment. In Proceedings of EMNLP/VLC-99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 2.0, Reference Guide.</title>
<date>1999</date>
<tech>ILK Technical Report 99-01. http://ilk.kub.n1/.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 1999</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 1999. TiMBL: Tilburg Memory Based Learner, version 2.0, Reference Guide. ILK Technical Report 99-01. http://ilk.kub.n1/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1033" citStr="Ramshaw and Marcus, 1995" startWordPosition="153" endWordPosition="156">hods, we will also examine if dividing the chunking process in a boundary recognition phase and a type identification phase would aid performance. 2 Approach Tjong Kim Sang (2000) describes how a systeminternal combination of memory-based learners can be used for base noun phrase (baseNP) recognition. The idea is to generate different chunking models by using different chunk representations. Chunks can be represented with bracket structures but alternatively one can use a tagging representation which classifies words as being inside a chunk (I), outside a chunk (0) or at a chunk boundary (B) (Ramshaw and Marcus, 1995). There are four variants of this representation. The B tags can be used for the first word of chunks that immediately follow another chunk (the IOB1 representation) or they can be used for every chunk-initial word (I0B2). Alternatively an E tag can be used for labeling the final word of a chunk immediately preceding another chunk (IOE1) or it can be used for every chunk-final word (10E2). Bracket structures can also be represented as tagging structures by using two streams of tags which define whether words start a chunk or not (0) or whether words are at the end of a chunk or not (C). We nee</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the CoNLL2000. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6250" citStr="Sang and Buchholz, 2000" startWordPosition="1047" endWordPosition="1050">type. In case a word is classified as belonging to more than one chunk type, preference will be given to the chunk type that occurs most often in the training data. We expect the n-pass method to outperform the other two methods. However, we are not sure if the performance difference will be large enough to compensate for the extra computation that is required for this processing method. 3 Results In order to find out which of the three processing methods and which of the nine combination methods performs best, we have applied them to the training data of the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) in a 10-fold cross-validation experiment (Weiss and Kulikowski, 1991). For the single-pass method, we trained ml-IG classifiers to produce the most likely output tags for the five data representations. In the input of the classifiers a word was represented as itself, its part-of-speech tag and a context of four left and four right word/partof-speech tag pairs. For the four JO representations we used a second phase with a limited input context (3) but with additionally the two previous and the two next chunk tags predicted by the first phase. The classifier output was converted to the 0 repres</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of the CoNLL2000. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Noun phrase recognition by system combination.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP-NAACL 2000.</booktitle>
<publisher>Morgan Kaufman Publishers.</publisher>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="1892" citStr="Sang, 2000" startWordPosition="309" endWordPosition="310">n be used for labeling the final word of a chunk immediately preceding another chunk (IOE1) or it can be used for every chunk-final word (10E2). Bracket structures can also be represented as tagging structures by using two streams of tags which define whether words start a chunk or not (0) or whether words are at the end of a chunk or not (C). We need both for encoding the phrase structure and hence we will treat the two tag streams as a single representation (0+C). A combination of baseNP classifiers that use the five representation performs better than any of the included systems (Tjong Kim Sang, 2000). We will apply such a classifier combination to the CoNLL-2000 shared task. The individual classifiers will use the memory-based learning algorithm IB1-IG (Daelemans et al., 1999) for determining the most probable tag for each word. In memory-based learning the training data is stored and a new item is classified by the most frequent classification among training items which are closest to this new item. Data items are represented as sets of feature-value pairs. Features receive weights which are based on the amount of information they provide for classifying the training data (Daelemans et a</context>
<context position="9116" citStr="Sang (2000)" startWordPosition="1524" endWordPosition="1525">quire extra preprocessed combinator training data so by using it we avoid the extra computation required for generating this data. We have applied the double-pass method with majority voting to the CoNLL2000 test data while using the complete training data. The results can be found in table 1. The recognition method performs well for the most frequently occurring chunk types (NP, VP and PP) and worse for the other seven (the test data did not contain UCP chunks). The recognition rate for NP chunks (F0=1=93.23) is close to the result for a related standard baseNP data set obtained by Tjong Kim Sang (2000) (93.26). Our method outperforms the results mentioned in Buchholz et al. (1999) in four of the five cases (ADJP, NP, PP and VP); only for ADVP chunks it performs slightly worse. This is surprising given that Buchholz et al. (1999) used 956696 tokens of training data and we have used only 211727 (78% less). 4 Concluding Remarks We have evaluated three methods for recognizing non-recursive non-overlapping text chunks of arbitrary syntactical categories. In each method a memory-based learner was trained to recognize chunks represented in five different ways. We have examined nine different metho</context>
</contexts>
<marker>Sang, 2000</marker>
<rawString>Erik F. Tjong Kim Sang. 2000. Noun phrase recognition by system combination. In Proceedings of the ANLP-NAACL 2000. Seattle, Washington, USA. Morgan Kaufman Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL &apos;98. Association for Computational Linguistics.</booktitle>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>Hans van Halteren, Jakub Zavrel, and Walter Daelemans. 1998. Improving data driven wordclass tagging by system combination. In Proceedings of COLING-ACL &apos;98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sholom M Weiss</author>
<author>Casimir A Kulikowski</author>
</authors>
<title>Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning and Expert Systems.</title>
<date>1991</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="6320" citStr="Weiss and Kulikowski, 1991" startWordPosition="1056" endWordPosition="1059">unk type, preference will be given to the chunk type that occurs most often in the training data. We expect the n-pass method to outperform the other two methods. However, we are not sure if the performance difference will be large enough to compensate for the extra computation that is required for this processing method. 3 Results In order to find out which of the three processing methods and which of the nine combination methods performs best, we have applied them to the training data of the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) in a 10-fold cross-validation experiment (Weiss and Kulikowski, 1991). For the single-pass method, we trained ml-IG classifiers to produce the most likely output tags for the five data representations. In the input of the classifiers a word was represented as itself, its part-of-speech tag and a context of four left and four right word/partof-speech tag pairs. For the four JO representations we used a second phase with a limited input context (3) but with additionally the two previous and the two next chunk tags predicted by the first phase. The classifier output was converted to the 0 representation (open brackets) and the C representation (close brackets) and</context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>Sholom M. Weiss and Casimir A. Kulikowski. 1991. Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning and Expert Systems. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>