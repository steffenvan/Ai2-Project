<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.946329">
Collocation Extraction Using Monolingual Word Alignment Method
</title>
<author confidence="0.99976">
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Sheng Li1
</author>
<affiliation confidence="0.9731725">
1Harbin Institute of Technology, Harbin, China
2Toshiba (China) Research and Development Center, Beijing, China
</affiliation>
<email confidence="0.9892355">
{liuzhanyi,wanghaifeng,wuhua}@rdc.toshiba.com.cn
lisheng@hit.edu.cn
</email>
<sectionHeader confidence="0.996567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996108862068966">
Statistical bilingual word alignment has been
well studied in the context of machine trans-
lation. This paper adapts the bilingual word
alignment algorithm to monolingual scenario
to extract collocations from monolingual cor-
pus. The monolingual corpus is first repli-
cated to generate a parallel corpus, where
each sentence pair consists of two identical
sentences in the same language. Then the
monolingual word alignment algorithm is
employed to align the potentially collocated
words in the monolingual sentences. Finally
the aligned word pairs are ranked according
to refined alignment probabilities and those
with higher scores are extracted as colloca-
tions. We conducted experiments using Chi-
nese and English corpora individually. Com-
pared with previous approaches, which use
association measures to extract collocations
from the co-occurring word pairs within a
given window, our method achieves higher
precision and recall. According to human
evaluation in terms of precision, our method
achieves absolute improvements of 27.9% on
the Chinese corpus and 23.6% on the English
corpus, respectively. Especially, we can ex-
tract collocations with longer spans, achiev-
ing a high precision of 69% on the long-span
(&gt;6) Chinese collocations.
</bodyText>
<sectionHeader confidence="0.999064" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962304347826">
Collocation is generally defined as a group of
words that occur together more often than by
chance (McKeown and Radev, 2000). In this pa-
per, a collocation is composed of two words oc-
curring as either a consecutive word sequence or
an interrupted word sequence in sentences, such
as &amp;quot;by accident&amp;quot; or &amp;quot;take ... advice&amp;quot;. The collo-
cations in this paper include phrasal verbs (e.g.
&amp;quot;put on&amp;quot;), proper nouns (e.g. &amp;quot;New York&amp;quot;), idi-
oms (e.g. &amp;quot;dry run&amp;quot;), compound nouns (e.g. &amp;quot;ice
cream&amp;quot;), correlative conjunctions (e.g. &amp;quot;either ...
or&amp;quot;), and the other commonly used combinations
in following types: verb+noun, adjective+noun,
adverb+verb, adverb+adjective and adjec-
tive+preposition (e.g. &amp;quot;break rules&amp;quot;, &amp;quot;strong tea&amp;quot;,
&amp;quot;softly whisper&amp;quot;, &amp;quot;fully aware&amp;quot;, and &amp;quot;fond of&amp;quot;).
Many studies on collocation extraction are
carried out based on co-occurring frequencies of
the word pairs in texts (Choueka et al., 1983;
Church and Hanks, 1990; Smadja, 1993; Dun-
ning, 1993; Pearce, 2002; Evert, 2004). These
approaches use association measures to discover
collocations from the word pairs in a given win-
dow. To avoid explosion, these approaches gen-
erally limit the window size to a small number.
As a result, long-span collocations can not be
extracted1. In addition, since the word pairs in
the given window are regarded as potential col-
locations, lots of false collocations exist. Al-
though these approaches used different associa-
tion measures to filter those false collocations,
the precision of the extracted collocations is not
high. The above problems could be partially
solved by introducing more resources into collo-
cation extraction, such as chunker (Wermter and
Hahn, 2004), parser (Lin, 1998; Seretan and We-
hrli, 2006) and WordNet (Pearce, 2001).
This paper proposes a novel monolingual
word alignment (MWA) method to extract collo-
cation of higher quality and with longer spans
only from monolingual corpus, without using
any additional resources. The difference between
MWA and bilingual word alignment (Brown et
al., 1993) is that the MWA method works on
monolingual parallel corpus instead of bilingual
corpus used by bilingual word alignment. The
</bodyText>
<footnote confidence="0.76122575">
1 Here, &amp;quot;span of collocation&amp;quot; means the distance of two
words in a collocation. For example, if the span of the col-
location (w1, w2) is 6, it means there are 5 words interrupt-
ing between w1 and w2 in a sentence.
</footnote>
<page confidence="0.932289">
487
</page>
<note confidence="0.9966225">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 487–495,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999915270833333">
monolingual corpus is replicated to generate a
parallel corpus, where each sentence pair con-
sists of two identical sentences in the same lan-
guage, instead of a sentence in one language and
its translation in another language. We adapt the
bilingual word alignment algorithm to the mono-
lingual scenario to align the potentially collo-
cated word pairs in the monolingual sentences,
with the constraint that a word is not allowed to
be aligned with itself in a sentence. In addition,
we propose a ranking method to finally extract
the collocations from the aligned word pairs.
This method assigns scores to the aligned word
pairs by using alignment probabilities multiplied
by a factor derived from the exponential function
on the frequencies of the aligned word pairs. The
pairs with higher scores are selected as colloca-
tions.
The main contribution of this paper is that the
well studied bilingual statistical word alignment
method is successfully adapted to monolingual
scenario for collocation extraction. Compared
with the previous approaches, which use associa-
tion measures to extract collocations, our method
achieves much higher precision and slightly
higher recall. The MWA method has the follow-
ing three advantages. First, it explicitly models
the co-occurring frequencies and position infor-
mation of word pairs, which are integrated into a
model to search for the potentially collocated
word pairs in a sentence. Second, a new feature,
fertility, is employed to model the number of
words that a word can collocate with in a sen-
tence. Finally, our method can obtain the long-
span collocations. Human evaluations on the ex-
tracted Chinese collocations show that 69% of
the long-span (&gt;6) collocations are correct. Al-
though the previous methods could also extract
long-span collocations by setting the larger win-
dow size, the precision is very low.
In the remainder of this paper, Section 2 de-
scribes the MWA model for collocation extrac-
tion. Section 3 describes the initial experimental
results. In Section 4, we propose a method to
improve the MWA models. Further experiments
are shown in Sections 5 and 6, followed by a dis-
cussion in Section 7. Finally, the conclusions are
presented in Section 8.
</bodyText>
<sectionHeader confidence="0.9303485" genericHeader="introduction">
2 Collocation Extraction With Mono-
lingual Word Alignment Method
</sectionHeader>
<subsectionHeader confidence="0.992821">
2.1 Monolingual Word Alignment
</subsectionHeader>
<bodyText confidence="0.997246">
Given a bilingual sentence pair, a source lan-
guage word can be aligned with its correspond-
</bodyText>
<equation confidence="0.919448666666667">
团队 负责人 在 项目 进行 中 起 关键 作用 。
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .
The team leader plays a key role in the project undertaking .
</equation>
<figureCaption confidence="0.979774">
Figure 1. Bilingual word alignment
</figureCaption>
<bodyText confidence="0.9981405625">
ing target language word. Figure 1 shows an ex-
ample of Chinese-to-English word alignment.
In Figure 1, a word in one language is aligned
with its counterpart in the other language. For
examples, the Chinese word &amp;quot;团队/tuan-dui&amp;quot; is
aligned with its English translation &amp;quot;team&amp;quot;, while
the Chinese word &amp;quot;负责人/fu-ze-ren&amp;quot; is aligned
with its English translation &amp;quot;leader&amp;quot;.
In the Chinese sentence in Figure 1, there are
some Chinese collocations, such as (团队/tuan-
dui, 负责人/fu-ze-ren). There are also some Eng-
lish collocations in the English sentence, such as
(team, leader). We separately illustrate the collo-
cations in the Chinese sentence and the English
sentence in Figure 2, where the collocated words
are aligned with each other.
</bodyText>
<equation confidence="0.813607833333333">
团队 负责人 在 项目 进行 中 起 关键 作用 。
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .
团队 负责人 在 项目 进行 中 起 关键 作用 。
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .
(a) Collocations in the Chinese sentence
(b) Collocations in the English sentence
</equation>
<figureCaption confidence="0.899716">
Figure 2. Word alignments of collocations in
sentence
</figureCaption>
<bodyText confidence="0.981218583333333">
Comparing the alignments in Figures 1 and 2,
we can see that the task of monolingual colloca-
tions construction is similar to that of bilingual
word alignment. In a bilingual sentence pair, a
source word is aligned with its corresponding
target word, while in a monolingual sentence, a
word is aligned with its collocates. Therefore, it
is reasonable to regard collocation construction
as a task of aligning the collocated words in
monolingual sentences.
The team leader plays a key role in the project undertaking.
The team leader plays a key role in the project undertaking.
</bodyText>
<page confidence="0.996718">
488
</page>
<bodyText confidence="0.990351363636364">
Statistical bilingual word alignment method,
which has been well studied in the context of
machine translation, can extract the aligned bi-
lingual word pairs from a bilingual corpus. This
paper adapts the bilingual word alignment algo-
rithm to monolingual scenario to align the collo-
cated words in a monolingual corpus.
Given a sentence with l words S = {w1,..., wl },
the word alignments A = {(i, ai)  |i ∈ [1, l ] } can be
obtained by maximizing the word alignment
probability of the sentence, according to Eq. (1).
</bodyText>
<equation confidence="0.9654362">
A argmax p(A  |S
= ′
∀′
A
Where (i, ai) ∈ A means that the word wi is
</equation>
<bodyText confidence="0.95636075">
aligned with the word wai .
In a monolingual sentence, a word never col-
locates with itself. Thus the alignment set is de-
noted asA= {(i,ai) |i∈ [1,l] &amp;ai ≠i}.
We adapt the bilingual word alignment model,
IBM Model 3 (Brown et al., 1993), to monolin-
gual word alignment. The probability of the
alignment sequence is calculated using Eq. (2).
</bodyText>
<equation confidence="0.99763775">
l
p A S
(  |) ∝ ∏n(φi  |wi)∏t(wj  |waj)d(j  |aj,l) (2)
i=1 j=1
</equation>
<bodyText confidence="0.84926175">
- Word collocation probability
,
which describes the possibility of
collo-
;
- Position collocation probability d(j,
which describes the probability of a word
in position
</bodyText>
<subsectionHeader confidence="0.596374">
collocating with another
</subsectionHeader>
<bodyText confidence="0.785376533333333">
word in position j;
- Fertility probability
, which de-
scribes the probability of the number of
words that a word
can collocate with
(refer to subsection 7.1 for further discus-
Figure 3 shows an example of word alignment
on the English sentence in Figure 2 (b) with the
MWA method. In the sentence, the 7th word
&amp;quot;role&amp;quot; collocates with both the 4th word &amp;quot;play&amp;quot;
and the 6th word &amp;quot;key&amp;quot;. Thus, t(w4
w7
t(wj|waj)
wj
</bodyText>
<equation confidence="0.9448492">
waj
aj,l),
aj
n(φi|wi)
wi
|
) and
t(w6
w7
|
</equation>
<bodyText confidence="0.840759333333333">
) describe the probabilities that the
word &amp;quot;role&amp;quot; collocates with &amp;quot;play&amp;quot; and &amp;quot;key&amp;quot;,
The team leader plays a key role in the project undertaking .
</bodyText>
<figure confidence="0.904092">
12)
</figure>
<figureCaption confidence="0.951491666666667">
Figure 3. Results of MWA method
respectively.
7,1 2) and
</figureCaption>
<bodyText confidence="0.93501975">
describe
the probabilities that the word in position 7 col-
locates with the words in position 4 and 6 in a
sentence with 12 words. For the word &amp;quot;role&amp;quot;,
</bodyText>
<equation confidence="0.974104333333333">
d(4
d(6  |7,1 2)
φ7
</equation>
<bodyText confidence="0.986758285714286">
is 2, which indicates that the word &amp;quot;role&amp;quot; collo-
cates with two words in the sentence.
To train the MWA model, we implement a
MWA tool for collocation extraction, which uses
similar training methods for bilingual word
alignment, except that a word can not be aligned
to itself.
</bodyText>
<subsectionHeader confidence="0.999033">
2.2 Collocation Extraction
</subsectionHeader>
<bodyText confidence="0.8308907">
noted as
,
In our method, we filtered
those aligned word pairs whose frequencies are
lower than
freq(wi
wj).
5. Based on the alignment frequency,
we estimate the alignment probabilities for each
aligned word pair as shown in Eq. (3) and (4).
</bodyText>
<equation confidence="0.977864571428572">
freq(wi , wj
pw|wj
= ∑ freq(w′ , wj )
w′
freq(wi, wj
p w w
w′
</equation>
<bodyText confidence="0.8894082">
to the aligned word pairs an
d those with higher
scores are selected as collocations, which are
estimated as shown in Eq. (5).
p w w p w w
</bodyText>
<equation confidence="0.998791333333333">
(  |) (  |)
+
i j j i
</equation>
<bodyText confidence="0.76127275">
2
Since our method does not use an
y linguistic in-
formation, we compared our method with the
</bodyText>
<equation confidence="0.989824923076923">
) (1)
l
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11)
The team leader plays a key role in the project
(1) (2) (3) (4) (5) (6) (7)
undertaking
(8) (9) (10) (11) (12)
)
(3)
)
p w w
( , ) =
i j
</equation>
<bodyText confidence="0.9669106875">
Where
denotes the number of words that are
aligned with
Three
φi
wi.
kinds of probabilities are
involved:
cating with
sion).
Given a monolingual corpus, we use the trained
MWA model to align the collocated words in
each sentence. As a result, we can generate a set
of aligned word pairs on the corpus. According
to the alignment results, we calculate the fre-
quency for two words aligned in the corpus, de-
</bodyText>
<equation confidence="0.992257333333333">
(  |)
j i freq w w
=
∑ ( , )
′
i
</equation>
<bodyText confidence="0.932753">
With alignment probabilities, we assign scores
</bodyText>
<sectionHeader confidence="0.99773" genericHeader="method">
3 Initial Experiments
</sectionHeader>
<bodyText confidence="0.998237">
In this experiment, we used the method as de-
scribed in Section 2 for collocation extraction.
</bodyText>
<page confidence="0.991062">
489
</page>
<figure confidence="0.977370777777778">
#
(CTop
=
# (C )
Top−N
N I I Cgold) (6)
precision
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
</figure>
<figureCaption confidence="0.999988">
Figure 4. Precision of collocations
</figureCaption>
<bodyText confidence="0.99994575">
baseline methods without using linguistic knowl-
edge. These baseline methods take all co-
occurring word pairs within a given window as
collocation candidates, and then use association
measures to rank the candidates. Those candi-
dates with higher association scores are extracted
as collocations. In this paper, the window size is
set to [-6, +6].
</bodyText>
<subsectionHeader confidence="0.988021">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999686181818182">
The experiments were carried out on a Chinese
corpus, which consists of one year (2004) of the
Xinhua news corpus from LDC 2 , containing
about 28 millions of Chinese words. Since punc-
tuations are rarely used to construct collocations,
they were removed from the corpora. To auto-
matically estimate the precision of extracted col-
locations on the Chinese corpus, we built a gold
set by collecting Chinese collocations from
handcrafted collocation dictionaries, containing
56,888 collocations.
</bodyText>
<subsectionHeader confidence="0.813765">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.9974075">
The precision is automatically calculated against
the gold set according to Eq. (6).
Where CTop-N and Cgold denote the top colloca-
tions in the N-best list and the collocations in the
gold set, respectively.
We compared our method with several base-
line methods using different association meas-
ures3: co-occurring frequency, log-likelihood
</bodyText>
<footnote confidence="0.890241">
2 Available at: http://www.ldc.upenn.edu/Catalog/Catalog
Entry.jsp?catalogId=LDC2007T03
3 The definitions of these measures can be found in Man-
ning and Schütze (1999).
</footnote>
<figure confidence="0.8391">
0.0 2.5 3.7 4.8 5.8 6.8 7.8 8.9
log(frequency)
</figure>
<figureCaption confidence="0.972375">
Figure 5. Frequency vs. precision/alignment
probability
</figureCaption>
<bodyText confidence="0.999808078947368">
ratio, chi-square test, mutual information, and t-
test. Among them, the log-likelihood ratio meas-
ure achieves the best performance. Thus, in this
paper, we only show the performance of the log-
likelihood ratio measure.
Figure 4 shows the precisions of the top N col-
locations as N steadily increases with an incre-
ment of 1K, which are extracted by our method
and the baseline method using log-likelihood
ratio as the association measure.
The absolute precision of collocations is not
high in the figure. For example, among the top
200K collocations, about 4% of the collocations
are correct. This is because our gold set contains
only about 57K collocations. Even if all colloca-
tions in the gold set are included in the 200K-
best list, the precision is only 28%. Thus, it is
more useful to compare precision curves for col-
locations in the N-best lists extracted by different
methods. In addition, since this gold set only in-
cludes a small number of collocations, the preci-
sion curves of our method and the baseline
method are getting closer, as N increases. For
example, when N is set to 200K, our method and
the baseline method achieved precisions of
4.09% and 3.12%, respectively. And when N is
set to 400K, they achieved 2.78% and 2.26%,
respectively. For convenience of comparison, we
set N up to 200K in the experiments.
From the results, it can also be seen that,
among the N-best lists with N less than 20K, the
precision of the collocations extracted by our
method is lower than that of the collocations ex-
tracted by the baseline, and became higher when
N is larger than 20K.
In order to analyze the possible reasons, we
investigated the relationships among the fre-
quencies of the aligned word pairs, the alignment
</bodyText>
<figure confidence="0.998094315789473">
Precision (%)
12
10
4
2
8
6
0
Our method (Probability)
Log-likelihood ratio
(%)
100
40
20
60
80
0
Precision
Alignment Probability
</figure>
<page confidence="0.581975">
490
</page>
<figureCaption confidence="0.709015">
Figure 6. y = e−b/x
</figureCaption>
<figure confidence="0.998671642857143">
y
b=2
b=4
x
Precision (%)
25
20
15
10
5
0
Refined probability
Probability
Baseline (Log-likelihood ratio)
</figure>
<bodyText confidence="0.995574636363637">
probabilities, and precisions of collocations,
which are shown in Figure 5. From the figure,
we can see (1) that the lower the frequencies of
the aligned word pairs are, the higher the align-
ment probabilities are; and (2) that the precisions
of the aligned word pairs with lower frequencies
is lower. According to the above observations,
we conclude that it is the word pairs with lower
frequencies but higher probabilities that caused
the lower precision of the top 20K collocations
extracted by our method.
</bodyText>
<sectionHeader confidence="0.997926" genericHeader="method">
4 Improved MWA Method
</sectionHeader>
<bodyText confidence="0.947578076923077">
According to the analysis in subsection 3.2, we
need to penalize the aligned word pairs with
lower frequencies. In order to achieve the above
goal, we need to refine the alignment probabili-
ties by using a penalization factor derived from a
function on the frequencies of the aligned word
pairs. This function y = f(x) should satisfy the
following two conditions, where x represents
the log function of frequencies.
(1) The function is monotonic. When x is set to
a smaller number, y is also small. This re-
sults in the penalization on the aligned word
pairs with lower frequencies.
</bodyText>
<listItem confidence="0.697952">
(2) When x → oo , y is set to 1. This means that
</listItem>
<bodyText confidence="0.95836575">
we don’t penalize the aligned word pairs
with higher frequencies.
According to the above descriptions, we pro-
pose to use the exponential function in Eq. (7).
</bodyText>
<equation confidence="0.977836333333333">
y e − /
b x
= (7)
</equation>
<bodyText confidence="0.9928794">
Figure 6 describes this function. The constant
b in the function is used to adjust the shape of the
line. The line is sharp with b set to a small num-
ber, while the line is flat with b set to a larger
number. In our case, if b is set to a larger number,
</bodyText>
<figure confidence="0.99882">
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
</figure>
<figureCaption confidence="0.9986955">
Figure 7. Precision of collocations extracted by
the improved method
</figureCaption>
<bodyText confidence="0.995544333333333">
we assign a larger penalization weight to those
aligned word pairs with lower frequencies.
According to the above discussion, we can use
the following measure to assign scores to the
aligned words pairs generated by the MWA
method.
</bodyText>
<equation confidence="0.964765">
p(wi  |wj) + p(wj  |wi
2
</equation>
<bodyText confidence="0.9997064">
Where wi and wj are two aligned words. p(wi|wj)
and p(wj|wi) are alignment probabilities as shown
in Eq. (3) and (4). log(freq(wi,wj)) is the log
function of the frequencies of the aligned word
pairs (wi, wj).
</bodyText>
<sectionHeader confidence="0.967014" genericHeader="method">
5 Evaluation on Chinese corpus
</sectionHeader>
<bodyText confidence="0.9999645">
We used the same Chinese corpus described in
Section 3 to evaluate the improved method as
shown in Section 4. In the experiments, b was
tuned by using a development set and set to 25.
</bodyText>
<subsectionHeader confidence="0.968699">
5.1 Precision
</subsectionHeader>
<bodyText confidence="0.999791333333333">
In this section, we evaluated the extracted collo-
cations in terms of precision using both auto-
matic evaluation and human evaluation.
</bodyText>
<sectionHeader confidence="0.612888" genericHeader="method">
Automatic Evaluation
</sectionHeader>
<figureCaption confidence="0.316740333333333">
Figure 7 shows the precisions of the colloca-
tions in the N-best lists extracted by our method
and the baseline method against the gold set in
Section 3. For our methods, we used two differ-
ent measures to rank the aligned word pairs:
alignment probabilities in Eq. (5) and refined
</figureCaption>
<figure confidence="0.640361">
,
)
wj
(wi
pr
=
)x − b (8)
e log( freqwi wj
( , ))
</figure>
<page confidence="0.98576">
491
</page>
<table confidence="0.995372">
Our method Baseline
True 569 290
False A 25 16
B 5 4
C 240 251
D 161 439
</table>
<tableCaption confidence="0.950912">
Table 1. Manual evaluation of the top 1K Chi-
</tableCaption>
<bodyText confidence="0.993491333333333">
nese collocations. The precisions of our method
and the baseline method are 56.9% and 29.0%,
respectively.
alignment probabilities in Eq. (8). From the re-
sults, it can be seen that with the refined align-
ment probabilities, our method achieved the
highest precision on the N-best lists, which
greatly outperforms the best baseline method.
For example, in the top 1K list, our method
achieves a precision of 20.6%, which is much
higher than the precision of the baseline method
(11.7%). This indicates that the exponential func-
tion used to penalize the alignment probabilities
plays a key role in demoting most of the aligned
word pairs with low frequencies.
</bodyText>
<subsectionHeader confidence="0.614449">
Human Evaluation
</subsectionHeader>
<bodyText confidence="0.999493148148148">
In automatic evaluation, the gold set only con-
tains collocations in the existing dictionaries.
Some collocations related to specific corpora are
not included in the set. Therefore, we selected
the top 1K collocations extracted by our im-
proved method to manually estimate the preci-
sion. During human evaluation, the true colloca-
tions are denoted as &amp;quot;True&amp;quot; in our experiments.
The false collocations were further classified into
the following classes.
A: The candidate consists of two words that
are semantically related, such as (X&apos;t doctor,
T&apos;± nurse).
B: The candidate is a part of the multi-word
(&gt;_ 3) collocation. For example, (n;J self, #L$q
mechanism) is a part of the three-word colloca-
tion (n;J self, f_h3k regulating, #L$q mecha-
nism).
C: The candidates consist of the adjacent
words that frequently occur together, such as (ft
he, iA say) and (TR very, 0 good).
D: Two words in the candidates have no rela-
tionship with each other, but occur together fre-
quently, such as (ALT, Beijing, M month) and
(*p and, )7 for).
Table 1 shows the evaluation results. Our
method extracted 569 true collocations, which
</bodyText>
<figure confidence="0.9729985">
0 1 2 3 4 5 6 7 8 9 10 11 12
Training corpus (Months)
</figure>
<figureCaption confidence="0.999908">
Figure 8. Corpus size vs. precision
</figureCaption>
<bodyText confidence="0.9999498">
are much more than those extracted by the base-
line method. Further analysis shows that, in addi-
tion to extracting short-span collocations, our
method extracted collocations with longer spans
as compared with the baseline method. For ex-
ample, (A-T in, %I,kk, state) and (EhT because,
Nk so) are two long-span collocations. Among
the 1K collocations, there are 48 collocation can-
didates whose spans are larger than 6, which are
not covered by the baseline method since the
window size is set to 6. And 33 of them are true
collocations, with a higher precision of 69%.
Classes C and D account for the most part of
the false collocations. Although the words in
these two classes co-occur frequently, they can
not be regarded as collocations. And we also
found out that the errors in class D produced by
the baseline method are much more than that of
those produced by our method. This indicates
that our MWA method can remove much more
noise from the frequently occurring word pairs.
In Class A, the two words are semantically re-
lated and occur together in the corpus. These
kinds of collocations can not be distinguished
from the true collocations by our method without
additional resources.
Since only bigram collocations were extracted
by our method, the multi-word (&gt;_ 3) collocations
were split into bigram collocations, which caused
the error collocations in Class B4.
</bodyText>
<subsectionHeader confidence="0.581214">
Corpus size vs. precision
</subsectionHeader>
<bodyText confidence="0.999992571428571">
Here, we investigated the effect of the corpus
size on the precision of the extracted collocations.
We evaluated the precision against the gold set
as shown in the automatic evaluation. First, the
whole corpus (one year of newspaper) was split
into 12 parts according to the published months.
Then we calculated the precisions as the training
</bodyText>
<footnote confidence="0.889165">
4 Since only a very small faction of collocations contain
more than two words, a few error collocations belong to
Class B.
</footnote>
<figure confidence="0.989038">
Precision (%)
12
10
4
2
6
0
8
Our method
Baseline
</figure>
<page confidence="0.503991">
492
</page>
<figure confidence="0.999685">
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
</figure>
<figureCaption confidence="0.999982">
Figure 9. Recall on the Chinese corpus
</figureCaption>
<bodyText confidence="0.999323235294118">
corpus increases part by part. The top 20K collo-
cations were selected for evaluation.
Figure 8 shows the experimental results. The
precision of collocations extracted by our method
is obviously higher than that of collocations ex-
tracted by the baseline method. When the size of
the training corpus became larger, the difference
between our method and the baseline method
also became bigger. When the training corpus
contains more than 9 months of corpora, the pre-
cision of collocations extracted by the baseline
method did not increase anymore. However, the
precision of collocations extracted by our method
kept on increasing. This indicates the MWA
method can extract more true collocations of
higher quality when it is trained with larger size
of training data.
</bodyText>
<subsectionHeader confidence="0.996362">
5.2 Recall
</subsectionHeader>
<bodyText confidence="0.999246714285714">
Recall was evaluated on a manually labeled sub-
set of the training corpus. The subset contains
100 sentences that were randomly selected from
the whole corpus. The sentence average length is
24. All true collocations (660) were labeled
manually. The recall was calculated according to
Eq. (9).
</bodyText>
<equation confidence="0.688969333333333">
recall = #(CTop-N n Cs,.t) (9)
#( )
Csubset
</equation>
<bodyText confidence="0.974405875">
Here, CTop-N denotes the top collocations in the
N-best list and Csubset denotes the true colloca-
tions in the subset.
Figure 9 shows the recalls of collocations ex-
tracted by our method and the baseline method
on the labeled subset. The results show that our
method can extract more true collocations than
the baseline method.
</bodyText>
<figure confidence="0.9986345">
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
</figure>
<figureCaption confidence="0.999551">
Figure 10. Recall on the English corpus
</figureCaption>
<table confidence="0.995431333333333">
Our method Baseline
True 591 355
False A 11 4
B 19 20
C 200 136
D 179 485
</table>
<bodyText confidence="0.9424118125">
and the baseline method are 59.1% an
d 35.5%,
respectively.
of collocation
Although the colloca-
tions of our method are much less than that of the
baseline, the experi
candidates5.
ments show that the recall of
our method is higher. This again proved that our
method has the stronger ability to distinguish
true collocations from false collocations.
randomly extracted from the Bri
tish National
Corpus6. The English corpus contains about 20
millions of words.
</bodyText>
<subsectionHeader confidence="0.96597">
6.1 Precision
</subsectionHeader>
<bodyText confidence="0.999947571428572">
We estimated the precision of the top 1K collo-
cations. Table 2 shows the results. The classifica-
tion of the false collocations is the same as that
in Table 1. The results show that our methods
outperformed the baseline method using log-
We set the threshold to 7.88 with a confidence level of
= 0.005 (cf. page 174 of Chapter 5 in (McKeown an
</bodyText>
<page confidence="0.914755">
5
</page>
<figure confidence="0.937462136363636">
α
d
Radev, 2000) for more details).
6 Available at: http://www.hcu.ox.ac.uk/BNC/
Recall (%)
100
40
20
60
80
0
Our method
Baseline
Recall (%)
100
40
20
60
80
0
Our method
Baseline
</figure>
<tableCaption confidence="0.86752">
Table 2. Manual evaluation of the top 1K Eng-
lish collocations. The precisions of our method
</tableCaption>
<bodyText confidence="0.999756">
In our experiments, the baseline method ex-
tracts about 20 millions of collocation candidates,
while our method only extracts about 3 millions
</bodyText>
<sectionHeader confidence="0.989776" genericHeader="evaluation">
6 Evaluation on English corpus
</sectionHeader>
<bodyText confidence="0.99866">
We also manually evaluated the proposed
method on an English corpus, which is a subset
</bodyText>
<page confidence="0.997752">
493
</page>
<figure confidence="0.99962">
0 20 40 60 80 100 120 140 160 180
Top-N collocation (K)
</figure>
<figureCaption confidence="0.999901">
Figure 11. Fertility vs. precision
</figureCaption>
<bodyText confidence="0.951744">
likelihood ratio. And the distribution of the false
collocations is similar to that on the Chinese cor-
pus.
</bodyText>
<subsectionHeader confidence="0.986572">
6.2 Recall
</subsectionHeader>
<bodyText confidence="0.999994">
We used the method described in subsection 5.2
to calculate the recall. 100 English sentences
were labeled manually, obtaining 205 true collo-
cations. Figure 10 shows the recall of the collo-
cations in the N-best lists. From the figure, it can
be seen that the trend on the English corpus is
similar to that on the Chinese corpus, which in-
dicates that our method is language-independent.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="discussions">
7 Discussion
</sectionHeader>
<subsectionHeader confidence="0.996132">
7.1 The Effect of Fertility
</subsectionHeader>
<bodyText confidence="0.997336526315789">
In the MWA model as described in subsection
2.1, 0; denotes the number of words that can
align with w; . Since a word only collocates with
a few other words in a sentence, we should set a
maximum number for 0 , denote as 0max.
In order to set 0max , we examined the true col-
locations in the manually labeled set described in
subsection 5.2. We found that 78% of words col-
locate with only one word, and 17% of words
collocate with two words. In sum, 95% of words
in the corpus can only collocate with at most two
words. According to the above observation, we
set 0max to 2.
In order to further examine the effect of 0max
on collocation extraction, we used several differ-
ent 0max in our experiments. The comparison
results are shown in Figure 11. The highest pre-
cision is achieved when 0max is set to 2. This
result verifies our observation on the corpus.
</bodyText>
<subsectionHeader confidence="0.995669">
7.2 Span of Collocation
</subsectionHeader>
<bodyText confidence="0.999907">
One of the advantages of our method is that
long-span collocations can be reliably extracted.
In this subsection, we investigate the distribution
of the span of the aligned word pairs. For the
aligned word pairs occurring more than once, we
calculated the average span as shown in Eq. (10).
</bodyText>
<equation confidence="0.75702925">
∑ Span
∈
= s corpus (10)
)
</equation>
<bodyText confidence="0.996158615384615">
Where, Span(w;,wi;s) is the span of the words
w; and wi in the sentence s; AveSpan(w;,wi) is
the average span.
The distribution is shown in Figure 12. It can
be seen that the number of the aligned word pairs
decreased exponentially as the average span in-
creased. About 17% of the aligned word pairs
have spans longer than 6. According to the hu-
man evaluation result for precision in subsection
5.1, the precision of the long-span collocations is
even higher than that of the short-span colloca-
tions. This indicates that our method can extract
reliable collocations with long spans.
</bodyText>
<sectionHeader confidence="0.998274" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999975125">
We have presented a monolingual word align-
ment method to extract collocations from mono-
lingual corpus. We first replicated the monolin-
gual corpus to generate a parallel corpus, in
which each sentence pair consists of the two
identical sentences in the same language. Then
we adapted the bilingual word alignment algo-
rithm to the monolingual scenario to align the
</bodyText>
<figure confidence="0.998016775000001">
Precision (%)
20
15
10
5
0
0max
0max
0max
0max
=
=
=
=
1
10
2
3
(w; , i ; )
w s
,
)
wi
AveSpan(w;
freq
(
,
w;
wi
log(#(aligned word pairs)) 8
7
6
5
4
3
2
1
0
0 20 40 60 80 100
Span of collocation
</figure>
<figureCaption confidence="0.998885">
Figure 12. Distribution of spans
</figureCaption>
<page confidence="0.99344">
494
</page>
<bodyText confidence="0.999956789473684">
potentially collocated word pairs in the monolin-
gual sentences. In addition, a ranking method
was proposed to finally extract the collocations
from the aligned word pairs. It scores collocation
candidates by using alignment probabilities mul-
tiplied by a factor derived from the exponential
function on the frequencies. Those with higher
scores are selected as collocations. Both Chinese
and English collocation extraction experiments
indicate that our method outperforms previous
approaches in terms of both precision and recall.
For example, according to the human evaluations
on the Chinese corpus, our method achieved a
precision of 56.9%, which is much higher than
that of the baseline method (29.0%). Moreover,
we can extract collocations with longer span.
Human evaluation on the extracted Chinese col-
locations shows that 69% of the long-span (&gt;6)
collocations are correct.
</bodyText>
<sectionHeader confidence="0.999194" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909173076923">
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2): 263-311.
Yaacov Choueka, S.T. Klein, and E. Neuwitz. 1983.
Automatic Retrieval of Frequent Idiomatic and
Collocational Expressions in a Large Corpus.
Journal for Literary and Linguistic computing,
4(1):34-38.
Kenneth Church and Patrick Hanks. 1990. Word As-
sociation Norms, Mutual Information, and Lexi-
cography. Computational Linguistics, 16(1):22-29.
Ted Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1): 61-74.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
University of Stuttgart.
Dekang Lin. 1998. Extracting Collocations from Text
Corpora. In Proceedings of the 1st Workshop on
Computational Terminology, pp. 57-63.
Christopher D. Manning and Hinrich Schütze. 1999.
Foundations of Statistical Natural Language Proc-
essing, Cambridge, MA; London, U.K.: Bradford
Book &amp; MIT Press.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
Collocations. In Robert Dale, Hermann Moisl, and
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523.
Darren Pearce. 2001. Synonymy in Collocation Ex-
traction. In Proceedings of NAACL-2001 Workshop
on Wordnet and Other Lexical Resources: Applica-
tions, Extensions and Customizations, pp. 41-46.
Darren Pearce. 2002. A Comparative Evaluation of
Collocation Extraction Techniques. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pp. 651-658.
Violeta Seretan and Eric Wehrli. 2006. Accurate Col-
location Extraction Using a Multilingual Parser. In
Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING/ACL-2006), pp. 953-960
Frank Smadja. 1993. Retrieving Collocations from
Text: Xtract. Computational Linguistics, 19(1):
143-177.
Joachim Wermter and Udo Hahn. 2004. Collocation
Extraction Based on Modifiability Statistics. In
Proceedings of the 20th International Conference
on Computational Linguistics (COLING-2004), pp.
980-986.
</reference>
<page confidence="0.999127">
495
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423270">
<title confidence="0.999809">Collocation Extraction Using Monolingual Word Alignment Method</title>
<author confidence="0.994077">Haifeng Hua Sheng</author>
<affiliation confidence="0.998614">Institute of Technology, Harbin, China</affiliation>
<address confidence="0.499212">(China) Research and Development Center, Beijing, China</address>
<email confidence="0.986173">lisheng@hit.edu.cn</email>
<abstract confidence="0.9951158">Statistical bilingual word alignment has been well studied in the context of machine translation. This paper adapts the bilingual word alignment algorithm to monolingual scenario to extract collocations from monolingual corpus. The monolingual corpus is first replicated to generate a parallel corpus, where each sentence pair consists of two identical sentences in the same language. Then the monolingual word alignment algorithm is employed to align the potentially collocated words in the monolingual sentences. Finally the aligned word pairs are ranked according to refined alignment probabilities and those with higher scores are extracted as collocations. We conducted experiments using Chinese and English corpora individually. Compared with previous approaches, which use association measures to extract collocations from the co-occurring word pairs within a given window, our method achieves higher precision and recall. According to human evaluation in terms of precision, our method achieves absolute improvements of 27.9% on the Chinese corpus and 23.6% on the English corpus, respectively. Especially, we can extract collocations with longer spans, achieving a high precision of 69% on the long-span (&gt;6) Chinese collocations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="3565" citStr="Brown et al., 1993" startWordPosition="526" endWordPosition="529">ifferent association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &amp;quot;span of collocation&amp;quot; means the distance of two words in a collocation. For example, if the span of the collocation (w1, w2) is 6, it means there are 5 words interrupting between w1 and w2 in a sentence. 487 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 487–495, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP monolingual corpus is replicated to generate a parallel corpus, where each sentence pair consists of two </context>
<context position="9098" citStr="Brown et al., 1993" startWordPosition="1456" endWordPosition="1459">the bilingual word alignment algorithm to monolingual scenario to align the collocated words in a monolingual corpus. Given a sentence with l words S = {w1,..., wl }, the word alignments A = {(i, ai) |i ∈ [1, l ] } can be obtained by maximizing the word alignment probability of the sentence, according to Eq. (1). A argmax p(A |S = ′ ∀′ A Where (i, ai) ∈ A means that the word wi is aligned with the word wai . In a monolingual sentence, a word never collocates with itself. Thus the alignment set is denoted asA= {(i,ai) |i∈ [1,l] &amp;ai ≠i}. We adapt the bilingual word alignment model, IBM Model 3 (Brown et al., 1993), to monolingual word alignment. The probability of the alignment sequence is calculated using Eq. (2). l p A S ( |) ∝ ∏n(φi |wi)∏t(wj |waj)d(j |aj,l) (2) i=1 j=1 - Word collocation probability , which describes the possibility of collo; - Position collocation probability d(j, which describes the probability of a word in position collocating with another word in position j; - Fertility probability , which describes the probability of the number of words that a word can collocate with (refer to subsection 7.1 for further discusFigure 3 shows an example of word alignment on the English sentence </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaacov Choueka</author>
<author>S T Klein</author>
<author>E Neuwitz</author>
</authors>
<title>Automatic Retrieval of Frequent Idiomatic and Collocational Expressions in a Large Corpus. Journal for Literary and Linguistic computing,</title>
<date>1983</date>
<pages>4--1</pages>
<contexts>
<context position="2450" citStr="Choueka et al., 1983" startWordPosition="352" endWordPosition="355">such as &amp;quot;by accident&amp;quot; or &amp;quot;take ... advice&amp;quot;. The collocations in this paper include phrasal verbs (e.g. &amp;quot;put on&amp;quot;), proper nouns (e.g. &amp;quot;New York&amp;quot;), idioms (e.g. &amp;quot;dry run&amp;quot;), compound nouns (e.g. &amp;quot;ice cream&amp;quot;), correlative conjunctions (e.g. &amp;quot;either ... or&amp;quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &amp;quot;break rules&amp;quot;, &amp;quot;strong tea&amp;quot;, &amp;quot;softly whisper&amp;quot;, &amp;quot;fully aware&amp;quot;, and &amp;quot;fond of&amp;quot;). Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004). These approaches use association measures to discover collocations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted colloca</context>
</contexts>
<marker>Choueka, Klein, Neuwitz, 1983</marker>
<rawString>Yaacov Choueka, S.T. Klein, and E. Neuwitz. 1983. Automatic Retrieval of Frequent Idiomatic and Collocational Expressions in a Large Corpus. Journal for Literary and Linguistic computing, 4(1):34-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<date>1990</date>
<journal>Word Association Norms, Mutual Information, and Lexicography. Computational Linguistics,</journal>
<pages>16--1</pages>
<contexts>
<context position="2474" citStr="Church and Hanks, 1990" startWordPosition="356" endWordPosition="359">or &amp;quot;take ... advice&amp;quot;. The collocations in this paper include phrasal verbs (e.g. &amp;quot;put on&amp;quot;), proper nouns (e.g. &amp;quot;New York&amp;quot;), idioms (e.g. &amp;quot;dry run&amp;quot;), compound nouns (e.g. &amp;quot;ice cream&amp;quot;), correlative conjunctions (e.g. &amp;quot;either ... or&amp;quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &amp;quot;break rules&amp;quot;, &amp;quot;strong tea&amp;quot;, &amp;quot;softly whisper&amp;quot;, &amp;quot;fully aware&amp;quot;, and &amp;quot;fond of&amp;quot;). Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004). These approaches use association measures to discover collocations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The a</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Patrick Hanks. 1990. Word Association Norms, Mutual Information, and Lexicography. Computational Linguistics, 16(1):22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>61--74</pages>
<contexts>
<context position="2503" citStr="Dunning, 1993" startWordPosition="362" endWordPosition="364"> in this paper include phrasal verbs (e.g. &amp;quot;put on&amp;quot;), proper nouns (e.g. &amp;quot;New York&amp;quot;), idioms (e.g. &amp;quot;dry run&amp;quot;), compound nouns (e.g. &amp;quot;ice cream&amp;quot;), correlative conjunctions (e.g. &amp;quot;either ... or&amp;quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &amp;quot;break rules&amp;quot;, &amp;quot;strong tea&amp;quot;, &amp;quot;softly whisper&amp;quot;, &amp;quot;fully aware&amp;quot;, and &amp;quot;fond of&amp;quot;). Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004). These approaches use association measures to discover collocations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partia</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1): 61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences: Word Pairs and Collocations.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="2531" citStr="Evert, 2004" startWordPosition="367" endWordPosition="368">l verbs (e.g. &amp;quot;put on&amp;quot;), proper nouns (e.g. &amp;quot;New York&amp;quot;), idioms (e.g. &amp;quot;dry run&amp;quot;), compound nouns (e.g. &amp;quot;ice cream&amp;quot;), correlative conjunctions (e.g. &amp;quot;either ... or&amp;quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &amp;quot;break rules&amp;quot;, &amp;quot;strong tea&amp;quot;, &amp;quot;softly whisper&amp;quot;, &amp;quot;fully aware&amp;quot;, and &amp;quot;fond of&amp;quot;). Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004). These approaches use association measures to discover collocations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing mo</context>
</contexts>
<marker>Evert, 2004</marker>
<rawString>Stefan Evert. 2004. The Statistics of Word Cooccurrences: Word Pairs and Collocations. Ph.D. thesis, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Extracting Collocations from Text Corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st Workshop on Computational Terminology,</booktitle>
<pages>57--63</pages>
<contexts>
<context position="3232" citStr="Lin, 1998" startWordPosition="477" endWordPosition="478">a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &amp;quot;span of collocation&amp;quot; means the distance of two words in a collocation. For example, if the span of the collocation (w1, w2) is 6, i</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Extracting Collocations from Text Corpora. In Proceedings of the 1st Workshop on Computational Terminology, pp. 57-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schütze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing,</booktitle>
<publisher>Bradford Book &amp; MIT Press.</publisher>
<location>Cambridge, MA; London, U.K.:</location>
<contexts>
<context position="13446" citStr="Manning and Schütze (1999)" startWordPosition="2234" endWordPosition="2238">by collecting Chinese collocations from handcrafted collocation dictionaries, containing 56,888 collocations. 3.2 Results The precision is automatically calculated against the gold set according to Eq. (6). Where CTop-N and Cgold denote the top collocations in the N-best list and the collocations in the gold set, respectively. We compared our method with several baseline methods using different association measures3: co-occurring frequency, log-likelihood 2 Available at: http://www.ldc.upenn.edu/Catalog/Catalog Entry.jsp?catalogId=LDC2007T03 3 The definitions of these measures can be found in Manning and Schütze (1999). 0.0 2.5 3.7 4.8 5.8 6.8 7.8 8.9 log(frequency) Figure 5. Frequency vs. precision/alignment probability ratio, chi-square test, mutual information, and ttest. Among them, the log-likelihood ratio measure achieves the best performance. Thus, in this paper, we only show the performance of the loglikelihood ratio measure. Figure 4 shows the precisions of the top N collocations as N steadily increases with an increment of 1K, which are extracted by our method and the baseline method using log-likelihood ratio as the association measure. The absolute precision of collocations is not high in the fi</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing, Cambridge, MA; London, U.K.: Bradford Book &amp; MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Dragomir R Radev</author>
</authors>
<title>Collocations. In</title>
<date>2000</date>
<booktitle>A Handbook of Natural Language Processing,</booktitle>
<pages>507--523</pages>
<contexts>
<context position="1680" citStr="McKeown and Radev, 2000" startWordPosition="235" endWordPosition="238">us approaches, which use association measures to extract collocations from the co-occurring word pairs within a given window, our method achieves higher precision and recall. According to human evaluation in terms of precision, our method achieves absolute improvements of 27.9% on the Chinese corpus and 23.6% on the English corpus, respectively. Especially, we can extract collocations with longer spans, achieving a high precision of 69% on the long-span (&gt;6) Chinese collocations. 1 Introduction Collocation is generally defined as a group of words that occur together more often than by chance (McKeown and Radev, 2000). In this paper, a collocation is composed of two words occurring as either a consecutive word sequence or an interrupted word sequence in sentences, such as &amp;quot;by accident&amp;quot; or &amp;quot;take ... advice&amp;quot;. The collocations in this paper include phrasal verbs (e.g. &amp;quot;put on&amp;quot;), proper nouns (e.g. &amp;quot;New York&amp;quot;), idioms (e.g. &amp;quot;dry run&amp;quot;), compound nouns (e.g. &amp;quot;ice cream&amp;quot;), correlative conjunctions (e.g. &amp;quot;either ... or&amp;quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &amp;quot;break rules&amp;quot;, &amp;quot;strong tea&amp;quot;, &amp;quot;softly whisper&amp;quot;</context>
</contexts>
<marker>McKeown, Radev, 2000</marker>
<rawString>Kathleen R. McKeown and Dragomir R. Radev. 2000. Collocations. In Robert Dale, Hermann Moisl, and Harold Somers (Ed.), A Handbook of Natural Language Processing, pp. 507-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Pearce</author>
</authors>
<title>Synonymy in Collocation Extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL-2001 Workshop on Wordnet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<pages>41--46</pages>
<contexts>
<context position="3286" citStr="Pearce, 2001" startWordPosition="486" endWordPosition="487">es generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &amp;quot;span of collocation&amp;quot; means the distance of two words in a collocation. For example, if the span of the collocation (w1, w2) is 6, it means there are 5 words interrupting between w1 and </context>
</contexts>
<marker>Pearce, 2001</marker>
<rawString>Darren Pearce. 2001. Synonymy in Collocation Extraction. In Proceedings of NAACL-2001 Workshop on Wordnet and Other Lexical Resources: Applications, Extensions and Customizations, pp. 41-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Pearce</author>
</authors>
<title>A Comparative Evaluation of Collocation Extraction Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>651--658</pages>
<contexts>
<context position="2517" citStr="Pearce, 2002" startWordPosition="365" endWordPosition="366">include phrasal verbs (e.g. &amp;quot;put on&amp;quot;), proper nouns (e.g. &amp;quot;New York&amp;quot;), idioms (e.g. &amp;quot;dry run&amp;quot;), compound nouns (e.g. &amp;quot;ice cream&amp;quot;), correlative conjunctions (e.g. &amp;quot;either ... or&amp;quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &amp;quot;break rules&amp;quot;, &amp;quot;strong tea&amp;quot;, &amp;quot;softly whisper&amp;quot;, &amp;quot;fully aware&amp;quot;, and &amp;quot;fond of&amp;quot;). Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004). These approaches use association measures to discover collocations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by </context>
</contexts>
<marker>Pearce, 2002</marker>
<rawString>Darren Pearce. 2002. A Comparative Evaluation of Collocation Extraction Techniques. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pp. 651-658.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Violeta Seretan</author>
<author>Eric Wehrli</author>
</authors>
<title>Accurate Collocation Extraction Using a Multilingual Parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-2006),</booktitle>
<pages>953--960</pages>
<contexts>
<context position="3259" citStr="Seretan and Wehrli, 2006" startWordPosition="479" endWordPosition="483">dow. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &amp;quot;span of collocation&amp;quot; means the distance of two words in a collocation. For example, if the span of the collocation (w1, w2) is 6, it means there are 5 words i</context>
</contexts>
<marker>Seretan, Wehrli, 2006</marker>
<rawString>Violeta Seretan and Eric Wehrli. 2006. Accurate Collocation Extraction Using a Multilingual Parser. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-2006), pp. 953-960</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving Collocations from Text: Xtract.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>143--177</pages>
<contexts>
<context position="2488" citStr="Smadja, 1993" startWordPosition="360" endWordPosition="361">e collocations in this paper include phrasal verbs (e.g. &amp;quot;put on&amp;quot;), proper nouns (e.g. &amp;quot;New York&amp;quot;), idioms (e.g. &amp;quot;dry run&amp;quot;), compound nouns (e.g. &amp;quot;ice cream&amp;quot;), correlative conjunctions (e.g. &amp;quot;either ... or&amp;quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &amp;quot;break rules&amp;quot;, &amp;quot;strong tea&amp;quot;, &amp;quot;softly whisper&amp;quot;, &amp;quot;fully aware&amp;quot;, and &amp;quot;fond of&amp;quot;). Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004). These approaches use association measures to discover collocations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems </context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Frank Smadja. 1993. Retrieving Collocations from Text: Xtract. Computational Linguistics, 19(1): 143-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>Collocation Extraction Based on Modifiability Statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING-2004),</booktitle>
<pages>980--986</pages>
<contexts>
<context position="3213" citStr="Wermter and Hahn, 2004" startWordPosition="472" endWordPosition="475">locations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted1. In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &amp;quot;span of collocation&amp;quot; means the distance of two words in a collocation. For example, if the span of the collocati</context>
</contexts>
<marker>Wermter, Hahn, 2004</marker>
<rawString>Joachim Wermter and Udo Hahn. 2004. Collocation Extraction Based on Modifiability Statistics. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-2004), pp. 980-986.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>