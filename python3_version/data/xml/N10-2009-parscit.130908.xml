<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010568">
<title confidence="0.989295">
Interpretation of Partial Utterances in Virtual Human Dialogue Systems
</title>
<author confidence="0.978926">
Kenji Sagae and David DeVault and David R. Traum
</author>
<affiliation confidence="0.996162">
Institute for Creative Technologies
University of Southern California
</affiliation>
<address confidence="0.61391">
Marina del Rey, CA 90292, USA
</address>
<email confidence="0.99964">
{sagae,devault,traum}@ict.usc.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999740166666667">
Dialogue systems typically follow a rigid pace
of interaction where the system waits until the
user has finished speaking before producing
a response. Interpreting user utterances be-
fore they are completed allows a system to
display more sophisticated conversational be-
havior, such as rapid turn-taking and appropri-
ate use of backchannels and interruptions. We
demonstrate a natural language understanding
approach for partial utterances, and its use in a
virtual human dialogue system that can often
complete a user’s utterances in real time.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983085714286">
In a typical spoken dialogue system pipeline, the
results of automatic speech recognition (ASR) for
each user utterance are sent to modules that per-
form natural language understanding (NLU) and di-
alogue management only after the utterance is com-
plete. This results in a rigid and often unnatural pac-
ing where the system must wait until the user stops
speaking before trying to understand and react to
user input. To achieve more flexible turn-taking with
human users, for whom turn-taking and feedback at
the sub-utterance level is natural, the system needs
the ability to start interpretation of user utterances
before they are completed.
We demonstrate an implementation of techniques
we have developed for partial utterance understand-
ing in virtual human dialogue systems (Sagae et al.,
2009; DeVault et al., 2009) with the goal of equip-
ping these systems with sophisticated conversational
behavior, such as interruptions and non-verbal feed-
back. Our demonstration highlights the understand-
ing of utterances before they are finished. It also
includes an utterance completion capability, where a
virtual human can make a strategic decision to dis-
play its understanding of an unfinished user utter-
ance by completing the utterance itself.
The work we demonstrate here is part of a grow-
ing research area in which new technical approaches
to incremental utterance processing are being de-
veloped (e.g. Schuler et al. (2009), Kruijff et al.
(2007)), new possible metrics for evaluating the per-
formance of incremental processing are being pro-
posed (e.g. Schlangen et al. (2009)), and the ad-
vantages for dialogue system performance and us-
ability are starting to be empirically quantified (e.g.
Skantze and Schlangen (2009), Aist et al. (2007)).
</bodyText>
<sectionHeader confidence="0.996641" genericHeader="method">
2 NLU for partial utterances
</sectionHeader>
<bodyText confidence="0.999954133333333">
In previous work (Sagae et al., 2009), we presented
an approach for prediction of semantic content from
partial speech recognition hypotheses, looking at
length of the speech hypothesis as a general indi-
cator of semantic accuracy in understanding. In
subsequent work (DeVault et al., 2009), we incor-
porated additional features of real-time incremen-
tal interpretation to develop a more nuanced predic-
tion model that can accurately identify moments of
maximal understanding within individual spoken ut-
terances. This research was conducted in the con-
text of the SASO-EN virtual human dialogue sys-
tem (Traum et al., 2008), using a corpus of approxi-
mately 4,500 utterances from user sessions. The cor-
pus includes a recording of each original utterance, a
</bodyText>
<page confidence="0.990943">
33
</page>
<note confidence="0.532469">
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 33–36,
</note>
<footnote confidence="0.189202">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</footnote>
<equation confidence="0.5481373">
�
� � � � � � � � �
11
mood: declarative
type : event
agent: captain − kirk
event : deliver
theme : power − generator
modal : [possibility: can �
speech − act : [ type : offer �
</equation>
<figureCaption confidence="0.997918">
Figure 1: AVM utterance representation.
</figureCaption>
<bodyText confidence="0.999424">
manual transcription, and a gold-standard semantic
frame, allowing us to develop and evaluate a data-
driven NLU approach.
</bodyText>
<subsectionHeader confidence="0.867552">
2.1 NLU in SASO-EN Virtual Humans
</subsectionHeader>
<bodyText confidence="0.999923545454545">
Our NLU module for the SASO-EN system,
mxNLU (Sagae et al., 2009), is based on maxi-
mum entropy classification (Berger et al., 1996) ,
where we treat entire individual semantic frames as
classes, and extract input features from ASR. The
NLU output representation is an attribute-value ma-
trix (AVM), where the attributes and values repre-
sent semantic information that is linked to a domain-
specific ontology and task model (Figure 1). The
AVMs are linearized, using a path-value notation, as
seen in the NLU input-output example below:
</bodyText>
<listItem confidence="0.9965354">
• Utterance (speech): we are prepared to give
you guys generators for electricity downtown
• ASR (NLU input): we up apparently give you
guys generators for a letter city don town
• Frame (NLU output):
</listItem>
<figure confidence="0.882580428571429">
&lt;s&gt;.mood declarative
&lt;s&gt;.sem.type event
&lt;s&gt;.sem.agent captain-kirk
&lt;s&gt;.sem.event deliver
&lt;s&gt;.sem.theme power-generator
&lt;s&gt;.sem.modal.possibility can
&lt;s&gt;.sem.speechact.type offer
</figure>
<bodyText confidence="0.982484296296296">
When mxNLU is trained on complete ASR out-
put for approximately 3,500 utterances, and tested
on a separate set of 350 complete ASR utterances,
the F-score of attribute-value pairs produced by the
NLU is 0.76 (0.78 precision and 0.74 recall). These
figures reflect the use of ASR at run-time, and most
errors are caused by incorrect speech recognition.
2.2 NLU with partial ASR results (Sagae et al.,
2009)
To interpret utterances before they are complete,
we use partial recognition hypotheses produced by
ASR every 200 milliseconds while the user is speak-
ing. To process these partial utterances produced by
ASR, we train length-specific models for mxNLU.
These models are trained using the partial ASR re-
sults we obtain by running ASR on the audio corre-
sponding to the utterances in the training data. The
NLU task is then to predict the meaning of the en-
tire utterance based only on a (noisy) prefix of the
utterance. On average, the accuracy of mxNLU on a
six-word prefix of an utterance (0.74 F-score) is al-
most as the same as the accuracy of mxNLU on en-
tire utterances. Approximately half of the utterances
in our corpus contain more than six words, creating
interesting opportunities for conversational behavior
that would be impossible under a model where each
utterance must be completed before it is interpreted.
</bodyText>
<subsectionHeader confidence="0.869053">
2.3 Detecting points of maximal
understanding (DeVault et al., 2009)
</subsectionHeader>
<bodyText confidence="0.9999505">
Although length-specific NLU models produce ac-
curate results on average, more effective use of the
interpretation provided by these models might be
achieved if we could automatically gauge their per-
formance on individual utterances at run-time. To
that end, we have developed an approach (DeVault et
al., 2009) that aims to detect those strategic points in
time, as specific utterances are occurring, when the
system reaches maximal understanding of the utter-
ance, in the sense that its interpretation will not sig-
nificantly improve during the rest of the utterance.
Figure 2 illustrates the incremental output of
mxNLU as a user asks, elder do you agree to move
the clinic downtown? Our ASR processes captured
audio in 200ms chunks. The figure shows the par-
tial ASR results after the ASR has processed each
200ms of audio, along with the F-score achieved by
mxNLU on each of these partials. Note that the NLU
F-score fluctuates somewhat as the ASR revises its
incremental hypotheses about the user utterance, but
generally increases over time.
For the purpose of initiating an overlapping re-
sponse to a user utterance such as this one, the agent
needs to be able (in the right circumstances) to make
</bodyText>
<equation confidence="0.943551">
�
� � � � � � �
sem :
</equation>
<page confidence="0.960523">
34
</page>
<figure confidence="0.997368769230769">
NLU F−score
Partial ASR result
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Utterance time (ms)
</figure>
<figureCaption confidence="0.999892">
Figure 2: Incremental interpretation of a user utterance.
</figureCaption>
<bodyText confidence="0.999980716981132">
an assessment that it has already understood the ut-
terance “well enough”, based on the partial ASR re-
sults that are currently available. We have imple-
mented a specific approach to this assessment which
views an utterance as understood “well enough” if
the agent would not understand the utterance any
better than it currently does even if it were to wait
for the user to finish their utterance (and for the ASR
to finish interpreting the complete utterance).
Concretely, Figure 2 shows that after the entire
2800ms utterance has been processed by the ASR,
mxNLU achieves an F-score of 0.91. However, in
fact, mxNLU already achieves this maximal F-score
at the moment it interprets the partial ASR result el-
der do you agree to move the at 1800ms. The agent
therefore could, in principle, initiate an overlapping
response at 1800ms without sacrificing any accuracy
in its understanding of the user’s utterance.
Of course the agent does not automatically realize
that it has achieved a maximal F-score at 1800ms.
To enable the agent to make this assessment, we
have trained a classifier, which we call MAXF, that
can be invoked for any specific partial ASR result,
and which uses various features of the ASR result
and the current mxNLU output to estimate whether
the NLU F-score for the current partial ASR result
is at least as high as the mxNLU F-score would be if
the agent were to wait for the entire utterance.
To facilitate training of a MAXF classifier, we
identified a range of potentially useful features that
the agent could use at run-time to assess its confi-
dence in mxNLU’s output for a given partial ASR
result. These features include: the number of par-
tial results that have been received from the ASR;
the length (in words) of the current partial ASR
result; the entropy in the probability distribution
mxNLU assigns to alternative output frames (lower
entropy corresponds to a more focused distribution);
the probability mxNLU assigns to the most probable
output frame; and the most probable output frame.
Based on these features, we trained a decision tree
to make the binary prediction that MAXF is TRUE
or FALSE for each partial ASR result. DeVault et al.
(2009) include a detailed evaluation and discussion
of the classifier. To briefly summarize our results,
the precision/recall/F-score of the trained MAXF
model are 0.88/0.52/0.65 respectively. The high pre-
cision means that 88% of the time that the model
predicts that F-score is maximized at a specific par-
tial, it really is. Our demonstration, which we out-
line in the next section, highlights the utility of a
high-precision MAXF classifier in making the deci-
sion whether to complete a user’s utterance.
</bodyText>
<sectionHeader confidence="0.973493" genericHeader="method">
3 Demo script outline
</sectionHeader>
<bodyText confidence="0.998718260869565">
We have implemented the approach for partial utter-
ance understanding described above in the SASO-
EN system (Traum et al., 2008), a virtual human
dialogue system with speech input and output (Fig-
ure 3), allowing us to demonstrate both partial utter-
ance understanding and some of the specific behav-
iors made possible by this capability. We divide this
demonstration in two parts: visualization of NLU
for partial utterances and user utterance completion.
(empty)
(empty)
all
elder
elder do you
elder to you d
elder do you agree
elder do you agree to
elder do you agree to move the
elder do you agree to move the
elder do you agree to move the clinic to
elder do you agree to move the clinic down
elder do you agree to move the clinic downtown
elder do you agree to move the clinic downtown
</bodyText>
<figure confidence="0.985124428571428">
200
400
600
800
1000
1200
1400
1600
1800
2000
2200
2400
2600
2800
</figure>
<page confidence="0.817929">
35
</page>
<figureCaption confidence="0.99656">
Figure 3: SASO-EN: Dr. Perez and Elder al-Hassan.
</figureCaption>
<table confidence="0.9933995">
Partial ASR result Predicted completion
we can provide transportation to move the patient there
the market is not safe
there are supplies where we are going
</table>
<tableCaption confidence="0.999861">
Table 1: Examples of user utterance completions.
</tableCaption>
<subsectionHeader confidence="0.998671">
3.1 Visualization of NLU for partial utterances
</subsectionHeader>
<bodyText confidence="0.9999925625">
Because the demonstration depends on usage of the
system within the domain for which it was designed,
the demo operator provides a brief description of the
system, task and domain. The demo operator (or
a volunteer user) then speaks normally to the sys-
tem, while a separate window visualizes the sys-
tem’s evolving understanding. This display is up-
dated every 200 milliseconds, allowing attendees to
see partial utterance understanding in action. For
ease of comprehension, the display will summarize
the NLU state using an English paraphrase of the
predicted meaning (rather than displaying the struc-
tured frame that is the actual output of NLU). The
display will also visualize the TRUE or FALSE state
of the MAXF classifier, highlighting the moment the
system thinks it reaches maximal understanding.
</bodyText>
<subsectionHeader confidence="0.999167">
3.2 User utterance completion
</subsectionHeader>
<bodyText confidence="0.999763333333333">
The demo operator (or volunteer user) starts to speak
and pauses briefly in mid-utterance, at which point,
if possible, one of the virtual humans jumps in and
completes the utterance (DeVault et al., 2009). Ta-
ble 1 includes a few examples of the many utterances
that can be completed by the virtual humans.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999953">
Interpretation of partial utterances, combined with
a way to predict points of maximal understanding,
opens exciting possibilities for more natural conver-
sational behavior in virtual humans. This demon-
stration showcases the NLU approach and a sample
application of the basic techniques.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999703833333333">
The work described here has been sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position
or the policy of the United States Government, and
no official endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999443696969697">
G. Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness,
M. Swift, and M. K. Tanenhaus. 2007. Incremental
dialogue system faster than and preferred to its non-
incremental counterpart. In Proc. of the 29th Annual
Conference of the Cognitive Science Society.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71.
D. DeVault, K. Sagae, and D. Traum. 2009. Can I finish?
Learning when to respond to incremental interpreta-
tion results in interactive dialogue. In Proc. SIGDIAL.
G. J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and
N. Hawes. 2007. Incremental, multi-level processing
for comprehending situated dialogue in human-robot
interaction. In Proc. LangRo’2007.
K. Sagae, G. Christian, D. DeVault, and D. R. Traum.
2009. Towards natural language understanding of par-
tial speech recognition results in dialogue systems. In
Short Paper Proceedings of NAACL HLT.
D. Schlangen, T. Baumann, and M. Atterer. 2009. In-
cremental reference resolution: The task, metrics for
evaluation, and a Bayesian filtering model that is sen-
sitive to disfluencies. In Proc. SIGDIAL, page 30–37.
W. Schuler, S. Wu, and L. Schwartz. 2009. A frame-
work for fast incremental interpretation during speech
decoding. Computational Linguistics, 35(3):313–343.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proc. EACL.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negoti-
ation for multi-modal virtual agents. In Proc. of the
Eighth International Conference on Intelligent Virtual
Agents.
</reference>
<page confidence="0.998925">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986399">
<title confidence="0.998451">Interpretation of Partial Utterances in Virtual Human Dialogue Systems</title>
<author confidence="0.996286">Sagae DeVault R Traum</author>
<affiliation confidence="0.99943">Institute for Creative Technologies University of Southern California</affiliation>
<address confidence="0.999246">Marina del Rey, CA 90292, USA</address>
<abstract confidence="0.999429230769231">Dialogue systems typically follow a rigid pace of interaction where the system waits until the user has finished speaking before producing a response. Interpreting user utterances before they are completed allows a system to display more sophisticated conversational behavior, such as rapid turn-taking and appropriate use of backchannels and interruptions. We demonstrate a natural language understanding approach for partial utterances, and its use in a virtual human dialogue system that can often complete a user’s utterances in real time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>J Allen</author>
<author>E Campana</author>
<author>C G Gallo</author>
<author>S Stoness</author>
<author>M Swift</author>
<author>M K Tanenhaus</author>
</authors>
<title>Incremental dialogue system faster than and preferred to its nonincremental counterpart.</title>
<date>2007</date>
<booktitle>In Proc. of the 29th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2558" citStr="Aist et al. (2007)" startWordPosition="388" endWordPosition="391">ke a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In subsequent work (DeVault et al., 2009), we incorporated additional features of real-time incremental interpretation to develop a more nuanced prediction model that can accurately identify moments of maximal understanding within individual spoken utterances. This research was conducted in the context of the SASO-EN v</context>
</contexts>
<marker>Aist, Allen, Campana, Gallo, Stoness, Swift, Tanenhaus, 2007</marker>
<rawString>G. Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness, M. Swift, and M. K. Tanenhaus. 2007. Incremental dialogue system faster than and preferred to its nonincremental counterpart. In Proc. of the 29th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="4003" citStr="Berger et al., 1996" startWordPosition="626" endWordPosition="629">stration Session, pages 33–36, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics � � � � � � � � � � 11 mood: declarative type : event agent: captain − kirk event : deliver theme : power − generator modal : [possibility: can � speech − act : [ type : offer � Figure 1: AVM utterance representation. manual transcription, and a gold-standard semantic frame, allowing us to develop and evaluate a datadriven NLU approach. 2.1 NLU in SASO-EN Virtual Humans Our NLU module for the SASO-EN system, mxNLU (Sagae et al., 2009), is based on maximum entropy classification (Berger et al., 1996) , where we treat entire individual semantic frames as classes, and extract input features from ASR. The NLU output representation is an attribute-value matrix (AVM), where the attributes and values represent semantic information that is linked to a domainspecific ontology and task model (Figure 1). The AVMs are linearized, using a path-value notation, as seen in the NLU input-output example below: • Utterance (speech): we are prepared to give you guys generators for electricity downtown • ASR (NLU input): we up apparently give you guys generators for a letter city don town • Frame (NLU output</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D DeVault</author>
<author>K Sagae</author>
<author>D Traum</author>
</authors>
<title>Can I finish? Learning when to respond to incremental interpretation results in interactive dialogue.</title>
<date>2009</date>
<booktitle>In Proc. SIGDIAL.</booktitle>
<contexts>
<context position="1639" citStr="DeVault et al., 2009" startWordPosition="244" endWordPosition="247">alogue management only after the utterance is complete. This results in a rigid and often unnatural pacing where the system must wait until the user stops speaking before trying to understand and react to user input. To achieve more flexible turn-taking with human users, for whom turn-taking and feedback at the sub-utterance level is natural, the system needs the ability to start interpretation of user utterances before they are completed. We demonstrate an implementation of techniques we have developed for partial utterance understanding in virtual human dialogue systems (Sagae et al., 2009; DeVault et al., 2009) with the goal of equipping these systems with sophisticated conversational behavior, such as interruptions and non-verbal feedback. Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (20</context>
<context position="2879" citStr="DeVault et al., 2009" startWordPosition="438" endWordPosition="441"> (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In subsequent work (DeVault et al., 2009), we incorporated additional features of real-time incremental interpretation to develop a more nuanced prediction model that can accurately identify moments of maximal understanding within individual spoken utterances. This research was conducted in the context of the SASO-EN virtual human dialogue system (Traum et al., 2008), using a corpus of approximately 4,500 utterances from user sessions. The corpus includes a recording of each original utterance, a 33 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 33–36, Los Angeles, California, June 2010. c�2010 Association for Comput</context>
<context position="6174" citStr="DeVault et al., 2009" startWordPosition="971" endWordPosition="974"> utterances in the training data. The NLU task is then to predict the meaning of the entire utterance based only on a (noisy) prefix of the utterance. On average, the accuracy of mxNLU on a six-word prefix of an utterance (0.74 F-score) is almost as the same as the accuracy of mxNLU on entire utterances. Approximately half of the utterances in our corpus contain more than six words, creating interesting opportunities for conversational behavior that would be impossible under a model where each utterance must be completed before it is interpreted. 2.3 Detecting points of maximal understanding (DeVault et al., 2009) Although length-specific NLU models produce accurate results on average, more effective use of the interpretation provided by these models might be achieved if we could automatically gauge their performance on individual utterances at run-time. To that end, we have developed an approach (DeVault et al., 2009) that aims to detect those strategic points in time, as specific utterances are occurring, when the system reaches maximal understanding of the utterance, in the sense that its interpretation will not significantly improve during the rest of the utterance. Figure 2 illustrates the increme</context>
<context position="9730" citStr="DeVault et al. (2009)" startWordPosition="1576" endWordPosition="1579">its confidence in mxNLU’s output for a given partial ASR result. These features include: the number of partial results that have been received from the ASR; the length (in words) of the current partial ASR result; the entropy in the probability distribution mxNLU assigns to alternative output frames (lower entropy corresponds to a more focused distribution); the probability mxNLU assigns to the most probable output frame; and the most probable output frame. Based on these features, we trained a decision tree to make the binary prediction that MAXF is TRUE or FALSE for each partial ASR result. DeVault et al. (2009) include a detailed evaluation and discussion of the classifier. To briefly summarize our results, the precision/recall/F-score of the trained MAXF model are 0.88/0.52/0.65 respectively. The high precision means that 88% of the time that the model predicts that F-score is maximized at a specific partial, it really is. Our demonstration, which we outline in the next section, highlights the utility of a high-precision MAXF classifier in making the decision whether to complete a user’s utterance. 3 Demo script outline We have implemented the approach for partial utterance understanding described </context>
<context position="12449" citStr="DeVault et al., 2009" startWordPosition="2030" endWordPosition="2033">nce understanding in action. For ease of comprehension, the display will summarize the NLU state using an English paraphrase of the predicted meaning (rather than displaying the structured frame that is the actual output of NLU). The display will also visualize the TRUE or FALSE state of the MAXF classifier, highlighting the moment the system thinks it reaches maximal understanding. 3.2 User utterance completion The demo operator (or volunteer user) starts to speak and pauses briefly in mid-utterance, at which point, if possible, one of the virtual humans jumps in and completes the utterance (DeVault et al., 2009). Table 1 includes a few examples of the many utterances that can be completed by the virtual humans. 4 Conclusion Interpretation of partial utterances, combined with a way to predict points of maximal understanding, opens exciting possibilities for more natural conversational behavior in virtual humans. This demonstration showcases the NLU approach and a sample application of the basic techniques. Acknowledgments The work described here has been sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM). Statements and opinions expressed do not necessarily reflect the </context>
</contexts>
<marker>DeVault, Sagae, Traum, 2009</marker>
<rawString>D. DeVault, K. Sagae, and D. Traum. 2009. Can I finish? Learning when to respond to incremental interpretation results in interactive dialogue. In Proc. SIGDIAL. G. J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and N. Hawes. 2007. Incremental, multi-level processing for comprehending situated dialogue in human-robot interaction. In Proc. LangRo’2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>G Christian</author>
<author>D DeVault</author>
<author>D R Traum</author>
</authors>
<title>Towards natural language understanding of partial speech recognition results in dialogue systems.</title>
<date>2009</date>
<booktitle>In Short Paper Proceedings of NAACL HLT.</booktitle>
<contexts>
<context position="1616" citStr="Sagae et al., 2009" startWordPosition="240" endWordPosition="243">tanding (NLU) and dialogue management only after the utterance is complete. This results in a rigid and often unnatural pacing where the system must wait until the user stops speaking before trying to understand and react to user input. To achieve more flexible turn-taking with human users, for whom turn-taking and feedback at the sub-utterance level is natural, the system needs the ability to start interpretation of user utterances before they are completed. We demonstrate an implementation of techniques we have developed for partial utterance understanding in virtual human dialogue systems (Sagae et al., 2009; DeVault et al., 2009) with the goal of equipping these systems with sophisticated conversational behavior, such as interruptions and non-verbal feedback. Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (</context>
<context position="3937" citStr="Sagae et al., 2009" startWordPosition="615" endWordPosition="618">original utterance, a 33 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 33–36, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics � � � � � � � � � � 11 mood: declarative type : event agent: captain − kirk event : deliver theme : power − generator modal : [possibility: can � speech − act : [ type : offer � Figure 1: AVM utterance representation. manual transcription, and a gold-standard semantic frame, allowing us to develop and evaluate a datadriven NLU approach. 2.1 NLU in SASO-EN Virtual Humans Our NLU module for the SASO-EN system, mxNLU (Sagae et al., 2009), is based on maximum entropy classification (Berger et al., 1996) , where we treat entire individual semantic frames as classes, and extract input features from ASR. The NLU output representation is an attribute-value matrix (AVM), where the attributes and values represent semantic information that is linked to a domainspecific ontology and task model (Figure 1). The AVMs are linearized, using a path-value notation, as seen in the NLU input-output example below: • Utterance (speech): we are prepared to give you guys generators for electricity downtown • ASR (NLU input): we up apparently give </context>
<context position="5188" citStr="Sagae et al., 2009" startWordPosition="807" endWordPosition="810"> city don town • Frame (NLU output): &lt;s&gt;.mood declarative &lt;s&gt;.sem.type event &lt;s&gt;.sem.agent captain-kirk &lt;s&gt;.sem.event deliver &lt;s&gt;.sem.theme power-generator &lt;s&gt;.sem.modal.possibility can &lt;s&gt;.sem.speechact.type offer When mxNLU is trained on complete ASR output for approximately 3,500 utterances, and tested on a separate set of 350 complete ASR utterances, the F-score of attribute-value pairs produced by the NLU is 0.76 (0.78 precision and 0.74 recall). These figures reflect the use of ASR at run-time, and most errors are caused by incorrect speech recognition. 2.2 NLU with partial ASR results (Sagae et al., 2009) To interpret utterances before they are complete, we use partial recognition hypotheses produced by ASR every 200 milliseconds while the user is speaking. To process these partial utterances produced by ASR, we train length-specific models for mxNLU. These models are trained using the partial ASR results we obtain by running ASR on the audio corresponding to the utterances in the training data. The NLU task is then to predict the meaning of the entire utterance based only on a (noisy) prefix of the utterance. On average, the accuracy of mxNLU on a six-word prefix of an utterance (0.74 F-score</context>
</contexts>
<marker>Sagae, Christian, DeVault, Traum, 2009</marker>
<rawString>K. Sagae, G. Christian, D. DeVault, and D. R. Traum. 2009. Towards natural language understanding of partial speech recognition results in dialogue systems. In Short Paper Proceedings of NAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schlangen</author>
<author>T Baumann</author>
<author>M Atterer</author>
</authors>
<title>Incremental reference resolution: The task, metrics for evaluation, and a Bayesian filtering model that is sensitive to disfluencies.</title>
<date>2009</date>
<booktitle>In Proc. SIGDIAL,</booktitle>
<pages>30--37</pages>
<contexts>
<context position="2394" citStr="Schlangen et al. (2009)" startWordPosition="362" endWordPosition="365"> Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In subsequent work (DeVault et al., 2009), we incorporated additional features of real-time incremental interpretation to develop a more nuanced prediction </context>
</contexts>
<marker>Schlangen, Baumann, Atterer, 2009</marker>
<rawString>D. Schlangen, T. Baumann, and M. Atterer. 2009. Incremental reference resolution: The task, metrics for evaluation, and a Bayesian filtering model that is sensitive to disfluencies. In Proc. SIGDIAL, page 30–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Schuler</author>
<author>S Wu</author>
<author>L Schwartz</author>
</authors>
<title>A framework for fast incremental interpretation during speech decoding.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="2242" citStr="Schuler et al. (2009)" startWordPosition="338" endWordPosition="341">ault et al., 2009) with the goal of equipping these systems with sophisticated conversational behavior, such as interruptions and non-verbal feedback. Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In s</context>
</contexts>
<marker>Schuler, Wu, Schwartz, 2009</marker>
<rawString>W. Schuler, S. Wu, and L. Schwartz. 2009. A framework for fast incremental interpretation during speech decoding. Computational Linguistics, 35(3):313–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
<author>D Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proc. EACL.</booktitle>
<contexts>
<context position="2538" citStr="Skantze and Schlangen (2009)" startWordPosition="384" endWordPosition="387">, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In subsequent work (DeVault et al., 2009), we incorporated additional features of real-time incremental interpretation to develop a more nuanced prediction model that can accurately identify moments of maximal understanding within individual spoken utterances. This research was conducted in the cont</context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>G. Skantze and D. Schlangen. 2009. Incremental dialogue processing in a micro-domain. In Proc. EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
<author>S Marsella</author>
<author>J Gratch</author>
<author>J Lee</author>
<author>A Hartholt</author>
</authors>
<title>Multi-party, multi-issue, multi-strategy negotiation for multi-modal virtual agents.</title>
<date>2008</date>
<booktitle>In Proc. of the Eighth International Conference on Intelligent Virtual Agents.</booktitle>
<contexts>
<context position="3207" citStr="Traum et al., 2008" startWordPosition="489" endWordPosition="492">s In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In subsequent work (DeVault et al., 2009), we incorporated additional features of real-time incremental interpretation to develop a more nuanced prediction model that can accurately identify moments of maximal understanding within individual spoken utterances. This research was conducted in the context of the SASO-EN virtual human dialogue system (Traum et al., 2008), using a corpus of approximately 4,500 utterances from user sessions. The corpus includes a recording of each original utterance, a 33 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 33–36, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics � � � � � � � � � � 11 mood: declarative type : event agent: captain − kirk event : deliver theme : power − generator modal : [possibility: can � speech − act : [ type : offer � Figure 1: AVM utterance representation. manual transcription, and a gold-standard semantic frame, allowing us to develop and evalu</context>
<context position="10377" citStr="Traum et al., 2008" startWordPosition="1680" endWordPosition="1683">on and discussion of the classifier. To briefly summarize our results, the precision/recall/F-score of the trained MAXF model are 0.88/0.52/0.65 respectively. The high precision means that 88% of the time that the model predicts that F-score is maximized at a specific partial, it really is. Our demonstration, which we outline in the next section, highlights the utility of a high-precision MAXF classifier in making the decision whether to complete a user’s utterance. 3 Demo script outline We have implemented the approach for partial utterance understanding described above in the SASOEN system (Traum et al., 2008), a virtual human dialogue system with speech input and output (Figure 3), allowing us to demonstrate both partial utterance understanding and some of the specific behaviors made possible by this capability. We divide this demonstration in two parts: visualization of NLU for partial utterances and user utterance completion. (empty) (empty) all elder elder do you elder to you d elder do you agree elder do you agree to elder do you agree to move the elder do you agree to move the elder do you agree to move the clinic to elder do you agree to move the clinic down elder do you agree to move the cl</context>
</contexts>
<marker>Traum, Marsella, Gratch, Lee, Hartholt, 2008</marker>
<rawString>D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt. 2008. Multi-party, multi-issue, multi-strategy negotiation for multi-modal virtual agents. In Proc. of the Eighth International Conference on Intelligent Virtual Agents.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>