<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005011">
<author confidence="0.554927">
FALLIBLE RATIONALISM AND MACHINE TRANSLATION
Geoffrey Sampson
</author>
<affiliation confidence="0.9883235">
Department of Linguistics &amp; Modern English Language
University of Lancaster
</affiliation>
<address confidence="0.785358">
LANCASTER LA1-4YT, G.B.
</address>
<email confidence="0.700921">
ABSTRACT
</email>
<bodyText confidence="0.999250584158416">
Approaches to MT have been heavily influenced
by changing trends in the philosophy of language
and mind. Because of the artificial hiatus which
followed the publication of the ALFAC Report, MT
research in the 1970s and early 1980s has had to
catch up with major developments that have occurred
in linguistic and philosophical thinking; current-
ly, MT seems to be uncritically loyal to a para-
digm of thought about language which is rapidly
losing most of its adherents in departments of
linguistics and philosophy. I argue, both in
theoretical terms and by reference to empirical
research on a particular translation problem, that
the Popperian &amp;quot;fallible rationalist&amp;quot; view of mental
processes which is winning acceptance as a more
sophisticated alternative to Chomskyan &amp;quot;determin-
istic rationalism&amp;quot; should lead MT researchers to
redefine their goals and to adopt certain current-
ly-neglected techniques in trying to achieve those
goals.
1 Since the Second World War, three rival views
of the nature of the human mind have competed for
the allegiance of philosophically-minded people.
Each of these views has implications for our
understanding of language.
The 1950s and early 1960s were dominated by a
behaviourist approach tracing its ancestry to John
Locke and represented recently e.g. by Leonard
Bloomfield and B.F. Skinner. On this view, &amp;quot;mind&amp;quot;
is merely a name for a set of associations that
have been established during a person&apos;s life
between external stimuli and behavioural responses.
The meaning of a sentence is to be understood not
as the effect it has on an unobservable internal
model of reality but as the behaviour it evokes in
the hearer.
During the 1960s this view lost ground to the
rationalist ideas of Noam Chomsky, working in an
intellectual tradition founded by Plato and rein-
augurated in modern times by Rene Descartes. On
this view, stimuli and responses are linked only
indirectly, via an immensely complex cognitive
mechanism having its own fixed principles of oper-
ation which are independent of experience. A
given behaviour is a response to an internal mental
event which is determined as the resultant of the
initial state of the mental apparatus together with
the entire history of inputs to it. The meaning of
a sentence must be explained in terms of the unseen
responses it evokes in the cognitive apparatus,
which might take the form of successive modific-
ations of an internal model of reality that could
be described as &amp;quot;inferencing&amp;quot;.
Chomskyan rationalism is undoubtedly more
satisfactory as an account of human cognition than
Skinnerian behaviourism. By the late 1970s, how-
ever, the mechanical determinism that is part of
Chomsky&apos;s view of mind appeared increasingly unre-
alistic to many writers. There is little empirical
support, for instance, for the Chomskyan assumpt-
ions that the child&apos;s acquisition of his first
language, or the adult&apos;s comprehension of a given
utterance, are processes that reach well-defined
terminations after a given period of mental pro-
cessing -- language seems typically to work in a
more &amp;quot;open-ended&amp;quot; fashion than that. Within
linguistics, as documented e.g. by Moore &amp; Carling
(1982), the Chomskyan paradigm is by now widely
rejected.
The view which is winning widespread accept-
ance as preserving the merits of rationalism
while avoiding its inadequacies is Karl Popper&apos;s
fallibilist version of the doctrine. On this
account, the mind responds to experiential inputs
not by a deterministic algorithm that reaches a
halt state, but by creatively formulating fallible
conjectures which experience is used to test.
Typically the conjectures formulated are radically
novel, in the sense that they could not be pre-
dicted even on the basis of ideally complete
knowledge of the person&apos;s prior state. This
version of rationalism is incompatible with the
materialist doctrine that the mind is nothing but
an arrangement of matter and wholly governed by
the laws of physics; but, historically, material-
ism has not commonly been regarded as an axiom
requiring no argument to support it (although it
may be that the ethos of Artificial Intelligence
makes practitioners of this discipline more than
averagely favourable towards materialism).
As a matter of logic, fallible conjectures in
any domain can be eliminated by adverse experience
but can never be decisively confirmed. Our
reaction to linguistic experience, consequently;
is for a Popperian both non-deterministic and
open-ended. There is no reason to expect a person
at any age to cease to improve his knowledge of
his mother-tongue, or to expect different members
of a speech-community to formulate identical
internalized grammars; and understanding an indiv-
idual utterance is a process which a person can
</bodyText>
<page confidence="0.994177">
86
</page>
<bodyText confidence="0.999734363636364">
execute to any desired degree of thoroughness --
we stop trying to improve our understanding of a
particular sample of language not because we reach
a natural stopping-place but because we judge that
the returns from further effort are likely to be
less than the resources invested.
For a Chomskyan linguist, divergences between
individuals in their, linguistic behaviour are to be
explained either in terms of mixture of &amp;quot;dialects&amp;quot;
or in terms of failure of practical &amp;quot;performance&amp;quot;
fully to match the abstract &amp;quot;competence&amp;quot; possessed
by the mature speaker. For the Popperian such
divergences require no explanation; we do not
possess algorithms which would lead to correct
results if they were executed thoroughly. Indeed,
since languages have no reality independent of
their speakers, the idea that there exists a
&amp;quot;correct&amp;quot; solution to the problem of acquiring a
language or of understanding an individual sent-
ence ceases to apply except as an untheoretical
approximation. The superiority of the Popperian
to the Chomskyan paradigm as a framework for
interpreting the facts of linguistic behaviour is
argued e.g. in my Making Sense (1980), Popperian
Linguistics (in press).
2 There is a major difference in style between
the MT of the 19505 and 1960s, and the projects of
the last decade. This reflects the difference
between behaviourist and deterministic-rationalist
paradigms. Speaking very broadly, early MT
research envisaged the problem of translation as
that of establishing equivalences between observ-
able, surface features of languages: vocabulary
items, taxemes of order, and the like. Recent MT
research has taken it as axiomatic that successful
MT must incorporate a large AI component. Human
translation, it is now realized, involves the
understanding of source texts rather than mere
transliteration from one set of linguistic con-
ventions to another: we make heavy use of infer-
encing in order to resolve textual ambiguities.
MT systems must therefore simulate these inferenc-
ing processes in order to produce human-like out-
put. Furthermore, the Chomskyan paradigm incorp-
orates axioms about the kinds of operation char-
acteristic of human linguistic processing, and MT
research inherits these. In particular, Chomsky
and his followers have been hostile to the idea
that any interesting linguistic rules or processes
might be probabilistic or statistical in nature
(e.g. Chomsky 1957: 15-17, and of. the controversy
about Labovian &amp;quot;variable rules&amp;quot;). The assumption
that human language-processing is invariably an
all-or-none phenomenon might well be questioned
even by someone who subscribed to the other tenets
of the Chomskyan paradigm (e.g. Suppes 1970), but
it is consistent with the heavily deterministic
flavour of that paradigm. Correspondingly, recent
MT projects known to me seem to make no use of
probabilities, and anecdotal evidence suggests
that MT (and other AI) researchers perceive pro-
posals for the exploitation of probabilistic tech-
niques as defeatist (&amp;quot;We ought to be modelling
what the mind actually does rather than using
purely artificial methods to achieve a rough
approximation to its output&amp;quot;).
</bodyText>
<listItem confidence="0.53429">
3. What are the implications for MT, and for Al
</listItem>
<bodyText confidence="0.99851696875">
in general, of a shift from a deterministic&apos;to a
fallibilist version of rationalism? (On the
general issue see e.g. the exchange between
Aravind Joshi and me in Smith 1982.) They can be
summed up as follows.
First, there is no such thing as an ideal
speaker&apos;s competence which, if simulated mechanic-
ally, would constitute perfect MT. In the case of
&amp;quot;literary&amp;quot; texts it is generally recognised that
different human translators may produce markedly
different translations none of which can be con-
sidered more &amp;quot;correct&amp;quot; than the others; from the
Popperian viewpoint literary texts do not differ
qualitatively from other genres. (Referring to
the translation requirements of the Secretariat of
the Council of the European Communities, P.J.
Arthern (1979: 81) has said that &amp;quot;the only quality
we can accept is 100% fidelity to the meaning of
the original&amp;quot;. From the fallibilist point of view
that is like saying &amp;quot;the only kind of motors we
are willing to use are perpetual-motion machines&amp;quot;.)
Second, there is no possibility of designing
an artificial system which simulates the actions
of an unpredictably creative mind, since any
machine is a material object governed by physical
law. Thus it will not, for instance, be possible
to design an artificial system which regularly
uses inferencing to resolve the meaning of given
texts in the same way as a human reader of the
texts. There is no principled barrier, of course,
to an artificial system which applies logical
transformations to derive conclusions from given
premisses. But an artificial system must be
restricted to some fixed, perhaps very large, data-
base of premisses (&amp;quot;world knowledge&amp;quot;). It is
central to the Popperian view of mind that human
inferencing is not limited to a fixed set of pre-
misses but involves the frequent invention of new
hypotheses which are not related in any logical
way to the previous contents of mind. An MT
system cannot aspire to perfect human performance.
(But then, neither can a human.)
Third: a situation in which the behaviour of
any individual is only approximately similar to
that of other individuals and is not in detail
predictable even in principle is just the kind of
situation in which probabilistic techniques are
valuable, irrespective of whether or not the pro-
cesses occurring within individual humans are
themselves intrinsically probabilistic. To draw
an analogy: life-insurance companies do not con-
demn the actuarial profession as a bunch of cop-
outs because they do not attempt to predict the
precise date of death of individual policyholders.
MT research ought to exploit any techniques that
offer the possibility of better approximations to
acceptable translation, whether or not it seems
likely that human translation exploits such tech-
niques; and it is likely that useful methods will
often be probabilistic.
Fourth: MT researchers will ultimately need
to appreciate that there is no natural end to the
process of improving the quality of translation
(though it may be premature to raise this issue
</bodyText>
<page confidence="0.995001">
87
</page>
<bodyText confidence="0.999459450704226">
at a stage when the best mechanical translation is
still quite bad). Human translation always invol-
ves a (usually tacit) cost-benefit analysis; it
is never a question of &amp;quot;How much work is needed to
translate this text &apos;properly&apos;?&amp;quot; but of &amp;quot;Will a
given increment of effort be profitable in terms
of achieved improvement in translation?&amp;quot; Likewise,
the question confronting MT is not &amp;quot;Is MT poss-
ible?&amp;quot; but &amp;quot;What are the disbenefits of translat-
ing this or that category of texts at this or
that level of inexactness, and how do the costs
of reducing the incidence of a given type of
error compare with the gains to the consumers?&amp;quot;
4 The value of probabilistic techniques is
sufficiently exemplified by the spectacular succ-
ess of the Lancaster-Oslo-Bergen Tagging System
(see e.g. Leech et al. 1983). The LOB Tagging
System, operational since 1981, assigns grammat-
ical tags drawn from a highly-differentiated (134-
member) tag-set to the words of &amp;quot;real-life&amp;quot;
English text. The system &amp;quot;knows&amp;quot; virtually nothing
of the syntax of English in terms of the kind of
grammar-rules believed by linguists to make up the
speaker&apos;s competence; it uses only facts about
local transition-probabilities between form-
classes, together with the relatively meagre clues
provided by English morphology. By late 1982 the
output of the system fell short of complete
success (defined as tagging identical to that done
independently by a human linguist) by only 3.4%.
Various methods are being used to reduce this
failure-rate further, but the nature of the tech-
niques used ensures that the ideal of 100% success
will be approached only asymptotically. However,
the point is that no other extant automatic tagg-
ing-system known to me approaches the current
success-level of the LOB system. I predict that
any system which eschews probabilistic methods
will perform at a significantly lower level.
5 In the remainder of this paper I illustrate
the argument that human language-comprehension
involves inferencing from unpredictable hypothes-
es, using research of my own on the problem of
&amp;quot;referring&amp;quot; pronouns.
approach can never attain 100% success, and that
it does not correspond to the method by which
humans resolve pronouns.
However, unlike Hobbs&apos;s syntactic algorithm,
his semantic algorithm is purely programmatic.
The implication that it will be able to achieve
100% success -- or even that it will be able to
match the success-level of the existing syntactic
algorithm -- rests purely on faith, though this
faith is quite understandable given the axioms of
deterministic rationalism.
I investigated these issues by examining a
set of examples of the pronoun it drawn from the
LOB Corpus (a standard million-word computer-read-
able corpus of modern written British English --
see Johansson 1978). The pronoun it is specially
interesting in connexion with MT because of the
problems of translation into gender-languages; my
examples were extracted from the texts in Category
H of the LOB Corpus, which includes governmental
and similar documents and thus matches the genres
which current large-scale MT projects such as
EUROTRA aim to translate. I began with 338
instances of it; after eliminating non-referential
cases I was left with 156 instances which I exam-
ined intensively.
I asked the following questions:
</bodyText>
<listItem confidence="0.91047775">
(1) In what proportion of cases do I as an educ-
ated native speaker feel confident about the
intended reference?
(2) Where I do feel confident and Hobbs&apos;s syn-
tactic algorithm gives a result which I believe to
be wrong, what kind of reasoning enabled me to
reach my solution?
(2) Where Hobbs&apos;s algorithm gives what I believe
</listItem>
<bodyText confidence="0.95794788372093">
to be the correct result, is it plausible that a
semantic algorithm would give the same result?
(4) Could the performance of Hobbs&apos;s syntactic
algorithm be improved, as an alternative to
replacing it by a semantic algorithm?
It emerged that:
My research was done in reaction to an
article by Jerry Hobbs (1976). Hobbs provides an
unusually clear example of the Chomskyan paradigm
of Al research, since he makes his methodological
axioms relatively explicit. He begins by defining
a complex and subtle algorithm for referring pro-
nouns which depends exclusively on the grammatical
structure of the sentences in which they occur.
This algorithm is highly successful: tested on a
sample of texts, it is 88.3% accurate (a figure
which rises slightly, to 91.7%, when the algorithm
is expanded to use the simple kind of semantic
information represented by Katz/Fodor &amp;quot;selection
restrictions&amp;quot;). Nevertheless, Hobbs argues that
this approach to the problem of pronoun resolution
must be abandoned in favour of a &amp;quot;semantic algo-
rithm&amp;quot;, meaning one which depends on inferencing
from a data-base of world knowledge rather than on
syntactic structure. He gives several reasons;
the important reasons are that the syntactic
(1) In about 10% of all cases, human resolution
was impossible; on careful consideration of the
alternatives I concluded that I did not know the
intended reference (even though, on a first
relatively cursory reading, most of these cases
had not struck me as ambiguous). An example is:
The lower platen, which supports the leather,
is raised hydraulically to bring it into contact
with the rollers on the upper platen (H6.148)
Does it refer to the lower platen or to the &apos;
leather (la platina, il cuoio!)? I really don&apos;t
know. In at least one instance (not this one) I
reached different confident conclusions about the
same case on different occasions (and this sugg-
ests that there are likely to be other cases
which I have confidently resolved in ways other
than the writer intended). The implication is
</bodyText>
<page confidence="0.997389">
88
</page>
<bodyText confidence="0.965261606382979">
that a system which performs at a level of success
much above 90% on the task of resolving referent-
ial it would be outperforming a human, which is
contradictory: language means what humans take it
to mean.
(2) In a number of cases where I judged the syn-
tactic algorithm to give the wrong result, the
premisses on which my own decisions were based
were propositions that were not pieces of factual
general knowledge and which I was not aware of
ever having consciously entertained before pro-
ducing them in the course of trying to interpret
the text in question. It would therefore be
quixotic to suggest that these propositions
would occur in the data-base available to a future
MT system. Consider, for instance:
Under the &amp;quot;permissive&amp;quot; powers, however, in
the worst cases when the Ministry was right and
the M.P. was right the local authority could still
dig its heels in and say that whatever the Mini-
stry said it was not going to give a grant. (H16.
24)
I feel sure that it refers to the local authority
rather than the Ministry, chiefly because it seems
to me much more plausible that a lower-level
branch of government would refuse to heed requests
for action from a higher-level branch than that it
would accuse the higher-level branch of deceit.
But this generalization about the sociology of
government was new to me when I thought it up for
the purpose of interpreting the example quoted
(and I am not certain that it is in fact univers-
ally true).
(3) In a number of cases it was very difficult to
believe that introduction of semantic consider-
ations into the syntactic algorithm would not
worsen its performance. Here, an example is:
... and the Isle of Man. We do by these
Presents for Us, our Heirs and Successors instit-
ute and create a new Medal and We do hereby direct
that it shall be governed by the following rules
and ordinances ... (H24.16)
Hobbs&apos;s syntactic algorithm refers it to Medal,
I believe rightly. Yet before reading the text
I was under the impression that medals, like other
small concrete inanimate objects, could not be
governed; while territories like the Isle of Man
can be, and indeed are. Syntax is more important
than semantics in this case.
(4) There are several syntactic phenomena (e.g.
parallelism of structure between successive
clauses) which turned out to be relevant to pro-
noun resolution but which are ignored by Hobbs&apos;s
algorithm. I have not undertaken the task of mod-
ifying the syntactic. algorithm in order to exploit
these phenomena, but it seems likely that the
already-good performance of the algorithm could be
further improved.
It is also worth pointing out that accepting
the legitimacy of probabilistic methods allows one
to exploit many crude (and therefore cheaply-
exploited) semantic considerations, such as Katz/
Fodor selection restrictions, which have to be
left out of a deterministic system because in
practice they are sometimes violated. As we have
seen, Hobbs suggested that only a small percentage
improvement in the performance of his pure syntac-
tic algorithm could be achieved by adding semantic
selection restrictions. Rules such as &amp;quot;the verb
&apos;fear&apos; must have an [-f-animate] subject&amp;quot; almost
never prove to be exceptionless in real-life usage:
even genres of text that appear soberly literal
contain many cases of figurative or extended usage.
This is one reason why advocates of a &amp;quot;semantic&amp;quot;
approach to artificial language-processing believe
in using relatively elaborate methods involving
complex inferential chains -- though they give us
little reason to expect that these techniques too
will not in practice be bedevilled by difficulties
similar to those that occur with straightforward
selection restrictions. However, while it may be
that the subject of &apos;fear&apos; is not always an anim-
ate noun, it may also be that this is true with
much more than chance frequency. If so, an arti-
ficial language-processing system can and should
use this as one factor to be balanced against
others in resolving ambiguities in sentences con-
taining &apos;fear&apos;.
6. To sum up: the deterministic-rationalist
philosophical paradigm has encouraged MT research-
ers to attempt an impossible task. The fallible-
rationalist paradigm requires them to lower their
sights, but may at the same time allow them to
attain greater actual success.
</bodyText>
<sectionHeader confidence="0.963428" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.998687517241379">
Arthern, P.J. (1979) &amp;quot;Machine translation and
computerized terminology systems&amp;quot;. In Bar-
bara Snell, ed., Translating and the Computer.
North-Holland.
Chomsky, A.N. (1957) Syntactic Structures. Mou-
ton.
Hobbs, J.R. (1976) &amp;quot;Pronoun resolution&amp;quot;. Research
Report 76-1. Department of Computer Sciences,
City College, City University of New York.
Johansson, S. (1978) &amp;quot;Manual of information to
accompany the Lancaster-Oslo/Bergen Corpus of
British English, for use with digital comput-
ers&amp;quot;. Department of English, University of
Oslo.
Leech, G.N., R. Garside, &amp; E. Atwell (1983) &amp;quot;The
automatic grammatical tagging of the LOB
Corpus&amp;quot;. ICAME News no. 7, pp. 13-33. Nor-
wegian Computing Centre for the Humanities.
Moore, T. &amp; Christine Carling (1982) Understand-
ing Language. Macmillan.
Sampson, G.R. (1980) Making Sense. Oxford Uni-
versity Press.
Sampson, G.R. (in press) Popperian Linguistics.
Hutchinson.
Smith, N.V., ed. (1982) Mutual Knowledge. Acad-
emic Press.
Suppes, P. (1970) &amp;quot;Probabilistic grammars for
natural languages&amp;quot;. Synthese vol. 22, pp.
95-116.
</reference>
<page confidence="0.999752">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.743133">
<title confidence="0.999012">FALLIBLE RATIONALISM AND MACHINE TRANSLATION</title>
<author confidence="0.999967">Geoffrey Sampson</author>
<affiliation confidence="0.99987">Department of Linguistics &amp; Modern English Language University of Lancaster</affiliation>
<address confidence="0.858006">LANCASTER LA1-4YT, G.B.</address>
<abstract confidence="0.999670437340155">Approaches to MT have been heavily influenced by changing trends in the philosophy of language and mind. Because of the artificial hiatus which followed the publication of the ALFAC Report, MT research in the 1970s and early 1980s has had to catch up with major developments that have occurred in linguistic and philosophical thinking; currently, MT seems to be uncritically loyal to a paradigm of thought about language which is rapidly losing most of its adherents in departments of linguistics and philosophy. I argue, both in theoretical terms and by reference to empirical research on a particular translation problem, that the Popperian &amp;quot;fallible rationalist&amp;quot; view of mental processes which is winning acceptance as a more sophisticated alternative to Chomskyan &amp;quot;deterministic rationalism&amp;quot; should lead MT researchers to redefine their goals and to adopt certain currently-neglected techniques in trying to achieve those goals. 1 Since the Second World War, three rival views of the nature of the human mind have competed for the allegiance of philosophically-minded people. Each of these views has implications for our understanding of language. The 1950s and early 1960s were dominated by a behaviourist approach tracing its ancestry to John Locke and represented recently e.g. by Leonard Bloomfield and B.F. Skinner. On this view, &amp;quot;mind&amp;quot; is merely a name for a set of associations that have been established during a person&apos;s life between external stimuli and behavioural responses. The meaning of a sentence is to be understood not as the effect it has on an unobservable internal model of reality but as the behaviour it evokes in the hearer. During the 1960s this view lost ground to the rationalist ideas of Noam Chomsky, working in an intellectual tradition founded by Plato and reinaugurated in modern times by Rene Descartes. On this view, stimuli and responses are linked only indirectly, via an immensely complex cognitive mechanism having its own fixed principles of operation which are independent of experience. A given behaviour is a response to an internal mental event which is determined as the resultant of the initial state of the mental apparatus together with the entire history of inputs to it. The meaning of a sentence must be explained in terms of the unseen responses it evokes in the cognitive apparatus, which might take the form of successive modifications of an internal model of reality that could be described as &amp;quot;inferencing&amp;quot;. Chomskyan rationalism is undoubtedly more satisfactory as an account of human cognition than Skinnerian behaviourism. By the late 1970s, however, the mechanical determinism that is part of Chomsky&apos;s view of mind appeared increasingly unrealistic to many writers. There is little empirical support, for instance, for the Chomskyan assumptions that the child&apos;s acquisition of his first language, or the adult&apos;s comprehension of a given utterance, are processes that reach well-defined terminations after a given period of mental processing -language seems typically to work in a more &amp;quot;open-ended&amp;quot; fashion than that. Within linguistics, as documented e.g. by Moore &amp; Carling (1982), the Chomskyan paradigm is by now widely rejected. The view which is winning widespread acceptance as preserving the merits of rationalism while avoiding its inadequacies is Karl Popper&apos;s fallibilist version of the doctrine. On this account, the mind responds to experiential inputs not by a deterministic algorithm that reaches a halt state, but by creatively formulating fallible conjectures which experience is used to test. Typically the conjectures formulated are radically novel, in the sense that they could not be predicted even on the basis of ideally complete knowledge of the person&apos;s prior state. This version of rationalism is incompatible with the materialist doctrine that the mind is nothing but an arrangement of matter and wholly governed by the laws of physics; but, historically, materialism has not commonly been regarded as an axiom requiring no argument to support it (although it may be that the ethos of Artificial Intelligence makes practitioners of this discipline more than averagely favourable towards materialism). As a matter of logic, fallible conjectures in any domain can be eliminated by adverse experience but can never be decisively confirmed. Our reaction to linguistic experience, consequently; is for a Popperian both non-deterministic and open-ended. There is no reason to expect a person at any age to cease to improve his knowledge of his mother-tongue, or to expect different members of a speech-community to formulate identical internalized grammars; and understanding an individual utterance is a process which a person can 86 execute to any desired degree of thoroughness -we stop trying to improve our understanding of a particular sample of language not because we reach a natural stopping-place but because we judge that the returns from further effort are likely to be less than the resources invested. For a Chomskyan linguist, divergences between individuals in their, linguistic behaviour are to be explained either in terms of mixture of &amp;quot;dialects&amp;quot; or in terms of failure of practical &amp;quot;performance&amp;quot; fully to match the abstract &amp;quot;competence&amp;quot; possessed by the mature speaker. For the Popperian such divergences require no explanation; we do not possess algorithms which would lead to correct results if they were executed thoroughly. Indeed, since languages have no reality independent of their speakers, the idea that there exists a &amp;quot;correct&amp;quot; solution to the problem of acquiring a language or of understanding an individual sentence ceases to apply except as an untheoretical approximation. The superiority of the Popperian to the Chomskyan paradigm as a framework for interpreting the facts of linguistic behaviour is e.g. in my Sense(1980), Linguistics(in press). 2 There is a major difference in style between the MT of the 19505 and 1960s, and the projects of the last decade. This reflects the difference between behaviourist and deterministic-rationalist paradigms. Speaking very broadly, early MT research envisaged the problem of translation as that of establishing equivalences between observable, surface features of languages: vocabulary items, taxemes of order, and the like. Recent MT research has taken it as axiomatic that successful MT must incorporate a large AI component. Human translation, it is now realized, involves the understanding of source texts rather than mere transliteration from one set of linguistic conventions to another: we make heavy use of inferencing in order to resolve textual ambiguities. MT systems must therefore simulate these inferencing processes in order to produce human-like output. Furthermore, the Chomskyan paradigm incorpaxioms about the kindsof operation characteristic of human linguistic processing, and MT research inherits these. In particular, Chomsky and his followers have been hostile to the idea that any interesting linguistic rules or processes might be probabilistic or statistical in nature (e.g. Chomsky 1957: 15-17, and of. the controversy about Labovian &amp;quot;variable rules&amp;quot;). The assumption that human language-processing is invariably an all-or-none phenomenon might well be questioned someone who subscribed to the other tenets of the Chomskyan paradigm (e.g. Suppes 1970), but it is consistent with the heavily deterministic flavour of that paradigm. Correspondingly, recent MT projects known to me seem to make no use of probabilities, and anecdotal evidence suggests that MT (and other AI) researchers perceive proposals for the exploitation of probabilistic techniques as defeatist (&amp;quot;We ought to be modelling what the mind actually does rather than using purely artificial methods to achieve a rough approximation to its output&amp;quot;). 3. What are the implications for MT, and for Al in general, of a shift from a deterministic&apos;to a fallibilist version of rationalism? (On the general issue see e.g. the exchange between Aravind Joshi and me in Smith 1982.) They can be summed up as follows. First, there is no such thing as an ideal speaker&apos;s competence which, if simulated mechanically, would constitute perfect MT. In the case of &amp;quot;literary&amp;quot; texts it is generally recognised that different human translators may produce markedly different translations none of which can be considered more &amp;quot;correct&amp;quot; than the others; from the Popperian viewpoint literary texts do not differ qualitatively from other genres. (Referring to the translation requirements of the Secretariat of the Council of the European Communities, P.J. Arthern (1979: 81) has said that &amp;quot;the only quality we can accept is 100% fidelity to the meaning of the original&amp;quot;. From the fallibilist point of view that is like saying &amp;quot;the only kind of motors we are willing to use are perpetual-motion machines&amp;quot;.) Second, there is no possibility of designing an artificial system which simulates the actions of an unpredictably creative mind, since any machine is a material object governed by physical law. Thus it will not, for instance, be possible to design an artificial system which regularly uses inferencing to resolve the meaning of given texts in the same way as a human reader of the texts. There is no principled barrier, of course, to an artificial system which applies logical transformations to derive conclusions from given premisses. But an artificial system must be restricted to some fixed, perhaps very large, database of premisses (&amp;quot;world knowledge&amp;quot;). It is central to the Popperian view of mind that human inferencing is not limited to a fixed set of premisses but involves the frequent invention of new hypotheses which are not related in any logical way to the previous contents of mind. An MT system cannot aspire to perfect human performance. (But then, neither can a human.) Third: a situation in which the behaviour of any individual is only approximately similar to that of other individuals and is not in detail predictable even in principle is just the kind of situation in which probabilistic techniques are valuable, irrespective of whether or not the processes occurring within individual humans are themselves intrinsically probabilistic. To draw an analogy: life-insurance companies do not condemn the actuarial profession as a bunch of copouts because they do not attempt to predict the precise date of death of individual policyholders. MT research ought to exploit any techniques that offer the possibility of better approximations to acceptable translation, whether or not it seems likely that human translation exploits such techniques; and it is likely that useful methods will often be probabilistic. Fourth: MT researchers will ultimately need to appreciate that there is no natural end to the process of improving the quality of translation (though it may be premature to raise this issue 87 at a stage when the best mechanical translation is still quite bad). Human translation always involves a (usually tacit) cost-benefit analysis; it is never a question of &amp;quot;How much work is needed to translate this text &apos;properly&apos;?&amp;quot; but of &amp;quot;Will a given increment of effort be profitable in terms of achieved improvement in translation?&amp;quot; Likewise, the question confronting MT is not &amp;quot;Is MT possible?&amp;quot; but &amp;quot;What are the disbenefits of translating this or that category of texts at this or that level of inexactness, and how do the costs of reducing the incidence of a given type of error compare with the gains to the consumers?&amp;quot; 4 The value of probabilistic techniques is sufficiently exemplified by the spectacular success of the Lancaster-Oslo-Bergen Tagging System (see e.g. Leech et al. 1983). The LOB Tagging System, operational since 1981, assigns grammatical tags drawn from a highly-differentiated (134member) tag-set to the words of &amp;quot;real-life&amp;quot; English text. The system &amp;quot;knows&amp;quot; virtually nothing of the syntax of English in terms of the kind of grammar-rules believed by linguists to make up the speaker&apos;s competence; it uses only facts about local transition-probabilities between formclasses, together with the relatively meagre clues provided by English morphology. By late 1982 the output of the system fell short of complete success (defined as tagging identical to that done independently by a human linguist) by only 3.4%. Various methods are being used to reduce this failure-rate further, but the nature of the techniques used ensures that the ideal of 100% success will be approached only asymptotically. However, the point is that no other extant automatic tagging-system known to me approaches the current success-level of the LOB system. I predict that any system which eschews probabilistic methods will perform at a significantly lower level. 5 In the remainder of this paper I illustrate the argument that human language-comprehension involves inferencing from unpredictable hypotheses, using research of my own on the problem of &amp;quot;referring&amp;quot; pronouns. approach can never attain 100% success, and that it does not correspond to the method by which humans resolve pronouns. However, unlike Hobbs&apos;s syntactic algorithm, his semantic algorithm is purely programmatic. The implication that it will be able to achieve 100% success -or even that it will be able to match the success-level of the existing syntactic algorithm -rests purely on faith, though this faith is quite understandable given the axioms of deterministic rationalism. I investigated these issues by examining a set of examples of the pronoun it drawn from the LOB Corpus (a standard million-word computer-readable corpus of modern written British English -see Johansson 1978). The pronoun it is specially interesting in connexion with MT because of the problems of translation into gender-languages; my examples were extracted from the texts in Category H of the LOB Corpus, which includes governmental and similar documents and thus matches the genres which current large-scale MT projects such as EUROTRA aim to translate. I began with 338 instances of it; after eliminating non-referential cases I was left with 156 instances which I examined intensively. I asked the following questions: (1) In what proportion of cases do I as an educated native speaker feel confident about the intended reference? (2) Where I do feel confident and Hobbs&apos;s syntactic algorithm gives a result which I believe to be wrong, what kind of reasoning enabled me to reach my solution? (2) Where Hobbs&apos;s algorithm gives what I believe to be the correct result, is it plausible that a semantic algorithm would give the same result? (4) Could the performance of Hobbs&apos;s syntactic algorithm be improved, as an alternative to replacing it by a semantic algorithm? It emerged that: My research was done in reaction to an article by Jerry Hobbs (1976). Hobbs provides an unusually clear example of the Chomskyan paradigm of Al research, since he makes his methodological axioms relatively explicit. He begins by defining a complex and subtle algorithm for referring pronouns which depends exclusively on the grammatical structure of the sentences in which they occur. This algorithm is highly successful: tested on a sample of texts, it is 88.3% accurate (a figure which rises slightly, to 91.7%, when the algorithm is expanded to use the simple kind of semantic information represented by Katz/Fodor &amp;quot;selection restrictions&amp;quot;). Nevertheless, Hobbs argues that this approach to the problem of pronoun resolution must be abandoned in favour of a &amp;quot;semantic algorithm&amp;quot;, meaning one which depends on inferencing from a data-base of world knowledge rather than on syntactic structure. He gives several reasons; the important reasons are that the syntactic (1) In about 10% of all cases, human resolution was impossible; on careful consideration of the alternatives I concluded that I did not know the intended reference (even though, on a first relatively cursory reading, most of these cases had not struck me as ambiguous). An example is: The lower platen, which supports the leather, is raised hydraulically to bring it into contact with the rollers on the upper platen (H6.148) it refer to the lower platenor to the &apos; leather(la platina,il cuoio!)?I really don&apos;t know. In at least one instance (not this one) I reached different confident conclusions about the same case on different occasions (and this suggests that there are likely to be other cases which I have confidently resolved in ways other than the writer intended). The implication is 88 that a system which performs at a level of success much above 90% on the task of resolving referential it would be outperforming a human, which is contradictory: language means what humans take it to mean. (2) In a number of cases where I judged the syntactic algorithm to give the wrong result, the premisses on which my own decisions were based were propositions that were not pieces of factual general knowledge and which I was not aware of ever having consciously entertained before producing them in the course of trying to interpret the text in question. It would therefore be quixotic to suggest that these propositions would occur in the data-base available to a future MT system. Consider, for instance: Under the &amp;quot;permissive&amp;quot; powers, however, in the worst cases when the Ministry was right and the M.P. was right the local authority could still dig its heels in and say that whatever the Ministry said it was not going to give a grant. (H16. 24) feel sure that it refers to the authority than the Ministry,chiefly because it seems to me much more plausible that a lower-level branch of government would refuse to heed requests for action from a higher-level branch than that it would accuse the higher-level branch of deceit. But this generalization about the sociology of government was new to me when I thought it up for the purpose of interpreting the example quoted (and I am not certain that it is in fact universally true).</abstract>
<intro confidence="0.952058">(3) In a number of cases it was very difficult to</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P J Arthern</author>
</authors>
<title>Machine translation and computerized terminology systems&amp;quot;.</title>
<date>1979</date>
<booktitle>Translating and the Computer.</booktitle>
<editor>In Barbara Snell, ed.,</editor>
<publisher>North-Holland.</publisher>
<contexts>
<context position="8819" citStr="Arthern (1979" startWordPosition="1376" endWordPosition="1377"> Aravind Joshi and me in Smith 1982.) They can be summed up as follows. First, there is no such thing as an ideal speaker&apos;s competence which, if simulated mechanically, would constitute perfect MT. In the case of &amp;quot;literary&amp;quot; texts it is generally recognised that different human translators may produce markedly different translations none of which can be considered more &amp;quot;correct&amp;quot; than the others; from the Popperian viewpoint literary texts do not differ qualitatively from other genres. (Referring to the translation requirements of the Secretariat of the Council of the European Communities, P.J. Arthern (1979: 81) has said that &amp;quot;the only quality we can accept is 100% fidelity to the meaning of the original&amp;quot;. From the fallibilist point of view that is like saying &amp;quot;the only kind of motors we are willing to use are perpetual-motion machines&amp;quot;.) Second, there is no possibility of designing an artificial system which simulates the actions of an unpredictably creative mind, since any machine is a material object governed by physical law. Thus it will not, for instance, be possible to design an artificial system which regularly uses inferencing to resolve the meaning of given texts in the same way as a hu</context>
</contexts>
<marker>Arthern, 1979</marker>
<rawString>Arthern, P.J. (1979) &amp;quot;Machine translation and computerized terminology systems&amp;quot;. In Barbara Snell, ed., Translating and the Computer. North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A N Chomsky</author>
</authors>
<title>Syntactic Structures.</title>
<date>1957</date>
<publisher>Mouton.</publisher>
<contexts>
<context position="7286" citStr="Chomsky 1957" startWordPosition="1137" endWordPosition="1138">rather than mere transliteration from one set of linguistic conventions to another: we make heavy use of inferencing in order to resolve textual ambiguities. MT systems must therefore simulate these inferencing processes in order to produce human-like output. Furthermore, the Chomskyan paradigm incorporates axioms about the kinds of operation characteristic of human linguistic processing, and MT research inherits these. In particular, Chomsky and his followers have been hostile to the idea that any interesting linguistic rules or processes might be probabilistic or statistical in nature (e.g. Chomsky 1957: 15-17, and of. the controversy about Labovian &amp;quot;variable rules&amp;quot;). The assumption that human language-processing is invariably an all-or-none phenomenon might well be questioned even by someone who subscribed to the other tenets of the Chomskyan paradigm (e.g. Suppes 1970), but it is consistent with the heavily deterministic flavour of that paradigm. Correspondingly, recent MT projects known to me seem to make no use of probabilities, and anecdotal evidence suggests that MT (and other AI) researchers perceive proposals for the exploitation of probabilistic techniques as defeatist (&amp;quot;We ought to</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>Chomsky, A.N. (1957) Syntactic Structures. Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Pronoun resolution&amp;quot;.</title>
<date>1976</date>
<tech>Research Report 76-1.</tech>
<institution>Department of Computer Sciences, City College, City University of New York.</institution>
<contexts>
<context position="14998" citStr="Hobbs (1976)" startWordPosition="2380" endWordPosition="2381">n of cases do I as an educated native speaker feel confident about the intended reference? (2) Where I do feel confident and Hobbs&apos;s syntactic algorithm gives a result which I believe to be wrong, what kind of reasoning enabled me to reach my solution? (2) Where Hobbs&apos;s algorithm gives what I believe to be the correct result, is it plausible that a semantic algorithm would give the same result? (4) Could the performance of Hobbs&apos;s syntactic algorithm be improved, as an alternative to replacing it by a semantic algorithm? It emerged that: My research was done in reaction to an article by Jerry Hobbs (1976). Hobbs provides an unusually clear example of the Chomskyan paradigm of Al research, since he makes his methodological axioms relatively explicit. He begins by defining a complex and subtle algorithm for referring pronouns which depends exclusively on the grammatical structure of the sentences in which they occur. This algorithm is highly successful: tested on a sample of texts, it is 88.3% accurate (a figure which rises slightly, to 91.7%, when the algorithm is expanded to use the simple kind of semantic information represented by Katz/Fodor &amp;quot;selection restrictions&amp;quot;). Nevertheless, Hobbs arg</context>
</contexts>
<marker>Hobbs, 1976</marker>
<rawString>Hobbs, J.R. (1976) &amp;quot;Pronoun resolution&amp;quot;. Research Report 76-1. Department of Computer Sciences, City College, City University of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Johansson</author>
</authors>
<title>Manual of information to accompany the Lancaster-Oslo/Bergen Corpus of British English, for use with digital computers&amp;quot;.</title>
<date>1978</date>
<institution>Department of English, University of Oslo.</institution>
<contexts>
<context position="13848" citStr="Johansson 1978" startWordPosition="2188" endWordPosition="2189">e method by which humans resolve pronouns. However, unlike Hobbs&apos;s syntactic algorithm, his semantic algorithm is purely programmatic. The implication that it will be able to achieve 100% success -- or even that it will be able to match the success-level of the existing syntactic algorithm -- rests purely on faith, though this faith is quite understandable given the axioms of deterministic rationalism. I investigated these issues by examining a set of examples of the pronoun it drawn from the LOB Corpus (a standard million-word computer-readable corpus of modern written British English -- see Johansson 1978). The pronoun it is specially interesting in connexion with MT because of the problems of translation into gender-languages; my examples were extracted from the texts in Category H of the LOB Corpus, which includes governmental and similar documents and thus matches the genres which current large-scale MT projects such as EUROTRA aim to translate. I began with 338 instances of it; after eliminating non-referential cases I was left with 156 instances which I examined intensively. I asked the following questions: (1) In what proportion of cases do I as an educated native speaker feel confident a</context>
</contexts>
<marker>Johansson, 1978</marker>
<rawString>Johansson, S. (1978) &amp;quot;Manual of information to accompany the Lancaster-Oslo/Bergen Corpus of British English, for use with digital computers&amp;quot;. Department of English, University of Oslo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G N Leech</author>
<author>R Garside</author>
<author>E Atwell</author>
</authors>
<title>The automatic grammatical tagging of the LOB Corpus&amp;quot;.</title>
<date>1983</date>
<journal>ICAME News</journal>
<volume>7</volume>
<pages>13--33</pages>
<contexts>
<context position="11873" citStr="Leech et al. 1983" startWordPosition="1878" endWordPosition="1881">uch work is needed to translate this text &apos;properly&apos;?&amp;quot; but of &amp;quot;Will a given increment of effort be profitable in terms of achieved improvement in translation?&amp;quot; Likewise, the question confronting MT is not &amp;quot;Is MT possible?&amp;quot; but &amp;quot;What are the disbenefits of translating this or that category of texts at this or that level of inexactness, and how do the costs of reducing the incidence of a given type of error compare with the gains to the consumers?&amp;quot; 4 The value of probabilistic techniques is sufficiently exemplified by the spectacular success of the Lancaster-Oslo-Bergen Tagging System (see e.g. Leech et al. 1983). The LOB Tagging System, operational since 1981, assigns grammatical tags drawn from a highly-differentiated (134- member) tag-set to the words of &amp;quot;real-life&amp;quot; English text. The system &amp;quot;knows&amp;quot; virtually nothing of the syntax of English in terms of the kind of grammar-rules believed by linguists to make up the speaker&apos;s competence; it uses only facts about local transition-probabilities between formclasses, together with the relatively meagre clues provided by English morphology. By late 1982 the output of the system fell short of complete success (defined as tagging identical to that done inde</context>
</contexts>
<marker>Leech, Garside, Atwell, 1983</marker>
<rawString>Leech, G.N., R. Garside, &amp; E. Atwell (1983) &amp;quot;The automatic grammatical tagging of the LOB Corpus&amp;quot;. ICAME News no. 7, pp. 13-33. Norwegian Computing Centre for the Humanities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Moore</author>
<author>Christine Carling</author>
</authors>
<title>Understanding Language.</title>
<date>1982</date>
<publisher>Macmillan.</publisher>
<contexts>
<context position="3318" citStr="Moore &amp; Carling (1982)" startWordPosition="521" endWordPosition="524">an account of human cognition than Skinnerian behaviourism. By the late 1970s, however, the mechanical determinism that is part of Chomsky&apos;s view of mind appeared increasingly unrealistic to many writers. There is little empirical support, for instance, for the Chomskyan assumptions that the child&apos;s acquisition of his first language, or the adult&apos;s comprehension of a given utterance, are processes that reach well-defined terminations after a given period of mental processing -- language seems typically to work in a more &amp;quot;open-ended&amp;quot; fashion than that. Within linguistics, as documented e.g. by Moore &amp; Carling (1982), the Chomskyan paradigm is by now widely rejected. The view which is winning widespread acceptance as preserving the merits of rationalism while avoiding its inadequacies is Karl Popper&apos;s fallibilist version of the doctrine. On this account, the mind responds to experiential inputs not by a deterministic algorithm that reaches a halt state, but by creatively formulating fallible conjectures which experience is used to test. Typically the conjectures formulated are radically novel, in the sense that they could not be predicted even on the basis of ideally complete knowledge of the person&apos;s pri</context>
</contexts>
<marker>Moore, Carling, 1982</marker>
<rawString>Moore, T. &amp; Christine Carling (1982) Understanding Language. Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Sampson</author>
</authors>
<title>Making Sense.</title>
<date>1980</date>
<publisher>Oxford University Press.</publisher>
<marker>Sampson, 1980</marker>
<rawString>Sampson, G.R. (1980) Making Sense. Oxford University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G R Sampson</author>
</authors>
<title>(in press) Popperian Linguistics.</title>
<publisher>Hutchinson.</publisher>
<marker>Sampson, </marker>
<rawString>Sampson, G.R. (in press) Popperian Linguistics. Hutchinson.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N V Smith</author>
<author>ed</author>
</authors>
<title>Mutual Knowledge.</title>
<date>1982</date>
<publisher>Academic Press.</publisher>
<marker>Smith, ed, 1982</marker>
<rawString>Smith, N.V., ed. (1982) Mutual Knowledge. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Suppes</author>
</authors>
<title>Probabilistic grammars for natural languages&amp;quot;.</title>
<date>1970</date>
<journal>Synthese</journal>
<volume>22</volume>
<pages>95--116</pages>
<contexts>
<context position="7559" citStr="Suppes 1970" startWordPosition="1176" endWordPosition="1177">the Chomskyan paradigm incorporates axioms about the kinds of operation characteristic of human linguistic processing, and MT research inherits these. In particular, Chomsky and his followers have been hostile to the idea that any interesting linguistic rules or processes might be probabilistic or statistical in nature (e.g. Chomsky 1957: 15-17, and of. the controversy about Labovian &amp;quot;variable rules&amp;quot;). The assumption that human language-processing is invariably an all-or-none phenomenon might well be questioned even by someone who subscribed to the other tenets of the Chomskyan paradigm (e.g. Suppes 1970), but it is consistent with the heavily deterministic flavour of that paradigm. Correspondingly, recent MT projects known to me seem to make no use of probabilities, and anecdotal evidence suggests that MT (and other AI) researchers perceive proposals for the exploitation of probabilistic techniques as defeatist (&amp;quot;We ought to be modelling what the mind actually does rather than using purely artificial methods to achieve a rough approximation to its output&amp;quot;). 3. What are the implications for MT, and for Al in general, of a shift from a deterministic&apos;to a fallibilist version of rationalism? (On </context>
</contexts>
<marker>Suppes, 1970</marker>
<rawString>Suppes, P. (1970) &amp;quot;Probabilistic grammars for natural languages&amp;quot;. Synthese vol. 22, pp. 95-116.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>