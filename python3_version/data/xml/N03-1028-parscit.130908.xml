<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<note confidence="0.959564">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 134-141
Edmonton, May-June 2003
</note>
<title confidence="0.996705">
Shallow Parsing with Conditional Random Fields
</title>
<author confidence="0.99656">
Fei Sha and Fernando Pereira
</author>
<affiliation confidence="0.9912565">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.944655">
200 South 33rd Street, Philadelphia, PA 19104
</address>
<email confidence="0.99958">
(feisha|pereira)@cis.upenn.edu
</email>
<sectionHeader confidence="0.996672" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999809894736842">
Conditional random fields for sequence label-
ing offer advantages over both generative mod-
els like HMMs and classifiers applied at each
sequence position. Among sequence labeling
tasks in language processing, shallow parsing
has received much attention, with the devel-
opment of standard evaluation datasets and ex-
tensive comparison among methods. We show
here how to train a conditional random field to
achieve performance as good as any reported
base noun-phrase chunking method on the
CoNLL task, and better than any reported sin-
gle model. Improved training methods based
on modern optimization algorithms were crit-
ical in achieving these results. We present ex-
tensive comparisons between models and train-
ing methods that confirm and strengthen pre-
vious results on shallow parsing and training
methods for maximum-entropy models.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995996129032258">
Sequence analysis tasks in language and biology are of-
ten described as mappings from input sequences to se-
quences of labels encoding the analysis. In language pro-
cessing, examples of such tasks include part-of-speech
tagging, named-entity recognition, and the task we shall
focus on here, shallow parsing. Shallow parsing iden-
tifies the non-recursive cores of various phrase types in
text, possibly as a precursor to full parsing or informa-
tion extraction (Abney, 1991). The paradigmatic shallow-
parsing problem is NP chunking, which finds the non-
recursive cores of noun phrases called base NPs. The
pioneering work of Ramshaw and Marcus (1995) in-
troduced NP chunking as a machine-learning problem,
with standard datasets and evaluation metrics. The task
was extended to additional phrase types for the CoNLL-
2000 shared task (Tjong Kim Sang and Buchholz, 2000),
which is now the standard evaluation task for shallow
parsing.
Most previous work used two main machine-learning
approaches to sequence labeling. The first approach re-
lies on k-order generative probabilistic models of paired
input sequences and label sequences, for instance hidden
Markov models (HMMs) (Freitag and McCallum, 2000;
Kupiec, 1992) or multilevel Markov models (Bikel et al.,
1999). The second approach views the sequence labeling
problem as a sequence of classification problems, one for
each of the labels in the sequence. The classification re-
sult at each position may depend on the whole input and
on the previous k classifications. 1
The generative approach provides well-understood
training and decoding algorithms for HMMs and more
general graphical models. However, effective genera-
tive models require stringent conditional independence
assumptions. For instance, it is not practical to make the
label at a given position depend on a window on the in-
put sequence as well as the surrounding labels, since the
inference problem for the corresponding graphical model
would be intractable. Non-independent features of the
inputs, such as capitalization, suffixes, and surrounding
words, are important in dealing with words unseen in
training, but they are difficult to represent in generative
models.
The sequential classification approach can handle
many correlated features, as demonstrated in work on
maximum-entropy (McCallum et al., 2000; Ratnaparkhi,
1996) and a variety of other linear classifiers, including
winnow (Punyakanok and Roth, 2001), AdaBoost (Ab-
ney et al., 1999), and support-vector machines (Kudo and
Matsumoto, 2001). Furthermore, they are trained to min-
imize some function related to labeling error, leading to
smaller error in practice if enough training data are avail-
able. In contrast, generative models are trained to max-
imize the joint probability of the training data, which is
&apos;Ramshaw and Marcus (1995) used transformation-based
learning (Brill, 1995), which for the present purposes can be
tought of as a classification-based method.
not as closely tied to the accuracy metrics of interest if the
actual data was not generated by the model, as is always
the case in practice.
However, since sequential classifiers are trained to
make the best local decision, unlike generative mod-
els they cannot trade off decisions at different positions
against each other. In other words, sequential classifiers
are myopic about the impact of their current decision
on later decisions (Bottou, 1991; Lafferty et al., 2001).
This forced the best sequential classifier systems to re-
sort to heuristic combinations of forward-moving and
backward-moving sequential classifiers (Kudo and Mat-
sumoto, 2001).
Conditional random fields (CRFs) bring together the
best of generative and classification models. Like classi-
fication models, they can accommodate many statistically
correlated features of the inputs, and they are trained dis-
criminatively. But like generative models, they can trade
off decisions at different sequence positions to obtain a
globally optimal labeling. Lafferty et al. (2001) showed
that CRFs beat related classification models as well as
HMMs on synthetic data and on a part-of-speech tagging
task.
In the present work, we show that CRFs beat all re-
ported single-model NP chunking results on the standard
evaluation dataset, and are statistically indistinguishable
from the previous best performer, a voting arrangement of
24 forward- and backward-looking support-vector clas-
sifiers (Kudo and Matsumoto, 2001). To obtain these
results, we had to abandon the original iterative scal-
ing CRF training algorithm for convex optimization al-
gorithms with better convergence properties. We provide
detailed comparisons between training methods.
The generalized perceptron proposed by Collins
(2002) is closely related to CRFs, but the best CRF train-
ing methods seem to have a slight edge over the general-
ized perceptron.
</bodyText>
<sectionHeader confidence="0.963881" genericHeader="method">
2 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.999248571428571">
We focus here on conditional random fields on sequences,
although the notion can be used more generally (Laf-
ferty et al., 2001; Taskar et al., 2002). Such CRFs define
conditional probability distributions p(Y |X) of label se-
quences given input sequences. We assume that the ran-
dom variable sequences X and Y have the same length,
and use x = x1 · · · xn and y = y1 · · · yn for the generic
input sequence and label sequence, respectively.
A CRF on (X, Y ) is specified by a vector f of local
features and a corresponding weight vector . Each local
feature is either a state feature s(y, x, i) or a transition
feature t(y, y, x, i), where y, y are labels, x an input
sequence, and i an input position. To make the notation
more uniform, we also write
</bodyText>
<equation confidence="0.999775">
s(y, y, x, i) = s(y, x, i)
s(y, x, i) = s(yi, x, i)
� t(yi−1, yi, x, i) i &gt; 1
t(y, x, i) =
0 i = 1
</equation>
<bodyText confidence="0.999941428571429">
for any state feature s and transition feature t. Typically,
features depend on the inputs around the given position,
although they may also depend on global properties of the
input, or be non-zero only at some positions, for instance
features that pick out the first or last labels.
The CRF’s global feature vector for input sequence x
and label sequence y is given by
</bodyText>
<equation confidence="0.9918035">
F(y, x) = � f(y, x, i)
i
</equation>
<bodyText confidence="0.9998805">
where i ranges over input positions. The conditional
probability distribution defined by the CRF is then
</bodyText>
<equation confidence="0.998202333333333">
exp · F(Y ,X)
p(Y |X) = (1)
Z(X)
</equation>
<bodyText confidence="0.860355">
where
</bodyText>
<equation confidence="0.995769">
�Z(x) = exp  · F(y, x)
y
</equation>
<bodyText confidence="0.8729125">
Any positive conditional distribution p(Y |X) that obeys
the Markov property
</bodyText>
<equation confidence="0.989647">
p(Yi|{Yj}j=i,X) = p(Yi|Yi−1,Yi+1,X)
</equation>
<bodyText confidence="0.9062276">
can be written in the form (1) for appropriate choice of
feature functions and weight vector (Hammersley and
Clifford, 1971).
The most probable label sequence for input sequence
x is
</bodyText>
<equation confidence="0.9982235">
yˆ = arg max p(y|x) = arg max  · F(y,x)
y y
</equation>
<bodyText confidence="0.998132714285714">
because Z(x) does not depend on y. F(y, x) decom-
poses into a sum of terms for consecutive pairs of labels,
so the most likely y can be found with the Viterbi algo-
rithm.
We train a CRF by maximizing the log-likelihood of a
given training set T = {(xk, yk)}Nk=1, which we assume
fixed for the rest of this section:
</bodyText>
<equation confidence="0.995899">
L = Ek logp(yk|xk)
Ek [ · F(yk, xk) − logZ(xk)]
</equation>
<bodyText confidence="0.973027">
To perform this optimization, we seek the zero of the gra-
dient
</bodyText>
<equation confidence="0.995088333333333">
L =
E [F(yk, xk) − Epa(Y |xk)F(Y , xk)] (2)
k
</equation>
<bodyText confidence="0.999668666666667">
In words, the maximum of the training data likelihood
is reached when the empirical average of the global fea-
ture vector equals its model expectation. The expectation
Ep,,(Y |.)F(Y , x) can be computed efficiently using a
variant of the forward-backward algorithm. For a given
x, define the transition matrix for position i as
</bodyText>
<equation confidence="0.762056">
Mi[y, y] = exp  · f(y, y, x, i)
</equation>
<bodyText confidence="0.716194666666667">
Let f be any local feature, fi[y, y] = f(y, y, x, i),
F(y, x) = Ei f(yi−1, yi, x, i), and let  denote
component-wise matrix product. Then
</bodyText>
<equation confidence="0.999797166666667">
Ep,,(Y |.)F(Y , x) = � pa(y|x)F(y, x)
y
i−1(fi  Mi)
i
Za(x)
Za(x) = n · 1
</equation>
<bodyText confidence="0.9909815">
where i and i the forward and backward state-cost
vectors defined by
</bodyText>
<equation confidence="0.996352285714286">
� i−1Mi 0 &lt; i  n
i = 1 i = 0

T = J M,&apos;1&apos;
i+1 Qi 1  i &lt; n
T+1
Z 11 1 i = n
</equation>
<bodyText confidence="0.999657608695652">
Therefore, we can use a forward pass to compute the i
and a backward bass to compute the i and accumulate
the feature expectations.
To avoid overfitting, we penalize the likelihood with
a spherical Gaussian weight prior (Chen and Rosenfeld,
1999):
optimization algorithms when many correlated features
are involved. Concurrently with the present work, Wal-
lach (2002) tested conjugate gradient and second-order
methods for CRF training, showing significant training
speed advantages over iterative scaling on a small shal-
low parsing problem. Our work shows that precon-
ditioned conjugate-gradient (CG) (Shewchuk, 1994) or
limited-memory quasi-Newton (L-BFGS) (Nocedal and
Wright, 1999) perform comparably on very large prob-
lems (around 3.8 million features). We compare those
algorithms to generalized iterative scaling (GIS) (Dar-
roch and Ratcliff, 1972), non-preconditioned CG, and
voted perceptron training (Collins, 2002). All algorithms
except voted perceptron maximize the penalized log-
likelihood:  = arg maxa La. However, for ease of
exposition, this discussion of training methods uses the
unpenalized log-likelihood La.
</bodyText>
<subsectionHeader confidence="0.994362">
3.1 Preconditioned Conjugate Gradient
</subsectionHeader>
<bodyText confidence="0.999845">
Conjugate-gradient (CG) methods have been shown to
be very effective in linear and non-linear optimization
(Shewchuk, 1994). Instead of searching along the gra-
dient, conjugate gradient searches along a carefully cho-
sen linear combination of the gradient and the previous
search direction.
CG methods can be accelerated by linearly trans-
forming the variables with preconditioner (Nocedal and
Wright, 1999; Shewchuk, 1994). The purpose of the pre-
conditioner is to improve the condition number of the
quadratic form that locally approximates the objective
function, so the inverse of Hessian is reasonable precon-
ditioner. However, this is not applicable to CRFs for two
reasons. First, the size of the Hessian is dim()2, lead-
ing to unacceptable space and time requirements for the
inversion. In such situations, it is common to use instead
the (inverse of) the diagonal of the Hessian. However in
our case the Hessian has the form
</bodyText>
<equation confidence="0.996693846153846">
H def 2L
Ha = a
�=
i
�La = [ · F(yk,xk) − logZa(xk)]
k
2
− 22 + const
with gradient �= − {E [F(Y , xk) × F(Y , xk)]
k
La = −EF(Y , xk) × EF(Y , xk)}

[F(yk, xk) − Epa(Y |.k)F(Y , xk)] − 2
</equation>
<sectionHeader confidence="0.999558" genericHeader="method">
3 Training Methods
</sectionHeader>
<bodyText confidence="0.999972222222222">
Lafferty et al. (2001) used iterative scaling algorithms
for CRF training, following earlier work on maximum-
entropy models for natural language (Berger et al., 1996;
Della Pietra et al., 1997). Those methods are very sim-
ple and guaranteed to converge, but as Minka (2001) and
Malouf (2002) showed for classification, their conver-
gence is much slower than that of general-purpose convex
where the expectations are taken with respect to
pa(Y |xk). Therefore, every Hessian element, includ-
ing the diagonal ones, involve the expectation of a prod-
uct of global feature values. Unfortunately, computing
those expectations is quadratic on sequence length, as the
forward-backward algorithm can only compute expecta-
tions of quantities that are additive along label sequences.
We solve both problems by discarding the off-diagonal
terms and approximating expectation of the square of a
global feature by the expectation of the sum of squares of
the corresponding local features at each position. The ap-
</bodyText>
<equation confidence="0.919031">
E
k
</equation>
<bodyText confidence="0.994607333333333">
proximated diagonal term Hf for feature f has the form
Hf = Ef(Y , xk)2
If this approximation is semidefinite, which is trivial to
check, its inverse is an excellent preconditioner for early
iterations of CG training. However, when the model is
close to the maximum, the approximation becomes un-
stable, which is not surprising since it is based on fea-
ture independence assumptions that become invalid as
the weights of interaction features move away from zero.
Therefore, we disable the preconditioner after a certain
number of iterations, determined from held-out data. We
call this strategy mixed CG training.
</bodyText>
<subsectionHeader confidence="0.999525">
3.2 Limited-Memory Quasi-Newton
</subsectionHeader>
<bodyText confidence="0.999736761904762">
Newton methods for nonlinear optimization use second-
order (curvature) information to find search directions.
As discussed in the previous section, it is not practi-
cal to obtain exact curvature information for CRF train-
ing. Limited-memory BFGS (L-BFGS) is a second-order
method that estimates the curvature numerically from
previous gradients and updates, avoiding the need for
an exact Hessian inverse computation. Compared with
preconditioned CG, L-BFGS can also handle large-scale
problems but does not require a specialized Hessian ap-
proximations. An earlier study indicates that L-BFGS
performs well in maximum-entropy classifier training
(Malouf, 2002).
There is no theoretical guidance on how much infor-
mation from previous steps we should keep to obtain
sufficiently accurate curvature estimates. In our exper-
iments, storing 3 to 10 pairs of previous gradients and
updates worked well, so the extra memory required over
preconditioned CG was modest. A more detailed descrip-
tion of this method can be found elsewhere (Nocedal and
Wright, 1999).
</bodyText>
<subsectionHeader confidence="0.994706">
3.3 Voted Perceptron
</subsectionHeader>
<bodyText confidence="0.999777285714286">
Unlike other methods discussed so far, voted perceptron
training (Collins, 2002) attempts to minimize the differ-
ence between the global feature vector for a training in-
stance and the same feature vector for the best-scoring
labeling of that instance according to the current model.
More precisely, for each training instance the method
computes a weight update
</bodyText>
<equation confidence="0.996589">
t+1 = t + F(yk, xk) − F(ˆyk, xk) (3)
</equation>
<bodyText confidence="0.590727">
in which ˆyk is the Viterbi path
</bodyText>
<equation confidence="0.95975">
t · F (y,xk)
</equation>
<bodyText confidence="0.999954428571429">
Like the familiar perceptron algorithm, this algorithm re-
peatedly sweeps over the training instances, updating the
weight vector as it considers each instance. Instead of
taking just the final weight vector, the voted perceptron
algorithm takes the average of the t. Collins (2002) re-
ported and we confirmed that this averaging reduces over-
fitting considerably.
</bodyText>
<equation confidence="0.655682">
= arg max
Y
ˆyk
</equation>
<sectionHeader confidence="0.964222" genericHeader="method">
4 Shallow Parsing
</sectionHeader>
<bodyText confidence="0.9196908">
Figure 1 shows the base NPs in an example sentence. Fol-
lowing Ramshaw and Marcus (1995), the input to the
NP
consists of the words in a sentence anno-
tated automatically with part-of-speech (POS) tags. The
task is to label each word with a label indi-
cating whether the word is outside a chunk
starts
a chunk (B), or continues a chunk (I). For example,
the tokens in first li
</bodyText>
<figure confidence="0.4661326">
chunker
chunker’s
(O),
ne of Figure 1 would be labeled
BIIBIIOBOBIIO.
</figure>
<subsectionHeader confidence="0.88766">
4.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.997294454545455">
NP chunking results have been reported on two slightly
different data sets: the original RM data set of Ramshaw
and Marcus (1995), and the modified CoNLL-2000 ver-
sion of Tjong Kim Sang and Buchholz (2000). Although
the chunk tags in the RM and CoNLL-2000 are somewhat
different, we found no significant accuracy differences
between models trained on these two data sets. There-
fore, all our results are reported on the CoNLL-2000 data
set. We also used a development test set, provided by
Michael Collins, derived from WSJ section 21 tagged
with the Brill (1995) POS tagger.
</bodyText>
<subsectionHeader confidence="0.935877">
4.2 CRFs for Shallow Parsing
</subsectionHeader>
<bodyText confidence="0.966647142857143">
Our chunking CRFs have asecond-order Markov depen-
dency between chunk tags. This is easily encoded by
making the CRF labels pairs of consecutive chunk tags.
That is, the label at position i is yi =
where ci is
the chunk tag of word i, one of
B, or I. Since B must be
</bodyText>
<equation confidence="0.96439475">
used to start a chunk, the label OI is impossible. In addi-
tion, successive labels are constrained:
=
yi =
</equation>
<bodyText confidence="0.891040363636364">
and
=
These contraints on the model
topology are enforced by giving appropriate features a
weight of
forcing all the forbidden labelings to have
zero probability.
Our choice of features was mainly governed by com-
puting power, since we do not use feature selection and
all features are used in training and testing. We use the
following factored representation for features
</bodyText>
<equation confidence="0.9931635">
x, i) = p(x,
yi) (4)
</equation>
<bodyText confidence="0.8512702">
where
i) is a predicate on the input sequence x and
current position i and
yi) is a predicate on pairs
of labels. For instance,
</bodyText>
<equation confidence="0.966330733333333">
ci−1ci,
O,
yi−1
ci−2ci−1,
ci−1ci,
c0
O.
−,
f(yi−1,yi,
i)q(yi−1,
p(x,
q(yi−1,
p(x, i) might be “word at posi-
tion i is the” or “the POS tags at positions i − 1, i are
Mi[y, y] f(Y xk)
</equation>
<figure confidence="0.83308625">
 Zx (x)
y,y�
−
i
 2

Rockwell International Corp. ’s Tulsa unit said it signed a tentative agreement extending
its contract with Boeing Co. to provide structural parts for Boeing ’s 747 jetliners .
</figure>
<figureCaption confidence="0.999201">
Figure 1: NP chunks
</figureCaption>
<equation confidence="0.998679304347826">
q(yi−1, yi) p(X, i)
yi = y true
yi = y, yi−1 = y&apos;
c(yi) = c
yi=y wi = w
or wi−1 = w
c(yi) = c wi+1 = w
wi−2 = w
wi+2 = w
wi−1 = w&apos;, wi = w
wi+1 = w&apos;, wi = w
ti = t
ti−1 = t
ti+1 = t
ti−2 = t
ti+2 = t
ti−1 = t&apos;, ti = t
ti−2 = t&apos;, ti−1 = t
ti = t&apos;, ti+1 = t
ti+1 = t&apos;, ti+2 = t
ti−2 = t&apos;&apos;, ti−1 = t&apos;, ti = t
ti−1 = t&apos;&apos;, ti = t&apos;, ti+1 = t
ti = t&apos;&apos;, ti+1 = t&apos;, ti+2 = t
</equation>
<tableCaption confidence="0.996077">
Table 1: Shallow parsing features
</tableCaption>
<bodyText confidence="0.999907782608696">
DT, NN.” Because the label set is finite, such a factoring
of f(yi−1, yi, x, i) is always possible, and it allows each
input predicate to be evaluated just once for many fea-
tures that use it, making it possible to work with millions
of features on large training sets.
Table 1 summarizes the feature set. For a given po-
sition i, wi is the word, ti its POS tag, and yi its label.
For any label y = c&apos;c, c(y) = c is the corresponding
chunk tag. For example, c(OB) = B. The use of chunk
tags as well as labels provides a form of backoff from
the very small feature counts that may arise in a second-
order model, while allowing significant associations be-
tween tag pairs and input predicates to be modeled. To
save time in some of our experiments, we used only the
820,000 features that are supported in the CoNLL train-
ing set, that is, the features that are on at least once. For
our highest F score, we used the complete feature set,
around 3.8 million in the CoNLL training set, which con-
tains all the features whose predicate is on at least once in
the training set. The complete feature set may in princi-
ple perform better because it can place negative weights
on transitions that should be discouraged if a given pred-
icate is on.
</bodyText>
<subsectionHeader confidence="0.997803">
4.3 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.9999726">
As discussed previously, we need a Gaussian weight prior
to reduce overfitting. We also need to choose the num-
ber of training iterations since we found that the best F
score is attained while the log-likelihood is still improv-
ing. The reasons for this are not clear, but the Gaussian
prior may not be enough to keep the optimization from
making weight adjustments that slighly improve training
log-likelihood but cause large F score fluctuations. We
used the development test set mentioned in Section 4.1 to
set the prior and the number of iterations.
</bodyText>
<subsectionHeader confidence="0.99254">
4.4 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.9999777">
The standard evaluation metrics for a chunker are preci-
sion P (fraction of output chunks that exactly match the
reference chunks), recall R (fraction of reference chunks
returned by the chunker), and their harmonic mean, the
F1 score F1 = 2 * P * R/(P + R) (which we call just
F score in what follows). The relationships between F
score and labeling error or log-likelihood are not direct,
so we report both F score and the other metrics for the
models we tested. For comparisons with other reported
results we use F score.
</bodyText>
<subsectionHeader confidence="0.992425">
4.5 Significance Tests
</subsectionHeader>
<bodyText confidence="0.999968555555556">
Ideally, comparisons among chunkers would control for
feature sets, data preparation, training and test proce-
dures, and parameter tuning, and estimate the statistical
significance of performance differences. Unfortunately,
reported results sometimes leave out details needed for
accurate comparisons. We report F scores for comparison
with previous work, but we also give statistical signifi-
cance estimates using McNemar’s test for those methods
that we evaluated directly.
Testing the significance of F scores is tricky because
the wrong chunks generated by two chunkers are not
directly comparable. Yeh (2000) examined randomized
tests for estimating the significance of F scores, and in
particular the bootstrap over the test set (Efron and Tib-
shirani, 1993; Sang, 2002). However, bootstrap variances
in preliminary experiments were too high to allow any
conclusions, so we used instead a McNemar paired test
on labeling disagreements (Gillick and Cox, 1989).
</bodyText>
<table confidence="0.97861125">
Model F score
SVM combination 94.39%
(Kudo and Matsumoto, 2001)
CRF 94.38%
Generalized winnow 93.89%
(Zhang et al., 2002)
Voted perceptron 94.09%
MEMM 93.70%
</table>
<tableCaption confidence="0.997889">
Table 2: NP chunking F scores
</tableCaption>
<sectionHeader confidence="0.999766" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999930714285714">
All the experiments were performed with our Java imple-
mentation of CRFs,designed to handle millions of fea-
tures, on 1.7 GHz Pentium IV processors with Linux and
IBM Java 1.3.0. Minor variants support voted perceptron
(Collins, 2002) and MEMMs (McCallum et al., 2000)
with the same efficient feature encoding. GIS, CG, and
L-BFGS were used to train CRFs and MEMMs.
</bodyText>
<subsectionHeader confidence="0.991283">
5.1 F Scores
</subsectionHeader>
<bodyText confidence="0.99994725">
Table 2 gives representative NP chunking F scores for
previous work and for our best model, with the com-
plete set of 3.8 million features. The last row of the table
gives the score for an MEMM trained with the mixed CG
method using an approximate preconditioner. The pub-
lished F score for voted perceptron is 93.53% with a dif-
ferent feature set (Collins, 2002). The improved result
given here is for the supported feature set; the complete
feature set gives a slightly lower score of 94.07%. Zhang
et al. (2002) reported a higher F score (94.38%) with gen-
eralized winnow using additional linguistic features that
were not available to us.
</bodyText>
<subsectionHeader confidence="0.999955">
5.2 Convergence Speed
</subsectionHeader>
<bodyText confidence="0.999929277777778">
All the results in the rest of this section are for the smaller
supported set of 820,000 features. Figures 2a and 2b
show how preconditioning helps training convergence.
Since each CG iteration involves a line search that may
require several forward-backward procedures (typically
between 4 and 5 in our experiments), we plot the progress
of penalized log-likelihood Ga with respect to the num-
ber of forward-backward evaluations. The objective func-
tion increases rapidly, achieving close proximity to the
maximum in a few iterations (typically 10). In contrast,
GIS training increases Ga rather slowly, never reaching
the value achieved by CG. The relative slowness of it-
erative scaling is also documented in a recent evaluation
of training methods for maximum-entropy classification
(Malouf, 2002). In theory, GIS would eventually con-
verge to the Ga optimum, but in practice convergence
may be so slow that Ga improvements may fall below
numerical accuracy, falsely indicating convergence.
</bodyText>
<table confidence="0.9834745">
training method time F score Ga
Precond. CG 130 94.19% -2968
Mixed CG 540 94.20% -2990
Plain CG 648 94.04% -2967
L-BFGS 84 94.19% -2948
GIS 3700 93.55% -5668
</table>
<tableCaption confidence="0.895334">
Table 3: Runtime for various training methods
</tableCaption>
<table confidence="0.9986356">
null hypothesis p-value
CRF vs. SVM 0.469
CRF vs. MEMM 0.00109
CRF vs. voted perceptron 0.116
MEMM vs. voted perceptron 0.0734
</table>
<tableCaption confidence="0.999855">
Table 4: McNemar’s tests on labeling disagreements
</tableCaption>
<bodyText confidence="0.998116083333333">
Mixed CG training converges slightly more slowly
than preconditioned CG. On the other hand, CG without
preconditioner converges much more slowly than both
preconditioned CG and mixed CG training. However, it
is still much faster than GIS. We believe that the superior
convergence rate of preconditioned CG is due to the use
of approximate second-order information. This is con-
firmed by the performance of L-BFGS, which also uses
approximate second-order information.2
Although there is no direct relationship between F
scores and log-likelihood, in these experiments F score
tends to follow log-likelihood. Indeed, Figure 3 shows
that preconditioned CG training improves test F scores
much more rapidly than GIS training.
Table 3 compares run times (in minutes) for reaching a
target penalized log-likelihood for various training meth-
ods with prior u = 1.0. GIS is the only method that failed
to reach the target, after 3,700 iterations. We cannot place
the voted perceptron in this table, as it does not opti-
mize log-likelihood and does not use a prior. However,
it reaches a fairly good F-score above 93% in just two
training sweeps, but after that it improves more slowly, to
a somewhat lower score, than preconditioned CG train-
ing.
</bodyText>
<subsectionHeader confidence="0.999875">
5.3 Labeling Accuracy
</subsectionHeader>
<bodyText confidence="0.958711833333333">
The accuracy rate for individual labeling decisions is
over-optimistic as an accuracy measure for shallow pars-
ing. For instance, if the chunk BTTTTTTT is labled as
OTTTTTTT, the labeling accuracy is 87.5%, but recall is
0. However, individual labeling errors provide a more
convenient basis for statistical significance tests. One
</bodyText>
<footnote confidence="0.51167">
2Although L-BFGS has a slightly higher penalized log-
likelihood, its log-likelihood on the data is actually lower than
that of preconditioned CG and mixed CG training.
</footnote>
<figure confidence="0.959266057142857">
Comparison of Fast Training Algorithms for CRF
# of Forward−backward evaluations
(a) Ga: CG (precond., mixed), L-BFGS
Comparison of CG Methods to GIS
# of Forward−backward evaluations
(b) Ga: CG (precond., plain), GIS
0
−5000
−30000
−35000
6 56 106 156 206 256
Penalized Log−likelihood
−10000
−15000
−20000
−25000
Preconditioned CG
Mixed CG Training
L−BFGS
Penalized Log−likelihood
−100000
−120000
−140000
−160000
−180000
−200000
0 50 100 150 200 250 300 350 400 450 500
−20000
−40000
−60000
−80000
0
Preconditioned CG
CG w/o Preconditioner
GIS
</figure>
<figureCaption confidence="0.954646">
Figure 2: Training convergence for various methods
</figureCaption>
<figure confidence="0.65019">
Comparison of CG Methods to GIS
</figure>
<figureCaption confidence="0.992822">
Figure 3: Test F scores vs. training time
</figureCaption>
<bodyText confidence="0.9994307">
such test is McNemar test on paired observations (Gillick
and Cox, 1989).
With McNemar’s test, we compare the correctness of
the labeling decisions of two models. The null hypothesis
is that the disagreements (correct vs. incorrect) are due to
chance. Table 4 summarizes the results of tests between
the models for which we had labeling decisions. These
tests suggest that MEMMs are significantly less accurate,
but that there are no significant differences in accuracy
among the other models.
</bodyText>
<sectionHeader confidence="0.999695" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999929">
We have shown that (log-)linear sequence labeling mod-
els trained discriminatively with general-purpose opti-
mization methods are a simple, competitive solution to
learning shallow parsers. These models combine the best
features of generative finite-state models and discrimina-
tive (log-)linear classifiers, and do NP chunking as well
as or better than “ad hoc” classifier combinations, which
were the most accurate approach until now. In a longer
version of this work we will also describe shallow pars-
ing results for other phrase types. There is no reason why
the same techniques cannot be used equally successfully
for the other types or for other related tasks, such as POS
tagging or named-entity recognition.
On the machine-learning side, it would be interest-
ing to generalize the ideas of large-margin classification
to sequence models, strengthening the results of Collins
(2002) and leading to new optimal training algorithms
with stronger guarantees against overfitting.
On the application side, (log-)linear parsing models
have the potential to supplant the currently dominant
lexicalized PCFG models for parsing by allowing much
richer feature sets and simpler smoothing, while avoid-
ing the label bias problem that may have hindered earlier
classifier-based parsers (Ratnaparkhi, 1997). However,
work in that direction has so far addressed only parse
reranking (Collins and Duffy, 2002; Riezler et al., 2002).
Full discriminative parser training faces significant algo-
rithmic challenges in the relationship between parsing al-
ternatives and feature values (Geman and Johnson, 2002)
and in computing feature expectations.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.985996833333333">
John Lafferty and Andrew McCallum worked with the
second author on developing CRFs. McCallum helped
by the second author implemented the first conjugate-
gradient trainer for CRFs, which convinced us that train-
ing of large CRFs on large datasets would be practical.
Michael Collins helped us reproduce his generalized per-
</bodyText>
<figure confidence="0.991284588235294">
# of Forward−backward evaluations
F score
0.95
0.85
0.75
0.65
0.55
0.45
0 50 100 150 200 250 300 350 400 450 500
0.9
0.8
0.7
0.6
0.5
Preconditioned CG
CG w/o Preconditioner
GIS
</figure>
<bodyText confidence="0.9982334">
cepton results and compare his method with ours. Erik
Tjong Kim Sang, who has created the best online re-
sources on shallow parsing, helped us with details of the
CoNLL-2000 shared task. Taku Kudo provided the out-
put of his SVM chunker for the significance test.
</bodyText>
<sectionHeader confidence="0.999322" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999771666666666">
S. Abney. Parsing by chunks. In R. Berwick, S. Abney, and
C. Tenny, editors, Principle-based Parsing. Kluwer Aca-
demic Publishers, 1991.
S. Abney, R. E. Schapire, and Y. Singer. Boosting applied to
tagging and PP attachment. In Proc. EMNLP-VLC, New
Brunswick, New Jersey, 1999. ACL.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maxi-
mum entropy approach to natural language processing. Com-
putational Linguistics, 22(1), 1996.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. An algo-
rithm that learns what’s in a name. Machine Learning, 34:
211–231, 1999.
L. Bottou. Une Approche th´eorique de l’Apprentissage Con-
nexionniste: Applications a` la Reconnaissance de la Parole.
PhD thesis, Universit´e de Paris XI, 1991.
E. Brill. Transformation-based error-driven learning and natural
language processing: a case study in part of speech tagging.
Computational Linguistics, 21:543–565, 1995.
S. F. Chen and R. Rosenfeld. A Gaussian prior for smoothing
maximum entropy models. Technical Report CMU-CS-99-
108, Carnegie Mellon University, 1999.
M. Collins. Discriminative training methods for hidden Markov
models: Theory and experiments with perceptron algo-
rithms. In Proc. EMNLP 2002. ACL, 2002.
M. Collins and N. Duffy. New ranking algorithms for parsing
and tagging: Kernels over discrete structures, and the voted
perceptron. In Proc. 40th ACL, 2002.
J. N. Darroch and D. Ratcliff. Generalized iterative scaling for
log-linear models. The Annals of Mathematical Statistics, 43
(5):1470–1480, 1972.
S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing fea-
tures of random fields. IEEE PAMI, 19(4):380–393, 1997.
B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap.
Chapman &amp; Hall/CRC, 1993.
D. Freitag and A. McCallum. Information extraction with
HMM structures learned by stochastic optimization. In
Proc. AAAI 2000, 2000.
S. Geman and M. Johnson. Dynamic programming for parsing
and estimation of stochastic unification-based grammars. In
Proc. 40th ACL, 2002.
L. Gillick and S. Cox. Some statistical issues in the compairson
of speech recognition algorithms. In International Confer-
ence on Acoustics Speech and Signal Processing, volume 1,
pages 532–535, 1989.
J. Hammersley and P. Clifford. Markov fields on finite graphs
and lattices. Unpublished manuscript, 1971.
T. Kudo and Y. Matsumoto. Chunking with support vector ma-
chines. In Proc. NAACL 2001. ACL, 2001.
J. Kupiec. Robust part-of-speech tagging using a hidden
Markov model. Computer Speech and Language, 6:225–242,
1992.
J. Lafferty, A. McCallum, and F. Pereira. Conditional random
fields: Probabilistic models for segmenting and labeling se-
quence data. In Proc. ICML-01, pages 282–289, 2001.
R. Malouf. A comparison of algorithms for maximum entropy
parameter estimation. In Proc. CoNLL-2002, 2002.
A. McCallum, D. Freitag, and F. Pereira. Maximum entropy
Markov models for information extraction and segmentation.
In Proc. ICML 2000, pages 591–598, Stanford, California,
2000.
T. P. Minka. Algorithms for maximum-likelihood logistic re-
gression. Technical Report 758, CMU Statistics Department,
2001.
J. Nocedal and S. J. Wright. Numerical Optimization. Springer,
1999.
V. Punyakanok and D. Roth. The use of classifiers in sequential
inference. In NIPS 13, pages 995–1001. MIT Press, 2001.
L. A. Ramshaw and M. P. Marcus. Text chunking using
transformation-based learning. In Proc. Third Workshop on
Very Large Corpora. ACL, 1995.
A. Ratnaparkhi. A maximum entropy model for part-of-speech
tagging. In Proc. EMNLP, New Brunswick, New Jersey,
1996. ACL.
A. Ratnaparkhi. A linear observed time statistical parser
based on maximum entropy models. In C. Cardie and
R. Weischedel, editors, EMNLP-2. ACL, 1997.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T.
Maxwell III, and M. Johnson. Parsing the Wall Street Journal
using a lexical-functional grammar and discriminative esti-
mation techniques. In Proc. 40th ACL, 2002.
E. F. T. K. Sang. Memory-based shallow parsing. Journal of
Machine Learning Research, 2:559–594, 2002.
J. R. Shewchuk. An introduction to the conjugate gradient
method without the agonizing pain, 1994. URL http://
www-2.cs.cmu.edu/˜jrs/jrspapers.html#cg.
B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilis-
tic models for relational data. In Eighteenth Conference on
Uncertainty in Artificial Intelligence, 2002.
E. F. Tjong Kim Sang and S. Buchholz. Introduction to the
CoNLL-2000 shared task: Chunking. In Proc. CoNLL-2000,
pages 127–132, 2000.
H. Wallach. Efficient training of conditional random fields. In
Proc. 6th Annual CLUK Research Colloquium, 2002.
A. Yeh. More accurate tests for the statistical significance of
result differences. In COLING-2000, pages 947–953, Saar-
bruecken, Germany, 2000.
T. Zhang, F. Damerau, and D. Johnson. Text chunking based
on a generalization of winnow. Journal of Machine Learning
Research, 2:615–637, 2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.677694">
<note confidence="0.913606">Proceedings of HLT-NAACL 2003 Main Papers , pp. 134-141 Edmonton, May-June 2003</note>
<title confidence="0.876906">Shallow Parsing with Conditional Random Fields</title>
<author confidence="0.996545">Fei Sha</author>
<author confidence="0.996545">Fernando</author>
<affiliation confidence="0.9996775">Department of Computer and Information University of</affiliation>
<address confidence="0.999352">200 South 33rd Street, Philadelphia, PA</address>
<email confidence="0.999367">(feisha|pereira)@cis.upenn.edu</email>
<abstract confidence="0.9997889">Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks. In</title>
<date>1991</date>
<editor>R. Berwick, S. Abney, and C. Tenny, editors, Principle-based Parsing.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="1631" citStr="Abney, 1991" startWordPosition="240" endWordPosition="241">aining methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models. 1 Introduction Sequence analysis tasks in language and biology are often described as mappings from input sequences to sequences of labels encoding the analysis. In language processing, examples of such tasks include part-of-speech tagging, named-entity recognition, and the task we shall focus on here, shallow parsing. Shallow parsing identifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or information extraction (Abney, 1991). The paradigmatic shallowparsing problem is NP chunking, which finds the nonrecursive cores of noun phrases called base NPs. The pioneering work of Ramshaw and Marcus (1995) introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics. The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. Most previous work used two main machine-learning approaches to sequence labeling. The first approach relies on k-order generative probabilistic </context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>S. Abney. Parsing by chunks. In R. Berwick, S. Abney, and C. Tenny, editors, Principle-based Parsing. Kluwer Academic Publishers, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Boosting applied to tagging and PP attachment.</title>
<date>1999</date>
<booktitle>In Proc. EMNLP-VLC,</booktitle>
<publisher>ACL.</publisher>
<location>New Brunswick, New Jersey,</location>
<contexts>
<context position="3621" citStr="Abney et al., 1999" startWordPosition="537" endWordPosition="541">ence as well as the surrounding labels, since the inference problem for the corresponding graphical model would be intractable. Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models. The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is &apos;Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method. not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is alw</context>
</contexts>
<marker>Abney, Schapire, Singer, 1999</marker>
<rawString>S. Abney, R. E. Schapire, and Y. Singer. Boosting applied to tagging and PP attachment. In Proc. EMNLP-VLC, New Brunswick, New Jersey, 1999. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11472" citStr="Berger et al., 1996" startWordPosition="1884" endWordPosition="1887">of the Hessian is dim()2, leading to unacceptable space and time requirements for the inversion. In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian. However in our case the Hessian has the form H def 2L Ha = a �= i �La = [ · F(yk,xk) − logZa(xk)] k 2 − 22 + const with gradient �= − {E [F(Y , xk) × F(Y , xk)] k La = −EF(Y , xk) × EF(Y , xk)}  [F(yk, xk) − Epa(Y |.k)F(Y , xk)] − 2 3 Training Methods Lafferty et al. (2001) used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language (Berger et al., 1996; Della Pietra et al., 1997). Those methods are very simple and guaranteed to converge, but as Minka (2001) and Malouf (2002) showed for classification, their convergence is much slower than that of general-purpose convex where the expectations are taken with respect to pa(Y |xk). Therefore, every Hessian element, including the diagonal ones, involve the expectation of a product of global feature values. Unfortunately, computing those expectations is quadratic on sequence length, as the forward-backward algorithm can only compute expectations of quantities that are additive along label sequenc</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1), 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>R L Schwartz</author>
<author>R M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<pages>211--231</pages>
<contexts>
<context position="2417" citStr="Bikel et al., 1999" startWordPosition="358" endWordPosition="361"> (1995) introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics. The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. Most previous work used two main machine-learning approaches to sequence labeling. The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) (Freitag and McCallum, 2000; Kupiec, 1992) or multilevel Markov models (Bikel et al., 1999). The second approach views the sequence labeling problem as a sequence of classification problems, one for each of the labels in the sequence. The classification result at each position may depend on the whole input and on the previous k classifications. 1 The generative approach provides well-understood training and decoding algorithms for HMMs and more general graphical models. However, effective generative models require stringent conditional independence assumptions. For instance, it is not practical to make the label at a given position depend on a window on the input sequence as well as</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. An algorithm that learns what’s in a name. Machine Learning, 34: 211–231, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bottou</author>
</authors>
<title>Une Approche th´eorique de l’Apprentissage Connexionniste: Applications a` la Reconnaissance de la Parole.</title>
<date>1991</date>
<tech>PhD thesis,</tech>
<institution>Universit´e de</institution>
<location>Paris XI,</location>
<contexts>
<context position="4555" citStr="Bottou, 1991" startWordPosition="686" endWordPosition="687">is &apos;Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method. not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is always the case in practice. However, since sequential classifiers are trained to make the best local decision, unlike generative models they cannot trade off decisions at different positions against each other. In other words, sequential classifiers are myopic about the impact of their current decision on later decisions (Bottou, 1991; Lafferty et al., 2001). This forced the best sequential classifier systems to resort to heuristic combinations of forward-moving and backward-moving sequential classifiers (Kudo and Matsumoto, 2001). Conditional random fields (CRFs) bring together the best of generative and classification models. Like classification models, they can accommodate many statistically correlated features of the inputs, and they are trained discriminatively. But like generative models, they can trade off decisions at different sequence positions to obtain a globally optimal labeling. Lafferty et al. (2001) showed </context>
</contexts>
<marker>Bottou, 1991</marker>
<rawString>L. Bottou. Une Approche th´eorique de l’Apprentissage Connexionniste: Applications a` la Reconnaissance de la Parole. PhD thesis, Universit´e de Paris XI, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<contexts>
<context position="4021" citStr="Brill, 1995" startWordPosition="600" endWordPosition="601">atures, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is &apos;Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method. not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is always the case in practice. However, since sequential classifiers are trained to make the best local decision, unlike generative models they cannot trade off decisions at different positions against each other. In other words, sequential classifiers are myopic about the impact of their current decision on later decisions (Bottou, 1991; Lafferty et al., 2001). This forced the best sequential classifi</context>
<context position="15891" citStr="Brill (1995)" startWordPosition="2597" endWordPosition="2598">uld be labeled BIIBIIOBOBIIO. 4.1 Data Preparation NP chunking results have been reported on two slightly different data sets: the original RM data set of Ramshaw and Marcus (1995), and the modified CoNLL-2000 version of Tjong Kim Sang and Buchholz (2000). Although the chunk tags in the RM and CoNLL-2000 are somewhat different, we found no significant accuracy differences between models trained on these two data sets. Therefore, all our results are reported on the CoNLL-2000 data set. We also used a development test set, provided by Michael Collins, derived from WSJ section 21 tagged with the Brill (1995) POS tagger. 4.2 CRFs for Shallow Parsing Our chunking CRFs have asecond-order Markov dependency between chunk tags. This is easily encoded by making the CRF labels pairs of consecutive chunk tags. That is, the label at position i is yi = where ci is the chunk tag of word i, one of B, or I. Since B must be used to start a chunk, the label OI is impossible. In addition, successive labels are constrained: = yi = and = These contraints on the model topology are enforced by giving appropriate features a weight of forcing all the forbidden labelings to have zero probability. Our choice of features </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging. Computational Linguistics, 21:543–565, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108,</tech>
<institution>Carnegie Mellon University,</institution>
<contexts>
<context position="9240" citStr="Chen and Rosenfeld, 1999" startWordPosition="1531" endWordPosition="1534"> i) Let f be any local feature, fi[y, y] = f(y, y, x, i), F(y, x) = Ei f(yi−1, yi, x, i), and let  denote component-wise matrix product. Then Ep,,(Y |.)F(Y , x) = � pa(y|x)F(y, x) y i−1(fi  Mi) i Za(x) Za(x) = n · 1 where i and i the forward and backward state-cost vectors defined by � i−1Mi 0 &lt; i  n i = 1 i = 0  T = J M,&apos;1&apos; i+1 Qi 1  i &lt; n T+1 Z 11 1 i = n Therefore, we can use a forward pass to compute the i and a backward bass to compute the i and accumulate the feature expectations. To avoid overfitting, we penalize the likelihood with a spherical Gaussian weight prior (Chen and Rosenfeld, 1999): optimization algorithms when many correlated features are involved. Concurrently with the present work, Wallach (2002) tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages over iterative scaling on a small shallow parsing problem. Our work shows that preconditioned conjugate-gradient (CG) (Shewchuk, 1994) or limited-memory quasi-Newton (L-BFGS) (Nocedal and Wright, 1999) perform comparably on very large problems (around 3.8 million features). We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff,</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>S. F. Chen and R. Rosenfeld. A Gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, Carnegie Mellon University, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP</booktitle>
<publisher>ACL,</publisher>
<contexts>
<context position="5863" citStr="Collins (2002)" startWordPosition="874" endWordPosition="875">f-speech tagging task. In the present work, we show that CRFs beat all reported single-model NP chunking results on the standard evaluation dataset, and are statistically indistinguishable from the previous best performer, a voting arrangement of 24 forward- and backward-looking support-vector classifiers (Kudo and Matsumoto, 2001). To obtain these results, we had to abandon the original iterative scaling CRF training algorithm for convex optimization algorithms with better convergence properties. We provide detailed comparisons between training methods. The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron. 2 Conditional Random Fields We focus here on conditional random fields on sequences, although the notion can be used more generally (Lafferty et al., 2001; Taskar et al., 2002). Such CRFs define conditional probability distributions p(Y |X) of label sequences given input sequences. We assume that the random variable sequences X and Y have the same length, and use x = x1 · · · xn and y = y1 · · · yn for the generic input sequence and label sequence, respectively. A CRF on (</context>
<context position="9916" citStr="Collins, 2002" startWordPosition="1624" endWordPosition="1625">volved. Concurrently with the present work, Wallach (2002) tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages over iterative scaling on a small shallow parsing problem. Our work shows that preconditioned conjugate-gradient (CG) (Shewchuk, 1994) or limited-memory quasi-Newton (L-BFGS) (Nocedal and Wright, 1999) perform comparably on very large problems (around 3.8 million features). We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002). All algorithms except voted perceptron maximize the penalized loglikelihood:  = arg maxa La. However, for ease of exposition, this discussion of training methods uses the unpenalized log-likelihood La. 3.1 Preconditioned Conjugate Gradient Conjugate-gradient (CG) methods have been shown to be very effective in linear and non-linear optimization (Shewchuk, 1994). Instead of searching along the gradient, conjugate gradient searches along a carefully chosen linear combination of the gradient and the previous search direction. CG methods can be accelerated by linearly transforming the variabl</context>
<context position="14101" citStr="Collins, 2002" startWordPosition="2290" endWordPosition="2291">roximations. An earlier study indicates that L-BFGS performs well in maximum-entropy classifier training (Malouf, 2002). There is no theoretical guidance on how much information from previous steps we should keep to obtain sufficiently accurate curvature estimates. In our experiments, storing 3 to 10 pairs of previous gradients and updates worked well, so the extra memory required over preconditioned CG was modest. A more detailed description of this method can be found elsewhere (Nocedal and Wright, 1999). 3.3 Voted Perceptron Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model. More precisely, for each training instance the method computes a weight update t+1 = t + F(yk, xk) − F(ˆyk, xk) (3) in which ˆyk is the Viterbi path t · F (y,xk) Like the familiar perceptron algorithm, this algorithm repeatedly sweeps over the training instances, updating the weight vector as it considers each instance. Instead of taking just the final weight vector, the voted perceptron algo</context>
<context position="21400" citStr="Collins, 2002" startWordPosition="3607" endWordPosition="3608">ver, bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements (Gillick and Cox, 1989). Model F score SVM combination 94.39% (Kudo and Matsumoto, 2001) CRF 94.38% Generalized winnow 93.89% (Zhang et al., 2002) Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs,designed to handle millions of features, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0. Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al., 2000) with the same efficient feature encoding. GIS, CG, and L-BFGS were used to train CRFs and MEMMs. 5.1 F Scores Table 2 gives representative NP chunking F scores for previous work and for our best model, with the complete set of 3.8 million features. The last row of the table gives the score for an MEMM trained with the mixed CG method using an approximate preconditioner. The published F score for voted perceptron is 93.53% with a different feature set (Collins, 2002). The improved result given here is for the supported feature set; the complete feature set giv</context>
<context position="27398" citStr="Collins (2002)" startWordPosition="4562" endWordPosition="4563">minative (log-)linear classifiers, and do NP chunking as well as or better than “ad hoc” classifier combinations, which were the most accurate approach until now. In a longer version of this work we will also describe shallow parsing results for other phrase types. There is no reason why the same techniques cannot be used equally successfully for the other types or for other related tasks, such as POS tagging or named-entity recognition. On the machine-learning side, it would be interesting to generalize the ideas of large-margin classification to sequence models, strengthening the results of Collins (2002) and leading to new optimal training algorithms with stronger guarantees against overfitting. On the application side, (log-)linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoiding the label bias problem that may have hindered earlier classifier-based parsers (Ratnaparkhi, 1997). However, work in that direction has so far addressed only parse reranking (Collins and Duffy, 2002; Riezler et al., 2002). Full discriminative parser training faces significant algorithmic</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP 2002. ACL, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. 40th ACL,</booktitle>
<contexts>
<context position="27908" citStr="Collins and Duffy, 2002" startWordPosition="4633" endWordPosition="4636">neralize the ideas of large-margin classification to sequence models, strengthening the results of Collins (2002) and leading to new optimal training algorithms with stronger guarantees against overfitting. On the application side, (log-)linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoiding the label bias problem that may have hindered earlier classifier-based parsers (Ratnaparkhi, 1997). However, work in that direction has so far addressed only parse reranking (Collins and Duffy, 2002; Riezler et al., 2002). Full discriminative parser training faces significant algorithmic challenges in the relationship between parsing alternatives and feature values (Geman and Johnson, 2002) and in computing feature expectations. Acknowledgments John Lafferty and Andrew McCallum worked with the second author on developing CRFs. McCallum helped by the second author implemented the first conjugategradient trainer for CRFs, which convinced us that training of large CRFs on large datasets would be practical. Michael Collins helped us reproduce his generalized per# of Forward−backward evaluati</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proc. 40th ACL, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>43</volume>
<pages>5--1470</pages>
<contexts>
<context position="9846" citStr="Darroch and Ratcliff, 1972" startWordPosition="1613" endWordPosition="1617"> and Rosenfeld, 1999): optimization algorithms when many correlated features are involved. Concurrently with the present work, Wallach (2002) tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages over iterative scaling on a small shallow parsing problem. Our work shows that preconditioned conjugate-gradient (CG) (Shewchuk, 1994) or limited-memory quasi-Newton (L-BFGS) (Nocedal and Wright, 1999) perform comparably on very large problems (around 3.8 million features). We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002). All algorithms except voted perceptron maximize the penalized loglikelihood:  = arg maxa La. However, for ease of exposition, this discussion of training methods uses the unpenalized log-likelihood La. 3.1 Preconditioned Conjugate Gradient Conjugate-gradient (CG) methods have been shown to be very effective in linear and non-linear optimization (Shewchuk, 1994). Instead of searching along the gradient, conjugate gradient searches along a carefully chosen linear combination of the gradient and the previous search directi</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43 (5):1470–1480, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE PAMI,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="11500" citStr="Pietra et al., 1997" startWordPosition="1889" endWordPosition="1892">leading to unacceptable space and time requirements for the inversion. In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian. However in our case the Hessian has the form H def 2L Ha = a �= i �La = [ · F(yk,xk) − logZa(xk)] k 2 − 22 + const with gradient �= − {E [F(Y , xk) × F(Y , xk)] k La = −EF(Y , xk) × EF(Y , xk)}  [F(yk, xk) − Epa(Y |.k)F(Y , xk)] − 2 3 Training Methods Lafferty et al. (2001) used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language (Berger et al., 1996; Della Pietra et al., 1997). Those methods are very simple and guaranteed to converge, but as Minka (2001) and Malouf (2002) showed for classification, their convergence is much slower than that of general-purpose convex where the expectations are taken with respect to pa(Y |xk). Therefore, every Hessian element, including the diagonal ones, involve the expectation of a product of global feature values. Unfortunately, computing those expectations is quadratic on sequence length, as the forward-backward algorithm can only compute expectations of quantities that are additive along label sequences. We solve both problems b</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing features of random fields. IEEE PAMI, 19(4):380–393, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman &amp; Hall/CRC,</title>
<date>1993</date>
<contexts>
<context position="20767" citStr="Efron and Tibshirani, 1993" startWordPosition="3505" endWordPosition="3509">ng, and estimate the statistical significance of performance differences. Unfortunately, reported results sometimes leave out details needed for accurate comparisons. We report F scores for comparison with previous work, but we also give statistical significance estimates using McNemar’s test for those methods that we evaluated directly. Testing the significance of F scores is tricky because the wrong chunks generated by two chunkers are not directly comparable. Yeh (2000) examined randomized tests for estimating the significance of F scores, and in particular the bootstrap over the test set (Efron and Tibshirani, 1993; Sang, 2002). However, bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements (Gillick and Cox, 1989). Model F score SVM combination 94.39% (Kudo and Matsumoto, 2001) CRF 94.38% Generalized winnow 93.89% (Zhang et al., 2002) Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs,designed to handle millions of features, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0. Minor variants support</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman &amp; Hall/CRC, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
<author>A McCallum</author>
</authors>
<title>Information extraction with HMM structures learned by stochastic optimization.</title>
<date>2000</date>
<booktitle>In Proc. AAAI</booktitle>
<contexts>
<context position="2353" citStr="Freitag and McCallum, 2000" startWordPosition="348" endWordPosition="351">noun phrases called base NPs. The pioneering work of Ramshaw and Marcus (1995) introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics. The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. Most previous work used two main machine-learning approaches to sequence labeling. The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) (Freitag and McCallum, 2000; Kupiec, 1992) or multilevel Markov models (Bikel et al., 1999). The second approach views the sequence labeling problem as a sequence of classification problems, one for each of the labels in the sequence. The classification result at each position may depend on the whole input and on the previous k classifications. 1 The generative approach provides well-understood training and decoding algorithms for HMMs and more general graphical models. However, effective generative models require stringent conditional independence assumptions. For instance, it is not practical to make the label at a gi</context>
</contexts>
<marker>Freitag, McCallum, 2000</marker>
<rawString>D. Freitag and A. McCallum. Information extraction with HMM structures learned by stochastic optimization. In Proc. AAAI 2000, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>M Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In Proc. 40th ACL,</booktitle>
<contexts>
<context position="28103" citStr="Geman and Johnson, 2002" startWordPosition="4660" endWordPosition="4663">overfitting. On the application side, (log-)linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoiding the label bias problem that may have hindered earlier classifier-based parsers (Ratnaparkhi, 1997). However, work in that direction has so far addressed only parse reranking (Collins and Duffy, 2002; Riezler et al., 2002). Full discriminative parser training faces significant algorithmic challenges in the relationship between parsing alternatives and feature values (Geman and Johnson, 2002) and in computing feature expectations. Acknowledgments John Lafferty and Andrew McCallum worked with the second author on developing CRFs. McCallum helped by the second author implemented the first conjugategradient trainer for CRFs, which convinced us that training of large CRFs on large datasets would be practical. Michael Collins helped us reproduce his generalized per# of Forward−backward evaluations F score 0.95 0.85 0.75 0.65 0.55 0.45 0 50 100 150 200 250 300 350 400 450 500 0.9 0.8 0.7 0.6 0.5 Preconditioned CG CG w/o Preconditioner GIS cepton results and compare his method with ours.</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>S. Geman and M. Johnson. Dynamic programming for parsing and estimation of stochastic unification-based grammars. In Proc. 40th ACL, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gillick</author>
<author>S Cox</author>
</authors>
<title>Some statistical issues in the compairson of speech recognition algorithms.</title>
<date>1989</date>
<booktitle>In International Conference on Acoustics Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>532--535</pages>
<contexts>
<context position="20968" citStr="Gillick and Cox, 1989" startWordPosition="3536" endWordPosition="3539"> previous work, but we also give statistical significance estimates using McNemar’s test for those methods that we evaluated directly. Testing the significance of F scores is tricky because the wrong chunks generated by two chunkers are not directly comparable. Yeh (2000) examined randomized tests for estimating the significance of F scores, and in particular the bootstrap over the test set (Efron and Tibshirani, 1993; Sang, 2002). However, bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements (Gillick and Cox, 1989). Model F score SVM combination 94.39% (Kudo and Matsumoto, 2001) CRF 94.38% Generalized winnow 93.89% (Zhang et al., 2002) Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs,designed to handle millions of features, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0. Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al., 2000) with the same efficient feature encoding. GIS, CG, and L-BFGS were used to train CRFs and MEMMs. 5.1 F Scores Table 2 gives represent</context>
<context position="26077" citStr="Gillick and Cox, 1989" startWordPosition="4357" endWordPosition="4360">mparison of CG Methods to GIS # of Forward−backward evaluations (b) Ga: CG (precond., plain), GIS 0 −5000 −30000 −35000 6 56 106 156 206 256 Penalized Log−likelihood −10000 −15000 −20000 −25000 Preconditioned CG Mixed CG Training L−BFGS Penalized Log−likelihood −100000 −120000 −140000 −160000 −180000 −200000 0 50 100 150 200 250 300 350 400 450 500 −20000 −40000 −60000 −80000 0 Preconditioned CG CG w/o Preconditioner GIS Figure 2: Training convergence for various methods Comparison of CG Methods to GIS Figure 3: Test F scores vs. training time such test is McNemar test on paired observations (Gillick and Cox, 1989). With McNemar’s test, we compare the correctness of the labeling decisions of two models. The null hypothesis is that the disagreements (correct vs. incorrect) are due to chance. Table 4 summarizes the results of tests between the models for which we had labeling decisions. These tests suggest that MEMMs are significantly less accurate, but that there are no significant differences in accuracy among the other models. 6 Conclusions We have shown that (log-)linear sequence labeling models trained discriminatively with general-purpose optimization methods are a simple, competitive solution to le</context>
</contexts>
<marker>Gillick, Cox, 1989</marker>
<rawString>L. Gillick and S. Cox. Some statistical issues in the compairson of speech recognition algorithms. In International Conference on Acoustics Speech and Signal Processing, volume 1, pages 532–535, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hammersley</author>
<author>P Clifford</author>
</authors>
<title>Markov fields on finite graphs and lattices.</title>
<date>1971</date>
<note>Unpublished manuscript,</note>
<contexts>
<context position="7675" citStr="Hammersley and Clifford, 1971" startWordPosition="1215" endWordPosition="1218">nput, or be non-zero only at some positions, for instance features that pick out the first or last labels. The CRF’s global feature vector for input sequence x and label sequence y is given by F(y, x) = � f(y, x, i) i where i ranges over input positions. The conditional probability distribution defined by the CRF is then exp · F(Y ,X) p(Y |X) = (1) Z(X) where �Z(x) = exp  · F(y, x) y Any positive conditional distribution p(Y |X) that obeys the Markov property p(Yi|{Yj}j=i,X) = p(Yi|Yi−1,Yi+1,X) can be written in the form (1) for appropriate choice of feature functions and weight vector (Hammersley and Clifford, 1971). The most probable label sequence for input sequence x is yˆ = arg max p(y|x) = arg max  · F(y,x) y y because Z(x) does not depend on y. F(y, x) decomposes into a sum of terms for consecutive pairs of labels, so the most likely y can be found with the Viterbi algorithm. We train a CRF by maximizing the log-likelihood of a given training set T = {(xk, yk)}Nk=1, which we assume fixed for the rest of this section: L = Ek logp(yk|xk) Ek [ · F(yk, xk) − logZ(xk)] To perform this optimization, we seek the zero of the gradient L = E [F(yk, xk) − Epa(Y |xk)F(Y , xk)] (2) k In words, the maxi</context>
</contexts>
<marker>Hammersley, Clifford, 1971</marker>
<rawString>J. Hammersley and P. Clifford. Markov fields on finite graphs and lattices. Unpublished manuscript, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proc. NAACL</booktitle>
<publisher>ACL,</publisher>
<contexts>
<context position="3677" citStr="Kudo and Matsumoto, 2001" startWordPosition="545" endWordPosition="548">inference problem for the corresponding graphical model would be intractable. Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models. The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is &apos;Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method. not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is always the case in practice. However, since sequential clas</context>
<context position="5582" citStr="Kudo and Matsumoto, 2001" startWordPosition="833" endWordPosition="836"> and they are trained discriminatively. But like generative models, they can trade off decisions at different sequence positions to obtain a globally optimal labeling. Lafferty et al. (2001) showed that CRFs beat related classification models as well as HMMs on synthetic data and on a part-of-speech tagging task. In the present work, we show that CRFs beat all reported single-model NP chunking results on the standard evaluation dataset, and are statistically indistinguishable from the previous best performer, a voting arrangement of 24 forward- and backward-looking support-vector classifiers (Kudo and Matsumoto, 2001). To obtain these results, we had to abandon the original iterative scaling CRF training algorithm for convex optimization algorithms with better convergence properties. We provide detailed comparisons between training methods. The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron. 2 Conditional Random Fields We focus here on conditional random fields on sequences, although the notion can be used more generally (Lafferty et al., 2001; Taskar et al., 2002). Such CRFs define c</context>
<context position="21033" citStr="Kudo and Matsumoto, 2001" startWordPosition="3546" endWordPosition="3549">mates using McNemar’s test for those methods that we evaluated directly. Testing the significance of F scores is tricky because the wrong chunks generated by two chunkers are not directly comparable. Yeh (2000) examined randomized tests for estimating the significance of F scores, and in particular the bootstrap over the test set (Efron and Tibshirani, 1993; Sang, 2002). However, bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements (Gillick and Cox, 1989). Model F score SVM combination 94.39% (Kudo and Matsumoto, 2001) CRF 94.38% Generalized winnow 93.89% (Zhang et al., 2002) Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs,designed to handle millions of features, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0. Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al., 2000) with the same efficient feature encoding. GIS, CG, and L-BFGS were used to train CRFs and MEMMs. 5.1 F Scores Table 2 gives representative NP chunking F scores for previous work and for our best mod</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. Chunking with support vector machines. In Proc. NAACL 2001. ACL, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<volume>6</volume>
<contexts>
<context position="2368" citStr="Kupiec, 1992" startWordPosition="352" endWordPosition="353">. The pioneering work of Ramshaw and Marcus (1995) introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics. The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. Most previous work used two main machine-learning approaches to sequence labeling. The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) (Freitag and McCallum, 2000; Kupiec, 1992) or multilevel Markov models (Bikel et al., 1999). The second approach views the sequence labeling problem as a sequence of classification problems, one for each of the labels in the sequence. The classification result at each position may depend on the whole input and on the previous k classifications. 1 The generative approach provides well-understood training and decoding algorithms for HMMs and more general graphical models. However, effective generative models require stringent conditional independence assumptions. For instance, it is not practical to make the label at a given position de</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>J. Kupiec. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6:225–242, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML-01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4579" citStr="Lafferty et al., 2001" startWordPosition="688" endWordPosition="691">d Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method. not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is always the case in practice. However, since sequential classifiers are trained to make the best local decision, unlike generative models they cannot trade off decisions at different positions against each other. In other words, sequential classifiers are myopic about the impact of their current decision on later decisions (Bottou, 1991; Lafferty et al., 2001). This forced the best sequential classifier systems to resort to heuristic combinations of forward-moving and backward-moving sequential classifiers (Kudo and Matsumoto, 2001). Conditional random fields (CRFs) bring together the best of generative and classification models. Like classification models, they can accommodate many statistically correlated features of the inputs, and they are trained discriminatively. But like generative models, they can trade off decisions at different sequence positions to obtain a globally optimal labeling. Lafferty et al. (2001) showed that CRFs beat related c</context>
<context position="6140" citStr="Lafferty et al., 2001" startWordPosition="920" endWordPosition="924">d-looking support-vector classifiers (Kudo and Matsumoto, 2001). To obtain these results, we had to abandon the original iterative scaling CRF training algorithm for convex optimization algorithms with better convergence properties. We provide detailed comparisons between training methods. The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron. 2 Conditional Random Fields We focus here on conditional random fields on sequences, although the notion can be used more generally (Lafferty et al., 2001; Taskar et al., 2002). Such CRFs define conditional probability distributions p(Y |X) of label sequences given input sequences. We assume that the random variable sequences X and Y have the same length, and use x = x1 · · · xn and y = y1 · · · yn for the generic input sequence and label sequence, respectively. A CRF on (X, Y ) is specified by a vector f of local features and a corresponding weight vector . Each local feature is either a state feature s(y, x, i) or a transition feature t(y, y, x, i), where y, y are labels, x an input sequence, and i an input position. To make the notation m</context>
<context position="11330" citStr="Lafferty et al. (2001)" startWordPosition="1863" endWordPosition="1866">ctive function, so the inverse of Hessian is reasonable preconditioner. However, this is not applicable to CRFs for two reasons. First, the size of the Hessian is dim()2, leading to unacceptable space and time requirements for the inversion. In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian. However in our case the Hessian has the form H def 2L Ha = a �= i �La = [ · F(yk,xk) − logZa(xk)] k 2 − 22 + const with gradient �= − {E [F(Y , xk) × F(Y , xk)] k La = −EF(Y , xk) × EF(Y , xk)}  [F(yk, xk) − Epa(Y |.k)F(Y , xk)] − 2 3 Training Methods Lafferty et al. (2001) used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language (Berger et al., 1996; Della Pietra et al., 1997). Those methods are very simple and guaranteed to converge, but as Minka (2001) and Malouf (2002) showed for classification, their convergence is much slower than that of general-purpose convex where the expectations are taken with respect to pa(Y |xk). Therefore, every Hessian element, including the diagonal ones, involve the expectation of a product of global feature values. Unfortunately, computing those expectations is qua</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML-01, pages 282–289, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proc. CoNLL-2002,</booktitle>
<contexts>
<context position="11597" citStr="Malouf (2002)" startWordPosition="1908" endWordPosition="1909">to use instead the (inverse of) the diagonal of the Hessian. However in our case the Hessian has the form H def 2L Ha = a �= i �La = [ · F(yk,xk) − logZa(xk)] k 2 − 22 + const with gradient �= − {E [F(Y , xk) × F(Y , xk)] k La = −EF(Y , xk) × EF(Y , xk)}  [F(yk, xk) − Epa(Y |.k)F(Y , xk)] − 2 3 Training Methods Lafferty et al. (2001) used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language (Berger et al., 1996; Della Pietra et al., 1997). Those methods are very simple and guaranteed to converge, but as Minka (2001) and Malouf (2002) showed for classification, their convergence is much slower than that of general-purpose convex where the expectations are taken with respect to pa(Y |xk). Therefore, every Hessian element, including the diagonal ones, involve the expectation of a product of global feature values. Unfortunately, computing those expectations is quadratic on sequence length, as the forward-backward algorithm can only compute expectations of quantities that are additive along label sequences. We solve both problems by discarding the off-diagonal terms and approximating expectation of the square of a global featu</context>
<context position="13606" citStr="Malouf, 2002" startWordPosition="2212" endWordPosition="2213">e secondorder (curvature) information to find search directions. As discussed in the previous section, it is not practical to obtain exact curvature information for CRF training. Limited-memory BFGS (L-BFGS) is a second-order method that estimates the curvature numerically from previous gradients and updates, avoiding the need for an exact Hessian inverse computation. Compared with preconditioned CG, L-BFGS can also handle large-scale problems but does not require a specialized Hessian approximations. An earlier study indicates that L-BFGS performs well in maximum-entropy classifier training (Malouf, 2002). There is no theoretical guidance on how much information from previous steps we should keep to obtain sufficiently accurate curvature estimates. In our experiments, storing 3 to 10 pairs of previous gradients and updates worked well, so the extra memory required over preconditioned CG was modest. A more detailed description of this method can be found elsewhere (Nocedal and Wright, 1999). 3.3 Voted Perceptron Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the sa</context>
<context position="23004" citStr="Malouf, 2002" startWordPosition="3869" endWordPosition="3870">teration involves a line search that may require several forward-backward procedures (typically between 4 and 5 in our experiments), we plot the progress of penalized log-likelihood Ga with respect to the number of forward-backward evaluations. The objective function increases rapidly, achieving close proximity to the maximum in a few iterations (typically 10). In contrast, GIS training increases Ga rather slowly, never reaching the value achieved by CG. The relative slowness of iterative scaling is also documented in a recent evaluation of training methods for maximum-entropy classification (Malouf, 2002). In theory, GIS would eventually converge to the Ga optimum, but in practice convergence may be so slow that Ga improvements may fall below numerical accuracy, falsely indicating convergence. training method time F score Ga Precond. CG 130 94.19% -2968 Mixed CG 540 94.20% -2990 Plain CG 648 94.04% -2967 L-BFGS 84 94.19% -2948 GIS 3700 93.55% -5668 Table 3: Runtime for various training methods null hypothesis p-value CRF vs. SVM 0.469 CRF vs. MEMM 0.00109 CRF vs. voted perceptron 0.116 MEMM vs. voted perceptron 0.0734 Table 4: McNemar’s tests on labeling disagreements Mixed CG training converg</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>R. Malouf. A comparison of algorithms for maximum entropy parameter estimation. In Proc. CoNLL-2002, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. ICML</booktitle>
<pages>591--598</pages>
<location>Stanford, California,</location>
<contexts>
<context position="3482" citStr="McCallum et al., 2000" startWordPosition="517" endWordPosition="520">tional independence assumptions. For instance, it is not practical to make the label at a given position depend on a window on the input sequence as well as the surrounding labels, since the inference problem for the corresponding graphical model would be intractable. Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models. The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is &apos;Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classi</context>
<context position="21434" citStr="McCallum et al., 2000" startWordPosition="3611" endWordPosition="3614">n preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements (Gillick and Cox, 1989). Model F score SVM combination 94.39% (Kudo and Matsumoto, 2001) CRF 94.38% Generalized winnow 93.89% (Zhang et al., 2002) Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs,designed to handle millions of features, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0. Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al., 2000) with the same efficient feature encoding. GIS, CG, and L-BFGS were used to train CRFs and MEMMs. 5.1 F Scores Table 2 gives representative NP chunking F scores for previous work and for our best model, with the complete set of 3.8 million features. The last row of the table gives the score for an MEMM trained with the mixed CG method using an approximate preconditioner. The published F score for voted perceptron is 93.53% with a different feature set (Collins, 2002). The improved result given here is for the supported feature set; the complete feature set gives a slightly lower score of 94.07</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. Maximum entropy Markov models for information extraction and segmentation. In Proc. ICML 2000, pages 591–598, Stanford, California, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T P Minka</author>
</authors>
<title>Algorithms for maximum-likelihood logistic regression.</title>
<date>2001</date>
<tech>Technical Report 758, CMU Statistics Department,</tech>
<contexts>
<context position="11579" citStr="Minka (2001)" startWordPosition="1905" endWordPosition="1906">ns, it is common to use instead the (inverse of) the diagonal of the Hessian. However in our case the Hessian has the form H def 2L Ha = a �= i �La = [ · F(yk,xk) − logZa(xk)] k 2 − 22 + const with gradient �= − {E [F(Y , xk) × F(Y , xk)] k La = −EF(Y , xk) × EF(Y , xk)}  [F(yk, xk) − Epa(Y |.k)F(Y , xk)] − 2 3 Training Methods Lafferty et al. (2001) used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language (Berger et al., 1996; Della Pietra et al., 1997). Those methods are very simple and guaranteed to converge, but as Minka (2001) and Malouf (2002) showed for classification, their convergence is much slower than that of general-purpose convex where the expectations are taken with respect to pa(Y |xk). Therefore, every Hessian element, including the diagonal ones, involve the expectation of a product of global feature values. Unfortunately, computing those expectations is quadratic on sequence length, as the forward-backward algorithm can only compute expectations of quantities that are additive along label sequences. We solve both problems by discarding the off-diagonal terms and approximating expectation of the square</context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>T. P. Minka. Algorithms for maximum-likelihood logistic regression. Technical Report 758, CMU Statistics Department, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
<author>S J Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>1999</date>
<publisher>Springer,</publisher>
<contexts>
<context position="9677" citStr="Nocedal and Wright, 1999" startWordPosition="1589" endWordPosition="1592"> backward bass to compute the i and accumulate the feature expectations. To avoid overfitting, we penalize the likelihood with a spherical Gaussian weight prior (Chen and Rosenfeld, 1999): optimization algorithms when many correlated features are involved. Concurrently with the present work, Wallach (2002) tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages over iterative scaling on a small shallow parsing problem. Our work shows that preconditioned conjugate-gradient (CG) (Shewchuk, 1994) or limited-memory quasi-Newton (L-BFGS) (Nocedal and Wright, 1999) perform comparably on very large problems (around 3.8 million features). We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002). All algorithms except voted perceptron maximize the penalized loglikelihood:  = arg maxa La. However, for ease of exposition, this discussion of training methods uses the unpenalized log-likelihood La. 3.1 Preconditioned Conjugate Gradient Conjugate-gradient (CG) methods have been shown to be very effective in linear and non-linear optimization (Shewchuk</context>
<context position="13998" citStr="Nocedal and Wright, 1999" startWordPosition="2274" endWordPosition="2277">with preconditioned CG, L-BFGS can also handle large-scale problems but does not require a specialized Hessian approximations. An earlier study indicates that L-BFGS performs well in maximum-entropy classifier training (Malouf, 2002). There is no theoretical guidance on how much information from previous steps we should keep to obtain sufficiently accurate curvature estimates. In our experiments, storing 3 to 10 pairs of previous gradients and updates worked well, so the extra memory required over preconditioned CG was modest. A more detailed description of this method can be found elsewhere (Nocedal and Wright, 1999). 3.3 Voted Perceptron Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model. More precisely, for each training instance the method computes a weight update t+1 = t + F(yk, xk) − F(ˆyk, xk) (3) in which ˆyk is the Viterbi path t · F (y,xk) Like the familiar perceptron algorithm, this algorithm repeatedly sweeps over the training instances, updating the weight vector a</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In NIPS 13,</booktitle>
<pages>995--1001</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="3590" citStr="Punyakanok and Roth, 2001" startWordPosition="532" endWordPosition="535">n depend on a window on the input sequence as well as the surrounding labels, since the inference problem for the corresponding graphical model would be intractable. Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models. The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is &apos;Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method. not as closely tied to the accuracy metrics of interest if the actual data was not ge</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok and D. Roth. The use of classifiers in sequential inference. In NIPS 13, pages 995–1001. MIT Press, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proc. Third Workshop on Very Large Corpora. ACL,</booktitle>
<contexts>
<context position="1805" citStr="Ramshaw and Marcus (1995)" startWordPosition="266" endWordPosition="269">tasks in language and biology are often described as mappings from input sequences to sequences of labels encoding the analysis. In language processing, examples of such tasks include part-of-speech tagging, named-entity recognition, and the task we shall focus on here, shallow parsing. Shallow parsing identifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or information extraction (Abney, 1991). The paradigmatic shallowparsing problem is NP chunking, which finds the nonrecursive cores of noun phrases called base NPs. The pioneering work of Ramshaw and Marcus (1995) introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics. The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. Most previous work used two main machine-learning approaches to sequence labeling. The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) (Freitag and McCallum, 2000; Kupiec, 1992) or multilevel Markov models (Bikel e</context>
<context position="3972" citStr="Ramshaw and Marcus (1995)" startWordPosition="593" endWordPosition="596">quential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is &apos;Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method. not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is always the case in practice. However, since sequential classifiers are trained to make the best local decision, unlike generative models they cannot trade off decisions at different positions against each other. In other words, sequential classifiers are myopic about the impact of their current decision on later decisions (Bottou, 1991; Lafferty et al.</context>
<context position="14952" citStr="Ramshaw and Marcus (1995)" startWordPosition="2433" endWordPosition="2436"> training instance the method computes a weight update t+1 = t + F(yk, xk) − F(ˆyk, xk) (3) in which ˆyk is the Viterbi path t · F (y,xk) Like the familiar perceptron algorithm, this algorithm repeatedly sweeps over the training instances, updating the weight vector as it considers each instance. Instead of taking just the final weight vector, the voted perceptron algorithm takes the average of the t. Collins (2002) reported and we confirmed that this averaging reduces overfitting considerably. = arg max Y ˆyk 4 Shallow Parsing Figure 1 shows the base NPs in an example sentence. Following Ramshaw and Marcus (1995), the input to the NP consists of the words in a sentence annotated automatically with part-of-speech (POS) tags. The task is to label each word with a label indicating whether the word is outside a chunk starts a chunk (B), or continues a chunk (I). For example, the tokens in first li chunker chunker’s (O), ne of Figure 1 would be labeled BIIBIIOBOBIIO. 4.1 Data Preparation NP chunking results have been reported on two slightly different data sets: the original RM data set of Ramshaw and Marcus (1995), and the modified CoNLL-2000 version of Tjong Kim Sang and Buchholz (2000). Although the chu</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. A. Ramshaw and M. P. Marcus. Text chunking using transformation-based learning. In Proc. Third Workshop on Very Large Corpora. ACL, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. EMNLP,</booktitle>
<publisher>ACL.</publisher>
<location>New Brunswick, New Jersey,</location>
<contexts>
<context position="3502" citStr="Ratnaparkhi, 1996" startWordPosition="521" endWordPosition="522">umptions. For instance, it is not practical to make the label at a given position depend on a window on the input sequence as well as the surrounding labels, since the inference problem for the corresponding graphical model would be intractable. Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models. The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is &apos;Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based metho</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. A maximum entropy model for part-of-speech tagging. In Proc. EMNLP, New Brunswick, New Jersey, 1996. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<editor>In C. Cardie and R. Weischedel, editors, EMNLP-2.</editor>
<publisher>ACL,</publisher>
<contexts>
<context position="27808" citStr="Ratnaparkhi, 1997" startWordPosition="4619" endWordPosition="4620">agging or named-entity recognition. On the machine-learning side, it would be interesting to generalize the ideas of large-margin classification to sequence models, strengthening the results of Collins (2002) and leading to new optimal training algorithms with stronger guarantees against overfitting. On the application side, (log-)linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoiding the label bias problem that may have hindered earlier classifier-based parsers (Ratnaparkhi, 1997). However, work in that direction has so far addressed only parse reranking (Collins and Duffy, 2002; Riezler et al., 2002). Full discriminative parser training faces significant algorithmic challenges in the relationship between parsing alternatives and feature values (Geman and Johnson, 2002) and in computing feature expectations. Acknowledgments John Lafferty and Andrew McCallum worked with the second author on developing CRFs. McCallum helped by the second author implemented the first conjugategradient trainer for CRFs, which convinced us that training of large CRFs on large datasets would</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>A. Ratnaparkhi. A linear observed time statistical parser based on maximum entropy models. In C. Cardie and R. Weischedel, editors, EMNLP-2. ACL, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>T H King</author>
<author>R M Kaplan</author>
<author>R Crouch</author>
<author>J T Maxwell</author>
<author>M Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proc. 40th ACL,</booktitle>
<contexts>
<context position="27931" citStr="Riezler et al., 2002" startWordPosition="4637" endWordPosition="4640">ge-margin classification to sequence models, strengthening the results of Collins (2002) and leading to new optimal training algorithms with stronger guarantees against overfitting. On the application side, (log-)linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoiding the label bias problem that may have hindered earlier classifier-based parsers (Ratnaparkhi, 1997). However, work in that direction has so far addressed only parse reranking (Collins and Duffy, 2002; Riezler et al., 2002). Full discriminative parser training faces significant algorithmic challenges in the relationship between parsing alternatives and feature values (Geman and Johnson, 2002) and in computing feature expectations. Acknowledgments John Lafferty and Andrew McCallum worked with the second author on developing CRFs. McCallum helped by the second author implemented the first conjugategradient trainer for CRFs, which convinced us that training of large CRFs on large datasets would be practical. Michael Collins helped us reproduce his generalized per# of Forward−backward evaluations F score 0.95 0.85 0</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>S. Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T. Maxwell III, and M. Johnson. Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques. In Proc. 40th ACL, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F T K Sang</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>2</volume>
<contexts>
<context position="20780" citStr="Sang, 2002" startWordPosition="3510" endWordPosition="3511">ical significance of performance differences. Unfortunately, reported results sometimes leave out details needed for accurate comparisons. We report F scores for comparison with previous work, but we also give statistical significance estimates using McNemar’s test for those methods that we evaluated directly. Testing the significance of F scores is tricky because the wrong chunks generated by two chunkers are not directly comparable. Yeh (2000) examined randomized tests for estimating the significance of F scores, and in particular the bootstrap over the test set (Efron and Tibshirani, 1993; Sang, 2002). However, bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements (Gillick and Cox, 1989). Model F score SVM combination 94.39% (Kudo and Matsumoto, 2001) CRF 94.38% Generalized winnow 93.89% (Zhang et al., 2002) Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs,designed to handle millions of features, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0. Minor variants support voted percep</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E. F. T. K. Sang. Memory-based shallow parsing. Journal of Machine Learning Research, 2:559–594, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Shewchuk</author>
</authors>
<title>An introduction to the conjugate gradient method without the agonizing pain,</title>
<date>1994</date>
<note>URL http:// www-2.cs.cmu.edu/˜jrs/jrspapers.html#cg.</note>
<contexts>
<context position="9610" citStr="Shewchuk, 1994" startWordPosition="1583" endWordPosition="1584">refore, we can use a forward pass to compute the i and a backward bass to compute the i and accumulate the feature expectations. To avoid overfitting, we penalize the likelihood with a spherical Gaussian weight prior (Chen and Rosenfeld, 1999): optimization algorithms when many correlated features are involved. Concurrently with the present work, Wallach (2002) tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages over iterative scaling on a small shallow parsing problem. Our work shows that preconditioned conjugate-gradient (CG) (Shewchuk, 1994) or limited-memory quasi-Newton (L-BFGS) (Nocedal and Wright, 1999) perform comparably on very large problems (around 3.8 million features). We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002). All algorithms except voted perceptron maximize the penalized loglikelihood:  = arg maxa La. However, for ease of exposition, this discussion of training methods uses the unpenalized log-likelihood La. 3.1 Preconditioned Conjugate Gradient Conjugate-gradient (CG) methods have been shown t</context>
</contexts>
<marker>Shewchuk, 1994</marker>
<rawString>J. R. Shewchuk. An introduction to the conjugate gradient method without the agonizing pain, 1994. URL http:// www-2.cs.cmu.edu/˜jrs/jrspapers.html#cg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>P Abbeel</author>
<author>D Koller</author>
</authors>
<title>Discriminative probabilistic models for relational data.</title>
<date>2002</date>
<booktitle>In Eighteenth Conference on Uncertainty in Artificial Intelligence,</booktitle>
<contexts>
<context position="6162" citStr="Taskar et al., 2002" startWordPosition="925" endWordPosition="928">r classifiers (Kudo and Matsumoto, 2001). To obtain these results, we had to abandon the original iterative scaling CRF training algorithm for convex optimization algorithms with better convergence properties. We provide detailed comparisons between training methods. The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron. 2 Conditional Random Fields We focus here on conditional random fields on sequences, although the notion can be used more generally (Lafferty et al., 2001; Taskar et al., 2002). Such CRFs define conditional probability distributions p(Y |X) of label sequences given input sequences. We assume that the random variable sequences X and Y have the same length, and use x = x1 · · · xn and y = y1 · · · yn for the generic input sequence and label sequence, respectively. A CRF on (X, Y ) is specified by a vector f of local features and a corresponding weight vector . Each local feature is either a state feature s(y, x, i) or a transition feature t(y, y, x, i), where y, y are labels, x an input sequence, and i an input position. To make the notation more uniform, we also w</context>
</contexts>
<marker>Taskar, Abbeel, Koller, 2002</marker>
<rawString>B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In Eighteenth Conference on Uncertainty in Artificial Intelligence, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proc. CoNLL-2000,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="2021" citStr="Sang and Buchholz, 2000" startWordPosition="300" endWordPosition="303">ity recognition, and the task we shall focus on here, shallow parsing. Shallow parsing identifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or information extraction (Abney, 1991). The paradigmatic shallowparsing problem is NP chunking, which finds the nonrecursive cores of noun phrases called base NPs. The pioneering work of Ramshaw and Marcus (1995) introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics. The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. Most previous work used two main machine-learning approaches to sequence labeling. The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) (Freitag and McCallum, 2000; Kupiec, 1992) or multilevel Markov models (Bikel et al., 1999). The second approach views the sequence labeling problem as a sequence of classification problems, one for each of the labels in the sequence. The classification result at each position may depend on the</context>
<context position="15534" citStr="Sang and Buchholz (2000)" startWordPosition="2536" endWordPosition="2539">tence. Following Ramshaw and Marcus (1995), the input to the NP consists of the words in a sentence annotated automatically with part-of-speech (POS) tags. The task is to label each word with a label indicating whether the word is outside a chunk starts a chunk (B), or continues a chunk (I). For example, the tokens in first li chunker chunker’s (O), ne of Figure 1 would be labeled BIIBIIOBOBIIO. 4.1 Data Preparation NP chunking results have been reported on two slightly different data sets: the original RM data set of Ramshaw and Marcus (1995), and the modified CoNLL-2000 version of Tjong Kim Sang and Buchholz (2000). Although the chunk tags in the RM and CoNLL-2000 are somewhat different, we found no significant accuracy differences between models trained on these two data sets. Therefore, all our results are reported on the CoNLL-2000 data set. We also used a development test set, provided by Michael Collins, derived from WSJ section 21 tagged with the Brill (1995) POS tagger. 4.2 CRFs for Shallow Parsing Our chunking CRFs have asecond-order Markov dependency between chunk tags. This is easily encoded by making the CRF labels pairs of consecutive chunk tags. That is, the label at position i is yi = wher</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. F. Tjong Kim Sang and S. Buchholz. Introduction to the CoNLL-2000 shared task: Chunking. In Proc. CoNLL-2000, pages 127–132, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wallach</author>
</authors>
<title>Efficient training of conditional random fields.</title>
<date>2002</date>
<booktitle>In Proc. 6th Annual CLUK Research Colloquium,</booktitle>
<contexts>
<context position="9360" citStr="Wallach (2002)" startWordPosition="1548" endWordPosition="1550">product. Then Ep,,(Y |.)F(Y , x) = � pa(y|x)F(y, x) y i−1(fi  Mi) i Za(x) Za(x) = n · 1 where i and i the forward and backward state-cost vectors defined by � i−1Mi 0 &lt; i  n i = 1 i = 0  T = J M,&apos;1&apos; i+1 Qi 1  i &lt; n T+1 Z 11 1 i = n Therefore, we can use a forward pass to compute the i and a backward bass to compute the i and accumulate the feature expectations. To avoid overfitting, we penalize the likelihood with a spherical Gaussian weight prior (Chen and Rosenfeld, 1999): optimization algorithms when many correlated features are involved. Concurrently with the present work, Wallach (2002) tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages over iterative scaling on a small shallow parsing problem. Our work shows that preconditioned conjugate-gradient (CG) (Shewchuk, 1994) or limited-memory quasi-Newton (L-BFGS) (Nocedal and Wright, 1999) perform comparably on very large problems (around 3.8 million features). We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002). All algorithms except voted perceptron max</context>
</contexts>
<marker>Wallach, 2002</marker>
<rawString>H. Wallach. Efficient training of conditional random fields. In Proc. 6th Annual CLUK Research Colloquium, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In COLING-2000,</booktitle>
<pages>947--953</pages>
<location>Saarbruecken, Germany,</location>
<contexts>
<context position="20618" citStr="Yeh (2000)" startWordPosition="3484" endWordPosition="3485">deally, comparisons among chunkers would control for feature sets, data preparation, training and test procedures, and parameter tuning, and estimate the statistical significance of performance differences. Unfortunately, reported results sometimes leave out details needed for accurate comparisons. We report F scores for comparison with previous work, but we also give statistical significance estimates using McNemar’s test for those methods that we evaluated directly. Testing the significance of F scores is tricky because the wrong chunks generated by two chunkers are not directly comparable. Yeh (2000) examined randomized tests for estimating the significance of F scores, and in particular the bootstrap over the test set (Efron and Tibshirani, 1993; Sang, 2002). However, bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements (Gillick and Cox, 1989). Model F score SVM combination 94.39% (Kudo and Matsumoto, 2001) CRF 94.38% Generalized winnow 93.89% (Zhang et al., 2002) Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java i</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>A. Yeh. More accurate tests for the statistical significance of result differences. In COLING-2000, pages 947–953, Saarbruecken, Germany, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F Damerau</author>
<author>D Johnson</author>
</authors>
<title>Text chunking based on a generalization of winnow.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>2</volume>
<contexts>
<context position="21091" citStr="Zhang et al., 2002" startWordPosition="3555" endWordPosition="3558">irectly. Testing the significance of F scores is tricky because the wrong chunks generated by two chunkers are not directly comparable. Yeh (2000) examined randomized tests for estimating the significance of F scores, and in particular the bootstrap over the test set (Efron and Tibshirani, 1993; Sang, 2002). However, bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements (Gillick and Cox, 1989). Model F score SVM combination 94.39% (Kudo and Matsumoto, 2001) CRF 94.38% Generalized winnow 93.89% (Zhang et al., 2002) Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs,designed to handle millions of features, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0. Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al., 2000) with the same efficient feature encoding. GIS, CG, and L-BFGS were used to train CRFs and MEMMs. 5.1 F Scores Table 2 gives representative NP chunking F scores for previous work and for our best model, with the complete set of 3.8 million features. The las</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>T. Zhang, F. Damerau, and D. Johnson. Text chunking based on a generalization of winnow. Journal of Machine Learning Research, 2:615–637, 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>