<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000397">
<title confidence="0.965649">
One Tense per Scene: Predicting Tense in Chinese Conversations
</title>
<author confidence="0.999759">
Tao Ge1,2, Heng Ji3, Baobao Chang1,2, Zhifang Sui1,2
</author>
<affiliation confidence="0.9973385">
1Key Laboratory of Computational Linguistics, Ministry of Education,
School of EECS, Peking University, Beijing, 100871, China
2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, 221009, China
3Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA
</affiliation>
<email confidence="0.983819">
getao@pku.edu.cn, jih@rpi.edu,
chbb@pku.edu.cn, szf@pku.edu.cn
</email>
<sectionHeader confidence="0.993806" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999965625">
We study the problem of predicting tense
in Chinese conversations. The unique
challenges include: (1) Chinese verbs do
not have explicit lexical or grammatical
forms to indicate tense; (2) Tense in-
formation is often implicitly hidden out-
side of the target sentence. To tackle
these challenges, we first propose a set
of novel sentence-level (local) features us-
ing rich linguistic resources and then pro-
pose a new hypothesis of “One tense per
scene” to incorporate scene-level (global)
evidence to enhance the performance. Ex-
perimental results demonstrate the power
of this hybrid approach, which can serve
as a new and promising benchmark.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988299631578947">
In natural languages, tense is important to indicate
the time at which an action or event takes place.
In some languages such as Chinese, verbs do not
have explicit morphological or grammatical forms
to indicate their tense information. Therefore, au-
tomatic tense prediction is important for both hu-
man’s deep understanding of these languages as
well as downstream natural language processing
tasks (e.g., machine translation (Liu et al., 2011)).
In this paper, we concern “semantic” tense (time
of the event relative to speech time) as opposed
to morphosyntactic tense systems found in many
languages. Our goal is to predict the tense (past,
present or future) of the main predicate1 of each
sentence in a Chinese conversation, which has
never been thoroughly studied before but is ex-
tremely important for conversation understanding.
Some recent work (Ye et al., 2006; Xue and
Zhang, 2014; Zhang and Xue, 2014) on Chinese
</bodyText>
<footnote confidence="0.9116215">
1The main predicate of a sentence can be considered equal
to the root of a dependency parse
</footnote>
<bodyText confidence="0.999431083333333">
tense prediction found that tense in written lan-
guage can be effectively predicted by some fea-
tures in local contexts such as aspectual markers
(e.g. 着 (zhe), 了 (le), 过 (guo)) and time ex-
pressions (e.g., 昨天 (yesterday)). However, it is
much more challenging to predict tense in Chinese
conversations and there has not been an effective
set of rules to predict Chinese tense so far due to
the complexity of language-specific phenomena.
Let’s look at the examples shown in Table 1.
In general, there are three unique challenges for
tense prediction in Chinese conversations:
</bodyText>
<listItem confidence="0.997233913043478">
(1) Informal verbal expressions: sentences in
a conversation are often grammatically incorrect,
which makes aspectual marker based evidence un-
reliable. Moreover, sentences in a conversation
often omit important sentence components. For
example, in conversation 1 in Table 1, “如果(if)”
which is a very important cue to predict tense of
verb “废(destroy)” is omitted.
(2) Effects of interactions on tense: In contrast to
other genres, conversations are interactive, which
may have an effect on tense: in some cases, tense
can only be inferred by understanding the interac-
tions. For example, we can see from conversations
2, 3 and 4 in Table 1 that when the second person
(你(you)) is used as the object of the predicate “告
诉(tell)”, the predicate describes the action during
the conversation and thus its tense is present. In
contrast, when the third person is used in a sen-
tence, it is unlikely that the tense of the predicate
is present because it does not describe an action
during the conversation. This challenge is unique
to Chinese conversations.
(3) Tense ambiguity in a single sentence:
</listItem>
<bodyText confidence="0.707426666666667">
Sentence-level analysis is often inadequate to dis-
ambiguate tense. For example, it is impossible to
determine whether “告诉(tell)” in conversations 3
and 4 in Table 1 is a past action (the speaker al-
ready told) or a future action (the speaker hasn’t
told yet) only based on sentence-level contexts.
</bodyText>
<page confidence="0.680564">
668
</page>
<note confidence="0.265895">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.9993994">
1 a: [如果(if)]你(you)动(touch)A(my)儿子(son)—7(once),A(I)先(first)废(destroy)3你(you)- (If you touch my
son, I’ll destroy you.)
2 b: A(I)Q诉(tell)你(you)—声,航班(flight)取消(cancel)3-(I’m telling you: the flight is canceled.)
3 c:你 (you)H&apos;]H&apos;] (just now)*&amp;quot;*(to him)A(say)ItA(what)3? (What did you say to him just now?)
d: A(I)[H&apos;]t(just now)]Q诉(tell)*(him)—声 ,航班(flight)取消(cancel)3-(I told him the flight is canceled.)
4 e: 你(you)要(will)干(do)&amp;quot; (what)去(go)? (What are you going to do?)
f: A(I)[要(will)]Q诉(tell)*(him)—声 ,航班(fight)取消(cancel)3-(I’ll tell him the flight is canceled.)
5 a: 发生(happen)3ItA(what) 情(event)? (What happened?)
b: A(I)跟-Y,-清(Wu Qing)—起(with) (I was with Wu Qing)
b: A41(We)在(keep)监视(surveilance)—4货(a cargo) (We were keeping surveillance on a cargo...)
b: A41(We)怀疑(suspect)那#-(thoses)是(are)偷来的(stolen)文物(antiques) (We suspected those were stolen an-
tiques)
b: 那#- (those guys),突然(suddenly)就走(walk)出来(out)4T(beat)A41(us) (Suddenly, all those guys walked out
to beat us up!)
b: A(I)要(want)49_警(call the police)*41(they)t停+(stop) (They stopped only when I tried to call the police)
</table>
<tableCaption confidence="0.693473333333333">
Table 1: Five sample conversations that show the challenges in tense prediction in Chinese conversations.
a,b,c,d at the beginning of each sentence denote various speakers. The words in square brackets are
omitted content in the original sentences and the underlined words are main predicates.
</tableCaption>
<bodyText confidence="0.999695473684211">
In fact, the sentence in conversation 3 omits “H&apos;]
t(just now)” which indicates past tense and the
sentence in the conversation 4 omits “要(will)”
which indicates future tense. If we add the omitted
word back to the original sentence, there will not
be tense ambiguity.
To tackle the above challenges, we propose
to predict tense in Chinese conversations from
two views – sentence-level (local) and scene-
level (global). We first develop a local classifier
with linguistic knowledge and new conversation-
specific features (Section 2.1). Then we propose
a novel framework to exploit the global contexts
of the entire scene to infer tense, based on a new
“One tense per scene” hypothesis (Section 2.2).
We created a new a benchmark data set2, which
contains 294 conversations (1,857 sentences) and
demonstrated the effectiveness of our approach.
ble
</bodyText>
<sectionHeader confidence="0.987105" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.999373">
2.1 Local Predictor
</subsectionHeader>
<bodyText confidence="0.687584">
We develop a Maximum Entropy (MaxEnt) clas-
sifier (Zhang, 2004) as the local predictor.
Basic features: The unigrams, bigrams and tri-
grams of a sentence.
Dependency parsing features: We use the Stan-
ford parser (Chen and Manning, 2014) to conduct
dependency parsing3 on the target sentences and
use dependency paths associated with the main
predicate of a sentence as well as their dependency
types as features. By using the parsing features,
</bodyText>
<footnote confidence="0.9868905">
2http://nlp.cs.rpi.edu/data/chinesetense.zip
3We use CCProcessed dependencies.
</footnote>
<listItem confidence="0.78581005">
we can not only find aspectual markers (e.g., “3”)
but also capture the effect of sentence structures on
the tense.
Linguistic knowledge features: We also ex-
ploit the following linguistic knowledge from the
Grammatical Knowledge-base of Contemporary
Chinese (Yu et al., 1998) (also known as GKB):
• Tense of time expressions: GKB lists all
common time expressions and their associ-
ated tense. For example, GKB can tell us “往
年 (previous years)” and “t*纪 (Middle
Ages)” can only be associated with the past
tense.
• Function of conjunction words: Some con-
junction words may have an effect on tense.
For example, the conjunction word “如
果(if)” indicates a conditional clause and the
main predicate of this sentence is likely to be
future tense. GKB can tell us the function of
common Chinese conjunction words.
</listItem>
<bodyText confidence="0.985963285714286">
Conversation-specific features: As mentioned in
Section 1, different person roles being the subject
or the object of a predicate may have an effect on
the tense in a conversation. We analyze the person
roles of the subject and the object of the main pred-
icate and encode them as features, which helps our
model understand effects of interactions on tense.
</bodyText>
<subsectionHeader confidence="0.998199">
2.2 Global Predictor
</subsectionHeader>
<bodyText confidence="0.99990925">
As we discussed before, tense ambiguity in a sen-
tence arises from the omissions of sentence com-
ponents. According to the principle of efficient
information transmission (Jaeger and Levy, 2006;
</bodyText>
<page confidence="0.994333">
669
</page>
<bodyText confidence="0.992245909090909">
Jaeger, 2010) and Gricean Maxims (Grice et al.,
1975) in cooperative theory, the omitted elements
can be predicted by considering contextual infor-
mation and the tense can be further disambiguated.
In order to better predict tense, we propose a new
hypothesis:
One tense per scene: Within a scene, tense in sen-
tences tends to be consistent and coherent.
During a conversation, a speaker/listener can
know the tense of a predicate by either a tense in-
dicator in the target sentence or scene-level tense
analysis. A scene is a subdivision of a conversa-
tion in which the time is continuous and the topic
is highly coherent and which does not usually in-
volve a change of tense. For example, for the con-
versation 3 in Table 1, we can learn the scene is
about the past from the word “H11H11 (just now)” in
the first sentence. Therefore, we can exploit this
clue to determine the tense of “告诉(tell)” as past.
Therefore, when we are not sure which tense
of the main predicate in a sentence should be,
we can consider the tense of the entire scene.
For example, the conversation 5 in Table 1 is
about a past scene because the whole conver-
sation is about a past event. For the sen-
tence “我们(We)在(keep)监视(surveillance)一批
货(a cargo)” where the tense of the predicate is
ambiguous (past tense and present tense are both
reasonable), we can exploit the tense of the scene
(past) to determine its tense as past.
Global tense prediction
Inspired by the burst detection algorithm proposed
by Kleinberg (2003), we use a 3-state automaton
sequence model to globally predict tense based on
the above hypothesis. In a conversation with n
sentences, each sentence is one element in the se-
quence. The sentence’s tense can be seen as the
hidden state and the sentence’s features are the ob-
servation. Formally, we define the tense in the ith
sentence as ti and the observations (i.e., features)
in the sentence as oi. The goal of this model is to
output an optimal sequence t* = {t∗1, t∗2, ..., t∗n}
that minimizes the cost function defined as fol-
lows:
</bodyText>
<equation confidence="0.998980666666667">
Cost(t, o) = a �n −lnP(ti|oi)+(1−a) n−1� 1(ti+1 =� ti)
i=1 i=1
(1)
</equation>
<bodyText confidence="0.999606933333333">
where 1(·) is an indicator function.
As we can see in (1), the cost function consists
of two parts. The first part is the negative log like-
lihood of the local prediction, allowing the model
to incorporate the results from the local predic-
tor. The second part is the cost of tense inconsis-
tency between adjacent sentences, which enables
the model to take into account tense consistency
in a scene. Finding the optimal sequence is a de-
coding process, which can be done using Viterbi
algorithm in O(n) time. The parameter A is used
for adjusting weights of these two parts. If A = 1,
the predictor will not consider global tense consis-
tency and thus the optimal sequence t* will be the
same as the output of the local predictor.
</bodyText>
<figureCaption confidence="0.930996666666667">
Figure 1 shows how the global predictor works
for predicting the tense in the conversation 5 in
Table 1. The global predictor can correct wrong
local predictions, especially less confident ones.
Figure 1: Global tense prediction for the conver-
sation 5 in Table 1.
</figureCaption>
<sectionHeader confidence="0.999095" genericHeader="background">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999542">
3.1 Data and Scoring Metric
</subsectionHeader>
<bodyText confidence="0.999771888888889">
To the best of our knowledge, tense prediction in
Chinese conversations has never been studied be-
fore and there is no existing benchmark for evalu-
ation. We collected 294 conversations (including
1,857 sentences) from 25 popular Chinese movies,
dramas and TV shows. Each conversation con-
tains 2-18 sentences. We manually annotate the
main predicate and its tense in each sentence. We
use ICTCLAS (Zhang et al., 2003) to do word seg-
mentation as preprocessing.
Since tense prediction can be seen as a multi-
class classification problem, we use accuracy as
the metric to evaluate the performance. We ran-
domly split our dataset into three sets: training set
(244 conversations), development set (25 conver-
sations) and test set (25 conversations) for eval-
uation. In evaluation, we ignore imperative sen-
tences and sentences without predicates.
</bodyText>
<subsectionHeader confidence="0.997463">
3.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.998137">
We compare our approach with the following
baselines:
</bodyText>
<listItem confidence="0.990788">
• Majority: We label every instance with the
majority tense (present tense).
</listItem>
<figure confidence="0.968783111111111">
correct tense P P P P P P
sentences
P P
�
P
local prediction
global prediction P P
P P P
P P P
</figure>
<page confidence="0.971789">
670
</page>
<listItem confidence="0.946312">
• Local predictor with basic features (Local(b))
• Local predictor with basic features + depen-
dency parsing features (Local(b+p))
• Local predictor with basic features + depen-
dency parsing features + linguistic knowl-
edge features (Local(b+p+l))
• Local predictor + all features introduced in
Section 2.1 (Local(all))
• Conditional Random Fields (CRFs): We
model a conversation as a sequence of sen-
tences and predict tense using CRFs (Laf-
ferty et al., 2001). We implement CRFs using
CRFsuite (Okazaki, 2007) with all features
introduced in Section 2.1.
</listItem>
<bodyText confidence="0.9998259">
Among the baselines, Local(b+p) is the most
similar model to the approaches in previous work
on Chinese tense prediction in written languages
(Ye et al., 2006; Xue, 2008; Liu et al., 2011). Re-
cent work (Zhang and Xue, 2014) used eventuality
and modality labels as features that derived from
a classifier trained on an annotated corpus. How-
ever, the annotated corpus for training the eventu-
ality and modality classifier is not publicly avail-
able, we cannot duplicate their approaches.
</bodyText>
<table confidence="0.999352625">
Dev Test
Majority 65.13% 54.01%
Local(b) 69.74% 66.42%
Local(b+p) 70.39% 67.15%
Local(b+p+l) 71.05% 69.34%
Local(all) 71.05% 69.34%
CRFs 69.74% 64.96%
Global 72.37% 72.26%
</table>
<tableCaption confidence="0.99736">
Table 2: Tense prediction accuracy.
</tableCaption>
<bodyText confidence="0.998399888888889">
Table 2 shows the results of various models. For
our global predictor, the optimal λ (0.4) is tuned
on the development set and used on the test set.
According to Table 2, n-grams and depen-
dency parsing features4 are useful to predict
tense, and linguistic knowledge can further im-
prove the accuracy of tense prediction. However,
adding conversation-specific features (interaction
features) does not benefit Local(b+p+l). The first
</bodyText>
<footnote confidence="0.7052515">
4We also tried adding POS tags to dependency paths but
didn’t see improvements because POS information has been
implicitly indicated by dependency types and thus becomes
redundant.
</footnote>
<bodyText confidence="0.955405405405405">
reason is that the subject and the object of the
predicates in many sentences are omitted, which
is common in Chinese conversations. The other
reason, also the main reason, is that simply using
the person roles of the subject and the object is
not sufficient to depict the interaction. For exam-
ple, the subject and the object of the following sen-
tences have the same person role but have different
tenses because “ 告(warn)” is the current action
of the speaker but “教(teach)” is not. Therefore,
to exploit the interaction features of a conversa-
tion, we must deeply understand the meanings of
action verbs.
我(I) 告(warn)你(you)。 (I’m warn-
ing you.)
我(I)教(teach)你(you)。 (I’ll teach
you.)
The global predictor significantly improves the
local predictor’s performance (at 95% confidence
level according to Wilcoxon Signed-Rank Test),
which verifies the effectiveness of “One tense per
scene” hypothesis for tense prediction. It is no-
table that CRFs do not work well on our dataset.
The reason is that the transition pattern of tenses
in a sequence of sentences is not easy to learn, es-
pecially when the size of training data is not very
large. In many cases, the tense of a verb in a
sentence is determined by features within the sen-
tence, which has nothing to do with tense tran-
sition. In these cases, learning tense transition
patterns will mislead the model and accordingly
affect the performance. In contrast, our global
model is more robust because it is based on our
“One tense per scene” hypothesis which can be
seen as prior linguistic knowledge, thus achieves
good performance even when the training data is
not sufficient.
</bodyText>
<subsectionHeader confidence="0.992955">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.987755454545455">
There are still many remaining challenges for
tense prediction in Chinese conversations:
Omission detection: The biggest challenge for
this task is the omission of sentence components.
As shown in Table 1, if omitted words can be re-
covered, it will be less likely to make a wrong pre-
diction.
Word Sense Disambiguation: Some function
words which can indicate tense are ambiguous.
For example, the function word “-c” has many
senses. It can mean 将-c(will), 想-c(want) and 需
</bodyText>
<page confidence="0.996661">
671
</page>
<bodyText confidence="0.999472">
*(need), and also it is sometimes used to present
an option. It is difficult for a system to correctly
predict tense unless it can disambiguate the sense
of such function words:
</bodyText>
<listItem confidence="0.938991875">
• —*儿(later)4A(he)* (will)过来 (come)。
(He’ll come here later.)
• A (I)* (want) 吃 (eat) 苹果 (apples)。 (I
want to eat apples)
• 你(you)*(need)5 5(much)锻 炼(exercise)
(You need to take more exercises.)
• h4+A(why)你(you)*(opt)救(save)A(me)?
(Why did you save me?)
</listItem>
<bodyText confidence="0.9786455">
Verb Tense Preference: Different verbs may have
different tense preferences. For example, “a
h(think)” is often used in the past tense while “认
h(think)” is usually in the present tense:
</bodyText>
<listItem confidence="0.999613">
• A(I)a h(think)4A(he)* *(won’t)来(co-
me) (I thought he would not come.)
• A(I)认 h(think)4A(he)* *(won’t)来(co-
me) (I think he won’t come.)
</listItem>
<bodyText confidence="0.99241875">
Generic and specific subject/object: Whether
the subject/object is generic or specific has an ef-
fect on tense. For example, in the sentence “那
场(that)A +(war)太(very)A 0(brutal)&apos;T”, the
predicate “A 0(brutal)” is in the past tense
while in the sentence “A +(war)太(very)A
0(brutal)&apos;T”, the predicate “A0(brutal)” is in
the present tense.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999772764705882">
Early work on Chinese tense prediction (Ye et
al., 2006; Xue, 2008) modeled this task as a
multi-class classification problem and used ma-
chine learning approaches to solve the problem.
Recent work (Liu et al., 2011; Xue and Zhang,
2014; Zhang and Xue, 2014) studied distant an-
notation of tense from a bilingual parallel cor-
pus. Among them, Xue and Zhang (2014) and
Zhang and Xue (2014) improved tense prediction
by using eventuality and modality labels. How-
ever, none of the previous work focused on the
specific challenge of the tense prediction in oral
languages although the dataset used by Liu et al.
(2011) includes conversations. In contrast, this
paper presents the unique challenges and corre-
sponding solutions to tense prediction in conver-
sations.
</bodyText>
<sectionHeader confidence="0.995158" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999978181818182">
This paper presents the importance and challenges
of tense prediction in Chinese conversations and
proposes a novel solution to the challenges.
In the future, we plan to further study this
problem by focusing on omission detection, verb
tense preference from the view of pragmatics, and
jointly learning the local and global predictors. In
addition, we will study predicting the tense of mul-
tiple predicates in a sentence and identifying im-
perative sentences in a conversation, which is also
a challenge of tense prediction.
</bodyText>
<sectionHeader confidence="0.998351" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.951270888888889">
We thank all the anonymous reviewers for their
constructive suggestions. We thank Prof. Shi-
wen Yu and Xun Wang for providing insights from
Chinese linguistics. This work is supported by
National Key Basic Research Program of China
2014CB340504, NSFC project 61375074, U.S.
DARPA Award No. FA8750-13-2-0045 and China
Scholarship Council (CSC, No. 201406010174).
The contact author of this paper is Zhifang Sui.
</bodyText>
<sectionHeader confidence="0.996254" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999389">
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP.
H Paul Grice, Peter Cole, and Jerry L Morgan. 1975.
Syntax and semantics. Logic and conversation,
3:41–58.
TF Jaeger and Roger P Levy. 2006. Speakers optimize
information density through syntactic reduction. In
Advances in neural information processing systems.
T Florian Jaeger. 2010. Redundancy and reduc-
tion: Speakers manage syntactic information den-
sity. Cognitive psychology, 61(1):23–62.
Jon Kleinberg. 2003. Bursty and hierarchical structure
in streams. Data Mining and Knowledge Discovery,
7(4):373–397.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Feifan Liu, Fei Liu, and Yang Liu. 2011. Learning
from Chinese-English parallel data for Chinese tense
prediction. In IJCNLP.
Naoaki Okazaki. 2007. CRFsuite: a fast implementa-
tion of conditional random fields (CRFs).
</reference>
<page confidence="0.980347">
672
</page>
<reference confidence="0.9996235">
Nianwen Xue and Yuchen Zhang. 2014. Buy one get
one free: Distant annotation of Chinese tense, event
type, and modality. In LREC.
Nianwen Xue. 2008. Automatic inference of the
temporal location of situations in Chinese text. In
EMNLP.
Yang Ye, Victoria Li Fossum, and Steven Abney. 2006.
Latent features in automatic tense translation be-
tween Chinese and English. In SIGHAN workshop.
Shiwen Yu, Xuefeng Zhu, Hui Wang, and Yunyun
Zhang. 1998. The grammatical knowledge-base of
contemporary Chinese—a complete specification.
Yucheng Zhang and Nianwen Xue. 2014. Automatic
inference of the tense of Chinese events using im-
plicit information. In EMNLP.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based Chinese lexical analyzer
ictclas. In SIGHAN workshop.
Le Zhang. 2004. Maximum entropy modeling toolkit
for Python and C++.
</reference>
<page confidence="0.999275">
673
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.559811">
<title confidence="0.999906">One Tense per Scene: Predicting Tense in Chinese Conversations</title>
<author confidence="0.99186">Heng Baobao Zhifang</author>
<affiliation confidence="0.9706515">Laboratory of Computational Linguistics, Ministry of School of EECS, Peking University, Beijing, 100871,</affiliation>
<address confidence="0.945434">Innovation Center for Language Ability, Xuzhou, Jiangsu, 221009, Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180,</address>
<email confidence="0.7832625">getao@pku.edu.cn,chbb@pku.edu.cn,szf@pku.edu.cn</email>
<abstract confidence="0.999811294117647">We study the problem of predicting tense in Chinese conversations. The unique challenges include: (1) Chinese verbs do not have explicit lexical or grammatical forms to indicate tense; (2) Tense information is often implicitly hidden outside of the target sentence. To tackle these challenges, we first propose a set of novel sentence-level (local) features using rich linguistic resources and then propose a new hypothesis of “One tense per scene” to incorporate scene-level (global) evidence to enhance the performance. Experimental results demonstrate the power of this hybrid approach, which can serve as a new and promising benchmark.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6883" citStr="Chen and Manning, 2014" startWordPosition="1027" endWordPosition="1030"> new conversationspecific features (Section 2.1). Then we propose a novel framework to exploit the global contexts of the entire scene to infer tense, based on a new “One tense per scene” hypothesis (Section 2.2). We created a new a benchmark data set2, which contains 294 conversations (1,857 sentences) and demonstrated the effectiveness of our approach. ble 2 Method 2.1 Local Predictor We develop a Maximum Entropy (MaxEnt) classifier (Zhang, 2004) as the local predictor. Basic features: The unigrams, bigrams and trigrams of a sentence. Dependency parsing features: We use the Stanford parser (Chen and Manning, 2014) to conduct dependency parsing3 on the target sentences and use dependency paths associated with the main predicate of a sentence as well as their dependency types as features. By using the parsing features, 2http://nlp.cs.rpi.edu/data/chinesetense.zip 3We use CCProcessed dependencies. we can not only find aspectual markers (e.g., “3”) but also capture the effect of sentence structures on the tense. Linguistic knowledge features: We also exploit the following linguistic knowledge from the Grammatical Knowledge-base of Contemporary Chinese (Yu et al., 1998) (also known as GKB): • Tense of time </context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
<author>Peter Cole</author>
<author>Jerry L Morgan</author>
</authors>
<title>Syntax and semantics. Logic and conversation,</title>
<date>1975</date>
<pages>3--41</pages>
<contexts>
<context position="8605" citStr="Grice et al., 1975" startWordPosition="1302" endWordPosition="1305">tion-specific features: As mentioned in Section 1, different person roles being the subject or the object of a predicate may have an effect on the tense in a conversation. We analyze the person roles of the subject and the object of the main predicate and encode them as features, which helps our model understand effects of interactions on tense. 2.2 Global Predictor As we discussed before, tense ambiguity in a sentence arises from the omissions of sentence components. According to the principle of efficient information transmission (Jaeger and Levy, 2006; 669 Jaeger, 2010) and Gricean Maxims (Grice et al., 1975) in cooperative theory, the omitted elements can be predicted by considering contextual information and the tense can be further disambiguated. In order to better predict tense, we propose a new hypothesis: One tense per scene: Within a scene, tense in sentences tends to be consistent and coherent. During a conversation, a speaker/listener can know the tense of a predicate by either a tense indicator in the target sentence or scene-level tense analysis. A scene is a subdivision of a conversation in which the time is continuous and the topic is highly coherent and which does not usually involve</context>
</contexts>
<marker>Grice, Cole, Morgan, 1975</marker>
<rawString>H Paul Grice, Peter Cole, and Jerry L Morgan. 1975. Syntax and semantics. Logic and conversation, 3:41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TF Jaeger</author>
<author>Roger P Levy</author>
</authors>
<title>Speakers optimize information density through syntactic reduction. In Advances in neural information processing systems.</title>
<date>2006</date>
<contexts>
<context position="8546" citStr="Jaeger and Levy, 2006" startWordPosition="1292" endWordPosition="1295">us the function of common Chinese conjunction words. Conversation-specific features: As mentioned in Section 1, different person roles being the subject or the object of a predicate may have an effect on the tense in a conversation. We analyze the person roles of the subject and the object of the main predicate and encode them as features, which helps our model understand effects of interactions on tense. 2.2 Global Predictor As we discussed before, tense ambiguity in a sentence arises from the omissions of sentence components. According to the principle of efficient information transmission (Jaeger and Levy, 2006; 669 Jaeger, 2010) and Gricean Maxims (Grice et al., 1975) in cooperative theory, the omitted elements can be predicted by considering contextual information and the tense can be further disambiguated. In order to better predict tense, we propose a new hypothesis: One tense per scene: Within a scene, tense in sentences tends to be consistent and coherent. During a conversation, a speaker/listener can know the tense of a predicate by either a tense indicator in the target sentence or scene-level tense analysis. A scene is a subdivision of a conversation in which the time is continuous and the </context>
</contexts>
<marker>Jaeger, Levy, 2006</marker>
<rawString>TF Jaeger and Roger P Levy. 2006. Speakers optimize information density through syntactic reduction. In Advances in neural information processing systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Florian Jaeger</author>
</authors>
<title>Redundancy and reduction: Speakers manage syntactic information density.</title>
<date>2010</date>
<booktitle>Cognitive psychology,</booktitle>
<pages>61--1</pages>
<contexts>
<context position="8565" citStr="Jaeger, 2010" startWordPosition="1297" endWordPosition="1298">hinese conjunction words. Conversation-specific features: As mentioned in Section 1, different person roles being the subject or the object of a predicate may have an effect on the tense in a conversation. We analyze the person roles of the subject and the object of the main predicate and encode them as features, which helps our model understand effects of interactions on tense. 2.2 Global Predictor As we discussed before, tense ambiguity in a sentence arises from the omissions of sentence components. According to the principle of efficient information transmission (Jaeger and Levy, 2006; 669 Jaeger, 2010) and Gricean Maxims (Grice et al., 1975) in cooperative theory, the omitted elements can be predicted by considering contextual information and the tense can be further disambiguated. In order to better predict tense, we propose a new hypothesis: One tense per scene: Within a scene, tense in sentences tends to be consistent and coherent. During a conversation, a speaker/listener can know the tense of a predicate by either a tense indicator in the target sentence or scene-level tense analysis. A scene is a subdivision of a conversation in which the time is continuous and the topic is highly coh</context>
</contexts>
<marker>Jaeger, 2010</marker>
<rawString>T Florian Jaeger. 2010. Redundancy and reduction: Speakers manage syntactic information density. Cognitive psychology, 61(1):23–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
</authors>
<title>Bursty and hierarchical structure in streams.</title>
<date>2003</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="10041" citStr="Kleinberg (2003)" startWordPosition="1554" endWordPosition="1555">ense of “告诉(tell)” as past. Therefore, when we are not sure which tense of the main predicate in a sentence should be, we can consider the tense of the entire scene. For example, the conversation 5 in Table 1 is about a past scene because the whole conversation is about a past event. For the sentence “我们(We)在(keep)监视(surveillance)一批 货(a cargo)” where the tense of the predicate is ambiguous (past tense and present tense are both reasonable), we can exploit the tense of the scene (past) to determine its tense as past. Global tense prediction Inspired by the burst detection algorithm proposed by Kleinberg (2003), we use a 3-state automaton sequence model to globally predict tense based on the above hypothesis. In a conversation with n sentences, each sentence is one element in the sequence. The sentence’s tense can be seen as the hidden state and the sentence’s features are the observation. Formally, we define the tense in the ith sentence as ti and the observations (i.e., features) in the sentence as oi. The goal of this model is to output an optimal sequence t* = {t∗1, t∗2, ..., t∗n} that minimizes the cost function defined as follows: Cost(t, o) = a �n −lnP(ti|oi)+(1−a) n−1� 1(ti+1 =� ti) i=1 i=1 </context>
</contexts>
<marker>Kleinberg, 2003</marker>
<rawString>Jon Kleinberg. 2003. Bursty and hierarchical structure in streams. Data Mining and Knowledge Discovery, 7(4):373–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<contexts>
<context position="13224" citStr="Lafferty et al., 2001" startWordPosition="2096" endWordPosition="2100">: We label every instance with the majority tense (present tense). correct tense P P P P P P sentences P P � P local prediction global prediction P P P P P P P P 670 • Local predictor with basic features (Local(b)) • Local predictor with basic features + dependency parsing features (Local(b+p)) • Local predictor with basic features + dependency parsing features + linguistic knowledge features (Local(b+p+l)) • Local predictor + all features introduced in Section 2.1 (Local(all)) • Conditional Random Fields (CRFs): We model a conversation as a sequence of sentences and predict tense using CRFs (Lafferty et al., 2001). We implement CRFs using CRFsuite (Okazaki, 2007) with all features introduced in Section 2.1. Among the baselines, Local(b+p) is the most similar model to the approaches in previous work on Chinese tense prediction in written languages (Ye et al., 2006; Xue, 2008; Liu et al., 2011). Recent work (Zhang and Xue, 2014) used eventuality and modality labels as features that derived from a classifier trained on an annotated corpus. However, the annotated corpus for training the eventuality and modality classifier is not publicly available, we cannot duplicate their approaches. Dev Test Majority 65</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifan Liu</author>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>Learning from Chinese-English parallel data for Chinese tense prediction.</title>
<date>2011</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="1584" citStr="Liu et al., 2011" startWordPosition="226" endWordPosition="229">l) evidence to enhance the performance. Experimental results demonstrate the power of this hybrid approach, which can serve as a new and promising benchmark. 1 Introduction In natural languages, tense is important to indicate the time at which an action or event takes place. In some languages such as Chinese, verbs do not have explicit morphological or grammatical forms to indicate their tense information. Therefore, automatic tense prediction is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1The main predicate of a sentence can be considered equal to the root of a dependency parse tense prediction found that t</context>
<context position="13508" citStr="Liu et al., 2011" startWordPosition="2143" endWordPosition="2146">(b+p)) • Local predictor with basic features + dependency parsing features + linguistic knowledge features (Local(b+p+l)) • Local predictor + all features introduced in Section 2.1 (Local(all)) • Conditional Random Fields (CRFs): We model a conversation as a sequence of sentences and predict tense using CRFs (Lafferty et al., 2001). We implement CRFs using CRFsuite (Okazaki, 2007) with all features introduced in Section 2.1. Among the baselines, Local(b+p) is the most similar model to the approaches in previous work on Chinese tense prediction in written languages (Ye et al., 2006; Xue, 2008; Liu et al., 2011). Recent work (Zhang and Xue, 2014) used eventuality and modality labels as features that derived from a classifier trained on an annotated corpus. However, the annotated corpus for training the eventuality and modality classifier is not publicly available, we cannot duplicate their approaches. Dev Test Majority 65.13% 54.01% Local(b) 69.74% 66.42% Local(b+p) 70.39% 67.15% Local(b+p+l) 71.05% 69.34% Local(all) 71.05% 69.34% CRFs 69.74% 64.96% Global 72.37% 72.26% Table 2: Tense prediction accuracy. Table 2 shows the results of various models. For our global predictor, the optimal λ (0.4) is tu</context>
<context position="18061" citStr="Liu et al., 2011" startWordPosition="2873" endWordPosition="2876">k)4A(he)* *(won’t)来(come) (I think he won’t come.) Generic and specific subject/object: Whether the subject/object is generic or specific has an effect on tense. For example, in the sentence “那 场(that)A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A 0(brutal)” is in the past tense while in the sentence “A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A0(brutal)” is in the present tense. 4 Related Work Early work on Chinese tense prediction (Ye et al., 2006; Xue, 2008) modeled this task as a multi-class classification problem and used machine learning approaches to solve the problem. Recent work (Liu et al., 2011; Xue and Zhang, 2014; Zhang and Xue, 2014) studied distant annotation of tense from a bilingual parallel corpus. Among them, Xue and Zhang (2014) and Zhang and Xue (2014) improved tense prediction by using eventuality and modality labels. However, none of the previous work focused on the specific challenge of the tense prediction in oral languages although the dataset used by Liu et al. (2011) includes conversations. In contrast, this paper presents the unique challenges and corresponding solutions to tense prediction in conversations. 5 Conclusions and Future Work This paper presents the imp</context>
</contexts>
<marker>Liu, Liu, Liu, 2011</marker>
<rawString>Feifan Liu, Fei Liu, and Yang Liu. 2011. Learning from Chinese-English parallel data for Chinese tense prediction. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of conditional random fields (CRFs).</title>
<date>2007</date>
<contexts>
<context position="13274" citStr="Okazaki, 2007" startWordPosition="2106" endWordPosition="2107"> tense). correct tense P P P P P P sentences P P � P local prediction global prediction P P P P P P P P 670 • Local predictor with basic features (Local(b)) • Local predictor with basic features + dependency parsing features (Local(b+p)) • Local predictor with basic features + dependency parsing features + linguistic knowledge features (Local(b+p+l)) • Local predictor + all features introduced in Section 2.1 (Local(all)) • Conditional Random Fields (CRFs): We model a conversation as a sequence of sentences and predict tense using CRFs (Lafferty et al., 2001). We implement CRFs using CRFsuite (Okazaki, 2007) with all features introduced in Section 2.1. Among the baselines, Local(b+p) is the most similar model to the approaches in previous work on Chinese tense prediction in written languages (Ye et al., 2006; Xue, 2008; Liu et al., 2011). Recent work (Zhang and Xue, 2014) used eventuality and modality labels as features that derived from a classifier trained on an annotated corpus. However, the annotated corpus for training the eventuality and modality classifier is not publicly available, we cannot duplicate their approaches. Dev Test Majority 65.13% 54.01% Local(b) 69.74% 66.42% Local(b+p) 70.3</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. CRFsuite: a fast implementation of conditional random fields (CRFs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Yuchen Zhang</author>
</authors>
<title>Buy one get one free: Distant annotation of Chinese tense, event type, and modality.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="2029" citStr="Xue and Zhang, 2014" startWordPosition="299" endWordPosition="302">tion is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1The main predicate of a sentence can be considered equal to the root of a dependency parse tense prediction found that tense in written language can be effectively predicted by some features in local contexts such as aspectual markers (e.g. 着 (zhe), 了 (le), 过 (guo)) and time expressions (e.g., 昨天 (yesterday)). However, it is much more challenging to predict tense in Chinese conversations and there has not been an effective set of rules to predict Chinese tense so far due to the complexity of language-specific phenomena. Let’s look at the examples shown in Tab</context>
<context position="18082" citStr="Xue and Zhang, 2014" startWordPosition="2877" endWordPosition="2880">来(come) (I think he won’t come.) Generic and specific subject/object: Whether the subject/object is generic or specific has an effect on tense. For example, in the sentence “那 场(that)A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A 0(brutal)” is in the past tense while in the sentence “A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A0(brutal)” is in the present tense. 4 Related Work Early work on Chinese tense prediction (Ye et al., 2006; Xue, 2008) modeled this task as a multi-class classification problem and used machine learning approaches to solve the problem. Recent work (Liu et al., 2011; Xue and Zhang, 2014; Zhang and Xue, 2014) studied distant annotation of tense from a bilingual parallel corpus. Among them, Xue and Zhang (2014) and Zhang and Xue (2014) improved tense prediction by using eventuality and modality labels. However, none of the previous work focused on the specific challenge of the tense prediction in oral languages although the dataset used by Liu et al. (2011) includes conversations. In contrast, this paper presents the unique challenges and corresponding solutions to tense prediction in conversations. 5 Conclusions and Future Work This paper presents the importance and challenge</context>
</contexts>
<marker>Xue, Zhang, 2014</marker>
<rawString>Nianwen Xue and Yuchen Zhang. 2014. Buy one get one free: Distant annotation of Chinese tense, event type, and modality. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Automatic inference of the temporal location of situations in Chinese text.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="13489" citStr="Xue, 2008" startWordPosition="2141" endWordPosition="2142">ures (Local(b+p)) • Local predictor with basic features + dependency parsing features + linguistic knowledge features (Local(b+p+l)) • Local predictor + all features introduced in Section 2.1 (Local(all)) • Conditional Random Fields (CRFs): We model a conversation as a sequence of sentences and predict tense using CRFs (Lafferty et al., 2001). We implement CRFs using CRFsuite (Okazaki, 2007) with all features introduced in Section 2.1. Among the baselines, Local(b+p) is the most similar model to the approaches in previous work on Chinese tense prediction in written languages (Ye et al., 2006; Xue, 2008; Liu et al., 2011). Recent work (Zhang and Xue, 2014) used eventuality and modality labels as features that derived from a classifier trained on an annotated corpus. However, the annotated corpus for training the eventuality and modality classifier is not publicly available, we cannot duplicate their approaches. Dev Test Majority 65.13% 54.01% Local(b) 69.74% 66.42% Local(b+p) 70.39% 67.15% Local(b+p+l) 71.05% 69.34% Local(all) 71.05% 69.34% CRFs 69.74% 64.96% Global 72.37% 72.26% Table 2: Tense prediction accuracy. Table 2 shows the results of various models. For our global predictor, the op</context>
<context position="17914" citStr="Xue, 2008" startWordPosition="2851" endWordPosition="2852">nse while “认 h(think)” is usually in the present tense: • A(I)a h(think)4A(he)* *(won’t)来(come) (I thought he would not come.) • A(I)认 h(think)4A(he)* *(won’t)来(come) (I think he won’t come.) Generic and specific subject/object: Whether the subject/object is generic or specific has an effect on tense. For example, in the sentence “那 场(that)A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A 0(brutal)” is in the past tense while in the sentence “A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A0(brutal)” is in the present tense. 4 Related Work Early work on Chinese tense prediction (Ye et al., 2006; Xue, 2008) modeled this task as a multi-class classification problem and used machine learning approaches to solve the problem. Recent work (Liu et al., 2011; Xue and Zhang, 2014; Zhang and Xue, 2014) studied distant annotation of tense from a bilingual parallel corpus. Among them, Xue and Zhang (2014) and Zhang and Xue (2014) improved tense prediction by using eventuality and modality labels. However, none of the previous work focused on the specific challenge of the tense prediction in oral languages although the dataset used by Liu et al. (2011) includes conversations. In contrast, this paper present</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Automatic inference of the temporal location of situations in Chinese text. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Ye</author>
<author>Victoria Li Fossum</author>
<author>Steven Abney</author>
</authors>
<title>Latent features in automatic tense translation between Chinese and English.</title>
<date>2006</date>
<booktitle>In SIGHAN workshop.</booktitle>
<contexts>
<context position="2008" citStr="Ye et al., 2006" startWordPosition="295" endWordPosition="298">atic tense prediction is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1The main predicate of a sentence can be considered equal to the root of a dependency parse tense prediction found that tense in written language can be effectively predicted by some features in local contexts such as aspectual markers (e.g. 着 (zhe), 了 (le), 过 (guo)) and time expressions (e.g., 昨天 (yesterday)). However, it is much more challenging to predict tense in Chinese conversations and there has not been an effective set of rules to predict Chinese tense so far due to the complexity of language-specific phenomena. Let’s look at the </context>
<context position="13478" citStr="Ye et al., 2006" startWordPosition="2137" endWordPosition="2140">ency parsing features (Local(b+p)) • Local predictor with basic features + dependency parsing features + linguistic knowledge features (Local(b+p+l)) • Local predictor + all features introduced in Section 2.1 (Local(all)) • Conditional Random Fields (CRFs): We model a conversation as a sequence of sentences and predict tense using CRFs (Lafferty et al., 2001). We implement CRFs using CRFsuite (Okazaki, 2007) with all features introduced in Section 2.1. Among the baselines, Local(b+p) is the most similar model to the approaches in previous work on Chinese tense prediction in written languages (Ye et al., 2006; Xue, 2008; Liu et al., 2011). Recent work (Zhang and Xue, 2014) used eventuality and modality labels as features that derived from a classifier trained on an annotated corpus. However, the annotated corpus for training the eventuality and modality classifier is not publicly available, we cannot duplicate their approaches. Dev Test Majority 65.13% 54.01% Local(b) 69.74% 66.42% Local(b+p) 70.39% 67.15% Local(b+p+l) 71.05% 69.34% Local(all) 71.05% 69.34% CRFs 69.74% 64.96% Global 72.37% 72.26% Table 2: Tense prediction accuracy. Table 2 shows the results of various models. For our global predic</context>
<context position="17902" citStr="Ye et al., 2006" startWordPosition="2847" endWordPosition="2850">ed in the past tense while “认 h(think)” is usually in the present tense: • A(I)a h(think)4A(he)* *(won’t)来(come) (I thought he would not come.) • A(I)认 h(think)4A(he)* *(won’t)来(come) (I think he won’t come.) Generic and specific subject/object: Whether the subject/object is generic or specific has an effect on tense. For example, in the sentence “那 场(that)A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A 0(brutal)” is in the past tense while in the sentence “A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A0(brutal)” is in the present tense. 4 Related Work Early work on Chinese tense prediction (Ye et al., 2006; Xue, 2008) modeled this task as a multi-class classification problem and used machine learning approaches to solve the problem. Recent work (Liu et al., 2011; Xue and Zhang, 2014; Zhang and Xue, 2014) studied distant annotation of tense from a bilingual parallel corpus. Among them, Xue and Zhang (2014) and Zhang and Xue (2014) improved tense prediction by using eventuality and modality labels. However, none of the previous work focused on the specific challenge of the tense prediction in oral languages although the dataset used by Liu et al. (2011) includes conversations. In contrast, this p</context>
</contexts>
<marker>Ye, Fossum, Abney, 2006</marker>
<rawString>Yang Ye, Victoria Li Fossum, and Steven Abney. 2006. Latent features in automatic tense translation between Chinese and English. In SIGHAN workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
<author>Xuefeng Zhu</author>
<author>Hui Wang</author>
<author>Yunyun Zhang</author>
</authors>
<title>The grammatical knowledge-base of contemporary Chinese—a complete specification.</title>
<date>1998</date>
<contexts>
<context position="7445" citStr="Yu et al., 1998" startWordPosition="1107" endWordPosition="1110">s: We use the Stanford parser (Chen and Manning, 2014) to conduct dependency parsing3 on the target sentences and use dependency paths associated with the main predicate of a sentence as well as their dependency types as features. By using the parsing features, 2http://nlp.cs.rpi.edu/data/chinesetense.zip 3We use CCProcessed dependencies. we can not only find aspectual markers (e.g., “3”) but also capture the effect of sentence structures on the tense. Linguistic knowledge features: We also exploit the following linguistic knowledge from the Grammatical Knowledge-base of Contemporary Chinese (Yu et al., 1998) (also known as GKB): • Tense of time expressions: GKB lists all common time expressions and their associated tense. For example, GKB can tell us “往 年 (previous years)” and “t*纪 (Middle Ages)” can only be associated with the past tense. • Function of conjunction words: Some conjunction words may have an effect on tense. For example, the conjunction word “如 果(if)” indicates a conditional clause and the main predicate of this sentence is likely to be future tense. GKB can tell us the function of common Chinese conjunction words. Conversation-specific features: As mentioned in Section 1, differen</context>
</contexts>
<marker>Yu, Zhu, Wang, Zhang, 1998</marker>
<rawString>Shiwen Yu, Xuefeng Zhu, Hui Wang, and Yunyun Zhang. 1998. The grammatical knowledge-base of contemporary Chinese—a complete specification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yucheng Zhang</author>
<author>Nianwen Xue</author>
</authors>
<title>Automatic inference of the tense of Chinese events using implicit information.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2051" citStr="Zhang and Xue, 2014" startWordPosition="303" endWordPosition="306"> both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1The main predicate of a sentence can be considered equal to the root of a dependency parse tense prediction found that tense in written language can be effectively predicted by some features in local contexts such as aspectual markers (e.g. 着 (zhe), 了 (le), 过 (guo)) and time expressions (e.g., 昨天 (yesterday)). However, it is much more challenging to predict tense in Chinese conversations and there has not been an effective set of rules to predict Chinese tense so far due to the complexity of language-specific phenomena. Let’s look at the examples shown in Table 1. In general, ther</context>
<context position="13543" citStr="Zhang and Xue, 2014" startWordPosition="2150" endWordPosition="2153">sic features + dependency parsing features + linguistic knowledge features (Local(b+p+l)) • Local predictor + all features introduced in Section 2.1 (Local(all)) • Conditional Random Fields (CRFs): We model a conversation as a sequence of sentences and predict tense using CRFs (Lafferty et al., 2001). We implement CRFs using CRFsuite (Okazaki, 2007) with all features introduced in Section 2.1. Among the baselines, Local(b+p) is the most similar model to the approaches in previous work on Chinese tense prediction in written languages (Ye et al., 2006; Xue, 2008; Liu et al., 2011). Recent work (Zhang and Xue, 2014) used eventuality and modality labels as features that derived from a classifier trained on an annotated corpus. However, the annotated corpus for training the eventuality and modality classifier is not publicly available, we cannot duplicate their approaches. Dev Test Majority 65.13% 54.01% Local(b) 69.74% 66.42% Local(b+p) 70.39% 67.15% Local(b+p+l) 71.05% 69.34% Local(all) 71.05% 69.34% CRFs 69.74% 64.96% Global 72.37% 72.26% Table 2: Tense prediction accuracy. Table 2 shows the results of various models. For our global predictor, the optimal λ (0.4) is tuned on the development set and used</context>
<context position="18104" citStr="Zhang and Xue, 2014" startWordPosition="2881" endWordPosition="2884">on’t come.) Generic and specific subject/object: Whether the subject/object is generic or specific has an effect on tense. For example, in the sentence “那 场(that)A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A 0(brutal)” is in the past tense while in the sentence “A +(war)太(very)A 0(brutal)&apos;T”, the predicate “A0(brutal)” is in the present tense. 4 Related Work Early work on Chinese tense prediction (Ye et al., 2006; Xue, 2008) modeled this task as a multi-class classification problem and used machine learning approaches to solve the problem. Recent work (Liu et al., 2011; Xue and Zhang, 2014; Zhang and Xue, 2014) studied distant annotation of tense from a bilingual parallel corpus. Among them, Xue and Zhang (2014) and Zhang and Xue (2014) improved tense prediction by using eventuality and modality labels. However, none of the previous work focused on the specific challenge of the tense prediction in oral languages although the dataset used by Liu et al. (2011) includes conversations. In contrast, this paper presents the unique challenges and corresponding solutions to tense prediction in conversations. 5 Conclusions and Future Work This paper presents the importance and challenges of tense prediction </context>
</contexts>
<marker>Zhang, Xue, 2014</marker>
<rawString>Yucheng Zhang and Nianwen Xue. 2014. Automatic inference of the tense of Chinese events using implicit information. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua-Ping Zhang</author>
<author>Hong-Kui Yu</author>
<author>De-Yi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>Hhmm-based Chinese lexical analyzer ictclas.</title>
<date>2003</date>
<booktitle>In SIGHAN workshop.</booktitle>
<contexts>
<context position="12092" citStr="Zhang et al., 2003" startWordPosition="1910" endWordPosition="1913">1. The global predictor can correct wrong local predictions, especially less confident ones. Figure 1: Global tense prediction for the conversation 5 in Table 1. 3 Experiments 3.1 Data and Scoring Metric To the best of our knowledge, tense prediction in Chinese conversations has never been studied before and there is no existing benchmark for evaluation. We collected 294 conversations (including 1,857 sentences) from 25 popular Chinese movies, dramas and TV shows. Each conversation contains 2-18 sentences. We manually annotate the main predicate and its tense in each sentence. We use ICTCLAS (Zhang et al., 2003) to do word segmentation as preprocessing. Since tense prediction can be seen as a multiclass classification problem, we use accuracy as the metric to evaluate the performance. We randomly split our dataset into three sets: training set (244 conversations), development set (25 conversations) and test set (25 conversations) for evaluation. In evaluation, we ignore imperative sentences and sentences without predicates. 3.2 Experimental Results We compare our approach with the following baselines: • Majority: We label every instance with the majority tense (present tense). correct tense P P P P P</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun Liu. 2003. Hhmm-based Chinese lexical analyzer ictclas. In SIGHAN workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum entropy modeling toolkit for Python and C++.</title>
<date>2004</date>
<marker>Le Zhang, 2004</marker>
<rawString>Le Zhang. 2004. Maximum entropy modeling toolkit for Python and C++.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>