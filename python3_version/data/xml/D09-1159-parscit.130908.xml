<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.993106">
Phrase Dependency Parsing for Opinion Mining
</title>
<author confidence="0.999758">
Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu
</author>
<affiliation confidence="0.9993395">
Fudan University
School of Computer Science
</affiliation>
<email confidence="0.98593">
{ybwu,qi zhang,xjhuang,ldwu}@fudan.edu.cn
</email>
<sectionHeader confidence="0.997235" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999256">
In this paper, we present a novel approach
for mining opinions from product reviews,
where it converts opinion mining task to
identify product features, expressions of
opinions and relations between them. By
taking advantage of the observation that a
lot of product features are phrases, a con-
cept of phrase dependency parsing is in-
troduced, which extends traditional depen-
dency parsing to phrase level. This con-
cept is then implemented for extracting re-
lations between product features and ex-
pressions of opinions. Experimental eval-
uations show that the mining task can ben-
efit from phrase dependency parsing.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979146341464">
As millions of users contribute rich information
to the Internet everyday, an enormous number of
product reviews are freely written in blog pages,
Web forums and other consumer-generated medi-
ums (CGMs). This vast richness of content be-
comes increasingly important information source
for collecting and tracking customer opinions. Re-
trieving this information and analyzing this con-
tent are impossible tasks if they were to be manu-
ally done. However, advances in machine learning
and natural language processing present us with
a unique opportunity to automate the decoding of
consumers’ opinions from online reviews.
Previous works on mining opinions can be di-
vided into two directions: sentiment classification
and sentiment related information extraction. The
former is a task of identifying positive and neg-
ative sentiments from a text which can be a pas-
sage, a sentence, a phrase and even a word (So-
masundaran et al., 2008; Pang et al., 2002; Dave
et al., 2003; Kim and Hovy, 2004; Takamura et
al., 2005). The latter focuses on extracting the el-
ements composing a sentiment text. The elements
include source of opinions who expresses an opin-
ion (Choi et al., 2005); target of opinions which
is a receptor of an opinion (Popescu and Etzioni,
2005); opinion expression which delivers an opin-
ion (Wilson et al., 2005b). Some researchers refer
this information extraction task as opinion extrac-
tion or opinion mining. Comparing with the for-
mer one, opinion mining usually produces richer
information.
In this paper, we define an opinion unit as a
triple consisting of a product feature, an expres-
sion of opinion, and an emotional attitude(positive
or negative). We use this definition as the basis for
our opinion mining task. Since a product review
may refer more than one product feature and ex-
press different opinions on each of them, the rela-
tion extraction is an important subtask of opinion
mining. Consider the following sentences:
</bodyText>
<listItem confidence="0.987619">
1. I highly [recommend](1) the Canon SD500(1) to
anybody looking for a compact camera that can take
[good](2) pictures(2).
2. This camera takes [amazing](3) image qualities(3)
</listItem>
<bodyText confidence="0.972280611111111">
and its size(4) [cannot be beat](4).
The phrases underlined are the product features,
marked with square brackets are opinion expres-
sions. Product features and opinion expressions
with identical superscript compose a relation. For
the first sentence, an opinion relation exists be-
tween “the Canon SD500” and “recommend”, but
not between “picture” and “recommend”. The ex-
ample shows that more than one relation may ap-
pear in a sentence, and the correct relations are not
simple Cartesian product of opinion expressions
and product features.
Simple inspection of the data reveals that prod-
uct features usually contain more than one word,
such as “LCD screen”, “image color”, “Canon
PowerShot SD500”, and so on. An incomplete
product feature will confuse the successive anal-
ysis. For example, in passage “Image color is dis-
</bodyText>
<page confidence="0.869398">
1533
</page>
<note confidence="0.9965615">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1533–1541,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999447225806452">
appointed”, the negative sentiment becomes ob-
scure if only “image” or “color” is picked out.
Since a product feature could not be represented
by a single word, dependency parsing might not be
the best approach here unfortunately, which pro-
vides dependency relations only between words.
Previous works on relation extraction usually use
the head word to represent the whole phrase and
extract features from the word level dependency
tree. This solution is problematic because the in-
formation provided by the phrase itself can not be
used by this kind of methods. And, experimental
results show that relation extraction task can ben-
efit from dependencies within a phrase.
To solve this issue, we introduce the concept
of phrase dependency parsing and propose an ap-
proach to construct it. Phrase dependency pars-
ing segments an input sentence into “phrases” and
links segments with directed arcs. The parsing
focuses on the “phrases” and the relations be-
tween them, rather than on the single words inside
each phrase. Because phrase dependency parsing
naturally divides the dependencies into local and
global, a novel tree kernel method has also been
proposed.
The remaining parts of this paper are organized
as follows: In Section 2 we discuss our phrase de-
pendency parsing and our approach. In Section 3,
experiments are given to show the improvements.
In Section 4, we present related work and Section
5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.947737" genericHeader="method">
2 The Approach
</sectionHeader>
<bodyText confidence="0.999221375">
Fig. 1 gives the architecture overview for our ap-
proach, which performs the opinion mining task
in three main steps: (1) constructing phrase de-
pendency tree from results of chunking and de-
pendency parsing; (2) extracting candidate prod-
uct features and candidate opinion expressions; (3)
extracting relations between product features and
opinion expressions.
</bodyText>
<subsectionHeader confidence="0.9895895">
2.1 Phrase Dependency Parsing
2.1.1 Overview of Dependency Grammar
</subsectionHeader>
<bodyText confidence="0.999793142857143">
Dependency grammar is a kind of syntactic the-
ories presented by Lucien Tesni`ere(1959). In de-
pendency grammar, structure is determined by the
relation between a head and its dependents. In
general, the dependent is a modifier or comple-
ment; the head plays a more important role in de-
termining the behaviors of the pair. Therefore, cri-
</bodyText>
<figureCaption confidence="0.999691">
Figure 1: The architecture of our approach.
</figureCaption>
<bodyText confidence="0.999975375">
teria of how to establish dependency relations and
how to distinguish the head and dependent in such
relations is central problem for dependency gram-
mar. Fig. 2(a) shows the dependency represen-
tation of an example sentence. The root of the
sentence is “enjoyed”. There are seven pairs of
dependency relationships, depicted by seven arcs
from heads to dependents.
</bodyText>
<subsectionHeader confidence="0.954458">
2.1.2 Phrase Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999981185185185">
Currently, the mainstream of dependency parsing
is conducted on lexical elements: relations are
built between single words. A major informa-
tion loss of this word level dependency tree com-
pared with constituent tree is that it doesn’t ex-
plicitly provide local structures and syntactic cat-
egories (i.e. NP, VP labels) of phrases (Xia and
Palmer, 2001). On the other hand, dependency
tree provides connections between distant words,
which are useful in extracting long distance rela-
tions. Therefore, compromising between the two,
we extend the dependency tree node with phrases.
That implies a noun phrase “Cannon SD500 Pow-
erShot” can be a dependent that modifies a verb
phrase head “really enjoy using” with relation type
“dobj”. The feasibility behind is that a phrase is a
syntactic unit regardless of the length or syntac-
tic category (Santorini and Kroch, 2007), and it is
acceptable to substitute a single word by a phrase
with same syntactic category in a sentence.
Formally, we define the dependency parsing
with phrase nodes as phrase dependency parsing.
A dependency relationship which is an asymmet-
ric binary relationship holds between two phrases.
One is called head, which is the central phrase in
the relation. The other phrase is called dependent,
which modifies the head. A label representing the
</bodyText>
<figure confidence="0.999706411764706">
Review Crawler
Review
Database
Chunking
Phrase Dependency Parsing
Phrase Dependency Tree
Dependency
Parsing
Candidate
Product Features
Identification
Candidate
Opinion Expressions
Extraction
Opinion
Database
Relation Extraction
</figure>
<page confidence="0.834875">
1534
</page>
<figureCaption confidence="0.999468">
Figure 2: Example of Phrase Dependency Parsing.
</figureCaption>
<bodyText confidence="0.989483135135135">
relation type is assigned to each dependency rela-
tionship, such as subj (subject), obj (object), and
so on. Fig.2(c) shows an example of phrase de-
pendency parsing result.
By comparing the phrase dependency tree and
the word level dependency tree in Fig.2, the for-
mer delivers a more succinct tree structure. Local
words in same phrase are compacted into a sin-
gle node. These words provide local syntactic and
semantic effects which enrich the phrase they be-
long to. But they should have limited influences on
the global tree topology, especially in applications
which emphasis the whole tree structures, such as
tree kernels. Pruning away local dependency re-
lations by additional phrase structure information,
phrase dependency parsing accelerates following
processing of opinion relation extraction.
To construct phrase dependency tree, we pro-
pose a method which combines results from an
existing shallow parser and a lexical dependency
parser. A phrase dependency tree is defined as
T = (V , E ), where V is the set of phrases,
E is the dependency relations among the phrases
in V representing by direct edges. To reserve
the word level dependencies inside a phrase, we
define a nested structure for a phrase Ti in V :
Ti = (U, Ei). Vi = {v1, v2, · · · , vm} is the inter-
nal words, Ei is the internal dependency relations.
We conduct the phrase dependency parsing in
this way: traverses word level dependency tree
in preorder (visits root node first, then traverses
the children recursively). When visits a node R,
searches in its children and finds the node set D
which are in the same phrase with R according
Algorithm 1 Pseudo-Code for constructing the
phrase dependency tree
INPUT:
</bodyText>
<equation confidence="0.9989995">
T0 = (V 0, E0) a word level dependency tree
P = phrases
</equation>
<bodyText confidence="0.5546255">
OUTPUT:
phrase dependency tree T = (V , E ) where
</bodyText>
<equation confidence="0.997873833333333">
V = {T1(V1, E1), T2(V2, E2),··· ,Tn(Vn, En)}
Initialize:
V +– {({v0}, {})|v0 E V 0}
E +– {(Ti, Tj)|(v0i, v0j) E E0, v0i E Vi, v0j E Vj}
R = (Vr, Er) root of T
PhraseDPTree(R, P)
</equation>
<listItem confidence="0.970903285714286">
1: Find pi E P where word[R] E pi
2: for each S = (Vs, Es), (R, S) E E do
3: if word[S] E pi then
4: Vr +– Vr U vs; vs E Vs
5: Er Er U (vr, root[S]); vr E Vr
6: V +– V − S
7: E +– E + (R, l); V(S, l) E E
8: E +– E − (R, S)
9: end if
10: end for
11: for each (R, S) E E do
12: PhraseDPTree(S,P)
13: end for
14: return (V , E )
</listItem>
<bodyText confidence="0.9983125">
to the shallow parsing result. Compacts D and R
into a single node. Then traverses all the remain-
ing children in the same way. The algorithm is
shown in Alg. 1.
The output of the algorithm is still a tree, for we
only cut edges which are compacted into a phrase,
the connectivity is keeped. Note that there will be
inevitable disagrees between shallow parser and
lexical dependency parser, the algorithm implies
that we simply follow the result of the latter one:
the phrases from shallow parser will not appear in
the final result if they cannot be found in the pro-
cedure.
Consider the following example:
</bodyText>
<subsubsectionHeader confidence="0.464758">
“We really enjoyed using the Canon PowerShot SD500.”
</subsubsectionHeader>
<bodyText confidence="0.999898916666667">
Fig.2 shows the procedure of phrase depen-
dency parsing. Fig.2(a) is the result of the lex-
ical dependency parser. Shallow parsers result
is shown in Fig.2(b). Chunk phrases “NP(We)”,
“VP(really enjoyed using)” and “NP(the Canon
PowerShot SD500)” are nodes in the output phrase
dependency tree. When visiting node “enjoyed” in
Fig.2(a), the shallow parser tells that “really” and
“using” which are children of “enjoy” are in the
same phrase with their parent, then the three nodes
are packed. The final phrase dependency parsing
tree is shown in the Fig. 2(c).
</bodyText>
<figure confidence="0.998810717948718">
nsubj
advmod
partmod
dobj
We really
using
SD500
det nn nn
the Canon PowerShot
(a)
NP SEGMENT:
[We]
VP SEGMENT:
[really]
[enjoyed ]
[using]
NP SEGMENT:
[the]
[Canon]
[PowerShot]
[SD500]
NP
NP
We
SD500
det
nn nn
(b)
the
Canon PowerShot
VP
enjoyed
advmod
partmod
really using
nsubj
dobj
(c)
enjoyed
</figure>
<page confidence="0.826585">
1535
</page>
<subsectionHeader confidence="0.97391">
2.2 Candidate Product Features and Opinion
Expressions Extraction
</subsectionHeader>
<bodyText confidence="0.999988888888889">
In this work, we define that product features
are products, product parts, properties of prod-
ucts, properties of parts, company names and re-
lated objects. For example,in consumer elec-
tronic domain, “Canon PowerShot”, “image qual-
ity”,“camera”, “laptop” are all product features.
From analyzing the labeled corpus, we observe
that more than 98% of product features are in a
single phrase, which is either noun phrase (NP) or
verb phrase (VP). Based on it, all NPs and VPs
are selected as candidate product features. While
prepositional phrases (PPs) and adjectival phrases
(ADJPs) are excluded. Although it can cover
nearly all the true product features, the precision
is relatively low. The large amount of noise can-
didates may confuse the relation extraction clas-
sifier. To shrink the size of candidate set, we in-
troduce language model by an intuition that the
more likely a phrase to be a product feature, the
more closely it related to the product review. In
practice, for a certain domain of product reviews,
a language model is build on easily acquired unla-
beled data. Each candidate NP or VP chunk in the
output of shallow parser is scored by the model,
and cut off if its score is less than a threshold.
Opinion expressions are spans of text that ex-
press a comment or attitude of the opinion holder,
which are usually evaluative or subjective phrases.
We also analyze the labeled corpus for opinion ex-
pressions and observe that many opinion expres-
sions are used in multiple domains, which is iden-
tical with the conclusion presented by Kobayashi
et al. (2007). They collected 5,550 opinion ex-
pressions from various sources . The coverage of
the dictionary is high in multiple domains. Moti-
vated by those observations, we use a dictionary
which contains 8221 opinion expressions to select
candidates (Wilson et al., 2005b). An assump-
tion we use to filter candidate opinion expressions
is that opinion expressions tend to appear closely
with product features, which is also used to extract
product features by Hu and Liu (2004). In our ex-
periments, the tree distance between product fea-
ture and opinion expression in a relation should be
less than 5 in the phrase dependency parsing tree.
</bodyText>
<subsectionHeader confidence="0.988402">
2.3 Relation Extraction
</subsectionHeader>
<bodyText confidence="0.99956355882353">
This section describes our method on extracting
relations between opinion expressions and product
features using phrase dependency tree. Manually
built patterns were used in previous works which
have an obvious drawback that those patterns can
hardly cover all possible situations. By taking ad-
vantage of the kernel methods which can search a
feature space much larger than that could be repre-
sented by a feature extraction-based approach, we
define a new tree kernel over phrase dependency
trees and incorporate this kernel within an SVM to
extract relations between opinion expressions and
product features.
The potential relation set consists of the all
combinations between candidate product features
and candidate opinion expressions in a sentence.
Given a phrase dependency parsing tree, we
choose the subtree rooted at the lowest common
parent(LCP) of opinion expression and product
feature to represent the relation.
Dependency tree kernels has been proposed by
(Culotta and Sorensen, 2004). Their kernel is de-
fined on lexical dependency tree by the convolu-
tion of similarities between all possible subtrees.
However, if the convolution containing too many
irrelevant subtrees, over-fitting may occur and de-
creases the performance of the classifier. In phrase
dependency tree, local words in a same phrase are
compacted, therefore it provides a way to treat “lo-
cal dependencies” and “global dependencies” dif-
ferently (Fig. 3). As a consequence, these two
kinds of dependencies will not disturb each other
in measuring similarity. Later experiments prove
the validity of this statement.
</bodyText>
<figureCaption confidence="0.948979">
Figure 3: Example of “local dependencies” and
“global dependencies”.
</figureCaption>
<bodyText confidence="0.9996721">
We generalize the definition by (Culotta and
Sorensen, 2004) to fit the phrase dependency tree.
Use the symbols in Section 2.1.2, 9 i and 9j are
two trees with root Ri and Rj, K(9 i, Jj) is the
kernel function for them. Firstly, each tree node
Tk E 9i is augmented with a set of features F,
and an instance of F for Tk is Fk = {fk}. A
match function m(Ti, Tj) is defined on comparing
a subset of nodes’ features M C_ F. And in the
same way, a similarity function s(Ti, Tj) are de-
</bodyText>
<figure confidence="0.9956367">
Phrase Local dependencies
B
B
C
A C
E
A
D E
Global dependencies
D
</figure>
<page confidence="0.747967">
1536
</page>
<bodyText confidence="0.382694">
fined on 5 C F
</bodyText>
<equation confidence="0.935744857142857">
�
1 if fm = fm bfm E M
m(Ti,Tj) = (1)
0 otherwise
and
s(Ti,Tj) = C(fis, fjs (2)
fsES
</equation>
<bodyText confidence="0.7959462">
where
i 1 if fis = fs
C(fs, fl 0 otherwise (3)
For the given phrase dependency parsing trees,
the kernel function K(9i, 9j) is defined as fol-
</bodyText>
<tableCaption confidence="0.6899945">
low:
Table 1: Statistics for the annotated corpus
</tableCaption>
<figure confidence="0.406476578947368">
Category # Products # Sentences
Cell Phone 2 1100
Diaper 1 375
Digital Camera 4 1470
DVD Player 1 740
MP3 Player 3 3258
subsequences of external children for
After
the kernel computing through training instances,
support
Kc.
vector machine (SVM) is used for classi-
fication.
3 Experiments and Results
K(9i, 9j) = { 0 if m(Ri,Rj) = 0 methods and our results on in-domain an
s(Ri, Rj) + Kin (Ri, Rj) d cross-
+ Kc(Ri.C, Rj.C) otherwise domain.
(4)
where Kin (Ri, Rj) is a kernel function over
</figure>
<equation confidence="0.806758666666667">
Ri = (Vri, Eir) and Rj = (Vrj, Ejr)’s internal
phrase structures,
Kin (Ri, Rj) = K(Ri, Rj) (5)
</equation>
<bodyText confidence="0.836287">
Kc is the kernel function over Ri and Rj’s chil-
dren. Denote a is a continuous subsequence of in-
dices a, a + 1, · · · a + l (a) for Ri’s children where
l(a) is its length, as is the s-th element in a. And
likewise b for Rj.
</bodyText>
<equation confidence="0.998145333333333">
Kc(Ri.C, Rj.C) =
Ea,b,l(a)=l(b) Il(a)K(Ri.[a],Rj.[b]) (6)
X 11s=1..l(a) m(Ri.[as],Rj.[bs])
</equation>
<bodyText confidence="0.996780826086957">
where the constant 0 &lt; A &lt; 1 normalizes the ef-
fects of children subsequences’ length.
Compared with the definitions in (Culotta and
Sorensen, 2004), we add term Kin to handle the
internal nodes of a pharse, and make this exten-
sion still satisfy the kernel function requirements
(composition of kernels is still a kernel (Joachims
et al., 2001)). The consideration is that the local
words should have limited effects on whole tree
structures. So the kernel is defined on external
children (Kc) and internal nodes (Kin) separately,
annotator
extracted 3595 relations, while the
other annotator A2 extracted 3745 relations, an
A1
d
3217 cases of them matched. In order to measure
the annotation quality, we use the following metric
to measure the inter-annotator agreement, which is
also used by Wiebe et al. (2005).
as the result, the local words are not involved in
In this section, we describe the annotated corpus
and experiment configurations including baseline
</bodyText>
<subsectionHeader confidence="0.989473">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999995222222222">
We conducted experiments with labeled corpus
which are selected from Hu and Liu (2004), Jin-
dal and Liu (2008) have built. Their documents
are collected from Amazon.com and CNet.com,
where products have a large number of reviews.
They also manually labeled product features and
polarity orientations. Our corpus is selected
from them, which contains customer reviews of
11 products belong to 5 categories(Diaper, Cell
Phone, Digital Camera, DVD Player, and MP3
Player). Table 1 gives the detail statistics.
Since we need to evaluate not only the prod-
uct features but also the opinion expressions and
relations between them, we asked two annotators
to annotate them independently. The annotators
started from identifying product features. Then for
each product feature, they annotated the opinion
expression which has relation with it. Finally, one
</bodyText>
<equation confidence="0.860080666666667">
ay a
(l |b) = |A matches B|
|A|
</equation>
<page confidence="0.992355">
1537
</page>
<tableCaption confidence="0.983438">
Table 2: Results for extracting product features
and opinion expressions
</tableCaption>
<table confidence="0.999961">
P R F
Product Feature 42.8% 85.5% 57.0%
Opinion Expression 52.5% 75.2% 61.8%
</table>
<tableCaption confidence="0.920861">
Table 3: Features used in SVM-1: o denotes an
opinion expression and t a product feature
</tableCaption>
<listItem confidence="0.9972148">
1) Positions of o/t in sentence(start, end, other);
2) The distance between o and t (1, 2, 3, 4, other);
3) Whether o and t have direct dependency relation;
4) Whether o precedes t;
5) POS-Tags of o/t.
</listItem>
<tableCaption confidence="0.943808">
Table 4: Features used in SVM-PTree
</tableCaption>
<bodyText confidence="0.383796">
Features for match function
</bodyText>
<listItem confidence="0.989441846153846">
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) Whether it is an opinion expression node
3) Whether it is a product future node.
Features for similarity function
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) POS-Tag of the head word of node’s internal
phrases.
3) The type of phrase dependency edge linking
to node’s parent.
4) Feature 2) for the node’s parent
5) Feature 3) for the node’s parent
</listItem>
<bodyText confidence="0.999156">
where agr(a||b) represents the inter-annotator
agreement between annotator a and b, A and B
are the sets of anchors annotated by annotators a
and b. agr(A1||A2) was 85.9% and agr(A2||A1)
was 89.5%. It indicates that the reliability of our
annotated corpus is satisfactory.
</bodyText>
<subsectionHeader confidence="0.999977">
3.2 Preprocessing Results
</subsectionHeader>
<bodyText confidence="0.999909">
Results of extracting product features and opin-
ion expressions are shown in Table 2. We use
precision, recall and F-measure to evaluate perfor-
mances. The candidate product features are ex-
tracted by the method described in Section 2.2,
whose result is in the first row. 6760 of 24414
candidate product features remained after the fil-
tering, which means we cut 72% of irrelevant can-
didates with a cost of 14.5%(1-85.5%) loss in true
answers. Similar to the product feature extraction,
the precision of extracting opinion expression is
relatively low, while the recall is 75.2%. Since
both product features and opinion expressions ex-
tractions are preprocessing steps, recall is more
important.
</bodyText>
<subsectionHeader confidence="0.998018">
3.3 Relation Extraction Experiments
3.3.1 Experiments Settings
</subsectionHeader>
<bodyText confidence="0.97737">
In order to compare with state-of-the-art results,
we also evaluated the following methods.
</bodyText>
<listItem confidence="0.98626862962963">
1. Adjacent method extracts relations between a
product feature and its nearest opinion expression,
which is also used in (Hu and Liu, 2004).
2. SVM-1. To compare with tree kernel based
approaches, we evaluated an SVM1 result with a
set of manually selected features(Table 3), which
are also used in (Kobayashi et al., 2007).
3. SVM-2 is designed to compare the effective-
ness of cross-domain performances. The features
used are simple bag of words and POS-Tags be-
tween opinion expressions and product features.
4. SVM-WTree uses head words of opinion ex-
pressions and product features in the word-level
dependency tree, as the previous works in infor-
mation extraction. Then conducts tree kernel pro-
posed by Culotta and Sorensen (2004).
5. SVM-PTree denotes the results of our tree-
kernel based SVM, which is described in the Sec-
tion 2.3. Stanford parser (Klein and Manning,
2002) and Sundance (Riloff and Phillips, 2004)
are used as lexical dependency parser and shallow
parser. The features in match function and simi-
larity function are shown in Table 4.
6. OERight is the result of SVM-PTree with
correct opinion expressions.
7. PFRight is the result of SVM-PTree with
correct product features.
</listItem>
<bodyText confidence="0.999843857142857">
Table 5 shows the performances of different
relation extraction methods with in-domain data.
For each domain, we conducted 5-fold cross val-
idation. Table 6 shows the performances of the
extraction methods on cross-domain data. We use
the digital camera and cell phone domain as train-
ing set. The other domains are used as testing set.
</bodyText>
<footnote confidence="0.992878">
1libsvm 2.88 is used in our experiments
</footnote>
<page confidence="0.995535">
1538
</page>
<tableCaption confidence="0.999592">
Table 5: Results of different methods
</tableCaption>
<table confidence="0.999947888888889">
Cell Phone MP3 Player Digital Camera DVD Player Diaper
Methods P R F P R F P R F P R F P R F
Adjacent 40.3% 60.5% 48.4% 26.5% 59.3% 36.7% 32.7% 59.1% 42.1% 31.8% 68.4% 43.4% 23.4% 78.8% 36.1%
SVM-1 69.5% 42.3% 52.6% 60.7% 30.6% 40.7% 61.4% 32.4% 42.4% 56.0% 27.6% 37.0% 29.3% 14.1% 19.0%
SVM-2 60.7% 19.7% 29.7% 63.6% 23.8% 34.6% 66.9% 23.3% 34.6% 66.7% 13.2% 22.0% 79.2% 22.4% 34.9%
SVM-WTree 52.6% 52.7% 52.6% 46.4% 43.8% 45.1% 49.1% 46.0% 47.5% 35.9% 32.0% 33.8% 36.6% 31.7% 34.0%
SVM-PTree 55.6% 57.2% 56.4% 51.7% 50.7% 51.2% 54.0% 49.9% 51.9% 37.1% 35.4% 36.2% 37.3% 30.5% 33.6%
OERight 66.7% 69.5% 68.1% 65.6% 65.9% 65.7% 64.3% 61.0% 62.6% 59.9% 63.9% 61.8% 55.8% 58.5% 57.1%
PFRight 62.8% 62.1% 62.4% 61.3% 56.8% 59.0% 59.7% 56.2% 57.9% 46.9% 46.6% 46.7% 58.5% 51.3% 53.4%
</table>
<tableCaption confidence="0.744357">
Table 6: Results for total performance with cross domain training data
</tableCaption>
<table confidence="0.999354285714286">
Diaper DVD Player MP3 Player
Methods P R F P R F P R F
Adjacent 23.4% 78.8% 36.1% 31.8% 68.4% 43.4% 26.5% 59.3% 36.7%
SVM-1 22.4% 30.6% 25.9% 52.8% 30.9% 39.0% 55.9% 36.8% 44.4%
SVM-2 71.9% 15.1% 25.0% 51.2% 13.2% 21.0% 63.1% 22.0% 32.6%
SVM-WTree 38.7% 52.4% 44.5% 30.7% 59.2% 40.4% 38.1% 47.2% 42.2%
SVM-PTree 37.3% 53.7% 44.0% 59.2% 48.3% 46.3% 43.0% 48.9% 45.8%
</table>
<sectionHeader confidence="0.872769" genericHeader="method">
3.3.2 Results Discussion
</sectionHeader>
<bodyText confidence="0.999841036363636">
Table 5 presents different methods’ results in five
domains. We observe that the three learning based
methods(SVM-1, SVM-WTree, SVM-PTree) per-
form better than the Adjacent baseline in the first
three domains. However, in other domains, di-
rectly adjacent method is better than the learning
based methods. The main difference between the
first three domains and the last two domains is the
size of data(Table 1). It implies that the simple Ad-
jacent method is also competent when the training
set is small.
A further inspection into the result of first 3
domains, we can also conclude that: 1) Tree
kernels(SVM-WTree and SVM-PTree) are better
than Adjacent, SVM-1 and SVM-2 in all domains.
It proofs that the dependency tree is important
in the opinion relation extraction. The reason
for that is a connection between an opinion and
its target can be discovered with various syntac-
tic structures. 2) The kernel defined on phrase
dependency tree (SVM-PTree) outperforms ker-
nel defined on word level dependency tree(SVM-
WTree) by 4.8% in average. We believe the main
reason is that phrase dependency tree provides a
more succinct tree structure, and the separative
treatment of local dependencies and global depen-
dencies in kernel computation can indeed improve
the performance of relation extraction.
To analysis the results of preprocessing steps’
influences on the following relation extraction,
we provide 2 additional experiments which the
product features and opinion expressions are all
correctly extracted respectively: OERight and
PFRight. These two results show that given an
exactly extraction of opinion expression and prod-
uct feature, the results of opinion relation extrac-
tion will be much better. Further, opinion expres-
sions are more influential which naturally means
the opinion expressions are crucial in opinion re-
lation extraction.
For evaluations on cross domain, the Adjacent
method doesn’t need training data, its results are
the same as the in-domain experiments. Note
in Table 3 and Table 4, we don’t use domain
related features in SVM-1, SVM-WTree, SVM-
PTree, but SVM-2’s features are domain depen-
dent. Since the cross-domain training set is larger
than the original one in Diaper and DVD domain,
the models are trained more sufficiently. The fi-
nal results on cross-domain are even better than
in-domain experiments on SVM-1, SVM-WTree,
and SVM-PTree with percentage of 4.6%, 8.6%,
10.3% in average. And the cross-domain train-
ing set is smaller than in-domain in MP3, but
it also achieve competitive performance with the
</bodyText>
<page confidence="0.984345">
1539
</page>
<bodyText confidence="0.9999426">
in-domain. On the other hand, SVM-2’s result
decreased compared with the in-domain experi-
ments because the test domain changed. At the
same time, SVM-PTree outperforms other meth-
ods which is similar in in-domain experiments.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999972416666667">
Opinion mining has recently received consider-
able attention. Amount of works have been
done on sentimental classification in different lev-
els (Zhang et al., 2009; Somasundaran et al., 2008;
Pang et al., 2002; Dave et al., 2003; Kim and
Hovy, 2004; Takamura et al., 2005). While we
focus on extracting product features, opinion ex-
pressions and mining relations in this paper.
Kobayashi et al. (2007) presented their work on
extracting opinion units including: opinion holder,
subject, aspect and evaluation. Subject and aspect
belong to product features, while evaluation is the
opinion expression in our work. They converted
the task to two kinds of relation extraction tasks
and proposed a machine learning-based method
which combines contextual clues and statistical
clues. Their experimental results showed that the
model using contextual clues improved the perfor-
mance. However since the contextual information
in a domain is specific, the model got by their ap-
proach can not easily converted to other domains.
Choi et al. (2006) used an integer linear pro-
gramming approach to jointly extract entities and
relations in the context of opinion oriented infor-
mation extraction. They identified expressions of
opinions, sources of opinions and the linking re-
lation that exists between them. The sources of
opinions denote to the person or entity that holds
the opinion.
Another area related to our work is opinion
expressions identification (Wilson et al., 2005a;
Breck et al., 2007). They worked on identify-
ing the words and phrases that express opinions
in text. According to Wiebe et al. (2005), there are
two types of opinion expressions, direct subjective
expressions and expressive subjective elements.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999970545454546">
In this paper, we described our work on min-
ing opinions from unstructured documents. We
focused on extracting relations between product
features and opinion expressions. The novelties
of our work included: 1) we defined the phrase
dependency parsing and proposed an approach
to construct the phrase dependency trees; 2) we
proposed a new tree kernel function to model
the phrase dependency trees. Experimental re-
sults show that our approach improved the perfor-
mances of the mining task.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998925">
This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302. The authors would like to thank the
reviewers for their useful comments.
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999846472222222">
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI-2007.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In Proceedings of HLT/EMNLP.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings EMNLP.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In In Proceed-
ings of ACL 2004.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of WWW 2003.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the ACM
SIGKDD 2004.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of WSDM ’08.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of ICML ’01.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of Coling
2004. COLING.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL 2007.
</reference>
<page confidence="0.790891">
1540
</page>
<reference confidence="0.999648829787234">
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proc. of EMNLP
2002.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT/EMNLP.
E. Riloff and W. Phillips. 2004. An introduction to
the sundance and autoslog systems. In University of
Utah School of Computing Technical Report UUCS-
04-015.
Beatrice Santorini and Anthony Kroch. 2007.
The syntax of natural language: An on-
line introduction using the Trees program.
http://www.ling.upenn.edu/ beatrice/syntax-
textbook.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of COLING 2008.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of ACL’05.
L. Tesni`ere. 1959. El´ements de syntaxe structurale.
Editions Klincksieck.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2/3).
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: A system for subjectiv-
ity analysis. In Demonstration Description in Con-
ference on Empirical Methods in Natural Language
Processing.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In HLT ’01:
Proceedings of the first international conference on
Human language technology research.
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,
Joseph Johnson, and Xuanjing Huang. 2009. Min-
ing product reviews based on shallow dependency
parsing. In Proceedings of SIGIR 2009.
</reference>
<page confidence="0.994274">
1541
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.444139">
<title confidence="0.999901">Phrase Dependency Parsing for Opinion Mining</title>
<author confidence="0.729075">Yuanbin Wu</author>
<author confidence="0.729075">Qi Zhang</author>
<author confidence="0.729075">Xuanjing Huang</author>
<author confidence="0.729075">Lide Fudan</author>
<affiliation confidence="0.9605">School of Computer</affiliation>
<abstract confidence="0.9957279375">In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them. By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level. This concept is then implemented for extracting relations between product features and expressions of opinions. Experimental evaluations show that the mining task can benefit from phrase dependency parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI-2007.</booktitle>
<contexts>
<context position="28709" citStr="Breck et al., 2007" startWordPosition="4737" endWordPosition="4740">formance. However since the contextual information in a domain is specific, the model got by their approach can not easily converted to other domains. Choi et al. (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. They identified expressions of opinions, sources of opinions and the linking relation that exists between them. The sources of opinions denote to the person or entity that holds the opinion. Another area related to our work is opinion expressions identification (Wilson et al., 2005a; Breck et al., 2007). They worked on identifying the words and phrases that express opinions in text. According to Wiebe et al. (2005), there are two types of opinion expressions, direct subjective expressions and expressive subjective elements. 5 Conclusions In this paper, we described our work on mining opinions from unstructured documents. We focused on extracting relations between product features and opinion expressions. The novelties of our work included: 1) we defined the phrase dependency parsing and proposed an approach to construct the phrase dependency trees; 2) we proposed a new tree kernel function t</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of IJCAI-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="1979" citStr="Choi et al., 2005" startWordPosition="307" endWordPosition="310"> automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b). Some researchers refer this information extraction task as opinion extraction or opinion mining. Comparing with the former one, opinion mining usually produces richer information. In this paper, we define an opinion unit as a triple consisting of a product feature, an expression of opinion, and an emotional attitude(positive or negative). We use this definition as the basis for our opinion mining task. Since a product review may refer more tha</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Eric Breck</author>
<author>Claire Cardie</author>
</authors>
<title>Joint extraction of entities and relations for opinion recognition.</title>
<date>2006</date>
<booktitle>In Proceedings EMNLP.</booktitle>
<contexts>
<context position="28259" citStr="Choi et al. (2006)" startWordPosition="4666" endWordPosition="4669">work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experimental results showed that the model using contextual clues improved the performance. However since the contextual information in a domain is specific, the model got by their approach can not easily converted to other domains. Choi et al. (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. They identified expressions of opinions, sources of opinions and the linking relation that exists between them. The sources of opinions denote to the person or entity that holds the opinion. Another area related to our work is opinion expressions identification (Wilson et al., 2005a; Breck et al., 2007). They worked on identifying the words and phrases that express opinions in text. According to Wiebe et al. (2005), there are two types of opinion exp</context>
</contexts>
<marker>Choi, Breck, Cardie, 2006</marker>
<rawString>Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition. In Proceedings EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction. In</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="15229" citStr="Culotta and Sorensen, 2004" startWordPosition="2500" endWordPosition="2503">an that could be represented by a feature extraction-based approach, we define a new tree kernel over phrase dependency trees and incorporate this kernel within an SVM to extract relations between opinion expressions and product features. The potential relation set consists of the all combinations between candidate product features and candidate opinion expressions in a sentence. Given a phrase dependency parsing tree, we choose the subtree rooted at the lowest common parent(LCP) of opinion expression and product feature to represent the relation. Dependency tree kernels has been proposed by (Culotta and Sorensen, 2004). Their kernel is defined on lexical dependency tree by the convolution of similarities between all possible subtrees. However, if the convolution containing too many irrelevant subtrees, over-fitting may occur and decreases the performance of the classifier. In phrase dependency tree, local words in a same phrase are compacted, therefore it provides a way to treat “local dependencies” and “global dependencies” differently (Fig. 3). As a consequence, these two kinds of dependencies will not disturb each other in measuring similarity. Later experiments prove the validity of this statement. Figu</context>
<context position="17779" citStr="Culotta and Sorensen, 2004" startWordPosition="2969" endWordPosition="2972">.C) otherwise domain. (4) where Kin (Ri, Rj) is a kernel function over Ri = (Vri, Eir) and Rj = (Vrj, Ejr)’s internal phrase structures, Kin (Ri, Rj) = K(Ri, Rj) (5) Kc is the kernel function over Ri and Rj’s children. Denote a is a continuous subsequence of indices a, a + 1, · · · a + l (a) for Ri’s children where l(a) is its length, as is the s-th element in a. And likewise b for Rj. Kc(Ri.C, Rj.C) = Ea,b,l(a)=l(b) Il(a)K(Ri.[a],Rj.[b]) (6) X 11s=1..l(a) m(Ri.[as],Rj.[bs]) where the constant 0 &lt; A &lt; 1 normalizes the effects of children subsequences’ length. Compared with the definitions in (Culotta and Sorensen, 2004), we add term Kin to handle the internal nodes of a pharse, and make this extension still satisfy the kernel function requirements (composition of kernels is still a kernel (Joachims et al., 2001)). The consideration is that the local words should have limited effects on whole tree structures. So the kernel is defined on external children (Kc) and internal nodes (Kin) separately, annotator extracted 3595 relations, while the other annotator A2 extracted 3745 relations, an A1 d 3217 cases of them matched. In order to measure the annotation quality, we use the following metric to measure the int</context>
<context position="22319" citStr="Culotta and Sorensen (2004)" startWordPosition="3706" endWordPosition="3709">is also used in (Hu and Liu, 2004). 2. SVM-1. To compare with tree kernel based approaches, we evaluated an SVM1 result with a set of manually selected features(Table 3), which are also used in (Kobayashi et al., 2007). 3. SVM-2 is designed to compare the effectiveness of cross-domain performances. The features used are simple bag of words and POS-Tags between opinion expressions and product features. 4. SVM-WTree uses head words of opinion expressions and product features in the word-level dependency tree, as the previous works in information extraction. Then conducts tree kernel proposed by Culotta and Sorensen (2004). 5. SVM-PTree denotes the results of our treekernel based SVM, which is described in the Section 2.3. Stanford parser (Klein and Manning, 2002) and Sundance (Riloff and Phillips, 2004) are used as lexical dependency parser and shallow parser. The features in match function and similarity function are shown in Table 4. 6. OERight is the result of SVM-PTree with correct opinion expressions. 7. PFRight is the result of SVM-PTree with correct product features. Table 5 shows the performances of different relation extraction methods with in-domain data. For each domain, we conducted 5-fold cross va</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW</booktitle>
<contexts>
<context position="1775" citStr="Dave et al., 2003" startWordPosition="272" endWordPosition="275">information and analyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b). Some researchers refer this information extraction task as opinion extraction or opinion mining. Comparing with the former one, opinion mining usually produces richer information. In this paper, we define an opinion unit as a triple consisting</context>
<context position="27452" citStr="Dave et al., 2003" startWordPosition="4542" endWordPosition="4545"> 8.6%, 10.3% in average. And the cross-domain training set is smaller than in-domain in MP3, but it also achieve competitive performance with the 1539 in-domain. On the other hand, SVM-2’s result decreased compared with the in-domain experiments because the test domain changed. At the same time, SVM-PTree outperforms other methods which is similar in in-domain experiments. 4 Related Work Opinion mining has recently received considerable attention. Amount of works have been done on sentimental classification in different levels (Zhang et al., 2009; Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). While we focus on extracting product features, opinion expressions and mining relations in this paper. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experimental results showed that the model u</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of WWW 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD</booktitle>
<contexts>
<context position="14045" citStr="Hu and Liu (2004)" startWordPosition="2319" endWordPosition="2322">ons and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree. 2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree. Manually built patterns were used in previous works which have an obvious drawback that those patterns can hardly cover all possible situations. By taking advantage of the kernel methods which can search a feature space much larger than that could be represented by a feature e</context>
<context position="18693" citStr="Hu and Liu (2004)" startWordPosition="3117" endWordPosition="3120"> the kernel is defined on external children (Kc) and internal nodes (Kin) separately, annotator extracted 3595 relations, while the other annotator A2 extracted 3745 relations, an A1 d 3217 cases of them matched. In order to measure the annotation quality, we use the following metric to measure the inter-annotator agreement, which is also used by Wiebe et al. (2005). as the result, the local words are not involved in In this section, we describe the annotated corpus and experiment configurations including baseline 3.1 Corpus We conducted experiments with labeled corpus which are selected from Hu and Liu (2004), Jindal and Liu (2008) have built. Their documents are collected from Amazon.com and CNet.com, where products have a large number of reviews. They also manually labeled product features and polarity orientations. Our corpus is selected from them, which contains customer reviews of 11 products belong to 5 categories(Diaper, Cell Phone, Digital Camera, DVD Player, and MP3 Player). Table 1 gives the detail statistics. Since we need to evaluate not only the product features but also the opinion expressions and relations between them, we asked two annotators to annotate them independently. The ann</context>
<context position="21726" citStr="Hu and Liu, 2004" startWordPosition="3610" endWordPosition="3613">elevant candidates with a cost of 14.5%(1-85.5%) loss in true answers. Similar to the product feature extraction, the precision of extracting opinion expression is relatively low, while the recall is 75.2%. Since both product features and opinion expressions extractions are preprocessing steps, recall is more important. 3.3 Relation Extraction Experiments 3.3.1 Experiments Settings In order to compare with state-of-the-art results, we also evaluated the following methods. 1. Adjacent method extracts relations between a product feature and its nearest opinion expression, which is also used in (Hu and Liu, 2004). 2. SVM-1. To compare with tree kernel based approaches, we evaluated an SVM1 result with a set of manually selected features(Table 3), which are also used in (Kobayashi et al., 2007). 3. SVM-2 is designed to compare the effectiveness of cross-domain performances. The features used are simple bag of words and POS-Tags between opinion expressions and product features. 4. SVM-WTree uses head words of opinion expressions and product features in the word-level dependency tree, as the previous works in information extraction. Then conducts tree kernel proposed by Culotta and Sorensen (2004). 5. SV</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the ACM SIGKDD 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of WSDM ’08.</booktitle>
<contexts>
<context position="18716" citStr="Jindal and Liu (2008)" startWordPosition="3121" endWordPosition="3125">ned on external children (Kc) and internal nodes (Kin) separately, annotator extracted 3595 relations, while the other annotator A2 extracted 3745 relations, an A1 d 3217 cases of them matched. In order to measure the annotation quality, we use the following metric to measure the inter-annotator agreement, which is also used by Wiebe et al. (2005). as the result, the local words are not involved in In this section, we describe the annotated corpus and experiment configurations including baseline 3.1 Corpus We conducted experiments with labeled corpus which are selected from Hu and Liu (2004), Jindal and Liu (2008) have built. Their documents are collected from Amazon.com and CNet.com, where products have a large number of reviews. They also manually labeled product features and polarity orientations. Our corpus is selected from them, which contains customer reviews of 11 products belong to 5 categories(Diaper, Cell Phone, Digital Camera, DVD Player, and MP3 Player). Table 1 gives the detail statistics. Since we need to evaluate not only the product features but also the opinion expressions and relations between them, we asked two annotators to annotate them independently. The annotators started from id</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceedings of WSDM ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Nello Cristianini</author>
<author>John ShaweTaylor</author>
</authors>
<title>Composite kernels for hypertext categorisation.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML ’01.</booktitle>
<contexts>
<context position="17975" citStr="Joachims et al., 2001" startWordPosition="3003" endWordPosition="3006">nd Rj’s children. Denote a is a continuous subsequence of indices a, a + 1, · · · a + l (a) for Ri’s children where l(a) is its length, as is the s-th element in a. And likewise b for Rj. Kc(Ri.C, Rj.C) = Ea,b,l(a)=l(b) Il(a)K(Ri.[a],Rj.[b]) (6) X 11s=1..l(a) m(Ri.[as],Rj.[bs]) where the constant 0 &lt; A &lt; 1 normalizes the effects of children subsequences’ length. Compared with the definitions in (Culotta and Sorensen, 2004), we add term Kin to handle the internal nodes of a pharse, and make this extension still satisfy the kernel function requirements (composition of kernels is still a kernel (Joachims et al., 2001)). The consideration is that the local words should have limited effects on whole tree structures. So the kernel is defined on external children (Kc) and internal nodes (Kin) separately, annotator extracted 3595 relations, while the other annotator A2 extracted 3745 relations, an A1 d 3217 cases of them matched. In order to measure the annotation quality, we use the following metric to measure the inter-annotator agreement, which is also used by Wiebe et al. (2005). as the result, the local words are not involved in In this section, we describe the annotated corpus and experiment configuration</context>
</contexts>
<marker>Joachims, Cristianini, ShaweTaylor, 2001</marker>
<rawString>Thorsten Joachims, Nello Cristianini, and John ShaweTaylor. 2001. Composite kernels for hypertext categorisation. In Proceedings of ICML ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<publisher>COLING.</publisher>
<contexts>
<context position="1795" citStr="Kim and Hovy, 2004" startWordPosition="276" endWordPosition="279">lyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b). Some researchers refer this information extraction task as opinion extraction or opinion mining. Comparing with the former one, opinion mining usually produces richer information. In this paper, we define an opinion unit as a triple consisting of a product featur</context>
<context position="27472" citStr="Kim and Hovy, 2004" startWordPosition="4546" endWordPosition="4549">rage. And the cross-domain training set is smaller than in-domain in MP3, but it also achieve competitive performance with the 1539 in-domain. On the other hand, SVM-2’s result decreased compared with the in-domain experiments because the test domain changed. At the same time, SVM-PTree outperforms other methods which is similar in in-domain experiments. 4 Related Work Opinion mining has recently received considerable attention. Amount of works have been done on sentimental classification in different levels (Zhang et al., 2009; Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). While we focus on extracting product features, opinion expressions and mining relations in this paper. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experimental results showed that the model using contextual clue</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of Coling 2004. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="22463" citStr="Klein and Manning, 2002" startWordPosition="3731" endWordPosition="3734">cted features(Table 3), which are also used in (Kobayashi et al., 2007). 3. SVM-2 is designed to compare the effectiveness of cross-domain performances. The features used are simple bag of words and POS-Tags between opinion expressions and product features. 4. SVM-WTree uses head words of opinion expressions and product features in the word-level dependency tree, as the previous works in information extraction. Then conducts tree kernel proposed by Culotta and Sorensen (2004). 5. SVM-PTree denotes the results of our treekernel based SVM, which is described in the Section 2.3. Stanford parser (Klein and Manning, 2002) and Sundance (Riloff and Phillips, 2004) are used as lexical dependency parser and shallow parser. The features in match function and similarity function are shown in Table 4. 6. OERight is the result of SVM-PTree with correct opinion expressions. 7. PFRight is the result of SVM-PTree with correct product features. Table 5 shows the performances of different relation extraction methods with in-domain data. For each domain, we conducted 5-fold cross validation. Table 6 shows the performances of the extraction methods on cross-domain data. We use the digital camera and cell phone domain as trai</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. Fast exact inference with a factored model for natural language parsing. In In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<contexts>
<context position="13579" citStr="Kobayashi et al. (2007)" startWordPosition="2244" endWordPosition="2247"> product review. In practice, for a certain domain of product reviews, a language model is build on easily acquired unlabeled data. Each candidate NP or VP chunk in the output of shallow parser is scored by the model, and cut off if its score is less than a threshold. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phra</context>
<context position="21910" citStr="Kobayashi et al., 2007" startWordPosition="3641" endWordPosition="3644">, while the recall is 75.2%. Since both product features and opinion expressions extractions are preprocessing steps, recall is more important. 3.3 Relation Extraction Experiments 3.3.1 Experiments Settings In order to compare with state-of-the-art results, we also evaluated the following methods. 1. Adjacent method extracts relations between a product feature and its nearest opinion expression, which is also used in (Hu and Liu, 2004). 2. SVM-1. To compare with tree kernel based approaches, we evaluated an SVM1 result with a set of manually selected features(Table 3), which are also used in (Kobayashi et al., 2007). 3. SVM-2 is designed to compare the effectiveness of cross-domain performances. The features used are simple bag of words and POS-Tags between opinion expressions and product features. 4. SVM-WTree uses head words of opinion expressions and product features in the word-level dependency tree, as the previous works in information extraction. Then conducts tree kernel proposed by Culotta and Sorensen (2004). 5. SVM-PTree denotes the results of our treekernel based SVM, which is described in the Section 2.3. Stanford parser (Klein and Manning, 2002) and Sundance (Riloff and Phillips, 2004) are u</context>
<context position="27624" citStr="Kobayashi et al. (2007)" startWordPosition="4570" endWordPosition="4573">he other hand, SVM-2’s result decreased compared with the in-domain experiments because the test domain changed. At the same time, SVM-PTree outperforms other methods which is similar in in-domain experiments. 4 Related Work Opinion mining has recently received considerable attention. Amount of works have been done on sentimental classification in different levels (Zhang et al., 2009; Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). While we focus on extracting product features, opinion expressions and mining relations in this paper. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experimental results showed that the model using contextual clues improved the performance. However since the contextual information in a domain is specific, the model got by their approach can not easily converted t</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="1756" citStr="Pang et al., 2002" startWordPosition="268" endWordPosition="271">s. Retrieving this information and analyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b). Some researchers refer this information extraction task as opinion extraction or opinion mining. Comparing with the former one, opinion mining usually produces richer information. In this paper, we define an opinion unit as </context>
<context position="27433" citStr="Pang et al., 2002" startWordPosition="4538" endWordPosition="4541">percentage of 4.6%, 8.6%, 10.3% in average. And the cross-domain training set is smaller than in-domain in MP3, but it also achieve competitive performance with the 1539 in-domain. On the other hand, SVM-2’s result decreased compared with the in-domain experiments because the test domain changed. At the same time, SVM-PTree outperforms other methods which is similar in in-domain experiments. 4 Related Work Opinion mining has recently received considerable attention. Amount of works have been done on sentimental classification in different levels (Zhang et al., 2009; Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). While we focus on extracting product features, opinion expressions and mining relations in this paper. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experimental results show</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proc. of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="2061" citStr="Popescu and Etzioni, 2005" startWordPosition="321" endWordPosition="324">s works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b). Some researchers refer this information extraction task as opinion extraction or opinion mining. Comparing with the former one, opinion mining usually produces richer information. In this paper, we define an opinion unit as a triple consisting of a product feature, an expression of opinion, and an emotional attitude(positive or negative). We use this definition as the basis for our opinion mining task. Since a product review may refer more than one product feature and express different opinions on each of them, the relation</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>W Phillips</author>
</authors>
<title>An introduction to the sundance and autoslog systems.</title>
<date>2004</date>
<tech>Technical Report UUCS04-015.</tech>
<institution>In University of Utah School of Computing</institution>
<contexts>
<context position="22504" citStr="Riloff and Phillips, 2004" startWordPosition="3737" endWordPosition="3740">used in (Kobayashi et al., 2007). 3. SVM-2 is designed to compare the effectiveness of cross-domain performances. The features used are simple bag of words and POS-Tags between opinion expressions and product features. 4. SVM-WTree uses head words of opinion expressions and product features in the word-level dependency tree, as the previous works in information extraction. Then conducts tree kernel proposed by Culotta and Sorensen (2004). 5. SVM-PTree denotes the results of our treekernel based SVM, which is described in the Section 2.3. Stanford parser (Klein and Manning, 2002) and Sundance (Riloff and Phillips, 2004) are used as lexical dependency parser and shallow parser. The features in match function and similarity function are shown in Table 4. 6. OERight is the result of SVM-PTree with correct opinion expressions. 7. PFRight is the result of SVM-PTree with correct product features. Table 5 shows the performances of different relation extraction methods with in-domain data. For each domain, we conducted 5-fold cross validation. Table 6 shows the performances of the extraction methods on cross-domain data. We use the digital camera and cell phone domain as training set. The other domains are used as t</context>
</contexts>
<marker>Riloff, Phillips, 2004</marker>
<rawString>E. Riloff and W. Phillips. 2004. An introduction to the sundance and autoslog systems. In University of Utah School of Computing Technical Report UUCS04-015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
<author>Anthony Kroch</author>
</authors>
<title>The syntax of natural language: An online introduction using the Trees program.</title>
<date>2007</date>
<note>http://www.ling.upenn.edu/ beatrice/syntaxtextbook.</note>
<contexts>
<context position="7404" citStr="Santorini and Kroch, 2007" startWordPosition="1169" endWordPosition="1172">plicitly provide local structures and syntactic categories (i.e. NP, VP labels) of phrases (Xia and Palmer, 2001). On the other hand, dependency tree provides connections between distant words, which are useful in extracting long distance relations. Therefore, compromising between the two, we extend the dependency tree node with phrases. That implies a noun phrase “Cannon SD500 PowerShot” can be a dependent that modifies a verb phrase head “really enjoy using” with relation type “dobj”. The feasibility behind is that a phrase is a syntactic unit regardless of the length or syntactic category (Santorini and Kroch, 2007), and it is acceptable to substitute a single word by a phrase with same syntactic category in a sentence. Formally, we define the dependency parsing with phrase nodes as phrase dependency parsing. A dependency relationship which is an asymmetric binary relationship holds between two phrases. One is called head, which is the central phrase in the relation. The other phrase is called dependent, which modifies the head. A label representing the Review Crawler Review Database Chunking Phrase Dependency Parsing Phrase Dependency Tree Dependency Parsing Candidate Product Features Identification Can</context>
</contexts>
<marker>Santorini, Kroch, 2007</marker>
<rawString>Beatrice Santorini and Anthony Kroch. 2007. The syntax of natural language: An online introduction using the Trees program. http://www.ling.upenn.edu/ beatrice/syntaxtextbook.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
<author>Josef Ruppenhofer</author>
</authors>
<title>Discourse level opinion interpretation.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1737" citStr="Somasundaran et al., 2008" startWordPosition="263" endWordPosition="267">d tracking customer opinions. Retrieving this information and analyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b). Some researchers refer this information extraction task as opinion extraction or opinion mining. Comparing with the former one, opinion mining usually produces richer information. In this paper, we define </context>
<context position="27414" citStr="Somasundaran et al., 2008" startWordPosition="4534" endWordPosition="4537">-WTree, and SVM-PTree with percentage of 4.6%, 8.6%, 10.3% in average. And the cross-domain training set is smaller than in-domain in MP3, but it also achieve competitive performance with the 1539 in-domain. On the other hand, SVM-2’s result decreased compared with the in-domain experiments because the test domain changed. At the same time, SVM-PTree outperforms other methods which is similar in in-domain experiments. 4 Related Work Opinion mining has recently received considerable attention. Amount of works have been done on sentimental classification in different levels (Zhang et al., 2009; Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). While we focus on extracting product features, opinion expressions and mining relations in this paper. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experi</context>
</contexts>
<marker>Somasundaran, Wiebe, Ruppenhofer, 2008</marker>
<rawString>Swapna Somasundaran, Janyce Wiebe, and Josef Ruppenhofer. 2008. Discourse level opinion interpretation. In Proceedings of COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05.</booktitle>
<contexts>
<context position="1819" citStr="Takamura et al., 2005" startWordPosition="280" endWordPosition="283">are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b). Some researchers refer this information extraction task as opinion extraction or opinion mining. Comparing with the former one, opinion mining usually produces richer information. In this paper, we define an opinion unit as a triple consisting of a product feature, an expression of opin</context>
<context position="27496" citStr="Takamura et al., 2005" startWordPosition="4550" endWordPosition="4553">domain training set is smaller than in-domain in MP3, but it also achieve competitive performance with the 1539 in-domain. On the other hand, SVM-2’s result decreased compared with the in-domain experiments because the test domain changed. At the same time, SVM-PTree outperforms other methods which is similar in in-domain experiments. 4 Related Work Opinion mining has recently received considerable attention. Amount of works have been done on sentimental classification in different levels (Zhang et al., 2009; Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). While we focus on extracting product features, opinion expressions and mining relations in this paper. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experimental results showed that the model using contextual clues improved the performan</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In Proceedings of ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale.</title>
<date>1959</date>
<journal>Editions Klincksieck.</journal>
<marker>Tesni`ere, 1959</marker>
<rawString>L. Tesni`ere. 1959. El´ements de syntaxe structurale. Editions Klincksieck.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="18444" citStr="Wiebe et al. (2005)" startWordPosition="3078" endWordPosition="3081"> of a pharse, and make this extension still satisfy the kernel function requirements (composition of kernels is still a kernel (Joachims et al., 2001)). The consideration is that the local words should have limited effects on whole tree structures. So the kernel is defined on external children (Kc) and internal nodes (Kin) separately, annotator extracted 3595 relations, while the other annotator A2 extracted 3745 relations, an A1 d 3217 cases of them matched. In order to measure the annotation quality, we use the following metric to measure the inter-annotator agreement, which is also used by Wiebe et al. (2005). as the result, the local words are not involved in In this section, we describe the annotated corpus and experiment configurations including baseline 3.1 Corpus We conducted experiments with labeled corpus which are selected from Hu and Liu (2004), Jindal and Liu (2008) have built. Their documents are collected from Amazon.com and CNet.com, where products have a large number of reviews. They also manually labeled product features and polarity orientations. Our corpus is selected from them, which contains customer reviews of 11 products belong to 5 categories(Diaper, Cell Phone, Digital Camer</context>
<context position="28823" citStr="Wiebe et al. (2005)" startWordPosition="4757" endWordPosition="4760">t easily converted to other domains. Choi et al. (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. They identified expressions of opinions, sources of opinions and the linking relation that exists between them. The sources of opinions denote to the person or entity that holds the opinion. Another area related to our work is opinion expressions identification (Wilson et al., 2005a; Breck et al., 2007). They worked on identifying the words and phrases that express opinions in text. According to Wiebe et al. (2005), there are two types of opinion expressions, direct subjective expressions and expressive subjective elements. 5 Conclusions In this paper, we described our work on mining opinions from unstructured documents. We focused on extracting relations between product features and opinion expressions. The novelties of our work included: 1) we defined the phrase dependency parsing and proposed an approach to construct the phrase dependency trees; 2) we proposed a new tree kernel function to model the phrase dependency trees. Experimental results show that our approach improved the performances of the </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2/3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Demonstration Description in Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2128" citStr="Wilson et al., 2005" startWordPosition="332" endWordPosition="335">lassification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b). Some researchers refer this information extraction task as opinion extraction or opinion mining. Comparing with the former one, opinion mining usually produces richer information. In this paper, we define an opinion unit as a triple consisting of a product feature, an expression of opinion, and an emotional attitude(positive or negative). We use this definition as the basis for our opinion mining task. Since a product review may refer more than one product feature and express different opinions on each of them, the relation extraction is an important subtask of opinion mining. Consider the</context>
<context position="13839" citStr="Wilson et al., 2005" startWordPosition="2285" endWordPosition="2288">old. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree. 2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree. Manually built patterns were used in previous works which have an obvio</context>
<context position="28687" citStr="Wilson et al., 2005" startWordPosition="4733" endWordPosition="4736">clues improved the performance. However since the contextual information in a domain is specific, the model got by their approach can not easily converted to other domains. Choi et al. (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. They identified expressions of opinions, sources of opinions and the linking relation that exists between them. The sources of opinions denote to the person or entity that holds the opinion. Another area related to our work is opinion expressions identification (Wilson et al., 2005a; Breck et al., 2007). They worked on identifying the words and phrases that express opinions in text. According to Wiebe et al. (2005), there are two types of opinion expressions, direct subjective expressions and expressive subjective elements. 5 Conclusions In this paper, we described our work on mining opinions from unstructured documents. We focused on extracting relations between product features and opinion expressions. The novelties of our work included: 1) we defined the phrase dependency parsing and proposed an approach to construct the phrase dependency trees; 2) we proposed a new </context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinionfinder: A system for subjectivity analysis. In Demonstration Description in Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP.</booktitle>
<contexts>
<context position="2128" citStr="Wilson et al., 2005" startWordPosition="332" endWordPosition="335">lassification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements include source of opinions who expresses an opinion (Choi et al., 2005); target of opinions which is a receptor of an opinion (Popescu and Etzioni, 2005); opinion expression which delivers an opinion (Wilson et al., 2005b). Some researchers refer this information extraction task as opinion extraction or opinion mining. Comparing with the former one, opinion mining usually produces richer information. In this paper, we define an opinion unit as a triple consisting of a product feature, an expression of opinion, and an emotional attitude(positive or negative). We use this definition as the basis for our opinion mining task. Since a product review may refer more than one product feature and express different opinions on each of them, the relation extraction is an important subtask of opinion mining. Consider the</context>
<context position="13839" citStr="Wilson et al., 2005" startWordPosition="2285" endWordPosition="2288">old. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree. 2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree. Manually built patterns were used in previous works which have an obvio</context>
<context position="28687" citStr="Wilson et al., 2005" startWordPosition="4733" endWordPosition="4736">clues improved the performance. However since the contextual information in a domain is specific, the model got by their approach can not easily converted to other domains. Choi et al. (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. They identified expressions of opinions, sources of opinions and the linking relation that exists between them. The sources of opinions denote to the person or entity that holds the opinion. Another area related to our work is opinion expressions identification (Wilson et al., 2005a; Breck et al., 2007). They worked on identifying the words and phrases that express opinions in text. According to Wiebe et al. (2005), there are two types of opinion expressions, direct subjective expressions and expressive subjective elements. 5 Conclusions In this paper, we described our work on mining opinions from unstructured documents. We focused on extracting relations between product features and opinion expressions. The novelties of our work included: 1) we defined the phrase dependency parsing and proposed an approach to construct the phrase dependency trees; 2) we proposed a new </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of HLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
</authors>
<title>Converting dependency structures to phrase structures.</title>
<date>2001</date>
<booktitle>In HLT ’01: Proceedings of the first international conference on Human language technology research.</booktitle>
<contexts>
<context position="6891" citStr="Xia and Palmer, 2001" startWordPosition="1087" endWordPosition="1090">roblem for dependency grammar. Fig. 2(a) shows the dependency representation of an example sentence. The root of the sentence is “enjoyed”. There are seven pairs of dependency relationships, depicted by seven arcs from heads to dependents. 2.1.2 Phrase Dependency Parsing Currently, the mainstream of dependency parsing is conducted on lexical elements: relations are built between single words. A major information loss of this word level dependency tree compared with constituent tree is that it doesn’t explicitly provide local structures and syntactic categories (i.e. NP, VP labels) of phrases (Xia and Palmer, 2001). On the other hand, dependency tree provides connections between distant words, which are useful in extracting long distance relations. Therefore, compromising between the two, we extend the dependency tree node with phrases. That implies a noun phrase “Cannon SD500 PowerShot” can be a dependent that modifies a verb phrase head “really enjoy using” with relation type “dobj”. The feasibility behind is that a phrase is a syntactic unit regardless of the length or syntactic category (Santorini and Kroch, 2007), and it is acceptable to substitute a single word by a phrase with same syntactic cate</context>
</contexts>
<marker>Xia, Palmer, 2001</marker>
<rawString>Fei Xia and Martha Palmer. 2001. Converting dependency structures to phrase structures. In HLT ’01: Proceedings of the first international conference on Human language technology research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Yuanbin Wu</author>
<author>Tao Li</author>
<author>Mitsunori Ogihara</author>
<author>Joseph Johnson</author>
<author>Xuanjing Huang</author>
</authors>
<title>Mining product reviews based on shallow dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<contexts>
<context position="27387" citStr="Zhang et al., 2009" startWordPosition="4530" endWordPosition="4533">iments on SVM-1, SVM-WTree, and SVM-PTree with percentage of 4.6%, 8.6%, 10.3% in average. And the cross-domain training set is smaller than in-domain in MP3, but it also achieve competitive performance with the 1539 in-domain. On the other hand, SVM-2’s result decreased compared with the in-domain experiments because the test domain changed. At the same time, SVM-PTree outperforms other methods which is similar in in-domain experiments. 4 Related Work Opinion mining has recently received considerable attention. Amount of works have been done on sentimental classification in different levels (Zhang et al., 2009; Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). While we focus on extracting product features, opinion expressions and mining relations in this paper. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and stat</context>
</contexts>
<marker>Zhang, Wu, Li, Ogihara, Johnson, Huang, 2009</marker>
<rawString>Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara, Joseph Johnson, and Xuanjing Huang. 2009. Mining product reviews based on shallow dependency parsing. In Proceedings of SIGIR 2009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>