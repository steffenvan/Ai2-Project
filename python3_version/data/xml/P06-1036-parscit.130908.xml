<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.77713">
Enhancing electronic dictionaries with an index based on associations
</title>
<author confidence="0.609998">
Olivier Ferret
</author>
<address confidence="0.536257">
CEA –LIST/LIC2M
18 Route du Panorama
F-92265 Fontenay-aux-Roses
</address>
<email confidence="0.997964">
ferreto@zoe.cea.fr
</email>
<author confidence="0.515689">
Michael Zock1
</author>
<affiliation confidence="0.232921">
LIF-CNRS
</affiliation>
<address confidence="0.6311515">
163 Avenue de Luminy
F-13288 Marseille Cedex 9
</address>
<email confidence="0.998521">
michael.zock@lif.univ-mrs.fr
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870294117647">
A good dictionary contains not only
many entries and a lot of information
concerning each one of them, but also
adequate means to reveal the stored in-
formation. Information access depends
crucially on the quality of the index. We
will present here some ideas of how a
dictionary could be enhanced to support a
speaker/writer to find the word s/he is
looking for. To this end we suggest to
add to an existing electronic resource an
index based on the notion of association.
We will also present preliminary work of
how a subset of such associations, for ex-
ample, topical associations, can be ac-
quired by filtering a network of lexical
co-occurrences extracted from a corpus.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999463125">
A dictionary user typically pursues one of two
goals (Humble, 2001): as a decoder (reading,
listening), he may look for the definition or the
translation of a specific target word, while as an
encoder (speaker, writer) he may want to find a
word that expresses well not only a given con-
cept, but is also appropriate in a given context.
Obviously, readers and writers come to the
dictionary with different mindsets, information
and expectations concerning input and output.
While the decoder can provide the word he wants
additional information for, the encoder (language
producer) provides the meaning of a word for
which he lacks the corresponding form. In sum,
users with different goals need access to different
indexes, one that is based on form (decoding),
</bodyText>
<footnote confidence="0.567613">
1 In alphabetical order
</footnote>
<bodyText confidence="0.978520073170732">
the other being based on meaning or meaning
relations (encoding).
Our concern here is more with the encoder, i.e.
lexical access in language production, a feature
largely neglected in lexicographical work. Yet, a
good dictionary contains not only many entries
and a lot of information concerning each one of
them, but also efficient means to reveal the
stored information. Because, what is a huge dic-
tionary good for, if one cannot access the infor-
mation it contains?
2 Lexical access on the basis of what:
concepts (i.e. meanings) or words?
Broadly speaking, there are two views concern-
ing lexicalization: the process is conceptually-
driven (meaning, or parts of it are the starting
point) or lexically-driven2: the target word is
accessed via a source word. This is typically the
case when we are looking for a synonym, anto-
nym, hypernym (paradigmatic associations), or
any of its syntagmatic associates (red-rose, cof-
fee-black), the kind of association we will be
concerned with here.
Yet, besides conceptual knowledge, people
seem also to know a lot of things concerning the
lexical form (Brown and Mc Neill, 1966): num-
ber of syllables, beginning/ending of the target
word, part of speech (noun, verb, adjective, etc.),
origin (Greek or Latin), gender (Vigliocco et al.,
2 Of course, the input can also be hybrid, that is, it can be
composed of a conceptual and a linguistic component. For
example, in order to express the notion of intensity, MAGN in
Mel’cuk’s theory (Mel’cuk et al., 1995), a speaker or writer
has to use different words (very, seriously, high) depending
on the form of the argument (ill, wounded, price), as he says
very ill, seriously wounded, high price. In each case he ex-
presses the very same notion, but by using a different word.
While he could use the adverb very for qualifying the state
of somebody’s health (he is ill), he cannot do so when quali-
fying the words injury or price. Likewise, he cannot use this
specific adverb to qualify the noun illness.
</bodyText>
<page confidence="0.963814">
281
</page>
<note confidence="0.531406">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 281–288,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999840914285714">
1997). While in principle, all this information
could be used to constrain the search space, we
will deal here only with one aspect, the words’
relations to other concepts or words (associative
knowledge).
Suppose, you were looking for a word expressing
the following ideas: domesticated animal, pro-
ducing milk suitable for making cheese. Suppose
further that you knew that the target word was
neither cow, buffalo nor sheep. While none of
this information is sufficient to guarantee the
access of the intended word goat, the information
at hand (part of the definition) could certainly be
used3. Besides this type of information, people
often have other kinds of knowledge concerning
the target word. In particular, they know how the
latter relates to other words. For example, they
know that goats and sheep are somehow con-
nected, sharing a great number of features, that
both are animals (hypernym), that sheep are ap-
preciated for their wool and meat, that they tend
to follow each other blindly, etc., while goats
manage to survive, while hardly eating anything,
etc. In sum, people have in their mind a huge
lexico-conceptual network, with words 4 , con-
cepts or ideas being highly interconnected.
Hence, any one of them can evoke the other. The
likelihood for this to happen depends on such
factors as frequency (associative strength), sali-
ency and distance (direct vs. indirect access). As
one can see, associations are a very general and
powerful mechanism. No matter what we hear,
read or say, anything is likely to remind us of
something else. This being so, we should make
use of it.
</bodyText>
<footnote confidence="0.784477333333333">
3 For some concrete proposals going in this direction, see
dictionaries offering reverse lookup: http://www.ultralingua.
net/ ,http://www.onelook.com/reverse-dictionary.shtml.
</footnote>
<bodyText confidence="0.971820194029851">
4 Of course, one can question the very fact that people store
words in their mind. Rather than considering the human
mind as a wordstore one might consider it as a wordfactory.
Indeed, by looking at some of the work done by psycholo-
gists who try to emulate the mental lexicon (Levelt et al.,
1999) one gets the impression that words are synthesized
rather than located and call up. In this case one might con-
clude that rather than having words in our mind we have a
set of highly distributed, more or less abstract information.
By propagating energy rather than data —(as there is no
message passing, transformation or cumulation of informa-
tion, there is only activation spreading, that is, changes of
energy levels, call it weights, electronic impulses, or what-
ever),— that we propagate signals, activating ultimately
certain peripherical organs (larynx, tongue, mouth, lips,
hands) in such a way as to produce movements or sounds,
that, not knowing better, we call words.
3 Accessing the target word by navigat-
ing in a huge associative network
If one agrees with what we have just said, one
could view the mental lexicon as a huge semantic
network composed of nodes (words and con-
cepts) and links (associations), with either being
able to activate the other5. Finding a word in-
volves entering the network and following the
links leading from the source node (the first
word that comes to your mind) to the target word
(the one you are looking for). Suppose you
wanted to find the word nurse (target word), yet
the only token coming to your mind is hospital.
In this case the system would generate internally
a graph with the source word at the center and all
the associated words at the periphery. Put differ-
ently, the system would build internally a seman-
tic network with hospital in the center and all its
associated words as satellites (see Figure 1, next
page).
Obviously, the greater the number of associa-
tions, the more complex the graph. Given the
diversity of situations in which a given object
may occur we are likely to build many associa-
tions. In other words, lexical graphs tend to be-
come complex, too complex to be a good repre-
sentation to support navigation. Readability is
hampered by at least two factors: high connec-
tivity (the great number of links or associations
emanating from each word), and distribution:
conceptually related nodes, that is, nodes acti-
vated by the same kind of association are scat-
tered around, that is, they do not necessarily oc-
cur next to each other, which is quite confusing
for the user. In order to solve this problem, we
suggest to display by category (chunks) all the
words linked by the same kind of association to
the source word (see Figure 2). Hence, rather
than displaying all the connected words as a flat
list, we suggest to present them in chunks to al-
low for categorial search. Having chosen a cate-
gory, the user will be presented a list of words or
categories from which he must choose. If the
target word is in the category chosen by the user
(suppose he looked for a hypernym, hence he
checked the ISA-bag), search stops, otherwise it
continues. The user could choose either another
category (e.g. AKO or TIORA), or a word in the
current list, which would then become the new
starting point.
</bodyText>
<footnote confidence="0.989970666666667">
5 While the links in our brain may only be weighted, they
need to be labelled to become interpretable for human be-
ings using them for navigational purposes in a lexicon.
</footnote>
<page confidence="0.995344">
282
</page>
<figure confidence="0.999914823529411">
Internal Representation
ASYLUM
ISA ISA
HEALTH
INSTITUTION
synonym
SANATORIUM
PSYCHIATRIC HOSPITAL
ISA AKO
CLINIC
ISA
AKO
HOSPITAL
PATIENT
TIORA
ISA
take care of
TIORA
ISA
treat
near-synonym
MILITARY HOSPITAL
TIORA
nurse
assistant
DOCTOR
INMATE
treat
AKO
AKO
AKO
PHYSICIAN
DENTIST
GYNECOLOGIST
</figure>
<figureCaption confidence="0.94377725">
Figure 1: Search based on navigating in a network (internal representation)
AKO: a kind of; ISA: subtype; TIORA: Typically Involved Object, Relation or Actor.
Figure 2: Proposed candidates, grouped by fam-
ily, i.e. according to the nature of the link
</figureCaption>
<bodyText confidence="0.986368172413793">
As one can see, the fact that the links are labeled
has some very important consequences:
(a) While maintaining the power of a highly
connected graph (possible cyclic navigation),
it has at the interface level the simplicity of a
tree: each node points only to data of the
same type, i.e. to the same kind of associa-
tion.
(b) With words being presented in clusters,
navigation can be accomplished by clicking
on the appropriate category.
The assumption being that the user generally
knows to which category the target word belongs
(or at least, he can recognize within which of the
listed categories it falls), and that categorical
search is in principle faster than search in a huge
list of unordered (or, alphabetically ordered)
words6.
Obviously, in order to allow for this kind of
access, the resource has to be built accordingly.
This requires at least two things: (a) indexing
words by the associations they evoke, (b) identi-
6 Even though very important, at this stage we shall not
worry too much for the names given to the links. Indeed,
one might question nearly all of them. What is important is
the underlying rational: help users to navigate on the basis
of symbolically qualified links. In reality a whole set of
words (synonyms, of course, but not only) could amount to
a link, i.e. be its conceptual equivalent.
</bodyText>
<figure confidence="0.998073583333333">
LOPTW
link
Abstract representation of the search graph
list of potential target words (LOPTW)
LOPTW
link
list of potential target words (LOPTW)
link
...
source word
link
link
A concrete example
AKO clinic, sanatorium, ...
hospital
military hospital, psychiatric hospital
SYNONYM
inmate
doctor, ...
patient
nurse
...
ISA
TIORA
</figure>
<page confidence="0.99613">
283
</page>
<bodyText confidence="0.999991545454545">
fying and labelling the most frequent/useful as-
sociations. This is precisely our goal. Actually,
we propose to build an associative network by
enriching an existing electronic dictionary (es-
sentially) with (syntagmatic) associations coming
from a corpus, representing the average citizen’s
shared, basic knowledge of the world (encyclo-
paedia). While some associations are too com-
plex to be extracted automatically by machine,
others are clearly within reach. We will illustrate
in the next section how this can be achieved.
</bodyText>
<sectionHeader confidence="0.94255" genericHeader="method">
4 Automatic extraction of topical rela-
</sectionHeader>
<subsectionHeader confidence="0.6873715">
tions
4.1 Definition of the problem
</subsectionHeader>
<bodyText confidence="0.999233258064516">
We have argued in the previous sections that dic-
tionaries must contain many kinds of relations on
the syntagmatic and paradigmatic axis to allow
for natural and flexible access of words. Synon-
ymy, hypernymy or meronymy fall clearly in this
latter category, and well known resources like
WordNet (Miller, 1995), EuroWordNet (Vossen,
1998) or MindNet (Richardson et al., 1998) con-
tain them. However, as various researchers have
pointed out (Harabagiu et al., 1999), these net-
works lack information, in particular with regard
to syntagmatic associations, which are generally
unsystematic. These latter, called TIORA (Zock
and Bilac, 2004) or topical relations (Ferret,
2002) account for the fact that two words refer to
the same topic, or take part in the same situation
or scenario. Word-pairs like doctor–hospital,
burglar–policeman or plane–airport, are exam-
ples in case. The lack of such topical relations in
resources like WordNet has been dubbed as the
tennis problem (Roger Chaffin, cited in Fell-
baum, 1998). Some of these links have been in-
troduced more recently in WordNet via the do-
main relation. Yet their number remains still very
small. For instance, WordNet 2.1 does not con-
tain any of the three associations mentioned here
above, despite their high frequency.
The lack of systematicity of these topical rela-
tions makes their extraction and typing very dif-
ficult on a large scale. This is why some re-
searchers have proposed to use automatic learn-
ing techniques to extend lexical networks like
WordNet. In (Harabagiu &amp; Moldovan, 1998),
this was done by extracting topical relations from
the glosses associated to the synsets. Other re-
searchers used external sources: Mandala et al.
(1999) integrated co-occurrences and a thesaurus
to WordNet for query expansion; Agirre et al.
(2001) built topic signatures from texts in rela-
tion to synsets; Magnini and Cavagliá (2000)
annotated the synsets with Subject Field Codes.
This last idea has been taken up and extended by
(Avancini et al., 2003) who expanded the do-
mains built from this annotation.
Despite the improvements, all these ap-
proaches are limited by the fact that they rely too
heavily on WordNet and some of its more so-
phisticated features (such as the definitions asso-
ciated with the synsets). While often being ex-
ploited by acquisition methods, these features are
generally lacking in similar lexico-semantic net-
works. Moreover, these methods attempt to learn
topical knowledge from a lexical network rather
than topical relations. Since our goal is different,
we have chosen not to rely on any significant
resource, all the more as we would like our
method to be applicable to a wide array of lan-
guages. In consequence, we took an incremental
approach (Ferret, 2006): starting from a network
of lexical co-occurrences7 collected from a large
corpus, we used these latter to select potential
topical relations by using a topical analyzer.
</bodyText>
<subsectionHeader confidence="0.955208">
4.2 From a network of co-occurrences to a
set of Topical Units
</subsectionHeader>
<bodyText confidence="0.996636833333333">
We start by extracting lexical co-occurrences
from a corpus to build a network. To this end we
follow the method introduced by (Church and
Hanks, 1990), i.e. by sliding a window of a given
size over some texts. The parameters of this ex-
traction were set in such a way as to catch the
most obvious topical relations: the window was
fairly large (20-words wide), and while it took
text boundaries into account, it ignored the order
of the co-occurrences. Like (Church and Hanks,
1990), we used mutual information to measure
the cohesion between two words. The finite size
of the corpus allows us to normalize this measure
in line with the maximal mutual information
relative to the corpus.
This network is used by TOPICOLL (Ferret,
2002), a topic analyzer, which performs simulta-
neously three tasks, relevant for this goal:
</bodyText>
<listItem confidence="0.993757">
• it segments texts into topically homogene-
ous segments;
• it selects in each segment the most repre-
sentative words of its topic;
</listItem>
<footnote confidence="0.930315666666667">
7 Such a network is only another view of a set of co-
occurrences: its nodes are the co-occurrent words and its
edges are the co-occurrence relations.
</footnote>
<page confidence="0.987205">
284
</page>
<listItem confidence="0.620153">
• it proposes a restricted set of words from
</listItem>
<bodyText confidence="0.9797969">
the co-occurrence network to expand the
selected words of the segment.
These three tasks rely on a common mecha-
nism: a window is moved over the text to be ana-
lyzed in order to limit the focus space of the
analysis. This latter contains a lemmatized ver-
sion of the text’s plain words. For each position
of this window, we select only words of the co-
occurrence network that are linked to at least
three other words of the window (see Figure 3).
This leads to select both words that are in the
window (first order co-occurrents) and words
coming from the network (second order co-
occurrents). The number of links between the
selected words of the network, called expansion
words, and those of the window is a good indica-
tor of the topical coherence of the window’s con-
tent. Hence, when their number is small, a seg-
ment boundary can be assumed. This is the basic
principle underlying our topic analyzer.
</bodyText>
<figure confidence="0.3953455">
1.0 1.0 for all words of the window in this example)
0.21 link in the co-occurrence network (with its cohesion value)
</figure>
<figureCaption confidence="0.9994445">
Figure 3: Selection and weighting of words
from the co-occurrence network
</figureCaption>
<bodyText confidence="0.999986384615385">
The words selected for each position of the
window are summed, to keep only those occur-
ring in 75% of the positions of the segment. This
allows reducing the number of words selected
from non-topical co-occurrences. Once a corpus
has been processed by TOPICOLL, we obtain a
set of segments and a set of expansion words for
each one of them. The association of the selected
words of a segment and its expansion words is
called a Topical Unit. Since both sets of words
are selected for reasons of topical homogeneity,
their co-occurrence is more likely to be a topical
relation than in our initial network.
</bodyText>
<subsectionHeader confidence="0.999211">
4.3 Filtering of Topical Units
</subsectionHeader>
<bodyText confidence="0.999789466666667">
Before recording the co-occurrences in the Topi-
cal Units built in this way, the units are filtered
twice. The first filter aims at discarding hetero-
geneous Topical Units, which can arise as a side
effect of a document whose topics are so inter-
mingled that it is impossible to get a reliable lin-
ear segmentation of the text. We consider that
this occurs when for a given text segment, no
word can be selected as a representative of the
topic of the segment. Moreover, we only keep
the Topical Units that contain at least two words
from their original segment. A topic is defined
here as a configuration of words. Note that the
identification of such a configuration cannot be
based solely on a single word.
</bodyText>
<figure confidence="0.6701746">
Text words Expansion words
surveillance police_judiciaire
(watch) (judiciary police)
téléphonique écrouer
(telephone) (to imprison)
juge garde_à_vue
(judge) (police custody)
policier écoute_téléphonique
(policeman) (phone tapping)
brigade juge_d’instruction
(squad) (examining judge)
enquête contrôle_judiciaire
(investigation) (judicial review)
placer
(to put)
</figure>
<tableCaption confidence="0.97937">
Table 1: Content of a filtered Topical Unit
</tableCaption>
<bodyText confidence="0.999967117647059">
The second filter is applied to the expansion
words of each Topical Unit to increase their topi-
cal homogeneity. The principle of the filtering of
these words is the same as the principle of their
selection described in Section 4.2: an expansion
word is kept if it is linked in the co-occurrence
network to at least three text words of the Topi-
cal Unit. Moreover, a selective threshold is ap-
plied to the frequency and the cohesion of the co-
occurrences supporting these links: only co-
occurrences whose frequency and cohesion are
respectively higher or equal to 15 and 0.15 are
used. For instance in Table 1, which shows an
example of a Topical Unit after its filtering,
écrouer (to imprison) is selected, because it is
linked in the co-occurrence network to the fol-
lowing words of the text:
</bodyText>
<listItem confidence="0.690506333333333">
juge (judge): 52 (frequency) – 0.17 (cohesion)
policier (policeman): 56 – 0.17
enquête (investigation): 42 – 0.16
</listItem>
<figure confidence="0.996720529411765">
0.48 = pw3x0.18+pw4x0.13
+pw5x0.17
0.21 0.10
0.14
0.17
0.18
0.13
1.0 1.0 1.0 1.0 1.0
w1
w2 w3
w4 w5
0.48
selected word from the co-occurrence network (with its weight)
word from text (with p its weight in the window, equal to
wi,
n1
n2
</figure>
<page confidence="0.993234">
285
</page>
<table confidence="0.998371222222222">
word freq. word freq. word freq. word freq.
scène (theater) théâtral 62 cynique 26 scénique 14
(stage) 884 (dramatic) 51 20 (theatrical) 13
théâtre 679 scénariste 51 16 Chabol 13
réalisateur 220 (scriptwriter) 40 15 (Chabol) 13
(director) 135 comique 38 14 Tchekov 13
cinéaste 104 (comic) 30 (Tchekov)
(film-marker) 63 oscar allocataire
comédie (oscar)
(comedy) film_américain
costumer (american film)
(to dress up) hollywoodien
(Hollywood)
(cynical)
miss
(miss)
parti pris
(bias)
monologue
(monolog)
revisiter
(beneficiary)
satirique
(to revisit) (satirical)
14
gros_plan
(close-up)
</table>
<tableCaption confidence="0.9928605">
Table 2: Co-occurrents of the word acteur (actor) with a cohesion of 0.16
(the co-occurrents removed by our filtering method are underlined)
</tableCaption>
<subsectionHeader confidence="0.9927035">
4.4 From Topical Units to a network of
topical relations
</subsectionHeader>
<bodyText confidence="0.999986093023256">
After the filtering, a Topical Unit gathers a set of
words supposed to be strongly coherent from the
topical point of view. Next, we record the co-
occurrences between these words for all the
Topical Units remaining after filtering. Hence,
we get a large set of topical co-occurrences, de-
spite the fact that a significant number of non-
topical co-occurrences remains, the filtering of
Topical Units being an unsupervised process.
The frequency of a co-occurrence in this case is
given by the number of Topical Units containing
both words simultaneously. No distinction con-
cerning the origin of the words of the Topical
Units is made.
The network of topical co-occurrences built
from Topical Units is a subset of the initial net-
work. However, it also contains co-occurrences
that are not part of it, i.e. co-occurrences that
were not extracted from the corpus used for set-
ting the initial network or co-occurrences whose
frequency in this corpus was too low. Only some
of these “new” co-occurrences are topical. Since
it is difficult to estimate globally which ones are
interesting, we have decided to focus our atten-
tion only on the co-occurrences of the topical
network already present in the initial network.
Thus, we only use the network of topical co-
occurrences as a filter for the initial co-
occurrence network. Before doing so, we filter
the topical network in order to discard co-
occurrences whose frequency is too low, that is,
co-occurrences that are unstable and not repre-
sentative. From the use of the final network by
TOPICOLL (see Section 4.5), we set the thresh-
old experimentally to 5. Finally, the initial net-
work is filtered by keeping only co-occurrences
present in the topical network. Their frequency
and cohesion are taken from the initial network.
While the frequencies given by the topical net-
work are potentially interesting for their topical
significance, we do not use them because the
results of the filtering of Topical Units are too
hard to evaluate.
</bodyText>
<subsectionHeader confidence="0.899205">
4.5 Results and evaluation
</subsectionHeader>
<bodyText confidence="0.999975818181818">
We applied the method described here to an ini-
tial co-occurrence network extracted from a cor-
pus of 24 months of Le Monde, a major French
newspaper. The size of the corpus was around 39
million words. The initial network contained
18,958 words and 341,549 relations. The first run
produced 382,208 Topical Units. After filtering,
we kept 59% of them. The network built from
these Topical Units was made of 11,674 words
and 2,864,473 co-occurrences. 70% of these co-
occurrences were new with regard to the initial
network and were discarded. Finally, we got a
filtered network of 7,160 words and 183,074 re-
lations, which represents a cut of 46% of the ini-
tial network. A qualitative study showed that
most of the discarded relations are non-topical.
This is illustrated by Table 2, which gives the co-
occurrents of the word acteur (actor) that are
filtered by our method among its co-occurrents
with a high cohesion (equal to 0.16). For in-
stance, the words cynique (cynical) or allocataire
(beneficiary) are cohesive co-occurrents of the
</bodyText>
<page confidence="0.994605">
286
</page>
<bodyText confidence="0.996255833333333">
word actor, even though they are not topically
linked to it. These words are filtered out, while
we keep words like gros_plan (close-up) or scé-
nique (theatrical), which topically cohere with
acteur (actor) despite their lower frequency than
the discarded words.
</bodyText>
<table confidence="0.999831666666667">
Recall8 Precision F1- Error
measure (Pk)9
initial (I) 0.85 0.79 0.82 0.20
topical 0.85 0.79 0.82 0.21
filtering
(T)
frequency 0.83 0.71 0.77 0.25
filtering
(F)
</table>
<tableCaption confidence="0.9368985">
Table 3: TOPICOLL’s results
with different networks
</tableCaption>
<bodyText confidence="0.995222351351352">
In order to evaluate more objectively our
work, we compared the quantitative results of
TOPICOLL with the initial network and its fil-
tered version. The evaluation showed that the
performance of the segmenter remains stable,
even if we use a topically filtered network (see
Table 3). Moreover, it became obvious that a
network filtered only by frequency and cohesion
performs significantly less well, even with a
comparable size. For testing the statistical sig-
nificance of these results, we applied to the Pk
values a one-side t-test with a null hypothesis of
equal means. Levels lower or equal to 0.05 are
considered as statistically significant:
pval (I-T): 0.08
pval (I-F): 0.02
pval (T-F): 0.05
These values confirm that the difference be-
tween the initial network (I) and the topically
filtered one (T) is actually not significant,
whereas the filtering based on co-occurrence fre-
quencies leads to significantly lower results, both
compared to the initial network and the topically
filtered one. Hence, one may conclude that our
8 Precision is given by Nt / Nb and recall by Nt / D, with D
being the number of document breaks, Nb the number of
boundaries found by TOPICOLL and Nt the number of
boundaries that are document breaks (the boundary should
not be farther than 9 plain words from the document break).
9 Pk (Beeferman et al., 1999) evaluates the probability that a
randomly chosen pair of words, separated by k words, is
wrongly classified, i.e. they are found in the same segment
by TOPICOLL, while they are actually in different ones (miss
of a document break), or they are found in different seg-
ments, while they are actually in the same one (false alarm).
method is an effective way of selecting topical
relations by preference.
</bodyText>
<sectionHeader confidence="0.980013" genericHeader="discussions">
5 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.999974456521739">
We have raised and partially answered the ques-
tion of how a dictionary should be indexed in
order to support word access, a question initially
addressed in (Zock, 2002) and (Zock and Bilac,
2004). We were particularly concerned with the
language producer, as his needs (and knowledge
at the onset) are quite different from the ones of
the language receiver (listener/reader). It seems
that, in order to achieve our goal, we need to do
two things: add to an existing electronic diction-
ary information that people tend to associate with
a word, that is, build and enrich a semantic net-
work, and provide a tool to navigate in it. To this
end we have suggested to label the links, as this
would reduce the graph complexity and allow for
type-based navigation. Actually our basic pro-
posal is to extend a resource like WordNet by
adding certain links, in particular on the syntag-
matic axis. These links are associations, and their
role consists in helping the encoder to find ideas
(concepts/words) related to a given stimulus
(brainstorming), or to find the word he is think-
ing of (word access).
One problem that we are confronted with is to
identify possible associations. Ideally we would
need a complete list, but unfortunately, this does
not exist. Yet, there is a lot of highly relevant
information out there. For example, Mel’cuk’s
lexical functions (Mel’cuk, 1995), Fillmore’s
FRAMENET10, work on ontologies (CYC), thesau-
rus (Roget), WordNets (the original version from
Princeton, various Euro-WordNets, BalkaNet),
HowNet11, the work done by MICRA, the FACTO-
TUM project 12 , or the Wordsmyth diction-
ary/thesaurus13.
Since words are linked via associations, it is
important to reveal these links. Once this is done,
words can be accessed by following these links.
We have presented here some preliminary work
for extracting an important subset of such links
from texts, topical associations, which are gener-
ally absent from dictionaries or resources like
WordNet. An evaluation of the topic segmenta-
tion has shown that the relations extracted are
sound from the topical point of view, and that
they can be extracted automatically. However,
</bodyText>
<footnote confidence="0.997984">
10 http://www.icsi.berkeley.edu/~framenet/
11 http://www.keenage.com/html/e index.html
12 http://humanities.uchicago.edu/homes/MICRA/
13 http://www.wordsmyth.com/
</footnote>
<page confidence="0.994672">
287
</page>
<bodyText confidence="0.9999283125">
they still contain too much noise to be directly
exploitable by an end user for accessing a word
in a dictionary. One way of reducing the noise of
the extracted relations would be to build from
each text a representation of its topics and to re-
cord the co-occurrences in these representations
rather than in the segments delimited by a topic
segmenter. This is a hypothesis we are currently
exploring. While we have focused here only on
word access on the basis of (other) words, one
should not forget that most of the time speakers
or writers start from meanings. Hence, we shall
consider this point more carefully in our future
work, by taking a serious look at the proposals
made by Bilac et al. (2004); Durgar and Oflazer
(2004), or Dutoit and Nugues (2002).
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990491215909092">
Eneko Agirre, Olatz Ansa, David Martinez and Edu-
ard Hovy. 2001. Enriching WordNet concepts with
topic signatures. In NAACL’01 Workshop on
WordNet and Other Lexical Resources: Applica-
tions, Extensions and Customizations.
Henri Avancini, Alberto Lavelli, Bernardo Magnini,
Fabrizio Sebastiani and Roberto Zanoli. 2003. Ex-
panding Domain-Specific Lexicons by Term Cate-
gorization. In 18th ACM Symposium on Applied
Computing (SAC-03).
Doug Beeferman, Adam Berger and Lafferty. 1999.
Statistical Models for Text Segmentation. Machine
Learning, 34(1): 177-210.
Slaven Bilac, Wataru Watanabe, Taiichi Hashimoto,
Takenobu Tokunaga and Hozumi Tanaka. 2004.
Dictionary search based on the target word descrip-
tion. In Tenth Annual Meeting of The Association
for Natural Language Processing (NLP2004),
pages 556-559.
Roger Brown and David McNeill. 1996. The tip of the
tongue phenomenon. Journal of Verbal Learning
and Verbal Behaviour, 5: 325-337.
Kenneth Church and Patrick Hanks. 1990. Word As-
sociation Norms, Mutual Information, And Lexi-
cography. Computational Linguistics, 16(1): 177-
210.
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2004.
Use of Wordnet for Retrieving Words from Their
Meanings, In 2nd Global WordNet Conference,
Brno
Dominique Dutoit and Pierre Nugues. 2002. A lexical
network and an algorithm to find words from defi-
nitions. In 15th European Conference on Artificial
Intelligence (ECAI 2002), Lyon, pages 450-454,
IOS Press.
Christiane Fellbaum. 1998. WordNet - An Electronic
Lexical Database, MIT Press.
Olivier Ferret. 2006. Building a network of topical
relations from a corpus. In LREC 2006.
Olivier Ferret. 2002. Using collocations for topic
segmentation and link detection. In COLING 2002,
pages 260-266.
Sanda M. Harabagiu, George A. Miller and Dan I.
Moldovan. 1999. WordNet 2 - A Morphologically
and Semantically Enhanced Resource. In ACL-
SIGLEX99: Standardizing Lexical Resources,
pages 1-8.
Sanda M. Harabagiu and Dan I. Moldovan. 1998.
Knowledge Processing on an Extended WordNet.
In WordNet - An Electronic Lexical Database,
pages 379-405.
Philip Humble. 2001. Dictionaries and Language
Learners, Haag and Herchen.
William Levelt, Ardi Roelofs and Antje Meyer. 1999.
A theory of lexical access in speech production,
Behavioral and Brain Sciences, 22: 1-75.
Bernardo Magnini and Gabriela Cavagliá. 2000. Inte-
grating Subject Field Codes into WordNet. In
LREC 2000.
Rila Mandala, Takenobu Tokunaga and Hozumi
Tanaka. 1999. Complementing WordNet with Ro-
get’s and Corpus-based Thesauri for Information
Retrieval. In EACL 99.
Igor Mel’cuk, Arno Clas and Alain Polguère. 1995.
Introduction à la lexicologie explicative et combi-
natoire, Louvain, Duculot.
George A. Miller. 1995. WordNet: A lexical Data-
base, Communications of the ACM. 38(11): 39-41.
Stephen D. Richardson, William B. Dolan and Lucy
Vanderwende. 1998. MindNet: Acquiring and
Structuring Semantic Information from Text. In
ACL-COLING’98, pages 1098-1102.
Piek Vossen. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publisher.
Gabriella Vigliocco, Antonini, T., and Merryl Garrett.
1997. Grammatical gender is on the tip of Italian
tongues. Psychological Science, 8: 314-317.
Michael Zock. 2002. Sorry, what was your name
again, or how to overcome the tip-of-the tongue
problem with the help of a computer? In SemaNet
workshop, COLING 2002, Taipei.
http://acl.ldc.upenn.edu /W/W02/W02-1118.pdf
Michael Zock and Slaven Bilac. 2004. Word lookup
on the basis of associations: from an idea to a
roadmap. In COLING 2004 workshop: Enhancing
and using dictionaries, Geneva.
http://acl.ldc.upenn.edu/ coling2004/W10/pdf/5.pdf
</reference>
<page confidence="0.995072">
288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.227800">
<title confidence="0.998861">Enhancing electronic dictionaries with an index based on associations</title>
<author confidence="0.992145">Olivier Ferret</author>
<address confidence="0.668097">CEA –LIST/LIC2M 18 Route du Panorama F-92265 Fontenay-aux-Roses</address>
<email confidence="0.975098">ferreto@zoe.cea.fr</email>
<affiliation confidence="0.915166">LIF-CNRS</affiliation>
<address confidence="0.835377">163 Avenue de Luminy F-13288 Marseille Cedex 9</address>
<email confidence="0.978309">michael.zock@lif.univ-mrs.fr</email>
<abstract confidence="0.999156">A good dictionary contains not only many entries and a lot of information concerning each one of them, but also adequate means to reveal the stored information. Information access depends crucially on the quality of the index. We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for. To this end we suggest to add to an existing electronic resource an index based on the notion of association. We will also present preliminary work of how a subset of such associations, for example, topical associations, can be acquired by filtering a network of lexical co-occurrences extracted from a corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>David Martinez</author>
<author>Eduard Hovy</author>
</authors>
<title>Enriching WordNet concepts with topic signatures.</title>
<date>2001</date>
<booktitle>In NAACL’01 Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations.</booktitle>
<contexts>
<context position="13726" citStr="Agirre et al. (2001)" startWordPosition="2255" endWordPosition="2258">does not contain any of the three associations mentioned here above, despite their high frequency. The lack of systematicity of these topical relations makes their extraction and typing very difficult on a large scale. This is why some researchers have proposed to use automatic learning techniques to extend lexical networks like WordNet. In (Harabagiu &amp; Moldovan, 1998), this was done by extracting topical relations from the glosses associated to the synsets. Other researchers used external sources: Mandala et al. (1999) integrated co-occurrences and a thesaurus to WordNet for query expansion; Agirre et al. (2001) built topic signatures from texts in relation to synsets; Magnini and Cavagliá (2000) annotated the synsets with Subject Field Codes. This last idea has been taken up and extended by (Avancini et al., 2003) who expanded the domains built from this annotation. Despite the improvements, all these approaches are limited by the fact that they rely too heavily on WordNet and some of its more sophisticated features (such as the definitions associated with the synsets). While often being exploited by acquisition methods, these features are generally lacking in similar lexico-semantic networks. Moreo</context>
</contexts>
<marker>Agirre, Ansa, Martinez, Hovy, 2001</marker>
<rawString>Eneko Agirre, Olatz Ansa, David Martinez and Eduard Hovy. 2001. Enriching WordNet concepts with topic signatures. In NAACL’01 Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henri Avancini</author>
<author>Alberto Lavelli</author>
<author>Bernardo Magnini</author>
<author>Fabrizio Sebastiani</author>
<author>Roberto Zanoli</author>
</authors>
<title>Expanding Domain-Specific Lexicons by Term Categorization.</title>
<date>2003</date>
<booktitle>In 18th ACM Symposium on Applied Computing (SAC-03).</booktitle>
<contexts>
<context position="13933" citStr="Avancini et al., 2003" startWordPosition="2290" endWordPosition="2293"> large scale. This is why some researchers have proposed to use automatic learning techniques to extend lexical networks like WordNet. In (Harabagiu &amp; Moldovan, 1998), this was done by extracting topical relations from the glosses associated to the synsets. Other researchers used external sources: Mandala et al. (1999) integrated co-occurrences and a thesaurus to WordNet for query expansion; Agirre et al. (2001) built topic signatures from texts in relation to synsets; Magnini and Cavagliá (2000) annotated the synsets with Subject Field Codes. This last idea has been taken up and extended by (Avancini et al., 2003) who expanded the domains built from this annotation. Despite the improvements, all these approaches are limited by the fact that they rely too heavily on WordNet and some of its more sophisticated features (such as the definitions associated with the synsets). While often being exploited by acquisition methods, these features are generally lacking in similar lexico-semantic networks. Moreover, these methods attempt to learn topical knowledge from a lexical network rather than topical relations. Since our goal is different, we have chosen not to rely on any significant resource, all the more a</context>
</contexts>
<marker>Avancini, Lavelli, Magnini, Sebastiani, Zanoli, 2003</marker>
<rawString>Henri Avancini, Alberto Lavelli, Bernardo Magnini, Fabrizio Sebastiani and Roberto Zanoli. 2003. Expanding Domain-Specific Lexicons by Term Categorization. In 18th ACM Symposium on Applied Computing (SAC-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>Lafferty</author>
</authors>
<title>Statistical Models for Text Segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<pages>177--210</pages>
<contexts>
<context position="25677" citStr="Beeferman et al., 1999" startWordPosition="4249" endWordPosition="4252">hat the difference between the initial network (I) and the topically filtered one (T) is actually not significant, whereas the filtering based on co-occurrence frequencies leads to significantly lower results, both compared to the initial network and the topically filtered one. Hence, one may conclude that our 8 Precision is given by Nt / Nb and recall by Nt / D, with D being the number of document breaks, Nb the number of boundaries found by TOPICOLL and Nt the number of boundaries that are document breaks (the boundary should not be farther than 9 plain words from the document break). 9 Pk (Beeferman et al., 1999) evaluates the probability that a randomly chosen pair of words, separated by k words, is wrongly classified, i.e. they are found in the same segment by TOPICOLL, while they are actually in different ones (miss of a document break), or they are found in different segments, while they are actually in the same one (false alarm). method is an effective way of selecting topical relations by preference. 5 Discussion and conclusion We have raised and partially answered the question of how a dictionary should be indexed in order to support word access, a question initially addressed in (Zock, 2002) a</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger and Lafferty. 1999. Statistical Models for Text Segmentation. Machine Learning, 34(1): 177-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Wataru Watanabe</author>
</authors>
<title>Taiichi Hashimoto, Takenobu Tokunaga and Hozumi Tanaka.</title>
<date>2004</date>
<booktitle>In Tenth Annual Meeting of The Association for Natural Language Processing (NLP2004),</booktitle>
<pages>556--559</pages>
<marker>Bilac, Watanabe, 2004</marker>
<rawString>Slaven Bilac, Wataru Watanabe, Taiichi Hashimoto, Takenobu Tokunaga and Hozumi Tanaka. 2004. Dictionary search based on the target word description. In Tenth Annual Meeting of The Association for Natural Language Processing (NLP2004), pages 556-559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Brown</author>
<author>David McNeill</author>
</authors>
<title>The tip of the tongue phenomenon.</title>
<date>1996</date>
<journal>Journal of Verbal Learning and Verbal Behaviour,</journal>
<volume>5</volume>
<pages>325--337</pages>
<marker>Brown, McNeill, 1996</marker>
<rawString>Roger Brown and David McNeill. 1996. The tip of the tongue phenomenon. Journal of Verbal Learning and Verbal Behaviour, 5: 325-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<date>1990</date>
<journal>Word Association Norms, Mutual Information, And Lexicography. Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>177--210</pages>
<contexts>
<context position="15055" citStr="Church and Hanks, 1990" startWordPosition="2476" endWordPosition="2479">. Since our goal is different, we have chosen not to rely on any significant resource, all the more as we would like our method to be applicable to a wide array of languages. In consequence, we took an incremental approach (Ferret, 2006): starting from a network of lexical co-occurrences7 collected from a large corpus, we used these latter to select potential topical relations by using a topical analyzer. 4.2 From a network of co-occurrences to a set of Topical Units We start by extracting lexical co-occurrences from a corpus to build a network. To this end we follow the method introduced by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts. The parameters of this extraction were set in such a way as to catch the most obvious topical relations: the window was fairly large (20-words wide), and while it took text boundaries into account, it ignored the order of the co-occurrences. Like (Church and Hanks, 1990), we used mutual information to measure the cohesion between two words. The finite size of the corpus allows us to normalize this measure in line with the maximal mutual information relative to the corpus. This network is used by TOPICOLL (Ferret, 2002), a topic analyz</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Patrick Hanks. 1990. Word Association Norms, Mutual Information, And Lexicography. Computational Linguistics, 16(1): 177-210.</rawString>
</citation>
<citation valid="true">
<title>Ilknur Durgar El-Kahlout and Kemal Oflazer.</title>
<date>2004</date>
<booktitle>In 2nd Global WordNet Conference,</booktitle>
<location>Brno</location>
<marker>2004</marker>
<rawString>Ilknur Durgar El-Kahlout and Kemal Oflazer. 2004. Use of Wordnet for Retrieving Words from Their Meanings, In 2nd Global WordNet Conference, Brno</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominique Dutoit</author>
<author>Pierre Nugues</author>
</authors>
<title>A lexical network and an algorithm to find words from definitions.</title>
<date>2002</date>
<booktitle>In 15th European Conference on Artificial Intelligence (ECAI 2002), Lyon,</booktitle>
<pages>450--454</pages>
<publisher>IOS Press.</publisher>
<marker>Dutoit, Nugues, 2002</marker>
<rawString>Dominique Dutoit and Pierre Nugues. 2002. A lexical network and an algorithm to find words from definitions. In 15th European Conference on Artificial Intelligence (ECAI 2002), Lyon, pages 450-454, IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet - An Electronic Lexical Database,</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12944" citStr="Fellbaum, 1998" startWordPosition="2128" endWordPosition="2130">, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998). Some of these links have been introduced more recently in WordNet via the domain relation. Yet their number remains still very small. For instance, WordNet 2.1 does not contain any of the three associations mentioned here above, despite their high frequency. The lack of systematicity of these topical relations makes their extraction and typing very difficult on a large scale. This is why some researchers have proposed to use automatic learning techniques to extend lexical networks like WordNet. In (Harabagiu &amp; Moldovan, 1998), this was done by extracting topical relations from the glosses as</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet - An Electronic Lexical Database, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Building a network of topical relations from a corpus. In LREC</title>
<date>2006</date>
<contexts>
<context position="14669" citStr="Ferret, 2006" startWordPosition="2414" endWordPosition="2415">t that they rely too heavily on WordNet and some of its more sophisticated features (such as the definitions associated with the synsets). While often being exploited by acquisition methods, these features are generally lacking in similar lexico-semantic networks. Moreover, these methods attempt to learn topical knowledge from a lexical network rather than topical relations. Since our goal is different, we have chosen not to rely on any significant resource, all the more as we would like our method to be applicable to a wide array of languages. In consequence, we took an incremental approach (Ferret, 2006): starting from a network of lexical co-occurrences7 collected from a large corpus, we used these latter to select potential topical relations by using a topical analyzer. 4.2 From a network of co-occurrences to a set of Topical Units We start by extracting lexical co-occurrences from a corpus to build a network. To this end we follow the method introduced by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts. The parameters of this extraction were set in such a way as to catch the most obvious topical relations: the window was fairly large (20-words wide), and </context>
</contexts>
<marker>Ferret, 2006</marker>
<rawString>Olivier Ferret. 2006. Building a network of topical relations from a corpus. In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Using collocations for topic segmentation and link detection.</title>
<date>2002</date>
<booktitle>In COLING</booktitle>
<pages>260--266</pages>
<contexts>
<context position="12604" citStr="Ferret, 2002" startWordPosition="2073" endWordPosition="2074">must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998). Some of these links have been introduced more recently in WordNet via the domain relation. Yet their number remains still very small. For instance, WordNet 2.1 does not contain any of the three associations mentioned here above, despite their high frequency.</context>
<context position="15639" citStr="Ferret, 2002" startWordPosition="2579" endWordPosition="2580">d by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts. The parameters of this extraction were set in such a way as to catch the most obvious topical relations: the window was fairly large (20-words wide), and while it took text boundaries into account, it ignored the order of the co-occurrences. Like (Church and Hanks, 1990), we used mutual information to measure the cohesion between two words. The finite size of the corpus allows us to normalize this measure in line with the maximal mutual information relative to the corpus. This network is used by TOPICOLL (Ferret, 2002), a topic analyzer, which performs simultaneously three tasks, relevant for this goal: • it segments texts into topically homogeneous segments; • it selects in each segment the most representative words of its topic; 7 Such a network is only another view of a set of cooccurrences: its nodes are the co-occurrent words and its edges are the co-occurrence relations. 284 • it proposes a restricted set of words from the co-occurrence network to expand the selected words of the segment. These three tasks rely on a common mechanism: a window is moved over the text to be analyzed in order to limit the</context>
</contexts>
<marker>Ferret, 2002</marker>
<rawString>Olivier Ferret. 2002. Using collocations for topic segmentation and link detection. In COLING 2002, pages 260-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>George A Miller</author>
<author>Dan I Moldovan</author>
</authors>
<title>WordNet 2 - A Morphologically and Semantically Enhanced Resource. In ACLSIGLEX99: Standardizing Lexical Resources,</title>
<date>1999</date>
<pages>1--8</pages>
<contexts>
<context position="12395" citStr="Harabagiu et al., 1999" startWordPosition="2042" endWordPosition="2045">clearly within reach. We will illustrate in the next section how this can be achieved. 4 Automatic extraction of topical relations 4.1 Definition of the problem We have argued in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998). Some of these links have been introduced more rec</context>
</contexts>
<marker>Harabagiu, Miller, Moldovan, 1999</marker>
<rawString>Sanda M. Harabagiu, George A. Miller and Dan I. Moldovan. 1999. WordNet 2 - A Morphologically and Semantically Enhanced Resource. In ACLSIGLEX99: Standardizing Lexical Resources, pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Dan I Moldovan</author>
</authors>
<title>Knowledge Processing on an Extended WordNet. In WordNet - An Electronic Lexical Database,</title>
<date>1998</date>
<pages>379--405</pages>
<contexts>
<context position="13477" citStr="Harabagiu &amp; Moldovan, 1998" startWordPosition="2217" endWordPosition="2220">es like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998). Some of these links have been introduced more recently in WordNet via the domain relation. Yet their number remains still very small. For instance, WordNet 2.1 does not contain any of the three associations mentioned here above, despite their high frequency. The lack of systematicity of these topical relations makes their extraction and typing very difficult on a large scale. This is why some researchers have proposed to use automatic learning techniques to extend lexical networks like WordNet. In (Harabagiu &amp; Moldovan, 1998), this was done by extracting topical relations from the glosses associated to the synsets. Other researchers used external sources: Mandala et al. (1999) integrated co-occurrences and a thesaurus to WordNet for query expansion; Agirre et al. (2001) built topic signatures from texts in relation to synsets; Magnini and Cavagliá (2000) annotated the synsets with Subject Field Codes. This last idea has been taken up and extended by (Avancini et al., 2003) who expanded the domains built from this annotation. Despite the improvements, all these approaches are limited by the fact that they rely too </context>
</contexts>
<marker>Harabagiu, Moldovan, 1998</marker>
<rawString>Sanda M. Harabagiu and Dan I. Moldovan. 1998. Knowledge Processing on an Extended WordNet. In WordNet - An Electronic Lexical Database, pages 379-405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Humble</author>
</authors>
<title>Dictionaries and Language Learners, Haag and Herchen.</title>
<date>2001</date>
<contexts>
<context position="1032" citStr="Humble, 2001" startWordPosition="160" endWordPosition="161">ored information. Information access depends crucially on the quality of the index. We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for. To this end we suggest to add to an existing electronic resource an index based on the notion of association. We will also present preliminary work of how a subset of such associations, for example, topical associations, can be acquired by filtering a network of lexical co-occurrences extracted from a corpus. 1 Introduction A dictionary user typically pursues one of two goals (Humble, 2001): as a decoder (reading, listening), he may look for the definition or the translation of a specific target word, while as an encoder (speaker, writer) he may want to find a word that expresses well not only a given concept, but is also appropriate in a given context. Obviously, readers and writers come to the dictionary with different mindsets, information and expectations concerning input and output. While the decoder can provide the word he wants additional information for, the encoder (language producer) provides the meaning of a word for which he lacks the corresponding form. In sum, user</context>
</contexts>
<marker>Humble, 2001</marker>
<rawString>Philip Humble. 2001. Dictionaries and Language Learners, Haag and Herchen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Levelt</author>
<author>Ardi Roelofs</author>
<author>Antje Meyer</author>
</authors>
<title>A theory of lexical access in speech production,</title>
<date>1999</date>
<journal>Behavioral and Brain Sciences,</journal>
<volume>22</volume>
<pages>1--75</pages>
<contexts>
<context position="6005" citStr="Levelt et al., 1999" startWordPosition="975" endWordPosition="978">werful mechanism. No matter what we hear, read or say, anything is likely to remind us of something else. This being so, we should make use of it. 3 For some concrete proposals going in this direction, see dictionaries offering reverse lookup: http://www.ultralingua. net/ ,http://www.onelook.com/reverse-dictionary.shtml. 4 Of course, one can question the very fact that people store words in their mind. Rather than considering the human mind as a wordstore one might consider it as a wordfactory. Indeed, by looking at some of the work done by psychologists who try to emulate the mental lexicon (Levelt et al., 1999) one gets the impression that words are synthesized rather than located and call up. In this case one might conclude that rather than having words in our mind we have a set of highly distributed, more or less abstract information. By propagating energy rather than data —(as there is no message passing, transformation or cumulation of information, there is only activation spreading, that is, changes of energy levels, call it weights, electronic impulses, or whatever),— that we propagate signals, activating ultimately certain peripherical organs (larynx, tongue, mouth, lips, hands) in such a way</context>
</contexts>
<marker>Levelt, Roelofs, Meyer, 1999</marker>
<rawString>William Levelt, Ardi Roelofs and Antje Meyer. 1999. A theory of lexical access in speech production, Behavioral and Brain Sciences, 22: 1-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavagliá</author>
</authors>
<title>Integrating Subject Field Codes into WordNet.</title>
<date>2000</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="13812" citStr="Magnini and Cavagliá (2000)" startWordPosition="2269" endWordPosition="2272">eir high frequency. The lack of systematicity of these topical relations makes their extraction and typing very difficult on a large scale. This is why some researchers have proposed to use automatic learning techniques to extend lexical networks like WordNet. In (Harabagiu &amp; Moldovan, 1998), this was done by extracting topical relations from the glosses associated to the synsets. Other researchers used external sources: Mandala et al. (1999) integrated co-occurrences and a thesaurus to WordNet for query expansion; Agirre et al. (2001) built topic signatures from texts in relation to synsets; Magnini and Cavagliá (2000) annotated the synsets with Subject Field Codes. This last idea has been taken up and extended by (Avancini et al., 2003) who expanded the domains built from this annotation. Despite the improvements, all these approaches are limited by the fact that they rely too heavily on WordNet and some of its more sophisticated features (such as the definitions associated with the synsets). While often being exploited by acquisition methods, these features are generally lacking in similar lexico-semantic networks. Moreover, these methods attempt to learn topical knowledge from a lexical network rather th</context>
</contexts>
<marker>Magnini, Cavagliá, 2000</marker>
<rawString>Bernardo Magnini and Gabriela Cavagliá. 2000. Integrating Subject Field Codes into WordNet. In LREC 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rila Mandala</author>
</authors>
<title>Takenobu Tokunaga and Hozumi Tanaka.</title>
<date>1999</date>
<booktitle>In EACL 99.</booktitle>
<marker>Mandala, 1999</marker>
<rawString>Rila Mandala, Takenobu Tokunaga and Hozumi Tanaka. 1999. Complementing WordNet with Roget’s and Corpus-based Thesauri for Information Retrieval. In EACL 99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’cuk</author>
</authors>
<title>Arno Clas and Alain Polguère.</title>
<date>1995</date>
<location>Louvain, Duculot.</location>
<marker>Mel’cuk, 1995</marker>
<rawString>Igor Mel’cuk, Arno Clas and Alain Polguère. 1995. Introduction à la lexicologie explicative et combinatoire, Louvain, Duculot.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical Database,</title>
<date>1995</date>
<journal>Communications of the ACM.</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="12242" citStr="Miller, 1995" startWordPosition="2021" endWordPosition="2022">ed, basic knowledge of the world (encyclopaedia). While some associations are too complex to be extracted automatically by machine, others are clearly within reach. We will illustrate in the next section how this can be achieved. 4 Automatic extraction of topical relations 4.1 Definition of the problem We have argued in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical Database, Communications of the ACM. 38(11): 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen D Richardson</author>
<author>William B Dolan</author>
<author>Lucy Vanderwende</author>
</authors>
<title>MindNet: Acquiring and Structuring Semantic Information from Text. In</title>
<date>1998</date>
<booktitle>ACL-COLING’98,</booktitle>
<pages>1098--1102</pages>
<contexts>
<context position="12307" citStr="Richardson et al., 1998" startWordPosition="2028" endWordPosition="2031">e some associations are too complex to be extracted automatically by machine, others are clearly within reach. We will illustrate in the next section how this can be achieved. 4 Automatic extraction of topical relations 4.1 Definition of the problem We have argued in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Ro</context>
</contexts>
<marker>Richardson, Dolan, Vanderwende, 1998</marker>
<rawString>Stephen D. Richardson, William B. Dolan and Lucy Vanderwende. 1998. MindNet: Acquiring and Structuring Semantic Information from Text. In ACL-COLING’98, pages 1098-1102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piek Vossen</author>
</authors>
<title>EuroWordNet: A Multilingual Database with Lexical Semantic Networks.</title>
<date>1998</date>
<publisher>Kluwer Academic Publisher.</publisher>
<contexts>
<context position="12270" citStr="Vossen, 1998" startWordPosition="2024" endWordPosition="2025">orld (encyclopaedia). While some associations are too complex to be extracted automatically by machine, others are clearly within reach. We will illustrate in the next section how this can be achieved. 4 Automatic extraction of topical relations 4.1 Definition of the problem We have argued in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has </context>
</contexts>
<marker>Vossen, 1998</marker>
<rawString>Piek Vossen. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer Academic Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriella Vigliocco</author>
<author>T Antonini</author>
<author>Merryl Garrett</author>
</authors>
<title>Grammatical gender is on the tip of Italian tongues.</title>
<date>1997</date>
<journal>Psychological Science,</journal>
<volume>8</volume>
<pages>314--317</pages>
<marker>Vigliocco, Antonini, Garrett, 1997</marker>
<rawString>Gabriella Vigliocco, Antonini, T., and Merryl Garrett. 1997. Grammatical gender is on the tip of Italian tongues. Psychological Science, 8: 314-317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Zock</author>
</authors>
<title>Sorry, what was your name again, or how to overcome the tip-of-the tongue problem with the help of a computer?</title>
<date>2002</date>
<booktitle>In SemaNet workshop, COLING 2002,</booktitle>
<pages>02--02</pages>
<location>Taipei. http://acl.ldc.upenn.edu</location>
<contexts>
<context position="26275" citStr="Zock, 2002" startWordPosition="4352" endWordPosition="4353">et al., 1999) evaluates the probability that a randomly chosen pair of words, separated by k words, is wrongly classified, i.e. they are found in the same segment by TOPICOLL, while they are actually in different ones (miss of a document break), or they are found in different segments, while they are actually in the same one (false alarm). method is an effective way of selecting topical relations by preference. 5 Discussion and conclusion We have raised and partially answered the question of how a dictionary should be indexed in order to support word access, a question initially addressed in (Zock, 2002) and (Zock and Bilac, 2004). We were particularly concerned with the language producer, as his needs (and knowledge at the onset) are quite different from the ones of the language receiver (listener/reader). It seems that, in order to achieve our goal, we need to do two things: add to an existing electronic dictionary information that people tend to associate with a word, that is, build and enrich a semantic network, and provide a tool to navigate in it. To this end we have suggested to label the links, as this would reduce the graph complexity and allow for type-based navigation. Actually our</context>
</contexts>
<marker>Zock, 2002</marker>
<rawString>Michael Zock. 2002. Sorry, what was your name again, or how to overcome the tip-of-the tongue problem with the help of a computer? In SemaNet workshop, COLING 2002, Taipei. http://acl.ldc.upenn.edu /W/W02/W02-1118.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Zock</author>
<author>Slaven Bilac</author>
</authors>
<title>Word lookup on the basis of associations: from an idea to a roadmap.</title>
<date>2004</date>
<booktitle>In COLING</booktitle>
<location>Geneva.</location>
<note>http://acl.ldc.upenn.edu/ coling2004/W10/pdf/5.pdf</note>
<contexts>
<context position="12568" citStr="Zock and Bilac, 2004" startWordPosition="2066" endWordPosition="2069"> in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998). Some of these links have been introduced more recently in WordNet via the domain relation. Yet their number remains still very small. For instance, WordNet 2.1 does not contain any of the three associations mentioned here </context>
<context position="26302" citStr="Zock and Bilac, 2004" startWordPosition="4355" endWordPosition="4358">luates the probability that a randomly chosen pair of words, separated by k words, is wrongly classified, i.e. they are found in the same segment by TOPICOLL, while they are actually in different ones (miss of a document break), or they are found in different segments, while they are actually in the same one (false alarm). method is an effective way of selecting topical relations by preference. 5 Discussion and conclusion We have raised and partially answered the question of how a dictionary should be indexed in order to support word access, a question initially addressed in (Zock, 2002) and (Zock and Bilac, 2004). We were particularly concerned with the language producer, as his needs (and knowledge at the onset) are quite different from the ones of the language receiver (listener/reader). It seems that, in order to achieve our goal, we need to do two things: add to an existing electronic dictionary information that people tend to associate with a word, that is, build and enrich a semantic network, and provide a tool to navigate in it. To this end we have suggested to label the links, as this would reduce the graph complexity and allow for type-based navigation. Actually our basic proposal is to exten</context>
</contexts>
<marker>Zock, Bilac, 2004</marker>
<rawString>Michael Zock and Slaven Bilac. 2004. Word lookup on the basis of associations: from an idea to a roadmap. In COLING 2004 workshop: Enhancing and using dictionaries, Geneva. http://acl.ldc.upenn.edu/ coling2004/W10/pdf/5.pdf</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>