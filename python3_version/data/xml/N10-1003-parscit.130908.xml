<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004506">
<title confidence="0.989244">
Products of Random Latent Variable Grammars
</title>
<author confidence="0.94864">
Slav Petrov
</author>
<affiliation confidence="0.890845">
Google Research
</affiliation>
<address confidence="0.975596">
New York, NY, 10011
</address>
<email confidence="0.999033">
slav@google.com
</email>
<sectionHeader confidence="0.993893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999358666666667">
We show that the automatically induced latent
variable grammars of Petrov et al. (2006) vary
widely in their underlying representations, de-
pending on their EM initialization point. We
use this to our advantage, combining multiple
automatically learned grammars into an un-
weighted product model, which gives signif-
icantly improved performance over state-of-
the-art individual grammars. In our model,
the probability of a constituent is estimated as
a product of posteriors obtained from multi-
ple grammars that differ only in the random
seed used for initialization, without any learn-
ing or tuning of combination weights. Despite
its simplicity, a product of eight automatically
learned grammars improves parsing accuracy
from 90.2% to 91.8% on English, and from
80.3% to 84.5% on German.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998808875">
Learning a context-free grammar for parsing re-
quires the estimation of a more highly articulated
model than the one embodied by the observed tree-
bank. This is because the naive treebank grammar
(Charniak, 1996) is too permissive, making unreal-
istic context-freedom assumptions. For example, it
postulates that there is only one type of noun phrase
(NP), which can appear in all positions (subject, ob-
ject, etc.), regardless of case, number or gender. As
a result, the grammar can generate millions of (in-
correct) parse trees for a given sentence, and has a
flat posterior distribution. High accuracy grammars
therefore add soft constraints on the way categories
can be combined, and enrich the label set with addi-
tional information. These constraints can be lexical-
ized (Collins, 1999; Charniak, 2000), unlexicalized
</bodyText>
<page confidence="0.988114">
19
</page>
<bodyText confidence="0.999658">
(Johnson, 1998; Klein and Manning, 2003b) or au-
tomatically learned (Matsuzaki et al., 2005; Petrov
et al., 2006). The constraints serve the purpose of
weakening the independence assumptions, and re-
duce the number of possible (but incorrect) parses.
Here, we focus on the latent variable approach of
Petrov et al. (2006), where an Expectation Maxi-
mization (EM) algorithm is used to induce a hier-
archy of increasingly more refined grammars. Each
round of refinement introduces new constraints on
how constituents can be combined, which in turn
leads to a higher parsing accuracy. However, EM is a
local method, and there are no guarantees that it will
find the same grammars when initialized from dif-
ferent starting points. In fact, it turns out that even
though the final performance of these grammars is
consistently high, there are significant variations in
the learned refinements.
We use these variations to our advantage, and
treat grammars learned from different random seeds
as independent and equipotent experts. We use a
product distribution for joint prediction, which gives
more peaked posteriors than a sum, and enforces all
constraints of the individual grammars, without the
need to tune mixing weights. It should be noted here
that our focus is on improving parsing performance
using a single underlying grammar class, which is
somewhat orthogonal to the issue of parser combina-
tion, that has been studied elsewhere in the literature
(Sagae and Lavie, 2006; Fossum and Knight, 2009;
Zhang et al., 2009). In contrast to that line of work,
we also do not restrict ourselves to working with k-
best output, but work directly with a packed forest
representation of the posteriors, much in the spirit
of Huang (2008), except that we work with several
forests rather than rescoring a single one.
</bodyText>
<note confidence="0.636749">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19–27,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.996284833333333">
In our experimental section we give empirical an-
swers to some of the remaining theoretical ques-
tions. We address the question of averaging versus
multiplying classifier predictions, we investigate dif-
ferent ways of introducing more diversity into the
underlying grammars, and also compare combining
partial (constituent-level) and complete (tree-level)
predictions. Quite serendipitously, the simplest ap-
proaches work best in our experiments. A product
of eight latent variable grammars, learned on the
same data, and only differing in the seed used in
the random number generator that initialized EM,
improves parsing accuracy from 90.2% to 91.8%
on English, and from 80.3% to 84.5% on German.
These parsing results are even better than those ob-
tained by discriminative systems which have access
to additional non-local features (Charniak and John-
son, 2005; Huang, 2008).
</bodyText>
<sectionHeader confidence="0.967823" genericHeader="method">
2 Latent Variable Grammars
</sectionHeader>
<bodyText confidence="0.9999645">
Before giving the details of our model, we briefly
review the basic properties of latent variable gram-
mars. Learning latent variable grammars consists of
two tasks: (1) determining the data representation
(the set of context-free productions to be used in the
grammar), and (2) estimating the parameters of the
model (the production probabilities). We focus on
the randomness introduced by the EM algorithm and
refer the reader to Matsuzaki et al. (2005) and Petrov
et al. (2006) for a more general introduction.
</bodyText>
<subsectionHeader confidence="0.977401">
2.1 Split &amp; Merge Learning
</subsectionHeader>
<bodyText confidence="0.999958363636363">
Latent variable grammars split the coarse (but ob-
served) grammar categories of a treebank into more
fine-grained (but hidden) subcategories, which are
better suited for modeling the syntax of natural
languages (e.g. NP becomes NP1 through NPO.
Accordingly, each grammar production A—*BC
over observed categories A,B,C is split into a set
of productions A,,—*ByCz over hidden categories
A,,,By,Cz. Computing the joint likelihood of the ob-
served parse trees T and sentences w requires sum-
ming over all derivations t over split subcategories:
</bodyText>
<equation confidence="0.9947765">
�P(wi,Ti) � E P(wi, t) (1)
i i t:Ti
</equation>
<bodyText confidence="0.999232315789474">
Matsuzaki et al. (2005) derive an EM algorithm
for maximizing the joint likelihood, and Petrov et
al. (2006) extend this algorithm to use a split&amp;merge
procedure to adaptively determine the optimal num-
ber of subcategories for each observed category.
Starting from a completely markovized X-Bar gram-
mar, each category is split in two, generating eight
new productions for each original binary production.
To break symmetries, the production probabilities
are perturbed by 1% of random noise. EM is then
initialized with this starting point and used to climb
the highly non-convex objective function given in
Eq. 1. Each splitting step is followed by a merging
step, which uses a likelihood ratio test to reverse the
least useful half of the splits. Learning proceeds by
iterating between those two steps for six rounds. To
prevent overfitting, the production probabilities are
linearly smoothed by shrinking them towards their
common base category.
</bodyText>
<subsectionHeader confidence="0.994336">
2.2 EM induced Randomness
</subsectionHeader>
<bodyText confidence="0.9999866">
While the split&amp;merge procedure described above
is shown in Petrov et al. (2006) to reduce the vari-
ance in final performance, we found after closer
examination that there are substantial differences
in the patterns learned by the grammars. Since
the initialization is not systematically biased in any
way, one can obtain different grammars by simply
changing the seed of the random number genera-
tor. We trained 16 different grammars by initial-
izing the random number generator with seed val-
ues 1 through 16, but without biasing the initial-
ization in any other way. Figure 1 shows that the
number of subcategories allocated to each observed
category varies significantly between the different
initialization points, especially for the phrasal cate-
gories. Figure 2 shows posteriors over the most fre-
quent subcategories given their base category for the
first four grammars. Clearly, EM is allocating the la-
tent variables in very different ways in each case.
As a more quantitative measure of difference,1 we
evaluated all 16 grammars on sections 22 and 24 of
the Penn Treebank. Figure 3 shows the performance
on those two sets, and reveals that there is no single
grammar that achieves the best score on both. While
the parsing accuracies are consistently high,2 there
</bodyText>
<footnote confidence="0.99912475">
1While cherry-picking similarities is fairly straight-forward,
it is less obvious how to quantify differences.
2Note that despite their variance, the performance is always
higher than the one of the lexicalized parser of Charniak (2000).
</footnote>
<page confidence="0.984714">
20
</page>
<figure confidence="0.344999">
Automatically determined number of subcategories
</figure>
<figureCaption confidence="0.9971815">
Figure 1: There is large variance in the number of subcat-
egories (error bars correspond to one standard deviation).
</figureCaption>
<bodyText confidence="0.99990315">
is only a weak correlation between the accuracies
on the two evaluation sets (Pearson coefficient 0.34).
This suggests that no single grammar should be pre-
ferred over the others. In previous work (Petrov et
al., 2006; Petrov and Klein, 2007) the final grammar
was chosen based on its performance on a held-out
set (section 22), and corresponds to the second best
grammar in Figure 3 (because only 8 different gram-
mars were trained).
A more detailed error analysis is given in Fig-
ure 4, where we show a breakdown of F1 scores for
selected phrasal categories in addition to the overall
F1 score and exact match (on the WSJ development
set). While grammar G2 has the highest overall F1
score, its exact match is not particularly high, and
it turns out to be the weakest at predicting quanti-
fier phrases (QP). Similarly, the performance of the
other grammars varies between the different error
measures, indicating again that no single grammar
dominates the others.
</bodyText>
<sectionHeader confidence="0.980644" genericHeader="method">
3 A Simple Product Model
</sectionHeader>
<bodyText confidence="0.999904076923077">
It should be clear by now that simply varying the
random seed used for initialization causes EM to
discover very different latent variable grammars.
While this behavior is worrisome in general, it turns
out that we can use it to our advantage in this partic-
ular case. Recall that we are using EM to learn both,
the data representation, as well as the parameters of
the model. Our analysis showed that changing the
initialization point results in learning grammars that
vary quite significantly in the errors they make, but
have comparable overall accuracies. This suggests
that the different local maxima found by EM corre-
spond to different data representations rather than to
</bodyText>
<figureCaption confidence="0.94108575">
Figure 2: Posterior probabilities of the eight most fre-
quent hidden subcategories given their observed base cat-
egories. The four grammars (indicated by shading) are
populating the subcategories in very different ways.
</figureCaption>
<bodyText confidence="0.997190272727273">
suboptimal parameter estimates.
To leverage the strengths of the individual gram-
mars, we combine them in a product model. Product
models have the nice property that their Kullback-
Liebler divergence from the true distribution will
always be smaller than the average of the KL di-
vergences of the individual distributions (Hinton,
2001). Therefore, as long as no individual gram-
mar Gi is significantly worse than the others, we can
only benefit from combining multiple latent variable
grammars and searching for the tree that maximizes
</bodyText>
<equation confidence="0.98324">
P(T |w) � � P(T|w,Gi) (2)
i
</equation>
<bodyText confidence="0.999982333333333">
Here, we are making the assumption that the individ-
ual grammars are conditionally independent, which
is of course not true in theory, but holds surprisingly
well in practice. To avoid this assumption, we could
use a sum model, but we will show in Section 4.1
that the product formulation performs significantly
better. Intuitively speaking, products have the ad-
vantage that the final prediction has a high poste-
rior under all models, giving each model veto power.
This is exactly the behavior that we need in the case
of parsing, where each grammar has learned differ-
ent constraints for ruling out improbable parses.
</bodyText>
<subsectionHeader confidence="0.998877">
3.1 Learning
</subsectionHeader>
<bodyText confidence="0.999825">
Joint training of our product model would couple the
parameters of the individual grammars, necessitat-
ing the computation of an intractable global parti-
tion function (Brown and Hinton, 2001). Instead,
we use EM to train each grammar independently,
</bodyText>
<figure confidence="0.988893879310345">
1 2 3 4 5 6 7 8
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
PP
NP
10%
7%
4%
25%
15%
0%
1 2 3 4 5 6 7 8
30%
15%
0%
IN
60%
30%
0%
DT
60
50
40
30
20
10
NP
VP
PP
ADVP
ADJP
S
SBAR
QP
NNP
JJ
NNS
NN
RB
VBN
VBG
VB
IN
CD
VBD
VBZ
DT
VBP
21
90.2
90.1
90
89.9
89.8
89.7
89.6
89.5
90.6 90.7 90.8 90.9 91 91.1 91.2 91.3 91.4
F1 Score on Section 22
</figure>
<figureCaption confidence="0.996030666666667">
Figure 3: Parsing accuracies for grammars learned from
different random seeds. The large variance and weak cor-
relation suggest that no single grammar is to be preferred.
</figureCaption>
<bodyText confidence="0.999976111111111">
but from a different, randomly chosen starting point.
To emphasize, we do not introduce any systematic
bias (but see Section 4.3 for some experiments), or
attempt to train the models to be maximally dif-
ferent (Hinton, 2002) – we simply train a random
collection of grammars by varying the random seed
used for initialization. We found in our experiments
that the randomness provided by EM is sufficient
to achieve diversity among the individual grammars,
and gives results that are as good as more involved
training procedures. Xu and Jelinek (2004) made
a similar observation when learning random forests
for language modeling.
Our model is reminiscent of Logarithmic Opinion
Pools (Bordley, 1982) and Products of Experts (Hin-
ton, 2001).3 However, because we believe that none
of the underlying grammars should be favored, we
deliberately do not use any combination weights.
</bodyText>
<subsectionHeader confidence="0.973801">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.9999892">
Computing the most likely parse tree is intractable
for latent variable grammars (Sima’an, 2002), and
therefore also for our product model. This is because
there are exponentially many derivations over split
subcategories that correspond to a single parse tree
over unsplit categories, and there is no dynamic pro-
gram to efficiently marginalize out the latent vari-
ables. Previous work on parse risk minimization has
addressed this problem in two different ways: by
changing the objective function, or by constraining
</bodyText>
<footnote confidence="0.80793">
3As a matter of fact, Hinton (2001) mentions syntactic pars-
ing as one of the motivating examples for Products of Experts.
</footnote>
<figureCaption confidence="0.9965292">
Figure 4: Breakdown of different accuracy measures for
four randomly selected grammars (G,-G4), as well as a
product model (P) that uses those four grammars. Note
that no single grammar does well on all measures, while
the product model does significantly better on all.
</figureCaption>
<bodyText confidence="0.9699564">
the search space (Goodman, 1996; Titov and Hen-
derson, 2006; Petrov and Klein, 2007).
The simplest approach is to stick to likelihood as
the objective function, but to limit the search space
to a set of high quality candidates T :
</bodyText>
<equation confidence="0.968583">
T* = argmax P(T |w) (3)
TET
</equation>
<bodyText confidence="0.999945105263158">
Because the likelihood of a given parse tree can be
computed exactly for our product model (Eq. 2), the
quality of this approximation is only limited by the
quality of the candidate list. To generate the candi-
date list, we produce k-best lists of Viterbi deriva-
tions with the efficient algorithm of Huang and Chi-
ang (2005), and erase the subcategory information
to obtain parse trees over unsplit categories. We re-
fer to this approximation as TREE-LEVEL inference,
because it considers a list of complete trees from
the underlying grammars, and selects the tree that
has the highest likelihood under the product model.
While the k-best lists are of very high quality, this is
a fairly crude and unsatisfactory way of approximat-
ing the posterior distribution of the product model,
as it does not allow the synthesis of new trees based
on tree fragments from different grammars.
An alternative is to use a tractable objective func-
tion that allows the efficient exploration of the entire
</bodyText>
<figure confidence="0.995687939393939">
F1 Score Exact Match
G1
G2
G3
G4
P
90% 91.5% 93% 40% 45% 50%
NP VP
G1
G2
G3
G4
P
91% 93% 95% 90% 92% 94%
PP QP
G1
G2
G3
G4
P
85% 88% 91% 90% 92.5% 95%
G1
G2
G3
G4
P
G1
G2
G3
G4
P
G1
G2
G3
G4
P
F1 Score on Section 24
22
SINV
“ S ” VP NP .
NP VP
,
said
-68.8 -65.9
?
?
-66.7 -67.4
, NP
Such
agency
‘self-help’
borrowing
is
ADJP
z } |{
Rep.
Fortney
Stark
(D.Calif.)
Rep.
Fortney
Stark
(D. Calif.)
NP PRN NP
G1 G2
Legend:
9
NP PRN
log G1-score log G2-score
 |{z }
the
bill’s
chief
sponsor
expensive
than
direct
Treasury
borrowing
far
more
PP
unauthorized
and
expensive
ADJP , ADJP
? ?
z } |{
-11.7 -12.4
ADJP ADJP
9
ADVP
-12.9 -11.5
 |{z }
G2
far
more
expensive
G1
</figure>
<figureCaption confidence="0.915008333333333">
Figure 5: Grammar G1 has a preference for flat structures, while grammar G2 prefers deeper hierarchical structures.
Both grammars therefore make one mistake each on their own. However, the correct parse tree (which uses a flat
ADJP in the first slot and a hierarchical NP in the second) scores highest under the product model.
</figureCaption>
<bodyText confidence="0.995218333333333">
search space. Petrov and Klein (2007) present such
an objective function, which maximizes the product
of expected correct productions r:
</bodyText>
<equation confidence="0.99954">
�T� = argmax
T rET
</equation>
<bodyText confidence="0.999954733333334">
These expectations can be easily computed from the
inside/outside scores, similarly as in the maximum
bracket recall algorithm of Goodman (1996), or in
the variational approximation of Matsuzaki et al.
(2005). We extend the algorithm to work over poste-
rior distributions from multiple grammars, by aggre-
gating their expectations into a product. In practice,
we use a packed forest representation to approxi-
mate the posterior distribution, as in Huang (2008).
We refer to this approximation as CONSTITUENT-
LEVEL, because it allows us to form new parse trees
from individual constituents.
Figure 5 illustrates a real case where the prod-
uct model was able to construct a completely correct
parse tree from two partially correct ones. In the ex-
ample, one of the underlying grammars (G1) had an
imperfect recall score, because of its preference for
flat structures (it missed an NP node in the second
part of the sentence). In contrast, the other gram-
mar (G2) favors deeper structures, and therefore in-
troduced a superfluous ADVP node. The product
model gives each underlying grammar veto power,
and picks the least controversial tree (which is the
correct one in this case). Note that a sum model al-
lows the most confident model to dominate the de-
cision, and would chose the incorrect hierarchical
ADJP construction here (as one can verify using the
provided model scores).
To make inference efficient, we can use the
same coarse-to-fine pruning techniques as Petrov
and Klein (2007). We generate a hierarchy of pro-
jected grammars for each individual grammar and
parse with each one in sequence. Because only the
very last pass requires scores from the different un-
derlying grammars, this computation can be trivially
parallelized across multiple CPUs. Additionally, the
first (X-Bar) pruning pass needs to be computed
only once because it is shared among all grammars.
Since the X-Bar pass is the bottleneck of the multi-
pass scheme (using nearly 50% of the total process-
ing time), the overhead of using a product model is
quite manageable. It would have also been possi-
ble to use A*-search for factored models (Klein and
Manning, 2003a; Sun and Tsujii, 2009), but we did
not attempt this in the present work.
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.996849333333333">
In our experiments, we follow the standard setups
described in Table 1, and use the EVALB tool for
computing parsing figures. Unless noted other-
wise, we use CONSTITUENT-LEVEL inference. All
our experiments are based on the publicly available
BerkeleyParser.4
</bodyText>
<footnote confidence="0.534683">
4http://code.google.com/p/berkeleyparser
</footnote>
<equation confidence="0.988456">
)E(rlw) (4)
</equation>
<page confidence="0.994774">
23
</page>
<table confidence="0.999271285714286">
Training Set Dev. Set Test Set
ENGLISH-WSJ Sections Section 22 Section 23
(Marcus et al., 1993) 2-21
ENGLISH-BROWN see 10% of 10% of the
(Francis et al. 1979) ENGLISH-WSJ the data5 the data5
GERMAN Sentences Sentences Sentences
(Skut et al., 1997) 1-18,602 18,603-19,602 19,603-20,602
</table>
<tableCaption confidence="0.999095">
Table 1: Corpora and standard experimental setups.
</tableCaption>
<figure confidence="0.97010225">
Parsing accuracy on the WSJ development set
CONSTITUENT-LEVEL Inference
TREE-LEVEL Inference
92.5
92
91.5
91
90.5
</figure>
<subsectionHeader confidence="0.827143">
4.1 (Weighted) Product vs. (Weighted) Sum
</subsectionHeader>
<bodyText confidence="0.999982742857143">
A great deal has been written on the topic of prod-
ucts versus sums of probability distributions for joint
prediction (Genest and Zidek, 1986; Tax et al.,
2000). However, those theoretical results do not
apply directly here, because we are using multi-
ple randomly permuted models from the same class,
rather models from different classes. To shed some
light on this issue, we addressed the question em-
pirically, and combined two grammars into an un-
weighted product model, and also an unweighted
sum model. The individual grammars had parsing
accuracies (F1) of 91.2 and 90.7 respectively, and
their product (91.7) clearly outperformed their sum
(91.3). When more grammars are added, the gap
widens even further, and the trends persist indepen-
dently of whether the models use TREE-LEVEL or
CONSTITUENT-LEVEL inference. At least for the
case of unweighted combinations, the product dis-
tribution seems to be superior.
In related work, Zhang et al. (2009) achieve ex-
cellent results with a weighted sum model. Using
weights learned on a held-out set and rescoring 50-
best lists from Charniak (2000) and Petrov et al.
(2006), they obtain an F1 score of 91.0 (which they
further improve to 91.4 using a voting scheme). We
replicated their experiment, but used an unweighted
product of the two model scores. Using TREE-
LEVEL inference, we obtained an F1 score of 91.6,
suggesting that weighting is not so important in the
product case, as long as the classifiers are of compa-
rable quality.6 This is in line with previous work on
product models, where weighting has been impor-
tant when combining heterogenous classifiers (Hes-
kes, 1998), and less important when the classifiers
are of similar accuracy (Smith et al., 2005).
</bodyText>
<footnote confidence="0.993864333333333">
5See Gildea (2001) for the exact setup.
6The unweighted sum model, however, underperforms the
individual models with an F, score of only 90.3.
</footnote>
<figure confidence="0.9136455">
1 2 4 8 16
Number of grammars in product model
</figure>
<figureCaption confidence="0.967367">
Figure 6: Adding more grammars to the product model
improves parsing accuracy, while CONSTITUENT-LEVEL
inference gives consistently better results.
</figureCaption>
<subsectionHeader confidence="0.985614">
4.2 Tree-Level vs. Constituent-Level Inference
</subsectionHeader>
<bodyText confidence="0.99917028125">
Figure 6 shows that accuracy increases when more
grammars are added to the product model, but levels
off after eight grammars. The plot also compares
our two inference approximations, and shows that
CONSTITUENT-LEVEL inference results in a small
(0.2), but consistent improvement in F1 score.
A first thought might be that the improvement is
due to the limited scope of the k-best lists. How-
ever, this is not the case, as the results hold even
when the candidate set for CONSTITUENT-LEVEL
inference is constrained to trees from the k-best lists.
While the packed forrest representation can very ef-
ficiently encode an exponential set of parse trees, in
our case the k-best lists appear to be already very di-
verse because they are generated by multiple gram-
mars. Starting at 96.1 for a single latent variable
grammar, merging two 50-best lists from different
grammars gives an oracle score of 97.4, and adding
more k-best lists further improves the oracle score to
98.6 for 16 grammars. This compares favorably to
the results of Huang (2008), where the oracle score
over a pruned forest is shown to be 97.8 (compared
to 96.7 for a 50-best list).
The accuracy improvement can instead be ex-
plained by the change in the objective function. Re-
call from section Section 3.2, that CONSTITUENT-
LEVEL inference maximizes the expected number
of correct productions, while TREE-LEVEL infer-
ence maximizes tree-likelihood. It is therefore not
too surprising that the two objective functions se-
lect the same tree only 41% of the time, even when
limited to the same candidate set. Maximizing the
</bodyText>
<page confidence="0.997014">
24
</page>
<bodyText confidence="0.99966825">
expected number of correct productions is superior
for F1 score (see the one grammar case in Figure 6).
However, as to be expected, likelihood is better for
exact match, giving a score of 47.6% vs. 46.8%.
</bodyText>
<subsectionHeader confidence="0.999092">
4.3 Systematic Bias
</subsectionHeader>
<bodyText confidence="0.999993194444444">
Diversity among the underlying models is what
gives combined models their strength. One way of
increasing diversity is by modifying the feature sets
of the individual models (Baldridge and Osborne,
2008; Smith and Osborne, 2007). This approach
has the disadvantage that it reduces the performance
of the individual models, and is not directly appli-
cable for latent variable grammars because the fea-
tures are automatically learned. Alternatively, one
can introduce diversity by changing the training dis-
tribution. Bagging (Breiman, 1996) and Boosting
(Freund and Shapire, 1996) fall into this category,
but have had limited success for parsing (Hender-
son and Brill, 2000). Furthermore boosting is im-
practical here, because it requires training dozens of
grammars in sequence.
Since training a single grammar takes roughly one
day, we opted for a different, parallelizable way of
changing the training distribution. In a first exper-
iment, we divided the training set into two disjoint
sets, and trained separate grammars on each half.
These truly disjoint grammars had low F1 scores
of 89.4 and 89.6 respectively (because they were
trained on less data). Their combination unfortu-
nately also achieves only an accuracy of 90.9, which
is lower than what we get when training a single
grammar on the entire training set. In another exper-
iment, we used a cross-validation setup where indi-
vidual sections of the treebank were held out. The
resulting grammars had parsing accuracies of about
90.5, and the product model was again not able to
overcome the lower starting point, despite the poten-
tially larger diversity among the underlying gram-
mars. It appears that any systematic bias that lowers
the accuracy of the individual grammars also hurts
the final performance of the product model.
</bodyText>
<subsectionHeader confidence="0.995409">
4.4 Product Distribution as Smoothing
</subsectionHeader>
<bodyText confidence="0.999988125">
Smith et al. (2005) interpret Logarithmic Opinion
Pools (LOPs) as a smoothing technique. They
compare regularizing Conditional Random Fields
(CRFs) with Gaussian priors (Lafferty et al., 2001),
to training a set of unregularized CRFs over differ-
ent feature sets and combining them in an LOP. In
their experiments, both approaches work compara-
bly well, but their combination, an LOP of regular-
ized CRFs works best.
Not too surprisingly, we find this to be the case
here as well. The parameters of each latent vari-
able grammar are typically smoothed in a linear
fashion to prevent excessive overfitting (Petrov et
al., 2006). While all the experiments so far used
smoothed grammars, we reran the experiments also
with a set of unsmoothed grammars. The individ-
ual unsmoothed grammars have on average an 1.2%
lower accuracy. Even though our product model
is able to increase accuracy by combining multiple
grammars, the gap to the smoothed models remains
consistent. This suggests that the product model is
doing more than just smoothing. In fact, because the
product distribution is more peaked, it seems to be
doing the opposite of smoothing.
</bodyText>
<subsectionHeader confidence="0.976827">
4.5 Final Results
</subsectionHeader>
<bodyText confidence="0.999907884615385">
Our final model uses an unweighted product of eight
grammars trained by initializing the random number
generator with seeds 1 through 8. Table 2 shows
our test set results (obtained with CONSTITUENT-
LEVEL inference), and compares them to related
work. There is a large body of work that has re-
ported parsing accuracies for English, and we have
grouped the different methods into categories for
better overview.
Our results on the English in-domain test set are
higher than those obtained by any single component
parser (SINGLE). The other methods quoted in Ta-
ble 2 operate over the output of one or more single
component parsers and are therefore largely orthog-
onal to our line of work. It is nonetheless exciting
to see that our product model is competitive with
the discriminative rescoring methods (RE) of Char-
niak and Johnson (2005) and Huang (2008), achiev-
ing higher F1 scores but lower exact match. These
two methods work on top of the Charniak (2000)
parser, and it would be possible to exchange that
parser with our product model. We did not attempt
this experiment, but we expect that those methods
would stack well with our model, because they use
primarily non-local features that are not available in
a context-free grammar.
</bodyText>
<page confidence="0.992849">
25
</page>
<bodyText confidence="0.999973051282051">
Techniques like self-training (SELF) and system
combinations (COMBO) can further improve pars-
ing accuracies, but are also orthogonal to our work.
In particular the COMBO methods seem related to
our work, but are very different in their nature.
While we use multiple grammars in our work, all
grammars are from the same model class for us. In
contrast, those methods rely on a diverse set of in-
dividual parsers, each of which requires a signifi-
cant effort to build. Furthermore, those techniques
have largely relied on different voting schemes in the
past (Henderson and Brill, 1999; Sagae and Lavie,
2006), and only more recently have started using ac-
tual posteriors from the underlying models (Fossum
and Knight, 2009; Zhang et al., 2009). Even then,
those methods operate only over k-best lists, and we
are the first to work directly with parse forests from
multiple grammars.
It is also interesting to note that the best results
in Zhang et al. (2009) are achieved by combining k-
best lists from a latent variable grammar of Petrov
et al. (2006) with the self-trained reranking parser of
McClosky et al. (2006). Clearly, replacing the sin-
gle latent variable grammar with a product of latent
variable grammars ought to improve performance.
The results on the other two corpora are similar.
A product of latent variable grammars very signifi-
cantly outperforms a single latent variable grammar
and sets new standards for the state-of-the-art.
We also analyzed the errors of the product mod-
els. In addition to the illustrative example in Fig-
ure 5, we computed detailed error metrics for differ-
ent phrasal categories. Figure 4 shows that a product
of four random grammars is always better than even
the best underlying grammar. The individual gram-
mars seem to learn different sets of constraints, and
the product model is able to model them all at once,
giving consistent accuracy improvements across all
metrics.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999541166666667">
We presented a simple product model that signifi-
cantly improves parsing accuracies on different do-
mains and languages. Our model leverages multi-
ple automatically learned latent variable grammars,
which differ only in the seed of the random num-
ber generator used to initialize the EM learning al-
</bodyText>
<table confidence="0.99981475">
Type all sentences
LP LR EX
Parser
ENGLISH-WSJ
This Paper 92.0 91.7 41.9
SINGLE Charniak (2000) 89.9 89.5 37.2
Petrov and Klein (2007) 90.2 90.1 36.7
Carreras et al. (2008) 91.4 90.7 -
RE Charniak et al. (2005) 91.8 91.2 44.8
Huang (2008) 92.2 91.2 43.5
SELF Huang and Harper (2009) 91.37 91.57 39.37
McClosky et al. (2006) 92.5 92.1 45.3
COMBO Sagae and Lavie (2006) 93.2 91.0 -
Fossum and Knight (2009) 93.2 91.7 -
Zhang et al. (2009) 93.3 92.0 -
ENGLISH-BROWN
This Paper 86.5 86.3 35.8
SING Charniak (2000) 82.9 82.9 31.7
Petrov and Klein (2007) 83.9 83.8 29.6
RE Charniak et al. (2005) 86.1 85.2 36.8
GERMAN
This Paper 84.5 84.0 51.2
SING Petrov and Klein (2007) 80.0 80.2 42.4
Petrov and Klein (2008) 80.6 80.8 43.9
</table>
<tableCaption confidence="0.999874">
Table 2: Final test set accuracies for English and German.
</tableCaption>
<bodyText confidence="0.999970230769231">
gorithm. As our analysis showed, the grammars vary
widely, making very different errors. This is in part
due to the fact that EM is used not only for estimat-
ing the parameters of the grammar, but also to deter-
mine the set of context-free productions that under-
lie it. Because the resulting data representations are
largely independent, they can be easily combined in
an unweighted product model. The product model
does not require any additional training and is ca-
pable of significantly improving the state-of-the-art
in parsing accuracy. It remains to be seen if a sim-
ilar approach can be used in other cases where EM
converges to widely varying local maxima.
</bodyText>
<sectionHeader confidence="0.997533" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.988724">
I would like to thank Ryan McDonald for numerous
discussions on this topic and his feedback on earlier
versions of this paper. This work also benefited from
conversations with Gideon Mann, Fernando Pereira,
Dan Klein and Mehryar Mohri.
</bodyText>
<footnote confidence="0.8000935">
7Note that these results are on a modified version of the tree-
bank where unary productions are removed.
</footnote>
<page confidence="0.997351">
26
</page>
<sectionHeader confidence="0.995876" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99996249">
J. Baldridge and M. Osborne. 2008. Active learning and
logarithmic opinion pools for HPSG parse selection.
Natural Language Engineering.
R. F. Bordley. 1982. A multiplicative formula for aggre-
gating probability assessments. Management Science.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing.
A. Brown and G. Hinton. 2001. Products of hidden
Markov models. In AISTATS ’01.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In CoNLL ’08.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL’05.
E. Charniak. 1996. Tree-bank grammars. In AAAI ’96.
E. Charniak. 2000. A maximum–entropy–inspired
parser. In NAACL ’00.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, UPenn.
V. Fossum and K. Knight. 2009. Combining constituent
parsers. In NAACL ’09.
W. N. Francis and H. Kucera. 1979. Manual of infor-
mation to accompany a standard corpus of present-day
edited American English. Technical report, Brown
University.
Y. Freund and R. E. Shapire. 1996. Experiments with a
new boosting algorithm. In ICML ’96.
C. Genest and J. V. Zidek. 1986. Combining probability
distributions: A critique and an annotated bibliogra-
phy. Statistical Science.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. EMNLP ’01.
J. Goodman. 1996. Parsing algorithms and metrics. ACL
’96.
J. Henderson and E. Brill. 1999. Exploiting diversity
in natural language processing: combining parsers. In
EMNLP ’99.
J. Henderson and E. Brill. 2000. Bagging and boosting a
treebank parser. In NAACL ’00.
T. Heskes. 1998. Selecting weighting factors in logarith-
mic opinion pools. In NIPS ’98.
G. Hinton. 2001. Products of experts. In ICANN ’01.
G. Hinton. 2002. Training products of experts by mini-
mizing contrastive divergence. Neural Computation.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
IWPT ’05.
Z. Huang and M. Harper. 2009. Self-training PCFG
grammars with latent annotations across languages. In
EMNLP ’09.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In ACL ’08.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24.
D. Klein and C. Manning. 2003a. A* parsing: fast exact
viterbi parse selection. In NAACL ’03.
D. Klein and C. Manning. 2003b. Accurate unlexicalized
parsing. In ACL ’03.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML ’01.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ’05.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL ’06.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL ’07.
S. Petrov and D. Klein. 2008. Sparse multi-scale gram-
mars for discriminative latent variable parsing. In
EMNLP ’08.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ’06.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In NAACL ’06.
K. Sima’an. 2002. Computatoinal complexity of proba-
bilistic disambiguation. Grammars.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In ANLP ’97.
A. Smith and M. Osborne. 2007. Diversity in logarith-
mic opinion pools. Lingvisticae Investigationes.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL
’05.
X. Sun and J. Tsujii. 2009. Sequential labeling with la-
tent variables: An exact inference algorithm and its
efficient approximation. In EACL ’09.
D. Tax, M. Van Breukelen, R. Duin, and J. Kittler. 2000.
Combining multiple classifiers by averaging or by
multiplying? Pattern Recognition.
I. Titov and J. Henderson. 2006. Loss minimization in
parse reranking. In EMNLP ’06.
P. Xu and F. Jelinek. 2004. Random forests in language
modeling. In EMNLP ’04.
H. Zhang, M. Zhang, C. L. Tan, and H. Li. 2009. K-best
combination of syntactic parsers. In EMNLP ’09.
</reference>
<page confidence="0.99881">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.410382">
<title confidence="0.964358666666667">Products of Random Latent Variable Grammars Slav Google</title>
<author confidence="0.971255">New York</author>
<author confidence="0.971255">NY</author>
<email confidence="0.999834">slav@google.com</email>
<abstract confidence="0.954860631578948">We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-ofthe-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Baldridge</author>
<author>M Osborne</author>
</authors>
<title>Active learning and logarithmic opinion pools for HPSG parse selection. Natural Language Engineering.</title>
<date>2008</date>
<contexts>
<context position="23530" citStr="Baldridge and Osborne, 2008" startWordPosition="3869" endWordPosition="3872">aximizes tree-likelihood. It is therefore not too surprising that the two objective functions select the same tree only 41% of the time, even when limited to the same candidate set. Maximizing the 24 expected number of correct productions is superior for F1 score (see the one grammar case in Figure 6). However, as to be expected, likelihood is better for exact match, giving a score of 47.6% vs. 46.8%. 4.3 Systematic Bias Diversity among the underlying models is what gives combined models their strength. One way of increasing diversity is by modifying the feature sets of the individual models (Baldridge and Osborne, 2008; Smith and Osborne, 2007). This approach has the disadvantage that it reduces the performance of the individual models, and is not directly applicable for latent variable grammars because the features are automatically learned. Alternatively, one can introduce diversity by changing the training distribution. Bagging (Breiman, 1996) and Boosting (Freund and Shapire, 1996) fall into this category, but have had limited success for parsing (Henderson and Brill, 2000). Furthermore boosting is impractical here, because it requires training dozens of grammars in sequence. Since training a single gra</context>
</contexts>
<marker>Baldridge, Osborne, 2008</marker>
<rawString>J. Baldridge and M. Osborne. 2008. Active learning and logarithmic opinion pools for HPSG parse selection. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Bordley</author>
</authors>
<title>A multiplicative formula for aggregating probability assessments.</title>
<date>1982</date>
<journal>Management Science.</journal>
<contexts>
<context position="12910" citStr="Bordley, 1982" startWordPosition="2098" endWordPosition="2099">uce any systematic bias (but see Section 4.3 for some experiments), or attempt to train the models to be maximally different (Hinton, 2002) – we simply train a random collection of grammars by varying the random seed used for initialization. We found in our experiments that the randomness provided by EM is sufficient to achieve diversity among the individual grammars, and gives results that are as good as more involved training procedures. Xu and Jelinek (2004) made a similar observation when learning random forests for language modeling. Our model is reminiscent of Logarithmic Opinion Pools (Bordley, 1982) and Products of Experts (Hinton, 2001).3 However, because we believe that none of the underlying grammars should be favored, we deliberately do not use any combination weights. 3.2 Inference Computing the most likely parse tree is intractable for latent variable grammars (Sima’an, 2002), and therefore also for our product model. This is because there are exponentially many derivations over split subcategories that correspond to a single parse tree over unsplit categories, and there is no dynamic program to efficiently marginalize out the latent variables. Previous work on parse risk minimizat</context>
</contexts>
<marker>Bordley, 1982</marker>
<rawString>R. F. Bordley. 1982. A multiplicative formula for aggregating probability assessments. Management Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="23864" citStr="Breiman, 1996" startWordPosition="3920" endWordPosition="3921">etter for exact match, giving a score of 47.6% vs. 46.8%. 4.3 Systematic Bias Diversity among the underlying models is what gives combined models their strength. One way of increasing diversity is by modifying the feature sets of the individual models (Baldridge and Osborne, 2008; Smith and Osborne, 2007). This approach has the disadvantage that it reduces the performance of the individual models, and is not directly applicable for latent variable grammars because the features are automatically learned. Alternatively, one can introduce diversity by changing the training distribution. Bagging (Breiman, 1996) and Boosting (Freund and Shapire, 1996) fall into this category, but have had limited success for parsing (Henderson and Brill, 2000). Furthermore boosting is impractical here, because it requires training dozens of grammars in sequence. Since training a single grammar takes roughly one day, we opted for a different, parallelizable way of changing the training distribution. In a first experiment, we divided the training set into two disjoint sets, and trained separate grammars on each half. These truly disjoint grammars had low F1 scores of 89.4 and 89.6 respectively (because they were traine</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Brown</author>
<author>G Hinton</author>
</authors>
<title>Products of hidden Markov models.</title>
<date>2001</date>
<booktitle>In AISTATS ’01.</booktitle>
<contexts>
<context position="11662" citStr="Brown and Hinton, 2001" startWordPosition="1850" endWordPosition="1853">on, we could use a sum model, but we will show in Section 4.1 that the product formulation performs significantly better. Intuitively speaking, products have the advantage that the final prediction has a high posterior under all models, giving each model veto power. This is exactly the behavior that we need in the case of parsing, where each grammar has learned different constraints for ruling out improbable parses. 3.1 Learning Joint training of our product model would couple the parameters of the individual grammars, necessitating the computation of an intractable global partition function (Brown and Hinton, 2001). Instead, we use EM to train each grammar independently, 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 PP NP 10% 7% 4% 25% 15% 0% 1 2 3 4 5 6 7 8 30% 15% 0% IN 60% 30% 0% DT 60 50 40 30 20 10 NP VP PP ADVP ADJP S SBAR QP NNP JJ NNS NN RB VBN VBG VB IN CD VBD VBZ DT VBP 21 90.2 90.1 90 89.9 89.8 89.7 89.6 89.5 90.6 90.7 90.8 90.9 91 91.1 91.2 91.3 91.4 F1 Score on Section 22 Figure 3: Parsing accuracies for grammars learned from different random seeds. The large variance and weak correlation suggest that no single grammar is to be preferred. but from a different, randomly chosen starting poi</context>
</contexts>
<marker>Brown, Hinton, 2001</marker>
<rawString>A. Brown and G. Hinton. 2001. Products of hidden Markov models. In AISTATS ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<journal>In CoNLL</journal>
<volume>08</volume>
<contexts>
<context position="29939" citStr="Carreras et al. (2008)" startWordPosition="4918" endWordPosition="4921">ent sets of constraints, and the product model is able to model them all at once, giving consistent accuracy improvements across all metrics. 5 Conclusions We presented a simple product model that significantly improves parsing accuracies on different domains and languages. Our model leverages multiple automatically learned latent variable grammars, which differ only in the seed of the random number generator used to initialize the EM learning alType all sentences LP LR EX Parser ENGLISH-WSJ This Paper 92.0 91.7 41.9 SINGLE Charniak (2000) 89.9 89.5 37.2 Petrov and Klein (2007) 90.2 90.1 36.7 Carreras et al. (2008) 91.4 90.7 - RE Charniak et al. (2005) 91.8 91.2 44.8 Huang (2008) 92.2 91.2 43.5 SELF Huang and Harper (2009) 91.37 91.57 39.37 McClosky et al. (2006) 92.5 92.1 45.3 COMBO Sagae and Lavie (2006) 93.2 91.0 - Fossum and Knight (2009) 93.2 91.7 - Zhang et al. (2009) 93.3 92.0 - ENGLISH-BROWN This Paper 86.5 86.3 35.8 SING Charniak (2000) 82.9 82.9 31.7 Petrov and Klein (2007) 83.9 83.8 29.6 RE Charniak et al. (2005) 86.1 85.2 36.8 GERMAN This Paper 84.5 84.0 51.2 SING Petrov and Klein (2007) 80.0 80.2 42.4 Petrov and Klein (2008) 80.6 80.8 43.9 Table 2: Final test set accuracies for English and </context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In CoNLL ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-Fine NBest Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In ACL’05.</booktitle>
<contexts>
<context position="4593" citStr="Charniak and Johnson, 2005" startWordPosition="712" endWordPosition="716"> diversity into the underlying grammars, and also compare combining partial (constituent-level) and complete (tree-level) predictions. Quite serendipitously, the simplest approaches work best in our experiments. A product of eight latent variable grammars, learned on the same data, and only differing in the seed used in the random number generator that initialized EM, improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German. These parsing results are even better than those obtained by discriminative systems which have access to additional non-local features (Charniak and Johnson, 2005; Huang, 2008). 2 Latent Variable Grammars Before giving the details of our model, we briefly review the basic properties of latent variable grammars. Learning latent variable grammars consists of two tasks: (1) determining the data representation (the set of context-free productions to be used in the grammar), and (2) estimating the parameters of the model (the production probabilities). We focus on the randomness introduced by the EM algorithm and refer the reader to Matsuzaki et al. (2005) and Petrov et al. (2006) for a more general introduction. 2.1 Split &amp; Merge Learning Latent variable g</context>
<context position="27144" citStr="Charniak and Johnson (2005)" startWordPosition="4453" endWordPosition="4457">rence), and compares them to related work. There is a large body of work that has reported parsing accuracies for English, and we have grouped the different methods into categories for better overview. Our results on the English in-domain test set are higher than those obtained by any single component parser (SINGLE). The other methods quoted in Table 2 operate over the output of one or more single component parsers and are therefore largely orthogonal to our line of work. It is nonetheless exciting to see that our product model is competitive with the discriminative rescoring methods (RE) of Charniak and Johnson (2005) and Huang (2008), achieving higher F1 scores but lower exact match. These two methods work on top of the Charniak (2000) parser, and it would be possible to exchange that parser with our product model. We did not attempt this experiment, but we expect that those methods would stack well with our model, because they use primarily non-local features that are not available in a context-free grammar. 25 Techniques like self-training (SELF) and system combinations (COMBO) can further improve parsing accuracies, but are also orthogonal to our work. In particular the COMBO methods seem related to ou</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-Fine NBest Parsing and MaxEnt Discriminative Reranking. In ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In AAAI ’96. E. Charniak.</booktitle>
<contexts>
<context position="1128" citStr="Charniak, 1996" startWordPosition="170" endWordPosition="171">, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German. 1 Introduction Learning a context-free grammar for parsing requires the estimation of a more highly articulated model than the one embodied by the observed treebank. This is because the naive treebank grammar (Charniak, 1996) is too permissive, making unrealistic context-freedom assumptions. For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexic</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>E. Charniak. 1996. Tree-bank grammars. In AAAI ’96. E. Charniak. 2000. A maximum–entropy–inspired parser. In NAACL ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis, UPenn.</tech>
<contexts>
<context position="1702" citStr="Collins, 1999" startWordPosition="262" endWordPosition="263">naive treebank grammar (Charniak, 1996) is too permissive, making unrealistic context-freedom assumptions. For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized 19 (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher pars</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, UPenn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Fossum</author>
<author>K Knight</author>
</authors>
<title>Combining constituent parsers.</title>
<date>2009</date>
<booktitle>In NAACL ’09.</booktitle>
<contexts>
<context position="3233" citStr="Fossum and Knight, 2009" startWordPosition="503" endWordPosition="506">efinements. We use these variations to our advantage, and treat grammars learned from different random seeds as independent and equipotent experts. We use a product distribution for joint prediction, which gives more peaked posteriors than a sum, and enforces all constraints of the individual grammars, without the need to tune mixing weights. It should be noted here that our focus is on improving parsing performance using a single underlying grammar class, which is somewhat orthogonal to the issue of parser combination, that has been studied elsewhere in the literature (Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009). In contrast to that line of work, we also do not restrict ourselves to working with kbest output, but work directly with a packed forest representation of the posteriors, much in the spirit of Huang (2008), except that we work with several forests rather than rescoring a single one. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19–27, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics In our experimental section we give empirical answers to some of the remaining theoretical questi</context>
<context position="28266" citStr="Fossum and Knight, 2009" startWordPosition="4641" endWordPosition="4644">g accuracies, but are also orthogonal to our work. In particular the COMBO methods seem related to our work, but are very different in their nature. While we use multiple grammars in our work, all grammars are from the same model class for us. In contrast, those methods rely on a diverse set of individual parsers, each of which requires a significant effort to build. Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999; Sagae and Lavie, 2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009; Zhang et al., 2009). Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars. It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of Petrov et al. (2006) with the self-trained reranking parser of McClosky et al. (2006). Clearly, replacing the single latent variable grammar with a product of latent variable grammars ought to improve performance. The results on the other two corpora are similar. A product of latent variabl</context>
<context position="30171" citStr="Fossum and Knight (2009)" startWordPosition="4961" endWordPosition="4964">curacies on different domains and languages. Our model leverages multiple automatically learned latent variable grammars, which differ only in the seed of the random number generator used to initialize the EM learning alType all sentences LP LR EX Parser ENGLISH-WSJ This Paper 92.0 91.7 41.9 SINGLE Charniak (2000) 89.9 89.5 37.2 Petrov and Klein (2007) 90.2 90.1 36.7 Carreras et al. (2008) 91.4 90.7 - RE Charniak et al. (2005) 91.8 91.2 44.8 Huang (2008) 92.2 91.2 43.5 SELF Huang and Harper (2009) 91.37 91.57 39.37 McClosky et al. (2006) 92.5 92.1 45.3 COMBO Sagae and Lavie (2006) 93.2 91.0 - Fossum and Knight (2009) 93.2 91.7 - Zhang et al. (2009) 93.3 92.0 - ENGLISH-BROWN This Paper 86.5 86.3 35.8 SING Charniak (2000) 82.9 82.9 31.7 Petrov and Klein (2007) 83.9 83.8 29.6 RE Charniak et al. (2005) 86.1 85.2 36.8 GERMAN This Paper 84.5 84.0 51.2 SING Petrov and Klein (2007) 80.0 80.2 42.4 Petrov and Klein (2008) 80.6 80.8 43.9 Table 2: Final test set accuracies for English and German. gorithm. As our analysis showed, the grammars vary widely, making very different errors. This is in part due to the fact that EM is used not only for estimating the parameters of the grammar, but also to determine the set of</context>
</contexts>
<marker>Fossum, Knight, 2009</marker>
<rawString>V. Fossum and K. Knight. 2009. Combining constituent parsers. In NAACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Manual of information to accompany a standard corpus of present-day edited American English.</title>
<date>1979</date>
<tech>Technical report,</tech>
<institution>Brown University.</institution>
<marker>Francis, Kucera, 1979</marker>
<rawString>W. N. Francis and H. Kucera. 1979. Manual of information to accompany a standard corpus of present-day edited American English. Technical report, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Shapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In ICML ’96.</booktitle>
<contexts>
<context position="23904" citStr="Freund and Shapire, 1996" startWordPosition="3924" endWordPosition="3927"> a score of 47.6% vs. 46.8%. 4.3 Systematic Bias Diversity among the underlying models is what gives combined models their strength. One way of increasing diversity is by modifying the feature sets of the individual models (Baldridge and Osborne, 2008; Smith and Osborne, 2007). This approach has the disadvantage that it reduces the performance of the individual models, and is not directly applicable for latent variable grammars because the features are automatically learned. Alternatively, one can introduce diversity by changing the training distribution. Bagging (Breiman, 1996) and Boosting (Freund and Shapire, 1996) fall into this category, but have had limited success for parsing (Henderson and Brill, 2000). Furthermore boosting is impractical here, because it requires training dozens of grammars in sequence. Since training a single grammar takes roughly one day, we opted for a different, parallelizable way of changing the training distribution. In a first experiment, we divided the training set into two disjoint sets, and trained separate grammars on each half. These truly disjoint grammars had low F1 scores of 89.4 and 89.6 respectively (because they were trained on less data). Their combination unfor</context>
</contexts>
<marker>Freund, Shapire, 1996</marker>
<rawString>Y. Freund and R. E. Shapire. 1996. Experiments with a new boosting algorithm. In ICML ’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Genest</author>
<author>J V Zidek</author>
</authors>
<title>Combining probability distributions: A critique and an annotated bibliography.</title>
<date>1986</date>
<journal>Statistical Science.</journal>
<contexts>
<context position="19563" citStr="Genest and Zidek, 1986" startWordPosition="3220" endWordPosition="3223">et Dev. Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 (Marcus et al., 1993) 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al. 1979) ENGLISH-WSJ the data5 the data5 GERMAN Sentences Sentences Sentences (Skut et al., 1997) 1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups. Parsing accuracy on the WSJ development set CONSTITUENT-LEVEL Inference TREE-LEVEL Inference 92.5 92 91.5 91 90.5 4.1 (Weighted) Product vs. (Weighted) Sum A great deal has been written on the topic of products versus sums of probability distributions for joint prediction (Genest and Zidek, 1986; Tax et al., 2000). However, those theoretical results do not apply directly here, because we are using multiple randomly permuted models from the same class, rather models from different classes. To shed some light on this issue, we addressed the question empirically, and combined two grammars into an unweighted product model, and also an unweighted sum model. The individual grammars had parsing accuracies (F1) of 91.2 and 90.7 respectively, and their product (91.7) clearly outperformed their sum (91.3). When more grammars are added, the gap widens even further, and the trends persist indepe</context>
</contexts>
<marker>Genest, Zidek, 1986</marker>
<rawString>C. Genest and J. V. Zidek. 1986. Combining probability distributions: A critique and an annotated bibliography. Statistical Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<journal>EMNLP</journal>
<volume>01</volume>
<contexts>
<context position="21154" citStr="Gildea (2001)" startWordPosition="3483" endWordPosition="3484">, they obtain an F1 score of 91.0 (which they further improve to 91.4 using a voting scheme). We replicated their experiment, but used an unweighted product of the two model scores. Using TREELEVEL inference, we obtained an F1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.6 This is in line with previous work on product models, where weighting has been important when combining heterogenous classifiers (Heskes, 1998), and less important when the classifiers are of similar accuracy (Smith et al., 2005). 5See Gildea (2001) for the exact setup. 6The unweighted sum model, however, underperforms the individual models with an F, score of only 90.3. 1 2 4 8 16 Number of grammars in product model Figure 6: Adding more grammars to the product model improves parsing accuracy, while CONSTITUENT-LEVEL inference gives consistently better results. 4.2 Tree-Level vs. Constituent-Level Inference Figure 6 shows that accuracy increases when more grammars are added to the product model, but levels off after eight grammars. The plot also compares our two inference approximations, and shows that CONSTITUENT-LEVEL inference result</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>D. Gildea. 2001. Corpus variation and parser performance. EMNLP ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<journal>ACL</journal>
<volume>96</volume>
<contexts>
<context position="14043" citStr="Goodman, 1996" startWordPosition="2277" endWordPosition="2278">ntly marginalize out the latent variables. Previous work on parse risk minimization has addressed this problem in two different ways: by changing the objective function, or by constraining 3As a matter of fact, Hinton (2001) mentions syntactic parsing as one of the motivating examples for Products of Experts. Figure 4: Breakdown of different accuracy measures for four randomly selected grammars (G,-G4), as well as a product model (P) that uses those four grammars. Note that no single grammar does well on all measures, while the product model does significantly better on all. the search space (Goodman, 1996; Titov and Henderson, 2006; Petrov and Klein, 2007). The simplest approach is to stick to likelihood as the objective function, but to limit the search space to a set of high quality candidates T : T* = argmax P(T |w) (3) TET Because the likelihood of a given parse tree can be computed exactly for our product model (Eq. 2), the quality of this approximation is only limited by the quality of the candidate list. To generate the candidate list, we produce k-best lists of Viterbi derivations with the efficient algorithm of Huang and Chiang (2005), and erase the subcategory information to obtain p</context>
<context position="16541" citStr="Goodman (1996)" startWordPosition="2737" endWordPosition="2738">mmar G1 has a preference for flat structures, while grammar G2 prefers deeper hierarchical structures. Both grammars therefore make one mistake each on their own. However, the correct parse tree (which uses a flat ADJP in the first slot and a hierarchical NP in the second) scores highest under the product model. search space. Petrov and Klein (2007) present such an objective function, which maximizes the product of expected correct productions r: �T� = argmax T rET These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al. (2005). We extend the algorithm to work over posterior distributions from multiple grammars, by aggregating their expectations into a product. In practice, we use a packed forest representation to approximate the posterior distribution, as in Huang (2008). We refer to this approximation as CONSTITUENTLEVEL, because it allows us to form new parse trees from individual constituents. Figure 5 illustrates a real case where the product model was able to construct a completely correct parse tree from two partially correct ones. In the example</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing algorithms and metrics. ACL ’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>E Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: combining parsers.</title>
<date>1999</date>
<booktitle>In EMNLP ’99.</booktitle>
<contexts>
<context position="28129" citStr="Henderson and Brill, 1999" startWordPosition="4619" endWordPosition="4622">not available in a context-free grammar. 25 Techniques like self-training (SELF) and system combinations (COMBO) can further improve parsing accuracies, but are also orthogonal to our work. In particular the COMBO methods seem related to our work, but are very different in their nature. While we use multiple grammars in our work, all grammars are from the same model class for us. In contrast, those methods rely on a diverse set of individual parsers, each of which requires a significant effort to build. Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999; Sagae and Lavie, 2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009; Zhang et al., 2009). Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars. It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of Petrov et al. (2006) with the self-trained reranking parser of McClosky et al. (2006). Clearly, replacing the single latent variable grammar with a produc</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>J. Henderson and E. Brill. 1999. Exploiting diversity in natural language processing: combining parsers. In EMNLP ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>E Brill</author>
</authors>
<title>Bagging and boosting a treebank parser.</title>
<date>2000</date>
<booktitle>In NAACL ’00.</booktitle>
<contexts>
<context position="23998" citStr="Henderson and Brill, 2000" startWordPosition="3939" endWordPosition="3943">t gives combined models their strength. One way of increasing diversity is by modifying the feature sets of the individual models (Baldridge and Osborne, 2008; Smith and Osborne, 2007). This approach has the disadvantage that it reduces the performance of the individual models, and is not directly applicable for latent variable grammars because the features are automatically learned. Alternatively, one can introduce diversity by changing the training distribution. Bagging (Breiman, 1996) and Boosting (Freund and Shapire, 1996) fall into this category, but have had limited success for parsing (Henderson and Brill, 2000). Furthermore boosting is impractical here, because it requires training dozens of grammars in sequence. Since training a single grammar takes roughly one day, we opted for a different, parallelizable way of changing the training distribution. In a first experiment, we divided the training set into two disjoint sets, and trained separate grammars on each half. These truly disjoint grammars had low F1 scores of 89.4 and 89.6 respectively (because they were trained on less data). Their combination unfortunately also achieves only an accuracy of 90.9, which is lower than what we get when training</context>
</contexts>
<marker>Henderson, Brill, 2000</marker>
<rawString>J. Henderson and E. Brill. 2000. Bagging and boosting a treebank parser. In NAACL ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Heskes</author>
</authors>
<title>Selecting weighting factors in logarithmic opinion pools.</title>
<date>1998</date>
<booktitle>In NIPS ’98. G. Hinton.</booktitle>
<contexts>
<context position="21048" citStr="Heskes, 1998" startWordPosition="3465" endWordPosition="3467">ights learned on a held-out set and rescoring 50- best lists from Charniak (2000) and Petrov et al. (2006), they obtain an F1 score of 91.0 (which they further improve to 91.4 using a voting scheme). We replicated their experiment, but used an unweighted product of the two model scores. Using TREELEVEL inference, we obtained an F1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.6 This is in line with previous work on product models, where weighting has been important when combining heterogenous classifiers (Heskes, 1998), and less important when the classifiers are of similar accuracy (Smith et al., 2005). 5See Gildea (2001) for the exact setup. 6The unweighted sum model, however, underperforms the individual models with an F, score of only 90.3. 1 2 4 8 16 Number of grammars in product model Figure 6: Adding more grammars to the product model improves parsing accuracy, while CONSTITUENT-LEVEL inference gives consistently better results. 4.2 Tree-Level vs. Constituent-Level Inference Figure 6 shows that accuracy increases when more grammars are added to the product model, but levels off after eight grammars. </context>
</contexts>
<marker>Heskes, 1998</marker>
<rawString>T. Heskes. 1998. Selecting weighting factors in logarithmic opinion pools. In NIPS ’98. G. Hinton. 2001. Products of experts. In ICANN ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence. Neural Computation.</title>
<date>2002</date>
<contexts>
<context position="12435" citStr="Hinton, 2002" startWordPosition="2024" endWordPosition="2025">IN 60% 30% 0% DT 60 50 40 30 20 10 NP VP PP ADVP ADJP S SBAR QP NNP JJ NNS NN RB VBN VBG VB IN CD VBD VBZ DT VBP 21 90.2 90.1 90 89.9 89.8 89.7 89.6 89.5 90.6 90.7 90.8 90.9 91 91.1 91.2 91.3 91.4 F1 Score on Section 22 Figure 3: Parsing accuracies for grammars learned from different random seeds. The large variance and weak correlation suggest that no single grammar is to be preferred. but from a different, randomly chosen starting point. To emphasize, we do not introduce any systematic bias (but see Section 4.3 for some experiments), or attempt to train the models to be maximally different (Hinton, 2002) – we simply train a random collection of grammars by varying the random seed used for initialization. We found in our experiments that the randomness provided by EM is sufficient to achieve diversity among the individual grammars, and gives results that are as good as more involved training procedures. Xu and Jelinek (2004) made a similar observation when learning random forests for language modeling. Our model is reminiscent of Logarithmic Opinion Pools (Bordley, 1982) and Products of Experts (Hinton, 2001).3 However, because we believe that none of the underlying grammars should be favored,</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>G. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In IWPT ’05.</booktitle>
<contexts>
<context position="14592" citStr="Huang and Chiang (2005)" startWordPosition="2374" endWordPosition="2378">ct model does significantly better on all. the search space (Goodman, 1996; Titov and Henderson, 2006; Petrov and Klein, 2007). The simplest approach is to stick to likelihood as the objective function, but to limit the search space to a set of high quality candidates T : T* = argmax P(T |w) (3) TET Because the likelihood of a given parse tree can be computed exactly for our product model (Eq. 2), the quality of this approximation is only limited by the quality of the candidate list. To generate the candidate list, we produce k-best lists of Viterbi derivations with the efficient algorithm of Huang and Chiang (2005), and erase the subcategory information to obtain parse trees over unsplit categories. We refer to this approximation as TREE-LEVEL inference, because it considers a list of complete trees from the underlying grammars, and selects the tree that has the highest likelihood under the product model. While the k-best lists are of very high quality, this is a fairly crude and unsatisfactory way of approximating the posterior distribution of the product model, as it does not allow the synthesis of new trees based on tree fragments from different grammars. An alternative is to use a tractable objectiv</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>L. Huang and D. Chiang. 2005. Better k-best parsing. In IWPT ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Huang</author>
<author>M Harper</author>
</authors>
<title>Self-training PCFG grammars with latent annotations across languages. In</title>
<date>2009</date>
<journal>EMNLP</journal>
<volume>09</volume>
<contexts>
<context position="30049" citStr="Huang and Harper (2009)" startWordPosition="4939" endWordPosition="4942">improvements across all metrics. 5 Conclusions We presented a simple product model that significantly improves parsing accuracies on different domains and languages. Our model leverages multiple automatically learned latent variable grammars, which differ only in the seed of the random number generator used to initialize the EM learning alType all sentences LP LR EX Parser ENGLISH-WSJ This Paper 92.0 91.7 41.9 SINGLE Charniak (2000) 89.9 89.5 37.2 Petrov and Klein (2007) 90.2 90.1 36.7 Carreras et al. (2008) 91.4 90.7 - RE Charniak et al. (2005) 91.8 91.2 44.8 Huang (2008) 92.2 91.2 43.5 SELF Huang and Harper (2009) 91.37 91.57 39.37 McClosky et al. (2006) 92.5 92.1 45.3 COMBO Sagae and Lavie (2006) 93.2 91.0 - Fossum and Knight (2009) 93.2 91.7 - Zhang et al. (2009) 93.3 92.0 - ENGLISH-BROWN This Paper 86.5 86.3 35.8 SING Charniak (2000) 82.9 82.9 31.7 Petrov and Klein (2007) 83.9 83.8 29.6 RE Charniak et al. (2005) 86.1 85.2 36.8 GERMAN This Paper 84.5 84.0 51.2 SING Petrov and Klein (2007) 80.0 80.2 42.4 Petrov and Klein (2008) 80.6 80.8 43.9 Table 2: Final test set accuracies for English and German. gorithm. As our analysis showed, the grammars vary widely, making very different errors. This is in pa</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Z. Huang and M. Harper. 2009. Self-training PCFG grammars with latent annotations across languages. In EMNLP ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL ’08.</booktitle>
<contexts>
<context position="3461" citStr="Huang (2008)" startWordPosition="546" endWordPosition="547"> a sum, and enforces all constraints of the individual grammars, without the need to tune mixing weights. It should be noted here that our focus is on improving parsing performance using a single underlying grammar class, which is somewhat orthogonal to the issue of parser combination, that has been studied elsewhere in the literature (Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009). In contrast to that line of work, we also do not restrict ourselves to working with kbest output, but work directly with a packed forest representation of the posteriors, much in the spirit of Huang (2008), except that we work with several forests rather than rescoring a single one. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19–27, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics In our experimental section we give empirical answers to some of the remaining theoretical questions. We address the question of averaging versus multiplying classifier predictions, we investigate different ways of introducing more diversity into the underlying grammars, and also compare combining partial (constituent-level</context>
<context position="16854" citStr="Huang (2008)" startWordPosition="2786" endWordPosition="2787">earch space. Petrov and Klein (2007) present such an objective function, which maximizes the product of expected correct productions r: �T� = argmax T rET These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al. (2005). We extend the algorithm to work over posterior distributions from multiple grammars, by aggregating their expectations into a product. In practice, we use a packed forest representation to approximate the posterior distribution, as in Huang (2008). We refer to this approximation as CONSTITUENTLEVEL, because it allows us to form new parse trees from individual constituents. Figure 5 illustrates a real case where the product model was able to construct a completely correct parse tree from two partially correct ones. In the example, one of the underlying grammars (G1) had an imperfect recall score, because of its preference for flat structures (it missed an NP node in the second part of the sentence). In contrast, the other grammar (G2) favors deeper structures, and therefore introduced a superfluous ADVP node. The product model gives eac</context>
<context position="22560" citStr="Huang (2008)" startWordPosition="3711" endWordPosition="3712">esults hold even when the candidate set for CONSTITUENT-LEVEL inference is constrained to trees from the k-best lists. While the packed forrest representation can very efficiently encode an exponential set of parse trees, in our case the k-best lists appear to be already very diverse because they are generated by multiple grammars. Starting at 96.1 for a single latent variable grammar, merging two 50-best lists from different grammars gives an oracle score of 97.4, and adding more k-best lists further improves the oracle score to 98.6 for 16 grammars. This compares favorably to the results of Huang (2008), where the oracle score over a pruned forest is shown to be 97.8 (compared to 96.7 for a 50-best list). The accuracy improvement can instead be explained by the change in the objective function. Recall from section Section 3.2, that CONSTITUENTLEVEL inference maximizes the expected number of correct productions, while TREE-LEVEL inference maximizes tree-likelihood. It is therefore not too surprising that the two objective functions select the same tree only 41% of the time, even when limited to the same candidate set. Maximizing the 24 expected number of correct productions is superior for F1</context>
<context position="27161" citStr="Huang (2008)" startWordPosition="4459" endWordPosition="4460">ated work. There is a large body of work that has reported parsing accuracies for English, and we have grouped the different methods into categories for better overview. Our results on the English in-domain test set are higher than those obtained by any single component parser (SINGLE). The other methods quoted in Table 2 operate over the output of one or more single component parsers and are therefore largely orthogonal to our line of work. It is nonetheless exciting to see that our product model is competitive with the discriminative rescoring methods (RE) of Charniak and Johnson (2005) and Huang (2008), achieving higher F1 scores but lower exact match. These two methods work on top of the Charniak (2000) parser, and it would be possible to exchange that parser with our product model. We did not attempt this experiment, but we expect that those methods would stack well with our model, because they use primarily non-local features that are not available in a context-free grammar. 25 Techniques like self-training (SELF) and system combinations (COMBO) can further improve parsing accuracies, but are also orthogonal to our work. In particular the COMBO methods seem related to our work, but are v</context>
<context position="30005" citStr="Huang (2008)" startWordPosition="4933" endWordPosition="4934">once, giving consistent accuracy improvements across all metrics. 5 Conclusions We presented a simple product model that significantly improves parsing accuracies on different domains and languages. Our model leverages multiple automatically learned latent variable grammars, which differ only in the seed of the random number generator used to initialize the EM learning alType all sentences LP LR EX Parser ENGLISH-WSJ This Paper 92.0 91.7 41.9 SINGLE Charniak (2000) 89.9 89.5 37.2 Petrov and Klein (2007) 90.2 90.1 36.7 Carreras et al. (2008) 91.4 90.7 - RE Charniak et al. (2005) 91.8 91.2 44.8 Huang (2008) 92.2 91.2 43.5 SELF Huang and Harper (2009) 91.37 91.57 39.37 McClosky et al. (2006) 92.5 92.1 45.3 COMBO Sagae and Lavie (2006) 93.2 91.0 - Fossum and Knight (2009) 93.2 91.7 - Zhang et al. (2009) 93.3 92.0 - ENGLISH-BROWN This Paper 86.5 86.3 35.8 SING Charniak (2000) 82.9 82.9 31.7 Petrov and Klein (2007) 83.9 83.8 29.6 RE Charniak et al. (2005) 86.1 85.2 36.8 GERMAN This Paper 84.5 84.0 51.2 SING Petrov and Klein (2007) 80.0 80.2 42.4 Petrov and Klein (2008) 80.6 80.8 43.9 Table 2: Final test set accuracies for English and German. gorithm. As our analysis showed, the grammars vary widely,</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>L. Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<contexts>
<context position="1752" citStr="Johnson, 1998" startWordPosition="268" endWordPosition="269">missive, making unrealistic context-freedom assumptions. For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized 19 (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and t</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>A* parsing: fast exact viterbi parse selection.</title>
<date>2003</date>
<booktitle>In NAACL ’03.</booktitle>
<contexts>
<context position="1777" citStr="Klein and Manning, 2003" startWordPosition="270" endWordPosition="273"> unrealistic context-freedom assumptions. For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized 19 (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and there are no guarantees th</context>
<context position="18527" citStr="Klein and Manning, 2003" startWordPosition="3064" endWordPosition="3067">projected grammars for each individual grammar and parse with each one in sequence. Because only the very last pass requires scores from the different underlying grammars, this computation can be trivially parallelized across multiple CPUs. Additionally, the first (X-Bar) pruning pass needs to be computed only once because it is shared among all grammars. Since the X-Bar pass is the bottleneck of the multipass scheme (using nearly 50% of the total processing time), the overhead of using a product model is quite manageable. It would have also been possible to use A*-search for factored models (Klein and Manning, 2003a; Sun and Tsujii, 2009), but we did not attempt this in the present work. 4 Experiments In our experiments, we follow the standard setups described in Table 1, and use the EVALB tool for computing parsing figures. Unless noted otherwise, we use CONSTITUENT-LEVEL inference. All our experiments are based on the publicly available BerkeleyParser.4 4http://code.google.com/p/berkeleyparser )E(rlw) (4) 23 Training Set Dev. Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 (Marcus et al., 1993) 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al. 1979) ENGLISH-WSJ the data5 the data5 GERMA</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003a. A* parsing: fast exact viterbi parse selection. In NAACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL ’03.</booktitle>
<contexts>
<context position="1777" citStr="Klein and Manning, 2003" startWordPosition="270" endWordPosition="273"> unrealistic context-freedom assumptions. For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized 19 (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and there are no guarantees th</context>
<context position="18527" citStr="Klein and Manning, 2003" startWordPosition="3064" endWordPosition="3067">projected grammars for each individual grammar and parse with each one in sequence. Because only the very last pass requires scores from the different underlying grammars, this computation can be trivially parallelized across multiple CPUs. Additionally, the first (X-Bar) pruning pass needs to be computed only once because it is shared among all grammars. Since the X-Bar pass is the bottleneck of the multipass scheme (using nearly 50% of the total processing time), the overhead of using a product model is quite manageable. It would have also been possible to use A*-search for factored models (Klein and Manning, 2003a; Sun and Tsujii, 2009), but we did not attempt this in the present work. 4 Experiments In our experiments, we follow the standard setups described in Table 1, and use the EVALB tool for computing parsing figures. Unless noted otherwise, we use CONSTITUENT-LEVEL inference. All our experiments are based on the publicly available BerkeleyParser.4 4http://code.google.com/p/berkeleyparser )E(rlw) (4) 23 Training Set Dev. Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 (Marcus et al., 1993) 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al. 1979) ENGLISH-WSJ the data5 the data5 GERMA</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003b. Accurate unlexicalized parsing. In ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML ’01.</booktitle>
<contexts>
<context position="25342" citStr="Lafferty et al., 2001" startWordPosition="4153" endWordPosition="4156">tions of the treebank were held out. The resulting grammars had parsing accuracies of about 90.5, and the product model was again not able to overcome the lower starting point, despite the potentially larger diversity among the underlying grammars. It appears that any systematic bias that lowers the accuracy of the individual grammars also hurts the final performance of the product model. 4.4 Product Distribution as Smoothing Smith et al. (2005) interpret Logarithmic Opinion Pools (LOPs) as a smoothing technique. They compare regularizing Conditional Random Fields (CRFs) with Gaussian priors (Lafferty et al., 2001), to training a set of unregularized CRFs over different feature sets and combining them in an LOP. In their experiments, both approaches work comparably well, but their combination, an LOP of regularized CRFs works best. Not too surprisingly, we find this to be the case here as well. The parameters of each latent variable grammar are typically smoothed in a linear fashion to prevent excessive overfitting (Petrov et al., 2006). While all the experiments so far used smoothed grammars, we reran the experiments also with a set of unsmoothed grammars. The individual unsmoothed grammars have on ave</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In ICML ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="19026" citStr="Marcus et al., 1993" startWordPosition="3139" endWordPosition="3142">model is quite manageable. It would have also been possible to use A*-search for factored models (Klein and Manning, 2003a; Sun and Tsujii, 2009), but we did not attempt this in the present work. 4 Experiments In our experiments, we follow the standard setups described in Table 1, and use the EVALB tool for computing parsing figures. Unless noted otherwise, we use CONSTITUENT-LEVEL inference. All our experiments are based on the publicly available BerkeleyParser.4 4http://code.google.com/p/berkeleyparser )E(rlw) (4) 23 Training Set Dev. Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 (Marcus et al., 1993) 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al. 1979) ENGLISH-WSJ the data5 the data5 GERMAN Sentences Sentences Sentences (Skut et al., 1997) 1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups. Parsing accuracy on the WSJ development set CONSTITUENT-LEVEL Inference TREE-LEVEL Inference 92.5 92 91.5 91 90.5 4.1 (Weighted) Product vs. (Weighted) Sum A great deal has been written on the topic of products versus sums of probability distributions for joint prediction (Genest and Zidek, 1986; Tax et al., 2000). However, those theoretical results do not </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL ’05.</booktitle>
<contexts>
<context position="1828" citStr="Matsuzaki et al., 2005" startWordPosition="278" endWordPosition="281">e, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized 19 (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and there are no guarantees that it will find the same grammars when initialized </context>
<context position="5090" citStr="Matsuzaki et al. (2005)" startWordPosition="791" endWordPosition="794">r than those obtained by discriminative systems which have access to additional non-local features (Charniak and Johnson, 2005; Huang, 2008). 2 Latent Variable Grammars Before giving the details of our model, we briefly review the basic properties of latent variable grammars. Learning latent variable grammars consists of two tasks: (1) determining the data representation (the set of context-free productions to be used in the grammar), and (2) estimating the parameters of the model (the production probabilities). We focus on the randomness introduced by the EM algorithm and refer the reader to Matsuzaki et al. (2005) and Petrov et al. (2006) for a more general introduction. 2.1 Split &amp; Merge Learning Latent variable grammars split the coarse (but observed) grammar categories of a treebank into more fine-grained (but hidden) subcategories, which are better suited for modeling the syntax of natural languages (e.g. NP becomes NP1 through NPO. Accordingly, each grammar production A—*BC over observed categories A,B,C is split into a set of productions A,,—*ByCz over hidden categories A,,,By,Cz. Computing the joint likelihood of the observed parse trees T and sentences w requires summing over all derivations t </context>
<context position="16605" citStr="Matsuzaki et al. (2005)" startWordPosition="2745" endWordPosition="2748">mmar G2 prefers deeper hierarchical structures. Both grammars therefore make one mistake each on their own. However, the correct parse tree (which uses a flat ADJP in the first slot and a hierarchical NP in the second) scores highest under the product model. search space. Petrov and Klein (2007) present such an objective function, which maximizes the product of expected correct productions r: �T� = argmax T rET These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al. (2005). We extend the algorithm to work over posterior distributions from multiple grammars, by aggregating their expectations into a product. In practice, we use a packed forest representation to approximate the posterior distribution, as in Huang (2008). We refer to this approximation as CONSTITUENTLEVEL, because it allows us to form new parse trees from individual constituents. Figure 5 illustrates a real case where the product model was able to construct a completely correct parse tree from two partially correct ones. In the example, one of the underlying grammars (G1) had an imperfect recall sc</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In NAACL ’06.</booktitle>
<contexts>
<context position="28660" citStr="McClosky et al. (2006)" startWordPosition="4710" endWordPosition="4713">ques have largely relied on different voting schemes in the past (Henderson and Brill, 1999; Sagae and Lavie, 2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009; Zhang et al., 2009). Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars. It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of Petrov et al. (2006) with the self-trained reranking parser of McClosky et al. (2006). Clearly, replacing the single latent variable grammar with a product of latent variable grammars ought to improve performance. The results on the other two corpora are similar. A product of latent variable grammars very significantly outperforms a single latent variable grammar and sets new standards for the state-of-the-art. We also analyzed the errors of the product models. In addition to the illustrative example in Figure 5, we computed detailed error metrics for different phrasal categories. Figure 4 shows that a product of four random grammars is always better than even the best underly</context>
<context position="30090" citStr="McClosky et al. (2006)" startWordPosition="4946" endWordPosition="4949">ions We presented a simple product model that significantly improves parsing accuracies on different domains and languages. Our model leverages multiple automatically learned latent variable grammars, which differ only in the seed of the random number generator used to initialize the EM learning alType all sentences LP LR EX Parser ENGLISH-WSJ This Paper 92.0 91.7 41.9 SINGLE Charniak (2000) 89.9 89.5 37.2 Petrov and Klein (2007) 90.2 90.1 36.7 Carreras et al. (2008) 91.4 90.7 - RE Charniak et al. (2005) 91.8 91.2 44.8 Huang (2008) 92.2 91.2 43.5 SELF Huang and Harper (2009) 91.37 91.57 39.37 McClosky et al. (2006) 92.5 92.1 45.3 COMBO Sagae and Lavie (2006) 93.2 91.0 - Fossum and Knight (2009) 93.2 91.7 - Zhang et al. (2009) 93.3 92.0 - ENGLISH-BROWN This Paper 86.5 86.3 35.8 SING Charniak (2000) 82.9 82.9 31.7 Petrov and Klein (2007) 83.9 83.8 29.6 RE Charniak et al. (2005) 86.1 85.2 36.8 GERMAN This Paper 84.5 84.0 51.2 SING Petrov and Klein (2007) 80.0 80.2 42.4 Petrov and Klein (2008) 80.6 80.8 43.9 Table 2: Final test set accuracies for English and German. gorithm. As our analysis showed, the grammars vary widely, making very different errors. This is in part due to the fact that EM is used not on</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Effective self-training for parsing. In NAACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In NAACL ’07.</booktitle>
<contexts>
<context position="8640" citStr="Petrov and Klein, 2007" startWordPosition="1354" endWordPosition="1357">milarities is fairly straight-forward, it is less obvious how to quantify differences. 2Note that despite their variance, the performance is always higher than the one of the lexicalized parser of Charniak (2000). 20 Automatically determined number of subcategories Figure 1: There is large variance in the number of subcategories (error bars correspond to one standard deviation). is only a weak correlation between the accuracies on the two evaluation sets (Pearson coefficient 0.34). This suggests that no single grammar should be preferred over the others. In previous work (Petrov et al., 2006; Petrov and Klein, 2007) the final grammar was chosen based on its performance on a held-out set (section 22), and corresponds to the second best grammar in Figure 3 (because only 8 different grammars were trained). A more detailed error analysis is given in Figure 4, where we show a breakdown of F1 scores for selected phrasal categories in addition to the overall F1 score and exact match (on the WSJ development set). While grammar G2 has the highest overall F1 score, its exact match is not particularly high, and it turns out to be the weakest at predicting quantifier phrases (QP). Similarly, the performance of the o</context>
<context position="14095" citStr="Petrov and Klein, 2007" startWordPosition="2284" endWordPosition="2287"> Previous work on parse risk minimization has addressed this problem in two different ways: by changing the objective function, or by constraining 3As a matter of fact, Hinton (2001) mentions syntactic parsing as one of the motivating examples for Products of Experts. Figure 4: Breakdown of different accuracy measures for four randomly selected grammars (G,-G4), as well as a product model (P) that uses those four grammars. Note that no single grammar does well on all measures, while the product model does significantly better on all. the search space (Goodman, 1996; Titov and Henderson, 2006; Petrov and Klein, 2007). The simplest approach is to stick to likelihood as the objective function, but to limit the search space to a set of high quality candidates T : T* = argmax P(T |w) (3) TET Because the likelihood of a given parse tree can be computed exactly for our product model (Eq. 2), the quality of this approximation is only limited by the quality of the candidate list. To generate the candidate list, we produce k-best lists of Viterbi derivations with the efficient algorithm of Huang and Chiang (2005), and erase the subcategory information to obtain parse trees over unsplit categories. We refer to this</context>
<context position="16278" citStr="Petrov and Klein (2007)" startWordPosition="2695" endWordPosition="2698"> NP G1 G2 Legend: 9 NP PRN log G1-score log G2-score |{z } the bill’s chief sponsor expensive than direct Treasury borrowing far more PP unauthorized and expensive ADJP , ADJP ? ? z } |{ -11.7 -12.4 ADJP ADJP 9 ADVP -12.9 -11.5 |{z } G2 far more expensive G1 Figure 5: Grammar G1 has a preference for flat structures, while grammar G2 prefers deeper hierarchical structures. Both grammars therefore make one mistake each on their own. However, the correct parse tree (which uses a flat ADJP in the first slot and a hierarchical NP in the second) scores highest under the product model. search space. Petrov and Klein (2007) present such an objective function, which maximizes the product of expected correct productions r: �T� = argmax T rET These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al. (2005). We extend the algorithm to work over posterior distributions from multiple grammars, by aggregating their expectations into a product. In practice, we use a packed forest representation to approximate the posterior distribution, as in Huang (2008). We refer to this appro</context>
<context position="17875" citStr="Petrov and Klein (2007)" startWordPosition="2955" endWordPosition="2958">res (it missed an NP node in the second part of the sentence). In contrast, the other grammar (G2) favors deeper structures, and therefore introduced a superfluous ADVP node. The product model gives each underlying grammar veto power, and picks the least controversial tree (which is the correct one in this case). Note that a sum model allows the most confident model to dominate the decision, and would chose the incorrect hierarchical ADJP construction here (as one can verify using the provided model scores). To make inference efficient, we can use the same coarse-to-fine pruning techniques as Petrov and Klein (2007). We generate a hierarchy of projected grammars for each individual grammar and parse with each one in sequence. Because only the very last pass requires scores from the different underlying grammars, this computation can be trivially parallelized across multiple CPUs. Additionally, the first (X-Bar) pruning pass needs to be computed only once because it is shared among all grammars. Since the X-Bar pass is the bottleneck of the multipass scheme (using nearly 50% of the total processing time), the overhead of using a product model is quite manageable. It would have also been possible to use A*</context>
<context position="29901" citStr="Petrov and Klein (2007)" startWordPosition="4911" endWordPosition="4914">ndividual grammars seem to learn different sets of constraints, and the product model is able to model them all at once, giving consistent accuracy improvements across all metrics. 5 Conclusions We presented a simple product model that significantly improves parsing accuracies on different domains and languages. Our model leverages multiple automatically learned latent variable grammars, which differ only in the seed of the random number generator used to initialize the EM learning alType all sentences LP LR EX Parser ENGLISH-WSJ This Paper 92.0 91.7 41.9 SINGLE Charniak (2000) 89.9 89.5 37.2 Petrov and Klein (2007) 90.2 90.1 36.7 Carreras et al. (2008) 91.4 90.7 - RE Charniak et al. (2005) 91.8 91.2 44.8 Huang (2008) 92.2 91.2 43.5 SELF Huang and Harper (2009) 91.37 91.57 39.37 McClosky et al. (2006) 92.5 92.1 45.3 COMBO Sagae and Lavie (2006) 93.2 91.0 - Fossum and Knight (2009) 93.2 91.7 - Zhang et al. (2009) 93.3 92.0 - ENGLISH-BROWN This Paper 86.5 86.3 35.8 SING Charniak (2000) 82.9 82.9 31.7 Petrov and Klein (2007) 83.9 83.8 29.6 RE Charniak et al. (2005) 86.1 85.2 36.8 GERMAN This Paper 84.5 84.0 51.2 SING Petrov and Klein (2007) 80.0 80.2 42.4 Petrov and Klein (2008) 80.6 80.8 43.9 Table 2: Fina</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In NAACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Sparse multi-scale grammars for discriminative latent variable parsing.</title>
<date>2008</date>
<booktitle>In EMNLP ’08.</booktitle>
<contexts>
<context position="30472" citStr="Petrov and Klein (2008)" startWordPosition="5016" endWordPosition="5019">arniak (2000) 89.9 89.5 37.2 Petrov and Klein (2007) 90.2 90.1 36.7 Carreras et al. (2008) 91.4 90.7 - RE Charniak et al. (2005) 91.8 91.2 44.8 Huang (2008) 92.2 91.2 43.5 SELF Huang and Harper (2009) 91.37 91.57 39.37 McClosky et al. (2006) 92.5 92.1 45.3 COMBO Sagae and Lavie (2006) 93.2 91.0 - Fossum and Knight (2009) 93.2 91.7 - Zhang et al. (2009) 93.3 92.0 - ENGLISH-BROWN This Paper 86.5 86.3 35.8 SING Charniak (2000) 82.9 82.9 31.7 Petrov and Klein (2007) 83.9 83.8 29.6 RE Charniak et al. (2005) 86.1 85.2 36.8 GERMAN This Paper 84.5 84.0 51.2 SING Petrov and Klein (2007) 80.0 80.2 42.4 Petrov and Klein (2008) 80.6 80.8 43.9 Table 2: Final test set accuracies for English and German. gorithm. As our analysis showed, the grammars vary widely, making very different errors. This is in part due to the fact that EM is used not only for estimating the parameters of the grammar, but also to determine the set of context-free productions that underlie it. Because the resulting data representations are largely independent, they can be easily combined in an unweighted product model. The product model does not require any additional training and is capable of significantly improving the state-of-the-art in pars</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>S. Petrov and D. Klein. 2008. Sparse multi-scale grammars for discriminative latent variable parsing. In EMNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL ’06.</booktitle>
<contexts>
<context position="1850" citStr="Petrov et al., 2006" startWordPosition="282" endWordPosition="285">ere is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized 19 (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and there are no guarantees that it will find the same grammars when initialized from different startin</context>
<context position="5115" citStr="Petrov et al. (2006)" startWordPosition="796" endWordPosition="799">criminative systems which have access to additional non-local features (Charniak and Johnson, 2005; Huang, 2008). 2 Latent Variable Grammars Before giving the details of our model, we briefly review the basic properties of latent variable grammars. Learning latent variable grammars consists of two tasks: (1) determining the data representation (the set of context-free productions to be used in the grammar), and (2) estimating the parameters of the model (the production probabilities). We focus on the randomness introduced by the EM algorithm and refer the reader to Matsuzaki et al. (2005) and Petrov et al. (2006) for a more general introduction. 2.1 Split &amp; Merge Learning Latent variable grammars split the coarse (but observed) grammar categories of a treebank into more fine-grained (but hidden) subcategories, which are better suited for modeling the syntax of natural languages (e.g. NP becomes NP1 through NPO. Accordingly, each grammar production A—*BC over observed categories A,B,C is split into a set of productions A,,—*ByCz over hidden categories A,,,By,Cz. Computing the joint likelihood of the observed parse trees T and sentences w requires summing over all derivations t over split subcategories:</context>
<context position="6806" citStr="Petrov et al. (2006)" startWordPosition="1062" endWordPosition="1065">ction probabilities are perturbed by 1% of random noise. EM is then initialized with this starting point and used to climb the highly non-convex objective function given in Eq. 1. Each splitting step is followed by a merging step, which uses a likelihood ratio test to reverse the least useful half of the splits. Learning proceeds by iterating between those two steps for six rounds. To prevent overfitting, the production probabilities are linearly smoothed by shrinking them towards their common base category. 2.2 EM induced Randomness While the split&amp;merge procedure described above is shown in Petrov et al. (2006) to reduce the variance in final performance, we found after closer examination that there are substantial differences in the patterns learned by the grammars. Since the initialization is not systematically biased in any way, one can obtain different grammars by simply changing the seed of the random number generator. We trained 16 different grammars by initializing the random number generator with seed values 1 through 16, but without biasing the initialization in any other way. Figure 1 shows that the number of subcategories allocated to each observed category varies significantly between th</context>
<context position="8615" citStr="Petrov et al., 2006" startWordPosition="1350" endWordPosition="1353">ile cherry-picking similarities is fairly straight-forward, it is less obvious how to quantify differences. 2Note that despite their variance, the performance is always higher than the one of the lexicalized parser of Charniak (2000). 20 Automatically determined number of subcategories Figure 1: There is large variance in the number of subcategories (error bars correspond to one standard deviation). is only a weak correlation between the accuracies on the two evaluation sets (Pearson coefficient 0.34). This suggests that no single grammar should be preferred over the others. In previous work (Petrov et al., 2006; Petrov and Klein, 2007) the final grammar was chosen based on its performance on a held-out set (section 22), and corresponds to the second best grammar in Figure 3 (because only 8 different grammars were trained). A more detailed error analysis is given in Figure 4, where we show a breakdown of F1 scores for selected phrasal categories in addition to the overall F1 score and exact match (on the WSJ development set). While grammar G2 has the highest overall F1 score, its exact match is not particularly high, and it turns out to be the weakest at predicting quantifier phrases (QP). Similarly,</context>
<context position="20541" citStr="Petrov et al. (2006)" startWordPosition="3378" endWordPosition="3381">model. The individual grammars had parsing accuracies (F1) of 91.2 and 90.7 respectively, and their product (91.7) clearly outperformed their sum (91.3). When more grammars are added, the gap widens even further, and the trends persist independently of whether the models use TREE-LEVEL or CONSTITUENT-LEVEL inference. At least for the case of unweighted combinations, the product distribution seems to be superior. In related work, Zhang et al. (2009) achieve excellent results with a weighted sum model. Using weights learned on a held-out set and rescoring 50- best lists from Charniak (2000) and Petrov et al. (2006), they obtain an F1 score of 91.0 (which they further improve to 91.4 using a voting scheme). We replicated their experiment, but used an unweighted product of the two model scores. Using TREELEVEL inference, we obtained an F1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.6 This is in line with previous work on product models, where weighting has been important when combining heterogenous classifiers (Heskes, 1998), and less important when the classifiers are of similar accuracy (Smith et al., 2005). 5See </context>
<context position="25772" citStr="Petrov et al., 2006" startWordPosition="4227" endWordPosition="4230">ith et al. (2005) interpret Logarithmic Opinion Pools (LOPs) as a smoothing technique. They compare regularizing Conditional Random Fields (CRFs) with Gaussian priors (Lafferty et al., 2001), to training a set of unregularized CRFs over different feature sets and combining them in an LOP. In their experiments, both approaches work comparably well, but their combination, an LOP of regularized CRFs works best. Not too surprisingly, we find this to be the case here as well. The parameters of each latent variable grammar are typically smoothed in a linear fashion to prevent excessive overfitting (Petrov et al., 2006). While all the experiments so far used smoothed grammars, we reran the experiments also with a set of unsmoothed grammars. The individual unsmoothed grammars have on average an 1.2% lower accuracy. Even though our product model is able to increase accuracy by combining multiple grammars, the gap to the smoothed models remains consistent. This suggests that the product model is doing more than just smoothing. In fact, because the product distribution is more peaked, it seems to be doing the opposite of smoothing. 4.5 Final Results Our final model uses an unweighted product of eight grammars tr</context>
<context position="28595" citStr="Petrov et al. (2006)" startWordPosition="4700" endWordPosition="4703">quires a significant effort to build. Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999; Sagae and Lavie, 2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009; Zhang et al., 2009). Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars. It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of Petrov et al. (2006) with the self-trained reranking parser of McClosky et al. (2006). Clearly, replacing the single latent variable grammar with a product of latent variable grammars ought to improve performance. The results on the other two corpora are similar. A product of latent variable grammars very significantly outperforms a single latent variable grammar and sets new standards for the state-of-the-art. We also analyzed the errors of the product models. In addition to the illustrative example in Figure 5, we computed detailed error metrics for different phrasal categories. Figure 4 shows that a product of</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In NAACL ’06.</booktitle>
<contexts>
<context position="3208" citStr="Sagae and Lavie, 2006" startWordPosition="499" endWordPosition="502">ations in the learned refinements. We use these variations to our advantage, and treat grammars learned from different random seeds as independent and equipotent experts. We use a product distribution for joint prediction, which gives more peaked posteriors than a sum, and enforces all constraints of the individual grammars, without the need to tune mixing weights. It should be noted here that our focus is on improving parsing performance using a single underlying grammar class, which is somewhat orthogonal to the issue of parser combination, that has been studied elsewhere in the literature (Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009). In contrast to that line of work, we also do not restrict ourselves to working with kbest output, but work directly with a packed forest representation of the posteriors, much in the spirit of Huang (2008), except that we work with several forests rather than rescoring a single one. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19–27, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics In our experimental section we give empirical answers to some of the rem</context>
<context position="28153" citStr="Sagae and Lavie, 2006" startWordPosition="4623" endWordPosition="4626">free grammar. 25 Techniques like self-training (SELF) and system combinations (COMBO) can further improve parsing accuracies, but are also orthogonal to our work. In particular the COMBO methods seem related to our work, but are very different in their nature. While we use multiple grammars in our work, all grammars are from the same model class for us. In contrast, those methods rely on a diverse set of individual parsers, each of which requires a significant effort to build. Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999; Sagae and Lavie, 2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009; Zhang et al., 2009). Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars. It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of Petrov et al. (2006) with the self-trained reranking parser of McClosky et al. (2006). Clearly, replacing the single latent variable grammar with a product of latent variable gra</context>
<context position="30134" citStr="Sagae and Lavie (2006)" startWordPosition="4954" endWordPosition="4957">t significantly improves parsing accuracies on different domains and languages. Our model leverages multiple automatically learned latent variable grammars, which differ only in the seed of the random number generator used to initialize the EM learning alType all sentences LP LR EX Parser ENGLISH-WSJ This Paper 92.0 91.7 41.9 SINGLE Charniak (2000) 89.9 89.5 37.2 Petrov and Klein (2007) 90.2 90.1 36.7 Carreras et al. (2008) 91.4 90.7 - RE Charniak et al. (2005) 91.8 91.2 44.8 Huang (2008) 92.2 91.2 43.5 SELF Huang and Harper (2009) 91.37 91.57 39.37 McClosky et al. (2006) 92.5 92.1 45.3 COMBO Sagae and Lavie (2006) 93.2 91.0 - Fossum and Knight (2009) 93.2 91.7 - Zhang et al. (2009) 93.3 92.0 - ENGLISH-BROWN This Paper 86.5 86.3 35.8 SING Charniak (2000) 82.9 82.9 31.7 Petrov and Klein (2007) 83.9 83.8 29.6 RE Charniak et al. (2005) 86.1 85.2 36.8 GERMAN This Paper 84.5 84.0 51.2 SING Petrov and Klein (2007) 80.0 80.2 42.4 Petrov and Klein (2008) 80.6 80.8 43.9 Table 2: Final test set accuracies for English and German. gorithm. As our analysis showed, the grammars vary widely, making very different errors. This is in part due to the fact that EM is used not only for estimating the parameters of the gram</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>K. Sagae and A. Lavie. 2006. Parser combination by reparsing. In NAACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima’an</author>
</authors>
<title>Computatoinal complexity of probabilistic disambiguation.</title>
<date>2002</date>
<journal>Grammars.</journal>
<marker>Sima’an, 2002</marker>
<rawString>K. Sima’an. 2002. Computatoinal complexity of probabilistic disambiguation. Grammars.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Skut</author>
<author>B Krenn</author>
<author>T Brants</author>
<author>H Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages. In</title>
<date>1997</date>
<booktitle>ANLP ’97.</booktitle>
<contexts>
<context position="19178" citStr="Skut et al., 1997" startWordPosition="3164" endWordPosition="3167">did not attempt this in the present work. 4 Experiments In our experiments, we follow the standard setups described in Table 1, and use the EVALB tool for computing parsing figures. Unless noted otherwise, we use CONSTITUENT-LEVEL inference. All our experiments are based on the publicly available BerkeleyParser.4 4http://code.google.com/p/berkeleyparser )E(rlw) (4) 23 Training Set Dev. Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 (Marcus et al., 1993) 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al. 1979) ENGLISH-WSJ the data5 the data5 GERMAN Sentences Sentences Sentences (Skut et al., 1997) 1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups. Parsing accuracy on the WSJ development set CONSTITUENT-LEVEL Inference TREE-LEVEL Inference 92.5 92 91.5 91 90.5 4.1 (Weighted) Product vs. (Weighted) Sum A great deal has been written on the topic of products versus sums of probability distributions for joint prediction (Genest and Zidek, 1986; Tax et al., 2000). However, those theoretical results do not apply directly here, because we are using multiple randomly permuted models from the same class, rather models from different classes. To shed some ligh</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997. An annotation scheme for free word order languages. In ANLP ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Smith</author>
<author>M Osborne</author>
</authors>
<title>Diversity in logarithmic opinion pools. Lingvisticae Investigationes.</title>
<date>2007</date>
<contexts>
<context position="23556" citStr="Smith and Osborne, 2007" startWordPosition="3873" endWordPosition="3876">is therefore not too surprising that the two objective functions select the same tree only 41% of the time, even when limited to the same candidate set. Maximizing the 24 expected number of correct productions is superior for F1 score (see the one grammar case in Figure 6). However, as to be expected, likelihood is better for exact match, giving a score of 47.6% vs. 46.8%. 4.3 Systematic Bias Diversity among the underlying models is what gives combined models their strength. One way of increasing diversity is by modifying the feature sets of the individual models (Baldridge and Osborne, 2008; Smith and Osborne, 2007). This approach has the disadvantage that it reduces the performance of the individual models, and is not directly applicable for latent variable grammars because the features are automatically learned. Alternatively, one can introduce diversity by changing the training distribution. Bagging (Breiman, 1996) and Boosting (Freund and Shapire, 1996) fall into this category, but have had limited success for parsing (Henderson and Brill, 2000). Furthermore boosting is impractical here, because it requires training dozens of grammars in sequence. Since training a single grammar takes roughly one day</context>
</contexts>
<marker>Smith, Osborne, 2007</marker>
<rawString>A. Smith and M. Osborne. 2007. Diversity in logarithmic opinion pools. Lingvisticae Investigationes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Smith</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>Logarithmic opinion pools for conditional random fields.</title>
<date>2005</date>
<booktitle>In ACL ’05.</booktitle>
<contexts>
<context position="21134" citStr="Smith et al., 2005" startWordPosition="3478" endWordPosition="3481">) and Petrov et al. (2006), they obtain an F1 score of 91.0 (which they further improve to 91.4 using a voting scheme). We replicated their experiment, but used an unweighted product of the two model scores. Using TREELEVEL inference, we obtained an F1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.6 This is in line with previous work on product models, where weighting has been important when combining heterogenous classifiers (Heskes, 1998), and less important when the classifiers are of similar accuracy (Smith et al., 2005). 5See Gildea (2001) for the exact setup. 6The unweighted sum model, however, underperforms the individual models with an F, score of only 90.3. 1 2 4 8 16 Number of grammars in product model Figure 6: Adding more grammars to the product model improves parsing accuracy, while CONSTITUENT-LEVEL inference gives consistently better results. 4.2 Tree-Level vs. Constituent-Level Inference Figure 6 shows that accuracy increases when more grammars are added to the product model, but levels off after eight grammars. The plot also compares our two inference approximations, and shows that CONSTITUENT-LE</context>
<context position="25169" citStr="Smith et al. (2005)" startWordPosition="4130" endWordPosition="4133">9, which is lower than what we get when training a single grammar on the entire training set. In another experiment, we used a cross-validation setup where individual sections of the treebank were held out. The resulting grammars had parsing accuracies of about 90.5, and the product model was again not able to overcome the lower starting point, despite the potentially larger diversity among the underlying grammars. It appears that any systematic bias that lowers the accuracy of the individual grammars also hurts the final performance of the product model. 4.4 Product Distribution as Smoothing Smith et al. (2005) interpret Logarithmic Opinion Pools (LOPs) as a smoothing technique. They compare regularizing Conditional Random Fields (CRFs) with Gaussian priors (Lafferty et al., 2001), to training a set of unregularized CRFs over different feature sets and combining them in an LOP. In their experiments, both approaches work comparably well, but their combination, an LOP of regularized CRFs works best. Not too surprisingly, we find this to be the case here as well. The parameters of each latent variable grammar are typically smoothed in a linear fashion to prevent excessive overfitting (Petrov et al., 20</context>
</contexts>
<marker>Smith, Cohn, Osborne, 2005</marker>
<rawString>A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic opinion pools for conditional random fields. In ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>J Tsujii</author>
</authors>
<title>Sequential labeling with latent variables: An exact inference algorithm and its efficient approximation.</title>
<date>2009</date>
<booktitle>In EACL ’09.</booktitle>
<contexts>
<context position="18551" citStr="Sun and Tsujii, 2009" startWordPosition="3068" endWordPosition="3071">h individual grammar and parse with each one in sequence. Because only the very last pass requires scores from the different underlying grammars, this computation can be trivially parallelized across multiple CPUs. Additionally, the first (X-Bar) pruning pass needs to be computed only once because it is shared among all grammars. Since the X-Bar pass is the bottleneck of the multipass scheme (using nearly 50% of the total processing time), the overhead of using a product model is quite manageable. It would have also been possible to use A*-search for factored models (Klein and Manning, 2003a; Sun and Tsujii, 2009), but we did not attempt this in the present work. 4 Experiments In our experiments, we follow the standard setups described in Table 1, and use the EVALB tool for computing parsing figures. Unless noted otherwise, we use CONSTITUENT-LEVEL inference. All our experiments are based on the publicly available BerkeleyParser.4 4http://code.google.com/p/berkeleyparser )E(rlw) (4) 23 Training Set Dev. Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 (Marcus et al., 1993) 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al. 1979) ENGLISH-WSJ the data5 the data5 GERMAN Sentences Sentences Se</context>
</contexts>
<marker>Sun, Tsujii, 2009</marker>
<rawString>X. Sun and J. Tsujii. 2009. Sequential labeling with latent variables: An exact inference algorithm and its efficient approximation. In EACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tax</author>
<author>M Van Breukelen</author>
<author>R Duin</author>
<author>J Kittler</author>
</authors>
<title>Combining multiple classifiers by averaging or by multiplying? Pattern Recognition.</title>
<date>2000</date>
<marker>Tax, Van Breukelen, Duin, Kittler, 2000</marker>
<rawString>D. Tax, M. Van Breukelen, R. Duin, and J. Kittler. 2000. Combining multiple classifiers by averaging or by multiplying? Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>Loss minimization in parse reranking.</title>
<date>2006</date>
<booktitle>In EMNLP ’06.</booktitle>
<contexts>
<context position="14070" citStr="Titov and Henderson, 2006" startWordPosition="2279" endWordPosition="2283">e out the latent variables. Previous work on parse risk minimization has addressed this problem in two different ways: by changing the objective function, or by constraining 3As a matter of fact, Hinton (2001) mentions syntactic parsing as one of the motivating examples for Products of Experts. Figure 4: Breakdown of different accuracy measures for four randomly selected grammars (G,-G4), as well as a product model (P) that uses those four grammars. Note that no single grammar does well on all measures, while the product model does significantly better on all. the search space (Goodman, 1996; Titov and Henderson, 2006; Petrov and Klein, 2007). The simplest approach is to stick to likelihood as the objective function, but to limit the search space to a set of high quality candidates T : T* = argmax P(T |w) (3) TET Because the likelihood of a given parse tree can be computed exactly for our product model (Eq. 2), the quality of this approximation is only limited by the quality of the candidate list. To generate the candidate list, we produce k-best lists of Viterbi derivations with the efficient algorithm of Huang and Chiang (2005), and erase the subcategory information to obtain parse trees over unsplit cat</context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>I. Titov and J. Henderson. 2006. Loss minimization in parse reranking. In EMNLP ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Xu</author>
<author>F Jelinek</author>
</authors>
<title>Random forests in language modeling.</title>
<date>2004</date>
<booktitle>In EMNLP ’04.</booktitle>
<contexts>
<context position="12761" citStr="Xu and Jelinek (2004)" startWordPosition="2075" endWordPosition="2078">and weak correlation suggest that no single grammar is to be preferred. but from a different, randomly chosen starting point. To emphasize, we do not introduce any systematic bias (but see Section 4.3 for some experiments), or attempt to train the models to be maximally different (Hinton, 2002) – we simply train a random collection of grammars by varying the random seed used for initialization. We found in our experiments that the randomness provided by EM is sufficient to achieve diversity among the individual grammars, and gives results that are as good as more involved training procedures. Xu and Jelinek (2004) made a similar observation when learning random forests for language modeling. Our model is reminiscent of Logarithmic Opinion Pools (Bordley, 1982) and Products of Experts (Hinton, 2001).3 However, because we believe that none of the underlying grammars should be favored, we deliberately do not use any combination weights. 3.2 Inference Computing the most likely parse tree is intractable for latent variable grammars (Sima’an, 2002), and therefore also for our product model. This is because there are exponentially many derivations over split subcategories that correspond to a single parse tre</context>
</contexts>
<marker>Xu, Jelinek, 2004</marker>
<rawString>P. Xu and F. Jelinek. 2004. Random forests in language modeling. In EMNLP ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>M Zhang</author>
<author>C L Tan</author>
<author>H Li</author>
</authors>
<title>K-best combination of syntactic parsers.</title>
<date>2009</date>
<booktitle>In EMNLP ’09.</booktitle>
<contexts>
<context position="3254" citStr="Zhang et al., 2009" startWordPosition="507" endWordPosition="510">variations to our advantage, and treat grammars learned from different random seeds as independent and equipotent experts. We use a product distribution for joint prediction, which gives more peaked posteriors than a sum, and enforces all constraints of the individual grammars, without the need to tune mixing weights. It should be noted here that our focus is on improving parsing performance using a single underlying grammar class, which is somewhat orthogonal to the issue of parser combination, that has been studied elsewhere in the literature (Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009). In contrast to that line of work, we also do not restrict ourselves to working with kbest output, but work directly with a packed forest representation of the posteriors, much in the spirit of Huang (2008), except that we work with several forests rather than rescoring a single one. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19–27, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics In our experimental section we give empirical answers to some of the remaining theoretical questions. We address the q</context>
<context position="20373" citStr="Zhang et al. (2009)" startWordPosition="3349" endWordPosition="3352">asses. To shed some light on this issue, we addressed the question empirically, and combined two grammars into an unweighted product model, and also an unweighted sum model. The individual grammars had parsing accuracies (F1) of 91.2 and 90.7 respectively, and their product (91.7) clearly outperformed their sum (91.3). When more grammars are added, the gap widens even further, and the trends persist independently of whether the models use TREE-LEVEL or CONSTITUENT-LEVEL inference. At least for the case of unweighted combinations, the product distribution seems to be superior. In related work, Zhang et al. (2009) achieve excellent results with a weighted sum model. Using weights learned on a held-out set and rescoring 50- best lists from Charniak (2000) and Petrov et al. (2006), they obtain an F1 score of 91.0 (which they further improve to 91.4 using a voting scheme). We replicated their experiment, but used an unweighted product of the two model scores. Using TREELEVEL inference, we obtained an F1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.6 This is in line with previous work on product models, where weightin</context>
<context position="28287" citStr="Zhang et al., 2009" startWordPosition="4645" endWordPosition="4648">o orthogonal to our work. In particular the COMBO methods seem related to our work, but are very different in their nature. While we use multiple grammars in our work, all grammars are from the same model class for us. In contrast, those methods rely on a diverse set of individual parsers, each of which requires a significant effort to build. Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999; Sagae and Lavie, 2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009; Zhang et al., 2009). Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars. It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of Petrov et al. (2006) with the self-trained reranking parser of McClosky et al. (2006). Clearly, replacing the single latent variable grammar with a product of latent variable grammars ought to improve performance. The results on the other two corpora are similar. A product of latent variable grammars very signi</context>
<context position="30203" citStr="Zhang et al. (2009)" startWordPosition="4968" endWordPosition="4971">guages. Our model leverages multiple automatically learned latent variable grammars, which differ only in the seed of the random number generator used to initialize the EM learning alType all sentences LP LR EX Parser ENGLISH-WSJ This Paper 92.0 91.7 41.9 SINGLE Charniak (2000) 89.9 89.5 37.2 Petrov and Klein (2007) 90.2 90.1 36.7 Carreras et al. (2008) 91.4 90.7 - RE Charniak et al. (2005) 91.8 91.2 44.8 Huang (2008) 92.2 91.2 43.5 SELF Huang and Harper (2009) 91.37 91.57 39.37 McClosky et al. (2006) 92.5 92.1 45.3 COMBO Sagae and Lavie (2006) 93.2 91.0 - Fossum and Knight (2009) 93.2 91.7 - Zhang et al. (2009) 93.3 92.0 - ENGLISH-BROWN This Paper 86.5 86.3 35.8 SING Charniak (2000) 82.9 82.9 31.7 Petrov and Klein (2007) 83.9 83.8 29.6 RE Charniak et al. (2005) 86.1 85.2 36.8 GERMAN This Paper 84.5 84.0 51.2 SING Petrov and Klein (2007) 80.0 80.2 42.4 Petrov and Klein (2008) 80.6 80.8 43.9 Table 2: Final test set accuracies for English and German. gorithm. As our analysis showed, the grammars vary widely, making very different errors. This is in part due to the fact that EM is used not only for estimating the parameters of the grammar, but also to determine the set of context-free productions that u</context>
</contexts>
<marker>Zhang, Zhang, Tan, Li, 2009</marker>
<rawString>H. Zhang, M. Zhang, C. L. Tan, and H. Li. 2009. K-best combination of syntactic parsers. In EMNLP ’09.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>