<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.491288">
TOWARDS A DICTIONARY SUPPORT ENVIRONMENT
FOR REAL TIME PARSING
</note>
<author confidence="0.574771">
Hiyan Alshawi, Bran Boguraev, Ted Briscoe
</author>
<affiliation confidence="0.952528">
Computer Laboratory, Cambridge University
</affiliation>
<address confidence="0.7417115">
Corn Exchange Street
Cambridge CB2 3QG, U.K.
</address>
<email confidence="0.580078">
ABSTRACT
</email>
<bodyText confidence="0.999962666666667">
In this article we describe research on the
development of large dictionaries for natural
language processing. We detail the development of a
dictionary support environment linking a
restructrured version of the Longman Dictionary of
Contemporary English to natural language
processing systems. We describe the process of
restructuring the information in the dictionary and
our use of the Longman grammar code system to
construct dictionary entries for the PATR-H parsing
system and our use of the Longman word definitions
for automated word sense classification.
</bodyText>
<sectionHeader confidence="0.989817" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.952580164383562">
Recent developments in linguistics, and
especially on grammatical theory - for example,
Generalised Phrase Structure Grammar (GPSG)
(Gazdar et al., In Press), Lexical Functional
Grammar (LFG) (Kaplan &amp; Bresnan, 1982) - and on
natural language parsing frameworks - for example,
Functional Unification Grammar (FUG) (Kay,
1984a), PATR-H (Shieber, 1984) - make it feasible to
consider the implementation of efficient systems for
the syntactic analysis of substantial fragments of
natural language. These developments also
demonstrate that if natural language processing
systems are to be able to handle the grammatical and
logical idiosyncracies of individual lexical items
elegantly and efficiently, then the lexicon must be a
central component of the parsing system. Real-time
parsing imposes stringent requirements on a
dictionary support environment; at the very least it
must allow frequent and rapid access to the
information in the dictionary via the dictionary head
words.
The idea of using the machine-readable
source of a published dictionary has occurred to a
wide range of researchers - for spelling correction,
lexical analysis, thesaurus construction, machine-
translation, to name but a few applications - very few
however have used such a dictionary to support a
natural language parsing system. Most of the work
on automated dictionaries has concentrated on
extracting lexical or other information in, essentially,
batch processing (eg. Amsler, 1981; Walker &amp;
Amsler, 1983), or on developing dictionary servers for
office automation systems (Kay, 1984b). Few parsing
• systems have substantial lexicons and even those
which employ very comprehensive grammars (eg.
Robinson, 1982; Bobrow, 1978) consult relatively
small lexicons, typically generated by hand. Two
exceptions to this generalisation are the Linguistic
String Project (Sager, 1981) and the Epistle Project
(Heidorn et al., 1982); the former employs a
dictionary of less than 10,000 words, most of which
are specialist medical terms, the latter has well over
100,000 entries, gathered from machine-readable
sources, however, their grammar formalism and the
limited grammatical information supplied by the
dictionary make this achievement, though
impressive, theoretically less interesting.
We chose to employ the Longman Dictionary
of Contemporary English (Procter 1978, henceforth
LDOCE) as the machine-readable source for our
dictionary environment because this dictionary has
several properties which make it uniquely
appropriate for use as the core knowledge base of a
natural language processing system. Most prominent
among these are the rich grammatical
subcategorisations of the 60,000 entries, the large
amount of information concerning phrasal verbs,
noun compounds and idioms, the individual subject,
collocational and semantic codes for the entries and
the consistent use of a controlled &apos;core&apos; vocabulary in
defining the words throughout the dictionary.
(Michiels (1982) gives further description and
discussion of LDOCE from the perspective of natural
language processing.)
The problem of utilising LDOCE in natural
language processing falls into two areas. Firstly, we
must provide a dictionary environment which links
the dictionary to our existing natural language
processing systems in the appropriate fashion and
secondly, we must restructure the information in the
dictionary in such a way that these systems are able
to utilise it effectively. These two tasks form the
subject matter of the next two sections.
</bodyText>
<page confidence="0.997843">
171
</page>
<sectionHeader confidence="0.932341" genericHeader="method">
THE ACCESS ENVIRONMENT
</sectionHeader>
<bodyText confidence="0.999991294117647">
To link the machine-readable version of
LDOCE to existing natural language processing
systems we need to provide fast access from Lisp to
data held in secondary storage. Furthermore, the
complexity of the data structures stored on disc
should not be constrained in any way by the method
of access, because we have little idea what form the
restructured dictionary may eventually take.
Our first task in providing an environment
was therefore the creation of a &apos;lispified&apos; version of the
machine-readable LDOCE file. A batch program
written in a general editing facility was used to
convert the entrire LDOCE typesetting tape into a
sequence of Lisp s-expressions without any loss of
generality or information. Figure 1 illustrates part of
an entry as it appears in the published dictionary, on
the typesetting tape and after lispification.
</bodyText>
<equation confidence="0.719997866666667">
rivet2 u 1 [T1;X91 to cause to fasten with RIVETs1:...
28289801&lt;R0154300&lt;rivet
28289902&lt;02«
28290005&lt;v&lt;
28290107&lt;0100&lt;71;X9&lt;NAZV&lt; H XS
28290208&lt;to cause to fasten with
28290318&lt;(*CA)RIVET(*C8)(*46)s(*44)(*8A}:
((rivet)
(1 R0154300!&lt; rivet)
(2 2 !&lt; !&lt;)
(5 v !&lt;)
(7 100 ! &lt; T1 !; X9 ! &lt; NAZV ! &lt;
(8 to cause to fasten with
*CA RIVET *CB *46 s *44 *8A :
))
</equation>
<figureCaption confidence="0.873193">
Figure 1
</figureCaption>
<bodyText confidence="0.999726791666667">
This still leaves the problem of access, from
Lisp, to the dictionary entry s-expressions held on
secondary storage. Ad hoc solutions, such as
sequential scanning of files on disc or extracting
subsets of such files which will fit in main memory
are not adequate as an efficient interface to a parser.
(Exactly the same problem would occur if our natural
language systems were implemented in Prolog, since
the Prolog &apos;database facility&apos;, refers to the knowledge
base that Prolog maintains in main memory.) In
principle, given that the dictionary is now in a Lisp-
readable format, a powerful virtual memory system
might be able to manage access to the internal Lisp
structures resulting from reading the entire
dictionary; we have, however, adopted an alternative
solution as outlined below.
We have implemented an efficient dictionary
access system which services requests for s-
expression entries made by client Cambridge Lisp
programs. The lispified file was sorted and converted
into a random access file together with indexing
information from which the disc addresses of
dictionary entries for words and compounds can be
recovered. Standard database indexing techniques
were used for this purpose. The current access system
is implemented in the programming language C. It
runs under UNIX and makes use of the random file
access and inter-process communication facilities
provided by this operating system. (UNIX is a Trade
Mark of Bell Laboratories.) To the Lisp programmer,
the creation of a dictionary process and subsequent
requests for information from the dictionary appear
simply as Lisp function calls.
We have provided for access to the dictionary
via head words and the first words of compounds and
phrasal verbs, either through the spelling or
pronunciation fields. Random selection of dictionary
entries is also provided to allow the testing of
software on an unbiased sample. This access is
sufficient to support our current parsing
requirements but could be supplemented with the
addition of further indexing files if required.
Eventually access to dictionary entries will need to be
considerably more intelligent and flexible than a
simple left-to-right sequential pass through the
lexical items to be parsed, if our processing systems
are to make full use of the information concerning
compounds and idioms stored in LDOCE.
</bodyText>
<sectionHeader confidence="0.99497" genericHeader="method">
RESTRUCTURING THE DICTIONARY
</sectionHeader>
<bodyText confidence="0.999899260869565">
The lispified LDOCE file retains the broad
structure of the typesetting tape and divides each
entry into a number of fields - head word,
pronunciation, grammar codes, definitions, examples
and so forth. However, each of these fields requires
further decoding and restructuring to provide client
programs with easy access to the information they
require (Calzolari (1984) discusses this need). For this
purpose the formatting codes on the typesetting tape
are crucial since they provide clues to the correct
structure of this information. For example, word
senses are largely defined in terms of the 2000 word
core vocabulary, however, in some cases other words
(themselves defined elsewhere in terms of this
vocabulary) are used. These words always appear in
small capitals and can therefore be recognised
because they will be preceded by a font change control
character. In Figure 1 above the definition of &amp;quot;rivet&amp;quot;
includes the noun definition of &amp;quot;RIVETI&amp;quot;, as signalled
by the font change and the numerical superscript
which indicates that it is the noun entry homograph;
additional notation exists for word senses within
homograhps. On the typesetting tape, font control
</bodyText>
<page confidence="0.989262">
172
</page>
<bodyText confidence="0.999980454545455">
characters are indicated within curly brackets by
hexadecimal numbers. In addition, there is a further
complication because this sense is used in the plural
and the plural morpheme must be removed before
&amp;quot;RIVET&amp;quot; can be associated with a dictionary entry.
However, the restructuring program can achieve this
because such morphology is always italicised, so the
program knows that in the context of non-core
vocabulary items the italic font control character
signals the occurrence of a morphological variant of a
LDOCE head entry.
A. suite of programs to unscramble and
restructure all the fields in LDOCE entries has been
written which is capable of decoding all the fields
except those providing cross-reference and usage
information for complete homographs. Figure 2
illustrates a simple lexical entry before and after the
application of these programs.
The development of the restructuring
programs is a non-trivial task because the
organisation of information on the typesetting tape
presupposes its visual presentation, and the ability of
human users to apply common sense, utilise basic
morphological knowledge, ignore minor notational
inconsistencies, and so forth. To provide a test-bed for
these programs we have implemented an interactive
dictionary browser capable of displaying the
restructured information in a variety of ways and
representing it in perspicuous and expanded form.
To illustrate the problems involved in the
restructuring process we will discuss the
restructuring of the grammar codes in some detail,
however, the reader should bear in mind that this
represents only one comparatively constrained field
of an LDOCE entry and therefore, a small proportion
of the overall restructuring task. Figure 3 illlustrates
the grammar code field for the third word sense of the
verb &amp;quot;believe&amp;quot; as it appears in the published
dictionary, on the typesetting tape and after
restructuring.
Multiple grammar codes are elided and
abbreviated in the dictionary to save space and
restructuring must reconstruct the full set of codes.
This can be done with knowledge of the syntax of the
grammar code system and the significance of
punctuation and font changes. For example, semi-
colons indicate concatenated codes and commas
indicate concatenated, elided codes. However,
discovering the syntax of the system is difficult since
no explicit description is available from Longman and
the code is geared more towards visual presentation
than formal precision; for example, words which
qualify codes, such as &amp;quot;to be&amp;quot; in Figure 3, appear in
italics and therefore, will be preceded by the font
control character &apos;*45&apos;. But sometimes the thin space
</bodyText>
<equation confidence="0.989735">
((pair)
(1 P0008800 &lt; pair)
(2 1 &lt; &lt;)
(3 peER)
(7 200 &lt; C9 !, esp !. *46 of &lt; CD--&lt; ----1---Y)
</equation>
<bodyText confidence="0.880661692307692">
(8 *45 a *442 things that are alike or of the same
kind !, and are usu I. used together: *46 a pair of
shoes !I a beautiful pair of legs *44 *63 compare
*CA COUPLE *C8 *88 *45 b *442 playing cards of the
same value but of different *CA SUIT *CB *46 s *84
*44 (3) : *46 a pair of kings)
(7 300 &lt; GC &lt; &lt; --S-U---Y)
(8 *45 a *442 people closely connected : *46 a pair
of dancers *45 b *CA COUPLE *CB *88 *44(2)
(esp !. in the phr !. *45 the happy pair *44) *45 c
*46 sl *442 people closely connected who cause
annoyance or displeasure: *46 You !&apos;re a fine pair
coming as late as this !!)
</bodyText>
<figure confidence="0.956702">
(Word-sense (Number 2)
((Sub-definition
(Item a) (Label NIL)
(Definition 2 things that are alike or of the same
kind I. and are usually used together)
((Example NIL (a pair of shoes))
(Example NIL (a beautiful pair of legs)))
(Cross-reference
compare-with
(Ldoce-entry (Lexical COUPLE)
(Morphology NIL)
(Homograph-number 2)
(Word-sense-number NIL)))
(Sub-definition
(Item b) (Label NIL)
</figure>
<table confidence="0.833420115384615">
(Definition 2 playing cards of the same value
but of different
(Ldoce-entry (SUIT)
(Morphology s)
(Homograph-number 1)
(Word-sense-number 3))
((Example NIL (a pair of kings))))))
(Word-sense (Number 3)
((Sub-definition
(Item a) (Label NIL)
(Definition 2 people closely connected)
((Example NIL (a pair of dancers))))
(Sub-definition
(Item b) (Label NIL)
(Definition
(Ldoce-entry (Lexical COUPLE)
(Morphology NIL)
(Homograph-number 2)
(Word-sense-number 2))
(Gloss: especially in the phrase the happy pair )))
(Sub-definition
(Item c) (Label slang)
(Definition 2 people closely connected who
cause annoyance or displeasure)
((Example NIL
(You! &apos;re a fine pair coming as late as this!))))))
</table>
<figureCaption confidence="0.871003">
Figure 2
</figureCaption>
<page confidence="0.935178">
173
</page>
<figure confidence="0.474054111111111">
believe v 3 [T5a,b;V3:X (to be) 1, (to be)?)
(7 300 !&lt; T5a !, b !; V3 !; X (*46 to be *44)
1 !, (*46 to be *44) 7 I&lt;
word sense 3
head: X7x
head: X1x
head: V3
head: T5a
head: T5b
</figure>
<figureCaption confidence="0.906164">
Figure 3
</figureCaption>
<bodyText confidence="0.991949714285714">
control character &apos;*64&apos; also appears; the insertion of
this code is based solely on visual criteria, rather
than the informational structure of the dictionary.
Similarly, choice of font can be varied for reasons of
appearance and occasionally information normally
associated with one field of an entry is shifted into
another to create a more compact or elegant printed
entry. In addition to the &apos;noise&apos; generated by the fact
that we are working with a typesetting tape geared to
visual presentation, rather than a database, there are
errors in the use of the grammar code system; for
example, Figure 4 illustrates the code for the first
sense of the noun &amp;quot;promise&amp;quot;.
promise n 1 IC (of),C3,5; under + tJJ
</bodyText>
<figureCaption confidence="0.692638">
Figure 4
</figureCaption>
<bodyText confidence="0.999046">
The occurrence of the full code &amp;quot;C3&amp;quot; between
commas is incorrect because commas are clearly
intended to delimit sequences of elided codes. This
type of error arises because grammatical codes are
constructed by hand and no automatic checking
procedure is attempted (see Michiels, 1982). Finally,
there are errors or omissions in the use of the codes;
for example, Figure 5 illustrates the grammar codes
for the listed senses of the verb &amp;quot;upset&amp;quot;.
</bodyText>
<table confidence="0.974336333333333">
upset: head Ti
for cat = v head I
word sense 1 head Ti
word sense 2 head Ti
word sense 3
word sense 4
</table>
<figureCaption confidence="0.859348">
Figure 5
</figureCaption>
<bodyText confidence="0.999837642857143">
These codes correspond to the simple
transitive and intransitive uses of &amp;quot;upset&amp;quot;; no codes
are given for the uses of &amp;quot;upset&amp;quot; with sentential
complements. Clearly, the restructuring programs
cannot correct this last type of error, however, we
have developed a system which is sufficiently robust
to handle the other problems described above. Rather
than apply these programs to the dictionary and
create a new restructured file, they are applied on a
demand basis, as required by the dictionary browser
or the other client programs described in the next
section; this allows us to continue to refine the
restructuring programs incrementally as further
problems emerge.
</bodyText>
<sectionHeader confidence="0.987968" genericHeader="method">
USING THE DICTIONARY
</sectionHeader>
<bodyText confidence="0.996499425">
Once the information in LDOCE has been
restructured into a format suitable for accessing by
client programs, it still remains to be shown that this
information is of use to our natural language
processing systems. In this section, we describe the
use that we have made of the grammar codes and
word sense definitions.
Grammar codes
The grammar code system used in LDOCE is
based quite closely on the descriptive grammatical
framework of Quirk et al. (1972). The codes are
doubly articulated; capital letters represent the
grammatical relations which hold between a verb and
its arguments and numbers represent
subcategorisation frames which a verb can appear in.
(The small letters which appear with some codes
represent a variety of less important information, for
example, whether a sentential complement will take
an obligatory or optional complementiser.) Most of
the subcategorisation frames are specified by
syntactic category, but some are very ill-specified; for
instance, 9 is defined as &amp;quot;needs a descriptive word or
phrase&amp;quot;. In practice anything functioning as an
adverbial will satisfy this code, when attached to a
verb. The criteria for assignment of capital letters to
verbs is not made explicit, but is influenced by the
syntactic and semantic relations which hold between
the verb and its arguments; for example, 15, L5 and
T5 can all be assigned to verbs which take a NP
subject and a sentential complement, but 15 will only
be assigned if there is a fairly close semantic link
between the two arguments and T5 will be used in
preference to 15 if the verb is felt to be semantically
two place rather than one place, such as &amp;quot;know&amp;quot;
versus &amp;quot;appear&amp;quot;. On the other hand, both &amp;quot;believe&amp;quot;
and &amp;quot;promise&amp;quot; are assigned V3 which means they
take a NP object and infinitival complement, yet
there is a similar semantic distinction to be made
between the two verbs; so the criteria for the
assignment of the V code seem to be syntactic.
</bodyText>
<page confidence="0.99653">
174
</page>
<bodyText confidence="0.99960575">
The parsing systems we are interested in all
employ grammars which carefully distinguish
syntactic and semantic information of this kind,
therefore, if the information provided by the
Longman grammar code system is to be of use we
need to be able to separate out this information and
map it into the representation scheme used for lexical
entries used by one of these parsing systems. To
demonstrate that this is possible we have
implemented a system which constructs dictionary
entries for the PATH-II system (Shieber, 1984 and
references therein). PATEL-II was chosen because the
system has been reimplemented in Cambridge and
was therefore, available; however, the task would be
nearly identical if we were constructing entries for a
system based on GPSG, FUG or LFG.
The PATR-II parsing system operates by
unifying directed graphs (DGs); the completed parse
for a sentence will be the result of successively
unifying the DGs associated with the words and
constituents of the sentence according to the rules of
the grammar. The DG for a lexical item is constructed
from its lexical entry which will consist of a set of
templates for each syntactically distinct variant.
Templates are themselves abbreviations for
unifications which define the DG. For example, the
basic entry and associated DG for the verb &amp;quot;storm&amp;quot;
are illustrated in Figure 6.
</bodyText>
<table confidence="0.8545182">
word storm:
word sense 4 &lt;head trans sense-no&gt; = 1
V Takes NP Dyadic
worddag storm:
[cat: v
head: [aux: false
trans: [pred: storm
sense-no: 1
arg1: &lt;DG15&gt; = a
arg2: &lt;DG16&gt; =
syncat: [first : [cat: NP
head: [trans: &lt; DG15 &gt;a
rest: [first: (cat: NP
head: [trans: &lt;DG16&gt;j]
rest: [first: lambda]]
</table>
<figureCaption confidence="0.918753">
Figure 6
</figureCaption>
<bodyText confidence="0.999847290322581">
The template Dyadic defines the way in
which the syntactic arguments to the verb contribute
to the logical structure of the sentence; thus, the
information that &amp;quot;storm&amp;quot; is transitive and that it is
logically a two-place predicate is kept distinct.
Consequently, the system can represent the fact that
some verbs which take two syntactic arguments are
nevertheless logically one-place predicates.
It is not possible to automatically construct
PATR-II dictionary entries for verbs just by mapping
one full grammar code from the restructured LDOCE
entry into a set of templates. However, it turns out
that if we compare the full set of grammar codes
associated with a particular sense of a verb, following
a suggestion of Michiels (1982), then we can construct
the correct set of templates. That is, we can extract all
the information that PATH-II requires concerning
the subcategorisation and semantic type of verbs. For
example, as we saw above, &amp;quot;believe&amp;quot; under one sense
is assigned the codes T5 and V3; the presence of the
T5 code tells us that &amp;quot;believe&amp;quot; is a &apos;raising-to-object&apos;
verb and logically two-place under the V3
interpretation. On the other hand, &amp;quot;persuade&amp;quot; is only
assigned the V3 code, so we can conclude that it is
three-place with object control of the infinitive. By
systematically exploiting the collocation of different
codes in the same field, it is possible to distinguish
the raising, equi and control properties of verbs. In
effect, we are utilising what was seen as the
transformational consequences of the semantic type
of the verb within classical generative grammar.
</bodyText>
<table confidence="0.964002">
word marry:
word sense 4 &lt; head trans sense-no&gt; = 1
V Takes NP Dyadic
word sense 4 &lt; head trans sense-no&gt; = 1
V TakesIntransNP Monadic
word sense 4 &lt; head trans sense-no&gt; = 2
V TakesNP Dyadic
word sense 4 &lt;head trans sense-no&gt; = 3
V TakesNPPP Triadic
word persuade:
word sense 4 &lt; head trans sense-no&gt; = 1
V Takes NP Dyadic
word sense 4 &lt;head trans sense-no&gt; = 1
V TakesNPSbar Triadic
word sense 4 &lt;head trans sense-no&gt; = 2
V TakesNP Dyadic
word sense 4 &lt;head trans sense-no&gt; = 2
V TakesNPInf ObjectControl Triadic
</table>
<figureCaption confidence="0.998699">
Figure 7
</figureCaption>
<bodyText confidence="0.999344666666667">
The modified version of PATR-II that we
have implemented contains a small dictionary and
constructs entries automatically from restructured
LDOCE entries for most verbs that it encounters. As
well as carrying over the grammar codes, PATR-II
has been modified to represent the word sense
numbers which particular grammar codes are
associated with. Thus, the analysis of a sentence by
the PATR-H system now represents its syntactic and
logical structure and the particular senses of the
words (as defined in LDOCE) which are relevant in
the grammatical context. Figure 7 illustrates the
</bodyText>
<page confidence="0.997216">
175
</page>
<bodyText confidence="0.9959296">
dictionary entries for &amp;quot;marry&amp;quot; and &amp;quot;persuade&amp;quot;
constructed by the system from LDOCE.
In Figure 8 we show one of the two analyses
produced by PATR-II for a sentence containing these
two verbs. The other analysis is syntactically and
</bodyText>
<tableCaption confidence="0.832095333333333">
parse: uther might persuade gwen to marry cornwall
analysis 1:
[cat: SENTENCE
head: [form: finite
agr: [per: p3 num: sg]
aux: true
trans: [pred: possible
sense-no: 1
arg1: (pred: persuade
sense-no: 2
arg1: [ref: uther sense-no: 1]
arg2: (ref: gwen sense-no: 11
arg3: [pred: marry
sense-no: 2
arg1: [ref: gwen
sense-no 1]
arg2: [ref: cornwal I
sense-no: 1111]]]
</tableCaption>
<figureCaption confidence="0.954047">
Figure 8
</figureCaption>
<bodyText confidence="0.980832682926829">
logically identical but incorporates sense two of
&amp;quot;marry&amp;quot;. Thus, the system knows that further
semantic analysis need only consider sense two of
&amp;quot;persuade&amp;quot; and sense one and two of &amp;quot;marry&amp;quot;; this
rules out one further sense of each, as defined in
LDOCE.
Word sense definitions
The automatic analysis of the definition
texts of LDOCE entries is aimed at making the
semantic information on word senses encoded in
these definitions available to natural language
processing systems. LDOCE is particularly suitable
to such an endeavour because of the 2000 word
restricted definition vocabulary, and in fact only
&apos;central&apos; senses of the words in this restricted
vocabulary occur in definition texts. It is thus
possible to process the LDOCE definition of a word
sense in order to produce some representation of the
sense definition in terms of senses of words in the
restricted vocabulary. This representation could then
be combined, for the benefit of the client language
processing system, with the other semantic
information encoded for word senses in LDOCE; in
particular the &apos;box codes&apos; that give simple selectional
restrictions and the &apos;subject codes&apos; that classify senses
according to subject area usage. (These are not in the
published version of the dictionary, but are available
on the tape.)
There are various possibilities for the form of
the output resulting from processing a definition. The
current experimental system produces output that is
convenient for incorporating new word senses into a
knowledge base organized around classification
hierarchies, as discussed shortly. However, the
system allows the form of output structures to be
specified in a flexible way. Alternative possible
output representations would be meaning postulates
and definitions based on semantic primitives.
As mentioned above, the implemented
experimental system is intended to enable the
classification (see e.g. Schmolze, 1983) of new word
senses with respect to a hierar-chically organized
knowledge base, for example the one described in
Alshawi (1983). The proposal being made here is that
the analysis of dictionary definitions can provide
enough information to link a new word sense to
domain knowledge already encoded in the knowledge
base of a limited domain natural language
application such as a database query system. Given a
hand-coded hierarchical organization of the relevant
(central) senses of the definition vocabulary together
with a classification of the relationships between
these senses and domain specific concepts, the
LDOCE definition of a new word sense often contains
enough information to enable the inclusion of the
word sense in this classification, and hence allow the
new word to be handled correctly when performing
the application task.
The information necessary for this process is
present, in the case of nouns, as restrictions on the
classes which subsume the new type of object, its
properties, and predications often expressed by
relative clauses. There are also a number of more
specific predications (such as &amp;quot;purpose&amp;quot; in the
example given below) that are very common in
dictionary definitions, and have immediate utility for
the classification of the relationships between word
senses. Similarly, the information relevant to the
classification of verb and adjective senses present in
sense definitions includes the classes of predicates
that subsume the new predicate corresponding to the
word sense, restrictions on the arguments of this
predicate, and words indicating opposites as is
frequently the case with adjective definitions.
Figure 9 below shows the output produced by
the implemented definition analyser for lispified
LDOCE definitions of one of the noun senses and one
of the verb senses of the word &amp;quot;launch&amp;quot;. It should be
emphasized that the output produced is not regarded
as a formal language, but rather as an intermediate
data structure containing information relevant to the
classification process.
</bodyText>
<page confidence="0.991599">
176
</page>
<figure confidence="0.963014692307692">
(launch)
(a large usu . motor-driven boat used for carrying people
on rivers , lakes , harbours, etc.)
((CLASS BOAT) (PROPERTIES (LARGE))
(PURPOSE
(PREDICATION (CLASS CARRY) (OBJECT PEOPLE))))
(to send (a modern weapon or instrument) into the sky or
space by means of scientific explosive apparatus)
((CLASS SEND)
(OBJECT
((CLASS INSTRUMENT) (OTHER-CLASSES (WEAPON))
(PROPERTIES (MODERN))))
(ADVERBIAL ((CASE INTO) (FILLER (CLASS SKY)))))
</figure>
<figureCaption confidence="0.997627">
Figure 9
</figureCaption>
<bodyText confidence="0.999994">
The analysis process is intended to extract
the most important information from definitions
without necessarily having to produce a complete
analysis of the whole of a particular definition text
since attempting to produce complete analyses would
be difficult for many LDOCE definition texts. In fact
the current definition analyser applies successively
more specific phrasal analysis patterns; more
detailed analyses being possible when relatively
specific phrasal patterns are applied successfully to a
definition. A description of the details of this analysis
mechanism is beyond the scope of the present paper.
Currently, around fifty phrasal patterns are used
altogether for noun, verb, and adjective definitions. A
major difficulty encountered so far in this work stems
from the liberal use in LDOCE definitions of
derivational morphology and phrasal verbs which
greatly expands the effective definition vocabulary.
</bodyText>
<sectionHeader confidence="0.992901" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.999979276595745">
The research reported in this paper
demonstrates that it is both possible and useful to
restructure the information contained in LDOCE for
use in natural language processing systems. Most
applications for natural language processing systems
will require vocabularies substantially larger than
those typically developed for theoretical or
demonstration purposes and it is often not practical,
and certainly never desirable, to generate these by
hand. The use of machine-readable sources of
published dictionaries represents a practical and
feasible alternative to hand generation.
Clearly, there is much more work to be done
with LDOCE in the extension of the use of grammar
codes and the improvement of the word sense
classification system. Similarly, there is a
considerable amount of information in LDOCE which
we have not attempted to exploit as yet; for example,
the box codes, which contain selection restrictions for
verbs or the subject codes, which classify word senses
according to the Merriam-Webster codes for subject
matter (see Walker &amp; Amsler (1983) for a suggested
use for these). The large amount of semi-formalised
information concerning the interpretation of noun
compounds and idioms also represents a rich and
potentially very useful source of information for
natural language processing systems. In particular,
we intend to investigate the automatic generation of
phrasal analysis rules from the information on
idiomatic word usage.
In the longer term, it is clear that no existing
published dictionary can meet all the requirements of
a natural language processing system and a
substantial component of the research reported above
has been devoted to restructuring LDOCE to make it
more suitable for automatic analysis. This suggests
that the automatic construction of dictionaries from
published sources intended for other purposes will
have a limited life unless lexicography is heavily
influenced by the requirements of automated natural
language analysis. In the longer term, therefore, the
automatic construction of dictionaries for natural
language processing systems may need to be based on
techniques for the automatic analysis of large corpora
(eg. Leech et al., 1983). However, in the short term,
the approach outlined in this paper will allow us to
produce a sophisticated and useful dictionary rapidly.
</bodyText>
<sectionHeader confidence="0.984949" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.999984714285714">
We would like to thank the Longman Group Limited
for kindly allowing us access to the LDOCE
typesetting tape for research purposes. We also thank
Karen Sparck Jones and John Tait for their
comments on the first draft, which substantially
improved this paper. We are very grateful to the
SERC for funding this research.
</bodyText>
<sectionHeader confidence="0.998669" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.998373545454545">
Alshawi, H.(1983) Memory and Context Mechanisms
for Automatic Text Processing, PhD Thesis, Technical
Report 60, University Computer Laboratory,
Cambridge
Amsler, R.(1981) &apos;A Taxonomy for English Nouns and
Verbs&apos;, Proceedings of the 19th Annual Meeting of the
Association for Computational Linguistics, Stanford,
California, pp.133-138
Bobrow, R.(1978) The RUS System, BBN Report
3878, Bolt, Beranek and Newman Inc., Cambridge,
Mass
</reference>
<page confidence="0.997473">
177
</page>
<bodyText confidence="0.805443142857143">
Calzolari, N.(1984) &apos;Machine-Readable Dictionaries,
Lexical Data Bases and the Lexical System&apos;,
Proceedings of the 10th International Congress on
Computational Linguistics, Stanford, CA, pp.460-461
Schmolze, J.G., and Lipkis, T.A.(1983) &apos;Classification
in the KL-ONE Knowledge Representation System&apos;,
Proceedings, IJCAI-83 , Karlsruhe, pp.330-332
</bodyText>
<reference confidence="0.998868953488372">
Gazdar, G., Klein, E., Pullum, G. and Sag, I.(In press)
Generalised Phrase Structure Grammar, Blackwell,
Oxford
Heidorn, G. et al.(1982) &apos;The EPISTLE text-
critiquing system&apos;, IBM Systems Journal, uo1.21, 305-
326
Kaplan, R. and Bresnan, J.(1982) &apos;Lexical-Functional
Grammar: A Formal System for Grammatical
Representation&apos; in J.Bresnan (dd.), The Mental
Representation of Grammatical Relations, The MIT
Press, Cambridge, Mass, pp.173-281
Kay, M.(1984a) &apos;Functional Unification Grammar: A
Formalism for Machine Translation&apos;, Proceedings of
the 10th International Congress on Computational
Linguistics, Stanford, CA, pp.75-79
Kay, M.(1984b) &apos;The Dictionary Server&apos;, Proceedings
of the 10th International Congress on Computational
Linguistics, Stanford, California, pp.461-462
Leech, G., Garside, R. and Atwell, E.(1983), The
Automatic Grammatical Tagging of the LOB Corpus,
Bulletin of the International Computer Archive of
Modern English, Norwegian Computing Centre for
the Humanities, Bergen
Michiels, A.(1982) Exploiting a Large Dictionary Data
Base, PhD Thesis, Universite de Liege, Liege
Procter, P.(1978) Longman Dictionary of
Contemporary English, Longman Group Limited,
Harlow and London
Quirk, R. et al.(1972) A Grammar of Contemporary
English, Longman Group Limited, Harlow and
London
Robinson, J.(1982) &apos;DIAGRAM: A Grammar for
Dialogues&apos;, Communications of the ACM, yo1.25, 27-
47
Sager, N.(1981) Natural Language Information
Processing, Addison-Wesley, Reading, Mass
Shieber, S.(1984) &apos;The Design of a Computer
Language for Linguistic Information&apos;, Proceedings of
the 10th International Congress on Computational
Linguistics, Stanford, CA, pp.362-366
Walker, D. and Amsler, A.(1983) The Use of Machine-
Readable Dictionaries in Sublanguage Analysis, SRI
International Technical Note, Menlo Park, CA
</reference>
<page confidence="0.997165">
178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.836546">
<title confidence="0.9994785">TOWARDS A DICTIONARY SUPPORT ENVIRONMENT FOR REAL TIME PARSING</title>
<author confidence="0.998211">Hiyan Alshawi</author>
<author confidence="0.998211">Bran Boguraev</author>
<author confidence="0.998211">Ted Briscoe</author>
<affiliation confidence="0.99999">Computer Laboratory, Cambridge University</affiliation>
<address confidence="0.949925">Corn Exchange Street Cambridge CB2 3QG, U.K.</address>
<abstract confidence="0.994669846153846">In this article we describe research on the development of large dictionaries for natural language processing. We detail the development of a dictionary support environment linking a version of the Dictionary of English natural language processing systems. We describe the process of restructuring the information in the dictionary and our use of the Longman grammar code system to construct dictionary entries for the PATR-H parsing system and our use of the Longman word definitions for automated word sense classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alshawi</author>
</authors>
<title>Memory and Context Mechanisms for Automatic Text Processing,</title>
<date>1983</date>
<tech>PhD Thesis, Technical Report 60,</tech>
<institution>University Computer Laboratory,</institution>
<location>Cambridge</location>
<contexts>
<context position="24570" citStr="Alshawi (1983)" startWordPosition="3909" endWordPosition="3910">duces output that is convenient for incorporating new word senses into a knowledge base organized around classification hierarchies, as discussed shortly. However, the system allows the form of output structures to be specified in a flexible way. Alternative possible output representations would be meaning postulates and definitions based on semantic primitives. As mentioned above, the implemented experimental system is intended to enable the classification (see e.g. Schmolze, 1983) of new word senses with respect to a hierar-chically organized knowledge base, for example the one described in Alshawi (1983). The proposal being made here is that the analysis of dictionary definitions can provide enough information to link a new word sense to domain knowledge already encoded in the knowledge base of a limited domain natural language application such as a database query system. Given a hand-coded hierarchical organization of the relevant (central) senses of the definition vocabulary together with a classification of the relationships between these senses and domain specific concepts, the LDOCE definition of a new word sense often contains enough information to enable the inclusion of the word sense</context>
</contexts>
<marker>Alshawi, 1983</marker>
<rawString>Alshawi, H.(1983) Memory and Context Mechanisms for Automatic Text Processing, PhD Thesis, Technical Report 60, University Computer Laboratory, Cambridge</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Amsler</author>
</authors>
<title>A Taxonomy for English Nouns and Verbs&apos;,</title>
<booktitle>Proceedings of the 19th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<location>Stanford, California,</location>
<marker>Amsler, </marker>
<rawString>Amsler, R.(1981) &apos;A Taxonomy for English Nouns and Verbs&apos;, Proceedings of the 19th Annual Meeting of the Association for Computational Linguistics, Stanford, California, pp.133-138</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bobrow</author>
</authors>
<title>The RUS System,</title>
<date>1978</date>
<tech>BBN Report 3878,</tech>
<institution>Bolt, Beranek and Newman Inc.,</institution>
<location>Cambridge, Mass</location>
<contexts>
<context position="2483" citStr="Bobrow, 1978" startWordPosition="359" endWordPosition="360">s - for spelling correction, lexical analysis, thesaurus construction, machinetranslation, to name but a few applications - very few however have used such a dictionary to support a natural language parsing system. Most of the work on automated dictionaries has concentrated on extracting lexical or other information in, essentially, batch processing (eg. Amsler, 1981; Walker &amp; Amsler, 1983), or on developing dictionary servers for office automation systems (Kay, 1984b). Few parsing • systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the Epistle Project (Heidorn et al., 1982); the former employs a dictionary of less than 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine-readable sources, however, their grammar formalism and the limited grammatical information supplied by the dictionary make this achievement, though impressive, theoretically less interesting. We chose to employ the Longman Dictionary </context>
</contexts>
<marker>Bobrow, 1978</marker>
<rawString>Bobrow, R.(1978) The RUS System, BBN Report 3878, Bolt, Beranek and Newman Inc., Cambridge, Mass</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
</authors>
<title>and Sag, I.(In press) Generalised Phrase Structure Grammar,</title>
<location>Blackwell, Oxford</location>
<marker>Gazdar, Klein, Pullum, </marker>
<rawString>Gazdar, G., Klein, E., Pullum, G. and Sag, I.(In press) Generalised Phrase Structure Grammar, Blackwell, Oxford</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Heidorn</author>
</authors>
<title>et al.(1982) &apos;The EPISTLE textcritiquing system&apos;,</title>
<journal>IBM Systems Journal,</journal>
<volume>1</volume>
<pages>305--326</pages>
<marker>Heidorn, </marker>
<rawString>Heidorn, G. et al.(1982) &apos;The EPISTLE textcritiquing system&apos;, IBM Systems Journal, uo1.21, 305-326</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: A Formal System for Grammatical Representation&apos; in J.Bresnan (dd.), The Mental Representation of Grammatical Relations,</title>
<pages>173--281</pages>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass,</location>
<marker>Kaplan, Bresnan, </marker>
<rawString>Kaplan, R. and Bresnan, J.(1982) &apos;Lexical-Functional Grammar: A Formal System for Grammatical Representation&apos; in J.Bresnan (dd.), The Mental Representation of Grammatical Relations, The MIT Press, Cambridge, Mass, pp.173-281</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay</author>
</authors>
<title>Functional Unification Grammar: A Formalism for Machine Translation&apos;,</title>
<date>1984</date>
<booktitle>Proceedings of the 10th International Congress on Computational Linguistics,</booktitle>
<pages>75--79</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="1102" citStr="Kay, 1984" startWordPosition="155" endWordPosition="156">s. We describe the process of restructuring the information in the dictionary and our use of the Longman grammar code system to construct dictionary entries for the PATR-H parsing system and our use of the Longman word definitions for automated word sense classification. INTRODUCTION Recent developments in linguistics, and especially on grammatical theory - for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., In Press), Lexical Functional Grammar (LFG) (Kaplan &amp; Bresnan, 1982) - and on natural language parsing frameworks - for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-H (Shieber, 1984) - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language. These developments also demonstrate that if natural language processing systems are to be able to handle the grammatical and logical idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the informa</context>
<context position="2341" citStr="Kay, 1984" startWordPosition="339" endWordPosition="340"> dictionary head words. The idea of using the machine-readable source of a published dictionary has occurred to a wide range of researchers - for spelling correction, lexical analysis, thesaurus construction, machinetranslation, to name but a few applications - very few however have used such a dictionary to support a natural language parsing system. Most of the work on automated dictionaries has concentrated on extracting lexical or other information in, essentially, batch processing (eg. Amsler, 1981; Walker &amp; Amsler, 1983), or on developing dictionary servers for office automation systems (Kay, 1984b). Few parsing • systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the Epistle Project (Heidorn et al., 1982); the former employs a dictionary of less than 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine-readable sources, however, their grammar formalism and the limited grammatical information s</context>
</contexts>
<marker>Kay, 1984</marker>
<rawString>Kay, M.(1984a) &apos;Functional Unification Grammar: A Formalism for Machine Translation&apos;, Proceedings of the 10th International Congress on Computational Linguistics, Stanford, CA, pp.75-79</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay</author>
</authors>
<title>The Dictionary Server&apos;,</title>
<date>1984</date>
<booktitle>Proceedings of the 10th International Congress on Computational Linguistics,</booktitle>
<pages>461--462</pages>
<location>Stanford, California,</location>
<contexts>
<context position="1102" citStr="Kay, 1984" startWordPosition="155" endWordPosition="156">s. We describe the process of restructuring the information in the dictionary and our use of the Longman grammar code system to construct dictionary entries for the PATR-H parsing system and our use of the Longman word definitions for automated word sense classification. INTRODUCTION Recent developments in linguistics, and especially on grammatical theory - for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., In Press), Lexical Functional Grammar (LFG) (Kaplan &amp; Bresnan, 1982) - and on natural language parsing frameworks - for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-H (Shieber, 1984) - make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language. These developments also demonstrate that if natural language processing systems are to be able to handle the grammatical and logical idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the informa</context>
<context position="2341" citStr="Kay, 1984" startWordPosition="339" endWordPosition="340"> dictionary head words. The idea of using the machine-readable source of a published dictionary has occurred to a wide range of researchers - for spelling correction, lexical analysis, thesaurus construction, machinetranslation, to name but a few applications - very few however have used such a dictionary to support a natural language parsing system. Most of the work on automated dictionaries has concentrated on extracting lexical or other information in, essentially, batch processing (eg. Amsler, 1981; Walker &amp; Amsler, 1983), or on developing dictionary servers for office automation systems (Kay, 1984b). Few parsing • systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the Epistle Project (Heidorn et al., 1982); the former employs a dictionary of less than 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine-readable sources, however, their grammar formalism and the limited grammatical information s</context>
</contexts>
<marker>Kay, 1984</marker>
<rawString>Kay, M.(1984b) &apos;The Dictionary Server&apos;, Proceedings of the 10th International Congress on Computational Linguistics, Stanford, California, pp.461-462</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Leech</author>
<author>R Garside</author>
<author>E Atwell</author>
</authors>
<title>The Automatic Grammatical Tagging of the LOB Corpus,</title>
<journal>Bulletin of the International Computer Archive of Modern English, Norwegian</journal>
<location>Bergen</location>
<marker>Leech, Garside, Atwell, </marker>
<rawString>Leech, G., Garside, R. and Atwell, E.(1983), The Automatic Grammatical Tagging of the LOB Corpus, Bulletin of the International Computer Archive of Modern English, Norwegian Computing Centre for the Humanities, Bergen</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michiels</author>
</authors>
<title>Exploiting a Large Dictionary Data Base,</title>
<date>1982</date>
<tech>PhD Thesis,</tech>
<institution>Universite de Liege, Liege</institution>
<contexts>
<context position="3734" citStr="Michiels (1982)" startWordPosition="535" endWordPosition="536">8, henceforth LDOCE) as the machine-readable source for our dictionary environment because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system. Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled &apos;core&apos; vocabulary in defining the words throughout the dictionary. (Michiels (1982) gives further description and discussion of LDOCE from the perspective of natural language processing.) The problem of utilising LDOCE in natural language processing falls into two areas. Firstly, we must provide a dictionary environment which links the dictionary to our existing natural language processing systems in the appropriate fashion and secondly, we must restructure the information in the dictionary in such a way that these systems are able to utilise it effectively. These two tasks form the subject matter of the next two sections. 171 THE ACCESS ENVIRONMENT To link the machine-reada</context>
<context position="14665" citStr="Michiels, 1982" startWordPosition="2291" endWordPosition="2292">ition to the &apos;noise&apos; generated by the fact that we are working with a typesetting tape geared to visual presentation, rather than a database, there are errors in the use of the grammar code system; for example, Figure 4 illustrates the code for the first sense of the noun &amp;quot;promise&amp;quot;. promise n 1 IC (of),C3,5; under + tJJ Figure 4 The occurrence of the full code &amp;quot;C3&amp;quot; between commas is incorrect because commas are clearly intended to delimit sequences of elided codes. This type of error arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982). Finally, there are errors or omissions in the use of the codes; for example, Figure 5 illustrates the grammar codes for the listed senses of the verb &amp;quot;upset&amp;quot;. upset: head Ti for cat = v head I word sense 1 head Ti word sense 2 head Ti word sense 3 word sense 4 Figure 5 These codes correspond to the simple transitive and intransitive uses of &amp;quot;upset&amp;quot;; no codes are given for the uses of &amp;quot;upset&amp;quot; with sentential complements. Clearly, the restructuring programs cannot correct this last type of error, however, we have developed a system which is sufficiently robust to handle the other problems desc</context>
<context position="19953" citStr="Michiels (1982)" startWordPosition="3167" endWordPosition="3168"> the sentence; thus, the information that &amp;quot;storm&amp;quot; is transitive and that it is logically a two-place predicate is kept distinct. Consequently, the system can represent the fact that some verbs which take two syntactic arguments are nevertheless logically one-place predicates. It is not possible to automatically construct PATR-II dictionary entries for verbs just by mapping one full grammar code from the restructured LDOCE entry into a set of templates. However, it turns out that if we compare the full set of grammar codes associated with a particular sense of a verb, following a suggestion of Michiels (1982), then we can construct the correct set of templates. That is, we can extract all the information that PATH-II requires concerning the subcategorisation and semantic type of verbs. For example, as we saw above, &amp;quot;believe&amp;quot; under one sense is assigned the codes T5 and V3; the presence of the T5 code tells us that &amp;quot;believe&amp;quot; is a &apos;raising-to-object&apos; verb and logically two-place under the V3 interpretation. On the other hand, &amp;quot;persuade&amp;quot; is only assigned the V3 code, so we can conclude that it is three-place with object control of the infinitive. By systematically exploiting the collocation of differ</context>
</contexts>
<marker>Michiels, 1982</marker>
<rawString>Michiels, A.(1982) Exploiting a Large Dictionary Data Base, PhD Thesis, Universite de Liege, Liege</rawString>
</citation>
<citation valid="true">
<authors>
<author>Procter</author>
</authors>
<title>Longman Dictionary of Contemporary English, Longman Group Limited,</title>
<date>1978</date>
<location>Harlow and London</location>
<contexts>
<context position="3120" citStr="Procter 1978" startWordPosition="448" endWordPosition="449"> lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the Epistle Project (Heidorn et al., 1982); the former employs a dictionary of less than 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine-readable sources, however, their grammar formalism and the limited grammatical information supplied by the dictionary make this achievement, though impressive, theoretically less interesting. We chose to employ the Longman Dictionary of Contemporary English (Procter 1978, henceforth LDOCE) as the machine-readable source for our dictionary environment because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system. Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled &apos;core&apos; vocabulary in defining the words throughout the dictionary. (M</context>
</contexts>
<marker>Procter, 1978</marker>
<rawString>Procter, P.(1978) Longman Dictionary of Contemporary English, Longman Group Limited, Harlow and London</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Quirk</author>
</authors>
<title>et al.(1972) A Grammar of Contemporary English, Longman Group Limited,</title>
<location>Harlow and London</location>
<marker>Quirk, </marker>
<rawString>Quirk, R. et al.(1972) A Grammar of Contemporary English, Longman Group Limited, Harlow and London</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Robinson</author>
</authors>
<title>DIAGRAM: A Grammar for Dialogues&apos;,</title>
<journal>Communications of the ACM,</journal>
<volume>1</volume>
<pages>27--47</pages>
<marker>Robinson, </marker>
<rawString>Robinson, J.(1982) &apos;DIAGRAM: A Grammar for Dialogues&apos;, Communications of the ACM, yo1.25, 27-47</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sager</author>
</authors>
<title>Natural Language Information Processing,</title>
<date>1981</date>
<booktitle>Proceedings of the 10th International Congress on Computational Linguistics,</booktitle>
<pages>362--366</pages>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Mass</location>
<contexts>
<context position="2633" citStr="Sager, 1981" startWordPosition="379" endWordPosition="380">such a dictionary to support a natural language parsing system. Most of the work on automated dictionaries has concentrated on extracting lexical or other information in, essentially, batch processing (eg. Amsler, 1981; Walker &amp; Amsler, 1983), or on developing dictionary servers for office automation systems (Kay, 1984b). Few parsing • systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the Epistle Project (Heidorn et al., 1982); the former employs a dictionary of less than 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine-readable sources, however, their grammar formalism and the limited grammatical information supplied by the dictionary make this achievement, though impressive, theoretically less interesting. We chose to employ the Longman Dictionary of Contemporary English (Procter 1978, henceforth LDOCE) as the machine-readable source for our dictionary environment because this dictionary has sev</context>
</contexts>
<marker>Sager, 1981</marker>
<rawString>Sager, N.(1981) Natural Language Information Processing, Addison-Wesley, Reading, Mass Shieber, S.(1984) &apos;The Design of a Computer Language for Linguistic Information&apos;, Proceedings of the 10th International Congress on Computational Linguistics, Stanford, CA, pp.362-366</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Walker</author>
<author>Amsler</author>
</authors>
<booktitle>A.(1983) The Use of MachineReadable Dictionaries in Sublanguage Analysis, SRI International Technical Note,</booktitle>
<location>Menlo Park, CA</location>
<marker>Walker, Amsler, </marker>
<rawString>Walker, D. and Amsler, A.(1983) The Use of MachineReadable Dictionaries in Sublanguage Analysis, SRI International Technical Note, Menlo Park, CA</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>