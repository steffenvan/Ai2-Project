<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002325">
<title confidence="0.992592">
Automatic Detection of Multilingual Dictionaries on the Web
</title>
<author confidence="0.954655">
Gintar˙e Grigonyt˙e♠ Timothy Baldwin♥
</author>
<affiliation confidence="0.850552">
♠ Department of Linguistics, Stockholm University
♥ Department of Computing and Information Systems, The University of Melbourne
</affiliation>
<email confidence="0.98949">
gintare@ling.su.se tb@ldwin.net
</email>
<sectionHeader confidence="0.993636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954545454546">
This paper presents an approach to query
construction to detect multilingual dictio-
naries for predetermined language combi-
nations on the web, based on the identifi-
cation of terms which are likely to occur
in bilingual dictionaries but not in general
web documents. We use eight target lan-
guages for our case study, and train our
method on pre-identified multilingual dic-
tionaries and the Wikipedia dump for each
of our languages.
</bodyText>
<sectionHeader confidence="0.987919" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999380193548387">
Translation dictionaries and other multilingual
lexical resources are valuable in a myriad of
contexts, from language preservation (Thieberger
and Berez, 2012) to language learning (Laufer
and Hadar, 1997), cross-language information
retrieval (Nie, 2010) and machine translation
(Munteanu and Marcu, 2005; Soderland et al.,
2009). While there are syndicated efforts
to produce multilingual dictionaries for differ-
ent pairings of the world’s languages such as
freedict.org, more commonly, multilingual
dictionaries are developed in isolation for a spe-
cific set of languages, with ad hoc formatting,
great variability in lexical coverage, and no cen-
tral indexing of the content or existence of that
dictionary (Baldwin et al., 2010). Projects such
as panlex.org aspire to aggregate these dic-
tionaries into a single lexical database, but are
hampered by the need to identify individual multi-
lingual dictionaries, especially for language pairs
where there is a sparsity of data from existing dic-
tionaries (Baldwin et al., 2010; Kamholz and Pool,
to appear). This paper is an attempt to automate
the detection of multilingual dictionaries on the
web, through query construction for an arbitrary
language pair. Note that for the method to work,
we require that the dictionary occurs in “list form”,
that is it takes the form of a single document (or
at least, a significant number of dictionary entries
on a single page), and is not split across multiple
small-scale sub-documents.
</bodyText>
<sectionHeader confidence="0.99981" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996143088235294">
This research seeks to identify documents of a
particular type on the web, namely multilingual
dictionaries. Related work broadly falls into
four categories: (1) mining of parallel corpora;
(2) automatic construction of bilingual dictionar-
ies/thesauri; (3) automatic detection of multilin-
gual documents; and (4) classification of docu-
ment genre.
Parallel corpus construction is the task of au-
tomatically detecting document sets that contain
the same content in different languages, com-
monly based on a combination of site-structural
and content-based features (Chen and Nie, 2000;
Resnik and Smith, 2003). Such methods could
potentially identify parallel word lists from which
to construct a bilingual dictionary, although more
realistically, bilingual dictionaries exist as single
documents and are not well suited to this style of
analysis.
Methods have also been proposed to automat-
ically construct bilingual dictionaries or thesauri,
e.g. based on crosslingual glossing in predictable
patterns such as a technical term being immedi-
ately proceeded by that term in a lingua franca
source language such as English (Nagata et al.,
2001; Yu and Tsujii, 2009). Alternatively, com-
parable or parallel corpora can be used to extract
bilingual dictionaries based on crosslingual distri-
butional similarity (Melamed, 1996; Fung, 1998).
While the precision of these methods is generally
relatively high, the recall is often very low, as there
is a strong bias towards novel technical terms be-
ing glossed but more conventional terms not.
Also relevant to this work is research on lan-
</bodyText>
<page confidence="0.986342">
93
</page>
<bodyText confidence="0.956741566666667">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93–98,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
guage identification, and specifically the detection
of multilingual documents (Prager, 1999; Yam-
aguchi and Tanaka-Ishii, 2012; Lui et al., 2014).
Here, multi-label document classification meth-
ods have been adapted to identify what mix of
languages is present in a given document, which
could be used as a pre-filter to locate documents
containing a given mixture of languages, although
there is, of course, no guarantee that a multilingual
document is a dictionary.
Finally, document genre classification is rele-
vant in that it is theoretically possible to develop
a document categorisation method which classi-
fies documents as multilingual dictionaries or not,
with the obvious downside that it would need to be
applied exhaustively to all documents on the web.
The general assumption in genre classification is
that the type of a document should be judged not
by its content but rather by its form. A variety
of document genre methods have been proposed,
generally based on a mixture of structural and
content-based features (Matsuda and Fukushima,
1999; Finn et al., 2002; zu Eissen and Stein, 2005).
While all of these lines of research are relevant
to this work, as far as we are aware, there has not
been work which has proposed a direct method
for identifying pre-existing multilingual dictionar-
ies in document collections.
</bodyText>
<sectionHeader confidence="0.998488" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999982040816327">
Our method is based on a query formulation ap-
proach, and querying against a pre-existing index
of a document collection (e.g. the web) via an in-
formation retrieval system.
The first intuition underlying our approach is
that certain words are a priori more “language-
discriminating” than others, and should be pre-
ferred in query construction (e.g. sushi occurs as
a [transliterated] word in a wide variety of lan-
guages, whereas anti-discriminatory is found pre-
dominantly in English documents). As such, we
prefer search terms wi with a higher value for
maxl P(l|wi), where l is the language of interest.
The second intuition is that the lexical cover-
age of dictionaries varies considerably, especially
with multilingual lexicons, which are often com-
piled by a single developer or small community
of developers, with little systematicity in what is
including or not included in the dictionary. As
such, if we are to follow a query construction ap-
proach to lexicon discovery, we need to be able
to predict the likelihood of a given word wi be-
ing included in an arbitrarily-selected dictionary
Dl incorporating language l (i.e. P(wi|Dl)). Fac-
tors which impact on this include the lexical prior
of the word in the language (e.g. P(paper|en) &gt;
P (papyrus|en)), whether they are lemmas or not
(noting that multilingual dictionaries tend not to
contain inflected word forms), and their word class
(e.g. multilingual dictionaries tend to contain more
nouns and verbs than function words).
The third intuition is that certain word combi-
nations are more selective of multilingual dictio-
naries than others, i.e. if certain words are found
together (e.g. cruiser, gospel and noodle), the con-
taining document is highly likely to be a dictionary
of some description rather than a “conventional”
document.
Below, we describe our methodology for query
construction based on these elements in greater de-
tail. The only assumption on the method is that
we have access to a selection of dictionaries D
(mono- or multilingual) and a corpus of conven-
tional (non-dictionary) documents C, and knowl-
edge of the language(s) contained in each dictio-
nary and document.
Given a set of dictionaries Dl for a language l
and the complement set Dl = D\Dl, we first con-
struct the lexicon Ll for that language as follows:
</bodyText>
<equation confidence="0.946738">
Ll = {wi|wi ∈ Dl ∩ wi ∈/ Dl} (1)
</equation>
<bodyText confidence="0.999984615384615">
This creates a language-discriminating lexicon for
each language, satisfying the first criterion.
Lexical resources differ in size, scope and cov-
erage. For instance, a well-developed, mature
multilingual dictionary may contain over 100,000
multilingual lexical records, while a specialised 5-
way multilingual domain dictionary may contain
as few as 100 multilingual lexical records. In line
with our second criterion, we want to select words
which have a higher likelihood of occurrence in
a multilingual dictionary involving that language.
To this end, we calculate the weight sdict(wi,l) for
each word wi,l ∈ Ll:
</bodyText>
<equation confidence="0.985632">
|Ll|−|d |if w- l ∈ d
sdict(wi,l) = |Ll |Z&apos; (2)
d∈Dl − |L  |otherwise
</equation>
<bodyText confidence="0.99986725">
where |d |is the size of dictionary d in terms of the
number of lexemes it contains.
The final step is to weight words by their typ-
icality in a given language, as calculated by their
</bodyText>
<page confidence="0.996534">
94
</page>
<bodyText confidence="0.9997355">
likelihood of occurrence in a random document in
that language. This is estimated by the proportion
of Wikipedia documents in that language which
contain the word in question:
</bodyText>
<equation confidence="0.990567">
df(wi,l)
Score(wi,l) = sdict(wi,l) (3)
Nl
</equation>
<bodyText confidence="0.978740625">
where df(wi,l) is the count of Wikipedia docu-
ments of language l which contain wi, and Nl is
the total number of Wikipedia documents in lan-
guage l.
In all experiments in this paper, we assume that
we have access to at least one multilingual dictio-
nary containing each of our target languages, but
in absence of such a dictionary, sdict(wi,l) could
be set to 1 for all words wi,l in the language.
The result of this term weighing is a ranked list
of words for each language. The next step is to
identify combinations of words that are likely to
be found in multilingual dictionaries and not stan-
dard documents for a given language, in accor-
dance with our third criterion.
95 Figure 1: Examples of learned queries for different
</bodyText>
<subsectionHeader confidence="0.999639">
3.1 Apriori-based query generation
</subsectionHeader>
<bodyText confidence="0.999578352941177">
We perform query construction for each language
based on frequent item set mining, using the Apri-
ori algorithm (Agrawal et al., 1993). For a given
combination of languages (e.g. English and Swa-
heli), queries are then formed simply by combin-
ing monolingual queries for the component lan-
guages.
The basic approach is to use a modified support
formulation within the Apriori algorithm to prefer
word combinations that do not cooccur in regular
documents. Based on the assumption that query-
ing a (pre-indexed) document collection is rela-
tively simple, we generate a range of queries of de-
creasing length and increasing likelihood of term
co-occurrence in standard documents, and query
until a non-empty set of results is returned.
The modified support formulation is as follows:
</bodyText>
<equation confidence="0.976090666666667">
cscore(wi, ..., wn) =
0 if ∃d, wi, wj : cod(wi, wj )
�Hi Score(wi)
</equation>
<bodyText confidence="0.928909793103448">
wj) is a Boolean function which
evaluates to true iff
and wj co-occur in doc-
ument d. That is, we reject any combinations of
words which are found to co-occur in Wikipedia
documents for that language. Note that the actual
calculation of this co-occurrence can
cod(wi,
wi
be performed
word that we are adding to the query
in
the terminology of Apriori) and each of the other
words in a non-zero support itemset from the pre-
vious iteration of the algorithm (which are guaran-
teed to not co-occur with each other); and (b) the
determination of whether two terms collocate can
be performed efficiently using an inverted index of
Wikipedia for that language.
In our experiments, we apply the Apriori al-
gorithm exhaustively for a given language with a
support threshold of 0.5, and return the resultant
item sets in ranked order of combined score for
the component words.
A random selection of queries learned for each
of the 8 lan
(“itemset”
guages targeted in this research is pre-
sented in Figure 1.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="method">
4 Experimental methodology
</sectionHeader>
<bodyText confidence="0.8796209">
We evaluate our proposed methodology in two
ways:
tentially an underestimate of the true retri
eval
effectiveness.
where
lan
guages
efficiently, as: (a) for a given iteration of Apri-
ori, it only needs to be performed between the new
</bodyText>
<listItem confidence="0.995678307692308">
1. against a synthetic dataset, whereby we in-
jected bilingual dictionaries into a collection
of web documents, and evaluated the ability
of the method to return multilingual dictio-
naries for individual languages; in this, we
naively assume that all web documents in the
background collection are not multilingual
dictionaries, and as such, the results are po-
2. against the open web via the Google search
API for a given combination of languages,
and hand evaluation of the return
ed docu-
ments
</listItem>
<table confidence="0.9924016">
otherwise
Lang Wikipedia articles (M) Dictionaries Queries learned Avg. query length
en 3.1 26 2546 3.2
zh 0.3 0 5034 3.6
es 0.5 2 356 2.9
ja 0.6 0 1532 3.3
de 1.0 13 634 2.7
fr 0.9 5 4126 3.0
it 0.6 4 1955 3.0
ar 0.1 2 9004 3.2
</table>
<tableCaption confidence="0.999805">
Table 1: Details of the training data and queries learned for each language
</tableCaption>
<bodyText confidence="0.999983607142857">
Note that the first evaluation with the synthetic
dataset is based on monolingual dictionary re-
trieval effectiveness because we have very few
(and often no) multilingual dictionaries for a given
pairing of our target languages. For a given lan-
guage, we are thus evaluating the ability of our
method to retrieve multilingual dictionaries con-
taining that language (and other indeterminate lan-
guages).
For both the synthetic dataset and open web ex-
periments, we evaluate our method based on mean
average precision (MAP), that is the mean of the
average precision scores for each query which re-
turns a non-empty result set.
To train our method, we use 52 bilingual Free-
dict (Freedict, 2011) dictionaries and Wikipedia1
documents for each of our target languages. As
there are no bilingual dictionaries in Freedict for
Chinese and Japanese, the training of Score values
is based on the Wikipedia documents only. Mor-
phological segmentation for these two languages
was carried out using MeCab (MeCab, 2011) and
the Stanford Word Segmenter (Tseng et al., 2005),
respectively. See Table 1 for details of the num-
ber of Wikipedia articles and dictionaries for each
language.
Below, we detail the construction of the syn-
thetic dataset.
</bodyText>
<subsectionHeader confidence="0.99681">
4.1 Synthetic dataset
</subsectionHeader>
<bodyText confidence="0.998679777777778">
The synthetic dataset was constructed using a sub-
set of ClueWeb09 (ClueWeb09, 2009) as the back-
ground web document collection. The original
ClueWeb09 dataset consists of around 1 billion
web pages in ten languages that were collected in
January and February 2009. The relative propor-
tions of documents in the different languages in
the original dataset are as detailed in Table 2.
We randomly downsampled ClueWeb09 to 10
</bodyText>
<table confidence="0.96155725">
1Based on 2009 dumps.
Language Proportion
en (English) 48.41%
zh (Chinese) 17.05%
es (Spanish) 7.62%
ja (Japanese) 6.47%
de (German) 4.89%
fr (French) 4.79%
ko (Korean) 3.61%
it (Italian) 2.8%
pt (Portuguese) 2.62%
ar (Arabic) 1.74%
</table>
<tableCaption confidence="0.998903">
Table 2: Language proportions in ClueWeb09.
</tableCaption>
<bodyText confidence="0.999754076923077">
million documents for the 8 languages targeted
in this research (the original 10 ClueWeb09 lan-
guages minus Korean and Portuguese). We then
sourced a random set of 246 multilingual dic-
tionaries that were used in the construction of
panlex.org, and injected them into the docu-
ment collection. Each of these dictionaries con-
tains at least one of our 8 target languages, with
the second language potentially being outside the
8. A total of 49 languages are contained in the
dictionaries.
We indexed the synthetic dataset using Indri (In-
dri, 2009).
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999984714285714">
First, we present results over the synthetic dataset
in Table 3. As our baseline, we simply query for
the language name and the term dictionary in the
local language (e.g. English dictionary, for En-
glish) in the given language.
For languages that had bilingual dictionaries for
training, the best results were obtained for Span-
ish, German, Italian and Arabic. Encouragingly,
the results for languages with only Wikipedia doc-
uments (and no dictionaries) were largely com-
parable to those for languages with dictionaries,
with Japanese achieving a MAP score compara-
ble to the best results for languages with dictio-
nary training data. The comparably low result for
</bodyText>
<page confidence="0.994064">
96
</page>
<table confidence="0.9998655">
Lang Dicts MAP Baseline
en 92 0.77 0.00
zh 7 0.75 0.00
es 34 0.98 0.04
ja 5 0.94 0.00
de 75 0.97 0.08
fr 34 0.84 0.03
it 8 0.95 0.01
ar 3 0.92 0.00
AVERAGE: 32.2 0.88 0.04
</table>
<tableCaption confidence="0.995191">
Table 3: Dictionary retrieval results over the syn-
</tableCaption>
<bodyText confidence="0.993687675">
thetic dataset (“Dicts” = the number of dictionaries
in the document collection for that language.
English is potentially affected by its prevalence
both in the bilingual dictionaries in training (re-
stricting the effective vocabulary size due to our
Li filtering), and in the document collection. Re-
call also that our MAP scores are an underestimate
of the true results, and some of the ClueWeb09
documents returned for our queries are potentially
relevant documents (i.e. multilingual dictionaries
including the language of interest). For all lan-
guages, the baseline results were below 0.1, and
substantially lower than the results for our method.
Looking next to the open web, we present in Ta-
ble 4 results based on querying the Google search
API with the 1000 longest queries for English
paired with each of the other 7 target languages.
Most queries returned no results; indeed, for the
en-ar language pair, only 49/1000 queries returned
documents. The results in Table 4 are based on
manual evaluation of all documents returned for
the first 50 queries, and determination of whether
they were multilingual dictionaries containing the
indicated languages.
The baseline results are substantially higher
than those for the synthetic dataset, almost cer-
tainly a direct result of the greater sophistication
and optimisation of the Google search engine (in-
cluding query log analysis, and link and anchor
text analysis). Despite this, the results for our
method are lower than those over the synthetic
dataset, we suspect largely as a result of the style
of queries we issue being so far removed from
standard Google query patterns. Having said this,
MAP scores of 0.32–0.92 suggest that the method
is highly usable (i.e. at any given cutoff in the doc-
ument ranking, an average of at least one in three
documents is a genuine multilingual dictionary),
and any non-dictionary documents returned by the
method could easily be pruned by a lexicographer.
</bodyText>
<table confidence="0.999756777777778">
Lang Dicts MAP Baseline
zh 16 0.55 0.19
es 17 0.92 0.13
ja 13 0.32 0.04
de 34 0.77 0.09
fr 36 0.77 0.08
it 23 0.69 0.11
ar 8 0.39 0.17
AVERAGE: 21.0 0.63 0.12
</table>
<tableCaption confidence="0.980801">
Table 4: Dictionary retrieval results over the open
</tableCaption>
<bodyText confidence="0.985392666666667">
web for dictionaries containing English and each
of the indicated languages (“Dicts” = the number
of unique multilingual dictionaries retrieved for
that language).
Among the 7 language pairs, en-es, en-de, en-fr
and en-it achieved the highest MAP scores. In
terms of unique lexical resources found with 50
queries, the most successful language pairs were
en-fr, en-de and en-it.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999992583333334">
We have described initial results for a method de-
signed to automatically detect multilingual dictio-
naries on the web, and attained highly credible re-
sults over both a synthetic dataset and an exper-
iment over the open web using a web search en-
gine.
In future work, we hope to explore the ability
of the method to detect domain-specific dictionar-
ies (e.g. training over domain-specific dictionar-
ies from other language pairs), and low-density
languages where there are few dictionaries and
Wikipedia articles to train the method on.
</bodyText>
<sectionHeader confidence="0.994984" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996484166666667">
We wish to thank the anonymous reviewers for
their valuable comments, and the Panlex devel-
opers for assistance with the dictionaries and ex-
perimental design. This research was supported
by funding from the Group of Eight and the Aus-
tralian Research Council.
</bodyText>
<sectionHeader confidence="0.998911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997457">
Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami.
1993. Mining association rules between sets of
items in large databases. ACM SIGMOD Record,
22(2):207–216.
Timothy Baldwin, Jonathan Pool, and Susan M.
Colowick. 2010. PanLex and LEXTRACT: Trans-
lating all words of all languages of the world. In
</reference>
<page confidence="0.998068">
97
</page>
<reference confidence="0.98984946">
Proceedings of the 23rd International Conference on
Computational Linguistics (COLING 2010), Demo
Volume, pages 37–40, Beijing, China.
Jiang Chen and Jian-Yun Nie. 2000. Parallel web text
mining for cross-language IR. In Proceedings of
Recherche d’Informations Assistee par Ordinateur
2000 (RIAO’2000), pages 62–77, College de France,
France.
ClueWeb09. 2009. The ClueWeb09 dataset. http:
//lemurproject.org/clueweb09/.
Aidan Finn, Nicholas Kushmerick, and Barry Smyth.
2002. Genre classification and domain transfer for
information filtering. In Proceedings of the 24th Eu-
ropean Conference on Information Retrieval (ECIR
2002), pages 353–362, Glasgow, UK.
Freedict. 2011. Freedict dictionaries. http://www.
freedict.com.
Pascale Fung. 1998. A statistical view on bilin-
gual lexicon extraction: From parallel corpora to
non-parallel corpora. In Proceedings of Associa-
tion for Machine Translation in the Americas (AMTA
1998): Machine Translation and the Information
Soup, pages 1–17, Langhorne, USA.
Indri. 2009. Indri search engine. http://www.
lemurproject.org/indri/.
David Kamholz and Jonathan Pool. to appear. PanLex:
Building a resource for panlingual lexical transla-
tion. In Proceedings of the 9th International Confer-
ence on Language Resources and Evaluation (LREC
2014), Reykjavik, Iceland.
Batia Laufer and Linor Hadar. 1997. Assessing the
effectiveness of monolingual, bilingual, and “bilin-
gualised” dictionaries in the comprehension and pro-
duction of new words. The Modern Language Jour-
nal, 81(2):189–196.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Associ-
ation for Computational Linguistics, 2(Feb):27–40.
Katsushi Matsuda and Toshikazu Fukushima. 1999.
Task-oriented world wide web retrieval by document
type classification. In Proceedings of the 1999 ACM
Conference on Information and Knowledge Man-
agement (CIKM 1999), pages 109–113, Kansas City,
USA.
MeCab. 2011. http://mecab.googlecode.
com.
I. Dan Melamed. 1996. Automatic construction of
clean broad-coverage translation lexicons. In Pro-
ceedings of the 2nd Conference of the Association
for Machine Translation in the Americas (AMTA
1996), Montreal, Canada.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477–504.
Masaaki Nagata, Teruka Saito, and Kenji Suzuki.
2001. Using the web as a bilingual dictionary. In
Proceedings of the ACL 2001 Workshop on Data-
driven Methods in Machine Translation, pages 1–8,
Toulouse, France.
Jian-Yun Nie. 2010. Cross-language information
retrieval. Morgan and Claypool Publishers, San
Rafael, USA.
John M. Prager. 1999. Linguini: language identifi-
cation for multilingual documents. In Proceedings
the 32nd Annual Hawaii International Conference
on Systems Sciences (HICSS-32), Maui, USA.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380.
Stephen Soderland, Christopher Lim, Mausam,
Bo Qin, Oren Etzioni, and Jonathan Pool. 2009.
Lemmatic machine translation. In Proceedings
of the Twelfth Machine Translation Summit (MT
Summit XII), Ottawa, Canada.
Nicholas Thieberger and Andrea L. Berez. 2012. Lin-
guistic data management. In Nicholas Thieberger,
editor, The Oxford Handbook of Linguistic Field-
work. Oxford University Press, Oxford, UK.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, volume
171.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 969–978,
Jeju Island, Korea.
Kun Yu and Junichi Tsujii. 2009. Bilingual dictio-
nary extraction from Wikipedia. In Proceedings of
the Twelfth Machine Translation Summit (MT Sum-
mit XII), pages 379–386, Ottawa, Canada.
Sven Meyer zu Eissen and Benno Stein. 2005. Genre
classification of web pages. In Proceedings of the
27th Annual German Conference in AI (KI 2005),
pages 256–269, Ulm, Germany.
</reference>
<page confidence="0.996243">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99978">Automatic Detection of Multilingual Dictionaries on the Web</title>
<author confidence="0.999385">Timothy</author>
<affiliation confidence="0.8731175">of Linguistics, Stockholm University of Computing and Information Systems, The University of Melbourne</affiliation>
<email confidence="0.650778">gintare@ling.su.setb@ldwin.net</email>
<abstract confidence="0.993759615384615">This paper presents an approach to query construction to detect multilingual dictionaries for predetermined language combinations on the web, based on the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents. We use eight target languages for our case study, and train our method on pre-identified multilingual dictionaries and the Wikipedia dump for each of our languages. 1 Motivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009). While there are syndicated to produce multilingual dictionaries for different pairings of the world’s languages such as more commonly, multilingual dictionaries are developed in isolation for a specific set of languages, with ad hoc formatting, great variability in lexical coverage, and no central indexing of the content or existence of that dictionary (Baldwin et al., 2010). Projects such to aggregate these dictionaries into a single lexical database, but are hampered by the need to identify individual multilingual dictionaries, especially for language pairs where there is a sparsity of data from existing dictionaries (Baldwin et al., 2010; Kamholz and Pool, to appear). This paper is an attempt to automate the detection of multilingual dictionaries on the web, through query construction for an arbitrary language pair. Note that for the method to work, we require that the dictionary occurs in “list form”, that is it takes the form of a single document (or at least, a significant number of dictionary entries on a single page), and is not split across multiple small-scale sub-documents. 2 Related Work This research seeks to identify documents of a particular type on the web, namely multilingual dictionaries. Related work broadly falls into four categories: (1) mining of parallel corpora; (2) automatic construction of bilingual dictionaries/thesauri; (3) automatic detection of multilingual documents; and (4) classification of document genre. Parallel corpus construction is the task of automatically detecting document sets that contain the same content in different languages, commonly based on a combination of site-structural and content-based features (Chen and Nie, 2000; Resnik and Smith, 2003). Such methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distributional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. relevant to this work is research on lan- 93 of the 52nd Annual Meeting of the Association for Computational Linguistics (Short pages Maryland, USA, June 23-25 2014. Association for Computational Linguistics guage identification, and specifically the detection of multilingual documents (Prager, 1999; Yamaguchi and Tanaka-Ishii, 2012; Lui et al., 2014). Here, multi-label document classification methods have been adapted to identify what mix of languages is present in a given document, which could be used as a pre-filter to locate documents containing a given mixture of languages, although there is, of course, no guarantee that a multilingual document is a dictionary. Finally, document genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exhaustively to all documents on the web. The general assumption in genre classification is that the type of a document should be judged not by its content but rather by its form. A variety of document genre methods have been proposed, generally based on a mixture of structural and content-based features (Matsuda and Fukushima, 1999; Finn et al., 2002; zu Eissen and Stein, 2005). While all of these lines of research are relevant to this work, as far as we are aware, there has not been work which has proposed a direct method for identifying pre-existing multilingual dictionaries in document collections. 3 Methodology Our method is based on a query formulation approach, and querying against a pre-existing index of a document collection (e.g. the web) via an information retrieval system. The first intuition underlying our approach is that certain words are a priori more “languagediscriminating” than others, and should be prein query construction (e.g. as a [transliterated] word in a wide variety of lanwhereas found predominantly in English documents). As such, we search terms a higher value for where the language of interest. The second intuition is that the lexical coverage of dictionaries varies considerably, especially with multilingual lexicons, which are often compiled by a single developer or small community of developers, with little systematicity in what is including or not included in the dictionary. As such, if we are to follow a query construction approach to lexicon discovery, we need to be able predict the likelihood of a given word being included in an arbitrarily-selected dictionary language Factors which impact on this include the lexical prior the word in the language (e.g. whether they are lemmas or not (noting that multilingual dictionaries tend not to contain inflected word forms), and their word class (e.g. multilingual dictionaries tend to contain more nouns and verbs than function words). third intuition is that certain word combimore selective of multilingual dictionaries than others, i.e. if certain words are found (e.g. the containing document is highly likely to be a dictionary of some description rather than a “conventional” document. Below, we describe our methodology for query construction based on these elements in greater detail. The only assumption on the method is that have access to a selection of dictionaries (monoor multilingual) and a corpus of conven- (non-dictionary) documents and knowledge of the language(s) contained in each dictionary and document. a set of dictionaries a language the complement set = we first conthe lexicon that language as follows: This creates a language-discriminating lexicon for each language, satisfying the first criterion. Lexical resources differ in size, scope and coverage. For instance, a well-developed, mature multilingual dictionary may contain over 100,000 multilingual lexical records, while a specialised 5way multilingual domain dictionary may contain as few as 100 multilingual lexical records. In line with our second criterion, we want to select words which have a higher likelihood of occurrence in a multilingual dictionary involving that language. this end, we calculate the weight for word if l = |otherwise the size of dictionary terms of the number of lexemes it contains. The final step is to weight words by their typicality in a given language, as calculated by their 94 likelihood of occurrence in a random document in that language. This is estimated by the proportion of Wikipedia documents in that language which contain the word in question: = is the count of Wikipedia docuof language contain and the total number of Wikipedia documents in lan- In all experiments in this paper, we assume that we have access to at least one multilingual dictionary containing each of our target languages, but absence of such a dictionary, could set to 1 for all words the language. The result of this term weighing is a ranked list of words for each language. The next step is to identify combinations of words that are likely to be found in multilingual dictionaries and not standard documents for a given language, in accordance with our third criterion. 95Figure 1: Examples of learned queries for different 3.1 Apriori-based query generation We perform query construction for each language based on frequent item set mining, using the Apriori algorithm (Agrawal et al., 1993). For a given combination of languages (e.g. English and Swaheli), queries are then formed simply by combining monolingual queries for the component languages. The basic approach is to use a modified support formulation within the Apriori algorithm to prefer word combinations that do not cooccur in regular documents. Based on the assumption that querying a (pre-indexed) document collection is relatively simple, we generate a range of queries of decreasing length and increasing likelihood of term co-occurrence in standard documents, and query until a non-empty set of results is returned. The modified support formulation is as follows: ..., = if is a Boolean function which evaluates to true iff and in doc- That is, we reject any combinations of words which are found to co-occur in Wikipedia documents for that language. Note that the actual calculation of this co-occurrence can cod(wi, wi be performed word that we are adding to the query in the terminology of Apriori) and each of the other words in a non-zero support itemset from the previous iteration of the algorithm (which are guaranteed to not co-occur with each other); and (b) the determination of whether two terms collocate can be performed efficiently using an inverted index of Wikipedia for that language. In our experiments, we apply the Apriori algorithm exhaustively for a given language with a support threshold of 0.5, and return the resultant item sets in ranked order of combined score for the component words. A random selection of queries learned for each of the 8 lan (“itemset” guages targeted in this research is presented in Figure 1. 4 Experimental methodology We evaluate our proposed methodology in two ways: tentially an underestimate of the true retri eval effectiveness. where lan guages efficiently, as: (a) for a given iteration of Apriori, it only needs to be performed between the new 1. against a synthetic dataset, whereby we injected bilingual dictionaries into a collection of web documents, and evaluated the ability of the method to return multilingual dictionaries for individual languages; in this, we naively assume that all web documents in the background collection are not multilingual and as such, the results are po- 2. against the open web via the Google search API for a given combination of languages, and hand evaluation of the return ed documents otherwise Lang Wikipedia articles (M) Dictionaries Queries learned Avg. query length en 3.1 26 2546 3.2 zh 0.3 0 5034 3.6 es 0.5 2 356 2.9 ja 0.6 0 1532 3.3 de 1.0 13 634 2.7 fr 0.9 5 4126 3.0 it 0.6 4 1955 3.0 ar 0.1 2 9004 3.2 Table 1: Details of the training data and queries learned for each language Note that the first evaluation with the synthetic is based on retrieval effectiveness because we have very few (and often no) multilingual dictionaries for a given pairing of our target languages. For a given language, we are thus evaluating the ability of our method to retrieve multilingual dictionaries containing that language (and other indeterminate languages). For both the synthetic dataset and open web experiments, we evaluate our method based on mean average precision (MAP), that is the mean of the average precision scores for each query which returns a non-empty result set. To train our method, we use 52 bilingual Free- (Freedict, 2011) dictionaries and documents for each of our target languages. As there are no bilingual dictionaries in Freedict for Chinese and Japanese, the training of Score values is based on the Wikipedia documents only. Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al., 2005), respectively. See Table 1 for details of the number of Wikipedia articles and dictionaries for each language. Below, we detail the construction of the synthetic dataset. 4.1 Synthetic dataset The synthetic dataset was constructed using a subset of ClueWeb09 (ClueWeb09, 2009) as the background web document collection. The original ClueWeb09 dataset consists of around 1 billion web pages in ten languages that were collected in January and February 2009. The relative proportions of documents in the different languages in the original dataset are as detailed in Table 2. We randomly downsampled ClueWeb09 to 10 on 2009 dumps. Language Proportion en (English) 48.41% zh (Chinese) 17.05% es (Spanish) 7.62% ja (Japanese) 6.47% de (German) 4.89% fr (French) 4.79% ko (Korean) 3.61% it (Italian) 2.8% pt (Portuguese) 2.62% ar (Arabic) 1.74% Table 2: Language proportions in ClueWeb09. million documents for the 8 languages targeted in this research (the original 10 ClueWeb09 languages minus Korean and Portuguese). We then sourced a random set of 246 multilingual dictionaries that were used in the construction of and injected them into the document collection. Each of these dictionaries contains at least one of our 8 target languages, with the second language potentially being outside the 8. A total of 49 languages are contained in the dictionaries. We indexed the synthetic dataset using Indri (Indri, 2009). 5 Results First, we present results over the synthetic dataset in Table 3. As our baseline, we simply query for language name and the term the language (e.g. for English) in the given language. For languages that had bilingual dictionaries for training, the best results were obtained for Spanish, German, Italian and Arabic. Encouragingly, the results for languages with only Wikipedia documents (and no dictionaries) were largely comparable to those for languages with dictionaries, with Japanese achieving a MAP score comparable to the best results for languages with dictionary training data. The comparably low result for 96 Lang Dicts MAP Baseline en 92 0.77 0.00 zh 7 0.75 0.00 es 34 0.98 0.04 ja 5 0.94 0.00 de 75 0.97 0.08 fr 34 0.84 0.03 it 8 0.95 0.01 ar 3 0.92 0.00 32.2 0.88 0.04 Table 3: Dictionary retrieval results over the synthetic dataset (“Dicts” = the number of dictionaries in the document collection for that language. English is potentially affected by its prevalence both in the bilingual dictionaries in training (restricting the effective vocabulary size due to our and in the document collection. Recall also that our MAP scores are an underestimate of the true results, and some of the ClueWeb09 documents returned for our queries are potentially relevant documents (i.e. multilingual dictionaries including the language of interest). For all languages, the baseline results were below 0.1, and substantially lower than the results for our method. Looking next to the open web, we present in Table 4 results based on querying the Google search API with the 1000 longest queries for English paired with each of the other 7 target languages. Most queries returned no results; indeed, for the en-ar language pair, only 49/1000 queries returned documents. The results in Table 4 are based on manual evaluation of all documents returned for the first 50 queries, and determination of whether they were multilingual dictionaries containing the indicated languages. The baseline results are substantially higher than those for the synthetic dataset, almost certainly a direct result of the greater sophistication and optimisation of the Google search engine (including query log analysis, and link and anchor text analysis). Despite this, the results for our method are lower than those over the synthetic dataset, we suspect largely as a result of the style of queries we issue being so far removed from standard Google query patterns. Having said this, MAP scores of 0.32–0.92 suggest that the method is highly usable (i.e. at any given cutoff in the document ranking, an average of at least one in three documents is a genuine multilingual dictionary), and any non-dictionary documents returned by the method could easily be pruned by a lexicographer. Lang Dicts MAP Baseline zh 16 0.55 0.19 es 17 0.92 0.13 ja 13 0.32 0.04 de 34 0.77 0.09 fr 36 0.77 0.08 it 23 0.69 0.11 ar 8 0.39 0.17 21.0 0.63 0.12 Table 4: Dictionary retrieval results over the open web for dictionaries containing English and each of the indicated languages (“Dicts” = the number of unique multilingual dictionaries retrieved for that language). Among the 7 language pairs, en-es, en-de, en-fr and en-it achieved the highest MAP scores. In terms of unique lexical resources found with 50 queries, the most successful language pairs were en-fr, en-de and en-it. 6 Conclusions We have described initial results for a method designed to automatically detect multilingual dictionaries on the web, and attained highly credible results over both a synthetic dataset and an experiment over the open web using a web search engine. In future work, we hope to explore the ability of the method to detect domain-specific dictionaries (e.g. training over domain-specific dictionaries from other language pairs), and low-density languages where there are few dictionaries and Wikipedia articles to train the method on. Acknowledgements We wish to thank the anonymous reviewers for their valuable comments, and the Panlex developers for assistance with the dictionaries and experimental design. This research was supported by funding from the Group of Eight and the Australian Research Council.</abstract>
<note confidence="0.93792185">References Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami. 1993. Mining association rules between sets of in large databases. SIGMOD 22(2):207–216. Timothy Baldwin, Jonathan Pool, and Susan M. Colowick. 2010. PanLex and LEXTRACT: Translating all words of all languages of the world. In 97 Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Demo pages 37–40, Beijing, China. Jiang Chen and Jian-Yun Nie. 2000. Parallel web text for cross-language IR. In of Recherche d’Informations Assistee par Ordinateur pages 62–77, College de France, France. 2009. The ClueWeb09 dataset. Aidan Finn, Nicholas Kushmerick, and Barry Smyth. 2002. Genre classification and domain transfer for</note>
<abstract confidence="0.58211985">filtering. In of the 24th European Conference on Information Retrieval (ECIR pages 353–362, Glasgow, UK. 2011. Freedict dictionaries. Pascale Fung. 1998. A statistical view on bilingual lexicon extraction: From parallel corpora to corpora. In of Association for Machine Translation in the Americas (AMTA 1998): Machine Translation and the Information pages 1–17, Langhorne, USA. 2009. Indri search engine. David Kamholz and Jonathan Pool. to appear. PanLex: Building a resource for panlingual lexical transla- In of the 9th International Conference on Language Resources and Evaluation (LREC Reykjavik, Iceland. Batia Laufer and Linor Hadar. 1997. Assessing the effectiveness of monolingual, bilingual, and “bilingualised” dictionaries in the comprehension and proof new words. Modern Language Jour-</abstract>
<note confidence="0.8476">81(2):189–196. Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of documents. of the Associfor Computational 2(Feb):27–40. Katsushi Matsuda and Toshikazu Fukushima. 1999. Task-oriented world wide web retrieval by document classification. In of the 1999 ACM Conference on Information and Knowledge Man- (CIKM pages 109–113, Kansas City, USA. 2011. I. Dan Melamed. 1996. Automatic construction of broad-coverage translation lexicons. In Proceedings of the 2nd Conference of the Association for Machine Translation in the Americas (AMTA Montreal, Canada. Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploitnon-parallel corpora. Linguis- 31(4):477–504. Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001. Using the web as a bilingual dictionary. In Proceedings of the ACL 2001 Workshop on Data- Methods in Machine pages 1–8, Toulouse, France. Nie. 2010. information Morgan and Claypool Publishers, San Rafael, USA. John M. Prager. 1999. Linguini: language identififor multilingual documents. In the 32nd Annual Hawaii International Conference Systems Sciences Maui, USA. Philip Resnik and Noah A. Smith. 2003. The web a parallel corpus. 29(3):349–380. Stephen Soderland, Christopher Lim, Mausam, Bo Qin, Oren Etzioni, and Jonathan Pool. 2009.</note>
<author confidence="0.682188">In</author>
<affiliation confidence="0.829468">of the Twelfth Machine Translation Summit (MT</affiliation>
<address confidence="0.849173">Ottawa, Canada.</address>
<author confidence="0.7728835">Linguistic data management In Nicholas Thieberger</author>
<affiliation confidence="0.9631755">Oxford Handbook of Linguistic Field- Oxford University Press, Oxford, UK.</affiliation>
<note confidence="0.312906">Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A condi-</note>
<abstract confidence="0.872436">tional random field word segmenter for sighan bake- 2005. In of the Fourth SIGHAN on Chinese Language volume 171. Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012. Text segmentation by language using minimum de-</abstract>
<note confidence="0.819982416666667">length. In the 50th Annual Meeting of the Association for Computational Lin- (Volume 1: Long pages 969–978, Jeju Island, Korea. Kun Yu and Junichi Tsujii. 2009. Bilingual dictioextraction from Wikipedia. In of the Twelfth Machine Translation Summit (MT Sumpages 379–386, Ottawa, Canada. Sven Meyer zu Eissen and Benno Stein. 2005. Genre of web pages. In of the Annual German Conference in AI (KI pages 256–269, Ulm, Germany.</note>
<intro confidence="0.518186">98</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Tomasz Imieli´nski</author>
<author>Arun Swami</author>
</authors>
<title>Mining association rules between sets of items in large databases.</title>
<date>1993</date>
<journal>ACM SIGMOD Record,</journal>
<volume>22</volume>
<issue>2</issue>
<marker>Agrawal, Imieli´nski, Swami, 1993</marker>
<rawString>Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami. 1993. Mining association rules between sets of items in large databases. ACM SIGMOD Record, 22(2):207–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Jonathan Pool</author>
<author>Susan M Colowick</author>
</authors>
<title>PanLex and LEXTRACT: Translating all words of all languages of the world.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Demo Volume,</booktitle>
<pages>37--40</pages>
<location>Beijing, China.</location>
<contexts>
<context position="1442" citStr="Baldwin et al., 2010" startWordPosition="205" endWordPosition="208">anguage preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009). While there are syndicated efforts to produce multilingual dictionaries for different pairings of the world’s languages such as freedict.org, more commonly, multilingual dictionaries are developed in isolation for a specific set of languages, with ad hoc formatting, great variability in lexical coverage, and no central indexing of the content or existence of that dictionary (Baldwin et al., 2010). Projects such as panlex.org aspire to aggregate these dictionaries into a single lexical database, but are hampered by the need to identify individual multilingual dictionaries, especially for language pairs where there is a sparsity of data from existing dictionaries (Baldwin et al., 2010; Kamholz and Pool, to appear). This paper is an attempt to automate the detection of multilingual dictionaries on the web, through query construction for an arbitrary language pair. Note that for the method to work, we require that the dictionary occurs in “list form”, that is it takes the form of a single</context>
</contexts>
<marker>Baldwin, Pool, Colowick, 2010</marker>
<rawString>Timothy Baldwin, Jonathan Pool, and Susan M. Colowick. 2010. PanLex and LEXTRACT: Translating all words of all languages of the world. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Demo Volume, pages 37–40, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Chen</author>
<author>Jian-Yun Nie</author>
</authors>
<title>Parallel web text mining for cross-language IR.</title>
<date>2000</date>
<booktitle>In Proceedings of Recherche d’Informations Assistee par Ordinateur</booktitle>
<pages>62--77</pages>
<location>College de France, France.</location>
<contexts>
<context position="2782" citStr="Chen and Nie, 2000" startWordPosition="413" endWordPosition="416">ll-scale sub-documents. 2 Related Work This research seeks to identify documents of a particular type on the web, namely multilingual dictionaries. Related work broadly falls into four categories: (1) mining of parallel corpora; (2) automatic construction of bilingual dictionaries/thesauri; (3) automatic detection of multilingual documents; and (4) classification of document genre. Parallel corpus construction is the task of automatically detecting document sets that contain the same content in different languages, commonly based on a combination of site-structural and content-based features (Chen and Nie, 2000; Resnik and Smith, 2003). Such methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, compa</context>
</contexts>
<marker>Chen, Nie, 2000</marker>
<rawString>Jiang Chen and Jian-Yun Nie. 2000. Parallel web text mining for cross-language IR. In Proceedings of Recherche d’Informations Assistee par Ordinateur 2000 (RIAO’2000), pages 62–77, College de France, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ClueWeb09</author>
</authors>
<title>The ClueWeb09 dataset.</title>
<date>2009</date>
<note>http: //lemurproject.org/clueweb09/.</note>
<marker>ClueWeb09, 2009</marker>
<rawString>ClueWeb09. 2009. The ClueWeb09 dataset. http: //lemurproject.org/clueweb09/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aidan Finn</author>
<author>Nicholas Kushmerick</author>
<author>Barry Smyth</author>
</authors>
<title>Genre classification and domain transfer for information filtering.</title>
<date>2002</date>
<booktitle>In Proceedings of the 24th European Conference on Information Retrieval (ECIR</booktitle>
<pages>353--362</pages>
<location>Glasgow, UK.</location>
<contexts>
<context position="5063" citStr="Finn et al., 2002" startWordPosition="764" endWordPosition="767">tionary. Finally, document genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exhaustively to all documents on the web. The general assumption in genre classification is that the type of a document should be judged not by its content but rather by its form. A variety of document genre methods have been proposed, generally based on a mixture of structural and content-based features (Matsuda and Fukushima, 1999; Finn et al., 2002; zu Eissen and Stein, 2005). While all of these lines of research are relevant to this work, as far as we are aware, there has not been work which has proposed a direct method for identifying pre-existing multilingual dictionaries in document collections. 3 Methodology Our method is based on a query formulation approach, and querying against a pre-existing index of a document collection (e.g. the web) via an information retrieval system. The first intuition underlying our approach is that certain words are a priori more “languagediscriminating” than others, and should be preferred in query co</context>
</contexts>
<marker>Finn, Kushmerick, Smyth, 2002</marker>
<rawString>Aidan Finn, Nicholas Kushmerick, and Barry Smyth. 2002. Genre classification and domain transfer for information filtering. In Proceedings of the 24th European Conference on Information Retrieval (ECIR 2002), pages 353–362, Glasgow, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freedict</author>
</authors>
<title>Freedict dictionaries.</title>
<date>2011</date>
<note>http://www. freedict.com.</note>
<contexts>
<context position="13081" citStr="Freedict, 2011" startWordPosition="2121" endWordPosition="2122">onolingual dictionary retrieval effectiveness because we have very few (and often no) multilingual dictionaries for a given pairing of our target languages. For a given language, we are thus evaluating the ability of our method to retrieve multilingual dictionaries containing that language (and other indeterminate languages). For both the synthetic dataset and open web experiments, we evaluate our method based on mean average precision (MAP), that is the mean of the average precision scores for each query which returns a non-empty result set. To train our method, we use 52 bilingual Freedict (Freedict, 2011) dictionaries and Wikipedia1 documents for each of our target languages. As there are no bilingual dictionaries in Freedict for Chinese and Japanese, the training of Score values is based on the Wikipedia documents only. Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al., 2005), respectively. See Table 1 for details of the number of Wikipedia articles and dictionaries for each language. Below, we detail the construction of the synthetic dataset. 4.1 Synthetic dataset The synthetic dataset was constructed us</context>
</contexts>
<marker>Freedict, 2011</marker>
<rawString>Freedict. 2011. Freedict dictionaries. http://www. freedict.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas (AMTA 1998): Machine Translation and the Information Soup,</booktitle>
<pages>1--17</pages>
<location>Langhorne, USA.</location>
<contexts>
<context position="3529" citStr="Fung, 1998" startWordPosition="526" endWordPosition="527">lthough more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distributional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lan93 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93–98, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics guage identification, and specifically the detection of multilingual documents (Prager, 1999; Yamaguchi and Tanaka-Ishii, 2012; Lui et al., 2</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>Pascale Fung. 1998. A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora. In Proceedings of Association for Machine Translation in the Americas (AMTA 1998): Machine Translation and the Information Soup, pages 1–17, Langhorne, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Indri</author>
</authors>
<title>Indri search engine.</title>
<date>2009</date>
<note>http://www. lemurproject.org/indri/.</note>
<contexts>
<context position="14881" citStr="Indri, 2009" startWordPosition="2408" endWordPosition="2410">) 2.62% ar (Arabic) 1.74% Table 2: Language proportions in ClueWeb09. million documents for the 8 languages targeted in this research (the original 10 ClueWeb09 languages minus Korean and Portuguese). We then sourced a random set of 246 multilingual dictionaries that were used in the construction of panlex.org, and injected them into the document collection. Each of these dictionaries contains at least one of our 8 target languages, with the second language potentially being outside the 8. A total of 49 languages are contained in the dictionaries. We indexed the synthetic dataset using Indri (Indri, 2009). 5 Results First, we present results over the synthetic dataset in Table 3. As our baseline, we simply query for the language name and the term dictionary in the local language (e.g. English dictionary, for English) in the given language. For languages that had bilingual dictionaries for training, the best results were obtained for Spanish, German, Italian and Arabic. Encouragingly, the results for languages with only Wikipedia documents (and no dictionaries) were largely comparable to those for languages with dictionaries, with Japanese achieving a MAP score comparable to the best results fo</context>
</contexts>
<marker>Indri, 2009</marker>
<rawString>Indri. 2009. Indri search engine. http://www. lemurproject.org/indri/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Kamholz</author>
<author>Jonathan Pool</author>
</authors>
<title>to appear. PanLex: Building a resource for panlingual lexical translation.</title>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014),</booktitle>
<location>Reykjavik, Iceland.</location>
<marker>Kamholz, Pool, </marker>
<rawString>David Kamholz and Jonathan Pool. to appear. PanLex: Building a resource for panlingual lexical translation. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014), Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Batia Laufer</author>
<author>Linor Hadar</author>
</authors>
<title>Assessing the effectiveness of monolingual, bilingual, and “bilingualised” dictionaries in the comprehension and production of new words.</title>
<date>1997</date>
<journal>The Modern Language Journal,</journal>
<volume>81</volume>
<issue>2</issue>
<contexts>
<context position="916" citStr="Laufer and Hadar, 1997" startWordPosition="128" endWordPosition="131">proach to query construction to detect multilingual dictionaries for predetermined language combinations on the web, based on the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents. We use eight target languages for our case study, and train our method on pre-identified multilingual dictionaries and the Wikipedia dump for each of our languages. 1 Motivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009). While there are syndicated efforts to produce multilingual dictionaries for different pairings of the world’s languages such as freedict.org, more commonly, multilingual dictionaries are developed in isolation for a specific set of languages, with ad hoc formatting, great variability in lexical coverage, and no central indexing of the content or existence of that dictionary (Baldwin et al., 2010). Projects such as panlex.org aspire to aggregate these dictionaries into </context>
</contexts>
<marker>Laufer, Hadar, 1997</marker>
<rawString>Batia Laufer and Linor Hadar. 1997. Assessing the effectiveness of monolingual, bilingual, and “bilingualised” dictionaries in the comprehension and production of new words. The Modern Language Journal, 81(2):189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Jey Han Lau</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic detection and language identification of multilingual documents.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--27</pages>
<contexts>
<context position="4133" citStr="Lui et al., 2014" startWordPosition="615" endWordPosition="618">; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lan93 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93–98, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics guage identification, and specifically the detection of multilingual documents (Prager, 1999; Yamaguchi and Tanaka-Ishii, 2012; Lui et al., 2014). Here, multi-label document classification methods have been adapted to identify what mix of languages is present in a given document, which could be used as a pre-filter to locate documents containing a given mixture of languages, although there is, of course, no guarantee that a multilingual document is a dictionary. Finally, document genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exhaustively to all doc</context>
</contexts>
<marker>Lui, Lau, Baldwin, 2014</marker>
<rawString>Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multilingual documents. Transactions of the Association for Computational Linguistics, 2(Feb):27–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsushi Matsuda</author>
<author>Toshikazu Fukushima</author>
</authors>
<title>Task-oriented world wide web retrieval by document type classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 ACM Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>109--113</pages>
<location>Kansas City, USA.</location>
<contexts>
<context position="5044" citStr="Matsuda and Fukushima, 1999" startWordPosition="760" endWordPosition="763">ultilingual document is a dictionary. Finally, document genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exhaustively to all documents on the web. The general assumption in genre classification is that the type of a document should be judged not by its content but rather by its form. A variety of document genre methods have been proposed, generally based on a mixture of structural and content-based features (Matsuda and Fukushima, 1999; Finn et al., 2002; zu Eissen and Stein, 2005). While all of these lines of research are relevant to this work, as far as we are aware, there has not been work which has proposed a direct method for identifying pre-existing multilingual dictionaries in document collections. 3 Methodology Our method is based on a query formulation approach, and querying against a pre-existing index of a document collection (e.g. the web) via an information retrieval system. The first intuition underlying our approach is that certain words are a priori more “languagediscriminating” than others, and should be pr</context>
</contexts>
<marker>Matsuda, Fukushima, 1999</marker>
<rawString>Katsushi Matsuda and Toshikazu Fukushima. 1999. Task-oriented world wide web retrieval by document type classification. In Proceedings of the 1999 ACM Conference on Information and Knowledge Management (CIKM 1999), pages 109–113, Kansas City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MeCab</author>
</authors>
<date>2011</date>
<note>http://mecab.googlecode. com.</note>
<contexts>
<context position="13394" citStr="MeCab, 2011" startWordPosition="2169" endWordPosition="2170">languages). For both the synthetic dataset and open web experiments, we evaluate our method based on mean average precision (MAP), that is the mean of the average precision scores for each query which returns a non-empty result set. To train our method, we use 52 bilingual Freedict (Freedict, 2011) dictionaries and Wikipedia1 documents for each of our target languages. As there are no bilingual dictionaries in Freedict for Chinese and Japanese, the training of Score values is based on the Wikipedia documents only. Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al., 2005), respectively. See Table 1 for details of the number of Wikipedia articles and dictionaries for each language. Below, we detail the construction of the synthetic dataset. 4.1 Synthetic dataset The synthetic dataset was constructed using a subset of ClueWeb09 (ClueWeb09, 2009) as the background web document collection. The original ClueWeb09 dataset consists of around 1 billion web pages in ten languages that were collected in January and February 2009. The relative proportions of documents in the different languages in the original dataset </context>
</contexts>
<marker>MeCab, 2011</marker>
<rawString>MeCab. 2011. http://mecab.googlecode. com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic construction of clean broad-coverage translation lexicons.</title>
<date>1996</date>
<booktitle>In Proceedings of the 2nd Conference of the Association for Machine Translation in the Americas (AMTA</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="3516" citStr="Melamed, 1996" startWordPosition="524" endWordPosition="525">l dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distributional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lan93 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93–98, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics guage identification, and specifically the detection of multilingual documents (Prager, 1999; Yamaguchi and Tanaka-Ishii, 2012; </context>
</contexts>
<marker>Melamed, 1996</marker>
<rawString>I. Dan Melamed. 1996. Automatic construction of clean broad-coverage translation lexicons. In Proceedings of the 2nd Conference of the Association for Machine Translation in the Americas (AMTA 1996), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="1016" citStr="Munteanu and Marcu, 2005" startWordPosition="140" endWordPosition="143">tions on the web, based on the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents. We use eight target languages for our case study, and train our method on pre-identified multilingual dictionaries and the Wikipedia dump for each of our languages. 1 Motivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009). While there are syndicated efforts to produce multilingual dictionaries for different pairings of the world’s languages such as freedict.org, more commonly, multilingual dictionaries are developed in isolation for a specific set of languages, with ad hoc formatting, great variability in lexical coverage, and no central indexing of the content or existence of that dictionary (Baldwin et al., 2010). Projects such as panlex.org aspire to aggregate these dictionaries into a single lexical database, but are hampered by the need to identify individual multilingual dictiona</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
<author>Teruka Saito</author>
<author>Kenji Suzuki</author>
</authors>
<title>Using the web as a bilingual dictionary.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL 2001 Workshop on Datadriven Methods in Machine Translation,</booktitle>
<pages>1--8</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="3338" citStr="Nagata et al., 2001" startWordPosition="497" endWordPosition="500"> site-structural and content-based features (Chen and Nie, 2000; Resnik and Smith, 2003). Such methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distributional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lan93 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93–98, Baltimore, Maryland, USA, June 23-25 2014.</context>
</contexts>
<marker>Nagata, Saito, Suzuki, 2001</marker>
<rawString>Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001. Using the web as a bilingual dictionary. In Proceedings of the ACL 2001 Workshop on Datadriven Methods in Machine Translation, pages 1–8, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Yun Nie</author>
</authors>
<title>Cross-language information retrieval.</title>
<date>2010</date>
<publisher>Morgan and Claypool Publishers,</publisher>
<location>San Rafael, USA.</location>
<contexts>
<context position="966" citStr="Nie, 2010" startWordPosition="135" endWordPosition="136">s for predetermined language combinations on the web, based on the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents. We use eight target languages for our case study, and train our method on pre-identified multilingual dictionaries and the Wikipedia dump for each of our languages. 1 Motivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009). While there are syndicated efforts to produce multilingual dictionaries for different pairings of the world’s languages such as freedict.org, more commonly, multilingual dictionaries are developed in isolation for a specific set of languages, with ad hoc formatting, great variability in lexical coverage, and no central indexing of the content or existence of that dictionary (Baldwin et al., 2010). Projects such as panlex.org aspire to aggregate these dictionaries into a single lexical database, but are hampered by the</context>
</contexts>
<marker>Nie, 2010</marker>
<rawString>Jian-Yun Nie. 2010. Cross-language information retrieval. Morgan and Claypool Publishers, San Rafael, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Prager</author>
</authors>
<title>Linguini: language identification for multilingual documents.</title>
<date>1999</date>
<booktitle>In Proceedings the 32nd Annual Hawaii International Conference on Systems Sciences (HICSS-32),</booktitle>
<location>Maui, USA.</location>
<contexts>
<context position="4080" citStr="Prager, 1999" startWordPosition="608" endWordPosition="609">lingual distributional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lan93 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93–98, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics guage identification, and specifically the detection of multilingual documents (Prager, 1999; Yamaguchi and Tanaka-Ishii, 2012; Lui et al., 2014). Here, multi-label document classification methods have been adapted to identify what mix of languages is present in a given document, which could be used as a pre-filter to locate documents containing a given mixture of languages, although there is, of course, no guarantee that a multilingual document is a dictionary. Finally, document genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside tha</context>
</contexts>
<marker>Prager, 1999</marker>
<rawString>John M. Prager. 1999. Linguini: language identification for multilingual documents. In Proceedings the 32nd Annual Hawaii International Conference on Systems Sciences (HICSS-32), Maui, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="2807" citStr="Resnik and Smith, 2003" startWordPosition="417" endWordPosition="420">ts. 2 Related Work This research seeks to identify documents of a particular type on the web, namely multilingual dictionaries. Related work broadly falls into four categories: (1) mining of parallel corpora; (2) automatic construction of bilingual dictionaries/thesauri; (3) automatic detection of multilingual documents; and (4) classification of document genre. Parallel corpus construction is the task of automatically detecting document sets that contain the same content in different languages, commonly based on a combination of site-structural and content-based features (Chen and Nie, 2000; Resnik and Smith, 2003). Such methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Christopher Lim</author>
<author>Bo Qin Mausam</author>
<author>Oren Etzioni</author>
<author>Jonathan Pool</author>
</authors>
<title>Lemmatic machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII),</booktitle>
<location>Ottawa, Canada.</location>
<contexts>
<context position="1041" citStr="Soderland et al., 2009" startWordPosition="144" endWordPosition="147"> the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents. We use eight target languages for our case study, and train our method on pre-identified multilingual dictionaries and the Wikipedia dump for each of our languages. 1 Motivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009). While there are syndicated efforts to produce multilingual dictionaries for different pairings of the world’s languages such as freedict.org, more commonly, multilingual dictionaries are developed in isolation for a specific set of languages, with ad hoc formatting, great variability in lexical coverage, and no central indexing of the content or existence of that dictionary (Baldwin et al., 2010). Projects such as panlex.org aspire to aggregate these dictionaries into a single lexical database, but are hampered by the need to identify individual multilingual dictionaries, especially for lang</context>
</contexts>
<marker>Soderland, Lim, Mausam, Etzioni, Pool, 2009</marker>
<rawString>Stephen Soderland, Christopher Lim, Mausam, Bo Qin, Oren Etzioni, and Jonathan Pool. 2009. Lemmatic machine translation. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Thieberger</author>
<author>Andrea L Berez</author>
</authors>
<title>Linguistic data management.</title>
<date>2012</date>
<booktitle>The Oxford Handbook of Linguistic Fieldwork.</booktitle>
<editor>In Nicholas Thieberger, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="870" citStr="Thieberger and Berez, 2012" startWordPosition="121" endWordPosition="124">se tb@ldwin.net Abstract This paper presents an approach to query construction to detect multilingual dictionaries for predetermined language combinations on the web, based on the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents. We use eight target languages for our case study, and train our method on pre-identified multilingual dictionaries and the Wikipedia dump for each of our languages. 1 Motivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009). While there are syndicated efforts to produce multilingual dictionaries for different pairings of the world’s languages such as freedict.org, more commonly, multilingual dictionaries are developed in isolation for a specific set of languages, with ad hoc formatting, great variability in lexical coverage, and no central indexing of the content or existence of that dictionary (Baldwin et al., 2010). Projects such as panlex.or</context>
</contexts>
<marker>Thieberger, Berez, 2012</marker>
<rawString>Nicholas Thieberger and Andrea L. Berez. 2012. Linguistic data management. In Nicholas Thieberger, editor, The Oxford Handbook of Linguistic Fieldwork. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>171</volume>
<contexts>
<context position="13447" citStr="Tseng et al., 2005" startWordPosition="2176" endWordPosition="2179"> open web experiments, we evaluate our method based on mean average precision (MAP), that is the mean of the average precision scores for each query which returns a non-empty result set. To train our method, we use 52 bilingual Freedict (Freedict, 2011) dictionaries and Wikipedia1 documents for each of our target languages. As there are no bilingual dictionaries in Freedict for Chinese and Japanese, the training of Score values is based on the Wikipedia documents only. Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al., 2005), respectively. See Table 1 for details of the number of Wikipedia articles and dictionaries for each language. Below, we detail the construction of the synthetic dataset. 4.1 Synthetic dataset The synthetic dataset was constructed using a subset of ClueWeb09 (ClueWeb09, 2009) as the background web document collection. The original ClueWeb09 dataset consists of around 1 billion web pages in ten languages that were collected in January and February 2009. The relative proportions of documents in the different languages in the original dataset are as detailed in Table 2. We randomly downsampled C</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Yamaguchi</author>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Text segmentation by language using minimum description length.</title>
<date>2012</date>
<booktitle>In Proceedings the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>969--978</pages>
<location>Jeju Island,</location>
<contexts>
<context position="4114" citStr="Yamaguchi and Tanaka-Ishii, 2012" startWordPosition="610" endWordPosition="614">butional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lan93 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93–98, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics guage identification, and specifically the detection of multilingual documents (Prager, 1999; Yamaguchi and Tanaka-Ishii, 2012; Lui et al., 2014). Here, multi-label document classification methods have been adapted to identify what mix of languages is present in a given document, which could be used as a pre-filter to locate documents containing a given mixture of languages, although there is, of course, no guarantee that a multilingual document is a dictionary. Finally, document genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exha</context>
</contexts>
<marker>Yamaguchi, Tanaka-Ishii, 2012</marker>
<rawString>Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012. Text segmentation by language using minimum description length. In Proceedings the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 969–978, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Yu</author>
<author>Junichi Tsujii</author>
</authors>
<title>Bilingual dictionary extraction from Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII),</booktitle>
<pages>379--386</pages>
<location>Ottawa, Canada.</location>
<contexts>
<context position="3360" citStr="Yu and Tsujii, 2009" startWordPosition="501" endWordPosition="504">content-based features (Chen and Nie, 2000; Resnik and Smith, 2003). Such methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distributional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lan93 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93–98, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association fo</context>
</contexts>
<marker>Yu, Tsujii, 2009</marker>
<rawString>Kun Yu and Junichi Tsujii. 2009. Bilingual dictionary extraction from Wikipedia. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), pages 379–386, Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Meyer zu Eissen</author>
<author>Benno Stein</author>
</authors>
<title>Genre classification of web pages.</title>
<date>2005</date>
<booktitle>In Proceedings of the 27th Annual German Conference in AI (KI</booktitle>
<pages>256--269</pages>
<location>Ulm, Germany.</location>
<contexts>
<context position="5091" citStr="Eissen and Stein, 2005" startWordPosition="769" endWordPosition="772">ment genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exhaustively to all documents on the web. The general assumption in genre classification is that the type of a document should be judged not by its content but rather by its form. A variety of document genre methods have been proposed, generally based on a mixture of structural and content-based features (Matsuda and Fukushima, 1999; Finn et al., 2002; zu Eissen and Stein, 2005). While all of these lines of research are relevant to this work, as far as we are aware, there has not been work which has proposed a direct method for identifying pre-existing multilingual dictionaries in document collections. 3 Methodology Our method is based on a query formulation approach, and querying against a pre-existing index of a document collection (e.g. the web) via an information retrieval system. The first intuition underlying our approach is that certain words are a priori more “languagediscriminating” than others, and should be preferred in query construction (e.g. sushi occur</context>
</contexts>
<marker>Eissen, Stein, 2005</marker>
<rawString>Sven Meyer zu Eissen and Benno Stein. 2005. Genre classification of web pages. In Proceedings of the 27th Annual German Conference in AI (KI 2005), pages 256–269, Ulm, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>