<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000194">
<title confidence="0.9960715">
To Link or Not to Link? A Study on End-to-End
Tweet Entity Linking
</title>
<author confidence="0.999332">
Stephen Guo Ming-Wei Chang Emre Kıcıman
</author>
<affiliation confidence="0.996279">
Stanford University Microsoft Research
</affiliation>
<email confidence="0.996562">
sdguo@cs.stanford.edu {minchang, emrek}@microsoft.com
</email>
<sectionHeader confidence="0.998584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994650884615385">
Information extraction from microblog posts
is an important task, as today microblogs cap-
ture an unprecedented amount of information
and provide a view into the pulse of the world.
As the core component of information extrac-
tion, we consider the task of Twitter entity
linking in this paper.
In the current entity linking literature, mention
detection and entity disambiguation are fre-
quently cast as equally important but distinct
problems. However, in our task, we find that
mention detection is often the performance
bottleneck. The reason is that messages on
micro-blogs are short, noisy and informal texts
with little context, and often contain phrases
with ambiguous meanings.
To rigorously address the Twitter entity link-
ing problem, we propose a structural SVM
algorithm for entity linking that jointly op-
timizes mention detection and entity disam-
biguation as a single end-to-end task. By com-
bining structural learning and a variety of first-
order, second-order, and context-sensitive fea-
tures, our system is able to outperform exist-
ing state-of-the art entity linking systems by
15% Fl.
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961256410257">
Microblogging services, such as Twitter and Face-
book, are today capturing the largest volume ever
recorded of fine-grained discussions spanning a
huge breadth of topics, from the mundane to the his-
toric. The micro-blogging service Twitter reports
that it alone captures over 340M short messages,
or tweets, per day.1 From such micro-blogging ser-
vices’ data streams, researchers have reported min-
ing insights about a variety of domains, from elec-
tion results (Tumasjan et al., 2010) and democracy
movements (Starbird and Palen, 2012) to health is-
sues and disease spreading (Paul and Dredze, 2011;
Sadilek et al., 2012), as well as tracking prod-
uct feedback and sentiment (Asur and Huberman,
2010).
A critical step in mining information from a
micro-blogging service, such as Twitter, is the iden-
tification of entities in tweets. In order to mine
the relationship between drugs, symptoms and side-
effects, or track the popularity of politicians or sen-
timent about social issues, we must first be able to
identify the topics and specific entities being dis-
cussed. The challenge is that messages on micro-
blogs are short, noisy, and informal texts with little
context, and often contain phrases with ambiguous
meanings. For example, “one day” may be either a
set phrase or a reference to a movie. Given such
difficulties, current mining and analysis of micro-
blogs lists limits its application to certain domains
with easy-to-recognize, unambiguous entities in or-
der to avoid noise in the extraction results.
We begin this paper with a thorough investigation
of mention detection and entity disambiguation for
social media, focused on the Twitter micro-blogging
service. Mention detection is the task of extraction
surface form candidates that can link to an entity in
the domain of interest. Entity disambiguation is the
task of linking an extracted mention to a specific def-
inition or instance of an entity in a knowledge base.
</bodyText>
<footnote confidence="0.975857">
1http://blog.twitter.com/2012/03/twitter-turns-six.html
</footnote>
<page confidence="0.882922">
1020
</page>
<note confidence="0.481171">
Proceedings of NAACL-HLT 2013, pages 1020–1030,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.997205727272727">
While mention detection and entity disambigua-
tion are frequently cast as equally important but dis-
tinct and separate problems, we find that mention
detection is where today’s systems and our base-
line techniques incur the most failures. Detecting
the correct entity mention is a significant challenge
given mis-capitalizations, incorrect grammar, and
ambiguous phrases. In (Ritter et al., 2011), the au-
thors report their system achieves 0.64 to 0.67 F1 on
named entity segmentation results with 34K tokens
of labeled examples. On the other hand, once the
correct entity mention is detected, a trivial disam-
biguation that maps to the most popular entity2 will
achieve 85% accuracy in our set.
Our primary contribution in this paper is a re-
casting and merging of the tasks of mention detec-
tion and entity disambiguation into a single end-
to-end entity linking task. We achieve significant
improvements by applying structural learning tech-
niques to jointly optimize the detection and disam-
biguation of entities. Treating detection and disam-
biguation as a single task also enables us to apply a
large set of new features, conventionally used only
for disambiguation, to the initial detection of men-
tions. These features, derived from external knowl-
edge bases, include entity popularity and inter-entity
relations from external knowledge bases, and are not
well utilized in current mention detection systems.
For example, consider the following partial tweet:
(1) The town is so, so good. And don’t
worry Ben, we already forgave you
for Gigli. Really.
Determining whether or not “The town” is a mention
of a location or other specific entity based solely on
lexical and syntactic features is challenging. Know-
ing “The Town” is the name of a recent movie helps,
and we can we be more confident if we know that
Ben Affleck is an actor in the movie, and Gigli is
another of his movies.
To train and evaluate our system, we created three
separate annotated data sets of approximately 500
tweets each. These data sets are hand annotated
with entity links to Wikipedia. We evaluate our sys-
tem by comparing its performance at detecting en-
</bodyText>
<footnote confidence="0.9165725">
2What we mean here is “the most linked entity”. See Sec-
tion 3 for details.
</footnote>
<bodyText confidence="0.99941975">
tities to the performance of two state-of-the-art en-
tity linking systems, Cucerzan (Cucerzan, 2007) and
TagMe (Ferragina and Scaiella, 2010), and find that
our system outperforms them significantly by 15%
in absolute F1.
The rest of this paper describes related work, our
structured learning approach to entity linking, and
our experimental results.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99990072972973">
Building an entity linking system requires solving
two interrelated sub-problems: mention detection
and entity disambiguation. The significant portion
of recent work in the literature (Ratinov et al., 2011;
Davis et al., 2012; Sil et al., 2012; Demartini et al.,
2012; Wang et al., 2012; Han and Sun, 2011; Han
et al., 2011) focuses solely upon the entity linking
problem. The entity linking systems of these studies
assume that entity mentions are provided by a sepa-
rate mention detection system. In contrast, our study
jointly identifies and disambiguates entity mentions
within tweets (short text fragments).
A subset of existing literature targets end-to-end
linking (Cucerzan, 2007; Milne and Witten, 2008;
Kulkarni et al., 2009; Ferragina and Scaiella, 2010;
Han and Sun, 2011; Meij et al., 2012), but there
are quite a few differences between our work and
each of these systems. Some systems (Milne and
Witten, 2008; Kulkarni et al., 2009; Han and Sun,
2011) heavily depend on Wikipedia text and might
not work well in short and noisy tweets. Many sys-
tems (Mihalcea and Csomai, 2007; Cucerzan, 2007;
Milne and Witten, 2008; Ferragina and Scaiella,
2010) treat mention detection and entity disam-
biguation as two different problems. (Meij et al.,
2012) is the most related to our paper. While their
system also considers mention detection and entity
disambiguation together, they do not consider entity-
to-entity relationships and do not incorporate con-
textual words from tweets.
An area of work closely related to the mention
detection problem is the Named Entity Recogni-
tion (NER) problem, the identification of textual
phrases which belong to core categories (Person,
Location, Organization). It is well-known that NER
systems trained on well-written documents perform
very poorly on short, noisy text, such as tweets (Rit-
</bodyText>
<page confidence="0.99057">
1021
</page>
<bodyText confidence="0.956811333333333">
ter et al., 2011). There have been a few recent stud-
ies proposing Twitter-specific NER systems (Li et
al., 2012; Ritter et al., 2011).
</bodyText>
<sectionHeader confidence="0.996605" genericHeader="method">
3 Preliminaries
</sectionHeader>
<bodyText confidence="0.996176092105263">
For performing entity linking on Twitter, we choose
Wikipedia as our external knowledge base of enti-
ties.
Entity We define an entity as a nonambiguous, ter-
minal page (e.g., The Town (the film)) in Wikipedia
(i.e., a Wikipedia page that is not a category, dis-
ambiguation, list, or redirect page). We define an
anchor phrase (surface form) as the textual phrase
(e.g., the town) which can potentially link to some
entities. We define an entity mention as an anchor
phrase and the context (“the town” in the exam-
ple tweet in Section 1), where its semantic meaning
umambiguously represents a specific entity. Note
that an entity may be represented by multiple sur-
face forms.
Wikipedia Lexicon Construction Following the
assumptions used in most prior entity linking re-
search, we assume that surface forms of entities can
be found as anchor phrases in Wikipedia. In or-
der to construct a Wikipedia lexicon, we first collect
all anchors phases in Wikipedia. For each anchor
phrase (surface form) s, we construct a lexicon en-
try by gathering the set of entities {e1, e2,... eK}
that can be linked from s. We also collect the num-
ber of times anchor a links to the entity ez, d(s, ez).
We define P(ez|s) = d(s, ez)/d(s), where d(s) rep-
resents the number of times s appears in Wikipedia.
We refer e&apos; as the most linked entity for anchor s if
e&apos; = arg max, P (ez|s).
Candidate Generation Given a tweet t, we ex-
tract all k-grams of size &lt; k. For each k-gram,
we find all entities where this k-gram is an anchor
phrase. If a k-gram is an anchor phrase for at least
one entity, then the k-gram is a candidate entity
mention. In general, we identify many candidate
phrase per tweet; let U(t) = {c1, c2, ...} denote
the set of candidates in tweet t. We refer to s(c)
as the surface form (e.g., the anchor phrase) of c.
Compared to the anchor phrase, the candidate also
carries the context and position information. Let
E(cz) = {e1, e2, ... , NIL} denote the set of entities
which candidate i may be linked to, plus the addi-
tional special token NIL. Note that the size of E(cz)
is always at least 2.
Task Definition First, our system generates candi-
date entity mentions, textual phrases which can pos-
sibly be entity mentions. Our system then performs
filtering and optimization to process the list of can-
didates. For each candidate, our system links the
candidate to a special NIL token or links the candi-
date to its corresponding entity in Wikipedia. More
formally, given a tweet t and its candidate set U(t),
the goal of the system is to predict yz E E(cz), bcz E
U(t).
Comparison to the TAC KBP Competition It is
important to state that our definition of the entity
linking problem differs significantly from the entity
linking problem as defined by the TAC KBP com-
petition (Ji et al., 2010; Ji et al., 2011). In the TAC,
there is no true mention detection problem; every
candidate in the TAC is an entity mention that rep-
resents an entity. Another difference is that the TAC
allows for an entity mention to map to an entity not
in the external knowledge base (Wikipedia); our sys-
tem does not provide special handling of this case.
Comparison to Named Entity Recognition
There are also important differences between our
task and the canonical NER task. For example,
NER systems identify common names, such as
“Robert,” as entities. In our task, we only consider a
prediction as a success if the system can determine
which person in Wikipedia “Robert” is referring to.
In other words, our definition of entities depends
on the given knowledge base, rather than human
judgment. Hence, it is difficult to make a fair system
comparison of our system to NER systems.
</bodyText>
<sectionHeader confidence="0.927493" genericHeader="method">
4 Entity Linking as Structural Learning
</sectionHeader>
<bodyText confidence="0.999893111111111">
In our framework, we use structural learning as a
tool to capture the relationship between entities. We
define yz as the output for cz, where yz E E(cz). Let
T = |U(t) |and y = {y1, y2, ... , yT}. The fea-
ture function for the whole assignment can be writ-
ten as -b(t, U(t), y). The score for the assignment
y can be obtained as the linear product between the
weight vector w and the feature vector. For an input
example, the prediction can be found by solving the
</bodyText>
<page confidence="0.781604">
1022
</page>
<bodyText confidence="0.382921">
inference problem:
</bodyText>
<equation confidence="0.995835">
y� = arg max wTb(t, U(t),y) (1)
Y
</equation>
<bodyText confidence="0.998934">
We use a Structural SVM (SSVM) (Taskar et
al., 2004; Tsochantaridis et al., 2005; Chang et al.,
2010) as our learning algorithm. To train the weight
vector w, we minimize the objective function of the
SSVM
</bodyText>
<equation confidence="0.989521">
�2 (2)
i
</equation>
<bodyText confidence="0.518301">
where l is the number of labeled examples and
</bodyText>
<equation confidence="0.9995185">
wT b(ti, c(ti), yi)
≥A(yi, y) + wTb(ti, c(ti), y) −�i, ∀i, y
</equation>
<bodyText confidence="0.997246">
We denote yi as the gold assignment for xi and de-
fine A(yi, y) as the Hamming distance between two
assignments yi and y.
</bodyText>
<subsectionHeader confidence="0.91109">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.999824666666667">
Feature definitions are very important as they define
the shapes of the structures. Our feature vector is
defined as
</bodyText>
<equation confidence="0.771478">
b(t, U(t),y) = I: 0(t, ci, yi)+ I: 0(t, ci, yi, cj, yj)
i i&lt;j
</equation>
<bodyText confidence="0.999805967213115">
where ci and cj is the i-th and j-th candidates in
U(t), respectively.
First, we assign b(t, ci, NIL) to be a special bias
feature. The corresponding weight value behaves as
a threshold to cut-off mentions. Recall in our defini-
tion that yi = NIL represents that the candidate ci is
not a mention.
The first order features for b(t, ci, e) are de-
scribed as follows. In general, we can classify our
features into two types: mention-specific features
and entity-specific features. For a given candidate
ci, mention-specific features only consider the sur-
face form of ci and the tweet t. Entity-specific fea-
tures also consider the knowledge base content of
the entity e. Prior work in the entity linking liter-
ature has primarily focused on entity-specific fea-
tures, as most prior work solves entity disambigua-
tion with given mentions.
Base and Capitalization Rate Our base features
are from two resources. Let s(c) denote the sur-
face form of candidate c. The link probability
Pl(s(c)) and P(e|s(c)) features are extracted from
Wikipedia. We explained P(e|s(c)) in Section 3.
Link probability Pl(s(c)) is the probability that a
phrase is used as an anchor in Wikipedia. We also
add a third feature that captures normalized link
count. Besides these three features, we also have
a feature to indicate if a is a stop word, and a fea-
ture indicating the number of tokens in a. The view
count and P(e|s) features are entity-specific, while
the other three features are mention-specific.
For each phrase s(c), we also collect statistics
about the probability that a phrase is capitalized in
Wikipedia. We refer to this feature as the capitaliza-
tion rate feature, Pc(s(c)).
Popularity Feature We have access to 300GBs
of Wikipedia page view counts, representing one
months worth of page view information, we use
this as popularity data.3 As mentioned in Sec-
tion 3, we find that the most often linked Wikipedia
articles might not be the most popular ones on
Twitter. Using page view statistics helps our sys-
tem correct this bias. We define another prob-
ability based on page view statistics Pv(ei|c) =
v(ei)/(EeEE(c)/{NIL} v(e)), where v(e) represents
the view count for the page e.
Context Capitalization Our context capitaliza-
tion features indicate if the current candidate, the
word before, and the word after the candidate are
capitalized.
Entity Type and Tf-idf We use the procedure pro-
posed in (Ratinov et al., 2011) to extract keyword
phrases from categories for each Wikipedia page,
and then build a rule-based system using keyword
phrases to classify if each entity page belongs to one
of the following entity types: Person, Location, Or-
ganization, TV Show, Book/Magazine and Movie.4
For a given candidate c and an entity e, the associ-
ated binary feature becomes active if the entity be-
longs to a specific entity type. There are six entity
type features in our system.
</bodyText>
<footnote confidence="0.969197333333333">
3http://dammit.lt/wikistats
4The entity type prediction accuracy of our rule-based sys-
tem on the development set is around 95%.
</footnote>
<equation confidence="0.961247833333333">
kwk2
2 + C
min
W
I:l
i=1
</equation>
<page confidence="0.889056">
1023
</page>
<table confidence="0.999696">
Features Descriptions
Base Pl(si), P(e|s), normalized link counts, stop
word, # tokens
Cap. Rate Pc(si)
Popularity Pv(e|s), normalized page view count,
Pv(e|s)P(e|s)
Context Cap. Three features indicating if the current candi-
date and the words before and after are capi-
talized
Entity Type Six binary features for each entity type
Tf-idf Two features for the similarity between the
word vectors of the entity and the tweet
Second-Order Jac(ei,ej), P(ei|si)P(ej|sj), Pc(si)Pc(sj),
Pl(si)Pl(sj)
</table>
<tableCaption confidence="0.9938865">
Table 1: Summary of the features used in our structural
learning systems.
</tableCaption>
<bodyText confidence="0.999830666666667">
We also include tf-idf features in our system. For
each Wikipedia page, we collect the top 100 tf-idf
words. We add one feature that is the dot product
between the tf-idf word vector of e and the words of
tweet t. We include a second feature that represents
the average tf-idf score of all words that appear in
both e and t.
Second-order features We include four very sim-
ple second-order features 0(t, ci, ei, cj, ej) to cap-
ture more complex relations between entities and
candidates. The first feature is the Jaccard distance
between two Wikipedia pages ei and ej. Let Γ(ei)
denote the set of Wikipedia pages that contain a hy-
perlink to ei. We define the Jaccard distance be-
tween ei and ej as:
</bodyText>
<equation confidence="0.9767765">
Γ(ei) n Γ(e
Jac(ei, ej) = |Γ(ei) U Γ(ej)|
</equation>
<bodyText confidence="0.999684285714286">
This feature has a similar effect as the normal-
ized Google distance (Cilibrasi and Vitanyi, 2007),
which has been used for many entity linking sys-
tems. Let us use the following shorthand: si = s(ci)
and sj = s(cj). We have also included three features
P(ei|si)P(ej|sj),Pc(si)Pc(sj) and Pl(si)Pl(sj) to
increase the expressivity of our model.
</bodyText>
<subsectionHeader confidence="0.996142">
4.2 Mining Additional Contextual Words
</subsectionHeader>
<bodyText confidence="0.999934416666667">
Unlike mention detection systems used in other NLP
tasks, there are no lexical features in our system.
Lexical features are important as they can capture
semantic meaning precisely. However, given that
we do not have many labeled examples, lexical fea-
tures can lead to overfitting. The diverse language
in tweets also make it more difficult to use lexical
features.
Our solution for this problem is to use a very sim-
ple method to mine context words for different enti-
ties from a large, unlabeled tweet corpus. The algo-
rithm works as follows:
</bodyText>
<listItem confidence="0.995252333333333">
1. Train an end-to-end entity linking system and
then apply it to a large, unlabeled tweet corpus
2. Extract contextual words for each entity type
based on the pseudo-labeled data.
3. Train the entity linking system again with new
contextual features.
</listItem>
<bodyText confidence="0.998979384615385">
In this paper, we only use the word before and the
word after as our contextual word for a candidate.
Note that while there are ambiguous phrases on the
surface (e.g., “friends” can be a TV show or just
a regular phrase), certain phrases are unambiguous
(e.g., “CSI : Miami”). As contextual words are often
shared within the same entity type (e.g. “watching”
is likely to appear before a tv show), those words can
potentially improve our final system.
Let wi denote the i-th word in the tweet and ti
denote the entity type for the i-th word.5 We use a
very simple rule to select a set of left context words
Q(R) for entity type R.
</bodyText>
<equation confidence="0.936062">
Q(R) = {wi  |P(ti+1 = R|wi) &gt; r, d(wi) &gt; z}
</equation>
<bodyText confidence="0.999701230769231">
where d(wi) represent the number of times the word
wi appears in the unlabeled set. The first rule is to
simply find a word which is more likely to be fol-
lowed by an entity. The second rule filter outs noisy
words (e.g., Twitter handles) in the unlabeled set.
The right context words are also extracted in a simi-
lar way.
To train the second end-to-end entity linking sys-
tem, we add one additional feature for the contextual
words. For the feature vector Φ(t, ci, e), the context
feature is active if the candidate ci is capitalized6 and
the context words around ci belongs to Q(R), given
R is the entity type for the entity e.
</bodyText>
<footnote confidence="0.9983065">
5The tag ti belongs to the entity type R if our system links a
candidate c to an entity with type R and c covers the word wi.
6The word “watching” can be a TV show while most of the
time it is not. These common makes this contextual feature
noisy. We found that the context feature can only be reliably
applied when the candidate is capitalized.
</footnote>
<page confidence="0.987558">
1024
</page>
<subsectionHeader confidence="0.996326">
4.3 Cohesiveness Score
</subsectionHeader>
<bodyText confidence="0.999867375">
There are several ways to consider entity-entity co-
hesiveness besides using the second-order features
directly. In our model, we also consider a modi-
fied cohesiveness score proposed in (Ferragina and
Scaiella, 2010). The idea behind the cohesiveness
score is to estimate the correlations between differ-
ent entities by using weighted Jaccard scores.7
There are two rounds in the procedure of com-
puting the cohesiveness score. We first estimate ap-
proximately the most probable entity for each candi-
date given all the other candidates in the same tweet.
In the second round, the cohesiveness score is then
produced with respect to the most probable entity
computed in the first round.
More formally, in the first round, we compute the
relevance score for each candidate and entity pair:
</bodyText>
<equation confidence="0.9685168">
Ecl=ft Eel∈E(cl) P(e0|c0)Jac(e, e0)
|U(t)|
Then, the cohesiveness score is computed by
Scoh(e, c|t) = Ecl=ft Jac(e, e(c0))P(�e(c0) |c0)
|U(t)|
</equation>
<bodyText confidence="0.999874666666667">
where the e(c0) = arg maxe∈E(cl) Rel(e, c0|t). We
then put the cohesiveness score as a feature for each
(e, c) pair. In practice, we found that the cohesive-
ness score in the model can significantly increase the
disambiguation ability of the model without using
the second-order information.
</bodyText>
<subsectionHeader confidence="0.774906">
4.4 Inference
</subsectionHeader>
<bodyText confidence="0.999982833333333">
In order to train and test the SSVM model, one needs
to solve both the inference problem Eq. (3) and the
loss-augmented inference problem. Without second-
order features, the inference and loss-augmented in-
ference problems can be easily solved, given that
each component can be solved independently by
</bodyText>
<equation confidence="0.990323">
y0� = arg max WTb(t, c,, y) (3)
y∈E(ci)
</equation>
<bodyText confidence="0.999412666666667">
While the inference problem can be solved inde-
pendently, the training algorithm still considers the
whole assignment together in the training procedure.
</bodyText>
<footnote confidence="0.786105666666667">
7In our experiments, we only apply the cohesiveness score
technique on candidates which pass the filtering procedure. See
section 5 for more details for our filtering process.
</footnote>
<table confidence="0.99938">
Data #Tweets #Cand #Men. P@1
Train 473 8212 218 85.3%
Test 1 500 8950 249 87.7%
Test 2 488 7781 332 89.6%
</table>
<tableCaption confidence="0.992224">
Table 2: Labeled example statistics. “#Cand” represents
</tableCaption>
<bodyText confidence="0.900827333333333">
the total number of candidates we found in this dataset.
“#Men.” is the total number of mentions that disam-
biguate to an entity. The top-1 rate (P@1) represents the
proportion of the mentions that disambiguate to the most
linked entity in Wikipedia.
With the second-order features, the inference
problem becomes NP-hard. While one can resort to
using integer linear programming to find the optimal
solution, we choose not to do so. We instead use the
beam search algorithm. Our beam search algorithm
first arranges the candidates from left to right, and
then solve the inference problems approximately.
</bodyText>
<sectionHeader confidence="0.999639" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999950545454546">
We collected unlabeled Twitter data from two re-
sources and then asked human annotators to label
each tweet with a set of entities present. Our anno-
tators ignored the following: duplicate entities per
tweet, ambiguous entity mentions, and entities not
present in Wikipedia. We next describe the two sets
of Twitter data used as our training data and test-
ing data. In addition to these two datasets, we also
randomly sampled another 200 tweets as our devel-
opment set.
Ritter We sampled 473 and 500 tweets8 from the
data used in (Ritter et al., 2011) to be our training
data and test data, respectively. We did not use any
labels generated by (Ritter et al., 2011); our annota-
tors completely re-annotated each tweets with its set
of entities. We refer to the first set as Train and the
second set as Test 1.
Entertainment To check if our system has the
ability to generalize across different domains, we
sampled another 488 tweets related to entertain-
ment entities. Our main focus was to extract
tweets that contained TV shows, Movies, and
</bodyText>
<footnote confidence="0.842218666666667">
8We originally labeled 1000 tweets but then found 27 re-
peated tweets in the dataset. Therefore, we remove those 27
tweets in the training set.
</footnote>
<equation confidence="0.893899333333333">
Rel(e, c|t) =
.
,
</equation>
<page confidence="0.962013">
1025
</page>
<bodyText confidence="0.992754263157895">
Books/Magazines. Identifying tweets from a spe-
cific domain is a research topic on its own, so we
followed (Dalvi et al., 2012), and used a keyword
matching method.9 After sampling this set of tweets,
we asked our annotators to label the data in the same
way as before (all entities are labeled, not just en-
tertainment entities). We refer to this tweet set as
Test 2.
After sampling, all tweets were then normalized
in the following way. First, we removed all retweet
symbols (RT) and special symbols, as these are to-
kens that may easily confuse NER systems. We
treated punctuation as separate tokens. Hashtags (#)
play a very important role in tweets as they often
carry critical information. We used the following
web service10 to break the hashtags into tokens (e.g.,
the service will break “#TheCloneWars” into “the
clone wars”) (Wang et al., 2011).
The statistics of our labeled examples are pre-
sented in Table 2. First, note that the average number
of mentions per tweet is well below 1. In fact, many
tweets are personal conversations and do not carry
any entities that can be linked to Wikipedia. Still,
many candidates are generated (such as “really”) for
those tweets, given that those candidates can still po-
tentially link to an entity (“really” could be a TV
channel). Therefore, it is very important to include
tweets without entities in the training set because we
do not want our system to create unnecessary links
to entities.
Another interesting thing to note is the percent-
age of entity mentions that disambiguate directly to
their most often linked entities in Wikipedia. If we
simply disambiguate each entity mention to its most
linked entity in Wikipedia, we can already achieve
85% to 90% accuracy, if mention detection is per-
fectly accurate. However, mention detection is a dif-
ficult problem as only about 3% of candidates are
valid entity mentions.
It is worthwhile to mention that, as per (Ferragina
and Scaiella, 2010), for computational efficiency,
9We use the following word list :“movie”, “tv”, “episode”,
“film”, “actor”, “actors”, “actress”, “director”, “directors”,
“movies”, “episodes”, “book”, “novel”, “reading”, “read”,
“watch”, “watching”, “show”, “books”, “novels”, “movies”,
“author” and “authors”.
10http://web-ngram.research.microsoft.
com/info/break.html
we apply several preprocessing steps before running
our entity linking system. First, for each anchor in
Wikipedia, we gather all entities it can disambiguate
to and remove from that anchor’s entity set all enti-
ties that are linked less than 2% of the time. Second,
we apply a modified filtering procedure similar to
that proposed in (Ferragina and Scaiella, 2010) to
filter the set of candidates per tweet.
Evaluation Our annotated datasets contain enti-
ties from many Wikipedia categories. For eval-
uation, we primarily focus on entities belonging
to a set of six core categories (Person, Location,
Organization, TV Show, Book/Magazine, Movie).
We believe it is necessary to focus upon core en-
tities, rather than considering all possible entities
in Wikipedia. Most common words in the English
language have their own Wikpedia page, but most
words are not important enough to be considered en-
tities. In general, there is a large degree of subjectiv-
ity when comparing different entity linking datasets;
different researchers have their own interpretation of
what constitutes an entity. For example, we exam-
ined the annotation used in (Meij et al., 2012) and
found it to be extremely lenient, when compared to
our own beliefs of what is an entity. Therefore, we
believe evaluating performance on restricted entity
types is the only fair way to compare different end-
to-end entity linking systems.
We evaluate the performance of our system on
a per-tweet basis, by comparing the set of anno-
tated “gold” entities with the set of entities predicted
by our system, and computing performance metrics
(precision, recall, Fl). We choose to evaluate our
system on a per-tweet basis, as opposed to a per-
entity basis, because we wish to avoid the issue of
matching segmentations. For example, it is quite
common to observe multiple overlapping phrases in
a tweet that should be linked to the same entity (e.g.,
“President Obama” and “Obama”). When evaluat-
ing our system, we compute performance metrics for
both all entities and core entities.11
Parameters In our implementation, we fixed the
regularization parameter C = 10. When beam-
11To decide if an entity is a core entity or not, we use the
following procedure. For the gold entities, the annotators also
annotate type of the entity. We decide the entity type of the
predicted entities using the procedure described in Section 4.1.
</bodyText>
<page confidence="0.969007">
1026
</page>
<table confidence="0.9995878">
Model P Test 1 F1 P Test 2 F1
R R
Cucerzan 64.8 42.2 51.1 64.9 39.7 49.5
TagMe 38.8 69.0 49.7 34.9 70.3 46.7
SSVM 78.8 59.9 68.0 75.0 57.7 65.2
</table>
<tableCaption confidence="0.91166325">
Table 3: Comparisons between different end-to-end en-
tity linking systems. We evaluate performance on core
entities, as it is the only fair way to compare different
systems.
</tableCaption>
<bodyText confidence="0.9990805">
search is used, the beam size is set to be 50, and
we only consider the top 10 candidates for each can-
didate to speed the inference process. In the context
word mining algorithm, r = 0.5% and z = 1000.
</bodyText>
<sectionHeader confidence="0.508935" genericHeader="evaluation">
5.1 Results
</sectionHeader>
<bodyText confidence="0.9994245">
In the following, we analyze the contributions of
each component in our system and compare our final
systems to other existing end-to-end entity linking
systems.
System Comparison We compare our final sys-
tem to other state-of-the-art systems in Table 3.
CUCERZAN represents a modified implementation
of the system in (Cucerzan, 2007). TagMe is an end-
to-end linking system that focuses on short texts,
including tweets. Our system significantly outper-
forms these two systems in both precision and re-
call. Note that CUCERZAN’s system is a state-of-
the-art system on well-written documents with pro-
vided entity mentions. The system (Cucerzan, 2007)
has been extended by the authors and won the TAC
KBP competition in 2010 (Ji et al., 2010).
There are two possible reasons to explain why our
system outperforms CUCERZAN. First, their men-
tion detection is a carefully designed system targeted
toward documents, not tweets. Their system has seg-
mentation issues when applied to Twitter, as it relies
heavily upon capitalization when identifying candi-
date entity mentions. Second, their system heav-
ily depends on the fact that related entities should
appear together within documents. However, given
that tweets are very short, some of their most impor-
tant features are not suitable for the Twitter domain.
Our system outperforms TagMe because we use a
more sophisticated machine learning approach, as
compared to their system. TagMe links too many
</bodyText>
<table confidence="0.99982">
Structural SVM Test 1 Test 2
All Core All Core
Base 35.9 42.9 47.7 52.5
+Cap. Rate 38.4 45.6 49.9 53.7
+Popularity 41.3 47.9 50.3 55.1
+Context Cap 43.7 52.0 50.7 54.8
+Entity Type 47.9 57.0 53.5 59.0
+Tfidf 53.2 63.1 56.8 61.9
</table>
<tableCaption confidence="0.987702">
Table 4: Feature Study: Fl for entity linking perfor-
mance. “All” means evaluation on all annotated entities.
“Core” means evaluation only on our six entity types.
Each row contains all additional features of the row above
it.
</tableCaption>
<bodyText confidence="0.997270161290323">
spurious entity mentions for common words. This is
a result of their algorithm’s over-emphasis on entity-
entity co-occurrence features.
Feature Study We study the contributions of each
feature group in our system in Table 4. We summa-
rize our discoveries as follows:
First, we find collecting statistics from a large cor-
pus helps the system significantly. In addition to
P(els), we find that capitalization rate features of-
fer around 3% to 4% F1 improvement in Test 1.
Similarly, popularity features are also important, as
it corrects bias existing in Wikipedia link statistics.
Compared to lexical features, using statistical fea-
tures offers a great advantage of reducing the need
for large amounts of labeled data.
We also find entity related features (Popularity,
Entity Type, Tf-idf) are crucial. Given that between
85% to 90% of our mentions should directly disam-
biguate to the most often linked entities, one might
think entity-specific features are not important in
our task. Interestingly, entity-specific features are
among the most important features. The discovery
confirms our hypothesis: it is critical to consider
mention detection and entity disambiguation as a
single problem, rather than as separate problems in
a two staged approach used by many other entity
linking systems. Note that capitalization rate and
context capitalization features are mention-specific.
Additionally, we find that mixing mention-specific
features and entity-specific features results in a bet-
ter model.
</bodyText>
<page confidence="0.971074">
1027
</page>
<table confidence="0.9997608">
Entity Type Words appearing before Words appearing after the
the mention mention
Person wr, dominating, rip, quar- tarde, format, noite, suf-
terback, singer, featuring, fers, dire, admits, sen-
defender, rb, minister, ac- ators, urges, performs,
tress, twitition, secretary joins
TV Show sbs, assistir, assistindo, skit, performances,
otm, watching, nw, premieres, finale, par-
watchn, viagra, watchin, ody, marathon, season,
ver episodes, spoilers, sketch
</table>
<tableCaption confidence="0.981306166666667">
Table 5: An example of context words that are automati-
cally extracted from 20 million unlabeled tweets. For the
sake of brevity, we only display context words for two
categories. Note that there are misspelled words (such
as “watchn”) and abbreviations (such as nw) that do not
appear in well-written documents.
</tableCaption>
<table confidence="0.999912833333333">
Advance Models Test 1 Test 2
All Core All Core
SSVM (Table 4) 53.2 63.1 56.8 61.9
+Context 53.9 64.6 58.6 63.4
+Cohesiveness 55.6 66.5 59.7 65.1
+2nd order 58.1 68.0 60.6 65.2
</table>
<tableCaption confidence="0.991721">
Table 6: Evaluation results (Fl) of the advanced models.
</tableCaption>
<bodyText confidence="0.969079510638298">
“+ Context” is the model that uses additional context fea-
tures extracted from 20 millions unlabeled tweets. “+ Co-
hesiveness” is the model with both additional context and
cohesiveness features. “+2nd order” is our final model
(which incorporates context, cohesiveness, and second-
order features).
Mining Context Words We verify the effective-
ness of adding contextual features that are extracted
automatically from large unlabeled data. We apply
our system (with all first-order features) on a set of
20 million unlabeled tweets we collected. Context
words are then extracted using the simple rules de-
scribed in Section 4. We list the top 10 words we
extracted in Table 5. Due to space limitations, we
only list the words for the Person and TV Show cat-
egories. The results are interesting as we are able
to find common misspelled words and abbreviations
used in Twitter. For example, we find that “watchn”
means “watching” and “nw” means “now watching,”
and they are usually words found before TV shows.
We also find tweeters frequently use abbreviations
for people’s jobs. For example, “wr” means “wide
receiver” and “rb” means “running back.” When
mined context is added into our system, the perfor-
mance improves significantly (Table 6). We note
that extending context mining algorithms in a large-
scale, principled approach is an important next re-
search topic.
Capturing Entity-Entity Relationships In this
paper, we use two methods to capture the relation-
ship between entities: adding the cohesiveness score
and using second order information. Until now, we
only considered features that can be extracted from
only one entity. Past research has shown that consid-
ering features that involve multiple entities can im-
prove entity linking performance, given that related
entities are more likely to appear together in a doc-
ument. When these type of features are added, we
need to perform beamsearch, as the exact inference
procedure can be prohibitively expensive.
As displayed in Table 6, we find that either adding
the cohesiveness score or using second order infor-
mation can improve prediction. Using both methods
improves the model even more. Comparing compu-
tation overhead, computing cohesiveness is signifi-
cantly more cost-effective than using second-order
information.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999907416666667">
In this paper, we propose a structural SVM method
to address the problem of end-to-end entity linking
on Twitter. By considering mention detection and
entity disambiguation together, we build a end-to-
end entity linking system that outperforms current
state-of-the-art systems.
There are plenty of research problems left to be
addressed. Developing a better algorithm for min-
ing contextual words is an important research topic.
It would also be interesting to design a method that
jointly learns NER models and entity linking mod-
els.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998440444444445">
S. Asur and B.A. Huberman. 2010. Predicting the future
with social media. arXiv preprint arXiv:1003.5699.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010. Structured output learning with indirect super-
vision. In Proceedings of the International Conference
on Machine Learning (ICML).
R.L. Cilibrasi and P.M.B. Vitanyi. 2007. The google
similarity distance. Knowledge and Data Engineering,
IEEE Transactions on, 19(3):370–383.
</reference>
<page confidence="0.918251">
1028
</page>
<reference confidence="0.999657141509434">
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
the 2007 Joint Conference of EMNLP-CoNLL, pages
708–716.
N. Dalvi, R. Kumar, and B. Pang. 2012. Object match-
ing in tweets with spatial models. In Proceedings of
the fifth ACM international conference on Web search
and data mining, WSDM ’12, pages 43–52, New York,
NY, USA. ACM.
A. Davis, A. Veloso, A. S. da Silva, W. Meira, Jr., and
A. H. F. Laender. 2012. Named entity disambiguation
in streaming data. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 815–824, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
G. Demartini, D. E. Difallah, and P. Cudr´e-Mauroux.
2012. Zencrowd: leveraging probabilistic reasoning
and crowdsourcing techniques for large-scale entity
linking. In The International World Wide Web Con-
ference, pages 469–478, New York, NY, USA. ACM.
P. Ferragina and U. Scaiella. 2010. Tagme: on-the-fly
annotation of short text fragments (by wikipedia enti-
ties). In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, CIKM ’10, pages 1625–1628, New York, NY,
USA. ACM.
X. Han and L. Sun. 2011. A generative entity-mention
model for linking entities with knowledge base. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages 945–
954, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: a graph-based method. In Pro-
ceedings of the 34th international ACM SIGIR con-
ference on Research and development in Information
Retrieval, SIGIR ’11, pages 765–774, New York, NY,
USA. ACM.
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. Ellis.
2010. Overview of the tac 2010 knowledge base pop-
ulation track. In Proceedings of the TAC 2010 Work-
shop.
H. Ji, R. Grishman, and Dang. 2011. Overview of the tac
2011 knowledge base population track. In Proceed-
ings of the TAC 2011 Workshop.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, Proceedings
of International Conference on Knowledge Discovery
and Data Mining (KDD), pages 457–466, New York,
NY, USA. ACM.
C. Li, J. Weng, Q. He, Y. Yao, A. Datta, A. Sun, and B.-S.
Lee. 2012. Twiner: named entity recognition in tar-
geted twitter stream. In Proceedings of the 35th inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, Proceedings of In-
ternational Conference on Research and Development
in Information Retrieval, SIGIR, pages 721–730, New
York, NY, USA. ACM.
E. Meij, W. Weerkamp, and M. de Rijke. 2012. Adding
semantics to microblog posts. In Proceedings of the
fifth ACM international conference on Web search and
data mining, pages 563–572, New York, NY, USA.
ACM.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In Proceedings
of ACM Conference on Information and Knowledge
Management (CIKM), pages 233–242. ACM.
D. Milne and I. H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of ACM Conference
on Information and Knowledge Management (CIKM),
pages 509–518, New York, NY, USA. ACM.
M.J. Paul and M. Dredze. 2011. You are what you tweet:
Analyzing twitter for public health. In Fifth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM 2011).
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1375–1384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empirical
Methods for Natural Language Processing (EMNLP),
pages 1524–1534, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Sadilek, H. Kautz, and V. Silenzio. 2012. Model-
ing spread of disease from social interactions. In Sixth
AAAI International Conference on Weblogs and Social
Media (ICWSM).
A. Sil, E. Cronin, P. Nie, Y. Yang, A.-M. Popescu,
and A. Yates. 2012. Linking named entities to
any database. In Proceedings of the Conference on
Empirical Methods for Natural Language Processing
(EMNLP), pages 116–127, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
K. Starbird and L. Palen. 2012. (how) will the revolu-
tion be retweeted?: information diffusion and the 2011
egyptian uprising. In Proceedings of the acm 2012
conference on computer supported cooperative work,
pages 7–16. ACM.
</reference>
<page confidence="0.878581">
1029
</page>
<reference confidence="0.999862142857143">
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-margin
markov networks. In The Conference on Advances in
Neural Information Processing Systems (NIPS).
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6:1453–1484, September.
A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M.
Welpe. 2010. Predicting elections with twitter: What
140 characters reveal about political sentiment. In
Proceedings of the fourth international aaai confer-
ence on weblogs and social media, pages 178–185.
K. Wang, C. Thrasher, and B.J.P. Hsu. 2011. Web scale
nlp: a case study on url word breaking. In The Inter-
national World Wide Web Conference, pages 357–366.
ACM.
C. Wang, K. Chakrabarti, T. Cheng, and S. Chaudhuri.
2012. Targeted disambiguation of ad-hoc, homoge-
neous sets of named entities. In The International
World Wide Web Conference, pages 719–728, New
York, NY, USA. ACM.
</reference>
<page confidence="0.990157">
1030
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.383766">
<title confidence="0.847284">To Link or Not to Link? A Study on End-to-End Tweet Entity Linking</title>
<author confidence="0.992769">Stephen Guo Ming-Wei Chang Emre Kıcıman</author>
<affiliation confidence="0.996505">Stanford University Microsoft</affiliation>
<abstract confidence="0.9989684">Information extraction from microblog posts is an important task, as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world. As the core component of information extraction, we consider the task of Twitter entity linking in this paper. In the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems. However, in our task, we find that mention detection is often the performance bottleneck. The reason is that messages on micro-blogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings. To rigorously address the Twitter entity linking problem, we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task. By combining structural learning and a variety of firstorder, second-order, and context-sensitive features, our system is able to outperform exist-</abstract>
<intro confidence="0.566494">ing state-of-the art entity linking systems by</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Asur</author>
<author>B A Huberman</author>
</authors>
<title>Predicting the future with social media. arXiv preprint arXiv:1003.5699.</title>
<date>2010</date>
<contexts>
<context position="2014" citStr="Asur and Huberman, 2010" startWordPosition="306" endWordPosition="309">ng the largest volume ever recorded of fine-grained discussions spanning a huge breadth of topics, from the mundane to the historic. The micro-blogging service Twitter reports that it alone captures over 340M short messages, or tweets, per day.1 From such micro-blogging services’ data streams, researchers have reported mining insights about a variety of domains, from election results (Tumasjan et al., 2010) and democracy movements (Starbird and Palen, 2012) to health issues and disease spreading (Paul and Dredze, 2011; Sadilek et al., 2012), as well as tracking product feedback and sentiment (Asur and Huberman, 2010). A critical step in mining information from a micro-blogging service, such as Twitter, is the identification of entities in tweets. In order to mine the relationship between drugs, symptoms and sideeffects, or track the popularity of politicians or sentiment about social issues, we must first be able to identify the topics and specific entities being discussed. The challenge is that messages on microblogs are short, noisy, and informal texts with little context, and often contain phrases with ambiguous meanings. For example, “one day” may be either a set phrase or a reference to a movie. Give</context>
</contexts>
<marker>Asur, Huberman, 2010</marker>
<rawString>S. Asur and B.A. Huberman. 2010. Predicting the future with social media. arXiv preprint arXiv:1003.5699.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>V Srikumar</author>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Structured output learning with indirect supervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="12292" citStr="Chang et al., 2010" startWordPosition="2027" endWordPosition="2030">our framework, we use structural learning as a tool to capture the relationship between entities. We define yz as the output for cz, where yz E E(cz). Let T = |U(t) |and y = {y1, y2, ... , yT}. The feature function for the whole assignment can be written as -b(t, U(t), y). The score for the assignment y can be obtained as the linear product between the weight vector w and the feature vector. For an input example, the prediction can be found by solving the 1022 inference problem: y� = arg max wTb(t, U(t),y) (1) Y We use a Structural SVM (SSVM) (Taskar et al., 2004; Tsochantaridis et al., 2005; Chang et al., 2010) as our learning algorithm. To train the weight vector w, we minimize the objective function of the SSVM �2 (2) i where l is the number of labeled examples and wT b(ti, c(ti), yi) ≥A(yi, y) + wTb(ti, c(ti), y) −�i, ∀i, y We denote yi as the gold assignment for xi and define A(yi, y) as the Hamming distance between two assignments yi and y. 4.1 Features Feature definitions are very important as they define the shapes of the structures. Our feature vector is defined as b(t, U(t),y) = I: 0(t, ci, yi)+ I: 0(t, ci, yi, cj, yj) i i&lt;j where ci and cj is the i-th and j-th candidates in U(t), respectiv</context>
</contexts>
<marker>Chang, Srikumar, Goldwasser, Roth, 2010</marker>
<rawString>M. Chang, V. Srikumar, D. Goldwasser, and D. Roth. 2010. Structured output learning with indirect supervision. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Cilibrasi</author>
<author>P M B Vitanyi</author>
</authors>
<title>The google similarity distance.</title>
<date>2007</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="17247" citStr="Cilibrasi and Vitanyi, 2007" startWordPosition="2870" endWordPosition="2873">weet t. We include a second feature that represents the average tf-idf score of all words that appear in both e and t. Second-order features We include four very simple second-order features 0(t, ci, ei, cj, ej) to capture more complex relations between entities and candidates. The first feature is the Jaccard distance between two Wikipedia pages ei and ej. Let Γ(ei) denote the set of Wikipedia pages that contain a hyperlink to ei. We define the Jaccard distance between ei and ej as: Γ(ei) n Γ(e Jac(ei, ej) = |Γ(ei) U Γ(ej)| This feature has a similar effect as the normalized Google distance (Cilibrasi and Vitanyi, 2007), which has been used for many entity linking systems. Let us use the following shorthand: si = s(ci) and sj = s(cj). We have also included three features P(ei|si)P(ej|sj),Pc(si)Pc(sj) and Pl(si)Pl(sj) to increase the expressivity of our model. 4.2 Mining Additional Contextual Words Unlike mention detection systems used in other NLP tasks, there are no lexical features in our system. Lexical features are important as they can capture semantic meaning precisely. However, given that we do not have many labeled examples, lexical features can lead to overfitting. The diverse language in tweets als</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2007</marker>
<rawString>R.L. Cilibrasi and P.M.B. Vitanyi. 2007. The google similarity distance. Knowledge and Data Engineering, IEEE Transactions on, 19(3):370–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference of EMNLP-CoNLL,</booktitle>
<pages>708--716</pages>
<contexts>
<context position="5732" citStr="Cucerzan, 2007" startWordPosition="902" endWordPosition="903">s challenging. Knowing “The Town” is the name of a recent movie helps, and we can we be more confident if we know that Ben Affleck is an actor in the movie, and Gigli is another of his movies. To train and evaluate our system, we created three separate annotated data sets of approximately 500 tweets each. These data sets are hand annotated with entity links to Wikipedia. We evaluate our system by comparing its performance at detecting en2What we mean here is “the most linked entity”. See Section 3 for details. tities to the performance of two state-of-the-art entity linking systems, Cucerzan (Cucerzan, 2007) and TagMe (Ferragina and Scaiella, 2010), and find that our system outperforms them significantly by 15% in absolute F1. The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results. 2 Related Work Building an entity linking system requires solving two interrelated sub-problems: mention detection and entity disambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses s</context>
<context position="7103" citStr="Cucerzan, 2007" startWordPosition="1120" endWordPosition="1121">ystem. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences between our work and each of these systems. Some systems (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Sun, 2011) heavily depend on Wikipedia text and might not work well in short and noisy tweets. Many systems (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Ferragina and Scaiella, 2010) treat mention detection and entity disambiguation as two different problems. (Meij et al., 2012) is the most related to our paper. While their system also considers mention detection and entity disambiguation together, they do not consider entityto-entity relationships and do not incorporate contextual words from tweets. An area of work closely related to the mention detection problem is the Named Entity Recognition (NER) problem, the identification of textual phrases which belong to core categories (Person, Location, Organization). It is</context>
<context position="29421" citStr="Cucerzan, 2007" startWordPosition="4911" endWordPosition="4912"> entities, as it is the only fair way to compare different systems. search is used, the beam size is set to be 50, and we only consider the top 10 candidates for each candidate to speed the inference process. In the context word mining algorithm, r = 0.5% and z = 1000. 5.1 Results In the following, we analyze the contributions of each component in our system and compare our final systems to other existing end-to-end entity linking systems. System Comparison We compare our final system to other state-of-the-art systems in Table 3. CUCERZAN represents a modified implementation of the system in (Cucerzan, 2007). TagMe is an endto-end linking system that focuses on short texts, including tweets. Our system significantly outperforms these two systems in both precision and recall. Note that CUCERZAN’s system is a state-ofthe-art system on well-written documents with provided entity mentions. The system (Cucerzan, 2007) has been extended by the authors and won the TAC KBP competition in 2010 (Ji et al., 2010). There are two possible reasons to explain why our system outperforms CUCERZAN. First, their mention detection is a carefully designed system targeted toward documents, not tweets. Their system has</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Joint Conference of EMNLP-CoNLL, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dalvi</author>
<author>R Kumar</author>
<author>B Pang</author>
</authors>
<title>Object matching in tweets with spatial models.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12,</booktitle>
<pages>43--52</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="24009" citStr="Dalvi et al., 2012" startWordPosition="4025" endWordPosition="4028">ets with its set of entities. We refer to the first set as Train and the second set as Test 1. Entertainment To check if our system has the ability to generalize across different domains, we sampled another 488 tweets related to entertainment entities. Our main focus was to extract tweets that contained TV shows, Movies, and 8We originally labeled 1000 tweets but then found 27 repeated tweets in the dataset. Therefore, we remove those 27 tweets in the training set. Rel(e, c|t) = . , 1025 Books/Magazines. Identifying tweets from a specific domain is a research topic on its own, so we followed (Dalvi et al., 2012), and used a keyword matching method.9 After sampling this set of tweets, we asked our annotators to label the data in the same way as before (all entities are labeled, not just entertainment entities). We refer to this tweet set as Test 2. After sampling, all tweets were then normalized in the following way. First, we removed all retweet symbols (RT) and special symbols, as these are tokens that may easily confuse NER systems. We treated punctuation as separate tokens. Hashtags (#) play a very important role in tweets as they often carry critical information. We used the following web service</context>
</contexts>
<marker>Dalvi, Kumar, Pang, 2012</marker>
<rawString>N. Dalvi, R. Kumar, and B. Pang. 2012. Object matching in tweets with spatial models. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12, pages 43–52, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Davis</author>
<author>A Veloso</author>
<author>A S da Silva</author>
<author>W Meira</author>
<author>A H F Laender</author>
</authors>
<title>Named entity disambiguation in streaming data.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>815--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6223" citStr="Davis et al., 2012" startWordPosition="973" endWordPosition="976"> See Section 3 for details. tities to the performance of two state-of-the-art entity linking systems, Cucerzan (Cucerzan, 2007) and TagMe (Ferragina and Scaiella, 2010), and find that our system outperforms them significantly by 15% in absolute F1. The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results. 2 Related Work Building an entity linking system requires solving two interrelated sub-problems: mention detection and entity disambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses solely upon the entity linking problem. The entity linking systems of these studies assume that entity mentions are provided by a separate mention detection system. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a</context>
</contexts>
<marker>Davis, Veloso, Silva, Meira, Laender, 2012</marker>
<rawString>A. Davis, A. Veloso, A. S. da Silva, W. Meira, Jr., and A. H. F. Laender. 2012. Named entity disambiguation in streaming data. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 815–824, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Demartini</author>
<author>D E Difallah</author>
<author>P Cudr´e-Mauroux</author>
</authors>
<title>Zencrowd: leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking.</title>
<date>2012</date>
<booktitle>In The International World Wide Web Conference,</booktitle>
<pages>469--478</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Demartini, Difallah, Cudr´e-Mauroux, 2012</marker>
<rawString>G. Demartini, D. E. Difallah, and P. Cudr´e-Mauroux. 2012. Zencrowd: leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking. In The International World Wide Web Conference, pages 469–478, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ferragina</author>
<author>U Scaiella</author>
</authors>
<title>Tagme: on-the-fly annotation of short text fragments (by wikipedia entities).</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM ’10,</booktitle>
<pages>1625--1628</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5773" citStr="Ferragina and Scaiella, 2010" startWordPosition="906" endWordPosition="909"> Town” is the name of a recent movie helps, and we can we be more confident if we know that Ben Affleck is an actor in the movie, and Gigli is another of his movies. To train and evaluate our system, we created three separate annotated data sets of approximately 500 tweets each. These data sets are hand annotated with entity links to Wikipedia. We evaluate our system by comparing its performance at detecting en2What we mean here is “the most linked entity”. See Section 3 for details. tities to the performance of two state-of-the-art entity linking systems, Cucerzan (Cucerzan, 2007) and TagMe (Ferragina and Scaiella, 2010), and find that our system outperforms them significantly by 15% in absolute F1. The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results. 2 Related Work Building an entity linking system requires solving two interrelated sub-problems: mention detection and entity disambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses solely upon the entity linking problem. Th</context>
<context position="7158" citStr="Ferragina and Scaiella, 2010" startWordPosition="1126" endWordPosition="1129">entifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences between our work and each of these systems. Some systems (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Sun, 2011) heavily depend on Wikipedia text and might not work well in short and noisy tweets. Many systems (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Ferragina and Scaiella, 2010) treat mention detection and entity disambiguation as two different problems. (Meij et al., 2012) is the most related to our paper. While their system also considers mention detection and entity disambiguation together, they do not consider entityto-entity relationships and do not incorporate contextual words from tweets. An area of work closely related to the mention detection problem is the Named Entity Recognition (NER) problem, the identification of textual phrases which belong to core categories (Person, Location, Organization). It is well-known that NER systems trained on well-written do</context>
<context position="20215" citStr="Ferragina and Scaiella, 2010" startWordPosition="3394" endWordPosition="3397"> R is the entity type for the entity e. 5The tag ti belongs to the entity type R if our system links a candidate c to an entity with type R and c covers the word wi. 6The word “watching” can be a TV show while most of the time it is not. These common makes this contextual feature noisy. We found that the context feature can only be reliably applied when the candidate is capitalized. 1024 4.3 Cohesiveness Score There are several ways to consider entity-entity cohesiveness besides using the second-order features directly. In our model, we also consider a modified cohesiveness score proposed in (Ferragina and Scaiella, 2010). The idea behind the cohesiveness score is to estimate the correlations between different entities by using weighted Jaccard scores.7 There are two rounds in the procedure of computing the cohesiveness score. We first estimate approximately the most probable entity for each candidate given all the other candidates in the same tweet. In the second round, the cohesiveness score is then produced with respect to the most probable entity computed in the first round. More formally, in the first round, we compute the relevance score for each candidate and entity pair: Ecl=ft Eel∈E(cl) P(e0|c0)Jac(e,</context>
<context position="25826" citStr="Ferragina and Scaiella, 2010" startWordPosition="4331" endWordPosition="4334"> include tweets without entities in the training set because we do not want our system to create unnecessary links to entities. Another interesting thing to note is the percentage of entity mentions that disambiguate directly to their most often linked entities in Wikipedia. If we simply disambiguate each entity mention to its most linked entity in Wikipedia, we can already achieve 85% to 90% accuracy, if mention detection is perfectly accurate. However, mention detection is a difficult problem as only about 3% of candidates are valid entity mentions. It is worthwhile to mention that, as per (Ferragina and Scaiella, 2010), for computational efficiency, 9We use the following word list :“movie”, “tv”, “episode”, “film”, “actor”, “actors”, “actress”, “director”, “directors”, “movies”, “episodes”, “book”, “novel”, “reading”, “read”, “watch”, “watching”, “show”, “books”, “novels”, “movies”, “author” and “authors”. 10http://web-ngram.research.microsoft. com/info/break.html we apply several preprocessing steps before running our entity linking system. First, for each anchor in Wikipedia, we gather all entities it can disambiguate to and remove from that anchor’s entity set all entities that are linked less than 2% of</context>
</contexts>
<marker>Ferragina, Scaiella, 2010</marker>
<rawString>P. Ferragina and U. Scaiella. 2010. Tagme: on-the-fly annotation of short text fragments (by wikipedia entities). In Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM ’10, pages 1625–1628, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Han</author>
<author>L Sun</author>
</authors>
<title>A generative entity-mention model for linking entities with knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>945--954</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6303" citStr="Han and Sun, 2011" startWordPosition="989" endWordPosition="992">ity linking systems, Cucerzan (Cucerzan, 2007) and TagMe (Ferragina and Scaiella, 2010), and find that our system outperforms them significantly by 15% in absolute F1. The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results. 2 Related Work Building an entity linking system requires solving two interrelated sub-problems: mention detection and entity disambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses solely upon the entity linking problem. The entity linking systems of these studies assume that entity mentions are provided by a separate mention detection system. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences between our work and each of these systems. Some systems (Milne</context>
</contexts>
<marker>Han, Sun, 2011</marker>
<rawString>X. Han and L. Sun. 2011. A generative entity-mention model for linking entities with knowledge base. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 945– 954, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Han</author>
<author>L Sun</author>
<author>J Zhao</author>
</authors>
<title>Collective entity linking in web text: a graph-based method.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR ’11,</booktitle>
<pages>765--774</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6322" citStr="Han et al., 2011" startWordPosition="993" endWordPosition="996">, Cucerzan (Cucerzan, 2007) and TagMe (Ferragina and Scaiella, 2010), and find that our system outperforms them significantly by 15% in absolute F1. The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results. 2 Related Work Building an entity linking system requires solving two interrelated sub-problems: mention detection and entity disambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses solely upon the entity linking problem. The entity linking systems of these studies assume that entity mentions are provided by a separate mention detection system. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences between our work and each of these systems. Some systems (Milne and Witten, 2008; </context>
</contexts>
<marker>Han, Sun, Zhao, 2011</marker>
<rawString>X. Han, L. Sun, and J. Zhao. 2011. Collective entity linking in web text: a graph-based method. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR ’11, pages 765–774, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
<author>H T Dang</author>
<author>K Griffitt</author>
<author>J Ellis</author>
</authors>
<title>Overview of the tac 2010 knowledge base population track.</title>
<date>2010</date>
<booktitle>In Proceedings of the TAC 2010 Workshop.</booktitle>
<contexts>
<context position="10758" citStr="Ji et al., 2010" startWordPosition="1753" endWordPosition="1756"> phrases which can possibly be entity mentions. Our system then performs filtering and optimization to process the list of candidates. For each candidate, our system links the candidate to a special NIL token or links the candidate to its corresponding entity in Wikipedia. More formally, given a tweet t and its candidate set U(t), the goal of the system is to predict yz E E(cz), bcz E U(t). Comparison to the TAC KBP Competition It is important to state that our definition of the entity linking problem differs significantly from the entity linking problem as defined by the TAC KBP competition (Ji et al., 2010; Ji et al., 2011). In the TAC, there is no true mention detection problem; every candidate in the TAC is an entity mention that represents an entity. Another difference is that the TAC allows for an entity mention to map to an entity not in the external knowledge base (Wikipedia); our system does not provide special handling of this case. Comparison to Named Entity Recognition There are also important differences between our task and the canonical NER task. For example, NER systems identify common names, such as “Robert,” as entities. In our task, we only consider a prediction as a success if</context>
<context position="29823" citStr="Ji et al., 2010" startWordPosition="4976" endWordPosition="4979">xisting end-to-end entity linking systems. System Comparison We compare our final system to other state-of-the-art systems in Table 3. CUCERZAN represents a modified implementation of the system in (Cucerzan, 2007). TagMe is an endto-end linking system that focuses on short texts, including tweets. Our system significantly outperforms these two systems in both precision and recall. Note that CUCERZAN’s system is a state-ofthe-art system on well-written documents with provided entity mentions. The system (Cucerzan, 2007) has been extended by the authors and won the TAC KBP competition in 2010 (Ji et al., 2010). There are two possible reasons to explain why our system outperforms CUCERZAN. First, their mention detection is a carefully designed system targeted toward documents, not tweets. Their system has segmentation issues when applied to Twitter, as it relies heavily upon capitalization when identifying candidate entity mentions. Second, their system heavily depends on the fact that related entities should appear together within documents. However, given that tweets are very short, some of their most important features are not suitable for the Twitter domain. Our system outperforms TagMe because </context>
</contexts>
<marker>Ji, Grishman, Dang, Griffitt, Ellis, 2010</marker>
<rawString>H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. Ellis. 2010. Overview of the tac 2010 knowledge base population track. In Proceedings of the TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
<author>Dang</author>
</authors>
<title>Overview of the tac 2011 knowledge base population track.</title>
<date>2011</date>
<booktitle>In Proceedings of the TAC 2011 Workshop.</booktitle>
<contexts>
<context position="10776" citStr="Ji et al., 2011" startWordPosition="1757" endWordPosition="1760">n possibly be entity mentions. Our system then performs filtering and optimization to process the list of candidates. For each candidate, our system links the candidate to a special NIL token or links the candidate to its corresponding entity in Wikipedia. More formally, given a tweet t and its candidate set U(t), the goal of the system is to predict yz E E(cz), bcz E U(t). Comparison to the TAC KBP Competition It is important to state that our definition of the entity linking problem differs significantly from the entity linking problem as defined by the TAC KBP competition (Ji et al., 2010; Ji et al., 2011). In the TAC, there is no true mention detection problem; every candidate in the TAC is an entity mention that represents an entity. Another difference is that the TAC allows for an entity mention to map to an entity not in the external knowledge base (Wikipedia); our system does not provide special handling of this case. Comparison to Named Entity Recognition There are also important differences between our task and the canonical NER task. For example, NER systems identify common names, such as “Robert,” as entities. In our task, we only consider a prediction as a success if the system can de</context>
</contexts>
<marker>Ji, Grishman, Dang, 2011</marker>
<rawString>H. Ji, R. Grishman, and Dang. 2011. Overview of the tac 2011 knowledge base population track. In Proceedings of the TAC 2011 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kulkarni</author>
<author>A Singh</author>
<author>G Ramakrishnan</author>
<author>S Chakrabarti</author>
</authors>
<title>Collective annotation of wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>457--466</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6731" citStr="Kulkarni et al., 2009" startWordPosition="1053" endWordPosition="1056">isambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses solely upon the entity linking problem. The entity linking systems of these studies assume that entity mentions are provided by a separate mention detection system. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences between our work and each of these systems. Some systems (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Sun, 2011) heavily depend on Wikipedia text and might not work well in short and noisy tweets. Many systems (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Ferragina and Scaiella, 2010) treat mention detection and entity disambiguation as two different problems. (Meij et al., 2012) is the most related to our paper. While their system also considers mention</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti. 2009. Collective annotation of wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD), pages 457–466, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Li</author>
<author>J Weng</author>
<author>Q He</author>
<author>Y Yao</author>
<author>A Datta</author>
<author>A Sun</author>
<author>B-S Lee</author>
</authors>
<title>Twiner: named entity recognition in targeted twitter stream.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, Proceedings of International Conference on Research and Development in Information Retrieval, SIGIR,</booktitle>
<pages>721--730</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7943" citStr="Li et al., 2012" startWordPosition="1249" endWordPosition="1252">ion detection and entity disambiguation together, they do not consider entityto-entity relationships and do not incorporate contextual words from tweets. An area of work closely related to the mention detection problem is the Named Entity Recognition (NER) problem, the identification of textual phrases which belong to core categories (Person, Location, Organization). It is well-known that NER systems trained on well-written documents perform very poorly on short, noisy text, such as tweets (Rit1021 ter et al., 2011). There have been a few recent studies proposing Twitter-specific NER systems (Li et al., 2012; Ritter et al., 2011). 3 Preliminaries For performing entity linking on Twitter, we choose Wikipedia as our external knowledge base of entities. Entity We define an entity as a nonambiguous, terminal page (e.g., The Town (the film)) in Wikipedia (i.e., a Wikipedia page that is not a category, disambiguation, list, or redirect page). We define an anchor phrase (surface form) as the textual phrase (e.g., the town) which can potentially link to some entities. We define an entity mention as an anchor phrase and the context (“the town” in the example tweet in Section 1), where its semantic meaning</context>
</contexts>
<marker>Li, Weng, He, Yao, Datta, Sun, Lee, 2012</marker>
<rawString>C. Li, J. Weng, Q. He, Y. Yao, A. Datta, A. Sun, and B.-S. Lee. 2012. Twiner: named entity recognition in targeted twitter stream. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, Proceedings of International Conference on Research and Development in Information Retrieval, SIGIR, pages 721–730, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Meij</author>
<author>W Weerkamp</author>
<author>M de Rijke</author>
</authors>
<title>Adding semantics to microblog posts.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining,</booktitle>
<pages>563--572</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Meij, Weerkamp, de Rijke, 2012</marker>
<rawString>E. Meij, W. Weerkamp, and M. de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 563–572, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>A Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>233--242</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7087" citStr="Mihalcea and Csomai, 2007" startWordPosition="1116" endWordPosition="1119">eparate mention detection system. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences between our work and each of these systems. Some systems (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Sun, 2011) heavily depend on Wikipedia text and might not work well in short and noisy tweets. Many systems (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Ferragina and Scaiella, 2010) treat mention detection and entity disambiguation as two different problems. (Meij et al., 2012) is the most related to our paper. While their system also considers mention detection and entity disambiguation together, they do not consider entityto-entity relationships and do not incorporate contextual words from tweets. An area of work closely related to the mention detection problem is the Named Entity Recognition (NER) problem, the identification of textual phrases which belong to core categories (Person, Location, Orga</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>R. Mihalcea and A. Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In Proceedings of ACM Conference on Information and Knowledge Management (CIKM), pages 233–242. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milne</author>
<author>I H Witten</author>
</authors>
<title>Learning to link with wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>509--518</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6708" citStr="Milne and Witten, 2008" startWordPosition="1049" endWordPosition="1052">n detection and entity disambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses solely upon the entity linking problem. The entity linking systems of these studies assume that entity mentions are provided by a separate mention detection system. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences between our work and each of these systems. Some systems (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Sun, 2011) heavily depend on Wikipedia text and might not work well in short and noisy tweets. Many systems (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Ferragina and Scaiella, 2010) treat mention detection and entity disambiguation as two different problems. (Meij et al., 2012) is the most related to our paper. While their system</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>D. Milne and I. H. Witten. 2008. Learning to link with wikipedia. In Proceedings of ACM Conference on Information and Knowledge Management (CIKM), pages 509–518, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Paul</author>
<author>M Dredze</author>
</authors>
<title>You are what you tweet: Analyzing twitter for public health.</title>
<date>2011</date>
<booktitle>In Fifth International AAAI Conference on Weblogs and Social Media (ICWSM</booktitle>
<contexts>
<context position="1913" citStr="Paul and Dredze, 2011" startWordPosition="289" endWordPosition="292"> by 15% Fl. 1 Introduction Microblogging services, such as Twitter and Facebook, are today capturing the largest volume ever recorded of fine-grained discussions spanning a huge breadth of topics, from the mundane to the historic. The micro-blogging service Twitter reports that it alone captures over 340M short messages, or tweets, per day.1 From such micro-blogging services’ data streams, researchers have reported mining insights about a variety of domains, from election results (Tumasjan et al., 2010) and democracy movements (Starbird and Palen, 2012) to health issues and disease spreading (Paul and Dredze, 2011; Sadilek et al., 2012), as well as tracking product feedback and sentiment (Asur and Huberman, 2010). A critical step in mining information from a micro-blogging service, such as Twitter, is the identification of entities in tweets. In order to mine the relationship between drugs, symptoms and sideeffects, or track the popularity of politicians or sentiment about social issues, we must first be able to identify the topics and specific entities being discussed. The challenge is that messages on microblogs are short, noisy, and informal texts with little context, and often contain phrases with </context>
</contexts>
<marker>Paul, Dredze, 2011</marker>
<rawString>M.J. Paul and M. Dredze. 2011. You are what you tweet: Analyzing twitter for public health. In Fifth International AAAI Conference on Weblogs and Social Media (ICWSM 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
<author>D Downey</author>
<author>M Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1375--1384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6203" citStr="Ratinov et al., 2011" startWordPosition="969" endWordPosition="972">e most linked entity”. See Section 3 for details. tities to the performance of two state-of-the-art entity linking systems, Cucerzan (Cucerzan, 2007) and TagMe (Ferragina and Scaiella, 2010), and find that our system outperforms them significantly by 15% in absolute F1. The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results. 2 Related Work Building an entity linking system requires solving two interrelated sub-problems: mention detection and entity disambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses solely upon the entity linking problem. The entity linking systems of these studies assume that entity mentions are provided by a separate mention detection system. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), b</context>
<context position="15233" citStr="Ratinov et al., 2011" startWordPosition="2531" endWordPosition="2534">tion, we use this as popularity data.3 As mentioned in Section 3, we find that the most often linked Wikipedia articles might not be the most popular ones on Twitter. Using page view statistics helps our system correct this bias. We define another probability based on page view statistics Pv(ei|c) = v(ei)/(EeEE(c)/{NIL} v(e)), where v(e) represents the view count for the page e. Context Capitalization Our context capitalization features indicate if the current candidate, the word before, and the word after the candidate are capitalized. Entity Type and Tf-idf We use the procedure proposed in (Ratinov et al., 2011) to extract keyword phrases from categories for each Wikipedia page, and then build a rule-based system using keyword phrases to classify if each entity page belongs to one of the following entity types: Person, Location, Organization, TV Show, Book/Magazine and Movie.4 For a given candidate c and an entity e, the associated binary feature becomes active if the entity belongs to a specific entity type. There are six entity type features in our system. 3http://dammit.lt/wikistats 4The entity type prediction accuracy of our rule-based system on the development set is around 95%. kwk2 2 + C min W</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>L. Ratinov, D. Roth, D. Downey, and M. Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1375–1384, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Clark</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>1524--1534</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3827" citStr="Ritter et al., 2011" startWordPosition="581" endWordPosition="584"> of an entity in a knowledge base. 1http://blog.twitter.com/2012/03/twitter-turns-six.html 1020 Proceedings of NAACL-HLT 2013, pages 1020–1030, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics While mention detection and entity disambiguation are frequently cast as equally important but distinct and separate problems, we find that mention detection is where today’s systems and our baseline techniques incur the most failures. Detecting the correct entity mention is a significant challenge given mis-capitalizations, incorrect grammar, and ambiguous phrases. In (Ritter et al., 2011), the authors report their system achieves 0.64 to 0.67 F1 on named entity segmentation results with 34K tokens of labeled examples. On the other hand, once the correct entity mention is detected, a trivial disambiguation that maps to the most popular entity2 will achieve 85% accuracy in our set. Our primary contribution in this paper is a recasting and merging of the tasks of mention detection and entity disambiguation into a single endto-end entity linking task. We achieve significant improvements by applying structural learning techniques to jointly optimize the detection and disambiguation</context>
<context position="7965" citStr="Ritter et al., 2011" startWordPosition="1253" endWordPosition="1256"> entity disambiguation together, they do not consider entityto-entity relationships and do not incorporate contextual words from tweets. An area of work closely related to the mention detection problem is the Named Entity Recognition (NER) problem, the identification of textual phrases which belong to core categories (Person, Location, Organization). It is well-known that NER systems trained on well-written documents perform very poorly on short, noisy text, such as tweets (Rit1021 ter et al., 2011). There have been a few recent studies proposing Twitter-specific NER systems (Li et al., 2012; Ritter et al., 2011). 3 Preliminaries For performing entity linking on Twitter, we choose Wikipedia as our external knowledge base of entities. Entity We define an entity as a nonambiguous, terminal page (e.g., The Town (the film)) in Wikipedia (i.e., a Wikipedia page that is not a category, disambiguation, list, or redirect page). We define an anchor phrase (surface form) as the textual phrase (e.g., the town) which can potentially link to some entities. We define an entity mention as an anchor phrase and the context (“the town” in the example tweet in Section 1), where its semantic meaning umambiguously represe</context>
<context position="23227" citStr="Ritter et al., 2011" startWordPosition="3887" endWordPosition="3890">ight, and then solve the inference problems approximately. 5 Experiments We collected unlabeled Twitter data from two resources and then asked human annotators to label each tweet with a set of entities present. Our annotators ignored the following: duplicate entities per tweet, ambiguous entity mentions, and entities not present in Wikipedia. We next describe the two sets of Twitter data used as our training data and testing data. In addition to these two datasets, we also randomly sampled another 200 tweets as our development set. Ritter We sampled 473 and 500 tweets8 from the data used in (Ritter et al., 2011) to be our training data and test data, respectively. We did not use any labels generated by (Ritter et al., 2011); our annotators completely re-annotated each tweets with its set of entities. We refer to the first set as Train and the second set as Test 1. Entertainment To check if our system has the ability to generalize across different domains, we sampled another 488 tweets related to entertainment entities. Our main focus was to extract tweets that contained TV shows, Movies, and 8We originally labeled 1000 tweets but then found 27 repeated tweets in the dataset. Therefore, we remove thos</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 1524–1534, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sadilek</author>
<author>H Kautz</author>
<author>V Silenzio</author>
</authors>
<title>Modeling spread of disease from social interactions.</title>
<date>2012</date>
<booktitle>In Sixth AAAI International Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="1936" citStr="Sadilek et al., 2012" startWordPosition="293" endWordPosition="296">ion Microblogging services, such as Twitter and Facebook, are today capturing the largest volume ever recorded of fine-grained discussions spanning a huge breadth of topics, from the mundane to the historic. The micro-blogging service Twitter reports that it alone captures over 340M short messages, or tweets, per day.1 From such micro-blogging services’ data streams, researchers have reported mining insights about a variety of domains, from election results (Tumasjan et al., 2010) and democracy movements (Starbird and Palen, 2012) to health issues and disease spreading (Paul and Dredze, 2011; Sadilek et al., 2012), as well as tracking product feedback and sentiment (Asur and Huberman, 2010). A critical step in mining information from a micro-blogging service, such as Twitter, is the identification of entities in tweets. In order to mine the relationship between drugs, symptoms and sideeffects, or track the popularity of politicians or sentiment about social issues, we must first be able to identify the topics and specific entities being discussed. The challenge is that messages on microblogs are short, noisy, and informal texts with little context, and often contain phrases with ambiguous meanings. For</context>
</contexts>
<marker>Sadilek, Kautz, Silenzio, 2012</marker>
<rawString>A. Sadilek, H. Kautz, and V. Silenzio. 2012. Modeling spread of disease from social interactions. In Sixth AAAI International Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sil</author>
<author>E Cronin</author>
<author>P Nie</author>
<author>Y Yang</author>
<author>A-M Popescu</author>
<author>A Yates</author>
</authors>
<title>Linking named entities to any database.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>116--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6241" citStr="Sil et al., 2012" startWordPosition="977" endWordPosition="980">etails. tities to the performance of two state-of-the-art entity linking systems, Cucerzan (Cucerzan, 2007) and TagMe (Ferragina and Scaiella, 2010), and find that our system outperforms them significantly by 15% in absolute F1. The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results. 2 Related Work Building an entity linking system requires solving two interrelated sub-problems: mention detection and entity disambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses solely upon the entity linking problem. The entity linking systems of these studies assume that entity mentions are provided by a separate mention detection system. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences b</context>
</contexts>
<marker>Sil, Cronin, Nie, Yang, Popescu, Yates, 2012</marker>
<rawString>A. Sil, E. Cronin, P. Nie, Y. Yang, A.-M. Popescu, and A. Yates. 2012. Linking named entities to any database. In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 116–127, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Starbird</author>
<author>L Palen</author>
</authors>
<title>(how) will the revolution be retweeted?: information diffusion and the 2011 egyptian uprising.</title>
<date>2012</date>
<booktitle>In Proceedings of the acm 2012 conference on computer supported cooperative work,</booktitle>
<pages>7--16</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1851" citStr="Starbird and Palen, 2012" startWordPosition="278" endWordPosition="281">ble to outperform existing state-of-the art entity linking systems by 15% Fl. 1 Introduction Microblogging services, such as Twitter and Facebook, are today capturing the largest volume ever recorded of fine-grained discussions spanning a huge breadth of topics, from the mundane to the historic. The micro-blogging service Twitter reports that it alone captures over 340M short messages, or tweets, per day.1 From such micro-blogging services’ data streams, researchers have reported mining insights about a variety of domains, from election results (Tumasjan et al., 2010) and democracy movements (Starbird and Palen, 2012) to health issues and disease spreading (Paul and Dredze, 2011; Sadilek et al., 2012), as well as tracking product feedback and sentiment (Asur and Huberman, 2010). A critical step in mining information from a micro-blogging service, such as Twitter, is the identification of entities in tweets. In order to mine the relationship between drugs, symptoms and sideeffects, or track the popularity of politicians or sentiment about social issues, we must first be able to identify the topics and specific entities being discussed. The challenge is that messages on microblogs are short, noisy, and infor</context>
</contexts>
<marker>Starbird, Palen, 2012</marker>
<rawString>K. Starbird and L. Palen. 2012. (how) will the revolution be retweeted?: information diffusion and the 2011 egyptian uprising. In Proceedings of the acm 2012 conference on computer supported cooperative work, pages 7–16. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin markov networks.</title>
<date>2004</date>
<booktitle>In The Conference on Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="12242" citStr="Taskar et al., 2004" startWordPosition="2019" endWordPosition="2022">stems. 4 Entity Linking as Structural Learning In our framework, we use structural learning as a tool to capture the relationship between entities. We define yz as the output for cz, where yz E E(cz). Let T = |U(t) |and y = {y1, y2, ... , yT}. The feature function for the whole assignment can be written as -b(t, U(t), y). The score for the assignment y can be obtained as the linear product between the weight vector w and the feature vector. For an input example, the prediction can be found by solving the 1022 inference problem: y� = arg max wTb(t, U(t),y) (1) Y We use a Structural SVM (SSVM) (Taskar et al., 2004; Tsochantaridis et al., 2005; Chang et al., 2010) as our learning algorithm. To train the weight vector w, we minimize the objective function of the SSVM �2 (2) i where l is the number of labeled examples and wT b(ti, c(ti), yi) ≥A(yi, y) + wTb(ti, c(ti), y) −�i, ∀i, y We denote yi as the gold assignment for xi and define A(yi, y) as the Hamming distance between two assignments yi and y. 4.1 Features Feature definitions are very important as they define the shapes of the structures. Our feature vector is defined as b(t, U(t),y) = I: 0(t, ci, yi)+ I: 0(t, ci, yi, cj, yj) i i&lt;j where ci and cj </context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2004</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2004. Max-margin markov networks. In The Conference on Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Joachims</author>
<author>T Hofmann</author>
<author>Y Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1453</pages>
<contexts>
<context position="12271" citStr="Tsochantaridis et al., 2005" startWordPosition="2023" endWordPosition="2026">ng as Structural Learning In our framework, we use structural learning as a tool to capture the relationship between entities. We define yz as the output for cz, where yz E E(cz). Let T = |U(t) |and y = {y1, y2, ... , yT}. The feature function for the whole assignment can be written as -b(t, U(t), y). The score for the assignment y can be obtained as the linear product between the weight vector w and the feature vector. For an input example, the prediction can be found by solving the 1022 inference problem: y� = arg max wTb(t, U(t),y) (1) Y We use a Structural SVM (SSVM) (Taskar et al., 2004; Tsochantaridis et al., 2005; Chang et al., 2010) as our learning algorithm. To train the weight vector w, we minimize the objective function of the SSVM �2 (2) i where l is the number of labeled examples and wT b(ti, c(ti), yi) ≥A(yi, y) + wTb(ti, c(ti), y) −�i, ∀i, y We denote yi as the gold assignment for xi and define A(yi, y) as the Hamming distance between two assignments yi and y. 4.1 Features Feature definitions are very important as they define the shapes of the structures. Our feature vector is defined as b(t, U(t),y) = I: 0(t, ci, yi)+ I: 0(t, ci, yi, cj, yj) i i&lt;j where ci and cj is the i-th and j-th candidat</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. 2005. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tumasjan</author>
<author>T O Sprenger</author>
<author>P G Sandner</author>
<author>I M Welpe</author>
</authors>
<title>Predicting elections with twitter: What 140 characters reveal about political sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the fourth international aaai conference on weblogs and social media,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="1800" citStr="Tumasjan et al., 2010" startWordPosition="271" endWordPosition="274"> and context-sensitive features, our system is able to outperform existing state-of-the art entity linking systems by 15% Fl. 1 Introduction Microblogging services, such as Twitter and Facebook, are today capturing the largest volume ever recorded of fine-grained discussions spanning a huge breadth of topics, from the mundane to the historic. The micro-blogging service Twitter reports that it alone captures over 340M short messages, or tweets, per day.1 From such micro-blogging services’ data streams, researchers have reported mining insights about a variety of domains, from election results (Tumasjan et al., 2010) and democracy movements (Starbird and Palen, 2012) to health issues and disease spreading (Paul and Dredze, 2011; Sadilek et al., 2012), as well as tracking product feedback and sentiment (Asur and Huberman, 2010). A critical step in mining information from a micro-blogging service, such as Twitter, is the identification of entities in tweets. In order to mine the relationship between drugs, symptoms and sideeffects, or track the popularity of politicians or sentiment about social issues, we must first be able to identify the topics and specific entities being discussed. The challenge is that</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M. Welpe. 2010. Predicting elections with twitter: What 140 characters reveal about political sentiment. In Proceedings of the fourth international aaai conference on weblogs and social media, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wang</author>
<author>C Thrasher</author>
<author>B J P Hsu</author>
</authors>
<title>Web scale nlp: a case study on url word breaking.</title>
<date>2011</date>
<booktitle>In The International World Wide Web Conference,</booktitle>
<pages>357--366</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24734" citStr="Wang et al., 2011" startWordPosition="4148" endWordPosition="4151">the data in the same way as before (all entities are labeled, not just entertainment entities). We refer to this tweet set as Test 2. After sampling, all tweets were then normalized in the following way. First, we removed all retweet symbols (RT) and special symbols, as these are tokens that may easily confuse NER systems. We treated punctuation as separate tokens. Hashtags (#) play a very important role in tweets as they often carry critical information. We used the following web service10 to break the hashtags into tokens (e.g., the service will break “#TheCloneWars” into “the clone wars”) (Wang et al., 2011). The statistics of our labeled examples are presented in Table 2. First, note that the average number of mentions per tweet is well below 1. In fact, many tweets are personal conversations and do not carry any entities that can be linked to Wikipedia. Still, many candidates are generated (such as “really”) for those tweets, given that those candidates can still potentially link to an entity (“really” could be a TV channel). Therefore, it is very important to include tweets without entities in the training set because we do not want our system to create unnecessary links to entities. Another i</context>
</contexts>
<marker>Wang, Thrasher, Hsu, 2011</marker>
<rawString>K. Wang, C. Thrasher, and B.J.P. Hsu. 2011. Web scale nlp: a case study on url word breaking. In The International World Wide Web Conference, pages 357–366. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>K Chakrabarti</author>
<author>T Cheng</author>
<author>S Chaudhuri</author>
</authors>
<title>Targeted disambiguation of ad-hoc, homogeneous sets of named entities.</title>
<date>2012</date>
<booktitle>In The International World Wide Web Conference,</booktitle>
<pages>719--728</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6284" citStr="Wang et al., 2012" startWordPosition="985" endWordPosition="988">tate-of-the-art entity linking systems, Cucerzan (Cucerzan, 2007) and TagMe (Ferragina and Scaiella, 2010), and find that our system outperforms them significantly by 15% in absolute F1. The rest of this paper describes related work, our structured learning approach to entity linking, and our experimental results. 2 Related Work Building an entity linking system requires solving two interrelated sub-problems: mention detection and entity disambiguation. The significant portion of recent work in the literature (Ratinov et al., 2011; Davis et al., 2012; Sil et al., 2012; Demartini et al., 2012; Wang et al., 2012; Han and Sun, 2011; Han et al., 2011) focuses solely upon the entity linking problem. The entity linking systems of these studies assume that entity mentions are provided by a separate mention detection system. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences between our work and each of these systems. </context>
</contexts>
<marker>Wang, Chakrabarti, Cheng, Chaudhuri, 2012</marker>
<rawString>C. Wang, K. Chakrabarti, T. Cheng, and S. Chaudhuri. 2012. Targeted disambiguation of ad-hoc, homogeneous sets of named entities. In The International World Wide Web Conference, pages 719–728, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>