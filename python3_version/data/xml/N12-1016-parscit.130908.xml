<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.986414">
Segmentation Similarity and Agreement
</title>
<author confidence="0.996379">
Chris Fournier Diana Inkpen
</author>
<affiliation confidence="0.850842">
University of Ottawa University of Ottawa
Ottawa, ON, Canada Ottawa, ON, Canada
</affiliation>
<email confidence="0.992269">
cfour037@eecs.uottawa.ca diana@eecs.uottawa.ca
</email>
<sectionHeader confidence="0.997266" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997656875">
We propose a new segmentation evaluation
metric, called segmentation similarity (S), that
quantifies the similarity between two segmen-
tations as the proportion of boundaries that
are not transformed when comparing them us-
ing edit distance, essentially using edit dis-
tance as a penalty function and scaling penal-
ties by segmentation size. We propose several
adapted inter-annotator agreement coefficients
which use S that are suitable for segmenta-
tion. We show that S is configurable enough
to suit a wide variety of segmentation evalua-
tions, and is an improvement upon the state of
the art. We also propose using inter-annotator
agreement coefficients to evaluate automatic
segmenters in terms of human performance.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994762962963">
Segmentation is the task of splitting up an item, such
as a document, into a sequence of segments by plac-
ing boundaries within. The purpose of segmenting
can vary greatly, but one common objective is to
denote shifts in the topic of a text, where multiple
boundary types can also be present (e.g., major ver-
sus minor topic shifts). Human-competitive auto-
matic segmentation methods can help a wide range
of computational linguistic tasks which depend upon
the identification of segment boundaries in text.
To evaluate automatic segmentation methods, a
method of comparing an automatic segmenter’s per-
formance against the segmentations produced by hu-
man judges (coders) is required. Current meth-
ods of performing this comparison designate only
one coder’s segmentation as a reference to com-
pare against. A single “true” reference segmentation
from a coder should not be trusted, given that inter-
annotator agreement is often reported to be rather
poor (Hearst, 1997, p. 54). Additionally, to en-
sure that an automatic segmenter does not over-fit
to the preference and bias of one particular coder,
an automatic segmenter should be compared directly
against multiple coders.
The state of the art segmentation evaluation met-
rics (Pk and WindowDiff) slide a window across a
designated reference and hypothesis segmentation,
and count the number of windows where the number
of boundaries differ. Window-based methods suffer
from a variety of problems, including: i) unequal
penalization of error types; ii) an arbitrarily defined
window size parameter (whose choice greatly af-
fects outcomes); iii) lack of clear intuition; iv) in-
applicability to multiply-coded corpora; and v) re-
liance upon a “true” reference segmentation.
In this paper, we propose a new method of
comparing two segmentations, called segmentation
similarity 1 (S), that: i) equally penalizes all er-
ror types (unless explicitly configured otherwise);
ii) appropriately responds to scenarios tested; iii) de-
fines no arbitrary parameters; iv) is intuitive; and
v) is adapted for use in a variety of popular inter-
annotator agreement coefficients to handle multiply-
coded corpora; and vi) does not rely upon a “true”
reference segmentation (it is symmetric). Capitaliz-
ing on the adapted inter-annotator agreement coeffi-
cients, the relative difficulty that human segmenters
have with various segmentation tasks can now be
quantified. We also propose that these coefficients
can be used to evaluate and compare automatic seg-
mentation methods in terms of human agreement.
This paper is organized as follows. In Sec-
tion 2, we review segmentation evaluation and inter-
annotator agreement. In Section 3, we present S and
</bodyText>
<footnote confidence="0.9852775">
1A software implementation of segmentation similarity (S)
is available at http://nlp.chrisfournier.ca/
</footnote>
<page confidence="0.634338666666667">
152
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 152–161,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.99969675">
inter-annotator agreement coefficient adaptations. In
Section 4, we evaluate S and WindowDiff in vari-
ous scenarios and simulations, and upon a multiply-
coded corpus.
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999313">
2.1 Segmentation Evaluation
</subsectionHeader>
<bodyText confidence="0.999970594594594">
Precision, recall, and their mean (F,3-measure) have
been previously applied to segmentation evaluation.
Precision is the proportion of boundaries chosen that
agree with a reference segmentation, and recall is
the proportion of boundaries chosen that agree with
a reference segmentation out of all boundaries in the
reference and hypothesis (Pevzner and Hearst, 2002,
p. 3). For segmentation, these metrics are unsuitable
because they penalize near-misses of boundaries as
full-misses, causing them to drastically overestimate
the error. Near-misses are prevalent in segmentation
and can account for a large proportion of the errors
produced by a coder, and as inter-annotator agree-
ment often shows, they do not reflect coder error,
but the difficulty of the task.
Pk (Beeferman and Berger, 1999, pp. 198–200)2
is a window-based metric which attempts to solve
the harsh near-miss penalization of precision, recall,
and F,3-measure. In Pk, a window of size k, where
k is defined as half of the mean reference segment
size, is slid across the text to compute penalties.
A penalty of 1 is assigned for each window whose
boundaries are detected to be in different segments
of the reference and hypothesis segmentations, and
this count is normalized by the number of windows.
Pevzner and Hearst (2002, pp. 5–10) highlighted
a number of issues with Pk, specifically that: i) False
negatives (FNs) are penalized more than false pos-
itives (FPs); ii) It does not penalize FPs that fall
within k units of a reference boundary; iii) Its sen-
sitivity to variations in segment size can cause it to
linearly decrease the penalty for FPs if the size of
any segments fall below k; and iv) Near-miss errors
are too harshly penalized.
To attempt to mitigate the shortcomings of Pk,
Pevzner and Hearst (2002, p. 10) proposed a
modified metric which changed how penalties were
</bodyText>
<footnote confidence="0.90966">
2Pk is a modification of PN, (Beeferman et al., 1997, p. 43).
Other modifications such as TDT C3e9 (Doddington, 1998, pp.
5–6) have been proposed, but Pk has seen greater usage.
</footnote>
<bodyText confidence="0.99979425">
counted, named WindowDiff (WD). A window of
size k is still slid across the text, but now penal-
ties are attributed to windows where the number of
boundaries in each segmentation differs (see Equa-
tion 1, where b(RZj) and b(HZj) represents the num-
ber of boundaries within the segments in a window
of size k from position i to j, and N the number of
sentences plus one), with the same normalization.
</bodyText>
<equation confidence="0.9838908">
N−k
1
�
WD(R, H) = N − k
z=1,j=z+k
</equation>
<bodyText confidence="0.999795">
WindowDiff is able to reduce, but not eliminate,
sensitivity to segment size, gives more equal weights
to both FPs and FNs (FNs are, in effect, penalized
less3), and is able to catch mistakes in both small
and large segments. It is not without issues though;
Lamprier et al. (2007) demonstrated that WindowD-
iff penalizes errors less at the beginning and end of
a segmentation (this is corrected by padding the seg-
mentation at each end by size k). Additionally, vari-
ations in the window size k lead to difficulties in in-
terpreting and comparing WindowDiff’s values, and
the intuition of the method remains vague.
Franz et al. (2007) proposed measuring perfor-
mance in terms of the number of words that are FNs
and FPs, normalized by the number of word posi-
tions present (see Equation 2).
</bodyText>
<equation confidence="0.91427075">
1 � 1 � FP(w) (2)
RF N = F N(w), RF P = N
N w
w
</equation>
<bodyText confidence="0.99783625">
RFN and RFP have the advantage that they take
into account the severity of an error in terms of seg-
ment size, allowing them to reflect the effects of er-
roneously missing, or added, words in a segment
better than window based metrics. Unfortunately,
RFN and RFP suffer from the same flaw as preci-
sion, recall, and F,3-measure in that they do not ac-
count for near misses.
</bodyText>
<subsectionHeader confidence="0.965387">
2.2 Inter-Annotator Agreement
</subsectionHeader>
<bodyText confidence="0.998138625">
The need to ascertain the agreement and reliabil-
ity between coders for segmentation was recognized
3Georgescul et al. (2006, p. 48) note that both FPs and FNs
are weighted by 1/N−k, and although there are “equiprobable
possibilities to have a [FP] in an interval of k units”, “the total
number of equiprobable possibilities to have a [FN] in an inter-
val of k units is smaller than (N−k)”, making the interpretation
of a full miss as a FN less probable than as a FP.
</bodyText>
<equation confidence="0.996377">
(|b(Rzj)−b(Hzj) |&gt; 0) (1)
</equation>
<page confidence="0.994458">
153
</page>
<bodyText confidence="0.9998275">
by Passonneau and Litman (1993), who adapted the
percentage agreement metric by Gale et al. (1992,
p. 254) for usage in segmentation. This percentage
agreement metric (Passonneau and Litman, 1993, p.
150) is the ratio of the total observed agreement of a
coder with the majority opinion for each boundary
over the total possible agreements. This measure
failed to take into account chance agreement, or to
less harshly penalize near-misses.
Hearst (1997) collected segmentations from 7
coders while developing the automatic segmenter
TextTiling, and reported mean n (Siegel and Castel-
lan, 1988) values for coders and automatic seg-
menters (Hearst, 1997, p. 56). Pairwise mean n
scores were calculated by comparing a coder’s seg-
mentation against a reference segmentation formu-
lated by the majority opinion strategy used in Pas-
sonneau and Litman (1993, p. 150) (Hearst, 1997,
pp. 53–54). Although mean n scores attempt to
take into account chance agreement, near misses are
still unaccounted for, and use of Siegel and Castel-
lan’s (1988) n has declined in favour of other coeffi-
cients (Artstein and Poesio, 2008, pp. 555–556).
Artstein and Poesio (2008) briefly touch upon
recommendations for coefficients for segmentation
evaluation, and though they do not propose a mea-
sure, they do conjecture that a modification of a
weighted form of α (Krippendorff, 1980; Krippen-
dorff, 2004) using unification and WindowDiff may
suffice (Artstein and Poesio, 2008, pp. 580–582).
</bodyText>
<sectionHeader confidence="0.943993" genericHeader="method">
3 Segmentation Similarity
</sectionHeader>
<bodyText confidence="0.9996872">
For discussing segmentation, a segment’s size (or
mass) is measured in units, the error is quantified
in potential boundaries (PBs), and we have adopted
a modified form of the notation used by Artstein and
Poesio (2008), where the set of:
</bodyText>
<listItem confidence="0.999107375">
• Items is {i|i ∈ I} with cardinality i;
• Categories is {k|k ∈ K} with cardinality k;
• Coders is {c|c ∈ C} with cardinality c;
• Segmentations of an item i by a coder c is {s|s ∈
S}, where when sic is specified with only one sub-
script, it denotes sc, for all relevant items (i); and
• Types of segmentation boundaries is {t|t ∈ T} with
cardinality t.
</listItem>
<subsectionHeader confidence="0.998913">
3.1 Sources of Dissimilarity
</subsectionHeader>
<bodyText confidence="0.981073">
Linear segmentation has three main types of errors:
</bodyText>
<listItem confidence="0.920707333333333">
1. s1 contains a boundary that is off by n PBs in s2;
2. s1 contains a boundary that s2 does not; or
3. s2 contains a boundary that s1 does not.
</listItem>
<bodyText confidence="0.999930428571429">
These types of errors can be seen in Figure 1, and
are conceptualized as a pairwise transposition of a
boundary for error 1, and the insertion or deletion
(depending upon your perspective) of a boundary for
errors 2 and 3. Since we do not designate either seg-
mentation as a reference or hypothesis, we refer to
insertions and deletions both as substitutions.
</bodyText>
<figure confidence="0.79378">
1 3 2
</figure>
<figureCaption confidence="0.999879">
Figure 1: Types of segmentations errors
</figureCaption>
<bodyText confidence="0.99996">
It is important to not penalize near misses as full
misses in many segmentation tasks because coders
often agree upon the existence of a boundary, but
disagree upon its exact location. In the previous sce-
nario, assigning a full miss would mean that even a
boundary loosely agreed-upon, as in Figure 1, error
1, would be regarded as completely disagreed-upon.
</bodyText>
<subsectionHeader confidence="0.999461">
3.2 Edit Distance
</subsectionHeader>
<bodyText confidence="0.99996205">
In S, concepts from Damereau-Levenshtein edit dis-
tance (Damereau, 1964; Levenshtein, 1966) are ap-
plied to model segmentation edit distance as two op-
erations: substitutions and transpositions.4 These
two operations represent full misses and near misses,
respectively. Using these two operations, a new
globally-optimal minimum edit distance is applied
to a pair of sequences of sets of boundaries to model
the sources of dissimilarity identified earlier.5
Near misses that are remedied by transposition are
penalized as b PBs of error (where b is the number
of boundaries transposed), as opposed to the 2b PBs
of errors by which they would be penalized if they
were considered to be two separate substitution op-
erations. Transpositions can also be considered over
n &gt; 2 PBs (n-wise transpositions). This is useful
if, for a specific task, near misses of up to n PBs are
not to be penalized as full misses (default n = 2).
The error represented by the two operations can
also be scaled (i.e., weighted) from 1 PB each to a
</bodyText>
<footnote confidence="0.9869755">
4Beeferman et al. (1997, p. 42) briefly mention using an edit
distance without transpositions, but discard it in favour of PN,.
5For multiple boundaries, an add/del operation is added, and
transpositions are considered only within boundary types.
</footnote>
<page confidence="0.924086666666667">
81
82
154
</page>
<bodyText confidence="0.999934">
fraction. The distance over which an n-wise trans-
position occurred can also be used in conjunction
with the scalar operation weighting so that a transpo-
sition is weighted using the function in Equation 3.
</bodyText>
<equation confidence="0.66228">
te(n, b) = b − (1/b)&amp;quot;−2 where n &gt; 2 and b &gt; 0 (3)
</equation>
<bodyText confidence="0.999911333333333">
This transposition error function was chosen so
that, in an n-wise transposition where n = 2 PBs
and the number of boundaries transposed b = 2, the
penalty would be 1 PB, and the maximum penalty as
limn--+oo te(n) would be b PBs, or in this case 2 PBs
(demonstrated later in Figure 5b).
</bodyText>
<subsectionHeader confidence="0.999485">
3.3 Method
</subsectionHeader>
<bodyText confidence="0.9997516875">
In S, we conceptualize the entire segmentation, and
individual segments, as having mass (i.e., unit mag-
nitude/length), and quantify similarity between two
segmentations as the proportion of boundaries that
are not transformed when comparing segmentations
using edit distance, essentially using edit distance as
a penalty function and scaling penalties by segmen-
tation size. S is a symmetric function that quantifies
the similarity between two segmentations as a per-
centage, and applies to any granularity or segmenta-
tion unit (e.g., paragraphs, sentences, clauses, etc.).
Consider a somewhat contrived example
containing–for simplicity and brevity–only one
boundary type (t = 1). First, a segmentation must
be converted into a sequence of segment mass
values (see Figure 2).
</bodyText>
<figureCaption confidence="0.998833">
Figure 2: Annotation of segmentation mass
</figureCaption>
<bodyText confidence="0.9998138">
Then, a pair of segmentations are converted into
parallel sequences of boundary sets, where each set
contains the types of boundaries present at that po-
tential boundary location (if there is no boundary
present, then the set is empty), as in Figure 3.
</bodyText>
<equation confidence="0.9239455">
{1} {} {1} {} {1} {} {} {1} {} {} {1} {1} {}
s1 1 2 2 3 3 1 2
s2 1 2 1 2 6 2
{1} {} {1} {1} {} {1} {} {} {} {} {} {1} {}
</equation>
<figureCaption confidence="0.997273">
Figure 3: Segmentations annotated with mass and their
corresponding boundary set sequences
</figureCaption>
<bodyText confidence="0.999193933333334">
The edit distance is calculated by first identify-
ing all potential substitution operations that could
occur (in this case 5). A search for all potential n-
wise transpositions that can be made over n adja-
cent sets between the sequences is then performed,
searching from the beginning of the sequence to the
end, keeping only those transpositions which do not
overlap and which result in transposing the most
boundaries between the sequences (to minimize the
edit distance). In this case, we have only one non-
overlapping 2-wise transposition. We then subtract
the number of boundaries involved in transpositions
between the sequences (2 boundaries) from the num-
ber of substitutions, giving us an edit distance of 4
PBs: 1 transposition PB and 3 substitution PBs.
</bodyText>
<equation confidence="0.845698166666667">
Sub. Sub. Sub.
{1} {} {1} {} {1} {} {} {1} {} {} {1} {1} {}
s1 1 2 2 3 3 1 2
s2 1 2 1 2 6 2
{1} {} {1} {1} {} {1} {} {} {} {} {} {1} {}
Transposition
</equation>
<figureCaption confidence="0.987609">
Figure 4: Edit operations performed on boundary sets
</figureCaption>
<bodyText confidence="0.999173733333333">
Edit distance, and especially the number of oper-
ations of each type performed, is useful in identi-
fying the number of full and near misses that have
occurred–which indicates whether one’s choice of
transposition window size n is either too generous
or too harsh. Edit distance as a penalty does not
incorporate information on the severity of an error
with respect to the size of a segment, and is not an
easily comparable value without some form of nor-
malization. To account for these issues, we define
S so that boundary edit distance is used to subtract
penalties for each edit operation that occurs, from
the number of potential boundaries in a segmenta-
tion, normalizing this value by the total number of
potential boundaries in a segmentation.
</bodyText>
<equation confidence="0.999754666666667">
t · mass(i) − t − d(si1, si1, T)
S(si1, si2) = (4)
t · mass(i) − t
</equation>
<bodyText confidence="0.9953256">
S, as shown in Equation 4, scales the mass of the
item by the cardinality of the set of boundary types
(t) because the edit distance function d(si1, si1, T)
will return a value of [0, t · mass(i)] PBs, where
t E Z+–while subtracting the edit distance and t.6
</bodyText>
<figure confidence="0.7896792">
6The number of potential boundaries in a segmentation si
0 1 2 3 4 5 6
1
�
3
</figure>
<page confidence="0.7441615">
2
155
</page>
<bodyText confidence="0.9999516">
The numerator is normalized by the total number
of potential boundaries per boundary type. This re-
sults in a function with a range of [0, 1]. It returns 0
when one segmentation contains no boundaries, and
the other contains the maximum number of possible
boundaries. It returns 1 when both segmentations
are identical.
Using the default configuration of this equation,
S = 9/13 = 0.6923, a very low similarity, which
WindowDiff also agrees upon (1−WD = 0.6154).
The edit-distance function d(si1, si1, T) can also be
assigned values of the range [0, 1] as scalar weights
(wsub, wtrp) to reduce the penalty attributed to par-
ticular edit operations, and configured to use a trans-
position error function (Equation 3, used by default).
</bodyText>
<subsectionHeader confidence="0.993226">
3.4 Evaluating Automatic Segmenters
</subsectionHeader>
<bodyText confidence="0.999992391304348">
Coders often disagree in segmentation tasks (Hearst,
1997, p. 56), making it improbable that a single,
correct, reference segmentation could be identified
from human codings. This improbability is the re-
sult of individual coders adopting slightly differ-
ent segmentation strategies (i.e., different granular-
ity). In light of this, we propose that the best avail-
able evaluation strategy for automatic segmentation
methods is to compare performance against multiple
coders directly, so that performance can be quanti-
fied relative to human reliability and agreement.
To evaluate whether an automatic segmenter
performs on par with human performance, inter-
annotator agreement can be calculated with and
without the inclusion of an automatic segmenter,
where an observed drop in the coefficients would
signify that the automatic segmenter does not per-
form as reliably as the group of human coders.7 This
can be performed independently for multiple auto-
matic segmenters to compare them to each other–
assuming that the coefficients model chance agree-
ment appropriately–because agreement is calculated
(and quantifies reliability) over all segmentations.
</bodyText>
<sectionHeader confidence="0.771274" genericHeader="method">
3.5 Inter-Annotator Agreement
</sectionHeader>
<bodyText confidence="0.945948">
Similarity alone is not a sufficiently insightful mea-
sure of reliability, or agreement, between coders.
with t boundary types is t · mass(i) − t.
</bodyText>
<footnote confidence="0.800145666666667">
7Similar to how human competitiveness is ascertained by
Medelyan et al. (2009, pp. 1324–1325) and Medelyan (2009,
pp. 143–145) by comparing drops in inter-indexer consistency.
</footnote>
<bodyText confidence="0.999795045454546">
Chance agreement occurs in segmentation when
coders operating at slightly different granularities
agree due to their codings, and not their own in-
nate segmentation heuristics. Inter-annotator agree-
ment coefficients have been developed that assume a
variety of prior distributions to characterize chance
agreement, and to attempt to offer a way to iden-
tify whether agreement is primarily due to chance,
or not, and to quantify reliability.
Artstein and Poesio (2008) note that most of a
coder’s judgements are non-boundaries. The class
imbalance caused by segmentations often contain-
ing few boundaries, paired with no handling of near
misses, causes most inter-annotator agreement co-
efficients to drastically underestimate agreement on
segmentations. To allow for agreement coefficients
to account for near misses, we have adapted S for use
with Cohen’s r., Scott’s 7r, Fleiss’s multi-7r (7r*), and
Fleiss’s multi-r. (r.*), which are all coefficients that
range from [Ae/1−Ae, 1], where 0 indicates chance
agreement, and 1 perfect agreement. All four coeffi-
cients have the general form:
</bodyText>
<equation confidence="0.744923">
κ, π, κ*, and π* = la− A e (5)
e
</equation>
<bodyText confidence="0.99837195">
For each agreement coefficient, the set of cate-
gories is defined as solely the presence of a bound-
ary (K = {segtJt E T}), per boundary type (t).
This category choice is similar to those chosen by
Hearst (1997, p. 53), who computed chance agree-
ment in terms of the probability that coders would
say that a segment boundary exists (segt), and the
probability that they would not (unsegt). We have
chosen to model chance agreement only in terms of
the presence of a boundary, and not the absence,
because coders have only two choices when seg-
menting: to place a boundary, or not. Coders do
not place non-boundaries. If they do not make a
choice, then the default choice is used: no boundary.
This default option makes it impossible to determine
whether a segmenter is making a choice by not plac-
ing a boundary, or whether they are not sure whether
a boundary is to be placed.8 For this reason, we
only characterize chance agreement between coders
in terms of one boundary presence category per type.
</bodyText>
<footnote confidence="0.975877">
8This could be modelled as another boundary type, which
would be modelled in S by the set of boundary types T.
</footnote>
<page confidence="0.983425">
156
</page>
<subsubsectionHeader confidence="0.335867">
3.5.1 Scott’s 7r
</subsubsectionHeader>
<bodyText confidence="0.991997">
Proposed by Scott (1955), 7r assumes that chance
agreement between coders can be characterized as
the proportion of items that have been assigned to
category k by both coders (Equation 7). We cal-
culate agreement (A&amp;quot;) as pairwise mean S (scaled
by each item’s size) to enable agreement to quantify
near misses leniently, and chance agreement (A&amp;quot;,)
can be calculated as in Artstein and Poesio (2008).
We calculate chance agreement per category as
the proportion of boundaries (segs) assigned by all
coders over the total number of potential boundaries
for segmentations, as shown in Equation 8.
</bodyText>
<equation confidence="0.99423">
P P
i∈I |boundaries(t, sic)|
Pπ c∈C
e (segt) = c · P (8)
�mass(i) − 1~
i∈I
</equation>
<bodyText confidence="0.996548">
This adapted coefficient appropriately estimates
chance agreement in situations where there no in-
dividual coder bias.
</bodyText>
<subsubsectionHeader confidence="0.706191">
3.5.2 Cohen’s r.
</subsubsectionHeader>
<bodyText confidence="0.993829857142857">
Proposed by Cohen (1960), r. characterizes
chance agreement as individual distributions per
coder, calculated as shown in Equations 9-10 using
our definition of agreement (A&amp;quot;) as shown earlier.
coders rate all items, we express agreement as pair-
wise mean S between all coders as shown in Equa-
tions 12-13, adapting only Equation 12.
</bodyText>
<equation confidence="0.998941285714286">
P
X c−1Xc
Aπ∗
a = 1 i∈I mass(i) · S(sim, sin)
�c � P �mass(i) − 1~ (12)
2 m=1 n=m+1 i∈I
(Pπe (k))2 (13)
</equation>
<subsectionHeader confidence="0.599671">
3.5.4 Fleiss’s Multi-r.
</subsectionHeader>
<bodyText confidence="0.993096833333333">
Proposed by Davies and Fleiss (1982), multi-r.
(r.*) adapts Cohen’s r. for multiple annotators. We
use Artstein and Poesio’s (2008, extended version)
proposal for calculating agreement just as in 7r*, but
with separate distributions per coder as shown in
Equations 14-15.
</bodyText>
<equation confidence="0.9986305">
Aκ∗ a = Aπ a ∗ (14)
Aeκ∗=X
k∈K 2 m=1 n=m+1
� 1 Xc
�c � Pκ e (k|cm) · Pκ e (k|cn) (15)
c−1X �
</equation>
<subsectionHeader confidence="0.983275">
3.6 Annotator Bias
</subsectionHeader>
<bodyText confidence="0.999824666666667">
To identify the degree of bias in a group of coders’
segmentations, we can use a measure of variance
proposed by Artstein and Poesio (2008, p. 572) that
is quantified in terms of the difference between ex-
pected agreement when chance is assumed to vary
between coders, and when it is assumed to not.
</bodyText>
<equation confidence="0.9895792">
Aπ = X (Pπe (k))2 (7)
e
k∈K
P
i∈I mass(i) · S(si1, si2)
Aπ a =
P
i∈I mass(i) (6)
X
Aπ∗
e =
k∈K
XAκ e =
k∈K
Aκa = Aπ (9)
a
Pκe(k|c1) · Pκe(k|c2) (10)
B = Aπ∗
e − Aκ∗ (16)
e
</equation>
<bodyText confidence="0.9990675">
We calculate category probabilities as in Scott’s
7r, but per coder, as shown in Equation 11.
</bodyText>
<equation confidence="0.99129325">
Pi∈I |boundaries(t, sic)|
Pκ e (segt|c) = P (11)
�mass(i) − 1~
i∈I
</equation>
<bodyText confidence="0.999887">
This adapted coefficient appropriately estimates
chance agreement for segmentation evaluations
where coder bias is present.
</bodyText>
<subsectionHeader confidence="0.857771">
3.5.3 Fleiss’s Multi-7r
</subsectionHeader>
<bodyText confidence="0.99908325">
Proposed by Fleiss (1971), multi-7r (7r*) adapts
Scott’s 7r for multiple annotators. We use Artstein
and Poesio’s (2008, p. 564) proposal for calculat-
ing actual and expected agreement, and because all
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.992447166666667">
To demonstrate the advantages of using S, as op-
posed to WindowDiff (WD), we compare both met-
rics using a variety of contrived scenarios, and then
compare our adapted agreement coefficients against
pairwise mean WD9 for the segmentations collected
by Kazantseva and Szpakowicz (2012).
In this section, because WD is a penalty-based
metric, it is reported as 1−WD so that it is easier
to compare against S values. When reported in this
way, 1−WD and S both range from [0, 1], where 1
represents no errors and 0 represents maximal error.
9Permuted, and with window size recalculated for each pair.
</bodyText>
<page confidence="0.971367">
157
</page>
<figure confidence="0.988605828571429">
1−WD
S
k/m
0 20 40 60 80 100
Segmentation mass (m)
0 20 40 60 80 100
Number of full misses / false positives
0 2 4 6 8 10
Distance between boundaries in each seg. (units)
0.8
0.6
0.4
0.2
0
1
1−WD
S
0.9
0.8
0.7
1
1−WD
S(n = 3)
S(n = 5,scale)
S(n = 5,wt, = 0)
1
0.8
0.6
0.4
0.2
(a) Increasing the number of full misses,
or FPs, where k = 25 for WD
(b) Increasing the distance between two
boundaries considered to be a near miss
until metrics consider them a full miss
</figure>
<figureCaption confidence="0.90547625">
(c) Increasing the mass m of segmenta-
tions configured as shown in Figure 10
showing the effect of k on 1−WD
Figure 5: Responses of 1−WD and S to various segmentation scenarios
</figureCaption>
<subsectionHeader confidence="0.989827">
4.1 Segmentation Cases
</subsectionHeader>
<bodyText confidence="0.990551333333333">
Maximal versus minimal segmentation When
proposing a new metric, its reactions to extrema
must be illustrated, for example when a maximal
segmentation is compared to a minimal segmenta-
tion, as shown in Figure 6. In this scenario, both
1−WD and S appropriately identify that this case
represents maximal error, or 0. Though not shown
here, both metrics also report a similarity of 1.0
when identical segmentations are compared.
</bodyText>
<figure confidence="0.6532975">
81 14
82 1 1 1 1 1 1 1 1 1 1 1 1 1 1
</figure>
<figureCaption confidence="0.999065">
Figure 6: Maximal versus minimal seg. masses
</figureCaption>
<bodyText confidence="0.999899">
Full misses For the most serious source of error,
full misses (i.e., FPs and FNs), both metrics appro-
priately report a reduction in similarity for cases
such as Figure 7 that is very similar (1−WD =
0.8462, S= 0.8461). Where the two metrics differ
is when this type of error is increased.
</bodyText>
<figure confidence="0.880991">
81 1 2 2 2 4 2 1
82 1 2 8 2 1
</figure>
<figureCaption confidence="0.999445">
Figure 7: Full misses in seg. masses
</figureCaption>
<bodyText confidence="0.99992225">
S reacts to increasing full misses linearly, whereas
WindowDiff can prematurely report a maximal
number of errors. Figure 5a demonstrates this ef-
fect, where for each iteration we have taken seg-
mentations of 100 units of mass with one matching
boundary at the first hypothesis boundary position,
and uniformly increased the number of internal hy-
pothesis segments, giving us 1 matching boundary,
and [0, 98] FPs. This premature report of maximal
error (at 7 FP) by WD is caused by the window size
(k = 25) being greater than all of the internal hy-
pothesis segment sizes, making all windows penal-
ized for containing errors.
Near misses When dealing with near misses, the
values of both metrics drop (1−WD = 0.8182,
S = 0.9231), but to greatly varying degrees. In
comparison to full misses, WindowDiff penalizes a
near miss, like that in Figure 8, far more than S.
This difference is due to the distance between the
two boundaries involved in a near miss; S shows,
in this case, 1 PB of error until it is outside of the
n-wise transposition window (where n = 2 PBs),
at which point it is considered an error of not one
transposition, but two substitutions (2 PBs).
</bodyText>
<figure confidence="0.887765">
81 6 8
82 7 7
</figure>
<figureCaption confidence="0.999473">
Figure 8: Near misses in seg. masses
</figureCaption>
<bodyText confidence="0.9999886">
If we wanted to completely forgive near misses
up to n PBs, we could set the weighting of trans-
positions in S to wtrp = 0. This is useful if a spe-
cific segmentation task accepts that near misses are
very probable, and that there is little cost associated
with a near miss in a window of n PBs. We can
also set n to a high number, i.e., 5 PBs, and use the
scaled transposition error (te) function (Equation 3)
to slowly increase the error from b = 1 PB to b = 2
PBs, as shown in Figure 5b, which shows how both
</bodyText>
<page confidence="0.993614">
158
</page>
<table confidence="0.999510714285714">
Scenario 1: FN, p = 0.5 Scenario 2: FP, p = 0.5 Scenario 3: FP and FN, p = 0.5
(20,30) (15,35) (20,30) (15,35) (20,30) (15,35)
WD 0.2340 f 0.0113 0.2292 f 0.0104 0.2265 f 0.0114 0.2265 f 0.0111 0.3635 f 0.0126 0.3599 f 0.0117
S 0.9801 f 0.0006 0.9801 f 0.0006 0.9800 f 0.0006 0.9800 f 0.0006 0.9605 f 0.0009 0.9603 f 0.0009
(10,40) (5,45) (10,40) (5,45) (10,40) (5,45)
WD 0.2297 f 0.0105 0.2206 f 0.0079 0.2256 f 0.0102 0.2184 f 0.0069 0.3516 f 0.0110 0.3254 f 0.0087
S 0.9799 f 0.0007 0.9796 f 0.0007 0.9800 f 0.0006 0.9796 f 0.0007 0.9606 f 0.0010 0.9598 f 0.0011
</table>
<tableCaption confidence="0.997498">
Table 1: Stability of mean (with standard deviation) values of WD and S in three different scenarios, each defining
the: probability of a false positive (FP), false negative (FN), or both. Each scenario varies the range of internal
segment sizes (e.g., (20, 30)). Low standard deviation and similar within-scenario means demonstrates low sensitivity
to variations in internal segment size.
</tableCaption>
<bodyText confidence="0.975964">
metrics react to increases in the distance between a
near miss in a segment of 25 units. These configura-
tions are all preferable to the drop of 1−WD.
</bodyText>
<subsectionHeader confidence="0.993432">
4.2 Segmentation Mass Scale Effects
</subsectionHeader>
<bodyText confidence="0.999960583333333">
It is important for a segmentation evaluation met-
ric to take into account the severity of an error in
terms of segment size. An error in a 100 unit seg-
ment should be considered less severe than an er-
ror in a 2 unit segment, because an extra boundary
placed within a 100 unit segment (e.g., Figure 9 with
m = 100) could probably indicate a weak boundary,
whereas in a 4 unit segment the probability that an
extra boundary exists right next to two agreed-upon
boundaries should be small for most tasks, meaning
that it is probable that the extra boundary is an error,
and not a weak boundary.
</bodyText>
<figure confidence="0.827481">
81 m/4 m/2 m/4
82 m/4 m/4 m/4 m/4
</figure>
<figureCaption confidence="0.999312">
Figure 9: Two segmentations of mass m with a full miss
</figureCaption>
<bodyText confidence="0.973127">
To demonstrate that S is sensitive to segment size,
Figure 5c shows how S and 1−WD respond when
comparing segmentations configured as shown in
Figure 10 (containing one match and one full miss)
with linearly increasing mass (4 &lt; m &lt; 100).
1−WD will eventually indicate 0.68, whereas S ap-
propriately discounts the error as mass is increased,
approaching 1 as limn,,,,,. 1−WD behaves in this
way because of how it calculates its window size pa-
rameter, k, which is plotted as k/m to show how its
value influences 1−WD.
</bodyText>
<figure confidence="0.5346845">
81 m/4 nt − (m/4)
82 m/4 m/4 m/2
</figure>
<figureCaption confidence="0.828832">
Figure 10: Two segmentations of mass m compared with
increasing m in Figure 5c (sl as reference)
</figureCaption>
<subsectionHeader confidence="0.997839">
4.3 Variation in Segment Sizes
</subsectionHeader>
<bodyText confidence="0.999978052631579">
When Pevzner and Hearst (2002) proposed WD,
they demonstrated that it was not as sensitive as
Pk to variations in the size of segments inside a
segmentation. To show this, they simulated how
WD performs upon a segmentation comprised of
1000 segments with four different uniformly dis-
tributed ranges of internal segment sizes (keeping
the mean at approximately 25 units) in compari-
son to a hypothesis segmentation with errors (false
positives, false negatives, and both) uniformly dis-
tributed within segments (Pevzner and Hearst, 2002,
pp. 11–12). 10 trials were performed for each seg-
ment size range and error probability, with 100 hy-
potheses generated per trial. Recreating this simu-
lation, we compare the stability of S in comparison
to WD, as shown in Table 1. We can see that WD
values show substantial within-scenario variation for
each segment size range, and larger standard devia-
tions, than S.
</bodyText>
<subsectionHeader confidence="0.952634">
4.4 Inter-Annotator Agreement Coefficients
</subsectionHeader>
<bodyText confidence="0.9997589375">
Here, we demonstrate the adapted inter-annotator
agreement coefficients upon topical paragraph-level
segmentations produced by 27 coders of 20 chapters
from the novel The Moonstone by Wilkie Collins
collected by Kazantseva and Szpakowicz (2012).
Figure 11 shows a heat map of each chapter where
the percentage of coders who agreed upon each po-
tential boundary is represented. Comparing this heat
map to the inter-annotator agreement coefficients in
Table 2 allows us to better understand why certain
chapters have lower reliability.
Chapter 1 has the lowest 7rs score in the table, and
also the highest bias (BS). One of the reasons for
this low reliability can be attributed to the chapter’s
small mass (m) and few coders (|c|), which makes
it more sensitive to chance agreement. Visually, the
</bodyText>
<page confidence="0.986799">
159
</page>
<figure confidence="0.97794">
Chapters
Potential boundary positions (between paragraphs)
</figure>
<figureCaption confidence="0.954052666666667">
Figure 11: Heat maps for the segmentations of each chap-
ter showing the percentage of coders who agree upon
boundary positions (darker shows higher agreement)
</figureCaption>
<bodyText confidence="0.99986596969697">
predominance of grey indicates that, although there
are probably two boundaries, their exact location is
not very well agreed upon. In this case, 1−WD
incorrectly indicates the opposite, that this chapter
may have relatively moderate reliability, because it
is not corrected for chance agreement.
1−WD indicates that the lowest reliability is
found in Chapter 19. 7r* S indicates that this is one
of the higher agreement chapters, and looking at the
heat map, we can see that it does not contain any
strongly agreed upon boundaries. In this chapter,
there is little opportunity to agree by chance due to
the low number of boundaries (|b|) placed, and be-
cause the judgements are tightly clustered in a fair
amount of mass, the S component of 7r* S appropri-
ately takes into account the near misses observed
and gives it a high reliability score.
Chapter 17 received the highest 7r* S in the table,
which is another example of how tight clustering of
boundary choices in a large mass leads 7r* S to appro-
priately indicate high reliability despite that there are
not as many individual highly-agreed-upon bound-
aries, whereas 1−WD indicates that there is low re-
liability. 1−WD and 7r* S both agree, however, that
chapter 16 has high reliability.
Despite WindowDiff’s sensitivity to near misses,
it is evident that its pairwise mean cannot be used
to consistently judge inter-annotator agreement, or
reliability. S demonstrates better versatility when
accounting for near misses, and when used as part
of inter-annotator agreement coefficients, it prop-
erly takes into account chance agreement. Follow-
ing Artstein and Poesio’s (2008, pp. 590–591) rec-
</bodyText>
<table confidence="0.999467">
Ch. �� �� BS 1−WD |c ||b |m
S S
1 0.7452 0.7463 0.0039 0.6641 f 0.1307 4 13 13
2 0.8839 0.8840 0.0009 0.7619 f 0.1743 6 20 15
3 0.8338 0.8340 0.0013 0.6732 f 0.1559 4 23 38
4 0.8414 0.8417 0.0019 0.6019 f 0.2245 4 25 46
5 0.8773 0.8774 0.0003 0.6965 f 0.1106 6 34 42
7 0.8132 0.8133 0.0002 0.6945 f 0.1822 6 20 15
8 0.8495 0.8496 0.0006 0.7505 f 0.0911 6 48 39
9 0.8104 0.8105 0.0009 0.6502 f 0.1319 6 35 33
10 0.9077 0.9078 0.0002 0.7729 f 0.0770 6 56 83
11 0.8130 0.8135 0.0022 0.6189 f 0.1294 4 73 111
12 0.9178 0.9178 0.0001 0.6504 f 0.1277 6 40 102
13 0.9354 0.9354 0.0002 0.5660 f 0.2187 6 21 58
14 0.9367 0.9367 0.0001 0.7128 f 0.1744 6 35 70
15 0.9344 0.9344 0.0001 0.7291 f 0.0856 6 40 97
16 0.9356 0.9356 0.0000 0.8016 f 0.0648 6 41 69
17 0.9447 0.9447 0.0002 0.6717 f 0.2044 5 23 70
18 0.8921 0.8922 0.0005 0.5998 f 0.1614 5 28 59
19 0.9021 0.9022 0.0009 0.4796 f 0.2666 5 15 36
20 0.8590 0.8591 0.0003 0.6657 f 0.1221 6 21 21
21 0.9286 0.9286 0.0004 0.6255 f 0.2003 5 17 60
</table>
<tableCaption confidence="0.988993333333333">
Table 2: S-based inter-annotator agreements and pairwise
mean 1−WD and standard deviation with the number of
coders, boundaries, and mass per chapter
</tableCaption>
<bodyText confidence="0.999945">
ommendation, and given the low bias (mean coder
group BS = 0.0061±0.0035), we propose reporting
reliability using 7r* for this corpus, where the mean
coder group 7r* S for the corpus is 0.8904 ± 0.0392
(counting 1039 full and 212 near misses).
</bodyText>
<sectionHeader confidence="0.997383" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999984058823529">
We have proposed a segmentation evaluation met-
ric which solves the key problems facing segmenta-
tion analysis today, including an inability to: appro-
priately quantify near misses when evaluating auto-
matic segmenters and human performance; penalize
errors equally (or, with configuration, in a manner
that suits a specific segmentation task); compare an
automatic segmenter directly against human perfor-
mance; require a “true” reference; and handle mul-
tiple boundary types. Using S, task-specific eval-
uation of automatic and human segmenters can be
performed using multiple human judgements unhin-
dered by the quirks of window-based metrics.
In current and future work, we will show how S
can be used to analyze hierarchical segmentations,
and illustrate how to apply S to linear segmentations
containing multiple boundary types.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998138666666667">
We thank Anna Kazantseva for her invaluable feed-
back and corpora, and Stan Szpakowicz, Martin Sca-
iano, and James Cracknell for their feedback.
</bodyText>
<figure confidence="0.997382">
1
2
3
4
5
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
0.0
1.0
Coder agreement per potential boundary (%)
</figure>
<page confidence="0.977438">
160
</page>
<sectionHeader confidence="0.997106" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808057142857">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596. MIT Press, Cam-
bridge, MA, USA.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. Text Segmentation Using Exponential Models.
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, 2:35–46.
Association for Computational Linguistics, Strouds-
burg, PA, USA.
Doug Beeferman and Adam Berger. 1999. Statisti-
cal models for text segmentation. Machine learning,
34(1–3):177–210. Springer Netherlands, NL.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37–46. Sage, Beverly Hills, CA, USA.
Frederick J. Damerau. 1964. A technique for computer
detection and correction of spelling errors. Commu-
nications of the ACM, 7(3):171–176. Association for
Computing Machinery, Stroudsburg, PA, USA.
Mark Davies and Joseph L. Fleiss. 1982. Measur-
ing agreement for multinomial data. Biometrics,
38(4):1047–1051. Blackwell Publishing Inc, Oxford,
UK.
George R. Doddington. 1998. The topic detection and
tracking phase 2 (TDT2) evaluation plan. DARPA
Broadcast News Transcription and Understanding
Workshop, pp. 223–229. Morgan Kaufmann, Waltham,
MA, USA.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378–382. American Psychological Association,
Washington, DC, USA.
Martin, Franz, J. Scott McCarley, and Jian-Ming Xu.
2007. User-oriented text segmentation evaluation
measure. Proceedings of the 30th annual international
ACM SIGIR conference on Research and development
in information retrieval, pp. 701–702. Association for
Computing Machinery, Stroudsburg, PA, USA.
William Gale, Kenneth Ward Church, and David
Yarowsky. 1992. Estimating upper and lower bounds
on the performance of word-sense disambiguation pro-
grams. Proceedings of the 30th annual meeting of
the Association for Computational Linguistics, pp.
249–256. Association for Computational Linguistics,
Stroudsburg, PA, USA.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. An analysis of quantitative aspects in
the evaluation of thematic segmentation algorithms.
Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue, pp. 144–151. Association for
Computational Linguistics, Stroudsburg, PA, USA.
Marti A. Hearst. 1997. TextTiling: Segmenting Text into
Multi-paragraph Subtopic Passages. Computational
Linguistics, 23(1):33–64. MIT Press, Cambridge, MA,
USA.
Anna Kazantseva and Stan Szpakowicz. 2012. Topical
Segmentation: a Study of Human Performance. Pro-
ceedings of Human Language Technologies: The 2012
Annual Conference of the North American Chapter
of the Association for Computational Linguistics. As-
sociation for Computational Linguistics, Stroudsburg,
PA, USA.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology, Chapter 12. Sage, Beverly
Hills, CA, USA.
Klaus Krippendorff. 2004. Content Analysis: An Intro-
duction to Its Methodology, Chapter 11. Sage, Beverly
Hills, CA, USA.
Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and
Frederic Saubion 2007. On evaluation methodologies
for text segmentation algorithms. Proceedings of the
19th IEEE International Conference on Tools with Ar-
tificial Intelligence, 2:19–26. IEEE Computer Society,
Washington, DC, USA.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707–710. American Institute
of Physics, College Park, MD, USA.
Olena Medelyan. 2009. Human-competitive automatic
topic indexing. PhD Thesis. University of Waikato,
Waikato, NZ.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pp. 1318–1327. Association for Compu-
tational Linguistics, Stroudsburg, PA, USA.
Rebecca J. Passonneau and Diane J. Litman. 1993.
Intention-based segmentation: human reliability and
correlation with linguistic cues. Proceedings of the
31st annual meeting of the Association for Computa-
tional Linguistics, pp. 148–155). Association for Com-
putational Linguistics, Stroudsburg, PA, USA.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19–36. MIT
Press, Cambridge, MA, USA.
William A. Scott. 1955. Reliability of content analy-
sis: The case of nominal scale coding. Public Opinion
Quarterly, 19(3):321–325. American Association for
Public Opinion Research, Deerfield, IL, USA.
Sidney Siegel and N. John Castellan, Jr. 1988. Non-
parametric Statistics for the Behavioral Sciences. 2nd
Edition, Chapter 9.8. McGraw-Hill, New York, USA.
</reference>
<page confidence="0.998204">
161
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.890752">
<title confidence="0.999865">Segmentation Similarity and Agreement</title>
<author confidence="0.999935">Chris Fournier Diana Inkpen</author>
<affiliation confidence="0.999954">University of Ottawa University of Ottawa</affiliation>
<address confidence="0.993658">Ottawa, ON, Canada Ottawa, ON, Canada</address>
<email confidence="0.909671">cfour037@eecs.uottawa.cadiana@eecs.uottawa.ca</email>
<abstract confidence="0.999104470588235">We propose a new segmentation evaluation called similarity that quantifies the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. We propose several adapted inter-annotator agreement coefficients which use S that are suitable for segmentation. We show that S is configurable enough to suit a wide variety of segmentation evaluations, and is an improvement upon the state of the art. We also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="9444" citStr="Artstein and Poesio, 2008" startWordPosition="1519" endWordPosition="1522"> 7 coders while developing the automatic segmenter TextTiling, and reported mean n (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean n scores were calculated by comparing a coder’s segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53–54). Although mean n scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan’s (1988) n has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555–556). Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of α (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp. 580–582). 3 Segmentation Similarity For discussing segmentation, a segment’s size (or mass) is measured in units, the error is quantified in potential boundaries (PBs), and we have adopted a modified form of the notation used by Artstein and Poesio (200</context>
<context position="19390" citStr="Artstein and Poesio (2008)" startWordPosition="3207" endWordPosition="3210"> human competitiveness is ascertained by Medelyan et al. (2009, pp. 1324–1325) and Medelyan (2009, pp. 143–145) by comparing drops in inter-indexer consistency. Chance agreement occurs in segmentation when coders operating at slightly different granularities agree due to their codings, and not their own innate segmentation heuristics. Inter-annotator agreement coefficients have been developed that assume a variety of prior distributions to characterize chance agreement, and to attempt to offer a way to identify whether agreement is primarily due to chance, or not, and to quantify reliability. Artstein and Poesio (2008) note that most of a coder’s judgements are non-boundaries. The class imbalance caused by segmentations often containing few boundaries, paired with no handling of near misses, causes most inter-annotator agreement coefficients to drastically underestimate agreement on segmentations. To allow for agreement coefficients to account for near misses, we have adapted S for use with Cohen’s r., Scott’s 7r, Fleiss’s multi-7r (7r*), and Fleiss’s multi-r. (r.*), which are all coefficients that range from [Ae/1−Ae, 1], where 0 indicates chance agreement, and 1 perfect agreement. All four coefficients ha</context>
<context position="21570" citStr="Artstein and Poesio (2008)" startWordPosition="3579" endWordPosition="3582">haracterize chance agreement between coders in terms of one boundary presence category per type. 8This could be modelled as another boundary type, which would be modelled in S by the set of boundary types T. 156 3.5.1 Scott’s 7r Proposed by Scott (1955), 7r assumes that chance agreement between coders can be characterized as the proportion of items that have been assigned to category k by both coders (Equation 7). We calculate agreement (A&amp;quot;) as pairwise mean S (scaled by each item’s size) to enable agreement to quantify near misses leniently, and chance agreement (A&amp;quot;,) can be calculated as in Artstein and Poesio (2008). We calculate chance agreement per category as the proportion of boundaries (segs) assigned by all coders over the total number of potential boundaries for segmentations, as shown in Equation 8. P P i∈I |boundaries(t, sic)| Pπ c∈C e (segt) = c · P (8) �mass(i) − 1~ i∈I This adapted coefficient appropriately estimates chance agreement in situations where there no individual coder bias. 3.5.2 Cohen’s r. Proposed by Cohen (1960), r. characterizes chance agreement as individual distributions per coder, calculated as shown in Equations 9-10 using our definition of agreement (A&amp;quot;) as shown earlier. </context>
<context position="22958" citStr="Artstein and Poesio (2008" startWordPosition="3824" endWordPosition="3827">ass(i) · S(sim, sin) �c � P �mass(i) − 1~ (12) 2 m=1 n=m+1 i∈I (Pπe (k))2 (13) 3.5.4 Fleiss’s Multi-r. Proposed by Davies and Fleiss (1982), multi-r. (r.*) adapts Cohen’s r. for multiple annotators. We use Artstein and Poesio’s (2008, extended version) proposal for calculating agreement just as in 7r*, but with separate distributions per coder as shown in Equations 14-15. Aκ∗ a = Aπ a ∗ (14) Aeκ∗=X k∈K 2 m=1 n=m+1 � 1 Xc �c � Pκ e (k|cm) · Pκ e (k|cn) (15) c−1X � 3.6 Annotator Bias To identify the degree of bias in a group of coders’ segmentations, we can use a measure of variance proposed by Artstein and Poesio (2008, p. 572) that is quantified in terms of the difference between expected agreement when chance is assumed to vary between coders, and when it is assumed to not. Aπ = X (Pπe (k))2 (7) e k∈K P i∈I mass(i) · S(si1, si2) Aπ a = P i∈I mass(i) (6) X Aπ∗ e = k∈K XAκ e = k∈K Aκa = Aπ (9) a Pκe(k|c1) · Pκe(k|c2) (10) B = Aπ∗ e − Aκ∗ (16) e We calculate category probabilities as in Scott’s 7r, but per coder, as shown in Equation 11. Pi∈I |boundaries(t, sic)| Pκ e (segt|c) = P (11) �mass(i) − 1~ i∈I This adapted coefficient appropriately estimates chance agreement for segmentation evaluations where coder</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Text Segmentation Using Exponential Models.</title>
<date>1997</date>
<booktitle>Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, 2:35–46. Association for Computational Linguistics,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6048" citStr="Beeferman et al., 1997" startWordPosition="925" endWordPosition="928">rst (2002, pp. 5–10) highlighted a number of issues with Pk, specifically that: i) False negatives (FNs) are penalized more than false positives (FPs); ii) It does not penalize FPs that fall within k units of a reference boundary; iii) Its sensitivity to variations in segment size can cause it to linearly decrease the penalty for FPs if the size of any segments fall below k; and iv) Near-miss errors are too harshly penalized. To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were 2Pk is a modification of PN, (Beeferman et al., 1997, p. 43). Other modifications such as TDT C3e9 (Doddington, 1998, pp. 5–6) have been proposed, but Pk has seen greater usage. counted, named WindowDiff (WD). A window of size k is still slid across the text, but now penalties are attributed to windows where the number of boundaries in each segmentation differs (see Equation 1, where b(RZj) and b(HZj) represents the number of boundaries within the segments in a window of size k from position i to j, and N the number of sentences plus one), with the same normalization. N−k 1 � WD(R, H) = N − k z=1,j=z+k WindowDiff is able to reduce, but not elim</context>
<context position="12471" citStr="Beeferman et al. (1997" startWordPosition="2040" endWordPosition="2043">ssimilarity identified earlier.5 Near misses that are remedied by transposition are penalized as b PBs of error (where b is the number of boundaries transposed), as opposed to the 2b PBs of errors by which they would be penalized if they were considered to be two separate substitution operations. Transpositions can also be considered over n &gt; 2 PBs (n-wise transpositions). This is useful if, for a specific task, near misses of up to n PBs are not to be penalized as full misses (default n = 2). The error represented by the two operations can also be scaled (i.e., weighted) from 1 PB each to a 4Beeferman et al. (1997, p. 42) briefly mention using an edit distance without transpositions, but discard it in favour of PN,. 5For multiple boundaries, an add/del operation is added, and transpositions are considered only within boundary types. 81 82 154 fraction. The distance over which an n-wise transposition occurred can also be used in conjunction with the scalar operation weighting so that a transposition is weighted using the function in Equation 3. te(n, b) = b − (1/b)&amp;quot;−2 where n &gt; 2 and b &gt; 0 (3) This transposition error function was chosen so that, in an n-wise transposition where n = 2 PBs and the number</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1997. Text Segmentation Using Exponential Models. Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, 2:35–46. Association for Computational Linguistics, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine learning,</booktitle>
<pages>34--1</pages>
<publisher>Springer</publisher>
<location>Netherlands, NL.</location>
<contexts>
<context position="4935" citStr="Beeferman and Berger, 1999" startWordPosition="734" endWordPosition="737"> a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). For segmentation, these metrics are unsuitable because they penalize near-misses of boundaries as full-misses, causing them to drastically overestimate the error. Near-misses are prevalent in segmentation and can account for a large proportion of the errors produced by a coder, and as inter-annotator agreement often shows, they do not reflect coder error, but the difficulty of the task. Pk (Beeferman and Berger, 1999, pp. 198–200)2 is a window-based metric which attempts to solve the harsh near-miss penalization of precision, recall, and F,3-measure. In Pk, a window of size k, where k is defined as half of the mean reference segment size, is slid across the text to compute penalties. A penalty of 1 is assigned for each window whose boundaries are detected to be in different segments of the reference and hypothesis segmentations, and this count is normalized by the number of windows. Pevzner and Hearst (2002, pp. 5–10) highlighted a number of issues with Pk, specifically that: i) False negatives (FNs) are </context>
</contexts>
<marker>Beeferman, Berger, 1999</marker>
<rawString>Doug Beeferman and Adam Berger. 1999. Statistical models for text segmentation. Machine learning, 34(1–3):177–210. Springer Netherlands, NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement, 20(1):37–46. Sage,</booktitle>
<location>Beverly Hills, CA, USA.</location>
<contexts>
<context position="22000" citStr="Cohen (1960)" startWordPosition="3652" endWordPosition="3653">) as pairwise mean S (scaled by each item’s size) to enable agreement to quantify near misses leniently, and chance agreement (A&amp;quot;,) can be calculated as in Artstein and Poesio (2008). We calculate chance agreement per category as the proportion of boundaries (segs) assigned by all coders over the total number of potential boundaries for segmentations, as shown in Equation 8. P P i∈I |boundaries(t, sic)| Pπ c∈C e (segt) = c · P (8) �mass(i) − 1~ i∈I This adapted coefficient appropriately estimates chance agreement in situations where there no individual coder bias. 3.5.2 Cohen’s r. Proposed by Cohen (1960), r. characterizes chance agreement as individual distributions per coder, calculated as shown in Equations 9-10 using our definition of agreement (A&amp;quot;) as shown earlier. coders rate all items, we express agreement as pairwise mean S between all coders as shown in Equations 12-13, adapting only Equation 12. P X c−1Xc Aπ∗ a = 1 i∈I mass(i) · S(sim, sin) �c � P �mass(i) − 1~ (12) 2 m=1 n=m+1 i∈I (Pπe (k))2 (13) 3.5.4 Fleiss’s Multi-r. Proposed by Davies and Fleiss (1982), multi-r. (r.*) adapts Cohen’s r. for multiple annotators. We use Artstein and Poesio’s (2008, extended version) proposal for c</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37–46. Sage, Beverly Hills, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick J Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communications of the ACM,</journal>
<volume>7</volume>
<issue>3</issue>
<institution>Association for Computing Machinery,</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Damerau, 1964</marker>
<rawString>Frederick J. Damerau. 1964. A technique for computer detection and correction of spelling errors. Communications of the ACM, 7(3):171–176. Association for Computing Machinery, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davies</author>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring agreement for multinomial data.</title>
<date>1982</date>
<journal>Biometrics,</journal>
<volume>38</volume>
<issue>4</issue>
<publisher>Blackwell Publishing Inc,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="22472" citStr="Davies and Fleiss (1982)" startWordPosition="3734" endWordPosition="3737">apted coefficient appropriately estimates chance agreement in situations where there no individual coder bias. 3.5.2 Cohen’s r. Proposed by Cohen (1960), r. characterizes chance agreement as individual distributions per coder, calculated as shown in Equations 9-10 using our definition of agreement (A&amp;quot;) as shown earlier. coders rate all items, we express agreement as pairwise mean S between all coders as shown in Equations 12-13, adapting only Equation 12. P X c−1Xc Aπ∗ a = 1 i∈I mass(i) · S(sim, sin) �c � P �mass(i) − 1~ (12) 2 m=1 n=m+1 i∈I (Pπe (k))2 (13) 3.5.4 Fleiss’s Multi-r. Proposed by Davies and Fleiss (1982), multi-r. (r.*) adapts Cohen’s r. for multiple annotators. We use Artstein and Poesio’s (2008, extended version) proposal for calculating agreement just as in 7r*, but with separate distributions per coder as shown in Equations 14-15. Aκ∗ a = Aπ a ∗ (14) Aeκ∗=X k∈K 2 m=1 n=m+1 � 1 Xc �c � Pκ e (k|cm) · Pκ e (k|cn) (15) c−1X � 3.6 Annotator Bias To identify the degree of bias in a group of coders’ segmentations, we can use a measure of variance proposed by Artstein and Poesio (2008, p. 572) that is quantified in terms of the difference between expected agreement when chance is assumed to vary </context>
</contexts>
<marker>Davies, Fleiss, 1982</marker>
<rawString>Mark Davies and Joseph L. Fleiss. 1982. Measuring agreement for multinomial data. Biometrics, 38(4):1047–1051. Blackwell Publishing Inc, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George R Doddington</author>
</authors>
<title>The topic detection and tracking phase 2 (TDT2) evaluation plan.</title>
<date>1998</date>
<booktitle>DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>223--229</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>Waltham, MA, USA.</location>
<contexts>
<context position="6112" citStr="Doddington, 1998" startWordPosition="937" endWordPosition="938">lly that: i) False negatives (FNs) are penalized more than false positives (FPs); ii) It does not penalize FPs that fall within k units of a reference boundary; iii) Its sensitivity to variations in segment size can cause it to linearly decrease the penalty for FPs if the size of any segments fall below k; and iv) Near-miss errors are too harshly penalized. To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were 2Pk is a modification of PN, (Beeferman et al., 1997, p. 43). Other modifications such as TDT C3e9 (Doddington, 1998, pp. 5–6) have been proposed, but Pk has seen greater usage. counted, named WindowDiff (WD). A window of size k is still slid across the text, but now penalties are attributed to windows where the number of boundaries in each segmentation differs (see Equation 1, where b(RZj) and b(HZj) represents the number of boundaries within the segments in a window of size k from position i to j, and N the number of sentences plus one), with the same normalization. N−k 1 � WD(R, H) = N − k z=1,j=z+k WindowDiff is able to reduce, but not eliminate, sensitivity to segment size, gives more equal weights to </context>
</contexts>
<marker>Doddington, 1998</marker>
<rawString>George R. Doddington. 1998. The topic detection and tracking phase 2 (TDT2) evaluation plan. DARPA Broadcast News Transcription and Understanding Workshop, pp. 223–229. Morgan Kaufmann, Waltham, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<publisher>American Psychological Association,</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="23625" citStr="Fleiss (1971)" startWordPosition="3953" endWordPosition="3954">e between expected agreement when chance is assumed to vary between coders, and when it is assumed to not. Aπ = X (Pπe (k))2 (7) e k∈K P i∈I mass(i) · S(si1, si2) Aπ a = P i∈I mass(i) (6) X Aπ∗ e = k∈K XAκ e = k∈K Aκa = Aπ (9) a Pκe(k|c1) · Pκe(k|c2) (10) B = Aπ∗ e − Aκ∗ (16) e We calculate category probabilities as in Scott’s 7r, but per coder, as shown in Equation 11. Pi∈I |boundaries(t, sic)| Pκ e (segt|c) = P (11) �mass(i) − 1~ i∈I This adapted coefficient appropriately estimates chance agreement for segmentation evaluations where coder bias is present. 3.5.3 Fleiss’s Multi-7r Proposed by Fleiss (1971), multi-7r (7r*) adapts Scott’s 7r for multiple annotators. We use Artstein and Poesio’s (2008, p. 564) proposal for calculating actual and expected agreement, and because all 4 Experiments To demonstrate the advantages of using S, as opposed to WindowDiff (WD), we compare both metrics using a variety of contrived scenarios, and then compare our adapted agreement coefficients against pairwise mean WD9 for the segmentations collected by Kazantseva and Szpakowicz (2012). In this section, because WD is a penalty-based metric, it is reported as 1−WD so that it is easier to compare against S values</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382. American Psychological Association, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Martin</author>
<author>J Scott McCarley</author>
<author>Jian-Ming Xu</author>
</authors>
<title>User-oriented text segmentation evaluation measure.</title>
<date>2007</date>
<booktitle>Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>701--702</pages>
<institution>Association for Computing Machinery,</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Martin, McCarley, Xu, 2007</marker>
<rawString>Martin, Franz, J. Scott McCarley, and Jian-Ming Xu. 2007. User-oriented text segmentation evaluation measure. Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 701–702. Association for Computing Machinery, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Ward Church</author>
<author>David Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<date>1992</date>
<booktitle>Proceedings of the 30th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>249--256</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8433" citStr="Gale et al. (1992" startWordPosition="1358" endWordPosition="1361">ter-Annotator Agreement The need to ascertain the agreement and reliability between coders for segmentation was recognized 3Georgescul et al. (2006, p. 48) note that both FPs and FNs are weighted by 1/N−k, and although there are “equiprobable possibilities to have a [FP] in an interval of k units”, “the total number of equiprobable possibilities to have a [FN] in an interval of k units is smaller than (N−k)”, making the interpretation of a full miss as a FN less probable than as a FP. (|b(Rzj)−b(Hzj) |&gt; 0) (1) 153 by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean n (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean n scores were calculat</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Ward Church, and David Yarowsky. 1992. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. Proceedings of the 30th annual meeting of the Association for Computational Linguistics, pp. 249–256. Association for Computational Linguistics, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Georgescul</author>
<author>Alexander Clark</author>
<author>Susan Armstrong</author>
</authors>
<title>An analysis of quantitative aspects in the evaluation of thematic segmentation algorithms.</title>
<date>2006</date>
<booktitle>Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>144--151</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7963" citStr="Georgescul et al. (2006" startWordPosition="1272" endWordPosition="1275"> number of word positions present (see Equation 2). 1 � 1 � FP(w) (2) RF N = F N(w), RF P = N N w w RFN and RFP have the advantage that they take into account the severity of an error in terms of segment size, allowing them to reflect the effects of erroneously missing, or added, words in a segment better than window based metrics. Unfortunately, RFN and RFP suffer from the same flaw as precision, recall, and F,3-measure in that they do not account for near misses. 2.2 Inter-Annotator Agreement The need to ascertain the agreement and reliability between coders for segmentation was recognized 3Georgescul et al. (2006, p. 48) note that both FPs and FNs are weighted by 1/N−k, and although there are “equiprobable possibilities to have a [FP] in an interval of k units”, “the total number of equiprobable possibilities to have a [FN] in an interval of k units is smaller than (N−k)”, making the interpretation of a full miss as a FN less probable than as a FP. (|b(Rzj)−b(Hzj) |&gt; 0) (1) 153 by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the to</context>
</contexts>
<marker>Georgescul, Clark, Armstrong, 2006</marker>
<rawString>Maria Georgescul, Alexander Clark, and Susan Armstrong. 2006. An analysis of quantitative aspects in the evaluation of thematic segmentation algorithms. Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pp. 144–151. Association for Computational Linguistics, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1895" citStr="Hearst, 1997" startWordPosition="285" endWordPosition="286">automatic segmentation methods can help a wide range of computational linguistic tasks which depend upon the identification of segment boundaries in text. To evaluate automatic segmentation methods, a method of comparing an automatic segmenter’s performance against the segmentations produced by human judges (coders) is required. Current methods of performing this comparison designate only one coder’s segmentation as a reference to compare against. A single “true” reference segmentation from a coder should not be trusted, given that interannotator agreement is often reported to be rather poor (Hearst, 1997, p. 54). Additionally, to ensure that an automatic segmenter does not over-fit to the preference and bias of one particular coder, an automatic segmenter should be compared directly against multiple coders. The state of the art segmentation evaluation metrics (Pk and WindowDiff) slide a window across a designated reference and hypothesis segmentation, and count the number of windows where the number of boundaries differ. Window-based methods suffer from a variety of problems, including: i) unequal penalization of error types; ii) an arbitrarily defined window size parameter (whose choice grea</context>
<context position="8790" citStr="Hearst (1997)" startWordPosition="1416" endWordPosition="1417">] in an interval of k units is smaller than (N−k)”, making the interpretation of a full miss as a FN less probable than as a FP. (|b(Rzj)−b(Hzj) |&gt; 0) (1) 153 by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean n (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean n scores were calculated by comparing a coder’s segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53–54). Although mean n scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan’s (1988) n has declined in f</context>
<context position="17483" citStr="Hearst, 1997" startWordPosition="2926" endWordPosition="2927">er contains the maximum number of possible boundaries. It returns 1 when both segmentations are identical. Using the default configuration of this equation, S = 9/13 = 0.6923, a very low similarity, which WindowDiff also agrees upon (1−WD = 0.6154). The edit-distance function d(si1, si1, T) can also be assigned values of the range [0, 1] as scalar weights (wsub, wtrp) to reduce the penalty attributed to particular edit operations, and configured to use a transposition error function (Equation 3, used by default). 3.4 Evaluating Automatic Segmenters Coders often disagree in segmentation tasks (Hearst, 1997, p. 56), making it improbable that a single, correct, reference segmentation could be identified from human codings. This improbability is the result of individual coders adopting slightly different segmentation strategies (i.e., different granularity). In light of this, we propose that the best available evaluation strategy for automatic segmentation methods is to compare performance against multiple coders directly, so that performance can be quantified relative to human reliability and agreement. To evaluate whether an automatic segmenter performs on par with human performance, interannota</context>
<context position="20252" citStr="Hearst (1997" startWordPosition="3352" endWordPosition="3353">timate agreement on segmentations. To allow for agreement coefficients to account for near misses, we have adapted S for use with Cohen’s r., Scott’s 7r, Fleiss’s multi-7r (7r*), and Fleiss’s multi-r. (r.*), which are all coefficients that range from [Ae/1−Ae, 1], where 0 indicates chance agreement, and 1 perfect agreement. All four coefficients have the general form: κ, π, κ*, and π* = la− A e (5) e For each agreement coefficient, the set of categories is defined as solely the presence of a boundary (K = {segtJt E T}), per boundary type (t). This category choice is similar to those chosen by Hearst (1997, p. 53), who computed chance agreement in terms of the probability that coders would say that a segment boundary exists (segt), and the probability that they would not (unsegt). We have chosen to model chance agreement only in terms of the presence of a boundary, and not the absence, because coders have only two choices when segmenting: to place a boundary, or not. Coders do not place non-boundaries. If they do not make a choice, then the default choice is used: no boundary. This default option makes it impossible to determine whether a segmenter is making a choice by not placing a boundary, </context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages. Computational Linguistics, 23(1):33–64. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Kazantseva</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Topical Segmentation: a Study of Human Performance.</title>
<date>2012</date>
<booktitle>Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="24097" citStr="Kazantseva and Szpakowicz (2012)" startWordPosition="4024" endWordPosition="4027">coefficient appropriately estimates chance agreement for segmentation evaluations where coder bias is present. 3.5.3 Fleiss’s Multi-7r Proposed by Fleiss (1971), multi-7r (7r*) adapts Scott’s 7r for multiple annotators. We use Artstein and Poesio’s (2008, p. 564) proposal for calculating actual and expected agreement, and because all 4 Experiments To demonstrate the advantages of using S, as opposed to WindowDiff (WD), we compare both metrics using a variety of contrived scenarios, and then compare our adapted agreement coefficients against pairwise mean WD9 for the segmentations collected by Kazantseva and Szpakowicz (2012). In this section, because WD is a penalty-based metric, it is reported as 1−WD so that it is easier to compare against S values. When reported in this way, 1−WD and S both range from [0, 1], where 1 represents no errors and 0 represents maximal error. 9Permuted, and with window size recalculated for each pair. 157 1−WD S k/m 0 20 40 60 80 100 Segmentation mass (m) 0 20 40 60 80 100 Number of full misses / false positives 0 2 4 6 8 10 Distance between boundaries in each seg. (units) 0.8 0.6 0.4 0.2 0 1 1−WD S 0.9 0.8 0.7 1 1−WD S(n = 3) S(n = 5,scale) S(n = 5,wt, = 0) 1 0.8 0.6 0.4 0.2 (a) Inc</context>
<context position="31358" citStr="Kazantseva and Szpakowicz (2012)" startWordPosition="5341" endWordPosition="5344">were performed for each segment size range and error probability, with 100 hypotheses generated per trial. Recreating this simulation, we compare the stability of S in comparison to WD, as shown in Table 1. We can see that WD values show substantial within-scenario variation for each segment size range, and larger standard deviations, than S. 4.4 Inter-Annotator Agreement Coefficients Here, we demonstrate the adapted inter-annotator agreement coefficients upon topical paragraph-level segmentations produced by 27 coders of 20 chapters from the novel The Moonstone by Wilkie Collins collected by Kazantseva and Szpakowicz (2012). Figure 11 shows a heat map of each chapter where the percentage of coders who agreed upon each potential boundary is represented. Comparing this heat map to the inter-annotator agreement coefficients in Table 2 allows us to better understand why certain chapters have lower reliability. Chapter 1 has the lowest 7rs score in the table, and also the highest bias (BS). One of the reasons for this low reliability can be attributed to the chapter’s small mass (m) and few coders (|c|), which makes it more sensitive to chance agreement. Visually, the 159 Chapters Potential boundary positions (betwee</context>
</contexts>
<marker>Kazantseva, Szpakowicz, 2012</marker>
<rawString>Anna Kazantseva and Stan Szpakowicz. 2012. Topical Segmentation: a Study of Human Performance. Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology, Chapter 12. Sage,</title>
<date>1980</date>
<location>Beverly Hills, CA, USA.</location>
<contexts>
<context position="9692" citStr="Krippendorff, 1980" startWordPosition="1559" endWordPosition="1560">ainst a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53–54). Although mean n scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan’s (1988) n has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555–556). Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of α (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp. 580–582). 3 Segmentation Similarity For discussing segmentation, a segment’s size (or mass) is measured in units, the error is quantified in potential boundaries (PBs), and we have adopted a modified form of the notation used by Artstein and Poesio (2008), where the set of: • Items is {i|i ∈ I} with cardinality i; • Categories is {k|k ∈ K} with cardinality k; • Coders is {c|c ∈ C} with cardinality c; • Segmentations of an item i by a coder c is {s|s ∈ S}, where when sic is specified with only one</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology, Chapter 12. Sage, Beverly Hills, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology,</title>
<date>2004</date>
<journal>Chapter</journal>
<volume>11</volume>
<location>Sage, Beverly Hills, CA, USA.</location>
<contexts>
<context position="9713" citStr="Krippendorff, 2004" startWordPosition="1561" endWordPosition="1563">gmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53–54). Although mean n scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan’s (1988) n has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555–556). Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of α (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp. 580–582). 3 Segmentation Similarity For discussing segmentation, a segment’s size (or mass) is measured in units, the error is quantified in potential boundaries (PBs), and we have adopted a modified form of the notation used by Artstein and Poesio (2008), where the set of: • Items is {i|i ∈ I} with cardinality i; • Categories is {k|k ∈ K} with cardinality k; • Coders is {c|c ∈ C} with cardinality c; • Segmentations of an item i by a coder c is {s|s ∈ S}, where when sic is specified with only one subscript, it denote</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology, Chapter 11. Sage, Beverly Hills, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Lamprier</author>
<author>Tassadit Amghar</author>
<author>Bernard Levrat</author>
<author>Frederic Saubion</author>
</authors>
<title>On evaluation methodologies for text segmentation algorithms.</title>
<date>2007</date>
<booktitle>Proceedings of the 19th IEEE International Conference on Tools with Artificial Intelligence,</booktitle>
<pages>2--19</pages>
<publisher>IEEE Computer Society,</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="6887" citStr="Lamprier et al. (2007)" startWordPosition="1077" endWordPosition="1080">enalties are attributed to windows where the number of boundaries in each segmentation differs (see Equation 1, where b(RZj) and b(HZj) represents the number of boundaries within the segments in a window of size k from position i to j, and N the number of sentences plus one), with the same normalization. N−k 1 � WD(R, H) = N − k z=1,j=z+k WindowDiff is able to reduce, but not eliminate, sensitivity to segment size, gives more equal weights to both FPs and FNs (FNs are, in effect, penalized less3), and is able to catch mistakes in both small and large segments. It is not without issues though; Lamprier et al. (2007) demonstrated that WindowDiff penalizes errors less at the beginning and end of a segmentation (this is corrected by padding the segmentation at each end by size k). Additionally, variations in the window size k lead to difficulties in interpreting and comparing WindowDiff’s values, and the intuition of the method remains vague. Franz et al. (2007) proposed measuring performance in terms of the number of words that are FNs and FPs, normalized by the number of word positions present (see Equation 2). 1 � 1 � FP(w) (2) RF N = F N(w), RF P = N N w w RFN and RFP have the advantage that they take i</context>
</contexts>
<marker>Lamprier, Amghar, Levrat, Saubion, 2007</marker>
<rawString>Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and Frederic Saubion 2007. On evaluation methodologies for text segmentation algorithms. Proceedings of the 19th IEEE International Conference on Tools with Artificial Intelligence, 2:19–26. IEEE Computer Society, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<publisher>American</publisher>
<institution>Institute of Physics,</institution>
<location>College Park, MD, USA.</location>
<contexts>
<context position="11517" citStr="Levenshtein, 1966" startWordPosition="1879" endWordPosition="1880">r segmentation as a reference or hypothesis, we refer to insertions and deletions both as substitutions. 1 3 2 Figure 1: Types of segmentations errors It is important to not penalize near misses as full misses in many segmentation tasks because coders often agree upon the existence of a boundary, but disagree upon its exact location. In the previous scenario, assigning a full miss would mean that even a boundary loosely agreed-upon, as in Figure 1, error 1, would be regarded as completely disagreed-upon. 3.2 Edit Distance In S, concepts from Damereau-Levenshtein edit distance (Damereau, 1964; Levenshtein, 1966) are applied to model segmentation edit distance as two operations: substitutions and transpositions.4 These two operations represent full misses and near misses, respectively. Using these two operations, a new globally-optimal minimum edit distance is applied to a pair of sequences of sets of boundaries to model the sources of dissimilarity identified earlier.5 Near misses that are remedied by transposition are penalized as b PBs of error (where b is the number of boundaries transposed), as opposed to the 2b PBs of errors by which they would be penalized if they were considered to be two sepa</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710. American Institute of Physics, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
</authors>
<title>Human-competitive automatic topic indexing. PhD Thesis.</title>
<date>2009</date>
<institution>University of Waikato,</institution>
<location>Waikato, NZ.</location>
<contexts>
<context position="18861" citStr="Medelyan (2009" startWordPosition="3131" endWordPosition="3132">c segmenter does not perform as reliably as the group of human coders.7 This can be performed independently for multiple automatic segmenters to compare them to each other– assuming that the coefficients model chance agreement appropriately–because agreement is calculated (and quantifies reliability) over all segmentations. 3.5 Inter-Annotator Agreement Similarity alone is not a sufficiently insightful measure of reliability, or agreement, between coders. with t boundary types is t · mass(i) − t. 7Similar to how human competitiveness is ascertained by Medelyan et al. (2009, pp. 1324–1325) and Medelyan (2009, pp. 143–145) by comparing drops in inter-indexer consistency. Chance agreement occurs in segmentation when coders operating at slightly different granularities agree due to their codings, and not their own innate segmentation heuristics. Inter-annotator agreement coefficients have been developed that assume a variety of prior distributions to characterize chance agreement, and to attempt to offer a way to identify whether agreement is primarily due to chance, or not, and to quantify reliability. Artstein and Poesio (2008) note that most of a coder’s judgements are non-boundaries. The class i</context>
</contexts>
<marker>Medelyan, 2009</marker>
<rawString>Olena Medelyan. 2009. Human-competitive automatic topic indexing. PhD Thesis. University of Waikato, Waikato, NZ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>Eibe Frank</author>
<author>Ian H Witten</author>
</authors>
<title>Human-competitive tagging using automatic keyphrase extraction.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1318--1327</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18826" citStr="Medelyan et al. (2009" startWordPosition="3124" endWordPosition="3127">efficients would signify that the automatic segmenter does not perform as reliably as the group of human coders.7 This can be performed independently for multiple automatic segmenters to compare them to each other– assuming that the coefficients model chance agreement appropriately–because agreement is calculated (and quantifies reliability) over all segmentations. 3.5 Inter-Annotator Agreement Similarity alone is not a sufficiently insightful measure of reliability, or agreement, between coders. with t boundary types is t · mass(i) − t. 7Similar to how human competitiveness is ascertained by Medelyan et al. (2009, pp. 1324–1325) and Medelyan (2009, pp. 143–145) by comparing drops in inter-indexer consistency. Chance agreement occurs in segmentation when coders operating at slightly different granularities agree due to their codings, and not their own innate segmentation heuristics. Inter-annotator agreement coefficients have been developed that assume a variety of prior distributions to characterize chance agreement, and to attempt to offer a way to identify whether agreement is primarily due to chance, or not, and to quantify reliability. Artstein and Poesio (2008) note that most of a coder’s judgeme</context>
</contexts>
<marker>Medelyan, Frank, Witten, 2009</marker>
<rawString>Olena Medelyan, Eibe Frank, and Ian H. Witten. 2009. Human-competitive tagging using automatic keyphrase extraction. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 1318–1327. Association for Computational Linguistics, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Intention-based segmentation: human reliability and correlation with linguistic cues.</title>
<date>1993</date>
<booktitle>Proceedings of the 31st annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>148--155</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8367" citStr="Passonneau and Litman (1993)" startWordPosition="1347" endWordPosition="1350">, recall, and F,3-measure in that they do not account for near misses. 2.2 Inter-Annotator Agreement The need to ascertain the agreement and reliability between coders for segmentation was recognized 3Georgescul et al. (2006, p. 48) note that both FPs and FNs are weighted by 1/N−k, and although there are “equiprobable possibilities to have a [FP] in an interval of k units”, “the total number of equiprobable possibilities to have a [FN] in an interval of k units is smaller than (N−k)”, making the interpretation of a full miss as a FN less probable than as a FP. (|b(Rzj)−b(Hzj) |&gt; 0) (1) 153 by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean n (Siegel and Castellan, 1988) values for coders and automatic segm</context>
</contexts>
<marker>Passonneau, Litman, 1993</marker>
<rawString>Rebecca J. Passonneau and Diane J. Litman. 1993. Intention-based segmentation: human reliability and correlation with linguistic cues. Proceedings of the 31st annual meeting of the Association for Computational Linguistics, pp. 148–155). Association for Computational Linguistics, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="4506" citStr="Pevzner and Hearst, 2002" startWordPosition="668" endWordPosition="671"> c�2012 Association for Computational Linguistics inter-annotator agreement coefficient adaptations. In Section 4, we evaluate S and WindowDiff in various scenarios and simulations, and upon a multiplycoded corpus. 2 Related Work 2.1 Segmentation Evaluation Precision, recall, and their mean (F,3-measure) have been previously applied to segmentation evaluation. Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). For segmentation, these metrics are unsuitable because they penalize near-misses of boundaries as full-misses, causing them to drastically overestimate the error. Near-misses are prevalent in segmentation and can account for a large proportion of the errors produced by a coder, and as inter-annotator agreement often shows, they do not reflect coder error, but the difficulty of the task. Pk (Beeferman and Berger, 1999, pp. 198–200)2 is a window-based metric which attempts to solve the harsh near-miss penalization of precision, recall, and F,3-measure. In Pk, a window of size k, where k</context>
<context position="5927" citStr="Pevzner and Hearst (2002" startWordPosition="904" endWordPosition="907">ments of the reference and hypothesis segmentations, and this count is normalized by the number of windows. Pevzner and Hearst (2002, pp. 5–10) highlighted a number of issues with Pk, specifically that: i) False negatives (FNs) are penalized more than false positives (FPs); ii) It does not penalize FPs that fall within k units of a reference boundary; iii) Its sensitivity to variations in segment size can cause it to linearly decrease the penalty for FPs if the size of any segments fall below k; and iv) Near-miss errors are too harshly penalized. To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were 2Pk is a modification of PN, (Beeferman et al., 1997, p. 43). Other modifications such as TDT C3e9 (Doddington, 1998, pp. 5–6) have been proposed, but Pk has seen greater usage. counted, named WindowDiff (WD). A window of size k is still slid across the text, but now penalties are attributed to windows where the number of boundaries in each segmentation differs (see Equation 1, where b(RZj) and b(HZj) represents the number of boundaries within the segments in a window of size k from position i to j, and N the number of senten</context>
<context position="30199" citStr="Pevzner and Hearst (2002)" startWordPosition="5162" endWordPosition="5165"> respond when comparing segmentations configured as shown in Figure 10 (containing one match and one full miss) with linearly increasing mass (4 &lt; m &lt; 100). 1−WD will eventually indicate 0.68, whereas S appropriately discounts the error as mass is increased, approaching 1 as limn,,,,,. 1−WD behaves in this way because of how it calculates its window size parameter, k, which is plotted as k/m to show how its value influences 1−WD. 81 m/4 nt − (m/4) 82 m/4 m/4 m/2 Figure 10: Two segmentations of mass m compared with increasing m in Figure 5c (sl as reference) 4.3 Variation in Segment Sizes When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11–12). 10 trials were performed for each segment size range and error probability, with 10</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19–36. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Scott</author>
</authors>
<title>Reliability of content analysis: The case of nominal scale coding.</title>
<date>1955</date>
<journal>Public Opinion Quarterly,</journal>
<volume>19</volume>
<issue>3</issue>
<publisher>American Association for</publisher>
<institution>Public Opinion Research,</institution>
<location>Deerfield, IL, USA.</location>
<contexts>
<context position="21197" citStr="Scott (1955)" startWordPosition="3519" endWordPosition="3520">nting: to place a boundary, or not. Coders do not place non-boundaries. If they do not make a choice, then the default choice is used: no boundary. This default option makes it impossible to determine whether a segmenter is making a choice by not placing a boundary, or whether they are not sure whether a boundary is to be placed.8 For this reason, we only characterize chance agreement between coders in terms of one boundary presence category per type. 8This could be modelled as another boundary type, which would be modelled in S by the set of boundary types T. 156 3.5.1 Scott’s 7r Proposed by Scott (1955), 7r assumes that chance agreement between coders can be characterized as the proportion of items that have been assigned to category k by both coders (Equation 7). We calculate agreement (A&amp;quot;) as pairwise mean S (scaled by each item’s size) to enable agreement to quantify near misses leniently, and chance agreement (A&amp;quot;,) can be calculated as in Artstein and Poesio (2008). We calculate chance agreement per category as the proportion of boundaries (segs) assigned by all coders over the total number of potential boundaries for segmentations, as shown in Equation 8. P P i∈I |boundaries(t, sic)| Pπ</context>
</contexts>
<marker>Scott, 1955</marker>
<rawString>William A. Scott. 1955. Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 19(3):321–325. American Association for Public Opinion Research, Deerfield, IL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N John Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences. 2nd Edition, Chapter 9.8.</title>
<date>1988</date>
<publisher>McGraw-Hill,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="8930" citStr="Siegel and Castellan, 1988" startWordPosition="1433" endWordPosition="1437">P. (|b(Rzj)−b(Hzj) |&gt; 0) (1) 153 by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean n (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean n scores were calculated by comparing a coder’s segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53–54). Although mean n scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan’s (1988) n has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555–556). Artstein and Poesio (2008) briefly touch upon recommendations for coef</context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Sidney Siegel and N. John Castellan, Jr. 1988. Nonparametric Statistics for the Behavioral Sciences. 2nd Edition, Chapter 9.8. McGraw-Hill, New York, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>