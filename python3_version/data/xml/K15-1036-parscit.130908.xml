<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000845">
<title confidence="0.998933">
Model Selection for Type-Supervised Learning
with Application to POS Tagging
</title>
<author confidence="0.992783">
Kristina Toutanova Waleed Ammar∗ Pallavi Chourdhury Hoifung Poon
</author>
<affiliation confidence="0.995034">
Microsoft Research School of Computer Science Microsoft Research
</affiliation>
<address confidence="0.75377">
Redmond, WA, USA Carnegie Mellon University Redmond, WA, USA
</address>
<sectionHeader confidence="0.97427" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999727291666667">
Model selection (picking, for example, the
feature set and the regularization strength)
is crucial for building high-accuracy NLP
models. In supervised learning, we can es-
timate the accuracy of a model on a subset
of the labeled data and choose the model
with the highest accuracy. In contrast,
here we focus on type-supervised learn-
ing, which uses constraints over the pos-
sible labels for word types for supervi-
sion, and labeled data is either not avail-
able or very small. For the setting where
no labeled data is available, we perform a
comparative study of previously proposed
and one novel model selection criterion on
type-supervised POS-tagging in nine lan-
guages. For the setting where a small la-
beled set is available, we show that the set
should be used for semi-supervised learn-
ing rather than for model selection only –
using it for model selection reduces the er-
ror by less than 5%, whereas using it for
semi-supervised learning reduces the error
by 44%.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995854">
Fully supervised training of NLP models (e.g.,
part-of-speech taggers, named entity recognizers,
relation extractors) works well when plenty of la-
beled examples are available. However, manu-
ally labeled corpora are expensive to construct
in many languages and domains, whereas an al-
ternative, if weaker, supervision is often read-
ily available. For example, corpora labeled with
POS tags at the token level are only available for
around 35 languages, while tag dictionaries of the
</bodyText>
<footnote confidence="0.4880575">
∗This research was conducted during the author’s intern-
ship at Microsoft Research.
</footnote>
<bodyText confidence="0.999958075">
form displayed in Fig. 1 are available for many
more languages, either in commercial dictionar-
ies or community created resources such as Wik-
tionary. Tag dictionaries provide type-level super-
vision for word types in the lexicon. Similarly,
while sentences labeled with named entities are
scarce, gazetteers and databases are more readily
available (Bollacker et al., 2008).
There has been substantial research on how best
to build models using such type-level supervision,
for POS tagging, super sense tagging, NER, and
relation extraction (Craven et al., 1999; Smith and
Eisner, 2005; Carlson et al., 2009; Mintz et al.,
2009; Johannsen et al., 2014), inter alia, focussing
on parametric forms and loss functions for model
training. However, there has been little research on
the practically important aspect of model selection
for type-supervised learning. While some previ-
ous work used criteria based on the type-level su-
pervision only (Smith and Eisner, 2005; Goldwa-
ter and Griffiths, 2007), much prior work used a la-
beled set for model selection (Vaswani et al., 2010;
Soderland and Weld, 2014). We are not aware of
any prior work aiming to compare or improve ex-
isting type-supervised model selection criteria.
For POS tagging, there is also work on us-
ing both type-level supervision from lexicons, and
projection from another language (T¨ackstr¨om et
al., 2013). Methods for training with a small
labeled set have also been developed (Søgaard,
2011; Garrette and Baldridge, 2013; Duong et al.,
2014), but there have not been studies on the util-
ity of a small labeled set for model selection ver-
sus model training. Our contributions are: 1) a
simple and generally applicable model selection
criterion for type-supervised learning, 2) the first
multi-lingual systematic evaluation of model se-
lection criteria for type-supervised models, 3) em-
pirical evidence that, if a small labeled set is avail-
able, the set should be used for semi-supervised
</bodyText>
<page confidence="0.981092">
332
</page>
<note confidence="0.74252">
Proceedings of the 19th Conference on Computational Language Learning, pages 332–337,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.591505722222222">
αυτούς det., pron.
σταθερά adj., adv., noun
μετέδωσα verb
ανάπαυση noun
παλιά adj., adv.
ενός det., num.
γαλακτικέ adj.
racy:
eval,up(m, Tdev) =
|Tdev|
�
i=1 Il(ym[i] = ygold[i])
|Tdev|
train
lexicon
dev
lexicon
Greek tag dictionary
</figure>
<figureCaption confidence="0.985292">
Figure 1: A tag dictionary (lexicon) for Greek. The splits
into lextrazn and lexdev are discussed in §2.
</figureCaption>
<bodyText confidence="0.814761">
learning and not only for model selection.
</bodyText>
<sectionHeader confidence="0.515759" genericHeader="method">
2 Model selection and training for
type-supervised learning
</sectionHeader>
<bodyText confidence="0.999960606060606">
Notation. In type-supervised learning, we have
unlabeled text T = {x} of token sequences, and
a lexicon lex which lists possible labels for word
types. Model training finds the model param-
eters θ which minimize a training loss function
L(θ; lex, T , h). We use h to represent the config-
urations and modeling decisions (also known as
hyperparameters). Examples include the depen-
dency structure between variables, feature tem-
plates, and regularization strengths. Given a set
of fully-specified hyperparameter configurations
{h1, ... , hM}, model selection aims to find the
configuration hmˆ that maximizes the expected
performance of the corresponding model θmˆ ac-
cording to a suitable accuracy measure. x is a to-
ken sequence, y is a label sequence, and lex[x] is
the set of label sequences compatible with token
sequence x according to lex.
Task. For the application in this paper, the task
is type-supervised POS tagging, and the paramet-
ric model family we consider is that of featurized
first order HMMs (Berg-Kirkpatrick et al., 2010).
The hyperparameters specify the feature set used
and the strength of an L2 regularizer on the pa-
rameters.
Evaluation function. The evaluation function
used in model selection is the main focus of this
work. We use a function eval(m, Tdev) to esti-
mate the performance of the model trained with
hyperparameters hm on a development set Tdev. In
the following subsections, we discuss definitions
of eval when the development set Tdev is labeled
and when it is unlabeled, respectively.
</bodyText>
<subsectionHeader confidence="0.956387">
2.1 Tdev is labeled
</subsectionHeader>
<bodyText confidence="0.998443166666667">
When the development set Tdev is labeled, a nat-
ural choice of eval is token-level prediction accu-
Here, we use i to index all tokens in Tdev; ygold[i]
denotes the correct POS tag, and ym[i] denotes the
predicted POS tag of the i-th token obtained with
hyperparameters hm.
</bodyText>
<subsectionHeader confidence="0.987453">
2.2 Tdev is unlabeled
</subsectionHeader>
<bodyText confidence="0.9949045">
When token-supervision is not available, we can-
not compute eval,up. Instead, previous work on
POS tagging with type supervision (Smith and
Eisner, 2005) used:
</bodyText>
<equation confidence="0.994616">
�evalcond(m,Tdev) = log � pθm(y  |x),
xETdev yElex[x]
�evaljoint(m, Tdev) = log � pθm(x, y)
xETdev yElex[x]
</equation>
<bodyText confidence="0.999899230769231">
evalcond estimates the conditional log-likelihood
of “lex-compatible” labels given token sequences,
while evaljoint estimates the joint log-likelihood
of lex-compatible labels and token sequences.
The held-out lexicon criterion. We propose a
new model selection criterion which estimates pre-
diction accuracy more directly and is applicable to
any model type, without requiring that the model
define conditional or joint probabilities of label se-
quences. The idea behind this proposed criterion
is simple: we hold out a portion of the lexicon en-
tries and use it to estimate model performance as
follows:
</bodyText>
<equation confidence="0.984675">
Il(ym[i] ∈ lex[xi])
|lex[xi] |× |TxElexdev|
</equation>
<bodyText confidence="0.999983857142857">
where lexdev is the held-out portion of the lexicon
entries, and xi is the i-th token in T .
The remainder of this section details the theory
behind this criterion. The expected token-level ac-
curacy of a model trained with hyperparameters
hm is defined as E(xygoldym)∼D [Il(ym = ygold)];
where D is a joint distribution over tokens x (in
context), their gold labels ygold, and the predicted
labels ym (for the configuration hm). Since when
no labeled data is available we do not have access
to samples from D, we derive an approximation to
this distribution using lex and T .
We first split the full lexicon into a training
lexicon lextrain and a held-out (or development)
</bodyText>
<equation confidence="0.987104">
evaldevlex(m,T) = � |T |
i=1:xzElexdev
</equation>
<page confidence="0.981493">
333
</page>
<bodyText confidence="0.99975175">
lexicon lexdev (see Fig. 1), by sampling words ac-
cording to their token frequency c(x) in T , and
placing them in the development or training por-
tions of the lexicon such that lexdev covers 25%
of the tokens in T . The goal of the sampling pro-
cess is to make the distribution of word tags for
words in the development lexicon representative
of the tag distribution for all words.
Given hm, we train a tagging model using
lextrain and use it to predict labels ym for all to-
kens in T . We then use the word tokens covered
by the development lexicon and their predicted
tags ym to approximate D by letting P(ygold  |x)
be a uniform distribution over gold labels consis-
tent with the lexicon for x, resulting in the follow-
ing approximation PD(x , ygold, ym) a
</bodyText>
<equation confidence="0.9905855">
c(x, ym) X 1(x E lexdev, ygold E lexdev[x])
|lexdev[x]|
</equation>
<bodyText confidence="0.991865">
We then compute the expected accuracy as
evaldevlex = ED [1(ygold = ym)], and select
the hyperparameter configuration mˆ which max-
imizes this criterion, then re-train the model with
the full lexicon lex.1
</bodyText>
<sectionHeader confidence="0.929021" genericHeader="method">
3 How to best use a small labeled set TL?
</sectionHeader>
<bodyText confidence="0.964596484848485">
Several prior works used a labeled set for super-
vised hyper-parameter selection even when only
type-level supervision is assumed to be available
for training (Vaswani et al., 2010; Soderland and
Weld, 2014). In this section, we want to an-
swer the question: if a small labeled set is avail-
able, what are the potential gains from using it
for model selection only, versus using it for both
model training and model selection?
A simple way to use a small labeled set for
parameter training together with a larger unla-
beled set in our type-supervised learning setting,
is to do semi-supervised model training as follows
(Nigam et al., 2000): Starting with our training
loss function defined using a lexicon lex and unla-
beled set TU L(B; lex, TU, hm), we define a com-
bined loss function using both the unlabeled set
TU and the labeled set TL: L(B; lex, TU, hm) +
AL(B; lex, TL, hm). We then select parameters
Bm to minimize the new loss function, where A
is now an additional hyperparameter that usually
1Note that this criterion underestimates the performance
of all models in consideration by virtue of evaluating model
versions trained using a subset of the full lexicon, but it can
still be useful for ranking the models.
gives more weight to the labeled set. An advan-
tage of this method is that it can be applied to any
type-supervised model using less than 100 lines
of code.2 We implement this method for semi-
supervised training, and we use the labeled set
both for semi-supervised model training and for
hyper-parameter selection using a standard five-
fold cross-validation approach.3
</bodyText>
<sectionHeader confidence="0.999794" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.992137048780488">
We evaluate the introduced methods for model se-
lection and training with type supervision in two
type-supervised settings: when no labeled exam-
ples are available, and when a small number of la-
beled examples are available.
We use a feature-rich first-order HMM
model (Berg-Kirkpatrick et al., 2010) with an
L2 prior on feature weights.4 Instead of using a
multinomial distribution for the local emissions
and transitions, this model uses a log-linear distri-
bution (i.e., p(xi  |yi) a exp ATf(xi, yi)) with a
feature vector f and a weight vector A. We use the
feature set described in (Li et al., 2012): transition
features, word-tag features ((yi, xi)) (lowercased
words with frequency greater than a threshold),
whether the word contains a hyphen and/or
starts with a capital letter, character suffixes, and
whether the word contains a digit. We initialize
the transition and emission distributions of the
HMM using unambiguous words as proposed
by (Zhang and DeNero, 2014). Data. We use
the Danish, Dutch, German, Greek, English,
Italian, Portuguese, Spanish and Swedish datasets
from CoNLL-X and CoNLL-2007 shared tasks
(Buchholz and Marsi, 2006; Nivre et al., 2007).
We map the POS labels in the CoNLL datasets
to the universal POS tagset (Petrov et al., 2012).
We use the tag dictionaries provided by Li et
al. (2012). Model configurations. For each
2The labeled data loss is the negative joint log-
likelihood of the observed token sequences and labels:
− E.,y∈TL log pθ(X, Y).
3We split the labeled set into five folds, and for each set-
ting of the hyper-parameters train five different models on
45 -ths of the data, estimating accuracy on the remaining 15 -th.
We average the accuracy estimates from different folds and
use this as a combined estimate of the accuracy of a model
trained using the full labeled and unlabeled set, given these
hyperparameters. After selecting a configuration of hyperpa-
rameters using cross-validation, we then use this configura-
tion and retrain the model on all available data.
</bodyText>
<footnote confidence="0.766774333333333">
4We used a first-order HMM for simplicity, but it is pos-
sible to obtain better results using a second-order HMM (Li
et al., 2012).
</footnote>
<page confidence="0.99688">
334
</page>
<figure confidence="0.686034">
Greek Danish Dutch Italian Spanish Portugese German English Swedish Average
Conditional Likelihood Joint Likelihood Held-out Lexicon English Labeled
</figure>
<figureCaption confidence="0.997175">
Figure 2: Token-level accuracy when doing training and model selection with no labeled data. Model selection with different
criteria (left to right): conditional log-likelihood, joint log-likelihood, held-out lexicon, and English-labeled.
</figureCaption>
<figure confidence="0.40813">
Greek Danish Dutch Italian Spanish Portugese German English Swedish Average
</figure>
<figureCaption confidence="0.9654925">
Figure 3: Token-level accuracy with (left to right): no labeled data, model selection on labeled data (300 sentences), semi-
supervised training + model selection on labeled data (300 sentences).
</figureCaption>
<figure confidence="0.992207423076923">
83.70
85.05
84.75
85.20
97
95
93
91
89
87
85
83
81
Joint Likelihood (0 labeled)
Supervised model selection (300 labeled)
Semi-supervised training and supervised model selection (300 labeled)
85.05
85.75
91.73
88
86
84
82
80
78
76
</figure>
<bodyText confidence="0.999869607142857">
language, we consider M = 15 configurations
of the hyperparameters which vary in the L2
regularization strength (5 values), and the min-
imum word frequency for word-tag features (3
values). When a small labeled set is available we
additionally choose one of 3 values for the weight
of the labeled set (see Section 3). We report final
performance of models selected using different
criteria using token-level accuracy on an unseen
test set.
No labeled examples. When no labeled exam-
ples are available, we do model training and se-
lection using only unlabeled text T and a tagging
lexicon lex. We compare three type-supervised
model selection criteria: conditional likelihood,
joint likelihood, and the held-out lexicon criterion
evaldevlex. Additionally, we include the perfor-
mance of a method which selects the hyperpa-
rameters using labeled data in English and uses
these (same) hyperparameters for English and all
other languages (we call this method “English La-
beled”). Fig. 2 shows the accuracy of the models
chosen by each of the four criteria on nine lan-
guages, as well as the average accuracy across lan-
guages. The lower (upper) bounds on average per-
formance obtained by always choosing the worst
(best) hyperparameters is 82.77 (85.83). evaljoZnt
outperformed evalcond on eight out of the nine
</bodyText>
<page confidence="0.997961">
335
</page>
<bodyText confidence="0.999981555555556">
languages and achieved a significantly higher av-
erage accuracy (85.05 vs 83.70). evaldevlex out-
performed evaljoint on six out of nine languages,
but did significantly worse on one language (Ger-
man), which resulted in a slightly lower average
accuracy. Choosing the hyper-parameters using
English labeled data and using the same hyper-
parameters for all languages performed compara-
bly to evaljoint, with slightly higher average ac-
curacy even when limited to the non-English lan-
guages (85.0 vs 84.9). Overall the results showed
that the conditional log-likelihood criterion was
dominated by the other three, which were com-
parable in average accuracy. Looking at the eight
languages excluding English (since one criterion
uses labeled data for English), the newly proposed
held-out lexicon criterion was the winning method
on five out of eight languages, evalcond was best
on one language, evaljoint was best (or tied for
best) on two, and English-labeled was tied for best
on one language.
Few labeled examples. We consider two ways
of leveraging the labeled examples: (i) type-
supervised model training + supervised model se-
lection: only use unlabeled examples to optimize
model parameters, then use the labeled exam-
ples for supervised model selection with evalsup,
and (ii) semi-supervised model training + super-
vised model selection (see Section 3 for details).
Fig. 3 shows how much we can improve on the
method with highest average accuracy from Fig-
ure 2 (evaljoint), when a small number of ex-
amples is available. Using the 300 labeled sen-
tences for semi-supervised training and model se-
lection reduced the error by 44.6% (comparing to
the model with best average accuracy using only
type-level supervision with average performance
of 85.05, the semi-supervised average is 91.8). In
contrast, using the 300 sentences to select hyper-
parameters only reduced the error by less than 5%
(the average accuracy was 85.75). Even when only
50 labeled sentences are used for semi-supervised
training and supervised model selection, we still
see a boost to average accuracy of 89% (results
not shown in the Figure).
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999966416666667">
We presented the first comparative evaluation of
model selection criteria for type-supervised POS-
tagging on many languages. We introduced a
novel, generally applicable model selection cri-
terion which outperformed previously proposed
ones for a majority of languages. We evaluated the
utility of a small labeled set for model selection
versus model training, and showed that when such
labeled set is available, it should not be used solely
for supervised model selection, because using it
additionally for model parameter training provides
strikingly larger accuracy gains.
</bodyText>
<sectionHeader confidence="0.997554" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9944435">
We thank Nathan Schneider and the anonymous
reviewers for helpful suggestions.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999371891891892">
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Proc. of NAACL.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, SIGMOD ’08, pages 1247–1250.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
CoNLL-X.
Andrew Carlson, Scott Gaffney, and Flavian Vasile.
2009. Learning a named entity tagger from
gazetteers with the partial perceptron. In AAAI
Spring Symposium: Learning by Reading and
Learning to Read, pages 7–13.
Mark Craven, Johan Kumlien, et al. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77–86.
Long Duong, Trevor Cohn, Karin Verspoor, Steven
Bird, and Paul Cook. 2014. What can we get
from 1000 tokens? a case study of multilingual pos
tagging for resource-poor languages. In Proc. of
EMNLP.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proc. of NAACL-HLT, pages 138–147.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Annual meeting-association for compu-
tational linguistics.
Anders Johannsen, Dirk Hovy, H´ector Martınez
Alonso, Barbara Plank, and Anders Søgaard. 2014.
More or less supervised supersense tagging of twit-
ter. Proc. of* SEM, pages 1–11.
</reference>
<page confidence="0.987471">
336
</page>
<figure confidence="0.346507666666667">
Shen Li, Jo˜ao Grac¸a, and Ben Taskar. 2012. Wiki- vised relation extraction with linked arguments. In
ly supervised part-of-speech tagging. In Proc. of Proc. of EMNLP.
EMNLP.
</figure>
<reference confidence="0.999529652173913">
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classifica-
tion from labeled and unlabeled documents using
em. Machine learning, 39.
Joakim Nivre, Johan Hall, Sandra Kubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proc. of LREC,
May.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proc. of ACL.
Mitchell Koch John Gilmer Stephen Soderland and
Daniel S Weld. 2014. Type-aware distantly super-
Anders Søgaard. 2011. Semisupervised condensed
nearest neighbor for part-of-speech tagging. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: Short Papers - Volume 2, HLT
’11, pages 48–52, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1–12.
Ashish Vaswani, Adam Pauls, and David Chiang.
2010. Efficient optimization of an mdl-inspired ob-
jective function for unsupervised part-of-speech tag-
ging. In Proceedings of the ACL 2010 Conference
Short Papers, pages 209–214. Association for Com-
putational Linguistics.
Hui Zhang and John DeNero. 2014. Observational ini-
tialization of type-supervised taggers. In Proceed-
ings of the Association for Computational Linguis-
tics (Short Paper Track).
</reference>
<page confidence="0.998453">
337
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.895554">
<title confidence="0.999727">Model Selection for Type-Supervised Learning with Application to POS Tagging</title>
<author confidence="0.964656">Toutanova Waleed Chourdhury Hoifung Poon</author>
<affiliation confidence="0.999407">Microsoft Research School of Computer Science Microsoft Research</affiliation>
<address confidence="0.999266">Redmond, WA, USA Carnegie Mellon University Redmond, WA, USA</address>
<abstract confidence="0.99704844">Model selection (picking, for example, the feature set and the regularization strength) is crucial for building high-accuracy NLP models. In supervised learning, we can estimate the accuracy of a model on a subset of the labeled data and choose the model with the highest accuracy. In contrast, here we focus on type-supervised learning, which uses constraints over the possible labels for word types for supervision, and labeled data is either not available or very small. For the setting where no labeled data is available, we perform a comparative study of previously proposed and one novel model selection criterion on type-supervised POS-tagging in nine languages. For the setting where a small labeled set is available, we show that the set should be used for semi-supervised learning rather than for model selection only – using it for model selection reduces the error by less than 5%, whereas using it for semi-supervised learning reduces the error by 44%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD ’08,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="2191" citStr="Bollacker et al., 2008" startWordPosition="339" endWordPosition="342">sion is often readily available. For example, corpora labeled with POS tags at the token level are only available for around 35 languages, while tag dictionaries of the ∗This research was conducted during the author’s internship at Microsoft Research. form displayed in Fig. 1 are available for many more languages, either in commercial dictionaries or community created resources such as Wiktionary. Tag dictionaries provide type-level supervision for word types in the lexicon. Similarly, while sentences labeled with named entities are scarce, gazetteers and databases are more readily available (Bollacker et al., 2008). There has been substantial research on how best to build models using such type-level supervision, for POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwate</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD ’08, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In CoNLL-X.</booktitle>
<contexts>
<context position="11647" citStr="Buchholz and Marsi, 2006" startWordPosition="1895" endWordPosition="1898"> and a weight vector A. We use the feature set described in (Li et al., 2012): transition features, word-tag features ((yi, xi)) (lowercased words with frequency greater than a threshold), whether the word contains a hyphen and/or starts with a capital letter, character suffixes, and whether the word contains a digit. We initialize the transition and emission distributions of the HMM using unambiguous words as proposed by (Zhang and DeNero, 2014). Data. We use the Danish, Dutch, German, Greek, English, Italian, Portuguese, Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We map the POS labels in the CoNLL datasets to the universal POS tagset (Petrov et al., 2012). We use the tag dictionaries provided by Li et al. (2012). Model configurations. For each 2The labeled data loss is the negative joint loglikelihood of the observed token sequences and labels: − E.,y∈TL log pθ(X, Y). 3We split the labeled set into five folds, and for each setting of the hyper-parameters train five different models on 45 -ths of the data, estimating accuracy on the remaining 15 -th. We average the accuracy estimates from different folds and use this as a combined</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Scott Gaffney</author>
<author>Flavian Vasile</author>
</authors>
<title>Learning a named entity tagger from gazetteers with the partial perceptron. In AAAI Spring Symposium: Learning by Reading and Learning to Read,</title>
<date>2009</date>
<pages>7--13</pages>
<contexts>
<context position="2425" citStr="Carlson et al., 2009" startWordPosition="376" endWordPosition="379">soft Research. form displayed in Fig. 1 are available for many more languages, either in commercial dictionaries or community created resources such as Wiktionary. Tag dictionaries provide type-level supervision for word types in the lexicon. Similarly, while sentences labeled with named entities are scarce, gazetteers and databases are more readily available (Bollacker et al., 2008). There has been substantial research on how best to build models using such type-level supervision, for POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection </context>
</contexts>
<marker>Carlson, Gaffney, Vasile, 2009</marker>
<rawString>Andrew Carlson, Scott Gaffney, and Flavian Vasile. 2009. Learning a named entity tagger from gazetteers with the partial perceptron. In AAAI Spring Symposium: Learning by Reading and Learning to Read, pages 7–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In ISMB,</booktitle>
<volume>volume</volume>
<pages>77--86</pages>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven, Johan Kumlien, et al. 1999. Constructing biological knowledge bases by extracting information from text sources. In ISMB, volume 1999, pages 77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Duong</author>
<author>Trevor Cohn</author>
<author>Karin Verspoor</author>
<author>Steven Bird</author>
<author>Paul Cook</author>
</authors>
<title>What can we get from 1000 tokens? a case study of multilingual pos tagging for resource-poor languages. In</title>
<date>2014</date>
<booktitle>Proc. of EMNLP.</booktitle>
<contexts>
<context position="3324" citStr="Duong et al., 2014" startWordPosition="520" endWordPosition="523">ed criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection criteria. For POS tagging, there is also work on using both type-level supervision from lexicons, and projection from another language (T¨ackstr¨om et al., 2013). Methods for training with a small labeled set have also been developed (Søgaard, 2011; Garrette and Baldridge, 2013; Duong et al., 2014), but there have not been studies on the utility of a small labeled set for model selection versus model training. Our contributions are: 1) a simple and generally applicable model selection criterion for type-supervised learning, 2) the first multi-lingual systematic evaluation of model selection criteria for type-supervised models, 3) empirical evidence that, if a small labeled set is available, the set should be used for semi-supervised 332 Proceedings of the 19th Conference on Computational Language Learning, pages 332–337, Beijing, China, July 30-31, 2015. c�2015 Association for Computati</context>
</contexts>
<marker>Duong, Cohn, Verspoor, Bird, Cook, 2014</marker>
<rawString>Long Duong, Trevor Cohn, Karin Verspoor, Steven Bird, and Paul Cook. 2014. What can we get from 1000 tokens? a case study of multilingual pos tagging for resource-poor languages. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Learning a part-of-speech tagger from two hours of annotation.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="3303" citStr="Garrette and Baldridge, 2013" startWordPosition="516" endWordPosition="519">g. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection criteria. For POS tagging, there is also work on using both type-level supervision from lexicons, and projection from another language (T¨ackstr¨om et al., 2013). Methods for training with a small labeled set have also been developed (Søgaard, 2011; Garrette and Baldridge, 2013; Duong et al., 2014), but there have not been studies on the utility of a small labeled set for model selection versus model training. Our contributions are: 1) a simple and generally applicable model selection criterion for type-supervised learning, 2) the first multi-lingual systematic evaluation of model selection criteria for type-supervised models, 3) empirical evidence that, if a small labeled set is available, the set should be used for semi-supervised 332 Proceedings of the 19th Conference on Computational Language Learning, pages 332–337, Beijing, China, July 30-31, 2015. c�2015 Asso</context>
</contexts>
<marker>Garrette, Baldridge, 2013</marker>
<rawString>Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. In Proc. of NAACL-HLT, pages 138–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging. In Annual meeting-association for computational linguistics.</title>
<date>2007</date>
<contexts>
<context position="2813" citStr="Goldwater and Griffiths, 2007" startWordPosition="435" endWordPosition="439">., 2008). There has been substantial research on how best to build models using such type-level supervision, for POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection criteria. For POS tagging, there is also work on using both type-level supervision from lexicons, and projection from another language (T¨ackstr¨om et al., 2013). Methods for training with a small labeled set have also been developed (Søgaard, 2011; Garrette and Baldridge, 2013; Duong et al., 2014), but there have not been studies on the utility of a small labeled set for model selecti</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Annual meeting-association for computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Johannsen</author>
<author>Dirk Hovy</author>
<author>H´ector Martınez Alonso</author>
<author>Barbara Plank</author>
<author>Anders Søgaard</author>
</authors>
<title>More or less supervised supersense tagging of twitter.</title>
<date>2014</date>
<booktitle>Proc. of* SEM,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="2470" citStr="Johannsen et al., 2014" startWordPosition="384" endWordPosition="387">e available for many more languages, either in commercial dictionaries or community created resources such as Wiktionary. Tag dictionaries provide type-level supervision for word types in the lexicon. Similarly, while sentences labeled with named entities are scarce, gazetteers and databases are more readily available (Bollacker et al., 2008). There has been substantial research on how best to build models using such type-level supervision, for POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection criteria. For POS tagging, there is also work</context>
</contexts>
<marker>Johannsen, Hovy, Alonso, Plank, Søgaard, 2014</marker>
<rawString>Anders Johannsen, Dirk Hovy, H´ector Martınez Alonso, Barbara Plank, and Anders Søgaard. 2014. More or less supervised supersense tagging of twitter. Proc. of* SEM, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2445" citStr="Mintz et al., 2009" startWordPosition="380" endWordPosition="383">splayed in Fig. 1 are available for many more languages, either in commercial dictionaries or community created resources such as Wiktionary. Tag dictionaries provide type-level supervision for word types in the lexicon. Similarly, while sentences labeled with named entities are scarce, gazetteers and databases are more readily available (Bollacker et al., 2008). There has been substantial research on how best to build models using such type-level supervision, for POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection criteria. For POS ta</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew Kachites McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using em.</title>
<date>2000</date>
<booktitle>Machine learning,</booktitle>
<pages>39</pages>
<contexts>
<context position="9536" citStr="Nigam et al., 2000" startWordPosition="1549" endWordPosition="1552">ks used a labeled set for supervised hyper-parameter selection even when only type-level supervision is assumed to be available for training (Vaswani et al., 2010; Soderland and Weld, 2014). In this section, we want to answer the question: if a small labeled set is available, what are the potential gains from using it for model selection only, versus using it for both model training and model selection? A simple way to use a small labeled set for parameter training together with a larger unlabeled set in our type-supervised learning setting, is to do semi-supervised model training as follows (Nigam et al., 2000): Starting with our training loss function defined using a lexicon lex and unlabeled set TU L(B; lex, TU, hm), we define a combined loss function using both the unlabeled set TU and the labeled set TL: L(B; lex, TU, hm) + AL(B; lex, TL, hm). We then select parameters Bm to minimize the new loss function, where A is now an additional hyperparameter that usually 1Note that this criterion underestimates the performance of all models in consideration by virtue of evaluating model versions trained using a subset of the full lexicon, but it can still be useful for ranking the models. gives more weig</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using em. Machine learning, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra Kubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="11668" citStr="Nivre et al., 2007" startWordPosition="1899" endWordPosition="1902"> use the feature set described in (Li et al., 2012): transition features, word-tag features ((yi, xi)) (lowercased words with frequency greater than a threshold), whether the word contains a hyphen and/or starts with a capital letter, character suffixes, and whether the word contains a digit. We initialize the transition and emission distributions of the HMM using unambiguous words as proposed by (Zhang and DeNero, 2014). Data. We use the Danish, Dutch, German, Greek, English, Italian, Portuguese, Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We map the POS labels in the CoNLL datasets to the universal POS tagset (Petrov et al., 2012). We use the tag dictionaries provided by Li et al. (2012). Model configurations. For each 2The labeled data loss is the negative joint loglikelihood of the observed token sequences and labels: − E.,y∈TL log pθ(X, Y). 3We split the labeled set into five folds, and for each setting of the hyper-parameters train five different models on 45 -ths of the data, estimating accuracy on the remaining 15 -th. We average the accuracy estimates from different folds and use this as a combined estimate of the accu</context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proc. of LREC,</booktitle>
<contexts>
<context position="11763" citStr="Petrov et al., 2012" startWordPosition="1917" endWordPosition="1920">yi, xi)) (lowercased words with frequency greater than a threshold), whether the word contains a hyphen and/or starts with a capital letter, character suffixes, and whether the word contains a digit. We initialize the transition and emission distributions of the HMM using unambiguous words as proposed by (Zhang and DeNero, 2014). Data. We use the Danish, Dutch, German, Greek, English, Italian, Portuguese, Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We map the POS labels in the CoNLL datasets to the universal POS tagset (Petrov et al., 2012). We use the tag dictionaries provided by Li et al. (2012). Model configurations. For each 2The labeled data loss is the negative joint loglikelihood of the observed token sequences and labels: − E.,y∈TL log pθ(X, Y). 3We split the labeled set into five folds, and for each setting of the hyper-parameters train five different models on 45 -ths of the data, estimating accuracy on the remaining 15 -th. We average the accuracy estimates from different folds and use this as a combined estimate of the accuracy of a model trained using the full labeled and unlabeled set, given these hyperparameters. </context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proc. of LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2403" citStr="Smith and Eisner, 2005" startWordPosition="372" endWordPosition="375">or’s internship at Microsoft Research. form displayed in Fig. 1 are available for many more languages, either in commercial dictionaries or community created resources such as Wiktionary. Tag dictionaries provide type-level supervision for word types in the lexicon. Similarly, while sentences labeled with named entities are scarce, gazetteers and databases are more readily available (Bollacker et al., 2008). There has been substantial research on how best to build models using such type-level supervision, for POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-super</context>
<context position="6366" citStr="Smith and Eisner, 2005" startWordPosition="1009" endWordPosition="1012">pment set Tdev. In the following subsections, we discuss definitions of eval when the development set Tdev is labeled and when it is unlabeled, respectively. 2.1 Tdev is labeled When the development set Tdev is labeled, a natural choice of eval is token-level prediction accuHere, we use i to index all tokens in Tdev; ygold[i] denotes the correct POS tag, and ym[i] denotes the predicted POS tag of the i-th token obtained with hyperparameters hm. 2.2 Tdev is unlabeled When token-supervision is not available, we cannot compute eval,up. Instead, previous work on POS tagging with type supervision (Smith and Eisner, 2005) used: �evalcond(m,Tdev) = log � pθm(y |x), xETdev yElex[x] �evaljoint(m, Tdev) = log � pθm(x, y) xETdev yElex[x] evalcond estimates the conditional log-likelihood of “lex-compatible” labels given token sequences, while evaljoint estimates the joint log-likelihood of lex-compatible labels and token sequences. The held-out lexicon criterion. We propose a new model selection criterion which estimates prediction accuracy more directly and is applicable to any model type, without requiring that the model define conditional or joint probabilities of label sequences. The idea behind this proposed cr</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Koch John Gilmer Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<date>2014</date>
<note>Type-aware distantly super-</note>
<contexts>
<context position="2918" citStr="Soderland and Weld, 2014" startWordPosition="455" endWordPosition="458">or POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection criteria. For POS tagging, there is also work on using both type-level supervision from lexicons, and projection from another language (T¨ackstr¨om et al., 2013). Methods for training with a small labeled set have also been developed (Søgaard, 2011; Garrette and Baldridge, 2013; Duong et al., 2014), but there have not been studies on the utility of a small labeled set for model selection versus model training. Our contributions are: 1) a simple and generally applicable model selection cri</context>
<context position="9106" citStr="Soderland and Weld, 2014" startWordPosition="1473" endWordPosition="1476">on over gold labels consistent with the lexicon for x, resulting in the following approximation PD(x , ygold, ym) a c(x, ym) X 1(x E lexdev, ygold E lexdev[x]) |lexdev[x]| We then compute the expected accuracy as evaldevlex = ED [1(ygold = ym)], and select the hyperparameter configuration mˆ which maximizes this criterion, then re-train the model with the full lexicon lex.1 3 How to best use a small labeled set TL? Several prior works used a labeled set for supervised hyper-parameter selection even when only type-level supervision is assumed to be available for training (Vaswani et al., 2010; Soderland and Weld, 2014). In this section, we want to answer the question: if a small labeled set is available, what are the potential gains from using it for model selection only, versus using it for both model training and model selection? A simple way to use a small labeled set for parameter training together with a larger unlabeled set in our type-supervised learning setting, is to do semi-supervised model training as follows (Nigam et al., 2000): Starting with our training loss function defined using a lexicon lex and unlabeled set TU L(B; lex, TU, hm), we define a combined loss function using both the unlabeled</context>
</contexts>
<marker>Soderland, Weld, 2014</marker>
<rawString>Mitchell Koch John Gilmer Stephen Soderland and Daniel S Weld. 2014. Type-aware distantly super-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Semisupervised condensed nearest neighbor for part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>48--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3273" citStr="Søgaard, 2011" startWordPosition="514" endWordPosition="515">ervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection criteria. For POS tagging, there is also work on using both type-level supervision from lexicons, and projection from another language (T¨ackstr¨om et al., 2013). Methods for training with a small labeled set have also been developed (Søgaard, 2011; Garrette and Baldridge, 2013; Duong et al., 2014), but there have not been studies on the utility of a small labeled set for model selection versus model training. Our contributions are: 1) a simple and generally applicable model selection criterion for type-supervised learning, 2) the first multi-lingual systematic evaluation of model selection criteria for type-supervised models, 3) empirical evidence that, if a small labeled set is available, the set should be used for semi-supervised 332 Proceedings of the 19th Conference on Computational Language Learning, pages 332–337, Beijing, China,</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Semisupervised condensed nearest neighbor for part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 48–52, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the Association for Computational Linguistics, 1:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Adam Pauls</author>
<author>David Chiang</author>
</authors>
<title>Efficient optimization of an mdl-inspired objective function for unsupervised part-of-speech tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>209--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2891" citStr="Vaswani et al., 2010" startWordPosition="451" endWordPosition="454">e-level supervision, for POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection criteria. For POS tagging, there is also work on using both type-level supervision from lexicons, and projection from another language (T¨ackstr¨om et al., 2013). Methods for training with a small labeled set have also been developed (Søgaard, 2011; Garrette and Baldridge, 2013; Duong et al., 2014), but there have not been studies on the utility of a small labeled set for model selection versus model training. Our contributions are: 1) a simple and generally app</context>
<context position="9079" citStr="Vaswani et al., 2010" startWordPosition="1469" endWordPosition="1472">e a uniform distribution over gold labels consistent with the lexicon for x, resulting in the following approximation PD(x , ygold, ym) a c(x, ym) X 1(x E lexdev, ygold E lexdev[x]) |lexdev[x]| We then compute the expected accuracy as evaldevlex = ED [1(ygold = ym)], and select the hyperparameter configuration mˆ which maximizes this criterion, then re-train the model with the full lexicon lex.1 3 How to best use a small labeled set TL? Several prior works used a labeled set for supervised hyper-parameter selection even when only type-level supervision is assumed to be available for training (Vaswani et al., 2010; Soderland and Weld, 2014). In this section, we want to answer the question: if a small labeled set is available, what are the potential gains from using it for model selection only, versus using it for both model training and model selection? A simple way to use a small labeled set for parameter training together with a larger unlabeled set in our type-supervised learning setting, is to do semi-supervised model training as follows (Nigam et al., 2000): Starting with our training loss function defined using a lexicon lex and unlabeled set TU L(B; lex, TU, hm), we define a combined loss functi</context>
</contexts>
<marker>Vaswani, Pauls, Chiang, 2010</marker>
<rawString>Ashish Vaswani, Adam Pauls, and David Chiang. 2010. Efficient optimization of an mdl-inspired objective function for unsupervised part-of-speech tagging. In Proceedings of the ACL 2010 Conference Short Papers, pages 209–214. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>John DeNero</author>
</authors>
<title>Observational initialization of type-supervised taggers.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics</booktitle>
<institution>(Short Paper Track).</institution>
<contexts>
<context position="11473" citStr="Zhang and DeNero, 2014" startWordPosition="1870" endWordPosition="1873">ing a multinomial distribution for the local emissions and transitions, this model uses a log-linear distribution (i.e., p(xi |yi) a exp ATf(xi, yi)) with a feature vector f and a weight vector A. We use the feature set described in (Li et al., 2012): transition features, word-tag features ((yi, xi)) (lowercased words with frequency greater than a threshold), whether the word contains a hyphen and/or starts with a capital letter, character suffixes, and whether the word contains a digit. We initialize the transition and emission distributions of the HMM using unambiguous words as proposed by (Zhang and DeNero, 2014). Data. We use the Danish, Dutch, German, Greek, English, Italian, Portuguese, Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We map the POS labels in the CoNLL datasets to the universal POS tagset (Petrov et al., 2012). We use the tag dictionaries provided by Li et al. (2012). Model configurations. For each 2The labeled data loss is the negative joint loglikelihood of the observed token sequences and labels: − E.,y∈TL log pθ(X, Y). 3We split the labeled set into five folds, and for each setting of the hyper-parameters trai</context>
</contexts>
<marker>Zhang, DeNero, 2014</marker>
<rawString>Hui Zhang and John DeNero. 2014. Observational initialization of type-supervised taggers. In Proceedings of the Association for Computational Linguistics (Short Paper Track).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>