<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007446">
<title confidence="0.878668">
Variational Inference for Grammar Induction with Prior Knowledge
</title>
<author confidence="0.873231">
Shay B. Cohen and Noah A. Smith
</author>
<affiliation confidence="0.903122">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.999502">
{scohen,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999694416666667">
Variational EM has become a popular
technique in probabilistic NLP with hid-
den variables. Commonly, for computa-
tional tractability, we make strong inde-
pendence assumptions, such as the mean-
field assumption, in approximating pos-
terior distributions over hidden variables.
We show how a looser restriction on the
approximate posterior, requiring it to be a
mixture, can help inject prior knowledge
to exploit soft constraints during the varia-
tional E-step.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999762206896552">
Learning natural language in an unsupervised way
commonly involves the expectation-maximization
(EM) algorithm to optimize the parameters of a
generative model, often a probabilistic grammar
(Pereira and Schabes, 1992). Later approaches in-
clude variational EM in a Bayesian setting (Beal
and Gharamani, 2003), which has been shown to
obtain even better results for various natural lan-
guage tasks over EM (e.g., Cohen et al., 2008).
Variational EM usually makes the mean-field
assumption, factoring the posterior over hidden
variables into independent distributions. Bishop et
al. (1998) showed how to use a less strict assump-
tion: a mixture of factorized distributions.
In other work, soft or hard constraints on the
posterior during the E-step have been explored
in order to improve performance. For example,
Smith and Eisner (2006) have penalized the ap-
proximate posterior over dependency structures
in a natural language grammar induction task to
avoid long range dependencies between words.
Grac¸a et al. (2007) added linear constraints on ex-
pected values of features of the hidden variables in
an alignment task.
In this paper, we use posterior mixtures to inject
bias or prior knowledge into a Bayesian model.
We show that empirically, injecting prior knowl-
edge improves performance on an unsupervised
Chinese grammar induction task.
</bodyText>
<sectionHeader confidence="0.985539" genericHeader="method">
2 Variational Mixtures with Constraints
</sectionHeader>
<bodyText confidence="0.9993313">
Our EM variant encodes prior knowledge in an ap-
proximate posterior by constraining it to be from
a mixture family of distributions. We will use x to
denote observable random variables, y to denote
hidden structure, and 0 to denote the to-be-learned
parameters of the model (coming from a subset of
R` for some E). α will denote the parameters of
a prior over 0. The mean-field assumption in the
Bayesian setting assumes that the posterior has a
factored form:
</bodyText>
<equation confidence="0.998779">
q(0, y) = q(0)q(y) (1)
</equation>
<bodyText confidence="0.9958985">
Traditionally, variational inference with the mean-
field assumption alternates between an E-step
which optimizes q(y) and then an M-step which
optimizes q(0).1 The mean-field assumption
makes inference feasible, at the expense of op-
timizing a looser lower bound on the likelihood
(Bishop, 2006). The lower bound that the algo-
rithm optimizes is the following:
</bodyText>
<equation confidence="0.993278">
F(q(0, y), α) = Eq(θ,y)[log p(x, y, 0  |α)]+H(q)
</equation>
<bodyText confidence="0.968864214285714">
(2)
where H(q) denotes the entropy of distribution q.
We focus on changing the E-step and as a result,
changing the underlying bound, F(q(0, y), α).
Similarly to Bishop et al. (1998), instead of mak-
ing the strict mean-field assumption, we assume
that the variational model is a mixture. One com-
ponent of the mixture might take the traditional
form, but others will be used to encourage certain
1This optimization can be nested inside another EM al-
gorithm that optimizes α; this is our approach. q(θ) is tra-
ditionally conjugate to the likelihood for computational rea-
sons, but our method is not limited to that kind of prior, as
seen in the experiments.
</bodyText>
<page confidence="0.783151">
1
</page>
<note confidence="0.9261125">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 1–4,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.8931565">
tendencies considered a priori to be appropriate.
Denoting the probability simplex of dimension r
</bodyText>
<equation confidence="0.980913666666667">
4r = {hλ1, ..., λri ∈ Rr : λi ≥ 0, Eri=1 λi =
1}, we require that:
q(θ,y  |λ) = Eri=1 λiqi(y)qi(θ) (3)
</equation>
<bodyText confidence="0.998567277777778">
for λ ∈ 4r. Qi will denote the family of distri-
butions for the ith mixture component, and Q(4r)
will denote the family implied by the mixture of
Q1, ... , Qr where the mixture coefficients λ ∈
4r. λ comprise r additional variational param-
eters, in addition to parameters for each qi(y) and
qi(θ).
When one of the mixture components qi is suf-
ficiently expressive, λ will tend toward a degener-
ate solution. In order to force all mixture compo-
nents to play a role—even at the expense of the
tightness of the variational bound—we will im-
pose hard constraints on λ: λ ∈ ˜4r ⊂ 4r. In
our experiments (§3), ˜4r will be mostly a line seg-
ment corresponding to two mixture coefficients.
The role of the variational EM algorithm is to
optimize the variational bound in Eq. 2 with re-
spect to q(y), q(θ), and λ. Keeping this intention
in mind, we can replace the E-step and M-step in
the original variational EM algorithm with 2r + 1
coordinate ascent steps, for 1 ≤ i ≤ r:
E-step: For each i ∈ {1, ..., r}, optimize the
bound given λ and qi0(y)|i0∈{1,...,r}\{i} and
qi0(θ)|i0∈{1,...,r} by selecting a new distribution
qi(y).
M-step: For each i ∈ {1, ..., r}, optimize the
bound given λ and qi0(θ)|i0∈{1,...,r}\{i} and
qi0(y)|i0∈{1,...,r} by selecting a new distribution
qi(θ).
C-step: Optimize the bound by selecting a new set
of coefficients λ ∈ ˜4r in order to optimize the
bound with respect to the mixture coefficients.
We call the revised algorithm constrained mix-
ture variational EM.
For a distribution r(h), we denote by KL(Qikr)
the following:
</bodyText>
<equation confidence="0.9990475">
KL(Qikr) = min
q∈Qi KL(q(h)kr)) (4)
</equation>
<bodyText confidence="0.989087333333333">
where KL(·k·) denotes the Kullback-Leibler di-
vergence.
The next proposition, which is based on a result
in Grac¸a et al. (2007), gives an intuition of how
modifying the variational EM algorithm with Q =
Q(˜4r) affects the solution:
Proposition 1. Constrained mixture variational
EM finds local maxima for a function G(q, α)
such that
</bodyText>
<equation confidence="0.966245833333333">
log p(x  |α)− min
λ∈ ˜4r L(λ, α) ≤ G(q, α) ≤ log p(x  |α)
(5)
r
where L(λ, α) = λiKL(Qikp(θ, y  |x, α)).
i=1
</equation>
<bodyText confidence="0.9997744">
We can understand mixture variational EM as
penalizing the likelihood with a term bounded by
a linear function of the λ, minimized over ˜4r. We
will exploit that bound in §2.2 for computational
tractability.
</bodyText>
<subsectionHeader confidence="0.993788">
2.1 Simplex Annealing
</subsectionHeader>
<bodyText confidence="0.999946769230769">
The variational EM algorithm still identifies only
local maxima. Different proposals have been for
pushing EM toward a global maximum. In many
cases, these methods are based on choosing dif-
ferent initializations for the EM algorithm (e.g.,
repeated random initializations or a single care-
fully designed initializer) such that it eventually
gets closer to a global maximum.
We follow the idea of annealing proposed in
Rose et al. (1990) and Smith and Eisner (2006) for
the λ by gradually loosening hard constraints on λ
as the variational EM algorithm proceeds. We de-
fine a sequence of ˜4r(t) for t = 0, 1, ... such that
</bodyText>
<equation confidence="0.839982">
˜4r(t) ⊆ ˜4r(t + 1). First, we have the inequality:
KL(Q(˜4r(t))kp(θ,y  |x, α) (6)
≥ KL(Q( ˜4r(t + 1))kp(θ, y  |x, α))
</equation>
<bodyText confidence="0.99974">
We say that the annealing schedule is τ-separated
if we have for any α:
</bodyText>
<equation confidence="0.912788">
KL(Q(˜4r(t))kp(θ,y  |x, α)) (7)
˜4r(t + 1))kp(θ, y  |x, α)) − τ
2(t+1)
requires consecutive families
Q(˜4r(t)) and Q(˜4r(t + 1)) to be similar.
</equation>
<bodyText confidence="0.9618558">
Proposition 1 stated the bound we optimize,
which penalizes the likelihood by subtracting a
positive KL divergence from it. With the τ-
separation condition we can show that even though
we penalize likelihood, the variational EM algo-
rithm will still increase likelihood by a certain
amount. Full details are omitted for space and can
be found in ?).
≤ KL(Q(
τ-separation
</bodyText>
<page confidence="0.965166">
2
</page>
<bodyText confidence="0.9147705">
Input: initial parameters α(0), observed data x,
annealing schedule ˜�r : ICY --+ 24r
Output: learned parameters α and approximate
posterior q(θ, y)
</bodyText>
<equation confidence="0.995254407407408">
t +— 1;
repeat
E-step: repeat
E-step: forall i E [r] do: q(t+1) i(y) +— argmax
q(Y)∈Qi
F 0(Ej6=i Ajq(t)
i (θ)q(y) + Aiq(t)
i q(y), α(t))
M-step: forall i E [r] do: q(t+1) i(θ) +— argmax
q(θ)∈Qi
F0(Ej6=i Ajq(θ)q(t)
i (y) + Aiq(t)
i q(y), α(t))
C-step: λ(t+1) +—
argmax F0(Erj=1 Ajq�t)(θ)q(t)(y), α(t))
λ∈
˜4r(t)
until convergence ;
M-step: α(t+1) +—
argmax
α F0(Er i=1 Aiq(t+1)
i (θ)q(t+1)
i (y), α)
t + —t + 1;
until convergence ;
return α(t), Eri=1 Aiqi(t) (θ)q(t)
i (y)
</equation>
<figureCaption confidence="0.999742">
Figure 1: The constrained variational mixture EM algorithm.
</figureCaption>
<bodyText confidence="0.505802">
[n] denotes {1, ..., n}.
</bodyText>
<subsectionHeader confidence="0.990165">
2.2 Tractability
</subsectionHeader>
<bodyText confidence="0.999970777777778">
We now turn to further alterations of the bound in
Eq. 2 to make it more tractable. The main problem
is the entropy term which is not easy to compute,
because it includes a log term over a mixture of
distributions from Qi. We require the distributions
in Qi to factorize over the hidden structure y, but
this only helps with the first term in Eq. 2.
We note that because the entropy function is
convex, we can get a lower bound on H(q):
</bodyText>
<equation confidence="0.977915">
H(q) �! Eri=1 AiH(qi) = Eri=1 AiH(qi(e,y))
</equation>
<bodyText confidence="0.987213363636364">
Substituting the modified entropy term into
Eq. 2 still yields a lower bound on the likeli-
hood. This change makes the E-step tractable,
because each distribution qi(y) can be computed
separately by optimizing a bound which depends
only on the variational parameters in that distribu-
tion. In fact, the bound on the left hand side in
Proposition 1 becomes the function that we opti-
mize instead of G(q, α).
Without proper constraints, the A update can be
intractable as well. It requires maximizing a lin-
ear objective (in A) while constraining the A to
be from a particular subspace of the probability
simplex, ˜Ar(t). To solve this issue, we require
that ˜Ar(t) is polyhedral, making it possible to ap-
ply linear programming (Boyd and Vandenberghe,
2004).
The bound we optimize is:2
with A E ˜Ar(tfinal) and (qi(e, y)) E Qi. The
algorithm for optimizing this bound is in Fig. 1,
which includes an extra M-step to optimize α (see
extended report).
</bodyText>
<sectionHeader confidence="0.999372" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999221805555556">
We tested our method on the unsupervised learn-
ing problem of dependency grammar induction.
For the generative model, we used the dependency
model with valence as it appears in Klein and Man-
ning (2004). We used the data from the Chi-
nese treebank (Xue et al., 2004). Following stan-
dard practice, sentences were stripped of words
and punctuation, leaving part-of-speech tags for
the unsupervised induction of dependency struc-
ture, and sentences of length more than 10 were
removed from the set. We experimented with
a Dirichlet prior over the parameters and logis-
tic normal priors over the parameters, and found
the latter to still be favorable with our method, as
in Cohen et al. (2008). We therefore report results
with our method only for the logistic normal prior.
We do inference on sections 1–270 and 301–1151
of CTB10 (4,909 sentences) by running the EM al-
gorithm for 20 iterations, for which all algorithms
have their variational bound converge.
To evaluate performance, we report the fraction
of words whose predicted parent matches the gold
standard (attachment accuracy). For parsing, we
use the minimum Bayes risk parse.
Our mixture components Qi are based on simple
linguistic tendencies of Chinese syntax. These ob-
servations include the tendency of dependencies to
(a) emanate from the right of the current position
and (b) connect words which are nearby (in string
distance). We experiment with six mixture com-
ponents: (1) RIGHTATTACH: Each word’s parent
is to the word’s right. The root, therefore, is al-
ways the rightmost word; (2) ALLRIGHT: The
rightmost word is the parent of all positions in the
sentence (there is only one such tree); (3) LEFT-
CHAIN: The tree forms a chain, such that each
</bodyText>
<footnote confidence="0.947409">
2This is a less tight bound than the one in Bishop et al.
(1998), but it is easier to handle computationally.
</footnote>
<equation confidence="0.997643857142857">
�Aiqi(e, y), α (8)
r
Ai (Eqi(θ,Y)[log p(e, y, x  |m)] + H(qi(e, y)))
i=1
r
F&apos;
i=1
</equation>
<page confidence="0.995292">
3
</page>
<table confidence="0.999935090909091">
learning setting LEFTCHAIN 34.9
vanilla EM 38.3
LN, mean-field 48.9
This paper: I II III
RIGHTATTACH 49.1 47.1 49.8
ALLRIGHT 49.4 49.4 48.4
LEFTCHAIN 47.9 46.5 49.9
VERBASROOT 50.5 50.2 49.4
NOUNSEQUENCE 48.9 48.9 49.9
SHORTDEP 49.5 48.4 48.4
RA+VAR+SD 50.5 50.6 50.1
</table>
<tableCaption confidence="0.995805833333333">
Table 1: Results (attachment accuracy). The baselines are
LEFTCHAIN as a parsing model (attaches each word to the
word on its right), non-Bayesian EM, and mean-field vari-
ational EM without any constraints. These are compared
against the six mixture components mentioned in the text. (I)
corresponds to simplex annealing experiments (A(0)
</tableCaption>
<bodyText confidence="0.988021106382979">
1 = 0.85);
(II–III) correspond to fixed values, 0.85 and 0.95, for the
mixture coefficients. With the last row, A2 to A4 are always
(1 − A1)/3. Boldface denotes the best result in each row.
word is governed by the word to its right; (4) VER-
BASROOT: Only verbs can attach to the wall node
$; (5) NOUNSEQUENCE: Every sequence of n NN
(nouns) is assumed to be a noun phrase, hence the
first n−1 NNs are attached to the last NN; and (6)
SHORTDEP: Allow only dependencies of length
four or less. This is a strict model reminiscent
of the successful application of structural bias to
grammar induction (Smith and Eisner, 2006).
These components are added to a variational
DMV model without the sum-to-1 constraint on
0. This complements variational techniques which
state that the optimal solution during the E-step
for the mean-field variational EM algorithm is a
weighted grammar of the same form of p(x, y  |0)
(DMV in our case). Using the mixture compo-
nents this way has the effect of smoothing the esti-
mated grammar event counts during the E-step, in
the direction of some prior expectations.
Let A1 correspond to the component of the orig-
inal DMV model, and let A2 correspond to one of
the components from the above list. Variational
techniques show that if we let A1 obtain the value
1, then the optimal solution will be A1 = 1 and
A2 = 0. We therefore restrict A1 to be smaller than
1. More specifically, we use an annealing process
which starts by limiting A1 to be ≤ s = 0.85 (and
hence limits A2 to be ≥ 0.15) and increases s at
each step by 1% until s reaches 0.95. In addition,
we also ran the algorithm with A1 fixed at 0.85 and
A1 fixed at 0.95 to check the effectiveness of an-
nealing on the simplex.
Table 1 describes the results of our experi-
ments. In general, using additional mixture com-
ponents has a clear advantage over the mean-field
assumption. The best result with a single mix-
ture is achieved with annealing, and the VERBAS-
ROOT component. A combination of the mix-
tures (RIGHTATTACH) together with VERBAS-
ROOT and SHORTDEP led to an additional im-
provement, implying that proper selection of sev-
eral mixture components together can achieve a
performance gain.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999974111111111">
We described a variational EM algorithm that uses
a mixture model for the variational model. We
refined the algorithm with an annealing mecha-
nism to avoid local maxima. We demonstrated
the effectiveness of the algorithm on a dependency
grammar induction task. Our results show that
with a good choice of mixture components and
annealing schedule, we achieve improvements for
this task over mean-field variational inference.
</bodyText>
<sectionHeader confidence="0.998677" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998534588235294">
M. J. Beal and Z. Gharamani. 2003. The variational
Bayesian EM algorithm for incomplete data: with appli-
cation to scoring graphical model structures. In Proc. of
Bayesian Statistics.
C. Bishop, N. Lawrence, T. S. Jaakkola, and M. I. Jordan.
1998. Approximating posterior distributions in belief net-
works using mixtures. In Advances in NIPS.
C. M. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
S. Boyd and L. Vandenberghe. 2004. Convex Optimization.
Cambridge Press.
S. B. Cohen and N. A. Smith. 2009. Variational inference
with prior knowledge. Technical report, Carnegie Mellon
University.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logis-
tic normal priors for unsupervised probabilistic grammar
induction. In Advances in NIPS.
J. V. Grac¸a, K. Ganchev, and B. Taskar. 2007. Expectation
maximization and posterior constraints. In Advances in
NIPS.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. of ACL.
F. C. N. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In Proc. of ACL.
K. Rose, E. Gurewitz, and G. C. Fox. 1990. Statistical me-
chanics and phrase transitions in clustering. Physical Re-
view Letters, 65(8):945–948.
N. A. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In Proc. of
COLING-ACL.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn
Chinese Treebank: Phrase structure annotation of a large
corpus. Natural Language Engineering, 10(4):1–30.
</reference>
<page confidence="0.996676">
4
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.623803">
<title confidence="0.99861">Variational Inference for Grammar Induction with Prior Knowledge</title>
<author confidence="0.999838">B Cohen A Smith</author>
<affiliation confidence="0.999401666666667">Language Technologies Institute School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.970713461538461">Variational EM has become a popular technique in probabilistic NLP with hidden variables. Commonly, for computational tractability, we make strong independence assumptions, such as the meanfield assumption, in approximating posterior distributions over hidden variables. We show how a looser restriction on the approximate posterior, requiring it to be a mixture, can help inject prior knowledge to exploit soft constraints during the variational E-step.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M J Beal</author>
<author>Z Gharamani</author>
</authors>
<title>The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures.</title>
<date>2003</date>
<booktitle>In Proc. of Bayesian Statistics.</booktitle>
<contexts>
<context position="1024" citStr="Beal and Gharamani, 2003" startWordPosition="141" endWordPosition="144">ndence assumptions, such as the meanfield assumption, in approximating posterior distributions over hidden variables. We show how a looser restriction on the approximate posterior, requiring it to be a mixture, can help inject prior knowledge to exploit soft constraints during the variational E-step. 1 Introduction Learning natural language in an unsupervised way commonly involves the expectation-maximization (EM) algorithm to optimize the parameters of a generative model, often a probabilistic grammar (Pereira and Schabes, 1992). Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008). Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions. Bishop et al. (1998) showed how to use a less strict assumption: a mixture of factorized distributions. In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance. For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a n</context>
</contexts>
<marker>Beal, Gharamani, 2003</marker>
<rawString>M. J. Beal and Z. Gharamani. 2003. The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures. In Proc. of Bayesian Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bishop</author>
<author>N Lawrence</author>
<author>T S Jaakkola</author>
<author>M I Jordan</author>
</authors>
<title>Approximating posterior distributions in belief networks using mixtures.</title>
<date>1998</date>
<booktitle>In Advances in</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="1302" citStr="Bishop et al. (1998)" startWordPosition="183" endWordPosition="186">ariational E-step. 1 Introduction Learning natural language in an unsupervised way commonly involves the expectation-maximization (EM) algorithm to optimize the parameters of a generative model, often a probabilistic grammar (Pereira and Schabes, 1992). Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008). Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions. Bishop et al. (1998) showed how to use a less strict assumption: a mixture of factorized distributions. In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance. For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. Grac¸a et al. (2007) added linear constraints on expected values of features of the hidden variables in an alignment task. In this paper, we use posterior mixtures to inject bias or prior kno</context>
<context position="3167" citStr="Bishop et al. (1998)" startWordPosition="487" endWordPosition="490">ed form: q(0, y) = q(0)q(y) (1) Traditionally, variational inference with the meanfield assumption alternates between an E-step which optimizes q(y) and then an M-step which optimizes q(0).1 The mean-field assumption makes inference feasible, at the expense of optimizing a looser lower bound on the likelihood (Bishop, 2006). The lower bound that the algorithm optimizes is the following: F(q(0, y), α) = Eq(θ,y)[log p(x, y, 0 |α)]+H(q) (2) where H(q) denotes the entropy of distribution q. We focus on changing the E-step and as a result, changing the underlying bound, F(q(0, y), α). Similarly to Bishop et al. (1998), instead of making the strict mean-field assumption, we assume that the variational model is a mixture. One component of the mixture might take the traditional form, but others will be used to encourage certain 1This optimization can be nested inside another EM algorithm that optimizes α; this is our approach. q(θ) is traditionally conjugate to the likelihood for computational reasons, but our method is not limited to that kind of prior, as seen in the experiments. 1 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 1–4, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP t</context>
<context position="11468" citStr="Bishop et al. (1998)" startWordPosition="1924" endWordPosition="1927">s Qi are based on simple linguistic tendencies of Chinese syntax. These observations include the tendency of dependencies to (a) emanate from the right of the current position and (b) connect words which are nearby (in string distance). We experiment with six mixture components: (1) RIGHTATTACH: Each word’s parent is to the word’s right. The root, therefore, is always the rightmost word; (2) ALLRIGHT: The rightmost word is the parent of all positions in the sentence (there is only one such tree); (3) LEFTCHAIN: The tree forms a chain, such that each 2This is a less tight bound than the one in Bishop et al. (1998), but it is easier to handle computationally. �Aiqi(e, y), α (8) r Ai (Eqi(θ,Y)[log p(e, y, x |m)] + H(qi(e, y))) i=1 r F&apos; i=1 3 learning setting LEFTCHAIN 34.9 vanilla EM 38.3 LN, mean-field 48.9 This paper: I II III RIGHTATTACH 49.1 47.1 49.8 ALLRIGHT 49.4 49.4 48.4 LEFTCHAIN 47.9 46.5 49.9 VERBASROOT 50.5 50.2 49.4 NOUNSEQUENCE 48.9 48.9 49.9 SHORTDEP 49.5 48.4 48.4 RA+VAR+SD 50.5 50.6 50.1 Table 1: Results (attachment accuracy). The baselines are LEFTCHAIN as a parsing model (attaches each word to the word on its right), non-Bayesian EM, and mean-field variational EM without any constraint</context>
</contexts>
<marker>Bishop, Lawrence, Jaakkola, Jordan, 1998</marker>
<rawString>C. Bishop, N. Lawrence, T. S. Jaakkola, and M. I. Jordan. 1998. Approximating posterior distributions in belief networks using mixtures. In Advances in NIPS. C. M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boyd</author>
<author>L Vandenberghe</author>
</authors>
<title>Convex Optimization.</title>
<date>2004</date>
<publisher>Cambridge Press.</publisher>
<contexts>
<context position="9490" citStr="Boyd and Vandenberghe, 2004" startWordPosition="1590" endWordPosition="1593">p tractable, because each distribution qi(y) can be computed separately by optimizing a bound which depends only on the variational parameters in that distribution. In fact, the bound on the left hand side in Proposition 1 becomes the function that we optimize instead of G(q, α). Without proper constraints, the A update can be intractable as well. It requires maximizing a linear objective (in A) while constraining the A to be from a particular subspace of the probability simplex, ˜Ar(t). To solve this issue, we require that ˜Ar(t) is polyhedral, making it possible to apply linear programming (Boyd and Vandenberghe, 2004). The bound we optimize is:2 with A E ˜Ar(tfinal) and (qi(e, y)) E Qi. The algorithm for optimizing this bound is in Fig. 1, which includes an extra M-step to optimize α (see extended report). 3 Experiments We tested our method on the unsupervised learning problem of dependency grammar induction. For the generative model, we used the dependency model with valence as it appears in Klein and Manning (2004). We used the data from the Chinese treebank (Xue et al., 2004). Following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised </context>
</contexts>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>S. Boyd and L. Vandenberghe. 2004. Convex Optimization. Cambridge Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Variational inference with prior knowledge.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Variational inference with prior knowledge. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context position="1146" citStr="Cohen et al., 2008" startWordPosition="163" endWordPosition="166">w a looser restriction on the approximate posterior, requiring it to be a mixture, can help inject prior knowledge to exploit soft constraints during the variational E-step. 1 Introduction Learning natural language in an unsupervised way commonly involves the expectation-maximization (EM) algorithm to optimize the parameters of a generative model, often a probabilistic grammar (Pereira and Schabes, 1992). Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008). Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions. Bishop et al. (1998) showed how to use a less strict assumption: a mixture of factorized distributions. In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance. For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. Grac¸a et al. (2007) added linear c</context>
<context position="10381" citStr="Cohen et al. (2008)" startWordPosition="1742" endWordPosition="1745">ndency grammar induction. For the generative model, we used the dependency model with valence as it appears in Klein and Manning (2004). We used the data from the Chinese treebank (Xue et al., 2004). Following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure, and sentences of length more than 10 were removed from the set. We experimented with a Dirichlet prior over the parameters and logistic normal priors over the parameters, and found the latter to still be favorable with our method, as in Cohen et al. (2008). We therefore report results with our method only for the logistic normal prior. We do inference on sections 1–270 and 301–1151 of CTB10 (4,909 sentences) by running the EM algorithm for 20 iterations, for which all algorithms have their variational bound converge. To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard (attachment accuracy). For parsing, we use the minimum Bayes risk parse. Our mixture components Qi are based on simple linguistic tendencies of Chinese syntax. These observations include the tendency of dependencies to (a) eman</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J V Grac¸a</author>
<author>K Ganchev</author>
<author>B Taskar</author>
</authors>
<title>Expectation maximization and posterior constraints.</title>
<date>2007</date>
<booktitle>In Advances in NIPS.</booktitle>
<marker>Grac¸a, Ganchev, Taskar, 2007</marker>
<rawString>J. V. Grac¸a, K. Ganchev, and B. Taskar. 2007. Expectation maximization and posterior constraints. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="9897" citStr="Klein and Manning (2004)" startWordPosition="1660" endWordPosition="1664">aining the A to be from a particular subspace of the probability simplex, ˜Ar(t). To solve this issue, we require that ˜Ar(t) is polyhedral, making it possible to apply linear programming (Boyd and Vandenberghe, 2004). The bound we optimize is:2 with A E ˜Ar(tfinal) and (qi(e, y)) E Qi. The algorithm for optimizing this bound is in Fig. 1, which includes an extra M-step to optimize α (see extended report). 3 Experiments We tested our method on the unsupervised learning problem of dependency grammar induction. For the generative model, we used the dependency model with valence as it appears in Klein and Manning (2004). We used the data from the Chinese treebank (Xue et al., 2004). Following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure, and sentences of length more than 10 were removed from the set. We experimented with a Dirichlet prior over the parameters and logistic normal priors over the parameters, and found the latter to still be favorable with our method, as in Cohen et al. (2008). We therefore report results with our method only for the logistic normal prior. We do inference on sections 1–270 </context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>Y Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="934" citStr="Pereira and Schabes, 1992" startWordPosition="127" endWordPosition="130"> NLP with hidden variables. Commonly, for computational tractability, we make strong independence assumptions, such as the meanfield assumption, in approximating posterior distributions over hidden variables. We show how a looser restriction on the approximate posterior, requiring it to be a mixture, can help inject prior knowledge to exploit soft constraints during the variational E-step. 1 Introduction Learning natural language in an unsupervised way commonly involves the expectation-maximization (EM) algorithm to optimize the parameters of a generative model, often a probabilistic grammar (Pereira and Schabes, 1992). Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008). Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions. Bishop et al. (1998) showed how to use a less strict assumption: a mixture of factorized distributions. In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance. For example, Smith an</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>F. C. N. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rose</author>
<author>E Gurewitz</author>
<author>G C Fox</author>
</authors>
<title>Statistical mechanics and phrase transitions in clustering.</title>
<date>1990</date>
<journal>Physical Review Letters,</journal>
<volume>65</volume>
<issue>8</issue>
<contexts>
<context position="6647" citStr="Rose et al. (1990)" startWordPosition="1090" endWordPosition="1093">alizing the likelihood with a term bounded by a linear function of the λ, minimized over ˜4r. We will exploit that bound in §2.2 for computational tractability. 2.1 Simplex Annealing The variational EM algorithm still identifies only local maxima. Different proposals have been for pushing EM toward a global maximum. In many cases, these methods are based on choosing different initializations for the EM algorithm (e.g., repeated random initializations or a single carefully designed initializer) such that it eventually gets closer to a global maximum. We follow the idea of annealing proposed in Rose et al. (1990) and Smith and Eisner (2006) for the λ by gradually loosening hard constraints on λ as the variational EM algorithm proceeds. We define a sequence of ˜4r(t) for t = 0, 1, ... such that ˜4r(t) ⊆ ˜4r(t + 1). First, we have the inequality: KL(Q(˜4r(t))kp(θ,y |x, α) (6) ≥ KL(Q( ˜4r(t + 1))kp(θ, y |x, α)) We say that the annealing schedule is τ-separated if we have for any α: KL(Q(˜4r(t))kp(θ,y |x, α)) (7) ˜4r(t + 1))kp(θ, y |x, α)) − τ 2(t+1) requires consecutive families Q(˜4r(t)) and Q(˜4r(t + 1)) to be similar. Proposition 1 stated the bound we optimize, which penalizes the likelihood by subtra</context>
</contexts>
<marker>Rose, Gurewitz, Fox, 1990</marker>
<rawString>K. Rose, E. Gurewitz, and G. C. Fox. 1990. Statistical mechanics and phrase transitions in clustering. Physical Review Letters, 65(8):945–948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="1549" citStr="Smith and Eisner (2006)" startWordPosition="224" endWordPosition="227">s, 1992). Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008). Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions. Bishop et al. (1998) showed how to use a less strict assumption: a mixture of factorized distributions. In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance. For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. Grac¸a et al. (2007) added linear constraints on expected values of features of the hidden variables in an alignment task. In this paper, we use posterior mixtures to inject bias or prior knowledge into a Bayesian model. We show that empirically, injecting prior knowledge improves performance on an unsupervised Chinese grammar induction task. 2 Variational Mixtures with Constraints Our EM variant encodes prior knowledge in an approxim</context>
<context position="6675" citStr="Smith and Eisner (2006)" startWordPosition="1095" endWordPosition="1098">with a term bounded by a linear function of the λ, minimized over ˜4r. We will exploit that bound in §2.2 for computational tractability. 2.1 Simplex Annealing The variational EM algorithm still identifies only local maxima. Different proposals have been for pushing EM toward a global maximum. In many cases, these methods are based on choosing different initializations for the EM algorithm (e.g., repeated random initializations or a single carefully designed initializer) such that it eventually gets closer to a global maximum. We follow the idea of annealing proposed in Rose et al. (1990) and Smith and Eisner (2006) for the λ by gradually loosening hard constraints on λ as the variational EM algorithm proceeds. We define a sequence of ˜4r(t) for t = 0, 1, ... such that ˜4r(t) ⊆ ˜4r(t + 1). First, we have the inequality: KL(Q(˜4r(t))kp(θ,y |x, α) (6) ≥ KL(Q( ˜4r(t + 1))kp(θ, y |x, α)) We say that the annealing schedule is τ-separated if we have for any α: KL(Q(˜4r(t))kp(θ,y |x, α)) (7) ˜4r(t + 1))kp(θ, y |x, α)) − τ 2(t+1) requires consecutive families Q(˜4r(t)) and Q(˜4r(t + 1)) to be similar. Proposition 1 stated the bound we optimize, which penalizes the likelihood by subtracting a positive KL divergen</context>
<context position="12823" citStr="Smith and Eisner, 2006" startWordPosition="2157" endWordPosition="2160">0) 1 = 0.85); (II–III) correspond to fixed values, 0.85 and 0.95, for the mixture coefficients. With the last row, A2 to A4 are always (1 − A1)/3. Boldface denotes the best result in each row. word is governed by the word to its right; (4) VERBASROOT: Only verbs can attach to the wall node $; (5) NOUNSEQUENCE: Every sequence of n NN (nouns) is assumed to be a noun phrase, hence the first n−1 NNs are attached to the last NN; and (6) SHORTDEP: Allow only dependencies of length four or less. This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006). These components are added to a variational DMV model without the sum-to-1 constraint on 0. This complements variational techniques which state that the optimal solution during the E-step for the mean-field variational EM algorithm is a weighted grammar of the same form of p(x, y |0) (DMV in our case). Using the mixture components this way has the effect of smoothing the estimated grammar event counts during the E-step, in the direction of some prior expectations. Let A1 correspond to the component of the original DMV model, and let A2 correspond to one of the components from the above list.</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>N. A. Smith and J. Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F-D Chiou</author>
<author>M Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="9960" citStr="Xue et al., 2004" startWordPosition="1674" endWordPosition="1677">ex, ˜Ar(t). To solve this issue, we require that ˜Ar(t) is polyhedral, making it possible to apply linear programming (Boyd and Vandenberghe, 2004). The bound we optimize is:2 with A E ˜Ar(tfinal) and (qi(e, y)) E Qi. The algorithm for optimizing this bound is in Fig. 1, which includes an extra M-step to optimize α (see extended report). 3 Experiments We tested our method on the unsupervised learning problem of dependency grammar induction. For the generative model, we used the dependency model with valence as it appears in Klein and Manning (2004). We used the data from the Chinese treebank (Xue et al., 2004). Following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure, and sentences of length more than 10 were removed from the set. We experimented with a Dirichlet prior over the parameters and logistic normal priors over the parameters, and found the latter to still be favorable with our method, as in Cohen et al. (2008). We therefore report results with our method only for the logistic normal prior. We do inference on sections 1–270 and 301–1151 of CTB10 (4,909 sentences) by running the EM algor</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2004</marker>
<rawString>N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 10(4):1–30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>