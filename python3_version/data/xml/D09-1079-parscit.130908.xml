<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.989845">
Stream-based Randomised Language Models for SMT
</title>
<author confidence="0.999134">
Abby Levenberg Miles Osborne
</author>
<affiliation confidence="0.999724">
School of Informatics School of Informatics
University of Edinburgh University of Edinburgh
</affiliation>
<email confidence="0.993364">
a.levenberg@sms.ed.ac.uk miles@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.99375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846388888889">
Randomised techniques allow very big
language models to be represented suc-
cinctly. However, being batch-based
they are unsuitable for modelling an un-
bounded stream of language whilst main-
taining a constant error rate. We present a
novel randomised language model which
uses an online perfect hash function
to efficiently deal with unbounded text
streams. Translation experiments over
a text stream show that our online ran-
domised model matches the performance
of batch-based LMs without incurring the
computational overhead associated with
full retraining. This opens up the possibil-
ity of randomised language models which
continuously adapt to the massive volumes
of texts published on the Web each day.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999892355555555">
Language models (LM) are an integral feature
of statistical machine translation (SMT) systems.
They assign probabilities to generated hypothe-
ses in the target language informing lexical selec-
tion. The most common form of LMs in SMT
systems are smoothed n-gram models which pre-
dict a word based on a contextual history of n − 1
words. For some languages (such as English) tril-
lions of words are available for training purposes.
This fact, along with the observation that ma-
chine translation quality improves as the amount
of monolingual training material increases, has
lead to the introduction of randomised techniques
for representing large LMs in small space (Talbot
and Osborne, 2007; Talbot and Brants, 2008).
Randomised LMs (RLMs) solve the problem of
representing large, static LMs but they are batch
oriented and cannot incorporate new data with-
out fully retraining from scratch. This property
makes current RLMs ill-suited for modelling the
massive volume of textual material published daily
on the Web. We present a novel RLM which is
capable of incremental (re)training. We use ran-
dom hash functions coupled with an online perfect
hashing algorithm to represent n-grams in small
space. This makes it well-suited for dealing with
an unbounded stream of training material. To our
knowledge this is the first stream-based RLM re-
ported in the machine translation literature. As
well as introducing the basic stream-based RLM,
we also consider adaptation strategies. Perplex-
ity and translation results show that populating
the language model with material chronologically
close to test points yields good results. As with
previous randomised language models, our experi-
ments focus on machine translation but we also ex-
pect that our findings are general and should help
inform the design of other stream-based models.
Section 2 introduces the incrementally retrain-
able randomised LM and section 3 considers re-
lated work; Section 4 then considers the question
of how unbounded text streams should be mod-
elled. Sections 5 and 6 show stream-based trans-
lation results and properties of our novel data-
structure. Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.98866" genericHeader="introduction">
2 Online Bloomier Filter LM
</sectionHeader>
<bodyText confidence="0.999897375">
Our online randomised LM (O-RLM) is based
on the dynamic Bloomier filter (Mortensen et al.,
2005). It is a variant of the batch-based Bloomier
filter LM of Talbot and Brants (2008) which we
refer to as the TB-LM henceforth. As with the
TB-LM, the O-RLM uses random hash functions
to represent n-grams as fingerprints which is the
main source of space savings for the model.
</bodyText>
<sectionHeader confidence="0.57643" genericHeader="method">
2.1 Online Perfect Hashing
</sectionHeader>
<bodyText confidence="0.8482635">
The key difference in our model as compared to
the TB-LM is we use an online perfect hashing
</bodyText>
<page confidence="0.98675">
756
</page>
<note confidence="0.998084">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 756–764,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.9694635">
Figure 1: Inserting an n-gram into the dynamic Bloomier filter. Above: an n-gram is hashed to its target
bucket. Below: the n-gram is transformed into a fingerprint and the same target bucket is scanned. If a
collision occurs that n-gram is diverted to the overflow dictionary; otherwise the fingerprint is stored in
the bucket.
</figureCaption>
<bodyText confidence="0.998300822222222">
function instead of having to precompute the per-
fect hash offline prior to data insertion.
The online perfect hash function uses two data
structures: A and D. A is the main, randomised
data structure and is an array of b dictionaries
A0, ... , Ab−1. D is a lossless data structure which
handles collisions in A. Each of the dictionaries in
A is referred to as a ‘bucket’. In our implementa-
tion the buckets are equally sized arrays of w-bit
cells. These cells hold the fingerprints and values
of n-grams (one n-gram-value pair per cell).
To insert an n-gram x and associated value
v(x) into the model, we select a bucket AZ by
hashing x into the range i → [0, ... , b − 1].
Each bucket has an associated random hash func-
tion, hAi, drawn from a universal hash func-
tion (UHF) family h (Carter and Wegman, 1977),
which is then used to generate the n-gram finger-
print: f(x) = hAi(x).
If the bucket AZ is not full we conduct a scan of
its cells. If the fingerprint f(x) is not already en-
coded in the bucket AZ we add the fingerprint and
value to the first empty cell available. We allocate
a preset number of the least significant bits of each
w-bit cell to hold v(x) and the remaining most sig-
nificant bits for f(x) but this is arbitrary. Any en-
coding scheme, such as the packed representation
of Talbot and Brants (2008), is viable here.
However, if f(x) ∈ AZ already (there is a colli-
sion) we store the n-gram x and associated value
v(x) in the lossless overflow dictionary D instead.
D also holds the n-grams that were hashed to any
buckets that are already full.
To query for the value of an n-gram, we first
check if the gram is in the overflow dictionary D.
If it is, we return the associated value. Otherwise
we query A using the same hash functions and
procedure as insertion. If we find a matching fin-
gerprint in the appropriate bucket AZ we have a
hit with high probability. Deletions and updates
are symmetric to querying except we reset the cell
to the null value or update its value respectively.
As with other randomised models we construct
queries with the appropriate sanity checks to lower
the error rate efficiently (Talbot and Brants, 2008).
</bodyText>
<subsectionHeader confidence="0.997237">
2.2 Data Insertion
</subsectionHeader>
<bodyText confidence="0.999950705882353">
Initially we seed the language model with a large
corpus S in the usual manner associated with
batch LMs. Then, when processing the stream,
we aggregate n-gram counts for some consecu-
tive portion, or epoch, of the input stream. We
can vary the size of stream window. For example
we might batch-up a day or week’s worth of mate-
rial. Intuitively, smaller windows produce results
that are sensitive to small variation in the stream,
while longer windows (corresponding to data over
a longer time period) average out local spikes. The
exact window size is a matter of experimentation.
In our MT experiments (section 5) we can com-
pute counts within the streaming window exactly
but randomised approaches (such as the approxi-
mate counting schemes from section 3) can easily
be employed instead.
</bodyText>
<page confidence="0.995357">
757
</page>
<bodyText confidence="0.999935428571428">
These n-grams and counts are then considered
for insertion into the online model. If we decide
to insert an n-gram, we either update the count of
that n-gram if we previously inserted it or else we
insert it as a new entry. Note that there is some
probability we may encounter a false positive and
update some other n-gram in the model.
</bodyText>
<subsectionHeader confidence="0.994713">
2.3 Properties
</subsectionHeader>
<bodyText confidence="0.999964">
The online perfect hash succeeds by associating
each n-gram with only one cell in A rather than
having it depend on cells (or bits) which may be
shared by other n-grams as with the TB-LM. Since
each n-gram’s encoding in the model uses distinct
bits and is independent of all other events it can
not corrupt other n-grams when deleted.
Adding the overflow dictionary D means that
we use more space than the TB-LM for the same
support. It is shown in Mortensen et al. (2005) that
the expected size of D is a small fraction of the to-
tal number of events and its space usage comprises
less than O(|S|) bits with high probability.
There is a nonzero probability for false posi-
tives. Since the overflow dictionary D has no er-
rors, the expected error rate for our dynamic struc-
ture is the probability of a random collision in the
hash range of each hAi for each bucket cell com-
pared. In our setup we have
</bodyText>
<equation confidence="0.978366">
Pr(falsepos) = |Ai|
2|f(x)|
</equation>
<bodyText confidence="0.999972066666667">
where |f(x) |is the number of bits of each w-bit
cell used for the fingerprint f(x). w also primar-
ily governs space used in the model. The O-RLM
assumes only valid updates and deletions are per-
formed (i.e. we do not remove or update entries
that were never inserted prior).
The O-RLM takes time linear to the input size
for training and uses worst-case constant time for
querying and deletions where the constant is de-
pendent on the number of cells per bucket in A.
The number of bucket cells also effects the overall
error rate significantly since smaller ranges reduce
the probability of a collision. However, too few
cells per bucket will result in many full buckets
when the bucket hash function is not highly IID.
</bodyText>
<subsectionHeader confidence="0.993238">
2.4 Basic RLM Comparisons
</subsectionHeader>
<bodyText confidence="0.99558175">
Table 1 compares expected versus observed false
positive rates for the Bloom filter, TB-LM, and O-
RLM obtained by querying a model of approxi-
mately 280M events with 100K unseen n-grams.
</bodyText>
<note confidence="0.338484">
LM Expected Observed RAM
</note>
<tableCaption confidence="0.5626435">
Table 1: Example false postive rates and corre-
sponding memory usage for all randomised LMs.
</tableCaption>
<bodyText confidence="0.999947">
We see the bit-based Bloom filter uses signifi-
cantly less memory than the cell-based alternatives
and the O-RLM consumes more memory than the
TB-LM for the same expected error rate.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="method">
3 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999646">
3.1 Randomised Language Models
</subsectionHeader>
<bodyText confidence="0.999969457142857">
Talbot and Osborne (2007) used a Bloom filter
(Bloom, 1970) to encode a smoothed LM. A
Bloom filter (BF) represents a set S from arbitrary
domain U and supports membership queries such
as“Is x E S?”. The BF uses an array of m bits and
k independent UHFs each with range 0,... , m−1.
For insertion, each item is hashed through the k
hash functions and the resulting target bits are set
to one. During testing, an event x E U is passed
through the same k hash functions and if any bit
tested is zero then x was not in the support S.
The Bloomier filter directly represents key-
value pairs by using a table of cells and a family of
k associated hash functions (Chazelle et al., 2004).
Each key-value pair is associated with k cells in
the table via a perfect hash function. Talbot and
Brants (2008) used a Bloomier filter to encode a
LM. Before data can be added to the Bloomier fil-
ter, a greedy perfect hashing of all entries needs to
be computed in advance; this attempts to associate
each event in the support with one unique table cell
so no other entry collides with it. The procedure
can fail and might need to be repeated many times.
Neither of these two randomised language mod-
els are suitable for modelling a stream. Given the
fact that the stream is of unbounded size, we are
forced to delete items if we wish to maintain a
constant error rate and account for novel n-grams.
However, the Bloom filter LM nor the Bloomier
Filter LM support deletions. The bit sharing of the
Bloom filter (BF) LM (Talbot and Osborne, 2007)
means deletions may corrupt shared stored events.
The Bloomier filter LM (Talbot and Brants, 2008)
has a precomputed matching of keys shared be-
tween a constant number of cells in the filter array.
</bodyText>
<figure confidence="0.9766255">
Lossless
Bloom
TB-LM
O-RLM
0 0 7450MB
0.0039 0.0038 390MB
0.0039 0.0033 640MB
0.0039 0.0031 705MB
</figure>
<page confidence="0.985388">
758
</page>
<bodyText confidence="0.996843">
Deleting items from a Bloomier Filter without re-
computing the perfect hash will corrupt it.
</bodyText>
<subsectionHeader confidence="0.999642">
3.2 Probabilistic Counting
</subsectionHeader>
<bodyText confidence="0.999981545454546">
Concurrent work has used approximate counting
schemes based on Morris (1978) to estimate in
small space frequencies over a high volume in-
put text stream (Van Durme and Lall, 2009; Goyal
et al., 2009). The space savings are due to com-
pact storage of counts and retention of only a
small subset of the available n-grams in the data
stream. Since the final LMs are still lossless (mod-
ulo counts), the resulting LM needs significant
space. It is trivial to use probabilistic counting
within our framework.
</bodyText>
<subsectionHeader confidence="0.997798">
3.3 Compact Exact Language Models
</subsectionHeader>
<bodyText confidence="0.9991962">
Randomised algorithms are not the only com-
pact representation schemes. Church et al. (2007)
looked at Golomb Coding and Brants et al. (2007)
used tries in a distributed setting. These methods
are less succinct than randomised approaches.
</bodyText>
<subsectionHeader confidence="0.992022">
3.4 Adaptive Language Models
</subsectionHeader>
<bodyText confidence="0.9999925">
There is a large literature on adaptive LMs from
the speech processing domain (Bellegarda, 2004).
The primary difference between the O-RLM and
other adaptive LMs is that we add and remove n-
grams from the model instead of adapting only the
parameters of the current support set.
</bodyText>
<subsectionHeader confidence="0.883117">
3.5 Domain adaptation in Machine
Translation
</subsectionHeader>
<bodyText confidence="0.999984583333333">
Within MT there has been a variety of approaches
dealing with domain adaption (for example (Wu
et al., 2008; Koehn and Schroeder, 2007). Typi-
cally LMs are interpolated with one another, yield-
ing good results. These models are usually stat-
ically trained, exact and unable to deal with an
unbounded stream of monolingual data. Domain
adaptation has similarities with streaming, in that
our stream may be non-stationary. A crucial dif-
ference however is that the stream is of unbounded
length, whereas domain adaptation usually as-
sumes some finite and fixed training set.
</bodyText>
<sectionHeader confidence="0.995835" genericHeader="method">
4 Stream-based translation
</sectionHeader>
<bodyText confidence="0.99430275">
Streaming algorithms have numerous applications
in mainstream computer science (Muthukrishnan,
2003) but to date there has been very little aware-
ness of this field within computational linguistics.
</bodyText>
<figureCaption confidence="0.871565666666667">
Figure 2: Stream-based translation. The online
RLM uses data from the target stream and the last
test point in the source stream for adaptation.
</figureCaption>
<bodyText confidence="0.999988">
A text stream can be thought of as a unbounded
sequence of documents that are time-stamped and
we have access to them in strict chronological or-
der. The volume of the stream is so large we can
afford only a limited number of passes over the
data (typically one).
Text streams naturally arise on the Web when
millions of new documents are published each day
in many languages. For instance, 18 thousand
websites continuously publish news stories in 40
languages and there are millions of multilingual
blog postings per day. There are over 30 billion
e-mails sent daily and social networking sites, in-
cluding services such as Twitter, generate an adun-
dance of textual data in real time. Web crawlers
that spidered all these new documents would pro-
duce an unbounded input stream.
The stream-based translation scenario is as fol-
lows: we assume that each day we see a source
stream of many new newswire stories that need
translation. We also assume a stream of newswire
stories in the target language. Intuitively, since the
concurrent streams are from the same domain, we
can use the contexts provided in the target stream
to help with the translation of the source stream
(Figure 2). From a theoretical perspective, since
we cannot represent the entirety of the stream and
wish to maintain a constant error rate, we are
forced to throw some information away.
Given that the incoming text stream contains far
too much data to store in its entirety an immediate
question we would like to answer is: within our
LM, which subset of the target text stream should
</bodyText>
<page confidence="0.985841">
759
</page>
<figure confidence="0.974598">
Reuters 96-97 LM subsets
20 25 30 35 40 45 50
weeks
</figure>
<figureCaption confidence="0.873147">
Figure 3: Perplexity results using streamed data.
Perplexity decreases as we retrain LMs using data
chronologically closer to the (two) test dates.
</figureCaption>
<bodyText confidence="0.998168848484849">
we represent in our model?
Using perplexity, we investigated this question
using a text stream based on Reuter’s RCV1 text
collection (Rose et al., 2002). This contains 800k
time-stamped newswire stories from a full calen-
der year (8.20.1996 - 8.19.1997). We used the
SRILM (Stolcke, 2002) to construct an exact tri-
gram model built using all the RCV1 data with the
exception of the final week which we held out as
test data. This served as an oracle since we store
all of the stream.
We then trained multiple exact LMs of much
smaller sizes, coined subset LMs, to simulate
memory constraints. For a given date in the RCV1
stream, these subset LMs were trained using a
fixed window of previously seen documents up to
that data. Then we obtained perplexity results for
each subset LM against our test set.
Figure 3 shows an example. For this experiment
subset LMs were trained using a sliding window
of 20 weeks with the window advancing over a
period of three weeks each time. The two arcs
correspond to two different test sets drawn from
different days. The arcs show that recency has a
clear effect: populating LMs using material closer
to the test data date produces improved perplexity
performance. The LM chronologically closest to
a given test set has perplexity closest to the results
of the significantly larger baseline LM which uses
all the stream. As expected, using all of the data
yields the lowest perplexity.
We note that this is a robust finding, since we
also observe it in other domains. For example, we
</bodyText>
<table confidence="0.87927825">
Epoch Stream Window
1 08.20.1996 to 01.01.1997
2 01.02.1997 to 04.23.1997
3 04.24.1997 to 08.18.1997
</table>
<tableCaption confidence="0.957252">
Table 2: The stream timeline is divided into win-
dowed epochs for our recency experiments.
</tableCaption>
<bodyText confidence="0.7814785">
conducted the same tests over a stream of 18 bil-
lion tokens drawn from 80 million time-stamped
blog posts downloaded from the web with match-
ing results. The effect of recency on perplexity has
also been observed elsewhere (see, for example,
Rosenfeld (1995) and Whittaker (2001)).
Our experiments show that a possible way to
tackle stream-based translation is to always focus
the attention of the LM on the most recent part
of the stream. This means we remove data from
the model that came from the receding parts of the
stream and replace it with the present.
</bodyText>
<sectionHeader confidence="0.994413" genericHeader="method">
5 SMT Experiments
</sectionHeader>
<subsectionHeader confidence="0.915002">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999984882352941">
We used publicly available resources for all our
tests: for decoding we used Moses (Koehn and
Hoang, 2007) and our parallel data was taken from
the Spanish-English section of Europarl. For test
material, we translated 63 documents (800 sen-
tences) from three randomly selected dates spaced
throughout the RCV1 year (January 2nd, April
24, and August 19).1 This effectively divided the
stream into three epochs between the test dates (
table 2). We held out 300 sentences for minimum
error rate training (MERT) (Och, 2003) and opti-
mised the parameters of the feature functions of
the decoder for each experimental run.
The RCV1 is not a large corpus when compared
to the entire web but it is multilingual, chronologi-
cal, and large enough to enable us to test the effect
of recency in a translation setting.
</bodyText>
<subsectionHeader confidence="0.996052">
5.2 Adaption
</subsectionHeader>
<bodyText confidence="0.741055">
We looked at a number of ways of adapting the
O-RLM:
1. (Random) Randomly sample the stream and
for each new n-gram encountered, insert
</bodyText>
<footnote confidence="0.989988">
1As RCV1 is not a parallel corpus we translated the ref-
erence documents ourselves. This parallel corpus is available
from the authors.
</footnote>
<figure confidence="0.994457454545455">
perplexity
280
260
240
220
200
300
180
51-week baseline
20-week subset test 1
20-week subset test 2
</figure>
<page confidence="0.975258">
760
</page>
<table confidence="0.999866857142857">
Order Full Epoch 1 Epoch 3
1 1.25M 0.6M 0.7M
2 14.6 M 6.8M 7.0M
3 50.6 M 21.3M 21.7M
4 90.3 M 34.8M 35.4M
5 114.7M 41.8M 42.6M
Total 271.5M 105M 107.5M
</table>
<tableCaption confidence="0.988261">
Table 3: Distinct n-grams (in millions) encoun-
tered in the full stream and example epochs.
</tableCaption>
<table confidence="0.9999336">
Date Lossless TB-LM O-RLM
Jan 37.83 37.12 37.17
Apr 34.88 34.21 34.79
Aug 29.05 28.52 28.44
Avg 33.92 33.28 33.46
</table>
<tableCaption confidence="0.976624">
Table 4: Baseline translation results in BLEU us-
</tableCaption>
<listItem confidence="0.875204352941176">
ing data from the first stream epoch with a lossless
LM (4.5GB RAM), the TB-LM and the O-RLM
(300MB RAM). All LMs are static.
it and remove some previously inserted n-
gram, irrespective of whether it was ever re-
quested by the decoder or is a prefix.
2. (Conservative) For each new n-gram en-
countered in the stream, insert it in the filter
and remove one previously inserted n-gram
which was never requested by the decoder.
To preserve consistency we do not remove
lower-order grams that are needed to estimate
backoff probability for higher-order smooth-
ing. Counts are updated for n-grams already
in the model if the new count observed is
larger than the current one.
3. (Severe) Differs from the conservative ap-
</listItem>
<bodyText confidence="0.955656523809524">
proach only in that we delete all unused n-
grams (i.e. all those not requested by the de-
coder in the previous translation task) from
the O-RLM before adapting with data from
the stream. This means the data structure is
sparsely populated for all runs.
All the TB-LMs and O-RLMs were unpruned 5-
gram models and used Stupid-backoff smoothing
(Brants et al., 2007) 2 with the backoff parameter
set to 0.4 as suggested. The number of distinct n-
grams encountered in the stream for two epochs is
shown in Table 3.
Table 6 shows translation results using these
adaption strategies. In practice, the random ap-
proach does not work while the conservative and
severe adaption techniques produce equivalent re-
sults due to the small proportion of data in the
model that is queried during decoding. All the MT
experiments that follow use the severe method and
the overflow dictionary always holds less than 1%
of the total elements in the model.
</bodyText>
<footnote confidence="0.759603">
2Smoothing text input data streams poses an interesting
problem we hope to investigate in the future.
</footnote>
<subsectionHeader confidence="0.99759">
5.3 Training Regimes
</subsectionHeader>
<bodyText confidence="0.999990333333333">
We now consider stream-based translation. Our
first naive approach is to continually add new data
from the stream to the training set without delet-
ing anything. Given a constant memory bound this
strategy only increases the error rate over time as
discussed. Our second, computationally demand-
ing approach is, before each test point, to rebuild
the TB-LM from scratch using the stream data
from the most recent epoch as the training set.
This is batch retraining. The final approach in-
crementally retrains online. This utilizes the same
training data as above (the stream data from the
last epoch) but instead of full retraining it replaces
n-grams currently in the model with unseen n-
grams and counts encountered in the data stream.
</bodyText>
<subsectionHeader confidence="0.998835">
5.4 Streaming Translation Results
</subsectionHeader>
<bodyText confidence="0.99995195">
Each table shows translation results for the three
different test times in the stream. All results re-
ported use the case-sensitive BLEU score.
For our baselines we use static LMs trained on
the first epoch’s data to test all three translation
points in the source stream. This is the tradi-
tional approach. We trained an exact, modified
Kneser-Ney smoothed LM (here we do not en-
force a memory constraint) and also used the TB-
LM and O-RLM to verify our structures adequecy.
Results are shown in table 4. The exact model
gives better performance overall due to the more
sophisticated smoothing used.
Table 5 shows results for a set of stream-based
LMs using the TB-LM and the O-RLM with mem-
ory bounds of 200MB and 300MB. As expected,
the naive models performance degrades over time
as we funnel more data into the TB-LM and the
error rises. The batch retrained TB-LMs and O-
RLMs have constant error rates of 28 1 and 1
</bodyText>
<page confidence="0.266322">
212 and
</page>
<bodyText confidence="0.6538805">
so outperform the naive approach. Since the train-
ing data is identical we see (approximately) equal
</bodyText>
<page confidence="0.990178">
761
</page>
<table confidence="0.992079333333333">
Naive TB-LM Batch Retrained TB-LM O-RLM
Date 200MB 300MB 200MB 300MB 200MB 300MB
Jan 35.94 37.12 35.94 37.12 36.44 37.17
Apr 33.55 35.79 36.01 35.99 35.87 36.10
Aug 22.44 26.07 28.97 29.38 29.00 29.18
Avg 30.64 32.99 33.64 34.16 33.77 34.15
</table>
<tableCaption confidence="0.9948795">
Table 5: Translation results for stream-based LMs in BLEU. Performance degrades with time using the
Naive approach. The batch retrained TB-LM and stream-based O-RLM use constant error rates of 281
</tableCaption>
<page confidence="0.206161">
1
</page>
<bodyText confidence="0.980151041666666">
and 212 .
performance from the batch retrained and online
models. We also see some improvement compared
to the static baselines when the LMs use the most
recent data from the target language stream with
respect to the current translation point.
The key difference is that each time we batch
retrain the TB-LM, we must compute a perfect
hashing of the new training set. This is computa-
tionally demanding since the perfect hashing algo-
rithm uses Monte Carlo randomisation which fails
routinely and must be repeated. To make the al-
gorithm tractable the training data set must be di-
vided into lexically sorted subsets as well. This
requires extra passes over the data which may not
be trivial in a streaming environment.
In contrast, the O-RLM is incrementally re-
trained online. This makes it more resource ef-
ficient since we find bits in the model for the n-
grams dynamically without using more memory
than we intially set. Note that even though the O-
RLM is theoretically less space efficient than the
TB-LM, when using the same amount of memory
translation performance is comparable.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="method">
6 O-RLM Properties
</sectionHeader>
<bodyText confidence="0.995877">
The previous experiments confirm that the O-
RLM can be employed as a LM in an SMT setting
but it is useful to get insight into the intrinsic prop-
erties of the data structure. Many of the properties
of the model, such as the number of bits per fin-
gerprint, follow directly from the TB-LM but the
relationship between the overflow dictionary and
the randomised buckets is novel.
Figures 4 and 5 shows properties of the O-RLM
while varying only the number of cells in each
bucket and keeping all other model parameters
constant. We test membership of n-grams in an
unseen corpus against those stored in the table.
Our tests were conducted over a larger stream of
1.25B n-grams from the Gigaword corpus(Graff,
</bodyText>
<table confidence="0.9999076">
Date Severe Random Conservative
Jan 36.44 36.44 36.44
Apr 35.87 31.08 35.51
Aug 29.00 19.31 29.14
Avg 33.77 29.11 33.70
</table>
<tableCaption confidence="0.932779">
Table 6: Adaptation results measured in BLEU.
Random deletions degrade performance when
adapting a 200MB O-RLM.
</tableCaption>
<bodyText confidence="0.99984725">
2003). We set our space usage to match the 3.08
bytes per n-gram reported in Talbot and Brants
(2008) and held out just over 1M unseen n-grams
to test the error rates of our models.
In Figure 4 we see a direct correlation between
model error and cells per buckets. As the num-
ber of cells decreases the false positive rate drops
as well since fewer cells to compare against per
bucket means a lower chance of producing colli-
sions. If the range is decreased too much though
more data is diverted to the overflow dictionary
due to many buckets reaching capacity when in-
serting and adapting. Clearly this is less space ef-
ficient. Figure 5 shows the relationship between
the percent of data in the overflow dictionary and
the total cells per bucket.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999875818181818">
Our experiments have shown that for stream-based
translation, using recent data can benefit perfor-
mance but simply adding entries to a randomised
representation will only reduce translation perfor-
mance over time. We have presented a novel ran-
domised language model based on dynamic per-
fect hashing that supports online insertions and
deletions. As a consequence, it is considerably
faster and more efficient than batch retraining.
While not advocating the idea that only small
amounts of data are needed for language mod-
</bodyText>
<page confidence="0.979481">
762
</page>
<table confidence="0.625907555555556">
O-RLM Error rate
false positive rates 0.007
0.006
0.005
0.004
0.003
0.002
0.001
0
</table>
<bodyText confidence="0.999902444444444">
ducting tests over much larger, higher variance
text streams from crawled blog data. In the fu-
ture we will also consider randomised representa-
tions of other adaptive LMs in the literature using
a static background LM in conjunction with our
online one. We ultimately hope to deploy large-
scale LMs which continuously adapt to the vast
amount of material published on the Web without
incurring significant computational overhead.
</bodyText>
<sectionHeader confidence="0.981709" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<figure confidence="0.5485395">
50 100 150 200 250
cells per bucket
</figure>
<figureCaption confidence="0.9974275">
Figure 4: The O-RLM error rises in correlation
with the number of cells per bucket.
</figureCaption>
<figure confidence="0.407702076923077">
Overflow Dictionary Size
0.018
% of data in overflow dictionary 0.016
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
50 100 150 200 250
cells per bucket
</figure>
<figureCaption confidence="0.714782">
Figure 5: Too few cells per bucket causes a higher
</figureCaption>
<bodyText confidence="0.981654083333333">
percentage of the data to be stored in the overflow
dictionary due to full buckets.
elling, within a bounded amount of space our re-
sults show that it is better to have a low error rate
and store a wisely chosen fraction of the data than
having a high error rate and storing more of it.
Clearly tradeoffs will vary between applications.
This is the first stream-based randomised lan-
guage model and associated machine translation
system reported in the literature. Clearly there are
many interesting open questions for future work.
For example, can we use small randomised repre-
sentations called sketches to compactly represent
side-information on the stream telling us which as-
pects of it we should insert into our data? How
can we efficiently deal with smoothing in this set-
ting? Our adaptation scheme is simple and our
data stream is tractable. Currently we are con-
The authors would like to thank David Talbot,
Adam Lopez and Phil Blunsom for their valu-
able comments and insight. This work was sup-
ported in part under the GALE program of the De-
fense Advanced Research Projects Agency, Con-
tract No. HR0011-06-C-0022.
</bodyText>
<sectionHeader confidence="0.99889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999348727272727">
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42:93–108.
Burton H. Bloom. 1970. Space/time trade-offs in
hash coding with allowable errors. Commun. ACM,
13(7):422–426.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858–867.
J. Lawrence Carter and Mark N. Wegman. 1977. Uni-
versal classes of hash functions (extended abstract).
In STOC ’77: Proceedings of the ninth annual ACM
symposium on Theory of computing, pages 106–112,
New York, NY, USA. ACM Press.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The bloomier filter: an ef-
ficient data structure for static support lookup ta-
bles. In SODA ’04: Proceedings of the fifteenth an-
nualACM-SIAM symposium on Discrete algorithms,
pages 30–39, Philadelphia, PA, USA. Society for In-
dustrial and Applied Mathematics.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with Golomb
coding. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 199–207,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.98245">
763
</page>
<reference confidence="0.999931493506494">
Amit Goyal, Hal Daum´e III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Boulder, CO.
David Graff. 2003. English Gigaword. Linguistic Data
Consortium (LDC-2003T05).
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868–876.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224–227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Robert Morris. 1978. Counting large numbers
of events in small registers. Commun. ACM,
21(10):840–842.
Christian Worm Mortensen, Rasmus Pagh, and Mihai
Pˇatrac¸cu. 2005. On dynamic range reporting in one
dimension. In STOC ’05: Proceedings of the thirty-
seventh annual ACM symposium on Theory of com-
puting, pages 104–111, New York, NY, USA. ACM.
S. Muthukrishnan. 2003. Data streams: algorithms
and applications. In SODA ’03: Proceedings of the
fourteenth annual ACM-SIAM symposium on Dis-
crete algorithms, pages 413–413, Philadelphia, PA,
USA. Society for Industrial and Applied Mathemat-
ics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160–
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The reuters corpus volume 1 - from yester-
days news to tomorrows language resources. In In
Proceedings of the Third International Conference
on Language Resources and Evaluation, pages 29–
31.
Ronald Rosenfeld. 1995. Optimizing lexical and n-
gram coverage via judicious use of linguistic data.
In In Proc. European Conf. on Speech Technology,
pages 1763–1766.
A. Stolcke. 2002. Srilm – an extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Lan-
guage Processing, 2002.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT, pages 505–513, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 468–476.
Benjamin Van Durme and Ashwin Lall. 2009. Prob-
abilistic counting with randomized storage. In
Twenty-First International Joint Conference on Ar-
tificial Intelligence (IJCAI-09), Pasadena, CA, July.
E. W. D. Whittaker. 2001. Temporal adaptation of lan-
guage models. In In Adaptation Methods for Speech
Recognition, ISCA Tutorial and Research Workshop
(ITRW), pages 203–206.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 993–1000. Coling 2008 Organizing
Committee, August.
</reference>
<page confidence="0.997877">
764
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975680">
<title confidence="0.99955">Stream-based Randomised Language Models for SMT</title>
<author confidence="0.99999">Abby Levenberg Miles Osborne</author>
<affiliation confidence="0.9998545">School of Informatics School of Informatics University of Edinburgh University of Edinburgh</affiliation>
<email confidence="0.984537">a.levenberg@sms.ed.ac.ukmiles@inf.ed.ac.uk</email>
<abstract confidence="0.999557631578947">Randomised techniques allow very models to be represented succinctly. However, being they are unsuitable for modelling an unbounded stream of language whilst maintaining a constant error rate. We present a novel randomised language model which an perfect hash to efficiently deal with unbounded text streams. Translation experiments over a text stream show that our online randomised model matches the performance of batch-based LMs without incurring the computational overhead associated with full retraining. This opens up the possibility of randomised language models which continuously adapt to the massive volumes of texts published on the Web each day.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: review and perspectives.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<pages>42--93</pages>
<contexts>
<context position="12456" citStr="Bellegarda, 2004" startWordPosition="2121" endWordPosition="2122">all subset of the available n-grams in the data stream. Since the final LMs are still lossless (modulo counts), the resulting LM needs significant space. It is trivial to use probabilistic counting within our framework. 3.3 Compact Exact Language Models Randomised algorithms are not the only compact representation schemes. Church et al. (2007) looked at Golomb Coding and Brants et al. (2007) used tries in a distributed setting. These methods are less succinct than randomised approaches. 3.4 Adaptive Language Models There is a large literature on adaptive LMs from the speech processing domain (Bellegarda, 2004). The primary difference between the O-RLM and other adaptive LMs is that we add and remove ngrams from the model instead of adapting only the parameters of the current support set. 3.5 Domain adaptation in Machine Translation Within MT there has been a variety of approaches dealing with domain adaption (for example (Wu et al., 2008; Koehn and Schroeder, 2007). Typically LMs are interpolated with one another, yielding good results. These models are usually statically trained, exact and unable to deal with an unbounded stream of monolingual data. Domain adaptation has similarities with streamin</context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R. Bellegarda. 2004. Statistical language model adaptation: review and perspectives. Speech Communication, 42:93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burton H Bloom</author>
</authors>
<title>Space/time trade-offs in hash coding with allowable errors.</title>
<date>1970</date>
<journal>Commun. ACM,</journal>
<volume>13</volume>
<issue>7</issue>
<contexts>
<context position="9671" citStr="Bloom, 1970" startWordPosition="1633" endWordPosition="1634"> Basic RLM Comparisons Table 1 compares expected versus observed false positive rates for the Bloom filter, TB-LM, and ORLM obtained by querying a model of approximately 280M events with 100K unseen n-grams. LM Expected Observed RAM Table 1: Example false postive rates and corresponding memory usage for all randomised LMs. We see the bit-based Bloom filter uses significantly less memory than the cell-based alternatives and the O-RLM consumes more memory than the TB-LM for the same expected error rate. 3 Related Work 3.1 Randomised Language Models Talbot and Osborne (2007) used a Bloom filter (Bloom, 1970) to encode a smoothed LM. A Bloom filter (BF) represents a set S from arbitrary domain U and supports membership queries such as“Is x E S?”. The BF uses an array of m bits and k independent UHFs each with range 0,... , m−1. For insertion, each item is hashed through the k hash functions and the resulting target bits are set to one. During testing, an event x E U is passed through the same k hash functions and if any bit tested is zero then x was not in the support S. The Bloomier filter directly represents keyvalue pairs by using a table of cells and a family of k associated hash functions (Ch</context>
</contexts>
<marker>Bloom, 1970</marker>
<rawString>Burton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>858--867</pages>
<contexts>
<context position="12233" citStr="Brants et al. (2007)" startWordPosition="2086" endWordPosition="2089">sed on Morris (1978) to estimate in small space frequencies over a high volume input text stream (Van Durme and Lall, 2009; Goyal et al., 2009). The space savings are due to compact storage of counts and retention of only a small subset of the available n-grams in the data stream. Since the final LMs are still lossless (modulo counts), the resulting LM needs significant space. It is trivial to use probabilistic counting within our framework. 3.3 Compact Exact Language Models Randomised algorithms are not the only compact representation schemes. Church et al. (2007) looked at Golomb Coding and Brants et al. (2007) used tries in a distributed setting. These methods are less succinct than randomised approaches. 3.4 Adaptive Language Models There is a large literature on adaptive LMs from the speech processing domain (Bellegarda, 2004). The primary difference between the O-RLM and other adaptive LMs is that we add and remove ngrams from the model instead of adapting only the parameters of the current support set. 3.5 Domain adaptation in Machine Translation Within MT there has been a variety of approaches dealing with domain adaption (for example (Wu et al., 2008; Koehn and Schroeder, 2007). Typically LMs</context>
<context position="20363" citStr="Brants et al., 2007" startWordPosition="3466" endWordPosition="3469">not remove lower-order grams that are needed to estimate backoff probability for higher-order smoothing. Counts are updated for n-grams already in the model if the new count observed is larger than the current one. 3. (Severe) Differs from the conservative approach only in that we delete all unused ngrams (i.e. all those not requested by the decoder in the previous translation task) from the O-RLM before adapting with data from the stream. This means the data structure is sparsely populated for all runs. All the TB-LMs and O-RLMs were unpruned 5- gram models and used Stupid-backoff smoothing (Brants et al., 2007) 2 with the backoff parameter set to 0.4 as suggested. The number of distinct ngrams encountered in the stream for two epochs is shown in Table 3. Table 6 shows translation results using these adaption strategies. In practice, the random approach does not work while the conservative and severe adaption techniques produce equivalent results due to the small proportion of data in the model that is queried during decoding. All the MT experiments that follow use the severe method and the overflow dictionary always holds less than 1% of the total elements in the model. 2Smoothing text input data st</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lawrence Carter</author>
<author>Mark N Wegman</author>
</authors>
<title>Universal classes of hash functions (extended abstract).</title>
<date>1977</date>
<booktitle>In STOC ’77: Proceedings of the ninth annual ACM symposium on Theory of computing,</booktitle>
<pages>106--112</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4898" citStr="Carter and Wegman, 1977" startWordPosition="787" endWordPosition="790">d data structure and is an array of b dictionaries A0, ... , Ab−1. D is a lossless data structure which handles collisions in A. Each of the dictionaries in A is referred to as a ‘bucket’. In our implementation the buckets are equally sized arrays of w-bit cells. These cells hold the fingerprints and values of n-grams (one n-gram-value pair per cell). To insert an n-gram x and associated value v(x) into the model, we select a bucket AZ by hashing x into the range i → [0, ... , b − 1]. Each bucket has an associated random hash function, hAi, drawn from a universal hash function (UHF) family h (Carter and Wegman, 1977), which is then used to generate the n-gram fingerprint: f(x) = hAi(x). If the bucket AZ is not full we conduct a scan of its cells. If the fingerprint f(x) is not already encoded in the bucket AZ we add the fingerprint and value to the first empty cell available. We allocate a preset number of the least significant bits of each w-bit cell to hold v(x) and the remaining most significant bits for f(x) but this is arbitrary. Any encoding scheme, such as the packed representation of Talbot and Brants (2008), is viable here. However, if f(x) ∈ AZ already (there is a collision) we store the n-gram </context>
</contexts>
<marker>Carter, Wegman, 1977</marker>
<rawString>J. Lawrence Carter and Mark N. Wegman. 1977. Universal classes of hash functions (extended abstract). In STOC ’77: Proceedings of the ninth annual ACM symposium on Theory of computing, pages 106–112, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Chazelle</author>
<author>Joe Kilian</author>
<author>Ronitt Rubinfeld</author>
<author>Ayellet Tal</author>
</authors>
<title>The bloomier filter: an efficient data structure for static support lookup tables.</title>
<date>2004</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<booktitle>In SODA ’04: Proceedings of the fifteenth annualACM-SIAM symposium on Discrete algorithms,</booktitle>
<pages>30--39</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="10291" citStr="Chazelle et al., 2004" startWordPosition="1751" endWordPosition="1754">0) to encode a smoothed LM. A Bloom filter (BF) represents a set S from arbitrary domain U and supports membership queries such as“Is x E S?”. The BF uses an array of m bits and k independent UHFs each with range 0,... , m−1. For insertion, each item is hashed through the k hash functions and the resulting target bits are set to one. During testing, an event x E U is passed through the same k hash functions and if any bit tested is zero then x was not in the support S. The Bloomier filter directly represents keyvalue pairs by using a table of cells and a family of k associated hash functions (Chazelle et al., 2004). Each key-value pair is associated with k cells in the table via a perfect hash function. Talbot and Brants (2008) used a Bloomier filter to encode a LM. Before data can be added to the Bloomier filter, a greedy perfect hashing of all entries needs to be computed in advance; this attempts to associate each event in the support with one unique table cell so no other entry collides with it. The procedure can fail and might need to be repeated many times. Neither of these two randomised language models are suitable for modelling a stream. Given the fact that the stream is of unbounded size, we a</context>
</contexts>
<marker>Chazelle, Kilian, Rubinfeld, Tal, 2004</marker>
<rawString>Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and Ayellet Tal. 2004. The bloomier filter: an efficient data structure for static support lookup tables. In SODA ’04: Proceedings of the fifteenth annualACM-SIAM symposium on Discrete algorithms, pages 30–39, Philadelphia, PA, USA. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Ted Hart</author>
<author>Jianfeng Gao</author>
</authors>
<title>Compressing trigram language models with Golomb coding.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>199--207</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="12184" citStr="Church et al. (2007)" startWordPosition="2077" endWordPosition="2080">ent work has used approximate counting schemes based on Morris (1978) to estimate in small space frequencies over a high volume input text stream (Van Durme and Lall, 2009; Goyal et al., 2009). The space savings are due to compact storage of counts and retention of only a small subset of the available n-grams in the data stream. Since the final LMs are still lossless (modulo counts), the resulting LM needs significant space. It is trivial to use probabilistic counting within our framework. 3.3 Compact Exact Language Models Randomised algorithms are not the only compact representation schemes. Church et al. (2007) looked at Golomb Coding and Brants et al. (2007) used tries in a distributed setting. These methods are less succinct than randomised approaches. 3.4 Adaptive Language Models There is a large literature on adaptive LMs from the speech processing domain (Bellegarda, 2004). The primary difference between the O-RLM and other adaptive LMs is that we add and remove ngrams from the model instead of adapting only the parameters of the current support set. 3.5 Domain adaptation in Machine Translation Within MT there has been a variety of approaches dealing with domain adaption (for example (Wu et al.</context>
</contexts>
<marker>Church, Hart, Gao, 2007</marker>
<rawString>Kenneth Church, Ted Hart, and Jianfeng Gao. 2007. Compressing trigram language models with Golomb coding. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 199–207, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Streaming for large scale NLP: Language modeling.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<location>Boulder, CO.</location>
<marker>Goyal, Daum´e, Venkatasubramanian, 2009</marker>
<rawString>Amit Goyal, Hal Daum´e III, and Suresh Venkatasubramanian. 2009. Streaming for large scale NLP: Language modeling. In North American Chapter of the Association for Computational Linguistics (NAACL), Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<date>2003</date>
<booktitle>English Gigaword. Linguistic Data Consortium (LDC-2003T05).</booktitle>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English Gigaword. Linguistic Data Consortium (LDC-2003T05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>868--876</pages>
<contexts>
<context position="17799" citStr="Koehn and Hoang, 2007" startWordPosition="3019" endWordPosition="3022">-stamped blog posts downloaded from the web with matching results. The effect of recency on perplexity has also been observed elsewhere (see, for example, Rosenfeld (1995) and Whittaker (2001)). Our experiments show that a possible way to tackle stream-based translation is to always focus the attention of the LM on the most recent part of the stream. This means we remove data from the model that came from the receding parts of the stream and replace it with the present. 5 SMT Experiments 5.1 Experimental Setup We used publicly available resources for all our tests: for decoding we used Moses (Koehn and Hoang, 2007) and our parallel data was taken from the Spanish-English section of Europarl. For test material, we translated 63 documents (800 sentences) from three randomly selected dates spaced throughout the RCV1 year (January 2nd, April 24, and August 19).1 This effectively divided the stream into three epochs between the test dates ( table 2). We held out 300 sentences for minimum error rate training (MERT) (Och, 2003) and optimised the parameters of the feature functions of the decoder for each experimental run. The RCV1 is not a large corpus when compared to the entire web but it is multilingual, ch</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="12818" citStr="Koehn and Schroeder, 2007" startWordPosition="2180" endWordPosition="2183">at Golomb Coding and Brants et al. (2007) used tries in a distributed setting. These methods are less succinct than randomised approaches. 3.4 Adaptive Language Models There is a large literature on adaptive LMs from the speech processing domain (Bellegarda, 2004). The primary difference between the O-RLM and other adaptive LMs is that we add and remove ngrams from the model instead of adapting only the parameters of the current support set. 3.5 Domain adaptation in Machine Translation Within MT there has been a variety of approaches dealing with domain adaption (for example (Wu et al., 2008; Koehn and Schroeder, 2007). Typically LMs are interpolated with one another, yielding good results. These models are usually statically trained, exact and unable to deal with an unbounded stream of monolingual data. Domain adaptation has similarities with streaming, in that our stream may be non-stationary. A crucial difference however is that the stream is of unbounded length, whereas domain adaptation usually assumes some finite and fixed training set. 4 Stream-based translation Streaming algorithms have numerous applications in mainstream computer science (Muthukrishnan, 2003) but to date there has been very little </context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Morris</author>
</authors>
<title>Counting large numbers of events in small registers.</title>
<date>1978</date>
<journal>Commun. ACM,</journal>
<volume>21</volume>
<issue>10</issue>
<contexts>
<context position="11633" citStr="Morris (1978)" startWordPosition="1985" endWordPosition="1986">M nor the Bloomier Filter LM support deletions. The bit sharing of the Bloom filter (BF) LM (Talbot and Osborne, 2007) means deletions may corrupt shared stored events. The Bloomier filter LM (Talbot and Brants, 2008) has a precomputed matching of keys shared between a constant number of cells in the filter array. Lossless Bloom TB-LM O-RLM 0 0 7450MB 0.0039 0.0038 390MB 0.0039 0.0033 640MB 0.0039 0.0031 705MB 758 Deleting items from a Bloomier Filter without recomputing the perfect hash will corrupt it. 3.2 Probabilistic Counting Concurrent work has used approximate counting schemes based on Morris (1978) to estimate in small space frequencies over a high volume input text stream (Van Durme and Lall, 2009; Goyal et al., 2009). The space savings are due to compact storage of counts and retention of only a small subset of the available n-grams in the data stream. Since the final LMs are still lossless (modulo counts), the resulting LM needs significant space. It is trivial to use probabilistic counting within our framework. 3.3 Compact Exact Language Models Randomised algorithms are not the only compact representation schemes. Church et al. (2007) looked at Golomb Coding and Brants et al. (2007)</context>
</contexts>
<marker>Morris, 1978</marker>
<rawString>Robert Morris. 1978. Counting large numbers of events in small registers. Commun. ACM, 21(10):840–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Worm Mortensen</author>
<author>Rasmus Pagh</author>
<author>Mihai Pˇatrac¸cu</author>
</authors>
<title>On dynamic range reporting in one dimension. In</title>
<date>2005</date>
<booktitle>STOC ’05: Proceedings of the thirtyseventh annual ACM symposium on Theory of computing,</booktitle>
<pages>104--111</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Mortensen, Pagh, Pˇatrac¸cu, 2005</marker>
<rawString>Christian Worm Mortensen, Rasmus Pagh, and Mihai Pˇatrac¸cu. 2005. On dynamic range reporting in one dimension. In STOC ’05: Proceedings of the thirtyseventh annual ACM symposium on Theory of computing, pages 104–111, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muthukrishnan</author>
</authors>
<title>Data streams: algorithms and applications. In</title>
<date>2003</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<booktitle>SODA ’03: Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms,</booktitle>
<pages>413--413</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="13378" citStr="Muthukrishnan, 2003" startWordPosition="2265" endWordPosition="2266">on (for example (Wu et al., 2008; Koehn and Schroeder, 2007). Typically LMs are interpolated with one another, yielding good results. These models are usually statically trained, exact and unable to deal with an unbounded stream of monolingual data. Domain adaptation has similarities with streaming, in that our stream may be non-stationary. A crucial difference however is that the stream is of unbounded length, whereas domain adaptation usually assumes some finite and fixed training set. 4 Stream-based translation Streaming algorithms have numerous applications in mainstream computer science (Muthukrishnan, 2003) but to date there has been very little awareness of this field within computational linguistics. Figure 2: Stream-based translation. The online RLM uses data from the target stream and the last test point in the source stream for adaptation. A text stream can be thought of as a unbounded sequence of documents that are time-stamped and we have access to them in strict chronological order. The volume of the stream is so large we can afford only a limited number of passes over the data (typically one). Text streams naturally arise on the Web when millions of new documents are published each day </context>
</contexts>
<marker>Muthukrishnan, 2003</marker>
<rawString>S. Muthukrishnan. 2003. Data streams: algorithms and applications. In SODA ’03: Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms, pages 413–413, Philadelphia, PA, USA. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18213" citStr="Och, 2003" startWordPosition="3088" endWordPosition="3089">s of the stream and replace it with the present. 5 SMT Experiments 5.1 Experimental Setup We used publicly available resources for all our tests: for decoding we used Moses (Koehn and Hoang, 2007) and our parallel data was taken from the Spanish-English section of Europarl. For test material, we translated 63 documents (800 sentences) from three randomly selected dates spaced throughout the RCV1 year (January 2nd, April 24, and August 19).1 This effectively divided the stream into three epochs between the test dates ( table 2). We held out 300 sentences for minimum error rate training (MERT) (Och, 2003) and optimised the parameters of the feature functions of the decoder for each experimental run. The RCV1 is not a large corpus when compared to the entire web but it is multilingual, chronological, and large enough to enable us to test the effect of recency in a translation setting. 5.2 Adaption We looked at a number of ways of adapting the O-RLM: 1. (Random) Randomly sample the stream and for each new n-gram encountered, insert 1As RCV1 is not a parallel corpus we translated the reference documents ourselves. This parallel corpus is available from the authors. perplexity 280 260 240 220 200 </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160– 167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Rose</author>
<author>Mark Stevenson</author>
<author>Miles Whitehead</author>
</authors>
<title>The reuters corpus volume 1 - from yesterdays news to tomorrows language resources. In</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation,</booktitle>
<pages>29--31</pages>
<contexts>
<context position="15533" citStr="Rose et al., 2002" startWordPosition="2630" endWordPosition="2633">ant error rate, we are forced to throw some information away. Given that the incoming text stream contains far too much data to store in its entirety an immediate question we would like to answer is: within our LM, which subset of the target text stream should 759 Reuters 96-97 LM subsets 20 25 30 35 40 45 50 weeks Figure 3: Perplexity results using streamed data. Perplexity decreases as we retrain LMs using data chronologically closer to the (two) test dates. we represent in our model? Using perplexity, we investigated this question using a text stream based on Reuter’s RCV1 text collection (Rose et al., 2002). This contains 800k time-stamped newswire stories from a full calender year (8.20.1996 - 8.19.1997). We used the SRILM (Stolcke, 2002) to construct an exact trigram model built using all the RCV1 data with the exception of the final week which we held out as test data. This served as an oracle since we store all of the stream. We then trained multiple exact LMs of much smaller sizes, coined subset LMs, to simulate memory constraints. For a given date in the RCV1 stream, these subset LMs were trained using a fixed window of previously seen documents up to that data. Then we obtained perplexity</context>
</contexts>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>Tony Rose, Mark Stevenson, and Miles Whitehead. 2002. The reuters corpus volume 1 - from yesterdays news to tomorrows language resources. In In Proceedings of the Third International Conference on Language Resources and Evaluation, pages 29– 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Optimizing lexical and ngram coverage via judicious use of linguistic data. In In</title>
<date>1995</date>
<booktitle>Proc. European Conf. on Speech Technology,</booktitle>
<pages>1763--1766</pages>
<contexts>
<context position="17348" citStr="Rosenfeld (1995)" startWordPosition="2943" endWordPosition="2944">eam. As expected, using all of the data yields the lowest perplexity. We note that this is a robust finding, since we also observe it in other domains. For example, we Epoch Stream Window 1 08.20.1996 to 01.01.1997 2 01.02.1997 to 04.23.1997 3 04.24.1997 to 08.18.1997 Table 2: The stream timeline is divided into windowed epochs for our recency experiments. conducted the same tests over a stream of 18 billion tokens drawn from 80 million time-stamped blog posts downloaded from the web with matching results. The effect of recency on perplexity has also been observed elsewhere (see, for example, Rosenfeld (1995) and Whittaker (2001)). Our experiments show that a possible way to tackle stream-based translation is to always focus the attention of the LM on the most recent part of the stream. This means we remove data from the model that came from the receding parts of the stream and replace it with the present. 5 SMT Experiments 5.1 Experimental Setup We used publicly available resources for all our tests: for decoding we used Moses (Koehn and Hoang, 2007) and our parallel data was taken from the Spanish-English section of Europarl. For test material, we translated 63 documents (800 sentences) from thr</context>
</contexts>
<marker>Rosenfeld, 1995</marker>
<rawString>Ronald Rosenfeld. 1995. Optimizing lexical and ngram coverage via judicious use of linguistic data. In In Proc. European Conf. on Speech Technology, pages 1763–1766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<contexts>
<context position="15668" citStr="Stolcke, 2002" startWordPosition="2653" endWordPosition="2654">s entirety an immediate question we would like to answer is: within our LM, which subset of the target text stream should 759 Reuters 96-97 LM subsets 20 25 30 35 40 45 50 weeks Figure 3: Perplexity results using streamed data. Perplexity decreases as we retrain LMs using data chronologically closer to the (two) test dates. we represent in our model? Using perplexity, we investigated this question using a text stream based on Reuter’s RCV1 text collection (Rose et al., 2002). This contains 800k time-stamped newswire stories from a full calender year (8.20.1996 - 8.19.1997). We used the SRILM (Stolcke, 2002) to construct an exact trigram model built using all the RCV1 data with the exception of the final week which we held out as test data. This served as an oracle since we store all of the stream. We then trained multiple exact LMs of much smaller sizes, coined subset LMs, to simulate memory constraints. For a given date in the RCV1 stream, these subset LMs were trained using a fixed window of previously seen documents up to that data. Then we obtained perplexity results for each subset LM against our test set. Figure 3 shows an example. For this experiment subset LMs were trained using a slidin</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Language Processing, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Thorsten Brants</author>
</authors>
<title>Randomized language models via perfect hash functions.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>505--513</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1654" citStr="Talbot and Brants, 2008" startWordPosition="244" endWordPosition="247">stems. They assign probabilities to generated hypotheses in the target language informing lexical selection. The most common form of LMs in SMT systems are smoothed n-gram models which predict a word based on a contextual history of n − 1 words. For some languages (such as English) trillions of words are available for training purposes. This fact, along with the observation that machine translation quality improves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space (Talbot and Osborne, 2007; Talbot and Brants, 2008). Randomised LMs (RLMs) solve the problem of representing large, static LMs but they are batch oriented and cannot incorporate new data without fully retraining from scratch. This property makes current RLMs ill-suited for modelling the massive volume of textual material published daily on the Web. We present a novel RLM which is capable of incremental (re)training. We use random hash functions coupled with an online perfect hashing algorithm to represent n-grams in small space. This makes it well-suited for dealing with an unbounded stream of training material. To our knowledge this is the fi</context>
<context position="3292" citStr="Talbot and Brants (2008)" startWordPosition="504" endWordPosition="507">t that our findings are general and should help inform the design of other stream-based models. Section 2 introduces the incrementally retrainable randomised LM and section 3 considers related work; Section 4 then considers the question of how unbounded text streams should be modelled. Sections 5 and 6 show stream-based translation results and properties of our novel datastructure. Section 7 concludes the paper. 2 Online Bloomier Filter LM Our online randomised LM (O-RLM) is based on the dynamic Bloomier filter (Mortensen et al., 2005). It is a variant of the batch-based Bloomier filter LM of Talbot and Brants (2008) which we refer to as the TB-LM henceforth. As with the TB-LM, the O-RLM uses random hash functions to represent n-grams as fingerprints which is the main source of space savings for the model. 2.1 Online Perfect Hashing The key difference in our model as compared to the TB-LM is we use an online perfect hashing 756 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 756–764, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Figure 1: Inserting an n-gram into the dynamic Bloomier filter. Above: an n-gram is hashed to its target bucket. Below: the n-gram</context>
<context position="5407" citStr="Talbot and Brants (2008)" startWordPosition="883" endWordPosition="886">associated random hash function, hAi, drawn from a universal hash function (UHF) family h (Carter and Wegman, 1977), which is then used to generate the n-gram fingerprint: f(x) = hAi(x). If the bucket AZ is not full we conduct a scan of its cells. If the fingerprint f(x) is not already encoded in the bucket AZ we add the fingerprint and value to the first empty cell available. We allocate a preset number of the least significant bits of each w-bit cell to hold v(x) and the remaining most significant bits for f(x) but this is arbitrary. Any encoding scheme, such as the packed representation of Talbot and Brants (2008), is viable here. However, if f(x) ∈ AZ already (there is a collision) we store the n-gram x and associated value v(x) in the lossless overflow dictionary D instead. D also holds the n-grams that were hashed to any buckets that are already full. To query for the value of an n-gram, we first check if the gram is in the overflow dictionary D. If it is, we return the associated value. Otherwise we query A using the same hash functions and procedure as insertion. If we find a matching fingerprint in the appropriate bucket AZ we have a hit with high probability. Deletions and updates are symmetric </context>
<context position="10406" citStr="Talbot and Brants (2008)" startWordPosition="1771" endWordPosition="1774">p queries such as“Is x E S?”. The BF uses an array of m bits and k independent UHFs each with range 0,... , m−1. For insertion, each item is hashed through the k hash functions and the resulting target bits are set to one. During testing, an event x E U is passed through the same k hash functions and if any bit tested is zero then x was not in the support S. The Bloomier filter directly represents keyvalue pairs by using a table of cells and a family of k associated hash functions (Chazelle et al., 2004). Each key-value pair is associated with k cells in the table via a perfect hash function. Talbot and Brants (2008) used a Bloomier filter to encode a LM. Before data can be added to the Bloomier filter, a greedy perfect hashing of all entries needs to be computed in advance; this attempts to associate each event in the support with one unique table cell so no other entry collides with it. The procedure can fail and might need to be repeated many times. Neither of these two randomised language models are suitable for modelling a stream. Given the fact that the stream is of unbounded size, we are forced to delete items if we wish to maintain a constant error rate and account for novel n-grams. However, the </context>
<context position="25430" citStr="Talbot and Brants (2008)" startWordPosition="4328" endWordPosition="4331">e O-RLM while varying only the number of cells in each bucket and keeping all other model parameters constant. We test membership of n-grams in an unseen corpus against those stored in the table. Our tests were conducted over a larger stream of 1.25B n-grams from the Gigaword corpus(Graff, Date Severe Random Conservative Jan 36.44 36.44 36.44 Apr 35.87 31.08 35.51 Aug 29.00 19.31 29.14 Avg 33.77 29.11 33.70 Table 6: Adaptation results measured in BLEU. Random deletions degrade performance when adapting a 200MB O-RLM. 2003). We set our space usage to match the 3.08 bytes per n-gram reported in Talbot and Brants (2008) and held out just over 1M unseen n-grams to test the error rates of our models. In Figure 4 we see a direct correlation between model error and cells per buckets. As the number of cells decreases the false positive rate drops as well since fewer cells to compare against per bucket means a lower chance of producing collisions. If the range is decreased too much though more data is diverted to the overflow dictionary due to many buckets reaching capacity when inserting and adapting. Clearly this is less space efficient. Figure 5 shows the relationship between the percent of data in the overflow</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>David Talbot and Thorsten Brants. 2008. Randomized language models via perfect hash functions. In Proceedings of ACL-08: HLT, pages 505–513, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Smoothed Bloom filter language models: Tera-scale LMs on the cheap.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>468--476</pages>
<contexts>
<context position="1628" citStr="Talbot and Osborne, 2007" startWordPosition="240" endWordPosition="243">chine translation (SMT) systems. They assign probabilities to generated hypotheses in the target language informing lexical selection. The most common form of LMs in SMT systems are smoothed n-gram models which predict a word based on a contextual history of n − 1 words. For some languages (such as English) trillions of words are available for training purposes. This fact, along with the observation that machine translation quality improves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space (Talbot and Osborne, 2007; Talbot and Brants, 2008). Randomised LMs (RLMs) solve the problem of representing large, static LMs but they are batch oriented and cannot incorporate new data without fully retraining from scratch. This property makes current RLMs ill-suited for modelling the massive volume of textual material published daily on the Web. We present a novel RLM which is capable of incremental (re)training. We use random hash functions coupled with an online perfect hashing algorithm to represent n-grams in small space. This makes it well-suited for dealing with an unbounded stream of training material. To ou</context>
<context position="9637" citStr="Talbot and Osborne (2007)" startWordPosition="1625" endWordPosition="1628">the bucket hash function is not highly IID. 2.4 Basic RLM Comparisons Table 1 compares expected versus observed false positive rates for the Bloom filter, TB-LM, and ORLM obtained by querying a model of approximately 280M events with 100K unseen n-grams. LM Expected Observed RAM Table 1: Example false postive rates and corresponding memory usage for all randomised LMs. We see the bit-based Bloom filter uses significantly less memory than the cell-based alternatives and the O-RLM consumes more memory than the TB-LM for the same expected error rate. 3 Related Work 3.1 Randomised Language Models Talbot and Osborne (2007) used a Bloom filter (Bloom, 1970) to encode a smoothed LM. A Bloom filter (BF) represents a set S from arbitrary domain U and supports membership queries such as“Is x E S?”. The BF uses an array of m bits and k independent UHFs each with range 0,... , m−1. For insertion, each item is hashed through the k hash functions and the resulting target bits are set to one. During testing, an event x E U is passed through the same k hash functions and if any bit tested is zero then x was not in the support S. The Bloomier filter directly represents keyvalue pairs by using a table of cells and a family </context>
<context position="11138" citStr="Talbot and Osborne, 2007" startWordPosition="1904" endWordPosition="1907">ashing of all entries needs to be computed in advance; this attempts to associate each event in the support with one unique table cell so no other entry collides with it. The procedure can fail and might need to be repeated many times. Neither of these two randomised language models are suitable for modelling a stream. Given the fact that the stream is of unbounded size, we are forced to delete items if we wish to maintain a constant error rate and account for novel n-grams. However, the Bloom filter LM nor the Bloomier Filter LM support deletions. The bit sharing of the Bloom filter (BF) LM (Talbot and Osborne, 2007) means deletions may corrupt shared stored events. The Bloomier filter LM (Talbot and Brants, 2008) has a precomputed matching of keys shared between a constant number of cells in the filter array. Lossless Bloom TB-LM O-RLM 0 0 7450MB 0.0039 0.0038 390MB 0.0039 0.0033 640MB 0.0039 0.0031 705MB 758 Deleting items from a Bloomier Filter without recomputing the perfect hash will corrupt it. 3.2 Probabilistic Counting Concurrent work has used approximate counting schemes based on Morris (1978) to estimate in small space frequencies over a high volume input text stream (Van Durme and Lall, 2009; G</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Smoothed Bloom filter language models: Tera-scale LMs on the cheap. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Probabilistic counting with randomized storage.</title>
<date>2009</date>
<booktitle>In Twenty-First International Joint Conference on Artificial Intelligence (IJCAI-09),</booktitle>
<location>Pasadena, CA,</location>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009. Probabilistic counting with randomized storage. In Twenty-First International Joint Conference on Artificial Intelligence (IJCAI-09), Pasadena, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W D Whittaker</author>
</authors>
<title>Temporal adaptation of language models.</title>
<date>2001</date>
<booktitle>In In Adaptation Methods for Speech Recognition, ISCA Tutorial and Research Workshop (ITRW),</booktitle>
<pages>203--206</pages>
<contexts>
<context position="17369" citStr="Whittaker (2001)" startWordPosition="2946" endWordPosition="2947">ng all of the data yields the lowest perplexity. We note that this is a robust finding, since we also observe it in other domains. For example, we Epoch Stream Window 1 08.20.1996 to 01.01.1997 2 01.02.1997 to 04.23.1997 3 04.24.1997 to 08.18.1997 Table 2: The stream timeline is divided into windowed epochs for our recency experiments. conducted the same tests over a stream of 18 billion tokens drawn from 80 million time-stamped blog posts downloaded from the web with matching results. The effect of recency on perplexity has also been observed elsewhere (see, for example, Rosenfeld (1995) and Whittaker (2001)). Our experiments show that a possible way to tackle stream-based translation is to always focus the attention of the LM on the most recent part of the stream. This means we remove data from the model that came from the receding parts of the stream and replace it with the present. 5 SMT Experiments 5.1 Experimental Setup We used publicly available resources for all our tests: for decoding we used Moses (Koehn and Hoang, 2007) and our parallel data was taken from the Spanish-English section of Europarl. For test material, we translated 63 documents (800 sentences) from three randomly selected </context>
</contexts>
<marker>Whittaker, 2001</marker>
<rawString>E. W. D. Whittaker. 2001. Temporal adaptation of language models. In In Adaptation Methods for Speech Recognition, ISCA Tutorial and Research Workshop (ITRW), pages 203–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>993--1000</pages>
<contexts>
<context position="12790" citStr="Wu et al., 2008" startWordPosition="2176" endWordPosition="2179">l. (2007) looked at Golomb Coding and Brants et al. (2007) used tries in a distributed setting. These methods are less succinct than randomised approaches. 3.4 Adaptive Language Models There is a large literature on adaptive LMs from the speech processing domain (Bellegarda, 2004). The primary difference between the O-RLM and other adaptive LMs is that we add and remove ngrams from the model instead of adapting only the parameters of the current support set. 3.5 Domain adaptation in Machine Translation Within MT there has been a variety of approaches dealing with domain adaption (for example (Wu et al., 2008; Koehn and Schroeder, 2007). Typically LMs are interpolated with one another, yielding good results. These models are usually statically trained, exact and unable to deal with an unbounded stream of monolingual data. Domain adaptation has similarities with streaming, in that our stream may be non-stationary. A crucial difference however is that the stream is of unbounded length, whereas domain adaptation usually assumes some finite and fixed training set. 4 Stream-based translation Streaming algorithms have numerous applications in mainstream computer science (Muthukrishnan, 2003) but to date</context>
</contexts>
<marker>Wu, Wang, Zong, 2008</marker>
<rawString>Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 993–1000. Coling 2008 Organizing Committee, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>