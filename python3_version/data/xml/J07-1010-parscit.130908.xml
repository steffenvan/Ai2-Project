<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003558">
<title confidence="0.8305325">
Last Words
Googleology is Bad Science
</title>
<author confidence="0.97805">
Adam Kilgarriff
</author>
<affiliation confidence="0.9470635">
Lexical Computing Ltd. and
University of Sussex
</affiliation>
<bodyText confidence="0.99289258974359">
The World Wide Web is enormous, free, immediately available, and largely linguistic.
As we discover, on ever more fronts, that language analysis and generation benefit
from big data, so it becomes appealing to use the Web as a data source. The question,
then, is how.
The low-entry-cost way to use the Web is via a commercial search engine. If the
goal is to find frequencies or probabilities for some phenomenon of interest, we can use
the hit count given in the search engine’s hits page to make an estimate. People have
been doing this for some time now. Early work using hit counts include Grefenstette
(1999), who identified likely translations for compositional phrases, and Turney (2001),
who found synonyms; perhaps the most cited study is Keller and Lapata (2003), who
established the validity of frequencies gathered in this way using experiments with
human subjects. Leading recent work includes Nakov and Hearst (2005), who build
models of noun compound bracketing.
The initial-entry cost for this kind of research is zero. Given a computer and an
Internet connection, you input the query and get a hit count. But if the work is to
proceed beyond the anecdotal, a range of issues must be addressed.
First, the commercial search engines do not lemmatize or part-of-speech tag. To take
a simple case: To estimate frequencies for the verb-object pair fulfil obligation, Keller and
Lapata make 36 queries (to cover the whole inflectional paradigm of both verb and
noun and to allow for definite and indefinite articles to come between them) to each of
Google and Altavista. It would be desirable to be able to search for fulfil obligation with
a single search. If the research question concerns a language with more inflection, or a
construction allowing more variability, the issues compound.
Secondly, the search syntax is limited. There are animated and intense discussions
on the CORPORA mailing list, the chief forum for such matters, on the availability or
otherwise of wild cards and ‘near’ operators with each of the search engines, and cries
of horror when one of the companies makes changes. (From my reading of the CORPORA
list, these changes seem mainly in the direction of offering less metalanguage.)
Thirdly, there are constraints on numbers of queries and numbers of hits per query.
Google only allows automated querying via its API, limited to 1,000 queries per user
per day. If there are 36 Google queries per single ’linguistic’ query, we can make just 27
linguistic queries per day. Other search engines are currently less restrictive but that
may arbitrarily change (particularly as corporate mergers are played out), and also
Google has (probably) the largest index—and size is what we are going to the Internet
for.
Fourthly, search hits are for pages, not for instances.
Working with commercial search engines makes us develop workarounds. We
become experts in the syntax and constraints of Google, Yahoo, Altavista, and so on.
We become ‘googleologists’. The argument that the commercial search engines provide
</bodyText>
<footnote confidence="0.3690725">
© 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 1
</footnote>
<bodyText confidence="0.998548833333333">
low-cost access to the Web fades as we realize how much of our time is devoted to
working with and against the constraints that the search engines impose.
But science is hard work, and there are usually many foothill problems to be
mastered before we get to the mountains that are our true goal. So this is all regular
science.
Or so it may seem until we consider the arbitrariness of search engine counts. They
depend on many specifics of the search engine’s practice, including how it handles spam
and duplicates. (See the entries “Yahoo’s missing pages” [2005] and “Crazy duplicates”
[2006] in Jean V´eronis’s blog.1) The engines will give you substantially different counts,
even for repeats of the same query. In a small experiment, queries repeated the following
day gave counts over 10% different 9 times in 30, and by a factor of two different 6 times
in 30. The reasons are that queries are sent to different computers, at different points in
the update cycle, and with different data in their caches.
People wishing to use the URLs, rather than the counts, that search engines provide
in their hits pages face another issue: The hits are sorted according to a complex and
unknown algorithm (with full listings of all results usually not permitted) so we do not
know what biases are being introduced. If we wish to investigate the biases, the area we
become expert in is googleology, not linguistics.
</bodyText>
<subsectionHeader confidence="0.97783">
An Academic-Community Alternative
</subsectionHeader>
<bodyText confidence="0.9999866">
An alternative is to work like the search engines, downloading and indexing substantial
proportions of the World Wide Web, but to do so transparently, giving reliable figures,
and supporting language researchers’ queries. In Baroni and Kilgarriff (2006) we report
on a feasibility study: We prepared Web corpora for German (‘DeWaC’) and Italian
(‘ItWaC’) with around 1.5 billion words each, now loaded into a sophisticated corpus
query tool and available for research use.2 (Of course there are various other large Web
datasets that research groups have downloaded and are using for NLP.) By sharing
good practice and resources and developing expertise, the prospects of the academic
research community having resources to compare with Google, Microsoft, and so forth,
improves.
</bodyText>
<subsectionHeader confidence="0.996581">
Data Cleaning
</subsectionHeader>
<bodyText confidence="0.999882333333333">
The process involves crawling, downloading, ’cleaning’, and de-duplicating the data,
then linguistically annotating it and loading it into a corpus query tool. Expertise and
tools are available for most of these steps, with the Internet community providing
crawlers and a de-duplication algorithm (Broder et al. 1997) and the NLP community
providing corpus query tools, lemmatizers, and POS-taggers for many languages. But
in the middle there is a logjam. The questions
</bodyText>
<listItem confidence="0.99944825">
• How do we detect and get rid of navigation bars, headers, footers, ?
• How do we identify paragraphs and other structural information?
• How do we produce output in a standard form suitable for further
processing?
</listItem>
<footnote confidence="0.996462">
1 http://aixtal.blogspot.com.
2 http://www.sketchengine.co.uk.
</footnote>
<page confidence="0.965292">
148
</page>
<bodyText confidence="0.984614756756757">
Kilgarriff Googleology is Bad Science
always arise. Cleaning is a low-level, unglamorous task, yet crucial: The better it is
done, the better the outcomes. All further layers of linguistic processing depend on the
cleanliness of the data.
To date, cleaning has been done in isolation (and it has not been seen as interesting
enough to publish on). Resources have not been pooled, and it has been done cursorily,
if at all. Thus, a paper which describes work with a vast Web corpus of 31 million
pages devotes just one paragraph to the corpus development process, and mentions
de-duplication and language-filtering but no other cleaning (Ravichandran, Pantel, and
Hovy 2005, Section 4). A paper using that same corpus notes, in a footnote, ”as a
preprocessing step we hand-edit the clusters to remove those containing non-English
words, terms related to adult content, and other Webpage-specific clusters” (Snow,
Jurafsky, and Ng 2006). The development of open-source tools that identify and filter
out each of the many sorts of ‘dirt’ found in Web pages to give clean output will have
many beneficiaries, and the CLEANEVAL project3 has been set up to this end. There
will, of course, be differences of opinion about what should be filtered out, and a full
toolset will provide a range of options as well as provoke discussion on what we should
include and exclude to develop a low-noise, general-language corpus that is suitable
for linguistic and language technology research by a wide range of researchers. (In the
following, I call the data that meet these criteria “running text.”)
How Much Non-Duplicate Running Text do the Commercial Search Engines
Index, and Can the Academic Community Compare?
Although the anti-googleology arguments may be acknowledged, researchers often
shake their heads and say “Ah, but the commercial search engines index so much
data.” If the goal is to find frequencies of arbitrary (noun, preposition, verb) and
(noun, preposition, noun) triples for PP-attachment disambiguation, then a very, very
large dataset is needed to get many non-zero counts. Researchers will continue to use
Google, Yahoo, and Altavista unless the NLP community’s resources are ‘Google-scale’.
The question this forces is, “How much non-duplicate running text do Google and
competitors index?”
For German and Italian, we addressed the question by comparing frequency counts
for a sample of words in DeWaC and ItWaC with Google frequencies. Thirty words were
randomly selected for each language. They were mid-frequency words that were not
common words in English, French, German (for Italian), Italian (for German), Portugese,
or Spanish, with at least five characters (because longer words are less likely to clash
with acronyms or words from other languages). For each of these words, Google was
searched with a number of parameters:
</bodyText>
<listItem confidence="0.935908666666667">
• with and without “safe search” for excluding adult material
• with language set to German/Italian
• with the “all-in-text” box checked, so that documents were only included
as hits if they contained the search term
• with and without the site filter set to .it domain only (for Italian), .de or .at
domains only for German
</listItem>
<footnote confidence="0.775098">
3 http://cleaneval.sigwac.org.uk.
</footnote>
<page confidence="0.9936">
149
</page>
<note confidence="0.4858">
Computational Linguistics Volume 33, Number 1
</note>
<bodyText confidence="0.9887451">
Results were not always consistent, with additional filters sometimes producing an
increased hit count, so for each word we took the midpoint of the maximum and
minimum of the results and compared this number to the DeWaC/ItWaC document
frequencies. Here there were two numbers to consider: the count before filtering and
cleaning, and the count after. A sample of the results is shown in Table 1.
It would have been convenient to use the Google API but it gave much lower counts
than browser queries: A substantial number were one-eighteenth as large. Altavista,
which has a reputation for NLP-friendliness, was also explored, but because Altavista’s
index is known to be smaller than Google’s, and the goal was to compare it with the
biggest index available, Altavista results were not going to answer the critical question.
The goal is to use the figures to assess the quantity of duplicate-free, Google-
indexed running text for German and Italian. The Google counts are best compared with
DeWaC/ItWaC ’raw’ counts, and a first scaling factor will give an indication of the size
of the Google-indexed German/Italian World Wide Web inclusive of non-running-text
and duplicates. Taking the midpoint between maximum and minimum and averaging
across words, the ratio for German is 83.5:1 and for Italian, 33:1. A further scaling factor
should then be applied, based on the raw:clean ratio, to assess how much of the material
is duplicated or not running text. However, we do not know to what extent Google
applies de-duplication and other rubbish-filtering strategies before calculating counts,
and DeWaC/ItWaC filtering and cleaning errs towards rejecting doubtful material. The
mean ratio raw:clean is 5.3 for German, 4.5 for Italian: For a best estimate, we halve
the figures. Best estimates for the Google-indexed, non-duplicative running text are
then 45 billion words for German and 25 billion words for Italian, as summarized in
Table 2.
Clearly this is highly approximate, and the notion of running text needs articulation.
The point here is that a pilot project (of half a person-year’s effort) was able to pro-
vide a corpus that was several percent of Google-scale, for two languages. It provides
grounds for optimism that the Web can be used, without reliance on commercial search
engines and, at least for languages other than English, without sacrificing too much in
terms of scale.
</bodyText>
<subsectionHeader confidence="0.801538">
In Sum
</subsectionHeader>
<bodyText confidence="0.999091">
The most talked-about presentation of ACL 2005 was Franz-Josef Och’s, in which he
presented statistical MT results based on a 250 billion-word English corpus. His results
</bodyText>
<tableCaption confidence="0.88411">
Table 1
</tableCaption>
<bodyText confidence="0.994105">
Comparing Google and DeWaC frequencies for a sample of words. ’max’ and ’min’ are the
maximum and minimum from a set of six Google searches. ’raw’ and ’clean’ are counts for the
numbers of documents that the word occurred in in DeWaC, before and after the cleaning,
filtering, and de-duplication. All numbers in thousands.
Word max min raw clean
besuchte 10,500 3,800 82 18
stirn 3,380 620 32 11
gerufen 7,140 3,720 67 27
verringert 6,860 3,460 52 16
bislang 24,400 11,600 239 90
brach 4,360 2,260 45 20
</bodyText>
<page confidence="0.99131">
150
</page>
<note confidence="0.796411">
Kilgarriff Googleology is Bad Science
</note>
<tableCaption confidence="0.996942">
Table 2
</tableCaption>
<table confidence="0.799819166666667">
Scaling up from DeWaC/ItWaC size to estimate non-duplicate German/Italian running
text indexed by Google. Scaling 1 compares Google frequencies with ’raw’ DeWac/ItWaC
frequencies. Scaling 2 compares ’raw’ and ’filtered’ DeWaC/ItWaC.
DeWaC/ItWaC Scaling 1 Scaling 2 % of Google Estimate
German 1.41 bn 83.5 2.65 3.1 45 bn
Italian 1.67 bn 33.0 2.25 6.8 25 bn
</table>
<bodyText confidence="0.997004833333333">
led the field. He was in a privileged position to have access to a corpus of that size. He
works at Google.
With enormous data, you get better results. There are two possible responses for
the academic NLP community. The first is to accept defeat: “We will never have re-
sources on the scale of Google, Microsoft, and Yahoo, so we should accept that our
systems will not really compete, that they will be proofs-of-concept or deal with niche
problems, but will be out of the mainstream of high-performance language technology
system development.” The second is to say: We too need to make resources on this
scale available, and they should be available to researchers in universities as well as
behind corporate firewalls, and we can do it, because resources of the right scale are
available, for free, on the World Wide Web, and between us we have the skills and
the talent.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.998701736842106">
Baroni, Marco and Adam Kilgarriff. 2006.
Large linguistically-processed web
corpora for multiple languages. In
Proceedings of European ACL, Trento,
Italy.
Broder, Andrei Z., Steven C. Glassman,
Mark S. Manasse, and Geoffrey Zweig.
1997. Syntactic clustering of the web.
Computer Networks, 29(8–13):1157–1166.
Grefenstette, Gregory. 1999. The WWW as
a resource for example-based MT tasks.
In ASLIB Translating and the Computer
Conference, London.
Keller, Frank and Mirella Lapata. 2003.
Using the web to obtain frequencies for
unseen bigrams. Computational Linguistics,
29(3):459–484.
Nakov, Preslav and Marti Hearst. 2005.
Search engine statistics beyond the
n-gram: Application to noun compound
bracketing. In Proceedings of the Ninth
Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 17–24, Ann Arbor, Michigan.
Ravichandran, Deepak, Patrick Pantel, and
Eduard Hovy. 2005. Randomized
algorithms and NLP: Using locality
sensitive hash functions for high speed
noun clustering. In Proceedings of ACL,
Ann Arbor, Michigan.
Snow, Rion, Daniel Jurafsky, and Andrew
Ng. 2006. Semantic taxonomy induction
from heterogenous evidence. In
Proceedings of ACL, Sydney.
Turney, Peter D. 2001. Mining the web for
synonyms: PMI-IR versus LSA on TOEFL.
In European Conference on Machine Learning,
pages 491–502.
</reference>
<page confidence="0.998326">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000529">
<title confidence="0.9538745">Last Words Googleology is Bad Science</title>
<author confidence="0.999664">Adam Kilgarriff</author>
<affiliation confidence="0.958363">Lexical Computing Ltd. and University of Sussex</affiliation>
<abstract confidence="0.987091285714286">The World Wide Web is enormous, free, immediately available, and largely linguistic. As we discover, on ever more fronts, that language analysis and generation benefit from big data, so it becomes appealing to use the Web as a data source. The question, then, is how. The low-entry-cost way to use the Web is via a commercial search engine. If the goal is to find frequencies or probabilities for some phenomenon of interest, we can use the hit count given in the search engine’s hits page to make an estimate. People have been doing this for some time now. Early work using hit counts include Grefenstette (1999), who identified likely translations for compositional phrases, and Turney (2001), who found synonyms; perhaps the most cited study is Keller and Lapata (2003), who established the validity of frequencies gathered in this way using experiments with human subjects. Leading recent work includes Nakov and Hearst (2005), who build models of noun compound bracketing. The initial-entry cost for this kind of research is zero. Given a computer and an Internet connection, you input the query and get a hit count. But if the work is to proceed beyond the anecdotal, a range of issues must be addressed. First, the commercial search engines do not lemmatize or part-of-speech tag. To take simple case: To estimate frequencies for the verb-object pair Keller and Lapata make 36 queries (to cover the whole inflectional paradigm of both verb and noun and to allow for definite and indefinite articles to come between them) to each of and Altavista. It would be desirable to be able to search for obligation a single search. If the research question concerns a language with more inflection, or a construction allowing more variability, the issues compound. Secondly, the search syntax is limited. There are animated and intense discussions the list, the chief forum for such matters, on the availability or otherwise of wild cards and ‘near’ operators with each of the search engines, and cries horror when one of the companies makes changes. (From my reading of the list, these changes seem mainly in the direction of offering less metalanguage.) Thirdly, there are constraints on numbers of queries and numbers of hits per query. Google only allows automated querying via its API, limited to 1,000 queries per user per day. If there are 36 Google queries per single ’linguistic’ query, we can make just 27 linguistic queries per day. Other search engines are currently less restrictive but that may arbitrarily change (particularly as corporate mergers are played out), and also Google has (probably) the largest index—and size is what we are going to the Internet for. Fourthly, search hits are for pages, not for instances. Working with commercial search engines makes us develop workarounds. We become experts in the syntax and constraints of Google, Yahoo, Altavista, and so on. We become ‘googleologists’. The argument that the commercial search engines provide © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 1 low-cost access to the Web fades as we realize how much of our time is devoted to working with and against the constraints that the search engines impose. But science is hard work, and there are usually many foothill problems to be mastered before we get to the mountains that are our true goal. So this is all regular science. Or so it may seem until we consider the arbitrariness of search engine counts. They depend on many specifics of the search engine’s practice, including how it handles spam and duplicates. (See the entries “Yahoo’s missing pages” [2005] and “Crazy duplicates” in Jean V´eronis’s The engines will give you substantially different counts, even for repeats of the same query. In a small experiment, queries repeated the following day gave counts over 10% different 9 times in 30, and by a factor of two different 6 times in 30. The reasons are that queries are sent to different computers, at different points in the update cycle, and with different data in their caches. People wishing to use the URLs, rather than the counts, that search engines provide in their hits pages face another issue: The hits are sorted according to a complex and unknown algorithm (with full listings of all results usually not permitted) so we do not know what biases are being introduced. If we wish to investigate the biases, the area we become expert in is googleology, not linguistics. An Academic-Community Alternative An alternative is to work like the search engines, downloading and indexing substantial proportions of the World Wide Web, but to do so transparently, giving reliable figures, and supporting language researchers’ queries. In Baroni and Kilgarriff (2006) we report on a feasibility study: We prepared Web corpora for German (‘DeWaC’) and Italian (‘ItWaC’) with around 1.5 billion words each, now loaded into a sophisticated corpus tool and available for research (Of course there are various other large Web datasets that research groups have downloaded and are using for NLP.) By sharing good practice and resources and developing expertise, the prospects of the academic research community having resources to compare with Google, Microsoft, and so forth, improves. Data Cleaning The process involves crawling, downloading, ’cleaning’, and de-duplicating the data, then linguistically annotating it and loading it into a corpus query tool. Expertise and tools are available for most of these steps, with the Internet community providing crawlers and a de-duplication algorithm (Broder et al. 1997) and the NLP community providing corpus query tools, lemmatizers, and POS-taggers for many languages. But in the middle there is a logjam. The questions How do we detect and get rid of navigation bars, headers, footers, ? • How do we identify paragraphs and other structural information? • How do we produce output in a standard form suitable for further processing? 1 http://aixtal.blogspot.com. 2 http://www.sketchengine.co.uk. 148 Kilgarriff Googleology is Bad Science always arise. Cleaning is a low-level, unglamorous task, yet crucial: The better it is done, the better the outcomes. All further layers of linguistic processing depend on the cleanliness of the data. To date, cleaning has been done in isolation (and it has not been seen as interesting enough to publish on). Resources have not been pooled, and it has been done cursorily, if at all. Thus, a paper which describes work with a vast Web corpus of 31 million pages devotes just one paragraph to the corpus development process, and mentions de-duplication and language-filtering but no other cleaning (Ravichandran, Pantel, and Hovy 2005, Section 4). A paper using that same corpus notes, in a footnote, ”as a preprocessing step we hand-edit the clusters to remove those containing non-English words, terms related to adult content, and other Webpage-specific clusters” (Snow, Jurafsky, and Ng 2006). The development of open-source tools that identify and filter out each of the many sorts of ‘dirt’ found in Web pages to give clean output will have beneficiaries, and the CLEANEVAL has been set up to this end. There will, of course, be differences of opinion about what should be filtered out, and a full toolset will provide a range of options as well as provoke discussion on what we should include and exclude to develop a low-noise, general-language corpus that is suitable for linguistic and language technology research by a wide range of researchers. (In the following, I call the data that meet these criteria “running text.”) How Much Non-Duplicate Running Text do the Commercial Search Engines Index, and Can the Academic Community Compare? Although the anti-googleology arguments may be acknowledged, researchers often shake their heads and say “Ah, but the commercial search engines index so much If the goal is to find frequencies of arbitrary preposition, preposition, for PP-attachment disambiguation, then a very, very large dataset is needed to get many non-zero counts. Researchers will continue to use Google, Yahoo, and Altavista unless the NLP community’s resources are ‘Google-scale’. The question this forces is, “How much non-duplicate running text do Google and competitors index?” For German and Italian, we addressed the question by comparing frequency counts for a sample of words in DeWaC and ItWaC with Google frequencies. Thirty words were randomly selected for each language. They were mid-frequency words that were not common words in English, French, German (for Italian), Italian (for German), Portugese, or Spanish, with at least five characters (because longer words are less likely to clash with acronyms or words from other languages). For each of these words, Google was searched with a number of parameters: • with and without “safe search” for excluding adult material • with language set to German/Italian • with the “all-in-text” box checked, so that documents were only included as hits if they contained the search term • with and without the site filter set to .it domain only (for Italian), .de or .at domains only for German 3 http://cleaneval.sigwac.org.uk. 149 Computational Linguistics Volume 33, Number 1 Results were not always consistent, with additional filters sometimes producing an increased hit count, so for each word we took the midpoint of the maximum and minimum of the results and compared this number to the DeWaC/ItWaC document frequencies. Here there were two numbers to consider: the count before filtering and cleaning, and the count after. A sample of the results is shown in Table 1. It would have been convenient to use the Google API but it gave much lower counts than browser queries: A substantial number were one-eighteenth as large. Altavista, which has a reputation for NLP-friendliness, was also explored, but because Altavista’s index is known to be smaller than Google’s, and the goal was to compare it with the biggest index available, Altavista results were not going to answer the critical question. The goal is to use the figures to assess the quantity of duplicate-free, Googleindexed running text for German and Italian. The Google counts are best compared with DeWaC/ItWaC ’raw’ counts, and a first scaling factor will give an indication of the size of the Google-indexed German/Italian World Wide Web inclusive of non-running-text and duplicates. Taking the midpoint between maximum and minimum and averaging across words, the ratio for German is 83.5:1 and for Italian, 33:1. A further scaling factor should then be applied, based on the raw:clean ratio, to assess how much of the material is duplicated or not running text. However, we do not know to what extent Google applies de-duplication and other rubbish-filtering strategies before calculating counts, and DeWaC/ItWaC filtering and cleaning errs towards rejecting doubtful material. The mean ratio raw:clean is 5.3 for German, 4.5 for Italian: For a best estimate, we halve the figures. Best estimates for the Google-indexed, non-duplicative running text are then 45 billion words for German and 25 billion words for Italian, as summarized in Table 2. Clearly this is highly approximate, and the notion of running text needs articulation. The point here is that a pilot project (of half a person-year’s effort) was able to provide a corpus that was several percent of Google-scale, for two languages. It provides grounds for optimism that the Web can be used, without reliance on commercial search engines and, at least for languages other than English, without sacrificing too much in terms of scale. In Sum The most talked-about presentation of ACL 2005 was Franz-Josef Och’s, in which he presented statistical MT results based on a 250 billion-word English corpus. His results Table 1 Comparing Google and DeWaC frequencies for a sample of words. ’max’ and ’min’ are the maximum and minimum from a set of six Google searches. ’raw’ and ’clean’ are counts for the numbers of documents that the word occurred in in DeWaC, before and after the cleaning, filtering, and de-duplication. All numbers in thousands. Word max min raw clean besuchte 10,500 3,800 82 18 stirn 3,380 620 32 11 gerufen 7,140 3,720 67 27 verringert 6,860 3,460 52 16 bislang 24,400 11,600 239 90 brach 4,360 2,260 45 20 150 Kilgarriff Googleology is Bad Science Table 2 Scaling up from DeWaC/ItWaC size to estimate non-duplicate German/Italian running text indexed by Google. Scaling 1 compares Google frequencies with ’raw’ DeWac/ItWaC frequencies. Scaling 2 compares ’raw’ and ’filtered’ DeWaC/ItWaC. DeWaC/ItWaC Scaling 1 Scaling 2 % of Google Estimate German 1.41 bn 83.5 2.65 3.1 45 bn Italian 1.67 bn 33.0 2.25 6.8 25 bn led the field. He was in a privileged position to have access to a corpus of that size. He works at Google. With enormous data, you get better results. There are two possible responses for the academic NLP community. The first is to accept defeat: “We will never have resources on the scale of Google, Microsoft, and Yahoo, so we should accept that our systems will not really compete, that they will be proofs-of-concept or deal with niche problems, but will be out of the mainstream of high-performance language technology system development.” The second is to say: We too need to make resources on this scale available, and they should be available to researchers in universities as well as behind corporate firewalls, and we can do it, because resources of the right scale are available, for free, on the World Wide Web, and between us we have the skills and the talent.</abstract>
<note confidence="0.8665305">References Baroni, Marco and Adam Kilgarriff. 2006.</note>
<title confidence="0.42037">Large linguistically-processed web</title>
<author confidence="0.713321">In</author>
<affiliation confidence="0.960013">of European Trento,</affiliation>
<address confidence="0.968339">Italy.</address>
<author confidence="0.94437">Andrei Z Broder</author>
<author confidence="0.94437">Steven C Glassman</author>
<author confidence="0.94437">Mark S Manasse</author>
<author confidence="0.94437">Geoffrey Zweig</author>
<note confidence="0.575043105263158">1997. Syntactic clustering of the web. 29(8–13):1157–1166. Grefenstette, Gregory. 1999. The WWW as a resource for example-based MT tasks. Translating and the Computer London. Keller, Frank and Mirella Lapata. 2003. Using the web to obtain frequencies for bigrams. 29(3):459–484. Nakov, Preslav and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound In of the Ninth Conference on Computational Natural Learning pages 17–24, Ann Arbor, Michigan. Ravichandran, Deepak, Patrick Pantel, and Eduard Hovy. 2005. Randomized</note>
<abstract confidence="0.882688166666667">algorithms and NLP: Using locality sensitive hash functions for high speed clustering. In of Ann Arbor, Michigan. Snow, Rion, Daniel Jurafsky, and Andrew Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In of Sydney. Turney, Peter D. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. Conference on Machine pages 491–502.</abstract>
<intro confidence="0.648687">151</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Large linguistically-processed web corpora for multiple languages. In</title>
<date>2006</date>
<booktitle>Proceedings of European ACL,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="4938" citStr="Baroni and Kilgarriff (2006)" startWordPosition="801" endWordPosition="804">at search engines provide in their hits pages face another issue: The hits are sorted according to a complex and unknown algorithm (with full listings of all results usually not permitted) so we do not know what biases are being introduced. If we wish to investigate the biases, the area we become expert in is googleology, not linguistics. An Academic-Community Alternative An alternative is to work like the search engines, downloading and indexing substantial proportions of the World Wide Web, but to do so transparently, giving reliable figures, and supporting language researchers’ queries. In Baroni and Kilgarriff (2006) we report on a feasibility study: We prepared Web corpora for German (‘DeWaC’) and Italian (‘ItWaC’) with around 1.5 billion words each, now loaded into a sophisticated corpus query tool and available for research use.2 (Of course there are various other large Web datasets that research groups have downloaded and are using for NLP.) By sharing good practice and resources and developing expertise, the prospects of the academic research community having resources to compare with Google, Microsoft, and so forth, improves. Data Cleaning The process involves crawling, downloading, ’cleaning’, and </context>
</contexts>
<marker>Baroni, Kilgarriff, 2006</marker>
<rawString>Baroni, Marco and Adam Kilgarriff. 2006. Large linguistically-processed web corpora for multiple languages. In Proceedings of European ACL, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Z Broder</author>
<author>Steven C Glassman</author>
<author>Mark S Manasse</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Syntactic clustering of the web.</title>
<date>1997</date>
<journal>Computer Networks,</journal>
<pages>29--8</pages>
<contexts>
<context position="5795" citStr="Broder et al. 1997" startWordPosition="930" endWordPosition="933">are various other large Web datasets that research groups have downloaded and are using for NLP.) By sharing good practice and resources and developing expertise, the prospects of the academic research community having resources to compare with Google, Microsoft, and so forth, improves. Data Cleaning The process involves crawling, downloading, ’cleaning’, and de-duplicating the data, then linguistically annotating it and loading it into a corpus query tool. Expertise and tools are available for most of these steps, with the Internet community providing crawlers and a de-duplication algorithm (Broder et al. 1997) and the NLP community providing corpus query tools, lemmatizers, and POS-taggers for many languages. But in the middle there is a logjam. The questions • How do we detect and get rid of navigation bars, headers, footers, ? • How do we identify paragraphs and other structural information? • How do we produce output in a standard form suitable for further processing? 1 http://aixtal.blogspot.com. 2 http://www.sketchengine.co.uk. 148 Kilgarriff Googleology is Bad Science always arise. Cleaning is a low-level, unglamorous task, yet crucial: The better it is done, the better the outcomes. All furt</context>
</contexts>
<marker>Broder, Glassman, Manasse, Zweig, 1997</marker>
<rawString>Broder, Andrei Z., Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. 1997. Syntactic clustering of the web. Computer Networks, 29(8–13):1157–1166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>The WWW as a resource for example-based MT tasks.</title>
<date>1999</date>
<booktitle>In ASLIB Translating and the Computer Conference,</booktitle>
<location>London.</location>
<contexts>
<context position="715" citStr="Grefenstette (1999)" startWordPosition="119" endWordPosition="120">ussex The World Wide Web is enormous, free, immediately available, and largely linguistic. As we discover, on ever more fronts, that language analysis and generation benefit from big data, so it becomes appealing to use the Web as a data source. The question, then, is how. The low-entry-cost way to use the Web is via a commercial search engine. If the goal is to find frequencies or probabilities for some phenomenon of interest, we can use the hit count given in the search engine’s hits page to make an estimate. People have been doing this for some time now. Early work using hit counts include Grefenstette (1999), who identified likely translations for compositional phrases, and Turney (2001), who found synonyms; perhaps the most cited study is Keller and Lapata (2003), who established the validity of frequencies gathered in this way using experiments with human subjects. Leading recent work includes Nakov and Hearst (2005), who build models of noun compound bracketing. The initial-entry cost for this kind of research is zero. Given a computer and an Internet connection, you input the query and get a hit count. But if the work is to proceed beyond the anecdotal, a range of issues must be addressed. Fi</context>
</contexts>
<marker>Grefenstette, 1999</marker>
<rawString>Grefenstette, Gregory. 1999. The WWW as a resource for example-based MT tasks. In ASLIB Translating and the Computer Conference, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="874" citStr="Keller and Lapata (2003)" startWordPosition="140" endWordPosition="143"> generation benefit from big data, so it becomes appealing to use the Web as a data source. The question, then, is how. The low-entry-cost way to use the Web is via a commercial search engine. If the goal is to find frequencies or probabilities for some phenomenon of interest, we can use the hit count given in the search engine’s hits page to make an estimate. People have been doing this for some time now. Early work using hit counts include Grefenstette (1999), who identified likely translations for compositional phrases, and Turney (2001), who found synonyms; perhaps the most cited study is Keller and Lapata (2003), who established the validity of frequencies gathered in this way using experiments with human subjects. Leading recent work includes Nakov and Hearst (2005), who build models of noun compound bracketing. The initial-entry cost for this kind of research is zero. Given a computer and an Internet connection, you input the query and get a hit count. But if the work is to proceed beyond the anecdotal, a range of issues must be addressed. First, the commercial search engines do not lemmatize or part-of-speech tag. To take a simple case: To estimate frequencies for the verb-object pair fulfil oblig</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Keller, Frank and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>17--24</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1032" citStr="Nakov and Hearst (2005)" startWordPosition="163" endWordPosition="166">s via a commercial search engine. If the goal is to find frequencies or probabilities for some phenomenon of interest, we can use the hit count given in the search engine’s hits page to make an estimate. People have been doing this for some time now. Early work using hit counts include Grefenstette (1999), who identified likely translations for compositional phrases, and Turney (2001), who found synonyms; perhaps the most cited study is Keller and Lapata (2003), who established the validity of frequencies gathered in this way using experiments with human subjects. Leading recent work includes Nakov and Hearst (2005), who build models of noun compound bracketing. The initial-entry cost for this kind of research is zero. Given a computer and an Internet connection, you input the query and get a hit count. But if the work is to proceed beyond the anecdotal, a range of issues must be addressed. First, the commercial search engines do not lemmatize or part-of-speech tag. To take a simple case: To estimate frequencies for the verb-object pair fulfil obligation, Keller and Lapata make 36 queries (to cover the whole inflectional paradigm of both verb and noun and to allow for definite and indefinite articles to </context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Nakov, Preslav and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 17–24, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and NLP: Using locality sensitive hash functions for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Ann Arbor, Michigan.</location>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Ravichandran, Deepak, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and NLP: Using locality sensitive hash functions for high speed noun clustering. In Proceedings of ACL, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sydney.</location>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Snow, Rion, Daniel Jurafsky, and Andrew Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of ACL, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="796" citStr="Turney (2001)" startWordPosition="129" endWordPosition="130">ic. As we discover, on ever more fronts, that language analysis and generation benefit from big data, so it becomes appealing to use the Web as a data source. The question, then, is how. The low-entry-cost way to use the Web is via a commercial search engine. If the goal is to find frequencies or probabilities for some phenomenon of interest, we can use the hit count given in the search engine’s hits page to make an estimate. People have been doing this for some time now. Early work using hit counts include Grefenstette (1999), who identified likely translations for compositional phrases, and Turney (2001), who found synonyms; perhaps the most cited study is Keller and Lapata (2003), who established the validity of frequencies gathered in this way using experiments with human subjects. Leading recent work includes Nakov and Hearst (2005), who build models of noun compound bracketing. The initial-entry cost for this kind of research is zero. Given a computer and an Internet connection, you input the query and get a hit count. But if the work is to proceed beyond the anecdotal, a range of issues must be addressed. First, the commercial search engines do not lemmatize or part-of-speech tag. To tak</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Turney, Peter D. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In European Conference on Machine Learning, pages 491–502.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>