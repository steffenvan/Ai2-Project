<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002880">
<title confidence="0.989273">
Sentence Dependency Tagging in Online Question Answering Forums
</title>
<author confidence="0.996759">
Zhonghua Qu and Yang Liu
</author>
<affiliation confidence="0.993532">
The University of Texas at Dallas
</affiliation>
<email confidence="0.99667">
{qzh,yangl@hlt.utdallas.edu}
</email>
<sectionHeader confidence="0.995593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999708434782609">
Online forums are becoming a popular re-
source in the state of the art question answer-
ing (QA) systems. Because of its nature as an
online community, it contains more updated
knowledge than other places. However, go-
ing through tedious and redundant posts to
look for answers could be very time consum-
ing. Most prior work focused on extracting
only question answering sentences from user
conversations. In this paper, we introduce the
task of sentence dependency tagging. Finding
dependency structure can not only help find
answer quickly but also allow users to trace
back how the answer is concluded through
user conversations. We use linear-chain con-
ditional random fields (CRF) for sentence type
tagging, and a 2D CRF to label the depen-
dency relation between sentences. Our ex-
perimental results show that our proposed ap-
proach performs well for sentence dependency
tagging. This dependency information can
benefit other tasks such as thread ranking and
answer summarization in online forums.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999308209302326">
Automatic Question Answering (QA) systems rely
heavily on good sources of data that contain ques-
tions and answers. Question answering forums, such
as technical support forums, are places where users
find answers through conversations. Because of
their nature as online communities, question answer-
ing forums provide more updated answers to new
problems. For example, when the latest release of
Linux has a bug, we can expect to find solutions
in forums first. However, unlike other structured
knowledge bases, often it is not straightforward to
extract information such as questions and answers in
online forums because such information spreads in
the conversations among multiple users in a thread.
A lot of previous work has focused on extract-
ing the question and answer sentences from forum
threads. However, there is much richer information
in forum conversations, and simply knowing a sen-
tence is a question or answer is not enough. For
example, in technical support forums, often it takes
several iterations of asking and clarifications to de-
scribe the question. The same happens to answers.
Usually several candidate answers are provided, and
not all answers are useful. In this case users’ feed-
back is needed to judge the correctness of answers.
Figure 1 shows an example thread in a technical
support forum. Each sentence is labeled with its type
(a detailed description of sentence types is provided
Table 1). We can see from the example that ques-
tions and answers are not expressed in a single sen-
tence or a single post. Only identifying question and
answering sentences from the thread is not enough
for automatic question answering. For this example,
in order to get the complete question, we would need
to know that sentence S3 is a question that inquires
for more details about the problem asked earlier, in-
stead of stating its own question. Also, sentence S5
should not be included in the correct answer since
it is not a working solution, which is indicated by a
negative feedback in sentence S6. The correct solu-
tion should be sentence S7, because of a user’s posi-
tive confirmation S9. We define that there is a depen-
dency between a pair of sentences if one sentence
</bodyText>
<page confidence="0.97841">
554
</page>
<note confidence="0.987781">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 554–562,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.960505444444445">
S1: M-GRET
S2: P-STAT
A: [S1:M-GRET] Hi everyone. [S2:P-STAT] I
have recently purchased USB flash and I am having
trouble renaming it, please help me.
B: [S3:A-INQU] What is the size and brand of this
flash?
A: [S4:Q-CLRF] It is a 4GB SanDisk flash.
B: [S5:A-SOLU] Install gparted, select flash drive
and rename.
A: [S6:M-NEGA] I got to the Right click on
partition and the label option was there but grayed
out.
B: [S7:A-SOLU] Sorry again, I meant to right click
the partition and select Unmount and then select
Change name while in gparted.
A: [S8:C-GRAT] Thank you so much. [S9:M-
POST] I now have an “Epic USB” You Rock!
</figure>
<figureCaption confidence="0.9171415">
Figure 1: Example of a Question Answering Thread in
Ubuntu Support Forum
</figureCaption>
<bodyText confidence="0.999919333333333">
exists as a result of another sentence. For example,
question context sentences exist because of the ques-
tion itself; an answering sentence exists because of
a question; or a feedback sentence exists because of
an answer. The sentence dependency structure of
this example dialog is shown in Figure 2.
</bodyText>
<equation confidence="0.9398985">
S3: A-INQU
S5:A-SOLU S6:M-NEGA S7:A-SOLU
S8:C-GRAT
S9:M-POST
</equation>
<figureCaption confidence="0.990103">
Figure 2: Dependency Structure of the Above Example
</figureCaption>
<bodyText confidence="0.999890704545455">
This example shows that in order to extract in-
formation from QA forums accurately, we need to
understand the sentence dependency structure of a
QA thread. Towards this goal, in this paper, we de-
fine two tasks: labeling the types for sentences, and
finding the dependency relations between sentences.
For the first task of sentence type labeling, we de-
fine a rich set of categories representing the purpose
of the sentences. We use linear-chain conditional
random fields (CRF) to take advantage of many
long-distance and non-local features. The second
task is to identify relations between sentences. Most
previous work only focused on finding the answer-
question relationship between sentences. However,
other relations can also be useful for information ex-
traction from online threads, such as user’s feed-
backs on the answers, problem detail inquiry and
question clarifications. In this study, we use two
approaches for labeling of dependency relation be-
tween sentences. First each sentence is considered
as a source, and we run a linear-chain CRF to la-
bel whether each of the other sentences is its tar-
get. Because multiple runs of separate linear-chain
CRFs ignore the dependency between source sen-
tences, the second approach we propose is to use a
2D CRF that models all pair relationships jointly.
The data we used was collected from Ubuntu
forum general help section. Our experimental re-
sults show that our proposed sentence type tagging
method works very well, even for the minority cate-
gories, and that using 2D CRF further improves per-
formance over linear-chain CRFs for identifying de-
pendency relation between sentences.
The paper is organized as follows. In the follow-
ing section, we discuss related work on finding ques-
tions and answers in online environment as well as
some dialog act tagging techniques. In Section 3, we
introduce the use of CRFs for sentence type and de-
pendency tagging. Section 4 describes data collec-
tion, annotation, and some analysis. In Section 5, we
show that our approach achieves promising results
in thread sentence dependency tagging. Finally we
conclude the paper and suggest some possible future
extensions.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9417035">
There is a lot of useful knowledge in the user gener-
ated content such as forums. This knowledge source
could substantially help automatic question answer-
ing systems. There has been some previous work
focusing on the extraction of question and corre-
sponding answer pairs in online forums. In (Ding
et al., 2008), a two-pass approach was used to find
relevant solutions for a given question, and a skip-
chain CRF was adopted to model long range de-
S4:Q-CLRF
</bodyText>
<page confidence="0.992705">
555
</page>
<bodyText confidence="0.999983828571429">
pendency between sentences. A graph propagation
method was used in (Cong et al., 2008) to rank
relevant answers to questions. An approach using
email structure to detect and summarize question an-
swer pairs was introduced in (Shrestha and Mck-
eown, 2004). These studies focused primarily on
finding questions and answers in an online envi-
ronment. In this paper, in order to provide a bet-
ter foundation for question answer detection in on-
line forums, we investigate tagging sentences with a
much richer set of categories, as well as identifying
their dependency relationships. The sentence types
we use are similar to dialog acts (DA), but defined
specifically for question answering forums. Work of
(Clark and Popescu-Belis, 2004) defined a reusable
multi-level tagset that can be mapped from conversa-
tional speech corpora such as the ICSI meeting data.
However, it is hard to reuse any available corpus or
DA tagset because our task is different, and also on-
line forum has a different style from speech data.
Automatic DA tagging has been studied a lot previ-
ously. For example, in (Stolcke et al., 2000), Hidden
Markov Models (HMMs) were used for DA tagging;
in (Ji and Bilmes, 2005), different types of graphical
models were explored.
Our study is different in several aspects: we are
using forum domains, unlike most work of DA tag-
ging on conversational speech; we use CRFs for sen-
tence type tagging; and more importantly, we also
propose to use different CRFs for sentence relation
detection. Unlike the pair-wise sentence analysis
proposed in (Boyer et al., 2009) in which HMM
was used to model the dialog structure, our model is
more flexible and does not require related sentences
to be adjacent.
</bodyText>
<sectionHeader confidence="0.950281" genericHeader="method">
3 Thread Structure Tagging
</sectionHeader>
<bodyText confidence="0.999868">
As described earlier, we decompose the structure
analysis of QA threads into two tasks, first deter-
mine the sentence type, and then identify related
sentences. This section provides details for each
task.
</bodyText>
<subsectionHeader confidence="0.999553">
3.1 Sentence Type Tagging
</subsectionHeader>
<bodyText confidence="0.9996034">
In human conversations, especially speech conver-
sations, DAs have been used to represent the pur-
pose or intention of a sentence. Different sets of
DAs have been adopted in various studies, ranging
from very coarse categories to fine grained ones. In
this study, we define 13 fine grained sentence types
(corresponding to 4 coarse categories) tailored to our
domain of QA forum threads. Table 1 shows the cat-
egories and their description. Some tags such as P-
STAT and A-SOLU are more important in that users
try to state a problem and provide solutions accord-
ingly. These are the typical ones used in previous
work on question answering. Our set includes other
useful tags. For example, C-NEGA and C-POSI can
evaluate how good an answer is. Even though C-
GRAT does not provide any direct feedback on the
solutions, existence of such a tag often strongly im-
plies a positive feedback to an answer. These sen-
tence types can be grouped into 4 coarse categories,
as shown in Table 1.
</bodyText>
<table confidence="0.998436785714286">
Types Category Description
Problems P-STAT question of problem
P-CONT problem context
P-CLRF problem clarification
Answers A-SOLU solution sentence
A-EXPL explanation on solutions
A-INQU inquire problem details
Confirm. C-GRAT gratitude
C-NEGA negative feedback
C-POSI positive feedback
Misc. M-QCOM question comment
M-ACOM comment on the answer
M-GRET greeting and politeness
M-OFF off-topic sentences
</table>
<tableCaption confidence="0.999702">
Table 1: Sentence Types for QA Threads
</tableCaption>
<bodyText confidence="0.999944692307692">
To automatically label sentences in a thread with
their types, we adopt a sequence labeling approach,
specifically linear-chain conditional random fields
(CRFs), which have shown good performance in
many other tasks (Lafferty, 2001). Intuitively there
is a strong dependency between adjacent sentences.
For example, in our data set, 45% sentences follow-
ing a greeting sentence (M-GRET) are question re-
lated sentences; 53% sentences following a question
inquiry sentence (Q-INQ) are solution related sen-
tences. The following describes our modeling ap-
proaches and features used for sentence type tag-
ging.
</bodyText>
<page confidence="0.990537">
556
</page>
<bodyText confidence="0.917718333333333">
where feature templates fek and fvk correspond to
edge features and node features respectively.
Feature Description
</bodyText>
<subsubsectionHeader confidence="0.491287">
3.1.1 Linear-chain Conditional Random Field
</subsubsectionHeader>
<bodyText confidence="0.943133333333333">
Linear-chain CRFs is a type of undirected graphi-
cal models. Distribution of a set of variables in undi-
rected graphical models can be written as
</bodyText>
<equation confidence="0.917222666666667">
1 Y
p(x, y) = Z
A
</equation>
<bodyText confidence="0.99417575">
Z is the normalization constant to guarantee valid
probability distributions. CRFs is a special case
of undirected graphical model in which ψ are log-
linear functions:
</bodyText>
<equation confidence="0.994185333333333">
(X )
ψA(xA, yA) = exp θAkfAk(xA, yA) (2)
k
</equation>
<bodyText confidence="0.998323166666667">
θA is a real value parameter vector for feature
function set fA. In the sequence labeling task, fea-
ture functions across the sequence are often tied to-
gether. In other words, feature functions at different
locations of the sequence share the same parameter
vector θ.
</bodyText>
<figureCaption confidence="0.992848">
Figure 3: Graphical Structure of Linear-chain CRFs.
</figureCaption>
<bodyText confidence="0.999240285714286">
Linear-chain CRF is a special case of the general
CRFs. In linear-chain CRF, cliques only involve two
adjacent variables in the sequence. Figure 3 shows
the graphical structure of a linear-chain CRF. In our
case of sentence tagging, cliques only contain two
adjacent sentences. Given the observation x, the
probability of label sequence y is as follows:
</bodyText>
<equation confidence="0.987729090909091">
Y |y|
1
p(y|x) = Z
i=1
(X )
ψe(x, y, i) = exp θekfek(yi−1, yi, x, i)
k
(4)
(X )
ψv(x, y, j) = exp θvkfvk(yj, x, j) (5)
k
</equation>
<table confidence="0.938592466666667">
Cosine similarity with previous sentence.
Quote segment within two adjacent sentences?
Code segment within two adjacent sentences?
Does this sentence belong to author’s post?
Is it the first sentence in a post?
Post author participated thread before?
Does the sentence contain any negative words?
Does the sentence contain any URL?
Does the sentence contain any positive words?
Does the sentence contain any question mark?
Length of the sentence.
Presence of verb.
Presence of adjective.
Sentence perplexity based on a background LM.
Bag of word features.
</table>
<tableCaption confidence="0.968121">
Table 2: Features Used in Sentence Type Tagging.
</tableCaption>
<subsectionHeader confidence="0.694784">
3.1.2 Sentence Type Tagging Features
</subsectionHeader>
<bodyText confidence="0.999989615384615">
We used various types of feature functions in sen-
tence type tagging. Table 2 shows the complete list
of features we used. Edge features are closely re-
lated to the transition between sentences. Here we
use the cosine similarity between sentences, where
each sentence is represented as a vector of words,
with term weight calculated using TD-IDF (term fre-
quency times inverse document frequency). High
similarity between adjacent sentences suggests sim-
ilar or related types. For node features, we explore
different sources of information about the sentence.
For example, the presence of a question mark indi-
cates that a sentence may be a question or inquiry.
Similarly, we include other cues, such as positive
or negative words, verb and adjective words. Since
technical forums tend to contain many system out-
puts, we include the perplexity of the sentence as a
feature which is calculated based on a background
language model (LM) learned from common En-
glish documents. We also use bag-of-word features
as in many other text categorization tasks.
Furthermore, we add features to represent post
level information to account for the structure of QA
threads, for example, whether or not a sentence be-
longs to the author’s post, or if a sentence is the be-
ginning sentence of a post.
</bodyText>
<equation confidence="0.99860925">
ψA(xA, yA) (1)
|y|
ψe(x, y, i) Y ψv(x,y,j) (3)
j=0
</equation>
<page confidence="0.98977">
557
</page>
<subsectionHeader confidence="0.997909">
3.2 Sentence Dependency Tagging
</subsectionHeader>
<bodyText confidence="0.999795849056604">
Knowing only the sentence types without their de-
pendency relations is not enough for question an-
swering tasks. For example, correct labeling of an
answer without knowing which question it actually
refers to is problematic; not knowing which answer
a positive or negative feedback refers to will not be
helpful at all. In this section we describe how sen-
tence dependency information is determined. Note
that sentence dependency relations might not be a
one-to-one relation. A many-to-many relation is also
possible. Take question answer relation as an ex-
ample. There could be potentially many answers
spreading in many sentences, all depending on the
same question. Also, it is very likely that a question
is expressed in multiple sentences too.
Dependency relationship could happen between
many different types of sentences, for example, an-
swer(s) to question(s), problem clarification to ques-
tion inquiry, feedback to solutions, etc. Instead of
developing models for each dependency type, we
treat them uniformly as dependency relations be-
tween sentences. Hence, for every two sentences,
it becomes a binary classification problem, i.e.,
whether or not there exists a dependency relation
between them. For a pair of sentences, we call the
depending sentence the source sentence, and the de-
pended sentence the target sentence. As described
earlier, one source sentence can potentially depend
on many different target sentences, and one target
sentence can also correspond to multiple sources.
The sentence dependency task is formally defined
as, given a set of sentences St of a thread, find the
dependency relation {(s, t)|s E St, t E St}, where s
is the source sentence and t is the target sentence that
s depends on.
We propose two methods to find the dependency
relationship. In the first approach, for each source
sentence, we run a labeling procedure to find the de-
pendent sentences. From the data, we found given a
source sentence, there is strong dependency between
adjacent target sentences. If one sentence is a tar-
get sentence of the source, often the next sentence
is a target sentence too. In order to take advantage
of such adjacent sentence dependency, we use the
linear-chain CRFs for the sequence labeling. Fea-
tures used in sentence dependency labeling are listed
in Table 3. Note that a lot of the node features used
here are relative to the source sentence since the task
here is to determine if the two sentences are related.
For a thread of N sentences, we need to perform N
runs of CRF labeling, one for each sentence (as the
source sentence) in order to label the target sentence
corresponding to this source sentence.
</bodyText>
<subsectionHeader confidence="0.625036">
Feature Description
</subsectionHeader>
<table confidence="0.989705666666667">
* Cosine similarity with previous sentence.
* Is adjacent sentence of the same type?
* Pair of types of the adjacent target sentences.
Pair of types of the source and target sentence.
Is target in the same post as the source?
Do target and source belong to the same author?
Cosine similarity between target and source sentence.
Does target sentence happen before source?
Post distance between source and target sentence.
</table>
<tableCaption confidence="0.979242">
* indicates an edge feature
Table 3: Features Used in Sentence Dependency Labeling
</tableCaption>
<bodyText confidence="0.9999575">
The linear-chain CRFs can represent the depen-
dency between adjacent target sentences quite well.
However they cannot model the dependency be-
tween adjacent source sentences, because labeling
is done for each source sentence individually. To
model the dependency between both the source sen-
tences and the target sentences, we propose to use
2D CRFs for sentence relation labeling. 2D CRFs
are used in many applications considering two di-
mension dependencies such as object recognitions
(Quattoni et al., 2004) and web information extrac-
tion (Zhu et al., 2005). The graphical structure of
a 2D CRF is shown in Figure 4. Unlike one di-
mensional sequence labeling, a node in 2D environ-
ment is dependent on both x-axis neighbors and y-
axis neighbors. In the sentence relation task, the
source and target pair is a 2D relation in which its
label depends on labels of both its adjacent source
and its adjacent target sentence. As shown in Fig-
ure 4, looking from x-axis is the sequence of target
sentences with a fixed source sentence, and from y-
axis is the sequence of source sentences with a fixed
target sentence. This model allows us to model all
the sentence relationships jointly. 2D CRFs contain
3 templates of features: node template, x-axis edge
template, and y-axis edge template. We use the same
edge features and node features as in linear-chain
CRFs for node features and y-axis edge features in
</bodyText>
<page confidence="0.94923">
558
</page>
<equation confidence="0.7268705">
2D CRFs. For the x-axis edge features, we use the
X
</equation>
<bodyText confidence="0.9872">
same feature functions as for y-axis, except that now
they represent the relation between adjacent source
sentences.
</bodyText>
<figureCaption confidence="0.996144">
Figure 4: Graphical Structure of 2D CRF for Sentence
Relation Labeling.
</figureCaption>
<bodyText confidence="0.9996953">
In a thread containing N sentences, we would
have a 2D CRF containing N2 nodes in a N x N
grid. Exact inference in such a graph is intractable.
In this paper we use loopy belief propagation algo-
rithm for the inference. Loopy belief propagation is
a message passing algorithm for graph inference. It
calculates the marginal distribution for each node in
the graph. The result is exact in some graph struc-
tures (e.g., linear-chain CRFs), and often converges
to a good approximation for general graphs.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999982130434783">
We used data from ubuntu community forum gen-
eral help section for the experiments and evalua-
tion. This is a technical support section that provides
answers to the latest problems in Ubuntu Linux.
Among all the threads that we have crawled, we se-
lected 200 threads for this initial study. They con-
tain between 2 −10 posts and at least 2 participants.
Sentences inside each thread are segmented using
Apache OpenNLP tools. In total, there are 706 posts
and 3,483 sentences. On average, each thread con-
tains 3.53 posts, and each post contains around 4.93
sentences. Two annotators were recruited to anno-
tate the sentence type and the dependency relation
between sentences. Annotators are both computer
science department undergraduate students. They
are provided with detailed explanation of the anno-
tation standard. The distribution of sentence types
in the annotated data is shown in Table 4, along with
inter-annotator Kappa statistics calculated using 20
common threads annotated by both annotators. We
can see that the majority of the sentences are about
problem descriptions and solutions. In general, the
agreement between the two annotators is quite good.
</bodyText>
<table confidence="0.999800214285714">
General Type Category Percentage Kappa
P-STAT 12.37 0.88
Problems P-CONT 37.30 0.77
P-CLRF 1.01 0.98
A-SOLU 9.94 0.89
Answers A-EXPL 11.60 0.89
A-INQU 1.38 0.99
C-GRAT 5.06 0.98
Confirmation C-NEGA 1.98 0.96
C-POSI 1.84 0.96
M-QCOM 1.98 0.93
Miscellaneous M-ACOM 1.47 0.96
M-GRET 1.01 0.96
M-OFF 7.92 0.96
</table>
<tableCaption confidence="0.976338">
Table 4: Distribution and Inter-annotator Agreement of
Sentence Types in Data
</tableCaption>
<bodyText confidence="0.999983619047619">
There are in total 1, 751 dependency relations
identified by the annotators among those tagged sen-
tences. Note that we are only dealing with intra-
thread sentence dependency, that is, no dependency
among sentences in different threads is labeled.
Considering all the possible sentence pairs in each
thread, the labeled dependency relations represent a
small percentage. The most common dependency
is problem description to problem question. This
shows that users tend to provide many details of
the problem. This is especially true in technical fo-
rums. Seeing questions without their context would
be confusing and hard to solve. The relation of an-
swering solutions and question dependency is also
very common, as expected. The third common re-
lation is the feedback dependency. Even though the
number of feedback sentences is small in the data
set, it plays a vital role to determine the quality of
answers. The main reason for the small number is
that, unlike problem descriptions, much fewer sen-
tences are needed to give feedbacks.
</bodyText>
<sectionHeader confidence="0.997046" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<bodyText confidence="0.99770925">
In the experiment, we randomly split annotated
threads into three disjoint sets, and run a three-fold
cross validation. Within each fold, first sentence
types are labeled using linear-chain CRFs, then the
</bodyText>
<figure confidence="0.899909833333333">
Target
Source
y00 y10 ...
y01 y11 ...
X
... ... ...
</figure>
<page confidence="0.996071">
559
</page>
<bodyText confidence="0.999966466666667">
resulting sentence type tagging is used in the sec-
ond pass to determine dependency relations. For
part-of-speech (POS) tagging of the sentences, we
used Stanford POS Tagger (Toutanova and Man-
ning, 2000). All the graphical inference and estima-
tions are done using MALLET package (McCallum,
2002).
In this paper, we evaluate the results using stan-
dard precision and recall. In the sentence type tag-
ging task, we calculate precision, recall, and Fl
score for each individual tag. For the dependency
tagging task, a pair identified by the system is cor-
rect only if the exact pair appears in the reference an-
notation. Precision and recall scores are calculated
accordingly.
</bodyText>
<subsectionHeader confidence="0.997285">
5.1 Sentence Type Tagging Results
</subsectionHeader>
<bodyText confidence="0.999938096774193">
The results of sentence type tagging using linear-
chain CRFs are shown in Table 5. For a comparison,
we include results using a basic first-order HMM
model. Because HMM is a generative model, we
use only bag of word features in the generative pro-
cess. The observation probability is the probabil-
ity of the sentence generated by a unigram language
model, trained for different sentence types. Since
for some applications, fine grained categories may
not be needed, for example, in the case of finding
questions and answers in a thread, we also include
in the table the tagging results when only the gen-
eral categories are used in both training and testing.
We can see from the table that using CRFs
achieves significantly better performance than
HMMs for most categories, except greeting and off-
topic types. This is mainly because of the advantage
of CRFs, allowing the incorporation of rich discrimi-
native features. For the two major types of problems
and answers, in general, our system shows very good
performance. Even for minority types like feed-
backs, it also performs reasonably well. When using
coarse types, the performance on average is better
compared to the finer grained categories, mainly be-
cause of the fewer classes in the classification task.
Using the fine grained categories, we found that the
system is able to tell the difference between “prob-
lem statement” (P-STAT) and “problem context” (P-
CONT). Note that in our task, a problem statement is
not necessarily a question sentence. Instead it could
be any sentence that expresses the need for a solu-
</bodyText>
<table confidence="0.989944909090909">
Linear-chain CRF First-order HMM
13 Fine Grained Types
Tag Prec. / Rec. Fl
M-GRET 0.45 / 0.58 0.51
P-STAT 0.79 / 0.72 0.75
P-CONT 0.80 / 0.74 0.77
A-INQU 0.37 / 0.48 0.42
A-SOLU 0.78 / 0.64 0.71
A-EXPL 0.4 / 0.76 0.53
M-POST 0.5 / 0.41 0.45
C-GRAT 0.43 / 0.53 0.48
M-NEGA 0.67 / 0.5 0.57
M-OFF 0.11 / 0.23 0.15
P-CLRF 0.15 / 0.33 0.21
M-ACOM 0.27 / 0.38 0.32
M-QCOM 0.34 / 0.32 0.33
4 General Types
Tag Prec. / Rec. Fl Prec. / Rec. Fl
Problem 0.85 / 0.76 0.80 0.73 / 0.27 0.39
Answers 0.65 / 0.72 0.68 0.45 / 0.36 0.40
Confirm. 0.80 / 0.74 0.77 0.06 / 0.26 0.10
Misc. 0.43 / 0.61 0.51 0.04 / 0.36 0.08
</table>
<tableCaption confidence="0.986557">
Table 5: Sentence Type Tagging Performance Using
CRFs and HMM.
</tableCaption>
<bodyText confidence="0.998988583333333">
tion.
We also performed some analysis of the features
using the feature weights in the trained CRF mod-
els. We find that some post level information is rela-
tively important. For example, the feature represent-
ing whether the sentence is before a “code” segment
has a high weight for problem description classifica-
tion. This is because in linux support forum, people
usually put some machine output after their problem
description. We also notice that the weights for verb
words are usually high. This is intuitive since the
“verb” of a sentence can often determine its purpose.
</bodyText>
<subsectionHeader confidence="0.999002">
5.2 Sentence Dependency Tagging Results
</subsectionHeader>
<bodyText confidence="0.999958727272727">
Table 6 shows the results using linear-chain CRFs
(L-CRF) and 2D CRFs for sentence dependency tag-
ging. We use different settings in our experiments.
For the categories of sentence types, we evaluate us-
ing both the fine grained (13 types) and the coarse
categories (4 types). Furthermore, we examine two
ways to obtain the sentence types. First, we use the
output from automatic sentence type tagging. In the
second one, we use the sentence type information
from the human annotated data in order to avoid the
error propagation from automatic sentence type la-
</bodyText>
<figure confidence="0.996592714285714">
Prec. / Rec. Fl
0.73 / 0.57 0.64
0.35 / 0.34 0.35
0.58 / 0.18 0.27
0.11 / 0.25 0.15
0.27 / 0.29 0.28
0.24 / 0.19 0.21
0.04 / 0.1 0.05
0.01 / 0.25 0.02
0.09 / 0.31 0.14
0.20 / 0.23 0.21
0.10 / 0.12 0.11
0.09 / 0.1 0.09
0.08 / 0.23 0.11
</figure>
<page confidence="0.972101">
560
</page>
<bodyText confidence="0.643295">
beling. This gives an oracle upper bound for the
second pass performance.
</bodyText>
<table confidence="0.997750833333333">
Using Oracle Sentence Type
Setup Precision Recall Fl
0.973 0.453 0.618
0.985 0.532 0.691
0.941 0.124 0.218
0.956 0.145 0.252
Using System Sentence Type
Setup Precision Recall Fl
L-CRF 0.943 0.362 0.523
13 types 2D-CRF 0.973 0.394 0.561
4 general L-CRF 0.939 0.101 0.182
2D-CRF 0.942 0.127 0.223
</table>
<tableCaption confidence="0.998696">
Table 6: Sentence Dependency Tagging Performance
</tableCaption>
<bodyText confidence="0.99998146875">
From the results we can see that 2D CRFs out-
perform linear-chain CRFs for all the conditions.
This shows that by modeling the 2D dependency in
source and target sentences, system performance is
improved. For the sentence types, when using auto-
matic sentence type tagging systems, there is a per-
formance drop. The performance gap between us-
ing the reference and automatic sentence types sug-
gests that there is still room for improvement from
better sentence type tagging. Regarding the cate-
gories used for the sentence types, we observe that
they have an impact on dependence tagging perfor-
mance. When using general categories, the perfor-
mance is far behind that using the fine grained types.
This is because some important information is lost
when grouping categories. For example, a depen-
dency relation can be: “A-EXPL” (explanation for
solutions) depends on “A-SOLU” (solutions); how-
ever, when using coarse categories, both are mapped
to “Solution”, and having one “Solution” depending
on another “Solution” is not very intuitive and hard
to model properly. This shows that detailed cate-
gory information is very important for dependency
tagging even though the tagging accuracy from the
first pass is far from perfect.
Currently our system does not put constraints on
the sentence types for which dependencies exist. In
the system output we find that sometimes there are
obvious dependency errors, such as a positive feed-
back depending on a negative feedback. We may
improve our models by taking into account different
sentence types and dependency relations.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999980952380953">
In this paper, we investigated sentence dependency
tagging of question and answer (QA) threads in on-
line forums. We define the thread tagging task as a
two-step process. In the first step, sentence types
are labeled. We defined 13 sentence types in or-
der to capture rich information of sentences to bene-
fit question answering systems. Linear chain CRF
is used for sentence type tagging. In the second
step, we label actual dependency between sentences.
First, we propose to use a linear-chain CRF to label
possible target sentences for each source sentence.
Then we improve the model to consider the depen-
dency between sentences along two dimensions us-
ing a 2D CRF. Our experiments show promising
performance in both tasks. This provides a good
pre-processing step towards automatic question an-
swering. In the future, we plan to explore using
constrained CRF for more accurate dependency tag-
ging. We will also use the result from this work in
other tasks such as answer quality ranking and an-
swer summarization.
</bodyText>
<sectionHeader confidence="0.998247" genericHeader="acknowledgments">
7 Acknowledgment
</sectionHeader>
<bodyText confidence="0.9996484">
This work is supported by DARPA under Contract
No. HR0011-12-C-0016 and NSF No. 0845484.
Any opinions expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA or NSF.
</bodyText>
<sectionHeader confidence="0.999235" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999809705882353">
Kristy Elizabeth Boyer, Robert Phillips, Eun Young Ha,
Michael D. Wallis, Mladen A. Vouk, and James C.
Lester. 2009. Modeling dialogue structure with ad-
jacency pair analysis and hidden markov models. In
Proc. NAACL-Short, pages 49–52.
Alexander Clark and Andrei Popescu-Belis. 2004.
Multi-level dialogue act tags. In Proc. SIGDIAL,
pages 163–170.
Gao Cong, Long Wang, Chinyew Lin, Youngin Song, and
Yueheng Sun. 2008. Finding question-answer pairs
from online forums. In Proc. SIGIR, pages 467–474.
Shilin Ding, Gao Cong, Chinyew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
texts and answers of questions from online forums. In
Proc. ACL-HLT.
Gang Ji and J Bilmes. 2005. Dialog Act Tagging Using
Graphical Models. In Proc. ICASSP.
</reference>
<figure confidence="0.9978875">
13 types L-CRF
2D-CRF
4 general L-CRF
2D-CRF
</figure>
<page confidence="0.972912">
561
</page>
<reference confidence="0.999927576923077">
John Lafferty. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proc. ICML, pages 282–289.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2004. Conditional random fields for object recogni-
tion. In Proc. NIPS, pages 1097–1104.
Lokesh Shrestha and Kathleen Mckeown. 2004. Detec-
tion of question-answer pairs in email conversations.
In Proc. Coling, pages 889–895.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26:339–373.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proc. EMNLP/VLC,
pages 63–70.
Jun Zhu, Zaiqing Nie, Ji R. Wen, Bo Zhang, and Wei Y.
Ma. 2005. 2D Conditional Random Fields for Web
information extraction. In Proc. ICML, pages 1044–
1051, New York, NY, USA.
</reference>
<page confidence="0.997348">
562
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.913547">
<title confidence="0.999828">Sentence Dependency Tagging in Online Question Answering Forums</title>
<author confidence="0.966658">Qu</author>
<affiliation confidence="0.936317">The University of Texas at</affiliation>
<abstract confidence="0.999409625">Online forums are becoming a popular resource in the state of the art question answering (QA) systems. Because of its nature as an online community, it contains more updated knowledge than other places. However, going through tedious and redundant posts to look for answers could be very time consuming. Most prior work focused on extracting only question answering sentences from user conversations. In this paper, we introduce the task of sentence dependency tagging. Finding dependency structure can not only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations. We use linear-chain conditional random fields (CRF) for sentence type tagging, and a 2D CRF to label the dependency relation between sentences. Our experimental results show that our proposed approach performs well for sentence dependency tagging. This dependency information can benefit other tasks such as thread ranking and answer summarization in online forums.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kristy Elizabeth Boyer</author>
<author>Robert Phillips</author>
<author>Eun Young Ha</author>
<author>Michael D Wallis</author>
<author>Mladen A Vouk</author>
<author>James C Lester</author>
</authors>
<title>Modeling dialogue structure with adjacency pair analysis and hidden markov models.</title>
<date>2009</date>
<booktitle>In Proc. NAACL-Short,</booktitle>
<pages>49--52</pages>
<contexts>
<context position="8853" citStr="Boyer et al., 2009" startWordPosition="1442" endWordPosition="1445">nd also online forum has a different style from speech data. Automatic DA tagging has been studied a lot previously. For example, in (Stolcke et al., 2000), Hidden Markov Models (HMMs) were used for DA tagging; in (Ji and Bilmes, 2005), different types of graphical models were explored. Our study is different in several aspects: we are using forum domains, unlike most work of DA tagging on conversational speech; we use CRFs for sentence type tagging; and more importantly, we also propose to use different CRFs for sentence relation detection. Unlike the pair-wise sentence analysis proposed in (Boyer et al., 2009) in which HMM was used to model the dialog structure, our model is more flexible and does not require related sentences to be adjacent. 3 Thread Structure Tagging As described earlier, we decompose the structure analysis of QA threads into two tasks, first determine the sentence type, and then identify related sentences. This section provides details for each task. 3.1 Sentence Type Tagging In human conversations, especially speech conversations, DAs have been used to represent the purpose or intention of a sentence. Different sets of DAs have been adopted in various studies, ranging from very</context>
</contexts>
<marker>Boyer, Phillips, Ha, Wallis, Vouk, Lester, 2009</marker>
<rawString>Kristy Elizabeth Boyer, Robert Phillips, Eun Young Ha, Michael D. Wallis, Mladen A. Vouk, and James C. Lester. 2009. Modeling dialogue structure with adjacency pair analysis and hidden markov models. In Proc. NAACL-Short, pages 49–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Multi-level dialogue act tags.</title>
<date>2004</date>
<booktitle>In Proc. SIGDIAL,</booktitle>
<pages>163--170</pages>
<contexts>
<context position="8015" citStr="Clark and Popescu-Belis, 2004" startWordPosition="1300" endWordPosition="1303">evant answers to questions. An approach using email structure to detect and summarize question answer pairs was introduced in (Shrestha and Mckeown, 2004). These studies focused primarily on finding questions and answers in an online environment. In this paper, in order to provide a better foundation for question answer detection in online forums, we investigate tagging sentences with a much richer set of categories, as well as identifying their dependency relationships. The sentence types we use are similar to dialog acts (DA), but defined specifically for question answering forums. Work of (Clark and Popescu-Belis, 2004) defined a reusable multi-level tagset that can be mapped from conversational speech corpora such as the ICSI meeting data. However, it is hard to reuse any available corpus or DA tagset because our task is different, and also online forum has a different style from speech data. Automatic DA tagging has been studied a lot previously. For example, in (Stolcke et al., 2000), Hidden Markov Models (HMMs) were used for DA tagging; in (Ji and Bilmes, 2005), different types of graphical models were explored. Our study is different in several aspects: we are using forum domains, unlike most work of DA</context>
</contexts>
<marker>Clark, Popescu-Belis, 2004</marker>
<rawString>Alexander Clark and Andrei Popescu-Belis. 2004. Multi-level dialogue act tags. In Proc. SIGDIAL, pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gao Cong</author>
<author>Long Wang</author>
<author>Chinyew Lin</author>
<author>Youngin Song</author>
<author>Yueheng Sun</author>
</authors>
<title>Finding question-answer pairs from online forums.</title>
<date>2008</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>467--474</pages>
<contexts>
<context position="7373" citStr="Cong et al., 2008" startWordPosition="1198" endWordPosition="1201">ude the paper and suggest some possible future extensions. 2 Related Work There is a lot of useful knowledge in the user generated content such as forums. This knowledge source could substantially help automatic question answering systems. There has been some previous work focusing on the extraction of question and corresponding answer pairs in online forums. In (Ding et al., 2008), a two-pass approach was used to find relevant solutions for a given question, and a skipchain CRF was adopted to model long range deS4:Q-CLRF 555 pendency between sentences. A graph propagation method was used in (Cong et al., 2008) to rank relevant answers to questions. An approach using email structure to detect and summarize question answer pairs was introduced in (Shrestha and Mckeown, 2004). These studies focused primarily on finding questions and answers in an online environment. In this paper, in order to provide a better foundation for question answer detection in online forums, we investigate tagging sentences with a much richer set of categories, as well as identifying their dependency relationships. The sentence types we use are similar to dialog acts (DA), but defined specifically for question answering forum</context>
</contexts>
<marker>Cong, Wang, Lin, Song, Sun, 2008</marker>
<rawString>Gao Cong, Long Wang, Chinyew Lin, Youngin Song, and Yueheng Sun. 2008. Finding question-answer pairs from online forums. In Proc. SIGIR, pages 467–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shilin Ding</author>
<author>Gao Cong</author>
<author>Chinyew Lin</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Using conditional random fields to extract contexts and answers of questions from online forums.</title>
<date>2008</date>
<booktitle>In Proc. ACL-HLT.</booktitle>
<contexts>
<context position="7139" citStr="Ding et al., 2008" startWordPosition="1157" endWordPosition="1160">s for sentence type and dependency tagging. Section 4 describes data collection, annotation, and some analysis. In Section 5, we show that our approach achieves promising results in thread sentence dependency tagging. Finally we conclude the paper and suggest some possible future extensions. 2 Related Work There is a lot of useful knowledge in the user generated content such as forums. This knowledge source could substantially help automatic question answering systems. There has been some previous work focusing on the extraction of question and corresponding answer pairs in online forums. In (Ding et al., 2008), a two-pass approach was used to find relevant solutions for a given question, and a skipchain CRF was adopted to model long range deS4:Q-CLRF 555 pendency between sentences. A graph propagation method was used in (Cong et al., 2008) to rank relevant answers to questions. An approach using email structure to detect and summarize question answer pairs was introduced in (Shrestha and Mckeown, 2004). These studies focused primarily on finding questions and answers in an online environment. In this paper, in order to provide a better foundation for question answer detection in online forums, we i</context>
</contexts>
<marker>Ding, Cong, Lin, Zhu, 2008</marker>
<rawString>Shilin Ding, Gao Cong, Chinyew Lin, and Xiaoyan Zhu. 2008. Using conditional random fields to extract contexts and answers of questions from online forums. In Proc. ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gang Ji</author>
<author>J Bilmes</author>
</authors>
<title>Dialog Act Tagging Using Graphical Models. In</title>
<date>2005</date>
<booktitle>Proc. ICASSP.</booktitle>
<contexts>
<context position="8469" citStr="Ji and Bilmes, 2005" startWordPosition="1380" endWordPosition="1383">ationships. The sentence types we use are similar to dialog acts (DA), but defined specifically for question answering forums. Work of (Clark and Popescu-Belis, 2004) defined a reusable multi-level tagset that can be mapped from conversational speech corpora such as the ICSI meeting data. However, it is hard to reuse any available corpus or DA tagset because our task is different, and also online forum has a different style from speech data. Automatic DA tagging has been studied a lot previously. For example, in (Stolcke et al., 2000), Hidden Markov Models (HMMs) were used for DA tagging; in (Ji and Bilmes, 2005), different types of graphical models were explored. Our study is different in several aspects: we are using forum domains, unlike most work of DA tagging on conversational speech; we use CRFs for sentence type tagging; and more importantly, we also propose to use different CRFs for sentence relation detection. Unlike the pair-wise sentence analysis proposed in (Boyer et al., 2009) in which HMM was used to model the dialog structure, our model is more flexible and does not require related sentences to be adjacent. 3 Thread Structure Tagging As described earlier, we decompose the structure anal</context>
</contexts>
<marker>Ji, Bilmes, 2005</marker>
<rawString>Gang Ji and J Bilmes. 2005. Dialog Act Tagging Using Graphical Models. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="10897" citStr="Lafferty, 2001" startWordPosition="1769" endWordPosition="1770">lem P-CONT problem context P-CLRF problem clarification Answers A-SOLU solution sentence A-EXPL explanation on solutions A-INQU inquire problem details Confirm. C-GRAT gratitude C-NEGA negative feedback C-POSI positive feedback Misc. M-QCOM question comment M-ACOM comment on the answer M-GRET greeting and politeness M-OFF off-topic sentences Table 1: Sentence Types for QA Threads To automatically label sentences in a thread with their types, we adopt a sequence labeling approach, specifically linear-chain conditional random fields (CRFs), which have shown good performance in many other tasks (Lafferty, 2001). Intuitively there is a strong dependency between adjacent sentences. For example, in our data set, 45% sentences following a greeting sentence (M-GRET) are question related sentences; 53% sentences following a question inquiry sentence (Q-INQ) are solution related sentences. The following describes our modeling approaches and features used for sentence type tagging. 556 where feature templates fek and fvk correspond to edge features and node features respectively. Feature Description 3.1.1 Linear-chain Conditional Random Field Linear-chain CRFs is a type of undirected graphical models. Distr</context>
</contexts>
<marker>Lafferty, 2001</marker>
<rawString>John Lafferty. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="23067" citStr="McCallum, 2002" startWordPosition="3766" endWordPosition="3767"> fewer sentences are needed to give feedbacks. 5 Experiment In the experiment, we randomly split annotated threads into three disjoint sets, and run a three-fold cross validation. Within each fold, first sentence types are labeled using linear-chain CRFs, then the Target Source y00 y10 ... y01 y11 ... X ... ... ... 559 resulting sentence type tagging is used in the second pass to determine dependency relations. For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000). All the graphical inference and estimations are done using MALLET package (McCallum, 2002). In this paper, we evaluate the results using standard precision and recall. In the sentence type tagging task, we calculate precision, recall, and Fl score for each individual tag. For the dependency tagging task, a pair identified by the system is correct only if the exact pair appears in the reference annotation. Precision and recall scores are calculated accordingly. 5.1 Sentence Type Tagging Results The results of sentence type tagging using linearchain CRFs are shown in Table 5. For a comparison, we include results using a basic first-order HMM model. Because HMM is a generative model, </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Michael Collins</author>
<author>Trevor Darrell</author>
</authors>
<title>Conditional random fields for object recognition.</title>
<date>2004</date>
<booktitle>In Proc. NIPS,</booktitle>
<pages>1097--1104</pages>
<contexts>
<context position="18278" citStr="Quattoni et al., 2004" startWordPosition="2976" endWordPosition="2979">een source and target sentence. * indicates an edge feature Table 3: Features Used in Sentence Dependency Labeling The linear-chain CRFs can represent the dependency between adjacent target sentences quite well. However they cannot model the dependency between adjacent source sentences, because labeling is done for each source sentence individually. To model the dependency between both the source sentences and the target sentences, we propose to use 2D CRFs for sentence relation labeling. 2D CRFs are used in many applications considering two dimension dependencies such as object recognitions (Quattoni et al., 2004) and web information extraction (Zhu et al., 2005). The graphical structure of a 2D CRF is shown in Figure 4. Unlike one dimensional sequence labeling, a node in 2D environment is dependent on both x-axis neighbors and yaxis neighbors. In the sentence relation task, the source and target pair is a 2D relation in which its label depends on labels of both its adjacent source and its adjacent target sentence. As shown in Figure 4, looking from x-axis is the sequence of target sentences with a fixed source sentence, and from yaxis is the sequence of source sentences with a fixed target sentence. T</context>
</contexts>
<marker>Quattoni, Collins, Darrell, 2004</marker>
<rawString>Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2004. Conditional random fields for object recognition. In Proc. NIPS, pages 1097–1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lokesh Shrestha</author>
<author>Kathleen Mckeown</author>
</authors>
<title>Detection of question-answer pairs in email conversations.</title>
<date>2004</date>
<booktitle>In Proc. Coling,</booktitle>
<pages>889--895</pages>
<contexts>
<context position="7539" citStr="Shrestha and Mckeown, 2004" startWordPosition="1224" endWordPosition="1228">is knowledge source could substantially help automatic question answering systems. There has been some previous work focusing on the extraction of question and corresponding answer pairs in online forums. In (Ding et al., 2008), a two-pass approach was used to find relevant solutions for a given question, and a skipchain CRF was adopted to model long range deS4:Q-CLRF 555 pendency between sentences. A graph propagation method was used in (Cong et al., 2008) to rank relevant answers to questions. An approach using email structure to detect and summarize question answer pairs was introduced in (Shrestha and Mckeown, 2004). These studies focused primarily on finding questions and answers in an online environment. In this paper, in order to provide a better foundation for question answer detection in online forums, we investigate tagging sentences with a much richer set of categories, as well as identifying their dependency relationships. The sentence types we use are similar to dialog acts (DA), but defined specifically for question answering forums. Work of (Clark and Popescu-Belis, 2004) defined a reusable multi-level tagset that can be mapped from conversational speech corpora such as the ICSI meeting data. </context>
</contexts>
<marker>Shrestha, Mckeown, 2004</marker>
<rawString>Lokesh Shrestha and Kathleen Mckeown. 2004. Detection of question-answer pairs in email conversations. In Proc. Coling, pages 889–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--339</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26:339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proc. EMNLP/VLC,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="22975" citStr="Toutanova and Manning, 2000" startWordPosition="3749" endWordPosition="3753">e the quality of answers. The main reason for the small number is that, unlike problem descriptions, much fewer sentences are needed to give feedbacks. 5 Experiment In the experiment, we randomly split annotated threads into three disjoint sets, and run a three-fold cross validation. Within each fold, first sentence types are labeled using linear-chain CRFs, then the Target Source y00 y10 ... y01 y11 ... X ... ... ... 559 resulting sentence type tagging is used in the second pass to determine dependency relations. For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000). All the graphical inference and estimations are done using MALLET package (McCallum, 2002). In this paper, we evaluate the results using standard precision and recall. In the sentence type tagging task, we calculate precision, recall, and Fl score for each individual tag. For the dependency tagging task, a pair identified by the system is correct only if the exact pair appears in the reference annotation. Precision and recall scores are calculated accordingly. 5.1 Sentence Type Tagging Results The results of sentence type tagging using linearchain CRFs are shown in Table 5. For a comparison,</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proc. EMNLP/VLC, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Zaiqing Nie</author>
<author>Ji R Wen</author>
<author>Bo Zhang</author>
<author>Wei Y Ma</author>
</authors>
<title>2D Conditional Random Fields for Web information extraction.</title>
<date>2005</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>1044--1051</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="18328" citStr="Zhu et al., 2005" startWordPosition="2985" endWordPosition="2988">ture Table 3: Features Used in Sentence Dependency Labeling The linear-chain CRFs can represent the dependency between adjacent target sentences quite well. However they cannot model the dependency between adjacent source sentences, because labeling is done for each source sentence individually. To model the dependency between both the source sentences and the target sentences, we propose to use 2D CRFs for sentence relation labeling. 2D CRFs are used in many applications considering two dimension dependencies such as object recognitions (Quattoni et al., 2004) and web information extraction (Zhu et al., 2005). The graphical structure of a 2D CRF is shown in Figure 4. Unlike one dimensional sequence labeling, a node in 2D environment is dependent on both x-axis neighbors and yaxis neighbors. In the sentence relation task, the source and target pair is a 2D relation in which its label depends on labels of both its adjacent source and its adjacent target sentence. As shown in Figure 4, looking from x-axis is the sequence of target sentences with a fixed source sentence, and from yaxis is the sequence of source sentences with a fixed target sentence. This model allows us to model all the sentence rela</context>
</contexts>
<marker>Zhu, Nie, Wen, Zhang, Ma, 2005</marker>
<rawString>Jun Zhu, Zaiqing Nie, Ji R. Wen, Bo Zhang, and Wei Y. Ma. 2005. 2D Conditional Random Fields for Web information extraction. In Proc. ICML, pages 1044– 1051, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>