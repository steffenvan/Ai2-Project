<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001117">
<title confidence="0.992986">
LT3: Sentiment Classification in User-Generated Content Using a Rich
Feature Set
</title>
<author confidence="0.992022">
Cynthia Van Hee, Marjan Van de Kauter, Orph´ee De Clercq, Els Lefever and V´eronique Hoste
</author>
<affiliation confidence="0.975224">
LT3, Language and Translation Technology Team
Department of Translation, Interpreting and Communication – Ghent University
</affiliation>
<address confidence="0.911412">
Groot-Brittanni¨elaan 45, 9000 Ghent, Belgium
</address>
<email confidence="0.995083">
Firstname.Lastname@UGent.be
</email>
<sectionHeader confidence="0.993756" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997945">
This paper describes our contribution to the
SemEval-2014 Task 9 on sentiment analysis in
Twitter. We participated in both strands of the
task, viz. classification at message-level (subtask
B), and polarity disambiguation of particular text
spans within a message (subtask A). Our experi-
ments with a variety of lexical and syntactic fea-
tures show that our systems benefit from rich fea-
ture sets for sentiment analysis on user-generated
content. Our systems ranked ninth among 27 and
sixteenth among 50 submissions for task A and B
respectively.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.92675575">
Over the past few years, Web 2.0 applications
such as microblogging services, social network-
ing sites, and short messaging services have con-
siderably increased the amount of user-generated
content produced online. Millions of people rely
on these services to send messages, share their
views or gather information about others. Si-
multaneously, companies, marketeers and politi-
cians are anxious to detect sentiment in UGC since
these messages might contain valuable informa-
tion about the public opinion. This explains why
sentiment analysis has been a research area of
great interest in the last few years (Wiebe et al.,
2005; Wilson et al., 2005; Pang and Lee, 2008;
Mohammad and Yang, 2011). Though first studies
focussed more on product or movie reviews, we
see that analyzing sentiment in UGC is currently
becoming increasingly popular. The main differ-
ence between these two sources of information is
that the former is rather long and contains quite
formal language whereas the latter one is gener-
ally very brief and noisy and thus represents some
different challenges (Maynard et al., 2012).
In this paper, we describe our contribution to
the SemEval-2014 Task 9: Sentiment Analysis in
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Twitter (Rosenthal et al., 2014), which was a rerun
of SemEval-2013 Task 2 (Nakov et al., 2013) and
consisted of two subtasks:
</bodyText>
<listItem confidence="0.999711833333333">
• Subtask A - Contextual Polarity
Disambiguation: Given a message contain-
ing a marked instance of a word or phrase,
determine whether that instance is positive,
negative or neutral in that context.
• Subtask B - Message Polarity
</listItem>
<bodyText confidence="0.996997035714286">
Classification: Given a message, classify
whether the message is of positive, negative,
or neutral sentiment. For messages convey-
ing both a positive and negative sentiment,
whichever is the stronger sentiment should be
chosen.
The datasets for training, development and test-
ing were provided by the task organizers. The
training datasets consisted of Twitter messages
on a variety of topics. The test sets con-
tained regular tweets (Twitter2013, Twitter2014),
tweets labeled as sarcastic (TwitterSarcasm), SMS
messages (SMS2013), and blog posts (LiveJour-
nal2014). For both subtasks, the possible polar-
ity labels were positive, negative, neutral, and ob-
jective. The datasets for subtask B contained an
additional label, i.e. objective-OR-neutral. Ta-
ble 1 presents an overview of all provided datasets.
For each task and test dataset, two runs could be
submitted: a constrained run using the provided
training data only, and an unconstrained one us-
ing additional training data. For both tasks, we
created a constrained model based on supervised
learning, relying on additional lexicons and us-
ing the test datasets of SemEval-2013 as develop-
ment data. Evaluation was based on averaged F-
measure, considering averaged F-positive and F-
negative.
</bodyText>
<page confidence="0.986808">
406
</page>
<note confidence="0.7599665">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 406–410,
Dublin, Ireland, August 23-24, 2014.
</note>
<table confidence="0.999843727272727">
Dataset Subtask A Subtask B
Training
Training data 26,928 9,684
Development data 1,135 1,654
Total training data 28,063 11,338
Dev-test (test SemEval-2013)
Tweets 4,435 3,813
SMS messages 2,334 2,094
Test SemEval-2014
Tweets + SMS messages + 10,681 8,987
blog posts + sarcastic tweets
</table>
<tableCaption confidence="0.996341333333333">
Table 1: Number of labeled instances contained
by the training, development (test data SemEval-
2013), and SemEval-2014 test sets.
</tableCaption>
<sectionHeader confidence="0.952811" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999986285714286">
Our main goal was to develop, for each polarity
classification task, a classifier to label a message
or an instance of that message as either positive,
negative, or neutral. We ran several experiments to
identify the most discriminative classifier features.
This section gives an overview of the pipeline we
developed and which features were implemented.
</bodyText>
<subsectionHeader confidence="0.993621">
2.1 Linguistic Preprocessing
</subsectionHeader>
<bodyText confidence="0.999982625">
First, we performed manual cleaning on the
datasets to replace non-UTF-8 characters, and we
tokenized all messages using the Carnegie Mellon
University Twitter Part-of-Speech Tagger (Gimpel
et al., 2011). Subsequently, we Part-of-Speech
tagged all instances using the CMU Twitter Part-
of-Speech Tagger (Gimpel et al., 2011), and per-
formed dependency parsing using a caseless pars-
ing model of the Stanford parser (de Marneffe et
al., 2006). Besides that, we also tagged all named
entities using the Twitter NLP tools (Ritter et al.,
2011) for Named Entity Recognition. As a final
preprocessing step, we decided to combine the la-
bels neutral, objective and neutral-OR-objective,
thus recasting the task as a three-way classifica-
tion task.
</bodyText>
<subsectionHeader confidence="0.999128">
2.2 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.887238">
We implemented a number of lexical and syntactic
features that represent every phrase (subtask A) or
message (subtask B) within a feature vector:
N-gram features
</bodyText>
<listItem confidence="0.943478464285714">
• Word token n-gram features: a binary value
for every token unigram, bigram, and trigram
found in the training data.
• Character n-gram features: a binary value
for every character trigram, and fourgram
(within word tokens) found in the training
data.
• Normalized n-gram features: n-grams that
consisted of URLs and mentions or @-
replies were replaced by http://someurl and
by @someuser, respectively. We also nor-
malized commonly used abbreviations 1 to
their full written form (e.g. h8 → hate).
Word shape features
• Character flooding: the number of word to-
kens with a character repeated more than two
times (e.g. sooooooo join).
• Punctuation flooding: the number of con-
tiguous sequences of exclamation/question
marks (e.g. GRADUATION?!?!).
• Punctuation of the last token: a binary value
indicating whether the last word token of
a message contains a question/exclamation
mark (e.g. Going to Helsinki tomorrow or on
the day after tomorrow, yay!).
• The number of capitalized words (e.g. SO
EXCITED).
• The number of hashtags (e.g. #win).
</listItem>
<bodyText confidence="0.788382909090909">
Lexicon features: As sentiment lexicons we
consulted existing resources: AFINN (Nielsen,
2011), General Inquirer (Stone et al., 1966),
MPQA (Wilson et al., 2005), NRC Emotion (Mo-
hammad and Turney, 2010; Mohammad and
Yang, 2011), Bing Liu (Hu and Liu, 2004), and
Bounce (K¨okciyan et al., 2013) – the latter three
are Twitter-specific. Additionally, we created a list
of emoticons extracted from the SemEval-2014
training data. Based on these resources, the fol-
lowing features were extracted:
</bodyText>
<listItem confidence="0.9749535">
• The number of positive, negative, and neutral
lexicon words averaged over text length
• The overall polarity, which is the sum of the
values of identified sentiment words
</listItem>
<bodyText confidence="0.99990725">
These features were extracted by 1) looking at all
tokens in the instance, and 2) looking at hash-
tag tokens only (e.g. win from #win). We also
considered negation cues by flipping the polarity
</bodyText>
<footnote confidence="0.990919">
1These were extracted from an existing list of chat abbre-
viations (http://www.chatslang.com/terms/abbreviations).
</footnote>
<page confidence="0.995402">
407
</page>
<bodyText confidence="0.938182333333333">
sign of a sentiment word if it occurred in a nega-
tion relation (e.g. @ 2Shades maybe 3rd team bro,
he’s not better than trey Burke from Michigan).
Negation relations were identified using the output
of the dependency parser. In the example above,
the positive polarity of the sentiment word better
is flipped into negative since it occurs in a relation
with not.
Syntactic features:
</bodyText>
<listItem confidence="0.999093578947369">
• Part-of-Speech – 25 tags, including Twitter-
specific tags such as # (hashtags), @ (at-
mentions), ~ (retweets), U (URLs or e-mail
addresses), and E (emoticons): binary (tag
occurs in the tweet or not), ternary (tag oc-
curs zero, one, or two or more times), abso-
lute (number of occurrences), and frequency
(frequency of the tag).
• Dependency relations – four binary values for
every dependency relation found in the train-
ing data. The first value indicates the pres-
ence of the lexicalized dependency relations
in the test data. Additionally, as proposed
by (Joshi and Penstein-Ros´e, 2009), the de-
pendency relation features are generalized in
three ways: by backing off the head word to
its PoS-tag, by backing off the modifier word
to its PoS-tag, and by backing off both the
head and modifier word.
</listItem>
<bodyText confidence="0.996584411764706">
Named entity features: This feature group con-
sists of four features: binary (tweet contains NEs
or not), absolute (number of NEs), absolute tokens
(number of tokens that are part of an NE), and fre-
quency tokens (frequency of NE tokens).
PMI features: PMI (pointwise mutual informa-
tion) values indicating the association of a word
with positive and negative sentiment. The higher
the PMI value, the stronger the word-sentiment as-
sociation. For each unigram and bigram in the
training data, PMI values were extracted from
the word-sentiment association lexicon created by
NRC Canada (Mohammad et al., 2013). A sec-
ond PMI feature was considered for each unigram
based on the word-sentiment associations found in
the SemEval-2014 training dataset. PMI values
were calculated as follows:
</bodyText>
<equation confidence="0.99759">
PMI(w) = PMI(w, positive) − PMI(w, negative)
(1)
</equation>
<bodyText confidence="0.99739175">
As the equation shows, the association score of a
word with negative sentiment is subtracted from
the word’s association score with positive senti-
ment.
</bodyText>
<subsectionHeader confidence="0.998718">
2.3 Optimizing the Classification Results
</subsectionHeader>
<bodyText confidence="0.99992745">
The core of our approach consisted in evaluating
the aforementioned features and selecting those
feature groups contributing most to the classifica-
tion results. To this end, we trained an SVM clas-
sifier using the LIBSVM package (Chang and Lin,
2001) and created models for various feature com-
binations. A linear kernel and a cost value of 1
were chosen as parameter settings for all further
experiments after cross-validation on the training
data. Our experimental setup consisted of three
steps: 1) training an SVM on the original train-
ing data provided by the task organizers (no de-
velopment data was used), 2) generating a model,
and 3) applying and evaluating the model on the
development data (Twitter and SMS test data of
SemEval-2013). We started our experiments with
sentiment lexicon and n-gram features only, and
gradually added other feature groups to identify
the most contributive features. Tables 2 and 3 re-
veal the obtained F-scores for each step.
</bodyText>
<table confidence="0.996584636363636">
Features Dev Twitter Dev SMS
lexicons 0.6855 0.6402
n-grams 0.8482 0.8229
n-grams + lexicons 0.8628 0.8489
+ normalization n-grams 0.8632 (+ 0.0004) 0.8502 (+ 0.0013)
+ Part-of-Speech 0.8646 (+ 0.0014) 0.8582 (+ 0.0080)
+ negation 0.8650 (+ 0.0004) 0.8654 (+ 0.0072)
+ word shape 0.8649 (- 0.0001) 0.8650 (- 0.0004)
+named entity 0.8642 (- 0.0007) 0.8660 (+ 0.0010)
+ dependency 0.8642 (=) 0.8660 (=)
+ PMI 0.8610 (- 0.0032) 0.8654 (- 0.0006)
</table>
<tableCaption confidence="0.911435">
Table 2: F-scores obtained after adding other fea-
tures for the Twitter and SMS development data
(test data SemEval-2013) – subtask A.
</tableCaption>
<table confidence="0.999638090909091">
Features Dev Twitter Dev SMS
lexicons 0.5342 0.5119
n-grams 0.5896 0.5628
n-grams + lexicons 0.6442 0.6040
+ normalization n-grams 0.6414 (- 0.0028) 0.6084 (+ 0.0044)
+ Part-of-Speech 0.6466 (+ 0.0052) 0.6333 (+ 0.0249)
+ negation 0.6542 (+ 0.0076) 0.6384 (+ 0.0051)
+ word shape 0.6581 (+ 0.0039) 0.6394 (+ 0.0010)
+ named entity 0.6559 (- 0.0022) 0.6399 (+ 0.0005)
+ dependency 0.6467 (- 0.0092) 0.6430 (+ 0.0031)
+ PMI 0.6525 (+ 0.0058) 0.6525 (+ 0.0095)
</table>
<tableCaption confidence="0.929830333333333">
Table 3: F-scores obtained after adding other fea-
tures for the Twitter and SMS development data
(test data SemEval-2013) – subtask B.
</tableCaption>
<bodyText confidence="0.836679">
As can be inferred from the tables, F-scores
</bodyText>
<page confidence="0.989398">
408
</page>
<table confidence="0.998032666666667">
SMS2013 Twitter2013 LiveJournal2014 Twitter2014 Twitter2014 Sarcasm
Task A 85.26 (7/27) 86.28 (8/27) 80.44 (13/27) 81.02 (9/27) 70.76 (13/27)
Task B 64.78 (7/50) 65.56 (14/50) 68.56 (20/50) 65.47 (16/50) 47.76 (22/50)
</table>
<tableCaption confidence="0.8924795">
Table 4: F-scores and rankings of our systems across the various data genres for subtask A (Contextual
Polarity Disambiguation) and subtask B (Message Polarity Classification).
</tableCaption>
<bodyText confidence="0.99991125">
were already relatively high (~0.8559 for subtask
A and ~0.6241 for subtask B) for the combined
lexicon and n-gram features (on average 0.8559
for subtask A and 0.6241 for subtask B), which we
therefore consider as a our baseline setup. Con-
sidering the results for both subtasks and data
genres, we conclude that n-grams, sentiment lex-
icons, and PoS-tags were the most contributive
feature groups, whereas named entity and depen-
dency features did not improve the overall classi-
fication performance. However, using all feature
groups (n-grams, lexicons, normalized n-grams,
Part-of-Speech features, negation features, word
shape features, named entity features, dependency
features, and PMI features) improved the classi-
fication results (reaching an averaged F= 0.8632
for subtask A, and F= 0.6525 for subtask B) com-
pared to classification based on lexicon (averaged
F= 0.6629 for subtask A, and F= 0.5231 for sub-
task B) or n-gram features only (averaged F=
0.8356 for subtask A, and F= 0.5762 for subtask
B). Based on these results, we conclude that using
the full feature set for the classification of unseen
data appears to be a promising approach, consid-
ering that it achieves good performance and that it
would not tune the training model to a particular
data genre.
For further optimization of the classification re-
sults, we performed feature selection in the fea-
ture groups by using a genetic algorithm approach
which can explore different areas of the search
space in parallel. In order to do so, we made use
of the Gallop (Genetic Algorithms for Linguistic
Learner Optimization) python package (Desmet
et al., 2013). This enabled us to select the most
contributive features from every feature group: n-
gram features at token and character level, lexi-
con features from General Inquirer, Liu, AFINN,
and Bounce, character flooding and token capital-
ization features, Part-of-Speech features (binary,
ternary, and absolute), named entity features (bi-
nary, absolute tokens, and frequency tokens), and
PMI features based on the NRC lexicon. None of
the dependency relation features were selected.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999983772727273">
We submitted sentiment labels for the Contextual
Polarity Disambiguation (subtask A) and for the
Message Polarity Classification (subtask B). Our
competition results are reported in Table 4. Rank-
ings for each dataset are added between brack-
ets. The results reveal that our systems achieved
good performance in the polarity classification of
unseen data across the various genres and tasks.
Overall, we achieved our best classification per-
formance on the Twitter2013 test set, obtaining an
F-score of 86.28, while the best performance for
this data genre is an F-score of 90.14. We saw a
drop in performance on the Twitter2014 Sarcasm
test set. This is consistent with most other teams
as sarcastic language is hard to handle in senti-
ment analysis. Considering the rankings, we con-
clude that we performed particularly well on the
SMS test dataset of SemEval-2013 for both sub-
tasks, ranking seventh for this genre. Our systems
ranked ninth among 27 submissions and sixteenth
among 50 submissions for subtasks A and B re-
spectively.
</bodyText>
<sectionHeader confidence="0.993535" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999913315789474">
Using a rich feature set proves to be beneficial for
automatic sentiment analysis on user-generated
content. Feature selection experiments revealed
that features based on n-grams, sentiment lexi-
cons, and PoS-tags were most contributive for
both classification tasks, while dependency fea-
tures did not contribute to overall classification
performance. As future work it will be interesting
to study the impact of normalization of the data on
the classification performance.
Based on a shallow error analysis, we believe
that including additional classification features
may also be promising: modifiers other than nega-
tion cues (diminishers, increasers, modal verbs,
etc.) that affect the polarity intensity, emoticon
flooding, and pre- and suffixes that indicate emo-
tion (un-, dis-, -less, etc.). Additionally, lemma-
tization and hashtag segmentation on the training
data could also improve classification results.
</bodyText>
<page confidence="0.997298">
409
</page>
<bodyText confidence="0.92932225">
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. CoRR,
abs/1308.6242.
</bodyText>
<figure confidence="0.567715">
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
</figure>
<reference confidence="0.99776828125">
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proc. of LREC’06).
Bart Desmet, V´eronique Hoste, David Verstraeten, and
Jan Verhasselt. 2013. Gallop Documentation.
Technical Report LT3 13-03, University of Ghent.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume 2,
HLT ’11, pages 42–47, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD04, pages
168–177, New York, NY. ACM.
Mahesh Joshi and Carolyn Penstein-Ros´e. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, ACLShort ’09, pages 313–316,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Nadin K¨okciyan, Arda C¸elebi, Arzucan ¨Ozg¨ur, and
Suzan ¨Usk¨udarli. 2013. Bounce: Sentiment classi-
fication in Twitter using rich feature sets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 554–561, Atlanta, Geor-
gia, USA. ACL.
Diane Maynard, Kalina Bontcheva, and Dominic Rout.
2012. Challenges in developing opinion mining
tools for social media. In Proc. of the LREC work-
shop NLP can u tag #usergeneratedcontent?!
Saif Mohammad and Peter Turney. 2010. Emotions
Evoked by Common Words and Phrases: Using Me-
chanical Turk to Create an Emotion Lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California.
Saif Mohammad and Tony Yang. 2011. Tracking
Sentiment in Mail: How Genders Differ on Emo-
tional Axes. In Proceedings of the 2nd Workshop on
Computational Approaches to Subjectivity and Sen-
timent Analysis (WASSA 2011), pages 70–79, Port-
land, Oregon. ACL.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312–
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Finn Nielsen. 2011. A new anew: Evaluation of a
word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on Mak-
ing Sense of Microposts: Big things come in small
packages.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135, January.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’11, pages 1524–1534, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’14, Dublin, Ireland.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Computer Intelligence,
39(2):165–210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT05, pages 347–354, Stroudsburg, PA. ACL.
</reference>
<page confidence="0.997014">
410
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.312200">
<title confidence="0.9769935">LT3: Sentiment Classification in User-Generated Content Using a Rich Feature Set</title>
<author confidence="0.8557125">Cynthia Van_Hee</author>
<author confidence="0.8557125">Marjan Van_de_Kauter</author>
<author confidence="0.8557125">Orph´ee De_Clercq</author>
<author confidence="0.8557125">Els Lefever</author>
<author confidence="0.8557125">V´eronique Language</author>
<author confidence="0.8557125">Translation Technology</author>
<affiliation confidence="0.997092">Department of Translation, Interpreting and Communication – Ghent</affiliation>
<address confidence="0.590367">Groot-Brittanni¨elaan 45, 9000 Ghent,</address>
<email confidence="0.752308">Firstname.Lastname@UGent.be</email>
<abstract confidence="0.988351230769231">This paper describes our contribution to the SemEval-2014 Task 9 on sentiment analysis in Twitter. We participated in both strands of the task, viz. classification at message-level (subtask B), and polarity disambiguation of particular text spans within a message (subtask A). Our experiments with a variety of lexical and syntactic features show that our systems benefit from rich feature sets for sentiment analysis on user-generated content. Our systems ranked ninth among 27 and sixteenth among 50 submissions for task A and B respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proc. of LREC’06).</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proc. of LREC’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Desmet</author>
<author>V´eronique Hoste</author>
<author>David Verstraeten</author>
<author>Jan Verhasselt</author>
</authors>
<title>Gallop Documentation.</title>
<date>2013</date>
<tech>Technical Report LT3 13-03,</tech>
<institution>University of Ghent.</institution>
<contexts>
<context position="14244" citStr="Desmet et al., 2013" startWordPosition="2229" endWordPosition="2232">k B). Based on these results, we conclude that using the full feature set for the classification of unseen data appears to be a promising approach, considering that it achieves good performance and that it would not tune the training model to a particular data genre. For further optimization of the classification results, we performed feature selection in the feature groups by using a genetic algorithm approach which can explore different areas of the search space in parallel. In order to do so, we made use of the Gallop (Genetic Algorithms for Linguistic Learner Optimization) python package (Desmet et al., 2013). This enabled us to select the most contributive features from every feature group: ngram features at token and character level, lexicon features from General Inquirer, Liu, AFINN, and Bounce, character flooding and token capitalization features, Part-of-Speech features (binary, ternary, and absolute), named entity features (binary, absolute tokens, and frequency tokens), and PMI features based on the NRC lexicon. None of the dependency relation features were selected. 3 Results We submitted sentiment labels for the Contextual Polarity Disambiguation (subtask A) and for the Message Polarity C</context>
</contexts>
<marker>Desmet, Hoste, Verstraeten, Verhasselt, 2013</marker>
<rawString>Bart Desmet, V´eronique Hoste, David Verstraeten, and Jan Verhasselt. 2013. Gallop Documentation. Technical Report LT3 13-03, University of Ghent.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 42–47, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="7117" citStr="Hu and Liu, 2004" startWordPosition="1085" endWordPosition="1088">sequences of exclamation/question marks (e.g. GRADUATION?!?!). • Punctuation of the last token: a binary value indicating whether the last word token of a message contains a question/exclamation mark (e.g. Going to Helsinki tomorrow or on the day after tomorrow, yay!). • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013) – the latter three are Twitter-specific. Additionally, we created a list of emoticons extracted from the SemEval-2014 training data. Based on these resources, the following features were extracted: • The number of positive, negative, and neutral lexicon words averaged over text length • The overall polarity, which is the sum of the values of identified sentiment words These features were extracted by 1) looking at all tokens in the instance, and 2) looking at hashtag tokens only (e.g. win from #win). We also considered negation cues by flipping the polarit</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD04, pages 168–177, New York, NY. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Carolyn Penstein-Ros´e</author>
</authors>
<title>Generalizing dependency features for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>313--316</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Joshi, Penstein-Ros´e, 2009</marker>
<rawString>Mahesh Joshi and Carolyn Penstein-Ros´e. 2009. Generalizing dependency features for opinion mining. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 313–316, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadin K¨okciyan</author>
<author>Arda C¸elebi</author>
<author>Arzucan ¨Ozg¨ur</author>
<author>Suzan ¨Usk¨udarli</author>
</authors>
<title>Bounce: Sentiment classification in Twitter using rich feature sets.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>554--561</pages>
<publisher>ACL.</publisher>
<location>Atlanta, Georgia, USA.</location>
<marker>K¨okciyan, C¸elebi, ¨Ozg¨ur, ¨Usk¨udarli, 2013</marker>
<rawString>Nadin K¨okciyan, Arda C¸elebi, Arzucan ¨Ozg¨ur, and Suzan ¨Usk¨udarli. 2013. Bounce: Sentiment classification in Twitter using rich feature sets. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 554–561, Atlanta, Georgia, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Maynard</author>
<author>Kalina Bontcheva</author>
<author>Dominic Rout</author>
</authors>
<title>Challenges in developing opinion mining tools for social media.</title>
<date>2012</date>
<booktitle>In Proc. of the LREC workshop NLP can u tag #usergeneratedcontent?!</booktitle>
<contexts>
<context position="2033" citStr="Maynard et al., 2012" startWordPosition="306" endWordPosition="309">on about the public opinion. This explains why sentiment analysis has been a research area of great interest in the last few years (Wiebe et al., 2005; Wilson et al., 2005; Pang and Lee, 2008; Mohammad and Yang, 2011). Though first studies focussed more on product or movie reviews, we see that analyzing sentiment in UGC is currently becoming increasingly popular. The main difference between these two sources of information is that the former is rather long and contains quite formal language whereas the latter one is generally very brief and noisy and thus represents some different challenges (Maynard et al., 2012). In this paper, we describe our contribution to the SemEval-2014 Task 9: Sentiment Analysis in This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Twitter (Rosenthal et al., 2014), which was a rerun of SemEval-2013 Task 2 (Nakov et al., 2013) and consisted of two subtasks: • Subtask A - Contextual Polarity Disambiguation: Given a message containing a marked instance of a word or phrase, determine whether that instance is positive, nega</context>
</contexts>
<marker>Maynard, Bontcheva, Rout, 2012</marker>
<rawString>Diane Maynard, Kalina Bontcheva, and Dominic Rout. 2012. Challenges in developing opinion mining tools for social media. In Proc. of the LREC workshop NLP can u tag #usergeneratedcontent?!</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Peter Turney</author>
</authors>
<title>Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<location>LA, California.</location>
<contexts>
<context position="7062" citStr="Mohammad and Turney, 2010" startWordPosition="1074" endWordPosition="1078">oooooo join). • Punctuation flooding: the number of contiguous sequences of exclamation/question marks (e.g. GRADUATION?!?!). • Punctuation of the last token: a binary value indicating whether the last word token of a message contains a question/exclamation mark (e.g. Going to Helsinki tomorrow or on the day after tomorrow, yay!). • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013) – the latter three are Twitter-specific. Additionally, we created a list of emoticons extracted from the SemEval-2014 training data. Based on these resources, the following features were extracted: • The number of positive, negative, and neutral lexicon words averaged over text length • The overall polarity, which is the sum of the values of identified sentiment words These features were extracted by 1) looking at all tokens in the instance, and 2) looking at hashtag tokens only (e.g. win from #win). W</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif Mohammad and Peter Turney. 2010. Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, LA, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Tony Yang</author>
</authors>
<title>Tracking Sentiment in Mail: How Genders Differ on Emotional Axes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2011),</booktitle>
<pages>70--79</pages>
<publisher>ACL.</publisher>
<location>Portland, Oregon.</location>
<contexts>
<context position="1629" citStr="Mohammad and Yang, 2011" startWordPosition="241" endWordPosition="244"> services, social networking sites, and short messaging services have considerably increased the amount of user-generated content produced online. Millions of people rely on these services to send messages, share their views or gather information about others. Simultaneously, companies, marketeers and politicians are anxious to detect sentiment in UGC since these messages might contain valuable information about the public opinion. This explains why sentiment analysis has been a research area of great interest in the last few years (Wiebe et al., 2005; Wilson et al., 2005; Pang and Lee, 2008; Mohammad and Yang, 2011). Though first studies focussed more on product or movie reviews, we see that analyzing sentiment in UGC is currently becoming increasingly popular. The main difference between these two sources of information is that the former is rather long and contains quite formal language whereas the latter one is generally very brief and noisy and thus represents some different challenges (Maynard et al., 2012). In this paper, we describe our contribution to the SemEval-2014 Task 9: Sentiment Analysis in This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers a</context>
<context position="7088" citStr="Mohammad and Yang, 2011" startWordPosition="1079" endWordPosition="1082"> flooding: the number of contiguous sequences of exclamation/question marks (e.g. GRADUATION?!?!). • Punctuation of the last token: a binary value indicating whether the last word token of a message contains a question/exclamation mark (e.g. Going to Helsinki tomorrow or on the day after tomorrow, yay!). • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013) – the latter three are Twitter-specific. Additionally, we created a list of emoticons extracted from the SemEval-2014 training data. Based on these resources, the following features were extracted: • The number of positive, negative, and neutral lexicon words averaged over text length • The overall polarity, which is the sum of the values of identified sentiment words These features were extracted by 1) looking at all tokens in the instance, and 2) looking at hashtag tokens only (e.g. win from #win). We also considered negation</context>
</contexts>
<marker>Mohammad, Yang, 2011</marker>
<rawString>Saif Mohammad and Tony Yang. 2011. Tracking Sentiment in Mail: How Genders Differ on Emotional Axes. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2011), pages 70–79, Portland, Oregon. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2436" citStr="Nakov et al., 2013" startWordPosition="365" endWordPosition="368">ources of information is that the former is rather long and contains quite formal language whereas the latter one is generally very brief and noisy and thus represents some different challenges (Maynard et al., 2012). In this paper, we describe our contribution to the SemEval-2014 Task 9: Sentiment Analysis in This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Twitter (Rosenthal et al., 2014), which was a rerun of SemEval-2013 Task 2 (Nakov et al., 2013) and consisted of two subtasks: • Subtask A - Contextual Polarity Disambiguation: Given a message containing a marked instance of a word or phrase, determine whether that instance is positive, negative or neutral in that context. • Subtask B - Message Polarity Classification: Given a message, classify whether the message is of positive, negative, or neutral sentiment. For messages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen. The datasets for training, development and testing were provided by the task organizers. The training datasets c</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in Twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312– 320, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn Nielsen</author>
</authors>
<title>A new anew: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on Making Sense of Microposts: Big things</booktitle>
<note>come in small packages.</note>
<contexts>
<context position="6955" citStr="Nielsen, 2011" startWordPosition="1059" endWordPosition="1060">acter flooding: the number of word tokens with a character repeated more than two times (e.g. sooooooo join). • Punctuation flooding: the number of contiguous sequences of exclamation/question marks (e.g. GRADUATION?!?!). • Punctuation of the last token: a binary value indicating whether the last word token of a message contains a question/exclamation mark (e.g. Going to Helsinki tomorrow or on the day after tomorrow, yay!). • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013) – the latter three are Twitter-specific. Additionally, we created a list of emoticons extracted from the SemEval-2014 training data. Based on these resources, the following features were extracted: • The number of positive, negative, and neutral lexicon words averaged over text length • The overall polarity, which is the sum of the values of identified sentiment words These features were extracted</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn Nielsen. 2011. A new anew: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on Making Sense of Microposts: Big things come in small packages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<pages>2--1</pages>
<contexts>
<context position="1603" citStr="Pang and Lee, 2008" startWordPosition="237" endWordPosition="240">uch as microblogging services, social networking sites, and short messaging services have considerably increased the amount of user-generated content produced online. Millions of people rely on these services to send messages, share their views or gather information about others. Simultaneously, companies, marketeers and politicians are anxious to detect sentiment in UGC since these messages might contain valuable information about the public opinion. This explains why sentiment analysis has been a research area of great interest in the last few years (Wiebe et al., 2005; Wilson et al., 2005; Pang and Lee, 2008; Mohammad and Yang, 2011). Though first studies focussed more on product or movie reviews, we see that analyzing sentiment in UGC is currently becoming increasingly popular. The main difference between these two sources of information is that the former is rather long and contains quite formal language whereas the latter one is generally very brief and noisy and thus represents some different challenges (Maynard et al., 2012). In this paper, we describe our contribution to the SemEval-2014 Task 9: Sentiment Analysis in This work is licensed under a Creative Commons Attribution 4.0 Internation</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1524--1534</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5431" citStr="Ritter et al., 2011" startWordPosition="820" endWordPosition="823">line we developed and which features were implemented. 2.1 Linguistic Preprocessing First, we performed manual cleaning on the datasets to replace non-UTF-8 characters, and we tokenized all messages using the Carnegie Mellon University Twitter Part-of-Speech Tagger (Gimpel et al., 2011). Subsequently, we Part-of-Speech tagged all instances using the CMU Twitter Partof-Speech Tagger (Gimpel et al., 2011), and performed dependency parsing using a caseless parsing model of the Stanford parser (de Marneffe et al., 2006). Besides that, we also tagged all named entities using the Twitter NLP tools (Ritter et al., 2011) for Named Entity Recognition. As a final preprocessing step, we decided to combine the labels neutral, objective and neutral-OR-objective, thus recasting the task as a three-way classification task. 2.2 Feature Extraction We implemented a number of lexical and syntactic features that represent every phrase (subtask A) or message (subtask B) within a feature vector: N-gram features • Word token n-gram features: a binary value for every token unigram, bigram, and trigram found in the training data. • Character n-gram features: a binary value for every character trigram, and fourgram (within wor</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1524–1534, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval ’14,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2373" citStr="Rosenthal et al., 2014" startWordPosition="353" endWordPosition="356">oming increasingly popular. The main difference between these two sources of information is that the former is rather long and contains quite formal language whereas the latter one is generally very brief and noisy and thus represents some different challenges (Maynard et al., 2012). In this paper, we describe our contribution to the SemEval-2014 Task 9: Sentiment Analysis in This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Twitter (Rosenthal et al., 2014), which was a rerun of SemEval-2013 Task 2 (Nakov et al., 2013) and consisted of two subtasks: • Subtask A - Contextual Polarity Disambiguation: Given a message containing a marked instance of a word or phrase, determine whether that instance is positive, negative or neutral in that context. • Subtask B - Message Polarity Classification: Given a message, classify whether the message is of positive, negative, or neutral sentiment. For messages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen. The datasets for training, development and testin</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval ’14, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6994" citStr="Stone et al., 1966" startWordPosition="1063" endWordPosition="1066"> tokens with a character repeated more than two times (e.g. sooooooo join). • Punctuation flooding: the number of contiguous sequences of exclamation/question marks (e.g. GRADUATION?!?!). • Punctuation of the last token: a binary value indicating whether the last word token of a message contains a question/exclamation mark (e.g. Going to Helsinki tomorrow or on the day after tomorrow, yay!). • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013) – the latter three are Twitter-specific. Additionally, we created a list of emoticons extracted from the SemEval-2014 training data. Based on these resources, the following features were extracted: • The number of positive, negative, and neutral lexicon words averaged over text length • The overall polarity, which is the sum of the values of identified sentiment words These features were extracted by 1) looking at all tokens in the ins</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Computer Intelligence,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="1562" citStr="Wiebe et al., 2005" startWordPosition="229" endWordPosition="232">he past few years, Web 2.0 applications such as microblogging services, social networking sites, and short messaging services have considerably increased the amount of user-generated content produced online. Millions of people rely on these services to send messages, share their views or gather information about others. Simultaneously, companies, marketeers and politicians are anxious to detect sentiment in UGC since these messages might contain valuable information about the public opinion. This explains why sentiment analysis has been a research area of great interest in the last few years (Wiebe et al., 2005; Wilson et al., 2005; Pang and Lee, 2008; Mohammad and Yang, 2011). Though first studies focussed more on product or movie reviews, we see that analyzing sentiment in UGC is currently becoming increasingly popular. The main difference between these two sources of information is that the former is rather long and contains quite formal language whereas the latter one is generally very brief and noisy and thus represents some different challenges (Maynard et al., 2012). In this paper, we describe our contribution to the SemEval-2014 Task 9: Sentiment Analysis in This work is licensed under a Cre</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Computer Intelligence, 39(2):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT05,</booktitle>
<pages>347--354</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="1583" citStr="Wilson et al., 2005" startWordPosition="233" endWordPosition="236">eb 2.0 applications such as microblogging services, social networking sites, and short messaging services have considerably increased the amount of user-generated content produced online. Millions of people rely on these services to send messages, share their views or gather information about others. Simultaneously, companies, marketeers and politicians are anxious to detect sentiment in UGC since these messages might contain valuable information about the public opinion. This explains why sentiment analysis has been a research area of great interest in the last few years (Wiebe et al., 2005; Wilson et al., 2005; Pang and Lee, 2008; Mohammad and Yang, 2011). Though first studies focussed more on product or movie reviews, we see that analyzing sentiment in UGC is currently becoming increasingly popular. The main difference between these two sources of information is that the former is rather long and contains quite formal language whereas the latter one is generally very brief and noisy and thus represents some different challenges (Maynard et al., 2012). In this paper, we describe our contribution to the SemEval-2014 Task 9: Sentiment Analysis in This work is licensed under a Creative Commons Attribu</context>
<context position="7022" citStr="Wilson et al., 2005" startWordPosition="1068" endWordPosition="1071">peated more than two times (e.g. sooooooo join). • Punctuation flooding: the number of contiguous sequences of exclamation/question marks (e.g. GRADUATION?!?!). • Punctuation of the last token: a binary value indicating whether the last word token of a message contains a question/exclamation mark (e.g. Going to Helsinki tomorrow or on the day after tomorrow, yay!). • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013) – the latter three are Twitter-specific. Additionally, we created a list of emoticons extracted from the SemEval-2014 training data. Based on these resources, the following features were extracted: • The number of positive, negative, and neutral lexicon words averaged over text length • The overall polarity, which is the sum of the values of identified sentiment words These features were extracted by 1) looking at all tokens in the instance, and 2) looking at has</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT05, pages 347–354, Stroudsburg, PA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>