<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.99568">
Approximate Factoring for A* Search
</title>
<author confidence="0.996816">
Aria Haghighi, John DeNero, Dan Klein
</author>
<affiliation confidence="0.9985065">
Computer Science Division
University of California Berkeley
</affiliation>
<email confidence="0.985546">
{aria42, denero, klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997329" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99968125">
We present a novel method for creating A* esti-
mates for structured search problems. In our ap-
proach, we project a complex model onto multiple
simpler models for which exact inference is effi-
cient. We use an optimization framework to es-
timate parameters for these projections in a way
which bounds the true costs. Similar to Klein and
Manning (2003), we then combine completion es-
timates from the simpler models to guide search
in the original complex model. We apply our ap-
proach to bitext parsing and lexicalized parsing,
demonstrating its effectiveness in these domains.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984912195122">
Inference tasks in NLP often involve searching for
an optimal output from a large set of structured out-
puts. For many complex models, selecting the high-
est scoring output for a given observation is slow or
even intractable. One general technique to increase
efficiency while preserving optimality is A* search
(Hart et al., 1968); however, successfully using A*
search is challenging in practice. The design of ad-
missible (or nearly admissible) heuristics which are
both effective (close to actual completion costs) and
also efficient to compute is a difficult, open prob-
lem in most domains. As a result, most work on
search has focused on non-optimal methods, such
as beam search or pruning based on approximate
models (Collins, 1999), though in certain cases ad-
missible heuristics are known (Och and Ney, 2000;
Zhang and Gildea, 2006). For example, Klein and
Manning (2003) show a class of projection-based A*
estimates, but their application is limited to models
which have a very restrictive kind of score decom-
position. In this work, we broaden their projection-
based technique to give A* estimates for models
which do not factor in this restricted way.
Like Klein and Manning (2003), we focus on
search problems where there are multiple projec-
tions or “views” of the structure, for example lexical
parsing, in which trees can be projected onto either
their CFG backbone or their lexical attachments. We
use general optimization techniques (Boyd and Van-
denberghe, 2005) to approximately factor a model
over these projections. Solutions to the projected
problems yield heuristics for the original model.
This approach is flexible, providing either admissi-
ble or nearly admissible heuristics, depending on the
details of the optimization problem solved. Further-
more, our approach allows a modeler explicit control
over the trade-off between the tightness of a heuris-
tic and its degree of inadmissibility (if any). We de-
scribe our technique in general and then apply it to
two concrete NLP search tasks: bitext parsing and
lexicalized monolingual parsing.
</bodyText>
<sectionHeader confidence="0.994169" genericHeader="method">
2 General Approach
</sectionHeader>
<bodyText confidence="0.999288076923077">
Many inference problems in NLP can be solved
with agenda-based methods, in which we incremen-
tally build hypotheses for larger items by combining
smaller ones with some local configurational struc-
ture. We can formalize such tasks as graph search
problems, where states encapsulate partial hypothe-
ses and edges combine or extend them locally.1 For
example, in HMM decoding, the states are anchored
labels, e.g. VBD[5], and edges correspond to hidden
transitions, e.g. VBD[5] —* DT[6].
The search problem is to find a minimal cost path
from the start state to a goal state, where the path
cost is the sum of the costs of the edges in the path.
</bodyText>
<footnote confidence="0.8695975">
1In most complex tasks, we will in fact have a hypergraph,
but the extension is trivial and not worth the added notation.
</footnote>
<page confidence="0.899372">
412
</page>
<note confidence="0.810398">
Proceedings of NAACL HLT 2007, pages 412–419,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<figure confidence="0.990923714285714">
a&apos; → b&apos; b&apos; → c&apos; a&apos; → b&apos; b&apos; → c&apos;
1.0 2.0
1.0 2.0
a&apos; - b&apos; b&apos; - c&apos; b
� da b,
b,
� b � c � � � �
� � � � �
� � �
a → b 2.0 3.0 a → b 2.0 3.0
b → c 3.0 4.0 b → c 3.0 5.0
Original Cost Matrix
c(a&apos; → b&apos;) c(a&apos; → b&apos;)
1.0
3.0
2.0
2.0
4.0
3.0
c(a → b)
c(b → c)
a - b
b - c
Original Cost Matrix
c(a&apos; → b&apos;) c(a&apos; → b&apos;)
2.0
3.0
c(a → b)
c(b → c)
4.0
3.0
1.0
2.0
Local Configurations Factored Cost Matrix Factored Cost Matrix
(a) (b) (c)
</figure>
<figureCaption confidence="0.880683666666667">
Figure 1: Example cost factoring: In (a), each cell of the matrix is a local configuration composed of two projections (the row and
column of the cell). In (b), the top matrix is an example cost matrix, which specifies the cost of each local configuration. The
bottom matrix represents our factored estimates, where each entry is the sum of configuration projections. For this example, the
actual cost matrix can be decomposed exactly into two projections. In (c), the top cost matrix cannot be exactly decomposed along
two dimensions. Our factored cost matrix has the property that each factored cost estimate is below the actual configuration cost.
Although our factorization is no longer tight, it still can be used to produce an admissible heuristic.
</figureCaption>
<bodyText confidence="0.999814166666667">
For probabilistic inference problems, the cost of an
edge is typically a negative log probability which de-
pends only on some local configuration type. For
instance, in PCFG parsing, the (hyper)edges refer-
ence anchored spans X[i, j], but the edge costs de-
pend only on the local rule type X —* Y Z. We will
</bodyText>
<equation confidence="0.737056">
1
</equation>
<bodyText confidence="0.99974625">
use a to refer to a local configuration and use c(a)
to refer to its cost. Because edge costs are sensi-
tive only to local configurations, the cost of a path
is Ea c(a). A* search requires a heuristic function,
which is an estimate h(s) of the completion cost, the
cost of a best path from state s to a goal.
In this work, following Klein and Manning
(2003), we consider problems with projections or
“views,” which define mappings to simpler state and
configuration spaces. For instance, suppose that we
are using an HMM to jointly model part-of-speech
(POS) and named-entity-recognition (NER) tagging.
There might be one projection onto the NER com-
ponent and another onto the POS component. For-
mally, a projection 7r is a mapping from states to
some coarser domain. A state projection induces
projections of edges and of the entire graph 7r(G).
We are particularly interested in search problems
with multiple projections 17r1, ... , 7r`l where each
projection, 7ri, has the following properties: its state
projections induce well-defined projections of the
local configurations 7ri(a) used for scoring, and the
projected search problem admits a simpler infer-
ence. For instance, the POS projection in our NER-
POS HMM is a simpler HMM, though the gains
from this method are greater when inference in the
projections have lower asymptotic complexity than
the original problem (see sections 3 and 4).
In defining projections, we have not yet dealt with
the projected scoring function. Suppose that the
cost of local configurations decomposes along pro-
jections as well. In this case,
</bodyText>
<equation confidence="0.9987305">
c (a) = �` ci(a) , ba E A (1)
i=1
</equation>
<bodyText confidence="0.999647416666667">
where A is the set of local configurations and ci(a)
represents the cost of configuration a under projec-
tion 7ri. A toy example of such a cost decomposi-
tion in the context of a Markov process over two-part
states is shown in figure 1(b), where the costs of the
joint transitions equal the sum of costs of their pro-
jections. Under the strong assumption of equation
(1), Klein and Manning (2003) give an admissible
A* bound. They note that the cost of a path decom-
poses as a sum of projected path costs. Hence, the
following is an admissible additive heuristic (Felner
et al., 2004),
</bodyText>
<equation confidence="0.992862">
h*i (s) (2)
</equation>
<bodyText confidence="0.99996775">
where h*i (s) denote the optimal completion costs in
the projected search graph 7ri(G). That is, the com-
pletion cost of a state bounds the sum of the comple-
tion costs in each projection.
In virtually all cases, however, configuration costs
will not decompose over projections, nor would we
expect them to. For instance, in our joint POS-NER
task, this assumption requires that the POS and NER
</bodyText>
<equation confidence="0.994209">
�`
i=1
h(s) =
</equation>
<page confidence="0.989034">
413
</page>
<bodyText confidence="0.999869777777778">
transitions and observations be generated indepen-
dently. This independence assumption undermines
the motivation for assuming a joint model. In the
central contribution of this work, we exploit the pro-
jection structure of our search problem without mak-
ing any assumption about cost decomposition.
Rather than assuming decomposition, we propose
to find scores φ for the projected configurations
which are pointwise admissible:
</bodyText>
<equation confidence="0.997603333333333">
P
φZ(a) G c(a),Va E A (3)
Z=1
</equation>
<bodyText confidence="0.900562833333333">
Here, φZ(a) represents a factored projection cost of
πZ(a), the πZ projection of configuration a. Given
pointwise admissible φZ’s we can again apply the
heuristic recipe of equation (2). An example of
factored projection costs are shown in figure 1(c),
where no exact decomposition exists, but a point-
wise admissible lower bound is easy to find.
Claim. If a set of factored projection costs
�φ1, . . . ,φP} satisfy pointwise admissibility, then
the heuristic from (2) is an admissible A* heuristic.
Proof. Assume a1, ... , ak are configurations used
to optimally reach the goal from state s. Then,
</bodyText>
<equation confidence="0.998797">
! X`
Oi(aj) &gt; hi (s) = h(s)
i=1
</equation>
<bodyText confidence="0.9999777">
The first inequality follows from pointwise admis-
sibility. The second inequality follows because each
inner sum is a completion cost for projected problem
πZ and therefore h*Z (s) lower bounds it. Intuitively,
we can see two sources of slack in such projection
heuristics. First, there may be slack in the pointwise
admissible scores. Second, the best paths in the pro-
jections will be overly optimistic because they have
been decoupled (see figure 5 for an example of de-
coupled best paths in projections).
</bodyText>
<subsectionHeader confidence="0.9952555">
2.1 Finding Factored Projections for
Non-Factored Costs
</subsectionHeader>
<bodyText confidence="0.999989571428571">
We can find factored costs φZ(a) which are point-
wise admissible by solving an optimization problem.
We think of our unknown factored costs as a block
vector φ = [φ1, .., φP], where vector φZ is composed
of the factored costs, φZ(a), for each configuration
a E A. We can then find admissible factored costs
by solving the following optimization problem,
</bodyText>
<equation confidence="0.957386333333333">
minimize I IγI I (4)
�
P
such that, γa = c(a) − φZ(a), Va E A
Z=1
γa &gt; 0, Va E A
</equation>
<bodyText confidence="0.999957242424242">
We can think of each γa as the amount by which
the cost of configuration a exceeds the factored pro-
jection estimates (the pointwise A* gap). Requiring
γa &gt; 0 insures pointwise admissibility. Minimiz-
ing the norm of the γa variables encourages tighter
bounds; indeed if I IγI I = 0, the solution corresponds
to an exact factoring of the search problem. In the
case where we minimize the 1-norm or oc-norm, the
problem above reduces to a linear program, which
can be solved efficiently for a large number of vari-
ables and constraints.2
Viewing our procedure decision-theoretically, by
minimizing the norm of the pointwise gaps we are
effectively choosing a loss function which decom-
poses along configuration types and takes the form
of the norm (i.e. linear or squared losses). A com-
plete investigation of the alternatives is beyond the
scope of this work, but it is worth pointing out that
in the end we will care only about the gap on entire
structures, not configurations, and individual config-
uration factored costs need not even be pointwise ad-
missible for the overall heuristic to be admissible.
Notice that the number of constraints is JAI, the
number of possible local configurations. For many
search problems, enumerating the possible configu-
rations is not feasible, and therefore neither is solv-
ing an optimization problem with all of these con-
straints. We deal with this situation in applying our
technique to lexicalized parsing models (section 4).
Sometimes, we might be willing to trade search
optimality for efficiency. In our approach, we can
explicitly make this trade-off by designing an alter-
native optimization problem which allows for slack
</bodyText>
<footnote confidence="0.7741235">
2We used the MOSEK package (Andersen and Andersen,
2000).
</footnote>
<equation confidence="0.998064235294118">
X`
i=1
h*(s) =
c(aj) &gt;
Oi(aj)
k
X
j=1
k
X
j=1
=
X`
i=1
k
X
j=1
</equation>
<page confidence="0.985585">
414
</page>
<bodyText confidence="0.9964985">
in the admissibility constraints. We solve the follow-
ing soft version of problem (4):
</bodyText>
<equation confidence="0.99769375">
minimize kγ+k + Ckγ−k (5)
φ
such that, γa = c(a) − �` φi(a), ∀a ∈ A
i=1
</equation>
<bodyText confidence="0.999793888888889">
where γ+ = max{0,γ} and γ− = max{0, −γ}
represent the componentwise positive and negative
elements of γ respectively. Each γ−a &gt; 0 represents
a configuration where our factored projection esti-
mate is not pointwise admissible. Since this situa-
tion may result in our heuristic becoming inadmis-
sible if used in the projected completion costs, we
more heavily penalize overestimating the cost by the
constant C.
</bodyText>
<subsectionHeader confidence="0.998926">
2.2 Bounding Search Error
</subsectionHeader>
<bodyText confidence="0.999903714285714">
In the case where we allow pointwise inadmissibil-
ity, i.e. variables γ−a , we can bound our search er-
ror. Suppose γ−max = maxaEA γ−a and that L* is
the length of the longest optimal solution for the
original problem. Then, h(s) ≤ h*(s) + L*γ−max,
∀s ∈ S. This c-admissible heuristic (Ghallab and
Allard, 1982) bounds our search error by L*γ−�a�.3
</bodyText>
<sectionHeader confidence="0.975526" genericHeader="method">
3 Bitext Parsing
</sectionHeader>
<bodyText confidence="0.999917444444444">
In bitext parsing, one jointly infers a synchronous
phrase structure tree over a sentence ws and its
translation wt (Melamed et al., 2004; Wu, 1997).
Bitext parsing is a natural candidate task for our
approximate factoring technique. A synchronous
tree projects monolingual phrase structure trees onto
each sentence. However, the costs assigned by
a weighted synchronous grammar (WSG) G do
not typically factor into independent monolingual
WCFGs. We can, however, produce a useful surro-
gate: a pair of monolingual WCFGs with structures
projected by G and weights that, when combined,
underestimate the costs of G.
Parsing optimally relative to a synchronous gram-
mar using a dynamic program requires time O(n6)
in the length of the sentence (Wu, 1997). This high
degree of complexity makes exhaustive bitext pars-
ing infeasible for all but the shortest sentences. In
</bodyText>
<footnote confidence="0.547366">
3This bound may be very loose if L is large.
</footnote>
<bodyText confidence="0.9805">
contrast, monolingual CFG parsing requires time
O(n3) in the length of the sentence.
</bodyText>
<subsectionHeader confidence="0.997974">
3.1 A* Parsing
</subsectionHeader>
<bodyText confidence="0.997886789473684">
Alternatively, we can search for an optimal parse
guided by a heuristic. The states in A* bitext pars-
ing are rooted bispans, denoted X [i, j] :: Y [k, l].
States represent a joint parse over subspans [i, j] of
ws and [k, l] of wt rooted by the nonterminals X and
Y respectively.
Given a WSG G, the algorithm prioritizes a state
(or edge) e by the sum of its inside cost βg(e) (the
negative log of its inside probability) and its outside
estimate h(e), or completion cost.4 We are guaran-
teed the optimal parse if our heuristic h(e) is never
greater than αg(e), the true outside cost of e.
We now consider a heuristic combining the com-
pletion costs of the monolingual projections of G,
and guarantee admissibility by enforcing point-wise
admissibility. Each state e = X [i, j] :: Y [k, l]
projects a pair of monolingual rooted spans. The
heuristic we propose sums independent outside costs
of these spans in each monolingual projection.
</bodyText>
<equation confidence="0.971682">
h(e) = αs(X [i, j]) + αt(Y [k, l])
</equation>
<bodyText confidence="0.999848">
These monolingual outside scores are computed rel-
ative to a pair of monolingual WCFG grammars Gs
and Gt given by splitting each synchronous rule
</bodyText>
<equation confidence="0.890492">
r = CY(t) γ δ /
</equation>
<bodyText confidence="0.915412454545455">
into its components πs(r) = X→ αβ and πt(r) =
Y→γδ and weighting them via optimized φs(r) and
φt(r), respectively.5
To learn pointwise admissible costs for the mono-
lingual grammars, we formulate the following opti-
mization problem:6
minimize kγk1
γ,φ.,φt
such that, γr = c(r) − [φs(r) + φt(r)]
for all synchronous rules r ∈ G
φs ≥ 0,φt ≥ 0,γ ≥ 0
</bodyText>
<footnote confidence="0.993645">
4All inside and outside costs are Viterbi, not summed.
5Note that we need only parse each sentence (monolin-
gually) once to compute the outside probabilities for every span.
6The stated objective is merely one reasonable choice
among many possibilities which require pointwise admissibil-
ity and encourage tight estimates.
</footnote>
<page confidence="0.992299">
415
</page>
<figureCaption confidence="0.966286333333333">
Figure 2: The gap between the heuristic (left) and true comple-
tion cost (right) comes from relaxing the synchronized problem
to independent subproblems and slack in the factored models.
</figureCaption>
<bodyText confidence="0.988053142857143">
Figure 2 diagrams the two bounds that enforce the
admissibility of h(e). For any outside cost αg(e),
there is a corresponding optimal completion struc-
ture o under !g, which is an outer shell of a syn-
chronous tree. o projects monolingual completions
os and ot which have well-defined costs cs(os) and
ct(ot) under !9s and !9t respectively. Their sum
cs(os) + ct(ot) will underestimate αg(e) by point-
wise admissibility.
Furthermore, the heuristic we compute underesti-
mates this sum. Recall that the monolingual outside
score αs(X [i, j]) is the minimal costs for any com-
pletion of the edge. Hence, αs(X [i, j]) G cs(os)
and αt(X [k, l]) G ct(ot). Admissibility follows.
</bodyText>
<subsectionHeader confidence="0.960091">
3.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999879375">
We demonstrate our technique using the syn-
chronous grammar formalism of tree-to-tree trans-
ducers (Knight and Graehl, 2004). In each weighted
rule, an aligned pair of nonterminals generates two
ordered lists of children. The non-terminals in each
list must align one-to-one to the non-terminals in the
other, while the terminals are placed freely on either
side. Figure 3(a) shows an example rule.
Following Galley et al. (2004), we learn a gram-
mar by projecting English syntax onto a foreign lan-
guage via word-level alignments, as in figure 3(b).7
We parsed 1200 English-Spanish sentences using
a grammar learned from 40,000 sentence pairs of
the English-Spanish Europarl corpus.8 Figure 4(a)
shows that A* expands substantially fewer states
while searching for the optimal parse with our op-
</bodyText>
<footnote confidence="0.99367">
7The bilingual corpus consists of translation pairs with fixed
English parses and word alignments. Rules were scored by their
relative frequencies.
8Rare words were replaced with their parts of speech to limit
the memory consumption of the parser.
</footnote>
<figure confidence="0.983956222222222">
(a) \ NP(s) NN(s) NNS(s) / � 2
NP(t) NNS(t) 2de NN(t)
1
NP
NN NNS
traduccion
funcionan
a
veces
</figure>
<figureCaption confidence="0.9994325">
Figure 3: (a) A tree-to-tree transducer rule. (b) An example
training sentence pair that yields rule (a).
</figureCaption>
<bodyText confidence="0.997062878787879">
timization heuristic. The exhaustive curve shows
edge expansions using the null heuristic. The in-
termediate result, labeled English only, used only
the English monolingual outside score as a heuris-
tic. Similar results using only Spanish demonstrate
that both projections contribute to parsing efficiency.
All three curves in figure 4 represent running times
for finding the optimal parse.
Zhang and Gildea (2006) offer a different heuris-
tic for A* parsing of ITG grammars that provides a
forward estimate of the cost of aligning the unparsed
words in both sentences. We cannot directly apply
this technique to our grammar because tree-to-tree
transducers only align non-terminals. Instead, we
can augment our synchronous grammar model to in-
clude a lexical alignment component, then employ
both heuristics. We learned the following two-stage
generative model: a tree-to-tree transducer generates
trees whose leaves are parts of speech. Then, the
words of each sentence are generated, either jointly
from aligned parts of speech or independently given
a null alignment. The cost of a complete parse un-
der this new model decomposes into the cost of the
synchronous tree over parts of speech and the cost
of generating the lexical items.
Given such a model, both our optimization heuris-
tic and the lexical heuristic of Zhang and Gildea
(2006) can be computed independently. Crucially,
the sum of these heuristics is still admissible. Re-
sults appear in figure 4(b). Both heuristics (lexi-
cal and optimization) alone improve parsing perfor-
mance, but their sum opt+lex substantially improves
upon either one.
</bodyText>
<figure confidence="0.996288153846154">
Monolingual completions Synchronized completion Synchronized completion
scored by factored model scored by factored model scored by original model
i j k l i j k l i j k l
Cost under 9.
� �
Cost under 9t Cost under g
NP
NNS
NN
sistemas
de
S
RB VB
</figure>
<page confidence="0.569793">
416
</page>
<figure confidence="0.999127396551724">
200
150
p
p
d
Exhaustive
Exha
English Only
Engis
Optimization
Optim
(b)
Edg
.5
50 .
0
a
50
ht
th
s
100
es
5 7 9 911 13 315
Sentence lgth
Sentence l
0
5 7 9 1 1 131 15
nc lgth
Sentence length
(a)
Avg. Edges Popped
(in thousands)
0
0
50
200
5
p
150
Avg. Edges Popped
0
e
0
g.
i
100
50
(in thousands)
zation
Optim
exO
Exh
tive
austive
Lexical
ization
pt+Lex
</figure>
<figureCaption confidence="0.9638795">
Figure 4: (a) Parsing efficiency results with optimization heuristics show that both component projections constrain the problem.
(b) Including a lexical model and corresponding heuristic further increases parsing efficiency.
</figureCaption>
<sectionHeader confidence="0.960923" genericHeader="method">
4 Lexicalized Parsing
</sectionHeader>
<bodyText confidence="0.999660684210526">
We next apply our technique to lexicalized pars-
ing (Charniak, 1997; Collins, 1999). In lexical-
ized parsing, the local configurations are lexicalized
rules of the form X[h, t] —* Y [h&apos;, t&apos;] Z[h, t], where
h, t, h&apos;, and t&apos; are the head word, head tag, ar-
gument word, and argument tag, respectively. We
will use r = X —* Y Z to refer to the CFG back-
bone of a lexicalized rule. As in Klein and Man-
ning (2003), we view each lexicalized rule, e, as
having a CFG projection, 7r,(e) = r, and a de-
pendency projection, 7rd(e) = (h, t, h&apos;, t&apos;)(see fig-
ure 5).9 Broadly, the CFG projection encodes con-
stituency structure, while the dependency projection
encodes lexical selection, and both projections are
asymptotically more efficient than the original prob-
lem. Klein and Manning (2003) present a factored
model where the CFG and dependency projections
are generated independently (though with compati-
ble bracketing):
</bodyText>
<equation confidence="0.9972775">
P(Y [h, t]Z[h&apos;, t&apos;]  |X[h, t]) = (6)
P(Y Z|X)P(h&apos;, t&apos;|t, h)
</equation>
<bodyText confidence="0.999596333333333">
In this work, we explore the following non-factored
model, which allows correlations between the CFG
and dependency projections:
</bodyText>
<equation confidence="0.829891">
P(Y [h, t]Z[h&apos;, t&apos;]  |X[h, t]) = P(Y Z|X, t, h) (7)
P(t&apos;|t, Z, h&apos;, h) P(h&apos;|t&apos;, t, Z, h&apos;, h)
</equation>
<bodyText confidence="0.829369">
This model is broadly representative of the suc-
cessful lexicalized models of Charniak (1997) and
</bodyText>
<footnote confidence="0.855105">
9We assume information about the distance and direction of
the dependency is encoded in the dependency tuple, but we omit
it from the notation for compactness.
Collins (1999), though simpler.10
</footnote>
<subsectionHeader confidence="0.9972325">
4.1 Choosing Constraints and Handling
Unseen Dependencies
</subsectionHeader>
<bodyText confidence="0.999665827586207">
Ideally we would like to be able to solve the op-
timization problem in (4) for this task. Unfortu-
nately, exhaustively listing all possible configura-
tions (lexical rules) yields an impractical number of
constraints. We therefore solve a relaxed problem in
which we enforce the constraints for only a subset
of the possible configurations, A&apos; C A. Once we
start dropping constraints, we can no longer guaran-
tee pointwise admissibility, and therefore there is no
reason not to also allow penalized violations of the
constraints we do list, so we solve (5) instead.
To generate the set of enforced constraints, we
first include all configurations observed in the gold
training trees. We then sample novel configurations
by choosing (X, h, t) from the training distribution
and then using the model to generate the rest of the
configuration. In our experiments, we ended up with
434,329 observed configurations, and sampled the
same number of novel configurations. Our penalty
multiplier C was 10.
Even if we supplement our training set with many
sample configurations, we will still see new pro-
jected dependency configurations at test time. It is
therefore necessary to generalize scores from train-
ing configurations to unseen ones. We enrich our
procedure by expressing the projected configuration
costs as linear functions of features. Specifically, we
define feature vectors f,(r) and fd(h, t, h&apos;t&apos;) over
the CFG and dependency projections, and intro-
</bodyText>
<footnote confidence="0.714278333333333">
10All probability distributions for the non-factored model are
estimated by Witten-Bell smoothing (Witten and Bell, 1991)
where conditioning lexical items are backed off first.
</footnote>
<page confidence="0.993379">
417
</page>
<figure confidence="0.999762096153846">
These
stocks
eventually-RB
eventually
reopened-VBD
reopened
NP
� ���
� �
NPNP
PP
VBD
�� � ���
DT
NPPP
NNS
reopened
RB
DT
NNS
VBD
These
stocks
reopened
eventually
S
s / VPS
P
stocks-NNS
These-DT
VPS, reopened-VBD
ADVPS, evenNxlly-ae
cks
o
NNS
NPS,
\
reopened-VBD
��� �� � �� � ��������
�
reop
ened
-VBD
��� � ����
S, reopened-VBD
��������� �� ���������
These stocks RB
eventually
Actual Cost: 18.7
Best Projected CFG Cost: 4.1 Best Projected Dep. Cost: 9.5 CFG Projection Cost: 6.9
Dep. Projection Cost: 11.1
(a) (b) (c)
</figure>
<figureCaption confidence="0.983473333333333">
Figure 5: Lexicalized parsing projections. The figure in (a) is the optimal CFG projection solution and the figure in (b) is the
optimal dependency projection solution. The tree in (c) is the optimal solution for the original problem. Note that the sum of the
CFG and dependency projections is a lower bound (albeit a fairly tight one) on actual solution cost.
</figureCaption>
<bodyText confidence="0.997067">
duce corresponding weight vectors wc and wd. The
weight vectors are learned by solving the following
optimization problem:
</bodyText>
<equation confidence="0.986223">
minimize 11Y+112 + C11Y 112 (8)
γ,wc,wd
such that, wc &gt; 0, wd &gt; 0
Y` = c(e) − [wTc fc(r) + wTd fd(h, t, h&apos;, t&apos;)�
for e = (r, h, t, h&apos;, t&apos;) E X
</equation>
<bodyText confidence="0.999884333333333">
Our CFG feature vector has only indicator features
for the specific rule. However, our dependency fea-
ture vector consists of an indicator feature of the tu-
ple (h, t, h&apos;, t&apos;) (including direction), an indicator of
the part-of-speech type (t, t&apos;) (also including direc-
tion), as well as a bias feature.
</bodyText>
<subsectionHeader confidence="0.951192">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999920714285714">
We tested our approximate projection heuristic on
two lexicalized parsing models. The first is the fac-
tored model of Klein and Manning (2003), given
by equation (6), and the second is the non-factored
model described in equation (7). Both models
use the same parent-annotated head-binarized CFG
backbone and a basic dependency projection which
models direction, but not distance or valence.11
In each case, we compared A* using our approxi-
mate projection heuristics to exhaustive search. We
measure efficiency in terms of the number of ex-
panded hypotheses (edges popped); see figure 6.12
In both settings, the factored A* approach substan-
tially outperforms exhaustive search. For the fac-
</bodyText>
<footnote confidence="0.9487668">
11The CFG and dependency projections correspond to the
PCFG-PA and DEP-BASIC settings in Klein and Manning
(2003).
12All models are trained on section 2 through 21 of the En-
glish Penn treebank, and tested on section 23.
</footnote>
<bodyText confidence="0.998462">
tored model of Klein and Manning (2003), we can
also compare our reconstructed bound to the known
tight bound which would result from solving the
pointwise admissible problem in (4) with all con-
straints. As figure 6 shows, the exact factored
heuristic does outperform our approximate factored
heuristic, primarily because of many looser, backed-
off cost estimates for unseen dependency tuples. For
the non-factored model, we compared our approxi-
mate factored heuristic to one which only bounds the
CFG projection as suggested by Klein and Manning
(2003). They suggest,
</bodyText>
<equation confidence="0.962889">
�c(r) = min
`EA:πc(`)=r
N
</equation>
<bodyText confidence="0.999964523809524">
where we obtain factored CFG costs by minimizing
over dependency projections. As figure 6 illustrates,
this CFG only heuristic is substantially less efficient
than our heuristic which bounds both projections.
Since our heuristic is no longer guaranteed to be
admissible, we evaluated its effect on search in sev-
eral ways. The first is to check for search errors,
where the model-optimal parse is not found. In the
case of the factored model, we can find the optimal
parse using the exact factored heuristic and compare
it to the parse found by our learned heuristic. In our
test set, the approximate projection heuristic failed
to return the model optimal parse in less than 1% of
sentences. Of these search errors, none of the costs
were more than 0.1% greater than the model optimal
cost in negative log-likelihood. For the non-factored
model, the model optimal parse is known only for
shorter sentences which can be parsed exhaustively.
For these sentences up to length 15, there were no
search errors. We can also check for violations of
pointwise admissibility for configurations encoun-
</bodyText>
<equation confidence="0.429503">
c(e)
</equation>
<page confidence="0.734811">
418
</page>
<figure confidence="0.999607807692308">
Avg. Edges Popped
(in thousands)
200
150
100
50
0
5 10 15 20 25 30 35 40
Sentence length
Exhaustive
CFG Only
Approx. Factored
200
150
100
50
0
5 10 15 20 25 30 35 40
Sentence length
(b)
(a)
Avg. Edges Popped
(in thousands)
Exhaustive
Approx. Factored
Exact Factored
</figure>
<figureCaption confidence="0.989503">
Figure 6: Edges popped by exhaustive versus factored A* search. The chart in (a) is using the factored lexicalized model from
Klein and Manning (2003). The chart in (b) is using the non-factored lexicalized model described in section 4.
</figureCaption>
<bodyText confidence="0.974808">
200
tered during search. For both the factored and non-
factored model, less than 2% of the configurations
scored by the approximate projection heuristic dur-
ing search violated pointwise admissibility.
While this is a paper about inference, we also
measured the accuracy in the standard way, on sen-
tences of length up to 40, using EVALB. The fac-
tored model with the approximate projection heuris-
tic achieves an F1 of 82.2, matching the performance
with the exact factored heuristic, though slower. The
non-factored model, using the approximate projec-
tion heuristic, achieves an F1 of 83.8 on the test set,
which is slightly better than the factored model.13
We note that the CFG and dependency projections
are as similar as possible across models, so the in-
crease in accuracy is likely due in part to the non-
factored model’s coupling of CFG and dependency
projections.
</bodyText>
<sectionHeader confidence="0.999597" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.986163466666667">
We have presented a technique for creating A* es-
timates for inference in complex models. Our tech-
nique can be used to generate provably admissible
estimates when all search transitions can be enumer-
ated, and an effective heuristic even for problems
where all transitions cannot be efficiently enumer-
ated. In the future, we plan to investigate alterna-
tive objective functions and error-driven methods for
learning heuristic bounds.
Acknowledgments We would like to thank the
anonymous reviewers for their comments. This
work is supported by a DHS fellowship to the first
13Since we cannot exhaustively parse with this model, we
cannot compare our Fl to an exact search method.
author and a Microsoft new faculty fellowship to the
</bodyText>
<figure confidence="0.977942642857143">
150
to
third author.
Pop
s
dsa
g
References
vg
(
E. D. Andersen and K. D. Andersen. 2000. The MOSEK in-
0
terior point optimizer for linear programming. In H. Frenk
5 10 15 20 25 30 35 40
</figure>
<reference confidence="0.96807055882353">
et al., editor, High Performanc Optimization. Kluwer Aca-
Sentence length
demic Publishers.
Stephen Boyd and Lieven Vandenberghe. 2005. Convex Opti-
mization. Cambridge University Press.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In National Conference on Ar-
tificial Intelligence.
Michael Collins. 1999. Head-driven statistical models for nat-
ural language parsing.
Ariel Felner, Richard Korf, and Sarit Hanan. 2004. Additive
pattern database heuristics. JAIR.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In HLT-NAACL.
Malik Ghallab and Dennis G. Allard. 1982. AE - an efficient
near admissible heuristic search algorithm. In IJCAI.
P. Hart, N. Nilsson, and B. Raphael. 1968. A formal basis for
the heuristic determination of minimum cost paths. In IEEE
Transactions on Systems Science and Cybernetics. IEEE.
Dan Klein and Christopher D. Manning. 2003. Factored A*
search for models over sequences and trees. In IJCAI.
Kevin Knight and Jonathan Graehl. 2004. Training tree trans-
ducers. In HLT-NAACL.
I. Dan Melamed, Giorgio Satta, and Ben Wellington. 2004.
Generalized multitext grammars. In ACL.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In ACL.
Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events in
adaptive text compression. IEEE.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Comput. Linguist.
Hao Zhang and Daniel Gildea. 2006. Efficient search for inver-
sion transduction grammar. In EMNLP.
</reference>
<figure confidence="0.6664515">
P
usa
</figure>
<page confidence="0.990896">
419
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966369">
<title confidence="0.999897">Factoring for Search</title>
<author confidence="0.997311">Aria Haghighi</author>
<author confidence="0.997311">John DeNero</author>
<author confidence="0.997311">Dan</author>
<affiliation confidence="0.999628">Computer Science University of California</affiliation>
<email confidence="0.986672">denero,</email>
<abstract confidence="0.998551923076923">present a novel method for creating estimates for structured search problems. In our approach, we project a complex model onto multiple simpler models for which exact inference is efficient. We use an optimization framework to estimate parameters for these projections in a way which bounds the true costs. Similar to Klein and Manning (2003), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and lexicalized parsing, demonstrating its effectiveness in these domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>High Performanc Optimization. Kluwer AcaSentence length demic Publishers.</booktitle>
<editor>et al., editor,</editor>
<marker></marker>
<rawString>et al., editor, High Performanc Optimization. Kluwer AcaSentence length demic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boyd</author>
<author>Lieven Vandenberghe</author>
</authors>
<title>Convex Optimization.</title>
<date>2005</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2244" citStr="Boyd and Vandenberghe, 2005" startWordPosition="353" endWordPosition="357">mple, Klein and Manning (2003) show a class of projection-based A* estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A* estimates for models which do not factor in this restricted way. Like Klein and Manning (2003), we focus on search problems where there are multiple projections or “views” of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use general optimization techniques (Boyd and Vandenberghe, 2005) to approximately factor a model over these projections. Solutions to the projected problems yield heuristics for the original model. This approach is flexible, providing either admissible or nearly admissible heuristics, depending on the details of the optimization problem solved. Furthermore, our approach allows a modeler explicit control over the trade-off between the tightness of a heuristic and its degree of inadmissibility (if any). We describe our technique in general and then apply it to two concrete NLP search tasks: bitext parsing and lexicalized monolingual parsing. 2 General Approa</context>
</contexts>
<marker>Boyd, Vandenberghe, 2005</marker>
<rawString>Stephen Boyd and Lieven Vandenberghe. 2005. Convex Optimization. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="20099" citStr="Charniak, 1997" startWordPosition="3417" endWordPosition="3418">zation Optim (b) Edg .5 50 . 0 a 50 ht th s 100 es 5 7 9 911 13 315 Sentence lgth Sentence l 0 5 7 9 1 1 131 15 nc lgth Sentence length (a) Avg. Edges Popped (in thousands) 0 0 50 200 5 p 150 Avg. Edges Popped 0 e 0 g. i 100 50 (in thousands) zation Optim exO Exh tive austive Lexical ization pt+Lex Figure 4: (a) Parsing efficiency results with optimization heuristics show that both component projections constrain the problem. (b) Including a lexical model and corresponding heuristic further increases parsing efficiency. 4 Lexicalized Parsing We next apply our technique to lexicalized parsing (Charniak, 1997; Collins, 1999). In lexicalized parsing, the local configurations are lexicalized rules of the form X[h, t] —* Y [h&apos;, t&apos;] Z[h, t], where h, t, h&apos;, and t&apos; are the head word, head tag, argument word, and argument tag, respectively. We will use r = X —* Y Z to refer to the CFG backbone of a lexicalized rule. As in Klein and Manning (2003), we view each lexicalized rule, e, as having a CFG projection, 7r,(e) = r, and a dependency projection, 7rd(e) = (h, t, h&apos;, t&apos;)(see figure 5).9 Broadly, the CFG projection encodes constituency structure, while the dependency projection encodes lexical selection</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<contexts>
<context position="1506" citStr="Collins, 1999" startWordPosition="236" endWordPosition="237">plex models, selecting the highest scoring output for a given observation is slow or even intractable. One general technique to increase efficiency while preserving optimality is A* search (Hart et al., 1968); however, successfully using A* search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and Manning (2003) show a class of projection-based A* estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A* estimates for models which do not factor in this restricted way. Like Klein and Manning (2003), we focus on search problems where there are multiple projections or “views” of the structure, for example lexical parsing, in which trees can be p</context>
<context position="20115" citStr="Collins, 1999" startWordPosition="3419" endWordPosition="3420"> Edg .5 50 . 0 a 50 ht th s 100 es 5 7 9 911 13 315 Sentence lgth Sentence l 0 5 7 9 1 1 131 15 nc lgth Sentence length (a) Avg. Edges Popped (in thousands) 0 0 50 200 5 p 150 Avg. Edges Popped 0 e 0 g. i 100 50 (in thousands) zation Optim exO Exh tive austive Lexical ization pt+Lex Figure 4: (a) Parsing efficiency results with optimization heuristics show that both component projections constrain the problem. (b) Including a lexical model and corresponding heuristic further increases parsing efficiency. 4 Lexicalized Parsing We next apply our technique to lexicalized parsing (Charniak, 1997; Collins, 1999). In lexicalized parsing, the local configurations are lexicalized rules of the form X[h, t] —* Y [h&apos;, t&apos;] Z[h, t], where h, t, h&apos;, and t&apos; are the head word, head tag, argument word, and argument tag, respectively. We will use r = X —* Y Z to refer to the CFG backbone of a lexicalized rule. As in Klein and Manning (2003), we view each lexicalized rule, e, as having a CFG projection, 7r,(e) = r, and a dependency projection, 7rd(e) = (h, t, h&apos;, t&apos;)(see figure 5).9 Broadly, the CFG projection encodes constituency structure, while the dependency projection encodes lexical selection, and both proje</context>
<context position="21489" citStr="Collins (1999)" startWordPosition="3657" endWordPosition="3658">re generated independently (though with compatible bracketing): P(Y [h, t]Z[h&apos;, t&apos;] |X[h, t]) = (6) P(Y Z|X)P(h&apos;, t&apos;|t, h) In this work, we explore the following non-factored model, which allows correlations between the CFG and dependency projections: P(Y [h, t]Z[h&apos;, t&apos;] |X[h, t]) = P(Y Z|X, t, h) (7) P(t&apos;|t, Z, h&apos;, h) P(h&apos;|t&apos;, t, Z, h&apos;, h) This model is broadly representative of the successful lexicalized models of Charniak (1997) and 9We assume information about the distance and direction of the dependency is encoded in the dependency tuple, but we omit it from the notation for compactness. Collins (1999), though simpler.10 4.1 Choosing Constraints and Handling Unseen Dependencies Ideally we would like to be able to solve the optimization problem in (4) for this task. Unfortunately, exhaustively listing all possible configurations (lexical rules) yields an impractical number of constraints. We therefore solve a relaxed problem in which we enforce the constraints for only a subset of the possible configurations, A&apos; C A. Once we start dropping constraints, we can no longer guarantee pointwise admissibility, and therefore there is no reason not to also allow penalized violations of the constraint</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel Felner</author>
<author>Richard Korf</author>
<author>Sarit Hanan</author>
</authors>
<title>Additive pattern database heuristics.</title>
<date>2004</date>
<publisher>JAIR.</publisher>
<contexts>
<context position="7412" citStr="Felner et al., 2004" startWordPosition="1272" endWordPosition="1275">case, c (a) = �` ci(a) , ba E A (1) i=1 where A is the set of local configurations and ci(a) represents the cost of configuration a under projection 7ri. A toy example of such a cost decomposition in the context of a Markov process over two-part states is shown in figure 1(b), where the costs of the joint transitions equal the sum of costs of their projections. Under the strong assumption of equation (1), Klein and Manning (2003) give an admissible A* bound. They note that the cost of a path decomposes as a sum of projected path costs. Hence, the following is an admissible additive heuristic (Felner et al., 2004), h*i (s) (2) where h*i (s) denote the optimal completion costs in the projected search graph 7ri(G). That is, the completion cost of a state bounds the sum of the completion costs in each projection. In virtually all cases, however, configuration costs will not decompose over projections, nor would we expect them to. For instance, in our joint POS-NER task, this assumption requires that the POS and NER �` i=1 h(s) = 413 transitions and observations be generated independently. This independence assumption undermines the motivation for assuming a joint model. In the central contribution of this</context>
</contexts>
<marker>Felner, Korf, Hanan, 2004</marker>
<rawString>Ariel Felner, Richard Korf, and Sarit Hanan. 2004. Additive pattern database heuristics. JAIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLT-NAACL. Malik Ghallab and</booktitle>
<contexts>
<context position="16766" citStr="Galley et al. (2004)" startWordPosition="2860" endWordPosition="2863">t the monolingual outside score αs(X [i, j]) is the minimal costs for any completion of the edge. Hence, αs(X [i, j]) G cs(os) and αt(X [k, l]) G ct(ot). Admissibility follows. 3.2 Experiments We demonstrate our technique using the synchronous grammar formalism of tree-to-tree transducers (Knight and Graehl, 2004). In each weighted rule, an aligned pair of nonterminals generates two ordered lists of children. The non-terminals in each list must align one-to-one to the non-terminals in the other, while the terminals are placed freely on either side. Figure 3(a) shows an example rule. Following Galley et al. (2004), we learn a grammar by projecting English syntax onto a foreign language via word-level alignments, as in figure 3(b).7 We parsed 1200 English-Spanish sentences using a grammar learned from 40,000 sentence pairs of the English-Spanish Europarl corpus.8 Figure 4(a) shows that A* expands substantially fewer states while searching for the optimal parse with our op7The bilingual corpus consists of translation pairs with fixed English parses and word alignments. Rules were scored by their relative frequencies. 8Rare words were replaced with their parts of speech to limit the memory consumption of </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLT-NAACL. Malik Ghallab and Dennis G. Allard. 1982. AE - an efficient near admissible heuristic search algorithm. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hart</author>
<author>N Nilsson</author>
<author>B Raphael</author>
</authors>
<title>A formal basis for the heuristic determination of minimum cost paths.</title>
<date>1968</date>
<booktitle>In IEEE Transactions on Systems Science and Cybernetics.</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="1100" citStr="Hart et al., 1968" startWordPosition="169" endWordPosition="172">e true costs. Similar to Klein and Manning (2003), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and lexicalized parsing, demonstrating its effectiveness in these domains. 1 Introduction Inference tasks in NLP often involve searching for an optimal output from a large set of structured outputs. For many complex models, selecting the highest scoring output for a given observation is slow or even intractable. One general technique to increase efficiency while preserving optimality is A* search (Hart et al., 1968); however, successfully using A* search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and Manning (2003) show a class of projection-based A* estimates, but th</context>
</contexts>
<marker>Hart, Nilsson, Raphael, 1968</marker>
<rawString>P. Hart, N. Nilsson, and B. Raphael. 1968. A formal basis for the heuristic determination of minimum cost paths. In IEEE Transactions on Systems Science and Cybernetics. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Factored A* search for models over sequences and trees.</title>
<date>2003</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="1646" citStr="Klein and Manning (2003)" startWordPosition="257" endWordPosition="260">crease efficiency while preserving optimality is A* search (Hart et al., 1968); however, successfully using A* search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and Manning (2003) show a class of projection-based A* estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A* estimates for models which do not factor in this restricted way. Like Klein and Manning (2003), we focus on search problems where there are multiple projections or “views” of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use general optimization techniques (Boyd and Vandenberghe, 2005) t</context>
<context position="5576" citStr="Klein and Manning (2003)" startWordPosition="964" endWordPosition="967">he cost of an edge is typically a negative log probability which depends only on some local configuration type. For instance, in PCFG parsing, the (hyper)edges reference anchored spans X[i, j], but the edge costs depend only on the local rule type X —* Y Z. We will 1 use a to refer to a local configuration and use c(a) to refer to its cost. Because edge costs are sensitive only to local configurations, the cost of a path is Ea c(a). A* search requires a heuristic function, which is an estimate h(s) of the completion cost, the cost of a best path from state s to a goal. In this work, following Klein and Manning (2003), we consider problems with projections or “views,” which define mappings to simpler state and configuration spaces. For instance, suppose that we are using an HMM to jointly model part-of-speech (POS) and named-entity-recognition (NER) tagging. There might be one projection onto the NER component and another onto the POS component. Formally, a projection 7r is a mapping from states to some coarser domain. A state projection induces projections of edges and of the entire graph 7r(G). We are particularly interested in search problems with multiple projections 17r1, ... , 7r`l where each project</context>
<context position="7225" citStr="Klein and Manning (2003)" startWordPosition="1238" endWordPosition="1241">ions 3 and 4). In defining projections, we have not yet dealt with the projected scoring function. Suppose that the cost of local configurations decomposes along projections as well. In this case, c (a) = �` ci(a) , ba E A (1) i=1 where A is the set of local configurations and ci(a) represents the cost of configuration a under projection 7ri. A toy example of such a cost decomposition in the context of a Markov process over two-part states is shown in figure 1(b), where the costs of the joint transitions equal the sum of costs of their projections. Under the strong assumption of equation (1), Klein and Manning (2003) give an admissible A* bound. They note that the cost of a path decomposes as a sum of projected path costs. Hence, the following is an admissible additive heuristic (Felner et al., 2004), h*i (s) (2) where h*i (s) denote the optimal completion costs in the projected search graph 7ri(G). That is, the completion cost of a state bounds the sum of the completion costs in each projection. In virtually all cases, however, configuration costs will not decompose over projections, nor would we expect them to. For instance, in our joint POS-NER task, this assumption requires that the POS and NER �` i=1</context>
<context position="20437" citStr="Klein and Manning (2003)" startWordPosition="3483" endWordPosition="3487">ency results with optimization heuristics show that both component projections constrain the problem. (b) Including a lexical model and corresponding heuristic further increases parsing efficiency. 4 Lexicalized Parsing We next apply our technique to lexicalized parsing (Charniak, 1997; Collins, 1999). In lexicalized parsing, the local configurations are lexicalized rules of the form X[h, t] —* Y [h&apos;, t&apos;] Z[h, t], where h, t, h&apos;, and t&apos; are the head word, head tag, argument word, and argument tag, respectively. We will use r = X —* Y Z to refer to the CFG backbone of a lexicalized rule. As in Klein and Manning (2003), we view each lexicalized rule, e, as having a CFG projection, 7r,(e) = r, and a dependency projection, 7rd(e) = (h, t, h&apos;, t&apos;)(see figure 5).9 Broadly, the CFG projection encodes constituency structure, while the dependency projection encodes lexical selection, and both projections are asymptotically more efficient than the original problem. Klein and Manning (2003) present a factored model where the CFG and dependency projections are generated independently (though with compatible bracketing): P(Y [h, t]Z[h&apos;, t&apos;] |X[h, t]) = (6) P(Y Z|X)P(h&apos;, t&apos;|t, h) In this work, we explore the following </context>
<context position="24809" citStr="Klein and Manning (2003)" startWordPosition="4205" endWordPosition="4208">ation problem: minimize 11Y+112 + C11Y 112 (8) γ,wc,wd such that, wc &gt; 0, wd &gt; 0 Y` = c(e) − [wTc fc(r) + wTd fd(h, t, h&apos;, t&apos;)� for e = (r, h, t, h&apos;, t&apos;) E X Our CFG feature vector has only indicator features for the specific rule. However, our dependency feature vector consists of an indicator feature of the tuple (h, t, h&apos;, t&apos;) (including direction), an indicator of the part-of-speech type (t, t&apos;) (also including direction), as well as a bias feature. 4.2 Experimental Results We tested our approximate projection heuristic on two lexicalized parsing models. The first is the factored model of Klein and Manning (2003), given by equation (6), and the second is the non-factored model described in equation (7). Both models use the same parent-annotated head-binarized CFG backbone and a basic dependency projection which models direction, but not distance or valence.11 In each case, we compared A* using our approximate projection heuristics to exhaustive search. We measure efficiency in terms of the number of expanded hypotheses (edges popped); see figure 6.12 In both settings, the factored A* approach substantially outperforms exhaustive search. For the fac11The CFG and dependency projections correspond to the</context>
<context position="26127" citStr="Klein and Manning (2003)" startWordPosition="4413" endWordPosition="4416">on 2 through 21 of the English Penn treebank, and tested on section 23. tored model of Klein and Manning (2003), we can also compare our reconstructed bound to the known tight bound which would result from solving the pointwise admissible problem in (4) with all constraints. As figure 6 shows, the exact factored heuristic does outperform our approximate factored heuristic, primarily because of many looser, backedoff cost estimates for unseen dependency tuples. For the non-factored model, we compared our approximate factored heuristic to one which only bounds the CFG projection as suggested by Klein and Manning (2003). They suggest, �c(r) = min `EA:πc(`)=r N where we obtain factored CFG costs by minimizing over dependency projections. As figure 6 illustrates, this CFG only heuristic is substantially less efficient than our heuristic which bounds both projections. Since our heuristic is no longer guaranteed to be admissible, we evaluated its effect on search in several ways. The first is to check for search errors, where the model-optimal parse is not found. In the case of the factored model, we can find the optimal parse using the exact factored heuristic and compare it to the parse found by our learned he</context>
<context position="27685" citStr="Klein and Manning (2003)" startWordPosition="4678" endWordPosition="4681">ter sentences which can be parsed exhaustively. For these sentences up to length 15, there were no search errors. We can also check for violations of pointwise admissibility for configurations encounc(e) 418 Avg. Edges Popped (in thousands) 200 150 100 50 0 5 10 15 20 25 30 35 40 Sentence length Exhaustive CFG Only Approx. Factored 200 150 100 50 0 5 10 15 20 25 30 35 40 Sentence length (b) (a) Avg. Edges Popped (in thousands) Exhaustive Approx. Factored Exact Factored Figure 6: Edges popped by exhaustive versus factored A* search. The chart in (a) is using the factored lexicalized model from Klein and Manning (2003). The chart in (b) is using the non-factored lexicalized model described in section 4. 200 tered during search. For both the factored and nonfactored model, less than 2% of the configurations scored by the approximate projection heuristic during search violated pointwise admissibility. While this is a paper about inference, we also measured the accuracy in the standard way, on sentences of length up to 40, using EVALB. The factored model with the approximate projection heuristic achieves an F1 of 82.2, matching the performance with the exact factored heuristic, though slower. The non-factored </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Factored A* search for models over sequences and trees. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="16461" citStr="Knight and Graehl, 2004" startWordPosition="2811" endWordPosition="2814">er shell of a synchronous tree. o projects monolingual completions os and ot which have well-defined costs cs(os) and ct(ot) under !9s and !9t respectively. Their sum cs(os) + ct(ot) will underestimate αg(e) by pointwise admissibility. Furthermore, the heuristic we compute underestimates this sum. Recall that the monolingual outside score αs(X [i, j]) is the minimal costs for any completion of the edge. Hence, αs(X [i, j]) G cs(os) and αt(X [k, l]) G ct(ot). Admissibility follows. 3.2 Experiments We demonstrate our technique using the synchronous grammar formalism of tree-to-tree transducers (Knight and Graehl, 2004). In each weighted rule, an aligned pair of nonterminals generates two ordered lists of children. The non-terminals in each list must align one-to-one to the non-terminals in the other, while the terminals are placed freely on either side. Figure 3(a) shows an example rule. Following Galley et al. (2004), we learn a grammar by projecting English syntax onto a foreign language via word-level alignments, as in figure 3(b).7 We parsed 1200 English-Spanish sentences using a grammar learned from 40,000 sentence pairs of the English-Spanish Europarl corpus.8 Figure 4(a) shows that A* expands substan</context>
</contexts>
<marker>Knight, Graehl, 2004</marker>
<rawString>Kevin Knight and Jonathan Graehl. 2004. Training tree transducers. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Giorgio Satta</author>
<author>Ben Wellington</author>
</authors>
<title>Generalized multitext grammars.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="12795" citStr="Melamed et al., 2004" startWordPosition="2193" endWordPosition="2196">jected completion costs, we more heavily penalize overestimating the cost by the constant C. 2.2 Bounding Search Error In the case where we allow pointwise inadmissibility, i.e. variables γ−a , we can bound our search error. Suppose γ−max = maxaEA γ−a and that L* is the length of the longest optimal solution for the original problem. Then, h(s) ≤ h*(s) + L*γ−max, ∀s ∈ S. This c-admissible heuristic (Ghallab and Allard, 1982) bounds our search error by L*γ−�a�.3 3 Bitext Parsing In bitext parsing, one jointly infers a synchronous phrase structure tree over a sentence ws and its translation wt (Melamed et al., 2004; Wu, 1997). Bitext parsing is a natural candidate task for our approximate factoring technique. A synchronous tree projects monolingual phrase structure trees onto each sentence. However, the costs assigned by a weighted synchronous grammar (WSG) G do not typically factor into independent monolingual WCFGs. We can, however, produce a useful surrogate: a pair of monolingual WCFGs with structures projected by G and weights that, when combined, underestimate the costs of G. Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6) in the length of the senten</context>
</contexts>
<marker>Melamed, Satta, Wellington, 2004</marker>
<rawString>I. Dan Melamed, Giorgio Satta, and Ben Wellington. 2004. Generalized multitext grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1582" citStr="Och and Ney, 2000" startWordPosition="247" endWordPosition="250"> is slow or even intractable. One general technique to increase efficiency while preserving optimality is A* search (Hart et al., 1968); however, successfully using A* search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and Manning (2003) show a class of projection-based A* estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A* estimates for models which do not factor in this restricted way. Like Klein and Manning (2003), we focus on search problems where there are multiple projections or “views” of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<publisher>IEEE.</publisher>
<contexts>
<context position="23136" citStr="Witten and Bell, 1991" startWordPosition="3911" endWordPosition="3914">igurations. Our penalty multiplier C was 10. Even if we supplement our training set with many sample configurations, we will still see new projected dependency configurations at test time. It is therefore necessary to generalize scores from training configurations to unseen ones. We enrich our procedure by expressing the projected configuration costs as linear functions of features. Specifically, we define feature vectors f,(r) and fd(h, t, h&apos;t&apos;) over the CFG and dependency projections, and intro10All probability distributions for the non-factored model are estimated by Witten-Bell smoothing (Witten and Bell, 1991) where conditioning lexical items are backed off first. 417 These stocks eventually-RB eventually reopened-VBD reopened NP � ��� � � NPNP PP VBD �� � ��� DT NPPP NNS reopened RB DT NNS VBD These stocks reopened eventually S s / VPS P stocks-NNS These-DT VPS, reopened-VBD ADVPS, evenNxlly-ae cks o NNS NPS, \ reopened-VBD ��� �� � �� � �������� � reop ened -VBD ��� � ���� S, reopened-VBD ��������� �� ��������� These stocks RB eventually Actual Cost: 18.7 Best Projected CFG Cost: 4.1 Best Projected Dep. Cost: 9.5 CFG Projection Cost: 6.9 Dep. Projection Cost: 11.1 (a) (b) (c) Figure 5: Lexicalize</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Comput. Linguist.</journal>
<contexts>
<context position="12806" citStr="Wu, 1997" startWordPosition="2197" endWordPosition="2198">s, we more heavily penalize overestimating the cost by the constant C. 2.2 Bounding Search Error In the case where we allow pointwise inadmissibility, i.e. variables γ−a , we can bound our search error. Suppose γ−max = maxaEA γ−a and that L* is the length of the longest optimal solution for the original problem. Then, h(s) ≤ h*(s) + L*γ−max, ∀s ∈ S. This c-admissible heuristic (Ghallab and Allard, 1982) bounds our search error by L*γ−�a�.3 3 Bitext Parsing In bitext parsing, one jointly infers a synchronous phrase structure tree over a sentence ws and its translation wt (Melamed et al., 2004; Wu, 1997). Bitext parsing is a natural candidate task for our approximate factoring technique. A synchronous tree projects monolingual phrase structure trees onto each sentence. However, the costs assigned by a weighted synchronous grammar (WSG) G do not typically factor into independent monolingual WCFGs. We can, however, produce a useful surrogate: a pair of monolingual WCFGs with structures projected by G and weights that, when combined, underestimate the costs of G. Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6) in the length of the sentence (Wu, 199</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Efficient search for inversion transduction grammar.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1607" citStr="Zhang and Gildea, 2006" startWordPosition="251" endWordPosition="254">tractable. One general technique to increase efficiency while preserving optimality is A* search (Hart et al., 1968); however, successfully using A* search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and Manning (2003) show a class of projection-based A* estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A* estimates for models which do not factor in this restricted way. Like Klein and Manning (2003), we focus on search problems where there are multiple projections or “views” of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use general optimization tec</context>
<context position="17991" citStr="Zhang and Gildea (2006)" startWordPosition="3053" endWordPosition="3056">e parser. (a) \ NP(s) NN(s) NNS(s) / � 2 NP(t) NNS(t) 2de NN(t) 1 NP NN NNS traduccion funcionan a veces Figure 3: (a) A tree-to-tree transducer rule. (b) An example training sentence pair that yields rule (a). timization heuristic. The exhaustive curve shows edge expansions using the null heuristic. The intermediate result, labeled English only, used only the English monolingual outside score as a heuristic. Similar results using only Spanish demonstrate that both projections contribute to parsing efficiency. All three curves in figure 4 represent running times for finding the optimal parse. Zhang and Gildea (2006) offer a different heuristic for A* parsing of ITG grammars that provides a forward estimate of the cost of aligning the unparsed words in both sentences. We cannot directly apply this technique to our grammar because tree-to-tree transducers only align non-terminals. Instead, we can augment our synchronous grammar model to include a lexical alignment component, then employ both heuristics. We learned the following two-stage generative model: a tree-to-tree transducer generates trees whose leaves are parts of speech. Then, the words of each sentence are generated, either jointly from aligned p</context>
</contexts>
<marker>Zhang, Gildea, 2006</marker>
<rawString>Hao Zhang and Daniel Gildea. 2006. Efficient search for inversion transduction grammar. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>