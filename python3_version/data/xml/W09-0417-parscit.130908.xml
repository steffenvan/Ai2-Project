<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009795">
<title confidence="0.906624">
LIMSI’s statistical translation systems for WMT’09
</title>
<author confidence="0.694638">
Alexandre Allauzen, Josep Crego, Aurélien Max and François Yvon
</author>
<affiliation confidence="0.350101">
LIMSI/CNRS and Université Paris-Sud 11, France
</affiliation>
<address confidence="0.618071">
BP 133, 91403 Orsay Cédex
</address>
<email confidence="0.997272">
firstname.lastname@limsi.fr
</email>
<sectionHeader confidence="0.995603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847916666667">
This paper describes our Statistical Ma-
chine Translation systems for the WMT09
(en:fr) shared task. For this evaluation, we
have developed four systems, using two
different MT Toolkits: our primary sub-
mission, in both directions, is based on
Moses, boosted with contextual informa-
tion on phrases, and is contrasted with a
conventional Moses-based system. Addi-
tional contrasts are based on the Ncode
toolkit, one of which uses (part of) the En-
glish/French GigaWord parallel corpus.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999381272727273">
This paper describes our Statistical Machine
Translation systems for the WMT09 (en:fr) shared
task. For this evaluation, we have developed four
systems, using two different MT toolkits: our
primary submission, in both direction, is based
on Moses, boosted with contextual information
on phrases; we also provided a contrast with a
vanilla Moses-based system. Additional contrasts
are based on the N-code decoder, one of which
takes advantage of (part of) the English/French Gi-
gaWord parallel corpus.
</bodyText>
<sectionHeader confidence="0.737473" genericHeader="method">
2 System architecture and resources
</sectionHeader>
<bodyText confidence="0.99968575">
In this section, we describe the main characteris-
tics of the baseline phrase-based systems used in
this evaluation and the resources that were used to
train our models.
</bodyText>
<subsectionHeader confidence="0.953034">
2.1 Pre- and post-processing tools
</subsectionHeader>
<bodyText confidence="0.999957294117647">
All the available textual corpora were processed
and normalized using in-house text processing
tools. Our last year experiments (Déchelotte et
al., 2008) revealed that using better normalization
tools provides a significant reward in BLEU, a fact
that we could observe again this year. The down-
side is the need to post-process our outputs so as
to “detokenize” them for scoring purposes, which
is unfortunately an error-prone process.
Based again on last year’s experiments, our sys-
tems are built in “true case”: the first letter of each
sentence is lowercased when it should be, and the
remaining tokens are left as is.
Finally, the N-code (see 2.5) and the context-
aware (see 3) systems require the source to be
morpho-syntactically analysed. This was per-
formed using the TreeTagger1 for both languages.
</bodyText>
<subsectionHeader confidence="0.999028">
2.2 Alignment and translation models
</subsectionHeader>
<bodyText confidence="0.999547125">
Our baseline translation models (see 2.4 and 2.5)
use all the parallel corpora distributed for this eval-
uation: Europarl V4, news commentary (2006-
2009) and the additional news data, totalling 1.5M
sentences. Our preliminary attempts with larger
translation models using the GigaWord corpus are
reported in section 3.2. All these corpora were
aligned with GIZA++2 using default settings.
</bodyText>
<subsectionHeader confidence="0.998308">
2.3 Language Models
</subsectionHeader>
<bodyText confidence="0.999953333333333">
To train our language models (LMs), we took ad-
vantage of the a priori information that the test
set would be of newspaper/newswire genre. We
</bodyText>
<footnote confidence="0.999978333333333">
1http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger.
2http://www.fjoch.com/GIZA++.html.
</footnote>
<note confidence="0.49365">
Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 100–104,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.725361">
100
</page>
<table confidence="0.999778444444444">
Source Period M. words
News texts 1994-06 3 317
En BN transcripts 2000-07 341
WMT 86
Newswires 1994-07 723
Newspapers 1987-06 486
Fr WEB 2008 23
WMT 46
News-train08 167
</table>
<tableCaption confidence="0.9647505">
Table 1: Corpora used to train the target language
models in English and French.
</tableCaption>
<bodyText confidence="0.999515022222222">
thus built much larger LMs for translating both to
French and to English, and optimized their combi-
nation on the first part of the official development
data (dev2009a).
Corpora and vocabulary Statistics regarding
the training material are summarized in table 1 in
terms of source, time period, and millions of oc-
currences. “WMT” stands for all text provided
for the evaluation. Development sets and the large
training corpora (news-train08 and the GigaWord
corpus) were not included. Altogether, these data
contain a total number of 3.7 billion tokens for En-
glish and 1.4 billion tokens for French.
To estimate such large LMs, a vocabulary was
first defined for both languages by including all to-
kens in the WMT parallel data. This initial vocab-
ulary of 130K words was then extended by adding
the most frequent words observed in the additional
training data. This procedure yielded a vocabulary
of one million words in both languages.
Language model training The training data
were divided into several sets based on dates on
genres (resp. 7 and 9 sets for English and French).
On each set, a standard 4-gram LM was estimated
from the 1M word vocabulary with in-house tools
using absolute discounting interpolated with lower
order models. The resulting LMs were then lin-
early interpolated using interpolation coefficients
chosen so as to minimise perplexity of the devel-
opment set (dev2009a). Due to memory limita-
tions, the final LMs were pruned using perplexity
as pruning criterion.
Out of vocabulary word and perplexity To
evaluate our vocabulary and LMs, we used the of-
ficial devtest and test sets. The out-of-vocabulary
(OOV) rate was drastically reduced by increasing
the vocabulary size, the mean OOV rate decreas-
ing from 2.5% to 0.7%, a trend observed in both
languages.
For French, using a small LM trained on the
&amp;quot;WMT&amp;quot; data only resulted in a perplexity of 301
on the devtest corpus and 299 on the test set. Us-
ing all additional data yielded a large decrease in
perplexity (106 on the devtest and 108 on the test);
again the same trend was observed for English.
</bodyText>
<subsectionHeader confidence="0.991175">
2.4 A Moses baseline
</subsectionHeader>
<bodyText confidence="0.9999787">
Our baseline system was a vanilla phrase-based
system built with Moses (Koehn et al., 2007) us-
ing default settings. Phrases were extracted using
the ’grow-diag-final-and’ heuristics, using a max-
imum phrase length of 7; non-contextual phrase
scores contain the 4 translation model scores, plus
a fixed phrase penalty; 6 additional scores param-
eterize the lexicalized reordering model. Default
decoding options were used (20 alternatives per
phrase, maximum distortion distance of 7, etc.)
</bodyText>
<subsectionHeader confidence="0.998033">
2.5 A N-code baseline
</subsectionHeader>
<bodyText confidence="0.999981095238095">
N-code implements the n-gram-based approach
to Statistical Machine Translation (Mariño et al.,
2006). In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training
such a model requires to reorder source sentences
so as to match the target word order. This is also
performed via a stochastic finite-state reordering
model, which uses part-of-speech information to
generalise reordering patterns beyond lexical reg-
ularities. The reordering model is trained on a ver-
sion of the parallel corpora where the source sen-
tences have been reordered via the unfold heuris-
tics (Crego and Mariño, 2007). A conventional n-
gram language model of the target language pro-
vides the third component of the system.
In all our experiments, we used 4-gram reorder-
ing models and bilingual tuple models built using
Kneser-Ney backoff (Chen and Goodman, 1996).
The maximum tuple size was also set to 7.
</bodyText>
<subsectionHeader confidence="0.998451">
2.6 Tuning procedure
</subsectionHeader>
<bodyText confidence="0.999968166666667">
The Moses-based systems were tuned using the
implementation of minimum error rate train-
ing (MERT) (Och, 2003) distributed with the
Moses decoder, using the development corpus
(dev2009a). For the context-less systems, tun-
ing concerned the 14 usual weights; tuning the
</bodyText>
<page confidence="0.99537">
101
</page>
<bodyText confidence="0.999165">
22 weights of the context-aware systems (see 3.1)
proved to be much more challenging, and the
weights used in our submissions are probably far
from optimal. The N-code systems only rely on
9 weights, since they dispense with the lexical re-
ordering model; these weights were tuned on the
same dataset, using an in-house implementation of
the simplex algorithm.
</bodyText>
<sectionHeader confidence="0.999776" genericHeader="method">
3 Extensions
</sectionHeader>
<subsectionHeader confidence="0.999974">
3.1 A context-aware system
</subsectionHeader>
<bodyText confidence="0.999213710526316">
In phrase-based translation, source phrases are
translated irrespective of their (source) context.
This is often not perceived as a limitation as
(i) typical text domains usually contain only few
senses for polysemous words, thus limiting the
use of word sense disambiguation (WSD); and (ii)
using long-span target language models (4-grams
and more) often capture sufficient context to se-
lect the more appropriate translation for a source
phrase based on the target context. In fact, at-
tempts at using source contexts in phrase-based
SMT have to date failed to show important gains
on standard evaluation test sets (Carpuat and Wu,
2007; Stroppa et al., 2007; Gimpel and Smith,
2008; Max et al., 2008). Importantly, in all con-
ditions where gains have been obtained, the tar-
get language was the “morphologically-poor” En-
glish.
Nonetheless, there seems to be a clear consen-
sus on the importance of better exploiting source
contexts in SMT, so as to improve phrase disam-
biguation. The following sentence extract from
the devtest corpus is a typical example where the
lack of context in our phrase-based system yields
an incorrect translation:
Source: the long weekend comes with a price ...
Target: Le long week-end vient avec un prix ...
(the long weekend comes accompanied by a price)
While grammatically correct, the French trans-
lation sounds unnatural, and getting the correct
meaning requires knowledge of the idiom in the
source language. In such a situation, the right con-
text of the phrase comes with can be successfully
used to propose a better translation.3
From an engineering perspective, integrating
context into phrase-based SMT systems can be
performed by (i) transforming source words into
unique tokens, so as to record the original context
</bodyText>
<footnote confidence="0.9469315">
3Our context-aware phrase-based system indeed proposes
the appropriate translation: Le long week-end a un prix.
</footnote>
<bodyText confidence="0.999662076923077">
of each entry of the phrase table; and by (ii) adding
one or several contextual scores to the phrase ta-
ble. Using standard MERT, the corresponding
weights can be optimized on development data.
A typical contextual score corresponds to
p(e|f, C(f)), where C(f) is some contextual in-
formation about the source phrase f. An exter-
nal disambiguation system can be used to pro-
vide one global context score (Stroppa et al., 2007;
Carpuat and Wu, 2007; Max et al., 2008)); alter-
natively, several scores based on single features
can be estimated using relative frequencies (Gim-
pel and Smith, 2008):
</bodyText>
<equation confidence="0.979570666666667">
count(e, f, C(f))
p(e|f, C(f)) =
Ee, count(e�, f, C(f))
</equation>
<bodyText confidence="0.99979525">
For these experiments, we followed the latter ap-
proach, restricting ourselves to features represent-
ing the local context up to a fixed distance d (using
the values 1 and 2 in our experiments) from the
</bodyText>
<figure confidence="0.831542285714286">
source phrase fend
start:
• lexical context features:
– left context: p(e|f, fstart−1
start−d )
– right context: p(e|f, fend+d
end+1 )
</figure>
<listItem confidence="0.928484">
• shallow syntactic features (denoting tF1 the
sequence of POS tags for the source sen-
tence):
– left context: p(e|f, tstart−1
</listItem>
<equation confidence="0.935701">
start−d)
– right context: p(e|f, tend+d
end+1)
</equation>
<bodyText confidence="0.999870736842105">
As in (Gimpel and Smith, 2008), we filtered out
all translations for which p(e|f) &lt; 0.0002. This
was necessary to make score computation practi-
cal given our available hardware resources.
Results on the devtest corpus for
English—*French were similar for the context-
aware phrase-based and the baseline phrase-based
system; small gains were achieved in the reverse
direction (see Table 2). The same trend was
observed on the test data.
Manual inspection of the output of the base-
line and context-aware systems on the devtest
corpus for English—*French translation confirmed
two facts: (1) performing phrase translation dis-
ambiguation is only useful if a more appropriate
translation has been seen during training ; and (2)
phrase translation disambiguation can capture im-
portant source dependencies that the target lan-
guage model can not recover. The following ex-
</bodyText>
<page confidence="0.997542">
102
</page>
<bodyText confidence="0.963257117647059">
ample, involving an unseen sense4 (ball in the se-
mantic field of dance rather than sports), illus-
trates our first remark:
Source: about 500 people attended the ball.
Baseline : Environ 500 personnes ont assisté à la
balle.
+Context: Environ 500 personnes ont participé à
la balle.
The next example is a case where contextual in-
formation helped selecting an appropriate transla-
tion, in constrast to the baseline system.
Source:... the new method for calculating pen-
sions due to begin next year ...
Baseline :... le nouveau mode de calcul des pen-
sions due à commencer l’année prochaine ...
+Context:... la nouvelle méthode de calcul des
pensions qui va débuter l’année prochaine ...
</bodyText>
<subsectionHeader confidence="0.9883965">
3.2 Preliminary experiments with the
GigaWord parallel corpus
</subsectionHeader>
<bodyText confidence="0.99999532">
One exciting novelty of this year’s campaign was
the availability of a very large parallel corpus for
the en:fr pair, containing about 20M aligned sen-
tences.
Our preliminary work consisted in selecting the
most useful pairs of sentences, based on their av-
erage perplexity, as computed on our develop-
ment language models. The top ranking sen-
tences (about 8M sentences) were then fed into the
usual system development procedure: alignment,
reordering (for the N-code system), phrase pair
extraction, model estimation. Given the unusual
size of this corpus, each of these steps proved
extremely resource intensive, and, for some sys-
tems, actually failed to complete. Contrarily, the
N-code systems, conceptually simpler, proved to
scale nicely.
Given the very late availability of this cor-
pus, our experiments were very limited and we
eventually failed to deliver the test submissions
of our “GigaWord” system. Preliminary exper-
iments using the N-code systems (see Table 2),
however, showed a clear improvement of perfor-
mance. There is no reason to doubt that similar
gains would be observed with the Moses systems.
</bodyText>
<subsectionHeader confidence="0.985376">
3.3 Experiments
</subsectionHeader>
<bodyText confidence="0.996913666666667">
The various systems presented above were all de-
veloped according to the same procedure: train-
ing used all the available parallel text; tuning was
</bodyText>
<footnote confidence="0.8186125">
4This was confirmed after careful inspection of the phrase
tables of the baseline system.
</footnote>
<table confidence="0.999196">
en—*fr fr—*en
Moses Ncode Moses Ncode
small LM 20.06 18.98 21.14 20.41
Large LM 22.93 21.95 22.20 22.28
+context 23.06 22.69
+giga 23.21 23.14
</table>
<tableCaption confidence="0.999845">
Table 2: Results on the devtest set
</tableCaption>
<bodyText confidence="0.999930076923077">
performed on dev2009a (1000 sentences), and our
internal tests were performed on dev2009b (1000
sentences). Results are reported in table 2.
Our primary submission corresponds to
the +context entry, our first contrast to
Moses+LargeLM, and our second contrast to
Ncode+largeLM. Due to lack of time, no official
submission was submitted for the +giga variant.
For the record, the score we eventually obtained
on the test corpus was 26.81, slightly better than
our primary submission which obtained a score of
25.74 (all these numbers were computed on the
complete test set).
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999962458333333">
In this paper, we presented our statistical MT sys-
tems developed for the WMT’09 shared task. We
used last year experiments to build competitive
systems, which greatly benefited from in-house
normalisation and language modeling tools.
One motivation for taking part in this campaign
was to use the GigaWord corpus. Even if time did
not allow us to submit a system based on this data,
it was a interesting opportunity to confront our-
selves with the technical challenge of scaling up
our system development tools to very large paral-
lel corpora. Our preliminary results indicate that
this new resource can actually help improve our
systems.
Naturally, future work includes adapting our
systems so that they can use models learnt from
corpora of the size of the GigaWord corpus. In
parallel, we intend to keep on working on context-
aware systems to study the impact of more types
of scores, e.g. based on grammatical dependencies
as in (Max et al., 2008). Given the difficulties we
had tuning our systems, we feel that a preliminary
task should be improving our tuning tools before
addressing these developments.
</bodyText>
<page confidence="0.999154">
103
</page>
<sectionHeader confidence="0.980424" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996967777777778">
This work was partly realised as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for SMT using context-
informed features. In Proceedings of the 11th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation (TMI’07),
pages 231–240, Skövde, Sweden.
</bodyText>
<sectionHeader confidence="0.99606" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999217590909091">
M. Carpuat and D. Wu. 2007. Context-Dependent
Phrasal Translation Lexicons for Statistical Machine
Translation. In Proceedings of Machine Translation
Summit XI, pages 73–80, Copenhagen, Denmark.
F. Casacuberta and E. Vidal. 2004. Machine transla-
tion with inferred stochastic finite-state transducers.
Computational Linguistics, 30(3):205–225.
S. F. Chen and J. T. Goodman. 1996. An empirical
study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 310–318, Santa Cruz, NM.
J. M. Crego and J. B. Mariño. 2007. Improving
SMT by coupling reordering and decoding. Ma-
chine Translation, 20(3):199–215.
D. Déchelotte, G. Adda, A. Allauzen, O. Galibert, J.-L.
Gauvain, H. Meynard, and F. Yvon. 2008. Limsi’s
statistical translation systems for WMT’08. In Pro-
ceedings of the NAACL-HTL Statistical Machine
Translation Workshop, pages 107-100, Columbus,
Ohio.
K. Gimpel and N. A. Smith. 2008. Rich Source-Side
Context for Statistical Machine Translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 9–17, Columbus, Ohio.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session, Prague, Czech Republic.
A. Max, R. Makhloufi, and P. Langlais. 2008. Explo-
rations in using grammatical dependencies for con-
textual phrase translation disambiguation. In Pro-
ceedings of EAMT, poster session, Hamburg, Ger-
many.
J. B. Mariño, R. E. Banchs R, J.M. Crego, A. de Gis-
pert, P. Lambert, J.A.R. Fonollosa, and M. R. Costa-
Jussà. 2006. N-gram-based machine translation.
Computational Linguistics, 32(4):527–549.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160–167, Sapporo, Japan.
</reference>
<page confidence="0.998772">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.354718">
<note confidence="0.6195305">statistical translation systems for WMT’09 Alexandre Allauzen, Josep Crego, Aurélien Max and François LIMSI/CNRS and Université Paris-Sud 11, BP 133, 91403 Orsay</note>
<email confidence="0.989526">firstname.lastname@limsi.fr</email>
<abstract confidence="0.987944846153846">This paper describes our Statistical Machine Translation systems for the WMT09 (en:fr) shared task. For this evaluation, we have developed four systems, using two different MT Toolkits: our primary submission, in both directions, is based on Moses, boosted with contextual information on phrases, and is contrasted with a conventional Moses-based system. Additional contrasts are based on the Ncode toolkit, one of which uses (part of) the English/French GigaWord parallel corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Context-Dependent Phrasal Translation Lexicons for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of Machine Translation Summit XI,</booktitle>
<pages>73--80</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="8315" citStr="Carpuat and Wu, 2007" startWordPosition="1299" endWordPosition="1302">translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source phrase based on the target context. In fact, attempts at using source contexts in phrase-based SMT have to date failed to show important gains on standard evaluation test sets (Carpuat and Wu, 2007; Stroppa et al., 2007; Gimpel and Smith, 2008; Max et al., 2008). Importantly, in all conditions where gains have been obtained, the target language was the “morphologically-poor” English. Nonetheless, there seems to be a clear consensus on the importance of better exploiting source contexts in SMT, so as to improve phrase disambiguation. The following sentence extract from the devtest corpus is a typical example where the lack of context in our phrase-based system yields an incorrect translation: Source: the long weekend comes with a price ... Target: Le long week-end vient avec un prix ... </context>
<context position="9989" citStr="Carpuat and Wu, 2007" startWordPosition="1571" endWordPosition="1574">e words into unique tokens, so as to record the original context 3Our context-aware phrase-based system indeed proposes the appropriate translation: Le long week-end a un prix. of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f, C(f)), where C(f) is some contextual information about the source phrase f. An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): count(e, f, C(f)) p(e|f, C(f)) = Ee, count(e�, f, C(f)) For these experiments, we followed the latter approach, restricting ourselves to features representing the local context up to a fixed distance d (using the values 1 and 2 in our experiments) from the source phrase fend start: • lexical context features: – left context: p(e|f, fstart−1 start−d ) – right context: p(e|f, fend+d end+1 ) • shallow syntactic features (denoting tF1 the sequence of PO</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Context-Dependent Phrasal Translation Lexicons for Statistical Machine Translation. In Proceedings of Machine Translation Summit XI, pages 73–80, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
<author>E Vidal</author>
</authors>
<title>Machine translation with inferred stochastic finite-state transducers.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="6262" citStr="Casacuberta and Vidal, 2004" startWordPosition="972" endWordPosition="975">-final-and’ heuristics, using a maximum phrase length of 7; non-contextual phrase scores contain the 4 translation model scores, plus a fixed phrase penalty; 6 additional scores parameterize the lexicalized reordering model. Default decoding options were used (20 alternatives per phrase, maximum distortion distance of 7, etc.) 2.5 A N-code baseline N-code implements the n-gram-based approach to Statistical Machine Translation (Mariño et al., 2006). In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training such a model requires to reorder source sentences so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models a</context>
</contexts>
<marker>Casacuberta, Vidal, 2004</marker>
<rawString>F. Casacuberta and E. Vidal. 2004. Machine translation with inferred stochastic finite-state transducers. Computational Linguistics, 30(3):205–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J T Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<location>Santa Cruz, NM.</location>
<contexts>
<context position="6943" citStr="Chen and Goodman, 1996" startWordPosition="1081" endWordPosition="1084">es so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney backoff (Chen and Goodman, 1996). The maximum tuple size was also set to 7. 2.6 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training (MERT) (Och, 2003) distributed with the Moses decoder, using the development corpus (dev2009a). For the context-less systems, tuning concerned the 14 usual weights; tuning the 101 22 weights of the context-aware systems (see 3.1) proved to be much more challenging, and the weights used in our submissions are probably far from optimal. The N-code systems only rely on 9 weights, since they dispense with the lexical reordering model; these wei</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. F. Chen and J. T. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 310–318, Santa Cruz, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mariño</author>
</authors>
<title>Improving SMT by coupling reordering and decoding.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="6699" citStr="Crego and Mariño, 2007" startWordPosition="1041" endWordPosition="1044">l., 2006). In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training such a model requires to reorder source sentences so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney backoff (Chen and Goodman, 1996). The maximum tuple size was also set to 7. 2.6 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training (MERT) (Och, 2003) distributed with the Moses decoder, using the development corpus (dev2009a). For the context-less systems, tuning concerned the 14 usual weights; tuning the 101 22 weights of the co</context>
</contexts>
<marker>Crego, Mariño, 2007</marker>
<rawString>J. M. Crego and J. B. Mariño. 2007. Improving SMT by coupling reordering and decoding. Machine Translation, 20(3):199–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Déchelotte</author>
<author>G Adda</author>
<author>A Allauzen</author>
<author>O Galibert</author>
<author>J-L Gauvain</author>
<author>H Meynard</author>
<author>F Yvon</author>
</authors>
<title>Limsi’s statistical translation systems for WMT’08.</title>
<date>2008</date>
<booktitle>In Proceedings of the NAACL-HTL Statistical Machine Translation Workshop,</booktitle>
<pages>107--100</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1614" citStr="Déchelotte et al., 2008" startWordPosition="236" endWordPosition="239">al information on phrases; we also provided a contrast with a vanilla Moses-based system. Additional contrasts are based on the N-code decoder, one of which takes advantage of (part of) the English/French GigaWord parallel corpus. 2 System architecture and resources In this section, we describe the main characteristics of the baseline phrase-based systems used in this evaluation and the resources that were used to train our models. 2.1 Pre- and post-processing tools All the available textual corpora were processed and normalized using in-house text processing tools. Our last year experiments (Déchelotte et al., 2008) revealed that using better normalization tools provides a significant reward in BLEU, a fact that we could observe again this year. The downside is the need to post-process our outputs so as to “detokenize” them for scoring purposes, which is unfortunately an error-prone process. Based again on last year’s experiments, our systems are built in “true case”: the first letter of each sentence is lowercased when it should be, and the remaining tokens are left as is. Finally, the N-code (see 2.5) and the contextaware (see 3) systems require the source to be morpho-syntactically analysed. This was </context>
</contexts>
<marker>Déchelotte, Adda, Allauzen, Galibert, Gauvain, Meynard, Yvon, 2008</marker>
<rawString>D. Déchelotte, G. Adda, A. Allauzen, O. Galibert, J.-L. Gauvain, H. Meynard, and F. Yvon. 2008. Limsi’s statistical translation systems for WMT’08. In Proceedings of the NAACL-HTL Statistical Machine Translation Workshop, pages 107-100, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Rich Source-Side Context for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>9--17</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="8361" citStr="Gimpel and Smith, 2008" startWordPosition="1307" endWordPosition="1310">rrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source phrase based on the target context. In fact, attempts at using source contexts in phrase-based SMT have to date failed to show important gains on standard evaluation test sets (Carpuat and Wu, 2007; Stroppa et al., 2007; Gimpel and Smith, 2008; Max et al., 2008). Importantly, in all conditions where gains have been obtained, the target language was the “morphologically-poor” English. Nonetheless, there seems to be a clear consensus on the importance of better exploiting source contexts in SMT, so as to improve phrase disambiguation. The following sentence extract from the devtest corpus is a typical example where the lack of context in our phrase-based system yields an incorrect translation: Source: the long weekend comes with a price ... Target: Le long week-end vient avec un prix ... (the long weekend comes accompanied by a price</context>
<context position="10134" citStr="Gimpel and Smith, 2008" startWordPosition="1593" endWordPosition="1597">ation: Le long week-end a un prix. of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f, C(f)), where C(f) is some contextual information about the source phrase f. An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): count(e, f, C(f)) p(e|f, C(f)) = Ee, count(e�, f, C(f)) For these experiments, we followed the latter approach, restricting ourselves to features representing the local context up to a fixed distance d (using the values 1 and 2 in our experiments) from the source phrase fend start: • lexical context features: – left context: p(e|f, fstart−1 start−d ) – right context: p(e|f, fend+d end+1 ) • shallow syntactic features (denoting tF1 the sequence of POS tags for the source sentence): – left context: p(e|f, tstart−1 start−d) – right context: p(e|f, tend+d end+1) As in (Gimpel and Smith, 2008), w</context>
</contexts>
<marker>Gimpel, Smith, 2008</marker>
<rawString>K. Gimpel and N. A. Smith. 2008. Rich Source-Side Context for Statistical Machine Translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 9–17, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL, demonstration session,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5566" citStr="Koehn et al., 2007" startWordPosition="872" endWordPosition="875">d the official devtest and test sets. The out-of-vocabulary (OOV) rate was drastically reduced by increasing the vocabulary size, the mean OOV rate decreasing from 2.5% to 0.7%, a trend observed in both languages. For French, using a small LM trained on the &amp;quot;WMT&amp;quot; data only resulted in a perplexity of 301 on the devtest corpus and 299 on the test set. Using all additional data yielded a large decrease in perplexity (106 on the devtest and 108 on the test); again the same trend was observed for English. 2.4 A Moses baseline Our baseline system was a vanilla phrase-based system built with Moses (Koehn et al., 2007) using default settings. Phrases were extracted using the ’grow-diag-final-and’ heuristics, using a maximum phrase length of 7; non-contextual phrase scores contain the 4 translation model scores, plus a fixed phrase penalty; 6 additional scores parameterize the lexicalized reordering model. Default decoding options were used (20 alternatives per phrase, maximum distortion distance of 7, etc.) 2.5 A N-code baseline N-code implements the n-gram-based approach to Statistical Machine Translation (Mariño et al., 2006). In a nutshell, the translation model is implemented as a stochastic finite-stat</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, demonstration session, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Max</author>
<author>R Makhloufi</author>
<author>P Langlais</author>
</authors>
<title>Explorations in using grammatical dependencies for contextual phrase translation disambiguation.</title>
<date>2008</date>
<booktitle>In Proceedings of EAMT, poster session,</booktitle>
<location>Hamburg, Germany.</location>
<contexts>
<context position="8380" citStr="Max et al., 2008" startWordPosition="1311" endWordPosition="1314">urce) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source phrase based on the target context. In fact, attempts at using source contexts in phrase-based SMT have to date failed to show important gains on standard evaluation test sets (Carpuat and Wu, 2007; Stroppa et al., 2007; Gimpel and Smith, 2008; Max et al., 2008). Importantly, in all conditions where gains have been obtained, the target language was the “morphologically-poor” English. Nonetheless, there seems to be a clear consensus on the importance of better exploiting source contexts in SMT, so as to improve phrase disambiguation. The following sentence extract from the devtest corpus is a typical example where the lack of context in our phrase-based system yields an incorrect translation: Source: the long weekend comes with a price ... Target: Le long week-end vient avec un prix ... (the long weekend comes accompanied by a price) While grammatical</context>
<context position="10008" citStr="Max et al., 2008" startWordPosition="1575" endWordPosition="1578">kens, so as to record the original context 3Our context-aware phrase-based system indeed proposes the appropriate translation: Le long week-end a un prix. of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f, C(f)), where C(f) is some contextual information about the source phrase f. An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): count(e, f, C(f)) p(e|f, C(f)) = Ee, count(e�, f, C(f)) For these experiments, we followed the latter approach, restricting ourselves to features representing the local context up to a fixed distance d (using the values 1 and 2 in our experiments) from the source phrase fend start: • lexical context features: – left context: p(e|f, fstart−1 start−d ) – right context: p(e|f, fend+d end+1 ) • shallow syntactic features (denoting tF1 the sequence of POS tags for the sour</context>
</contexts>
<marker>Max, Makhloufi, Langlais, 2008</marker>
<rawString>A. Max, R. Makhloufi, and P. Langlais. 2008. Explorations in using grammatical dependencies for contextual phrase translation disambiguation. In Proceedings of EAMT, poster session, Hamburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Mariño</author>
<author>R E Banchs R</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A R Fonollosa</author>
<author>M R CostaJussà</author>
</authors>
<title>N-gram-based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mariño, R, Crego, de Gispert, Lambert, Fonollosa, CostaJussà, 2006</marker>
<rawString>J. B. Mariño, R. E. Banchs R, J.M. Crego, A. de Gispert, P. Lambert, J.A.R. Fonollosa, and M. R. CostaJussà. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="7117" citStr="Och, 2003" startWordPosition="1112" endWordPosition="1113">ond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney backoff (Chen and Goodman, 1996). The maximum tuple size was also set to 7. 2.6 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training (MERT) (Och, 2003) distributed with the Moses decoder, using the development corpus (dev2009a). For the context-less systems, tuning concerned the 14 usual weights; tuning the 101 22 weights of the context-aware systems (see 3.1) proved to be much more challenging, and the weights used in our submissions are probably far from optimal. The N-code systems only rely on 9 weights, since they dispense with the lexical reordering model; these weights were tuned on the same dataset, using an in-house implementation of the simplex algorithm. 3 Extensions 3.1 A context-aware system In phrase-based translation, source ph</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>