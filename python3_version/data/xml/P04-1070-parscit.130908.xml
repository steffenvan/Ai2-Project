<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992769">
An alternative method of training probabilistic LR parsers
</title>
<author confidence="0.981451">
Mark-Jan Nederhof
</author>
<affiliation confidence="0.995973">
Faculty of Arts
University of Groningen
</affiliation>
<address confidence="0.827913333333333">
P.O. Box 716
NL-9700 AS Groningen
The Netherlands
</address>
<email confidence="0.995471">
markjan@let.rug.nl
</email>
<author confidence="0.994942">
Giorgio Satta
</author>
<affiliation confidence="0.998509">
Dept. of Information Engineering
University of Padua
</affiliation>
<address confidence="0.877405333333333">
via Gradenigo, 6/A
I-35131 Padova
Italy
</address>
<email confidence="0.995305">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.997323" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997379375">
We discuss existing approaches to train LR parsers,
which have been used for statistical resolution of
structural ambiguity. These approaches are non-
optimal, in the sense that a collection of probability
distributions cannot be obtained. In particular, some
probability distributions expressible in terms of a
context-free grammar cannot be expressed in terms
of the LR parser constructed from that grammar,
under the restrictions of the existing approaches to
training of LR parsers. We present an alternative
way of training that is provably optimal, and that al-
lows all probability distributions expressible in the
context-free grammar to be carried over to the LR
parser. We also demonstrate empirically that this
kind of training can be effectively applied on a large
treebank.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989549382717">
The LR parsing strategy was originally devised
for programming languages (Sippu and Soisalon-
Soininen, 1990), but has been used in a wide range
of other areas as well, such as for natural language
processing (Lavie and Tomita, 1993; Briscoe and
Carroll, 1993; Ruland, 2000). The main difference
between the application to programming languages
and the application to natural languages is that in
the latter case the parsers should be nondetermin-
istic, in order to deal with ambiguous context-free
grammars (CFGs). Nondeterminism can be han-
dled in a number of ways, but the most efficient
is tabulation, which allows processing in polyno-
mial time. Tabular LR parsing is known from the
work by (Tomita, 1986), but can also be achieved
by the generic tabulation technique due to (Lang,
1974; Billot and Lang, 1989), which assumes an in-
put pushdown transducer (PDT). In this context, the
LR parsing strategy can be seen as a particular map-
ping from context-free grammars to PDTs.
The acronym ‘LR’ stands for ‘Left-to-right pro-
cessing of the input, producing a Right-most deriva-
tion (in reverse)’. When we construct a PDT A from
a CFG G by the LR parsing strategy and apply it on
an input sentence, then the set of output strings of A
represents the set of all right-most derivations that G
allows for that sentence. Such an output string enu-
merates the rules (or labels that identify the rules
uniquely) that occur in the corresponding right-most
derivation, in reversed order.
If LR parsers do not use lookahead to decide be-
tween alternative transitions, they are called LR(0)
parsers. More generally, if LR parsers look ahead k
symbols, they are called LR(k) parsers; some sim-
plified LR parsing models that use lookahead are
called SLR(k) and LALR(k) parsing (Sippu and
Soisalon-Soininen, 1990). In order to simplify the
discussion, we abstain from using lookahead in this
article, and ‘LR parsing’ can further be read as
‘LR(0) parsing’. We would like to point out how-
ever that our observations carry over to LR parsing
with lookahead.
The theory of probabilistic pushdown automata
(Santos, 1972) can be easily applied to LR parsing.
A probability is then assigned to each transition, by
a function that we will call the probability function
pA, and the probability of an accepting computa-
tion of A is the product of the probabilities of the
applied transitions. As each accepting computation
produces a right-most derivation as output string, a
probabilistic LR parser defines a probability distri-
bution on the set of parses, and thereby also a prob-
ability distribution on the set of sentences generated
by grammar G. Disambiguation of an ambiguous
sentence can be achieved on the basis of a compari-
son between the probabilities assigned to the respec-
tive parses by the probabilistic LR model.
The probability function can be obtained on the
basis of a treebank, as proposed by (Briscoe and
Carroll, 1993) (see also (Su et al., 1991)). The
model by (Briscoe and Carroll, 1993) however in-
corporated a mistake involving lookahead, which
was corrected by (Inui et al., 2000). As we will not
discuss lookahead here, this matter does not play a
significant role in the current study. Noteworthy is
that (Sornlertlamvanich et al., 1999) showed empir-
ically that an LR parser may be more accurate than
the original CFG, if both are trained on the basis
of the same treebank. In other words, the resulting
probability function pA on transitions of the PDT
allows better disambiguation than the correspond-
ing function pg on rules of the original grammar.
A plausible explanation of this is that stack sym-
bols of an LR parser encode some amount of left
context, i.e. information on rules applied earlier, so
that the probability function on transitions may en-
code dependencies between rules that cannot be en-
coded in terms of the original CFG extended with
rule probabilities. The explicit use of left con-
text in probabilistic context-free models was inves-
tigated by e.g. (Chitrao and Grishman, 1990; John-
son, 1998), who also demonstrated that this may
significantly improve accuracy. Note that the prob-
ability distributions of language may be beyond the
reach of a given context-free grammar, as pointed
out by e.g. (Collins, 2001). Therefore, the use of left
context, and the resulting increase in the number of
parameters of the model, may narrow the gap be-
tween the given grammar and ill-understood mech-
anisms underlying actual language.
One important assumption that is made by
(Briscoe and Carroll, 1993) and (Inui et al., 2000)
is that trained probabilistic LR parsers should be
proper, i.e. if several transitions are applicable for
a given stack, then the sum of probabilities as-
signed to those transitions by probability function
pA should be 1. This assumption may be moti-
vated by pragmatic considerations, as such a proper
model is easy to train by relative frequency estima-
tion: count the number of times a transition is ap-
plied with respect to a treebank, and divide it by
the number of times the relevant stack symbol (or
pair of stack symbols) occurs at the top of the stack.
Let us call the resulting probability function prfe.
This function is provably optimal in the sense that
the likelihood it assigns to the training corpus is
maximal among all probability functions pA that are
proper in the above sense.
However, properness restricts the space of prob-
ability distributions that a PDT allows. This means
that a (consistent) probability function pA may ex-
ist that is not proper and that assigns a higher like-
lihood to the training corpus than prfe does. (By
‘consistent’ we mean that the probabilities of all
strings that are accepted sum to 1.) It may even
be the case that a (proper and consistent) probabil-
ity function pg on the rules of the input grammar !9
exists that assigns a higher likelihood to the corpus
than prfe, and therefore it is not guaranteed that LR
parsers allow better probability estimates than the
CFGs from which they were constructed, if we con-
strain probability functions pA to be proper. In this
respect, LR parsing differs from at least one other
well-known parsing strategy, viz. left-corner pars-
ing. See (Nederhof and Satta, 2004) for a discus-
sion of a property that is shared by left-corner pars-
ing but not by LR parsing, and which explains the
above difference.
As main contribution of this paper we establish
that this restriction on expressible probability dis-
tributions can be dispensed with, without losing the
ability to perform training by relative frequency es-
timation. What comes in place of properness is
reverse-properness, which can be seen as proper-
ness of the reversed pushdown automaton that pro-
cesses input from right to left instead of from left to
right, interpreting the transitions of A backwards.
As we will show, reverse-properness does not re-
strict the space of probability distributions express-
ible by an LR automaton. More precisely, assume
some probability distribution on the set of deriva-
tions is specified by a probability function pA on
transitions of PDT A that realizes the LR strat-
egy for a given grammar !9. Then the same prob-
ability distribution can be specified by an alterna-
tive such function pA that is reverse-proper. In ad-
dition, for each probability distribution on deriva-
tions expressible by a probability function pg for !9,
there is a reverse-proper probability function pA for
A that expresses the same probability distribution.
Thereby we ensure that LR parsers become at least
as powerful as the original CFGs in terms of allow-
able probability distributions.
This article is organized as follows. In Sec-
tion 2 we outline our formalization of LR pars-
ing as a construction of PDTs from CFGs, making
some superficial changes with respect to standard
formulations. Properness and reverse-properness
are discussed in Section 3, where we will show
that reverse-properness does not restrict the space
of probability distributions. Section 4 reports on ex-
periments, and Section 5 concludes this article.
</bodyText>
<sectionHeader confidence="0.963638" genericHeader="method">
2 LR parsing
</sectionHeader>
<bodyText confidence="0.999389086956522">
As LR parsing has been extensively treated in exist-
ing literature, we merely recapitulate the main defi-
nitions here. For more explanation, the reader is re-
ferred to standard literature such as (Harrison, 1978;
Sippu and Soisalon-Soininen, 1990).
An LR parser is constructed on the basis of a CFG
that is augmented with an additional rule 5† —* �- 5,
where 5 is the former start symbol, and the new
nonterminal 5† becomes the start symbol of the
augmented grammar. The new terminal �- acts as
an imaginary start-of-sentence marker. We denote
the set of terminals by Σ and the set of nontermi-
nals by N. We assume each rule has a unique label
r.
As explained before, we construct LR parsers as
pushdown transducers. The main stack symbols
of these automata are sets of dotted rules, which
consist of rules from the augmented grammar with
a distinguished position in the right-hand side in-
dicated by a dot ‘•’. The initial stack symbol is
pinit = {S† → ` • S}.
We define the closure of a set p of dotted rules as
the smallest set closure(p) such that:
</bodyText>
<listItem confidence="0.9784785">
1. p ⊆ closure(p); and
2. for (B → α • Aβ) ∈ closure(p) and A →
γ a rule in the grammar, also (A → • γ) ∈
closure(p).
</listItem>
<bodyText confidence="0.9471982">
We define the operation goto on a set p of dotted
rules and a grammar symbol X ∈ Σ ∪ N as:
goto(p, X) = {A → αX • β |
(A → α • Xβ) ∈ closure(p)}
The set of LR states is the smallest set such that:
</bodyText>
<listItem confidence="0.997907">
1. pinit is an LR state; and
2. if p is an LR state and goto(p, X) = q =6 ∅, for
some X ∈ Σ ∪ N, then q is an LR state.
</listItem>
<bodyText confidence="0.997990294117647">
We will assume that PDTs consist of three types
of transitions, of the form P a,b
7→ P Q (a push tran-
sition), of the form P a,b
7→ Q (a swap transition), and
of the form P Q a,b
7→ R (a pop transition). Here P, Q
and R are stack symbols, a is one input terminal or
is the empty string ε, and b is one output terminal or
is the empty string ε. In our notation, stacks grow
from left to right, so that P a,b
7→ P Q means that Q is
pushed on top of P. We do not have internal states
next to stack symbols.
For the PDT that implements the LR strategy, the
stack symbols are the LR states, plus symbols of the
form [p; X], where p is an LR state and X is a gram-
mar symbol, and symbols of the form (p, A, m),
where p is an LR state, A is the left-hand side of
some rule, and m is the length of some prefix of the
right-hand side of that rule. More explanation on
these additional stack symbols will be given below.
The stack symbols and transitions are simultane-
ously defined in Figure 1. The final stack symbol
is pfinal = (pinit, S†, 0). This means that an input
a1 · · · an is accepted if and only if it is entirely read
by a sequence of transitions that take the stack con-
sisting only of pinit to the stack consisting only of
pfinal. The computed output consists of the string of
terminals b1 · · · bn&apos; from the output components of
the applied transitions. For the PDTs that we will
use, this output string will consist of a sequence of
rule labels expressing a right-most derivation of the
input. On the basis of the original grammar, the cor-
responding parse tree can be constructed from such
an output string.
There are a few superficial differences with LR
parsing as it is commonly found in the literature.
The most obvious difference is that we divide re-
ductions into ‘binary’ steps. The main reason is that
this allows tabular interpretation with a time com-
plexity cubic in the length of the input. Otherwise,
the time complexity would be O(nm+1), where m
is the length of the longest right-hand side of a rule
in the CFG. This observation was made before by
(Kipps, 1991), who proposed a solution similar to
ours, albeit formulated differently. See also a related
formulation of tabular LR parsing by (Nederhof and
Satta, 1996).
To be more specific, instead of one step of the
PDT taking stack:
</bodyText>
<equation confidence="0.915836333333333">
σp0p1 · · · pm
immediately to stack:
σp0q
</equation>
<bodyText confidence="0.999821">
where (A → X1 · · · Xm •) ∈ pm, σ is a string
of stack symbols and goto(p0, A) = q, we have
a number of smaller steps leading to a series of
stacks:
</bodyText>
<equation confidence="0.94479">
σp0p1 · · · pm−1pm
σp0p1 ··· pm−1(A, m−1)
σp0p1 · · · (A, m−2)
.
..
σp0(A, 0)
σp0q
</equation>
<bodyText confidence="0.9255688">
There are two additional differences. First, we
want to avoid steps of the form:
σp0(A, 0)
σp0q
by transitions p0 (A, 0) ε,ε
7→ p0 q, as such transitions
complicate the generic definition of ‘properness’
for PDTs, to be discussed in the following section.
For this reason, we use stack symbols of the form
[p; X] next to p, and split up p0 (A, 0) ε,ε
</bodyText>
<equation confidence="0.38610325">
7→ p0 q into
pop [p0; X0] (A, 0) ε,ε
7→ [p0; A] and push [p0; A] ε,ε
7→
</equation>
<bodyText confidence="0.994685666666667">
[p0; A] q. This is a harmless modification, which in-
creases the number of steps in any computation by
at most a factor 2.
Secondly, we use stack symbols of the form
(p, A, m) instead of (A, m). This concerns the con-
ditions of reverse-properness to be discussed in the
</bodyText>
<listItem confidence="0.995786">
• For LR state p and a ∈ Σ such that goto(p, a) =6 ∅:
</listItem>
<equation confidence="0.8278665">
a,ε
p 7→ [p; a] (1)
</equation>
<listItem confidence="0.848784">
• For LR state p and (A → •) ∈ p, where A → ε has label r:
</listItem>
<equation confidence="0.8604375">
ε,r
p 7→ [p; A] (2)
</equation>
<listItem confidence="0.868228">
• For LR state p and (A → α •) ∈ p, where |α |= m &gt; 0 and A → α has label r:
</listItem>
<equation confidence="0.7059595">
ε,r
p 7→ (p, A, m − 1) (3)
</equation>
<listItem confidence="0.990809285714286">
• For LR state p and (A → α • Xβ) ∈ p, where |α |= m &gt; 0, such that goto(p, X) = q =6 ∅:
[p; X] (q, A, m) ε,ε
7→ (p, A, m − 1) (4)
• For LR state p and (A → • Xβ) ∈ p, such that goto(p, X) = q =6 ∅:
[p; X] (q, A, 0) ε,ε
7→ [p; A] (5)
• For LR state p and X ∈ Σ ∪ N such that goto(p, X) = q =6 ∅:
</listItem>
<equation confidence="0.3891565">
[p; X] ε,ε
7→ [p; X] q (6)
</equation>
<figureCaption confidence="0.991388">
Figure 1: The transitions of a PDT implementing LR(0) parsing.
</figureCaption>
<bodyText confidence="0.999977666666667">
following section. By this condition, we consider
LR parsing as being performed from right to left, so
backwards with regard to the normal processing or-
der. If we were to omit the first components p from
stack symbols (p, A, m), we may obtain ‘dead ends’
in the computation. We know that such dead ends
make a (reverse-)proper PDT inconsistent, as proba-
bility mass lost in dead ends causes the sum of prob-
abilities of all computations to be strictly smaller
than 1. (See also (Nederhof and Satta, 2004).) It
is interesting to note that the addition of the compo-
nents p to stack symbols (p, A, m) does not increase
the number of transitions, and the nature of LR pars-
ing in the normal processing order from left to right
is preserved.
With all these changes together, reductions
are implemented by transitions resulting in the
following sequence of stacks:
</bodyText>
<equation confidence="0.995343375">
σ0[p0; X0][p1; X1] ··· [pm−1; Xm−1]pm
σ0[p0; X0][p1; X1] ··· [pm−1; Xm−1](pm, A, m−1)
σ0[p0; X0][p1; X1] ··· (pm−1, A, m−2)
.
..
σ0[p0; X0](p1, A, 0)
σ0[p0; A]
σ0[p0; A]q
</equation>
<bodyText confidence="0.976148428571429">
Please note that transitions of the form
[p; X] (q, A, m) ε,ε
7→ (p, A, m − 1) may corre-
spond to several dotted rules (A → α • Xβ) ∈ p,
with different α of length m and different β. If we
were to multiply such transitions for different α and
β, the PDT would become prohibitively large.
</bodyText>
<sectionHeader confidence="0.959629" genericHeader="method">
3 Properness and reverse-properness
</sectionHeader>
<bodyText confidence="0.999890275">
If a PDT is regarded to process input from left to
right, starting with a stack consisting only of pinit,
and ending in a stack consisting only of pfinal, then
it seems reasonable to cast this process into a prob-
abilistic framework in such a way that the sum of
probabilities of all choices that are possible at any
given moment is 1. This is similar to how the notion
of ‘properness’ is defined for probabilistic context-
free grammars (PCFGs); we say a PCFG is proper if
for each nonterminal A, the probabilities of all rules
with left-hand side A sum to 1.
Properness for PCFGs does not restrict the space
of probability distributions on the set of parse trees.
In other words, if a probability distribution can be
defined by attaching probabilities to rules, then we
may reassign the probabilities such that that PCFG
becomes proper, while preserving the probability
distribution. This even holds if the input grammar
is non-tight, meaning that probability mass is lost
in ‘infinite derivations’ (S´anchez and Benedi, 1997;
Chi and Geman, 1998; Chi, 1999; Nederhof and
Satta, 2003).
Although CFGs and PDTs are weakly equiva-
lent, they behave very differently when they are ex-
tended with probabilities. In particular, there seems
to be no notion similar to PCFG properness that
can be imposed on all types of PDTs without los-
ing generality. Below we will discuss two con-
straints, which we will call properness and reverse-
properness. Neither of these is suitable for all types
of PDTs, but as we will show, the second is more
suitable for probabilistic LR parsing than the first.
This is surprising, as only properness has been de-
scribed in existing literature on probabilistic PDTs
(PPDTs). In particular, all existing approaches to
probabilistic LR parsing have assumed properness
rather than anything related to reverse-properness.
For properness we have to assume that for each
stack symbol P, we either have one or more tran-
sitions of the form P ���
</bodyText>
<listItem confidence="0.82365275">
�� P Q or P ���
�� Q, or one
or more transitions of the form Q P ���
�� R, but no
combination thereof. In the first case, properness
demands that the sum of probabilities of all transi-
tions P ���
�� P Q and P ���
</listItem>
<bodyText confidence="0.691864333333333">
�� Q is 1, and in the second
case properness demands that the sum of probabili-
ties of all transitions Q P ���
</bodyText>
<equation confidence="0.907627">
�� R is 1 for each Q.
</equation>
<bodyText confidence="0.791279">
Note that our assumption above is without loss
of generality, as we may introduce swap transitions
</bodyText>
<equation confidence="0.8287174">
P ���
�� P1 and P ���
�� P2, where P1 and P2 are new
stack symbols, and replace transitions P ���
�� P Q
</equation>
<bodyText confidence="0.937009777777778">
and P ��� ��� ���
�� Q by P1 �� P1 Q and P1 �� Q, and
replace transitions Q P H R by Q P2 H R.
The notion of properness underlies the normal
training process for PDTs, as follows. We assume
a corpus of PDT computations. In these computa-
tions, we count the number of occurrences for each
transition. For each P we sum the total number of
all occurrences of transitions P ���
</bodyText>
<equation confidence="0.7013195">
�� P Q or P ���
�� Q.
</equation>
<bodyText confidence="0.994241838709677">
The probability of, say, a transition P ���
�� P Q is
now estimated by dividing the number of occur-
rences thereof in the corpus by the above total num-
ber of occurrences of transitions with P in the left-
hand side. Similarly, for each pair (Q, P) we sum
the total number of occurrences of all transitions of
the form Q P ���
�� R, and thereby estimate the proba-
bility of a particular transition Q P ���
�� R by relative
frequency estimation. The resulting PPDT is proper.
It has been shown that imposing properness is
without loss of generality in the case of PDTs
constructed by a wide range of parsing strategies,
among which are top-down parsing and left-corner
parsing. This does not hold for PDTs constructed by
the LR parsing strategy however, and in fact, proper-
ness for such automata may reduce the expressive
power in terms of available probability distributions
to strictly less than that offered by the original CFG.
This was formally proven by (Nederhof and Satta,
2004), after (Ng and Tomita, 1991) and (Wright and
Wrigley, 1991) had already suggested that creating
a probabilistic LR parser that is equivalent to an in-
put PCFG is difficult in general. The same difficulty
for ELR parsing was suggested by (Tendeau, 1997).
For this reason, we investigate a practical alter-
native, viz. reverse-properness. Now we have to as-
sume that for each stack symbol R, we either have
one or more transitions of the form P ���
</bodyText>
<equation confidence="0.73178075">
�� R or
Q P ���
�� R, or one or more transitions of the form
P���
</equation>
<bodyText confidence="0.945884509433963">
�� P R, but no combination thereof. In the first
case, reverse-properness demands that the sum of
probabilities of all transitions P ���
�� R or Q P ���
�� R
is 1, and in the second case reverse-properness de-
mands that the sum of probabilities of transitions
P���
�� P R is 1 for each P. Again, our assumption
above is without loss of generality.
In order to apply relative frequency estimation,
we now sum the total number of occurrences of tran-
sitions P ���
�� R or Q P ���
�� R for each R, and we
sum the total number of occurrences of transitions
P���
�� P R for each pair (P, R).
We now prove that reverse-properness does not
restrict the space of probability distributions, by
means of the construction of a ‘cover’ grammar
from an input CFG, as reported in Figure 2. This
cover CFG has almost the same structure as the PDT
resulting from Figure 1. Rules and transitions al-
most stand in a one-to-one relation. The only note-
worthy difference is between transitions of type (6)
and rules of type (12). The right-hand sides of those
rules can be a because the corresponding transitions
are deterministic if seen from right to left. Now it
becomes clear why we needed the components p in
stack symbols of the form (p, A, m). Without it, one
could obtain an LR state q that does not match the
underlying [p; X] in a reversed computation.
We may assume without loss of generality that
rules of type (12) are assigned probability 1, as a
probability other than 1 could be moved to corre-
sponding rules of types (10) or (11) where state
q was introduced. In the same way, we may as-
sume that transitions of type (6) are assigned prob-
ability 1. After making these assumptions, we ob-
tain a bijection between probability functions pA for
the PDT and probability functions pG for the cover
CFG. As was shown by e.g. (Chi, 1999) and (Neder-
hof and Satta, 2003), properness for CFGs does not
restrict the space of probability distributions, and
thereby the same holds for reverse-properness for
PDTs that implement the LR parsing strategy.
It is now also clear that a reverse-proper LR
parser can describe any probability distribution that
the original CFG can. The proof is as follows.
Given a probability function pG for the input CFG,
we define a probability function pA for the LR
parser, by letting transitions of types (2) and (3)
</bodyText>
<listItem confidence="0.983895181818182">
• For LR state p and a E Σ such that goto(p, a) =� 0:
[p; a] —* p (7)
• For LR state p and (A •) E p, where A —* ε has label r:
[p; A] —* p r (8)
• For LR state p and (A α •) E p, where |α |= m &gt; 0 and A —* α has label r:
(p, A, m — 1) —* p r (9)
• For LR state p and (A α • XQ) E p, where |α |= m &gt; 0, such that goto(p, X) = q =� 0:
(p, A, m — 1) —* [p; X] (q, A, m) (10)
• For LR state p and (A • XQ) E p, such that goto(p, X) = q =� 0:
[p; A] —* [p; X] (q, A, 0) (11)
• For LR state q:
</listItem>
<equation confidence="0.870689">
q —* ε (12)
</equation>
<figureCaption confidence="0.903193">
Figure 2: A grammar that describes the set of computations of the LR(0) parser. Start symbol is pfinal =
(pinit, S†, 0). Terminals are rule labels. Generated language consists of right-most derivations in reverse.
</figureCaption>
<bodyText confidence="0.999705833333333">
have probability pG(r), and letting all other transi-
tions have probability 1. This gives us the required
probability distribution in terms of a PPDT that is
not reverse-proper in general. This PPDT can now
be recast into reverse-proper form, as proven by the
above.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.922407105263158">
We have implemented both the traditional training
method for LR parsing and the novel one, and have
compared their performance, with two concrete ob-
jectives:
1. We show that the number of free parameters
is significantly larger with the new training
method. (The number of free parameters is
the number of probabilities of transitions that
can be freely chosen within the constraints of
properness or reverse-properness.)
2. The larger number of free parameters does not
make the problem of sparse data any worse,
and precision and recall are at least compara-
ble to, if not better than, what we would obtain
with the established method.
The experiments were performed on the Wall
Street Journal (WSJ) corpus, from the Penn Tree-
bank, version II. Training was done on sections 02-
21, i.e., first a context-free grammar was derived
from the ‘stubs’ of the combined trees, taking parts
of speech as leaves of the trees, omitting all af-
fixes from the nonterminal names, and removing ε-
generating subtrees. Such preprocessing of the WSJ
corpus is consistent with earlier attempts to derive
CFGs from that corpus, as e.g. by (Johnson, 1998).
The obtained CFG has 10,035 rules. The dimen-
sions of the LR parser constructed from this gram-
mar are given in Table 1.
The PDT was then trained on the trees from the
same sections 02-21, to determine the number of
times that transitions are used. At first sight it is not
clear how to determine this on the basis of the tree-
bank, as the structure of LR parsers is very differ-
ent from the structure of the grammars from which
they are constructed. The solution is to construct a
second PDT from the PDT to be trained, replacing
each transition α ���
�—* Q with label r by transition
���
α �—* Q. By this second PDT we parse the tree-
bank, encoded as a series of right-most derivations
in reverse.&apos; For each input string, there is exactly
one parse, of which the output is the list of used
transitions. The same method can be used for other
parsing strategies as well, such as left-corner pars-
ing, replacing right-most derivations by a suitable
alternative representation of parse trees.
By the counts of occurrences of transitions, we
may then perform maximum likelihood estimation
to obtain probabilities for transitions. This can
be done under the constraints of properness or of
reverse-properness, as explained in the previous
section. We have not applied any form of smooth-
&apos;We have observed an enormous gain in computational ef-
ficiency when we also incorporate the ‘shifts’ next to ‘reduc-
tions’ in these right-most derivations, as this eliminates a con-
siderable amount of nondeterminism.
</bodyText>
<table confidence="0.99784125">
total # transitions 8,340,315
# push transitions 753,224
# swap transitions 589,811
# pop transitions 6,997,280
</table>
<tableCaption confidence="0.991962">
Table 1: Dimensions of PDT implementing LR
strategy for CFG derived from WSJ, sect. 02-21.
</tableCaption>
<table confidence="0.9982828">
proper rev.-prop.
577,650 6,589,716
137,134 137,134
0.772 0.777
0.747 0.749
</table>
<tableCaption confidence="0.9688225">
Table 2: The two methods of training, based on
properness and reverse-properness.
</tableCaption>
<bodyText confidence="0.983372811320755">
ing or back-off, as this could obscure properties in-
herent in the difference between the two discussed
training methods. (Back-off for probabilistic LR
parsing has been proposed by (Ruland, 2000).) All
transitions that were not seen during training were
given probability 0.
The results are outlined in Table 2. Note that the
number of free parameters in the case of reverse-
properness is much larger than in the case of normal
properness. Despite of this, the number of transi-
tions that actually receive non-zero probabilities is
(predictably) identical in both cases, viz. 137,134.
However, the potential for fine-grained probability
estimates and for smoothing and parameter-tying
techniques is clearly greater in the case of reverse-
properness.
That in both cases the number of non-zero prob-
abilities is lower than the total number of parame-
ters can be explained as follows. First, the treebank
contains many rules that occur a small number of
times. Secondly, the LR automaton is much larger
than the CFG; in general, the size of an LR automa-
ton is bounded by a function that is exponential in
the size of the input CFG. Therefore, if we use the
same treebank to estimate the probability function,
then many transitions are never visited and obtain a
zero probability.
We have applied the two trained LR automata
on section 22 of the WSJ corpus, measuring la-
belled precision and recall, as done by e.g. (John-
son, 1998).2 We observe that in the case of reverse-
properness, precision and recall are slightly better.
2We excluded all sentences with more than 30 words how-
ever, as some required prohibitive amounts of memory. Only
one of the remaining 1441 sentences was not accepted by the
parser.
The most important conclusion that can be drawn
from this is that the substantially larger space of
obtainable probability distributions offered by the
reverse-properness method does not come at the ex-
pense of a degradation of accuracy for large gram-
mars such as those derived from the WSJ. For com-
parison, with a standard PCFG we obtain labelled
precision and recall of 0.725 and 0.670, respec-
tively.3
We would like to stress that our experiments
did not have as main objective the improvement of
state-of-the-art parsers, which can certainly not be
done without much additional fine-tuning and the
incorporation of some form of lexicalization. Our
main objectives concerned the relation between our
newly proposed training method for LR parsers and
the traditional one.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.911159555555556">
We have presented a novel way of assigning proba-
bilities to transitions of an LR automaton. Theoreti-
cal analysis and empirical data reveal the following.
• The efficiency of LR parsing remains unaf-
fected. Although a right-to-left order of read-
ing input underlies the novel training method,
we may continue to apply the parser from left
to right, and benefit from the favourable com-
putational properties of LR parsing.
• The available space of probability distributions
is significantly larger than in the case of the
methods published before. In terms of the
number of free parameters, the difference that
we found empirically exceeds one order of
magnitude. By the same criteria, we can now
guarantee that LR parsers are at least as pow-
erful as the CFGs from which they are con-
structed.
</bodyText>
<listItem confidence="0.80161025">
• Despite the larger number of free parameters,
no increase of sparse data problems was ob-
served, and in fact there was a small increase
in accuracy.
</listItem>
<sectionHeader confidence="0.997327" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996738571428572">
Helpful comments from John Carroll and anony-
mous reviewers are gratefully acknowledged. The
first author is supported by the PIONIER Project
Algorithms for Linguistic Processing, funded by
NWO (Dutch Organization for Scientific Research).
The second author is partially supported by MIUR
under project PRIN No. 2003091149 005.
</bodyText>
<footnote confidence="0.875195">
3In this case, all 1441 sentences were accepted.
</footnote>
<figure confidence="0.56416425">
# free parameters
# non-zero probabilities
labelled precision
labelled recall
</figure>
<sectionHeader confidence="0.956583" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999700116504854">
S. Billot and B. Lang. 1989. The structure of shared
forests in ambiguous parsing. In 27th Annual
Meeting of the Association for Computational
Linguistics, pages 143–151, Vancouver, British
Columbia, Canada, June.
T. Briscoe and J. Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (cor-
pora) with unification-based grammars. Compu-
tational Linguistics, 19(1):25–59.
Z. Chi and S. Geman. 1998. Estimation of prob-
abilistic context-free grammars. Computational
Linguistics, 24(2):299–305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguis-
tics, 25(1):131–160.
M.V. Chitrao and R. Grishman. 1990. Statistical
parsing of messages. In Speech and Natural Lan-
guage, Proceedings, pages 263–266, Hidden Val-
ley, Pennsylvania, June.
M. Collins. 2001. Parameter estimation for sta-
tistical parsing models: Theory and practice of
distribution-free methods. In Proceedings of the
Seventh International Workshop on Parsing Tech-
nologies, Beijing, China, October.
M.A. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison-Wesley.
K. Inui, V. Sornlertlamvanich, H. Tanaka, and
T. Tokunaga. 2000. Probabilistic GLR parsing.
In H. Bunt and A. Nijholt, editors, Advances
in Probabilistic and other Parsing Technologies,
chapter 5, pages 85–104. Kluwer Academic Pub-
lishers.
M. Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613–632.
J.R. Kipps. 1991. GLR parsing in time 0(n3). In
M. Tomita, editor, Generalized LR Parsing, chap-
ter 4, pages 43–59. Kluwer Academic Publishers.
B. Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Automata,
Languages and Programming, 2nd Colloquium,
volume 14 of Lecture Notes in Computer Science,
pages 255–269, Saarbr¨ucken. Springer-Verlag.
A. Lavie and M. Tomita. 1993. GLR∗ – an efficient
noise-skipping parsing algorithm for context free
grammars. In Third International Workshop on
Parsing Technologies, pages 123–134, Tilburg
(The Netherlands) and Durbuy (Belgium), Au-
gust.
M.-J. Nederhof and G. Satta. 1996. Efficient tab-
ular LR parsing. In 34th Annual Meeting of the
Association for Computational Linguistics, pages
239–246, Santa Cruz, California, USA, June.
M.-J. Nederhof and G. Satta. 2003. Probabilis-
tic parsing as intersection. In 8th International
Workshop on Parsing Technologies, pages 137–
148, LORIA, Nancy, France, April.
M.-J. Nederhof and G. Satta. 2004. Probabilis-
tic parsing strategies. In 42nd Annual Meeting
of the Association for Computational Linguistics,
Barcelona, Spain, July.
S.-K. Ng and M. Tomita. 1991. Probabilistic LR
parsing for general context-free grammars. In
Proc. of the Second International Workshop on
Parsing Technologies, pages 154–163, Cancun,
Mexico, February.
T. Ruland. 2000. A context-sensitive model for
probabilistic LR parsing of spoken language
with transformation-based postprocessing. In
The 18th International Conference on Compu-
tational Linguistics, volume 2, pages 677–683,
Saarbr¨ucken, Germany, July–August.
J.-A. S´anchez and J.-M. Benedi. 1997. Consis-
tency of stochastic context-free grammars from
probabilistic estimation based on growth trans-
formations. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(9):1052–1055,
September.
E.S. Santos. 1972. Probabilistic grammars and au-
tomata. Information and Control, 21:27–47.
S. Sippu and E. Soisalon-Soininen. 1990. Parsing
Theory, Vol. II: LR(k) and LL(k) Parsing, vol-
ume 20 of EATCS Monographs on Theoretical
Computer Science. Springer-Verlag.
V. Sornlertlamvanich, K. Inui, H. Tanaka, T. Toku-
naga, and T. Takezawa. 1999. Empirical sup-
port for new probabilistic generalized LR pars-
ing. Journal of Natural Language Processing,
6(3):3–22.
K.-Y. Su, J.-N. Wang, M.-H. Su, and J.-S. Chang.
1991. GLR parsing with scoring. In M. Tomita,
editor, Generalized LR Parsing, chapter 7, pages
93–112. Kluwer Academic Publishers.
F. Tendeau. 1997. Analyse syntaxique et
s´emantique avec ´evaluation d’attributs dans
un demi-anneau. Ph.D. thesis, University of
Orl´eans.
M. Tomita. 1986. Efficient Parsing for Natural
Language. Kluwer Academic Publishers.
J.H. Wright and E.N. Wrigley. 1991. GLR pars-
ing with probability. In M. Tomita, editor, Gen-
eralized LR Parsing, chapter 8, pages 113–128.
Kluwer Academic Publishers.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805518">
<title confidence="0.998363">An alternative method of training probabilistic LR parsers</title>
<author confidence="0.989861">Mark-Jan Nederhof</author>
<affiliation confidence="0.999023">Faculty of Arts University of Groningen</affiliation>
<address confidence="0.976752333333333">P.O. Box 716 NL-9700 AS Groningen The Netherlands</address>
<email confidence="0.989973">markjan@let.rug.nl</email>
<author confidence="0.999152">Giorgio Satta</author>
<affiliation confidence="0.999687">Dept. of Information Engineering University of Padua</affiliation>
<address confidence="0.977764666666667">via Gradenigo, 6/A I-35131 Padova Italy</address>
<email confidence="0.998894">satta@dei.unipd.it</email>
<abstract confidence="0.996519941176471">We discuss existing approaches to train LR parsers, which have been used for statistical resolution of structural ambiguity. These approaches are nonoptimal, in the sense that a collection of probability distributions cannot be obtained. In particular, some probability distributions expressible in terms of a context-free grammar cannot be expressed in terms of the LR parser constructed from that grammar, under the restrictions of the existing approaches to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Billot</author>
<author>B Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>143--151</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1929" citStr="Billot and Lang, 1989" startWordPosition="292" endWordPosition="295">anguage processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (PDT). In this context, the LR parsing strategy can be seen as a particular mapping from context-free grammars to PDTs. The acronym ‘LR’ stands for ‘Left-to-right processing of the input, producing a Right-most derivation (in reverse)’. When we construct a PDT A from a CFG G by the LR parsing strategy and apply it on an input sentence, then the set of output strings of A represents the set of all right-most derivations that G allows for that sentence. Such an output string enumerates the rules (or labels that identify the rules uniquely) that occur </context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>S. Billot and B. Lang. 1989. The structure of shared forests in ambiguous parsing. In 27th Annual Meeting of the Association for Computational Linguistics, pages 143–151, Vancouver, British Columbia, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="1376" citStr="Briscoe and Carroll, 1993" startWordPosition="202" endWordPosition="205"> of the existing approaches to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 1 Introduction The LR parsing strategy was originally devised for programming languages (Sippu and SoisalonSoininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (P</context>
<context position="4023" citStr="Briscoe and Carroll, 1993" startWordPosition="642" endWordPosition="645">g computation of A is the product of the probabilities of the applied transitions. As each accepting computation produces a right-most derivation as output string, a probabilistic LR parser defines a probability distribution on the set of parses, and thereby also a probability distribution on the set of sentences generated by grammar G. Disambiguation of an ambiguous sentence can be achieved on the basis of a comparison between the probabilities assigned to the respective parses by the probabilistic LR model. The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993) (see also (Su et al., 1991)). The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al., 2000). As we will not discuss lookahead here, this matter does not play a significant role in the current study. Noteworthy is that (Sornlertlamvanich et al., 1999) showed empirically that an LR parser may be more accurate than the original CFG, if both are trained on the basis of the same treebank. In other words, the resulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding functi</context>
<context position="5617" citStr="Briscoe and Carroll, 1993" startWordPosition="906" endWordPosition="909">licit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e. if several transitions are applicable for a given stack, then the sum of probabilities assigned to those transitions by probability function pA should be 1. This assumption may be motivated by pragmatic considerations, as such a proper model is easy to train by relative frequency estimation: count the number of times a transition is applied with respect to a treebank, and divide it by the number of times the relevant stack symbol (or pair of stack symbols) occurs at the top of the stack. Let us call the re</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>T. Briscoe and J. Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
<author>S Geman</author>
</authors>
<title>Estimation of probabilistic context-free grammars.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="16904" citStr="Chi and Geman, 1998" startWordPosition="3046" endWordPosition="3049">grammars (PCFGs); we say a PCFG is proper if for each nonterminal A, the probabilities of all rules with left-hand side A sum to 1. Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees. In other words, if a probability distribution can be defined by attaching probabilities to rules, then we may reassign the probabilities such that that PCFG becomes proper, while preserving the probability distribution. This even holds if the input grammar is non-tight, meaning that probability mass is lost in ‘infinite derivations’ (S´anchez and Benedi, 1997; Chi and Geman, 1998; Chi, 1999; Nederhof and Satta, 2003). Although CFGs and PDTs are weakly equivalent, they behave very differently when they are extended with probabilities. In particular, there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality. Below we will discuss two constraints, which we will call properness and reverseproperness. Neither of these is suitable for all types of PDTs, but as we will show, the second is more suitable for probabilistic LR parsing than the first. This is surprising, as only properness has been described in exist</context>
</contexts>
<marker>Chi, Geman, 1998</marker>
<rawString>Z. Chi and S. Geman. 1998. Estimation of probabilistic context-free grammars. Computational Linguistics, 24(2):299–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="16915" citStr="Chi, 1999" startWordPosition="3050" endWordPosition="3051">say a PCFG is proper if for each nonterminal A, the probabilities of all rules with left-hand side A sum to 1. Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees. In other words, if a probability distribution can be defined by attaching probabilities to rules, then we may reassign the probabilities such that that PCFG becomes proper, while preserving the probability distribution. This even holds if the input grammar is non-tight, meaning that probability mass is lost in ‘infinite derivations’ (S´anchez and Benedi, 1997; Chi and Geman, 1998; Chi, 1999; Nederhof and Satta, 2003). Although CFGs and PDTs are weakly equivalent, they behave very differently when they are extended with probabilities. In particular, there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality. Below we will discuss two constraints, which we will call properness and reverseproperness. Neither of these is suitable for all types of PDTs, but as we will show, the second is more suitable for probabilistic LR parsing than the first. This is surprising, as only properness has been described in existing literat</context>
<context position="22068" citStr="Chi, 1999" startWordPosition="4008" endWordPosition="4009">e form (p, A, m). Without it, one could obtain an LR state q that does not match the underlying [p; X] in a reversed computation. We may assume without loss of generality that rules of type (12) are assigned probability 1, as a probability other than 1 could be moved to corresponding rules of types (10) or (11) where state q was introduced. In the same way, we may assume that transitions of type (6) are assigned probability 1. After making these assumptions, we obtain a bijection between probability functions pA for the PDT and probability functions pG for the cover CFG. As was shown by e.g. (Chi, 1999) and (Nederhof and Satta, 2003), properness for CFGs does not restrict the space of probability distributions, and thereby the same holds for reverse-properness for PDTs that implement the LR parsing strategy. It is now also clear that a reverse-proper LR parser can describe any probability distribution that the original CFG can. The proof is as follows. Given a probability function pG for the input CFG, we define a probability function pA for the LR parser, by letting transitions of types (2) and (3) • For LR state p and a E Σ such that goto(p, a) =� 0: [p; a] —* p (7) • For LR state p and (A</context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Z. Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M V Chitrao</author>
<author>R Grishman</author>
</authors>
<title>Statistical parsing of messages.</title>
<date>1990</date>
<booktitle>In Speech and Natural Language, Proceedings,</booktitle>
<pages>263--266</pages>
<location>Hidden Valley, Pennsylvania,</location>
<contexts>
<context position="5106" citStr="Chitrao and Grishman, 1990" startWordPosition="824" endWordPosition="827">ank. In other words, the resulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pg on rules of the original grammar. A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities. The explicit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e. </context>
</contexts>
<marker>Chitrao, Grishman, 1990</marker>
<rawString>M.V. Chitrao and R. Grishman. 1990. Statistical parsing of messages. In Speech and Natural Language, Proceedings, pages 263–266, Hidden Valley, Pennsylvania, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Workshop on Parsing Technologies,</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="5339" citStr="Collins, 2001" startWordPosition="863" endWordPosition="864"> parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities. The explicit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e. if several transitions are applicable for a given stack, then the sum of probabilities assigned to those transitions by probability function pA should be 1. This assumption may be motivated by pragmatic considerations, as such a prop</context>
</contexts>
<marker>Collins, 2001</marker>
<rawString>M. Collins. 2001. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In Proceedings of the Seventh International Workshop on Parsing Technologies, Beijing, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Harrison</author>
</authors>
<title>Introduction to Formal Language Theory.</title>
<date>1978</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="9325" citStr="Harrison, 1978" startWordPosition="1527" endWordPosition="1528">tion 2 we outline our formalization of LR parsing as a construction of PDTs from CFGs, making some superficial changes with respect to standard formulations. Properness and reverse-properness are discussed in Section 3, where we will show that reverse-properness does not restrict the space of probability distributions. Section 4 reports on experiments, and Section 5 concludes this article. 2 LR parsing As LR parsing has been extensively treated in existing literature, we merely recapitulate the main definitions here. For more explanation, the reader is referred to standard literature such as (Harrison, 1978; Sippu and Soisalon-Soininen, 1990). An LR parser is constructed on the basis of a CFG that is augmented with an additional rule 5† —* �- 5, where 5 is the former start symbol, and the new nonterminal 5† becomes the start symbol of the augmented grammar. The new terminal �- acts as an imaginary start-of-sentence marker. We denote the set of terminals by Σ and the set of nonterminals by N. We assume each rule has a unique label r. As explained before, we construct LR parsers as pushdown transducers. The main stack symbols of these automata are sets of dotted rules, which consist of rules from </context>
</contexts>
<marker>Harrison, 1978</marker>
<rawString>M.A. Harrison. 1978. Introduction to Formal Language Theory. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>V Sornlertlamvanich</author>
<author>H Tanaka</author>
<author>T Tokunaga</author>
</authors>
<title>Probabilistic GLR parsing.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and other Parsing Technologies, chapter 5,</booktitle>
<pages>85--104</pages>
<editor>In H. Bunt and A. Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="4189" citStr="Inui et al., 2000" startWordPosition="670" endWordPosition="673">istic LR parser defines a probability distribution on the set of parses, and thereby also a probability distribution on the set of sentences generated by grammar G. Disambiguation of an ambiguous sentence can be achieved on the basis of a comparison between the probabilities assigned to the respective parses by the probabilistic LR model. The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993) (see also (Su et al., 1991)). The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al., 2000). As we will not discuss lookahead here, this matter does not play a significant role in the current study. Noteworthy is that (Sornlertlamvanich et al., 1999) showed empirically that an LR parser may be more accurate than the original CFG, if both are trained on the basis of the same treebank. In other words, the resulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pg on rules of the original grammar. A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context, i.e. information on </context>
<context position="5641" citStr="Inui et al., 2000" startWordPosition="911" endWordPosition="914">babilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e. if several transitions are applicable for a given stack, then the sum of probabilities assigned to those transitions by probability function pA should be 1. This assumption may be motivated by pragmatic considerations, as such a proper model is easy to train by relative frequency estimation: count the number of times a transition is applied with respect to a treebank, and divide it by the number of times the relevant stack symbol (or pair of stack symbols) occurs at the top of the stack. Let us call the resulting probability func</context>
</contexts>
<marker>Inui, Sornlertlamvanich, Tanaka, Tokunaga, 2000</marker>
<rawString>K. Inui, V. Sornlertlamvanich, H. Tanaka, and T. Tokunaga. 2000. Probabilistic GLR parsing. In H. Bunt and A. Nijholt, editors, Advances in Probabilistic and other Parsing Technologies, chapter 5, pages 85–104. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="5122" citStr="Johnson, 1998" startWordPosition="828" endWordPosition="830">ulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pg on rules of the original grammar. A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities. The explicit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e. if several trans</context>
<context position="24705" citStr="Johnson, 1998" startWordPosition="4511" endWordPosition="4512">cision and recall are at least comparable to, if not better than, what we would obtain with the established method. The experiments were performed on the Wall Street Journal (WSJ) corpus, from the Penn Treebank, version II. Training was done on sections 02- 21, i.e., first a context-free grammar was derived from the ‘stubs’ of the combined trees, taking parts of speech as leaves of the trees, omitting all affixes from the nonterminal names, and removing ε- generating subtrees. Such preprocessing of the WSJ corpus is consistent with earlier attempts to derive CFGs from that corpus, as e.g. by (Johnson, 1998). The obtained CFG has 10,035 rules. The dimensions of the LR parser constructed from this grammar are given in Table 1. The PDT was then trained on the trees from the same sections 02-21, to determine the number of times that transitions are used. At first sight it is not clear how to determine this on the basis of the treebank, as the structure of LR parsers is very different from the structure of the grammars from which they are constructed. The solution is to construct a second PDT from the PDT to be trained, replacing each transition α ��� �—* Q with label r by transition ��� α �—* Q. By </context>
<context position="27980" citStr="Johnson, 1998" startWordPosition="5058" endWordPosition="5060">ower than the total number of parameters can be explained as follows. First, the treebank contains many rules that occur a small number of times. Secondly, the LR automaton is much larger than the CFG; in general, the size of an LR automaton is bounded by a function that is exponential in the size of the input CFG. Therefore, if we use the same treebank to estimate the probability function, then many transitions are never visited and obtain a zero probability. We have applied the two trained LR automata on section 22 of the WSJ corpus, measuring labelled precision and recall, as done by e.g. (Johnson, 1998).2 We observe that in the case of reverseproperness, precision and recall are slightly better. 2We excluded all sentences with more than 30 words however, as some required prohibitive amounts of memory. Only one of the remaining 1441 sentences was not accepted by the parser. The most important conclusion that can be drawn from this is that the substantially larger space of obtainable probability distributions offered by the reverse-properness method does not come at the expense of a degradation of accuracy for large grammars such as those derived from the WSJ. For comparison, with a standard P</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Kipps</author>
</authors>
<title>GLR parsing in time 0(n3).</title>
<date>1991</date>
<booktitle>Generalized LR Parsing, chapter 4,</booktitle>
<pages>43--59</pages>
<editor>In M. Tomita, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="12677" citStr="Kipps, 1991" startWordPosition="2201" endWordPosition="2202">t-most derivation of the input. On the basis of the original grammar, the corresponding parse tree can be constructed from such an output string. There are a few superficial differences with LR parsing as it is commonly found in the literature. The most obvious difference is that we divide reductions into ‘binary’ steps. The main reason is that this allows tabular interpretation with a time complexity cubic in the length of the input. Otherwise, the time complexity would be O(nm+1), where m is the length of the longest right-hand side of a rule in the CFG. This observation was made before by (Kipps, 1991), who proposed a solution similar to ours, albeit formulated differently. See also a related formulation of tabular LR parsing by (Nederhof and Satta, 1996). To be more specific, instead of one step of the PDT taking stack: σp0p1 · · · pm immediately to stack: σp0q where (A → X1 · · · Xm •) ∈ pm, σ is a string of stack symbols and goto(p0, A) = q, we have a number of smaller steps leading to a series of stacks: σp0p1 · · · pm−1pm σp0p1 ··· pm−1(A, m−1) σp0p1 · · · (A, m−2) . .. σp0(A, 0) σp0q There are two additional differences. First, we want to avoid steps of the form: σp0(A, 0) σp0q by tra</context>
</contexts>
<marker>Kipps, 1991</marker>
<rawString>J.R. Kipps. 1991. GLR parsing in time 0(n3). In M. Tomita, editor, Generalized LR Parsing, chapter 4, pages 43–59. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<booktitle>In Automata, Languages and Programming, 2nd Colloquium,</booktitle>
<volume>14</volume>
<pages>255--269</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1905" citStr="Lang, 1974" startWordPosition="290" endWordPosition="291">or natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (PDT). In this context, the LR parsing strategy can be seen as a particular mapping from context-free grammars to PDTs. The acronym ‘LR’ stands for ‘Left-to-right processing of the input, producing a Right-most derivation (in reverse)’. When we construct a PDT A from a CFG G by the LR parsing strategy and apply it on an input sentence, then the set of output strings of A represents the set of all right-most derivations that G allows for that sentence. Such an output string enumerates the rules (or labels that identify the rul</context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>B. Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In Automata, Languages and Programming, 2nd Colloquium, volume 14 of Lecture Notes in Computer Science, pages 255–269, Saarbr¨ucken. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>M Tomita</author>
</authors>
<title>GLR∗ – an efficient noise-skipping parsing algorithm for context free grammars.</title>
<date>1993</date>
<booktitle>In Third International Workshop on Parsing Technologies,</booktitle>
<pages>123--134</pages>
<institution>Tilburg (The Netherlands) and Durbuy (Belgium),</institution>
<contexts>
<context position="1349" citStr="Lavie and Tomita, 1993" startWordPosition="198" endWordPosition="201">, under the restrictions of the existing approaches to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 1 Introduction The LR parsing strategy was originally devised for programming languages (Sippu and SoisalonSoininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an i</context>
</contexts>
<marker>Lavie, Tomita, 1993</marker>
<rawString>A. Lavie and M. Tomita. 1993. GLR∗ – an efficient noise-skipping parsing algorithm for context free grammars. In Third International Workshop on Parsing Technologies, pages 123–134, Tilburg (The Netherlands) and Durbuy (Belgium), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Efficient tabular LR parsing.</title>
<date>1996</date>
<booktitle>In 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>239--246</pages>
<location>Santa Cruz, California, USA,</location>
<contexts>
<context position="12833" citStr="Nederhof and Satta, 1996" startWordPosition="2223" endWordPosition="2226">g. There are a few superficial differences with LR parsing as it is commonly found in the literature. The most obvious difference is that we divide reductions into ‘binary’ steps. The main reason is that this allows tabular interpretation with a time complexity cubic in the length of the input. Otherwise, the time complexity would be O(nm+1), where m is the length of the longest right-hand side of a rule in the CFG. This observation was made before by (Kipps, 1991), who proposed a solution similar to ours, albeit formulated differently. See also a related formulation of tabular LR parsing by (Nederhof and Satta, 1996). To be more specific, instead of one step of the PDT taking stack: σp0p1 · · · pm immediately to stack: σp0q where (A → X1 · · · Xm •) ∈ pm, σ is a string of stack symbols and goto(p0, A) = q, we have a number of smaller steps leading to a series of stacks: σp0p1 · · · pm−1pm σp0p1 ··· pm−1(A, m−1) σp0p1 · · · (A, m−2) . .. σp0(A, 0) σp0q There are two additional differences. First, we want to avoid steps of the form: σp0(A, 0) σp0q by transitions p0 (A, 0) ε,ε 7→ p0 q, as such transitions complicate the generic definition of ‘properness’ for PDTs, to be discussed in the following section. Fo</context>
</contexts>
<marker>Nederhof, Satta, 1996</marker>
<rawString>M.-J. Nederhof and G. Satta. 1996. Efficient tabular LR parsing. In 34th Annual Meeting of the Association for Computational Linguistics, pages 239–246, Santa Cruz, California, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Probabilistic parsing as intersection.</title>
<date>2003</date>
<booktitle>In 8th International Workshop on Parsing Technologies,</booktitle>
<pages>137--148</pages>
<location>LORIA, Nancy, France,</location>
<contexts>
<context position="16942" citStr="Nederhof and Satta, 2003" startWordPosition="3052" endWordPosition="3055">is proper if for each nonterminal A, the probabilities of all rules with left-hand side A sum to 1. Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees. In other words, if a probability distribution can be defined by attaching probabilities to rules, then we may reassign the probabilities such that that PCFG becomes proper, while preserving the probability distribution. This even holds if the input grammar is non-tight, meaning that probability mass is lost in ‘infinite derivations’ (S´anchez and Benedi, 1997; Chi and Geman, 1998; Chi, 1999; Nederhof and Satta, 2003). Although CFGs and PDTs are weakly equivalent, they behave very differently when they are extended with probabilities. In particular, there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality. Below we will discuss two constraints, which we will call properness and reverseproperness. Neither of these is suitable for all types of PDTs, but as we will show, the second is more suitable for probabilistic LR parsing than the first. This is surprising, as only properness has been described in existing literature on probabilistic PDTs (</context>
<context position="22099" citStr="Nederhof and Satta, 2003" startWordPosition="4011" endWordPosition="4015">. Without it, one could obtain an LR state q that does not match the underlying [p; X] in a reversed computation. We may assume without loss of generality that rules of type (12) are assigned probability 1, as a probability other than 1 could be moved to corresponding rules of types (10) or (11) where state q was introduced. In the same way, we may assume that transitions of type (6) are assigned probability 1. After making these assumptions, we obtain a bijection between probability functions pA for the PDT and probability functions pG for the cover CFG. As was shown by e.g. (Chi, 1999) and (Nederhof and Satta, 2003), properness for CFGs does not restrict the space of probability distributions, and thereby the same holds for reverse-properness for PDTs that implement the LR parsing strategy. It is now also clear that a reverse-proper LR parser can describe any probability distribution that the original CFG can. The proof is as follows. Given a probability function pG for the input CFG, we define a probability function pA for the LR parser, by letting transitions of types (2) and (3) • For LR state p and a E Σ such that goto(p, a) =� 0: [p; a] —* p (7) • For LR state p and (A •) E p, where A —* ε has label</context>
</contexts>
<marker>Nederhof, Satta, 2003</marker>
<rawString>M.-J. Nederhof and G. Satta. 2003. Probabilistic parsing as intersection. In 8th International Workshop on Parsing Technologies, pages 137– 148, LORIA, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Probabilistic parsing strategies.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7290" citStr="Nederhof and Satta, 2004" startWordPosition="1194" endWordPosition="1197">s than prfe does. (By ‘consistent’ we mean that the probabilities of all strings that are accepted sum to 1.) It may even be the case that a (proper and consistent) probability function pg on the rules of the input grammar !9 exists that assigns a higher likelihood to the corpus than prfe, and therefore it is not guaranteed that LR parsers allow better probability estimates than the CFGs from which they were constructed, if we constrain probability functions pA to be proper. In this respect, LR parsing differs from at least one other well-known parsing strategy, viz. left-corner parsing. See (Nederhof and Satta, 2004) for a discussion of a property that is shared by left-corner parsing but not by LR parsing, and which explains the above difference. As main contribution of this paper we establish that this restriction on expressible probability distributions can be dispensed with, without losing the ability to perform training by relative frequency estimation. What comes in place of properness is reverse-properness, which can be seen as properness of the reversed pushdown automaton that processes input from right to left instead of from left to right, interpreting the transitions of A backwards. As we will </context>
<context position="15011" citStr="Nederhof and Satta, 2004" startWordPosition="2712" endWordPosition="2715">N such that goto(p, X) = q =6 ∅: [p; X] ε,ε 7→ [p; X] q (6) Figure 1: The transitions of a PDT implementing LR(0) parsing. following section. By this condition, we consider LR parsing as being performed from right to left, so backwards with regard to the normal processing order. If we were to omit the first components p from stack symbols (p, A, m), we may obtain ‘dead ends’ in the computation. We know that such dead ends make a (reverse-)proper PDT inconsistent, as probability mass lost in dead ends causes the sum of probabilities of all computations to be strictly smaller than 1. (See also (Nederhof and Satta, 2004).) It is interesting to note that the addition of the components p to stack symbols (p, A, m) does not increase the number of transitions, and the nature of LR parsing in the normal processing order from left to right is preserved. With all these changes together, reductions are implemented by transitions resulting in the following sequence of stacks: σ0[p0; X0][p1; X1] ··· [pm−1; Xm−1]pm σ0[p0; X0][p1; X1] ··· [pm−1; Xm−1](pm, A, m−1) σ0[p0; X0][p1; X1] ··· (pm−1, A, m−2) . .. σ0[p0; X0](p1, A, 0) σ0[p0; A] σ0[p0; A]q Please note that transitions of the form [p; X] (q, A, m) ε,ε 7→ (p, A, m −</context>
<context position="19738" citStr="Nederhof and Satta, 2004" startWordPosition="3575" endWordPosition="3578">robability of a particular transition Q P ��� �� R by relative frequency estimation. The resulting PPDT is proper. It has been shown that imposing properness is without loss of generality in the case of PDTs constructed by a wide range of parsing strategies, among which are top-down parsing and left-corner parsing. This does not hold for PDTs constructed by the LR parsing strategy however, and in fact, properness for such automata may reduce the expressive power in terms of available probability distributions to strictly less than that offered by the original CFG. This was formally proven by (Nederhof and Satta, 2004), after (Ng and Tomita, 1991) and (Wright and Wrigley, 1991) had already suggested that creating a probabilistic LR parser that is equivalent to an input PCFG is difficult in general. The same difficulty for ELR parsing was suggested by (Tendeau, 1997). For this reason, we investigate a practical alternative, viz. reverse-properness. Now we have to assume that for each stack symbol R, we either have one or more transitions of the form P ��� �� R or Q P ��� �� R, or one or more transitions of the form P��� �� P R, but no combination thereof. In the first case, reverse-properness demands that th</context>
</contexts>
<marker>Nederhof, Satta, 2004</marker>
<rawString>M.-J. Nederhof and G. Satta. 2004. Probabilistic parsing strategies. In 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-K Ng</author>
<author>M Tomita</author>
</authors>
<title>Probabilistic LR parsing for general context-free grammars.</title>
<date>1991</date>
<booktitle>In Proc. of the Second International Workshop on Parsing Technologies,</booktitle>
<pages>154--163</pages>
<location>Cancun, Mexico,</location>
<contexts>
<context position="19767" citStr="Ng and Tomita, 1991" startWordPosition="3580" endWordPosition="3583">ion Q P ��� �� R by relative frequency estimation. The resulting PPDT is proper. It has been shown that imposing properness is without loss of generality in the case of PDTs constructed by a wide range of parsing strategies, among which are top-down parsing and left-corner parsing. This does not hold for PDTs constructed by the LR parsing strategy however, and in fact, properness for such automata may reduce the expressive power in terms of available probability distributions to strictly less than that offered by the original CFG. This was formally proven by (Nederhof and Satta, 2004), after (Ng and Tomita, 1991) and (Wright and Wrigley, 1991) had already suggested that creating a probabilistic LR parser that is equivalent to an input PCFG is difficult in general. The same difficulty for ELR parsing was suggested by (Tendeau, 1997). For this reason, we investigate a practical alternative, viz. reverse-properness. Now we have to assume that for each stack symbol R, we either have one or more transitions of the form P ��� �� R or Q P ��� �� R, or one or more transitions of the form P��� �� P R, but no combination thereof. In the first case, reverse-properness demands that the sum of probabilities of all</context>
</contexts>
<marker>Ng, Tomita, 1991</marker>
<rawString>S.-K. Ng and M. Tomita. 1991. Probabilistic LR parsing for general context-free grammars. In Proc. of the Second International Workshop on Parsing Technologies, pages 154–163, Cancun, Mexico, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ruland</author>
</authors>
<title>A context-sensitive model for probabilistic LR parsing of spoken language with transformation-based postprocessing.</title>
<date>2000</date>
<booktitle>In The 18th International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>677--683</pages>
<location>Saarbr¨ucken, Germany, July–August.</location>
<contexts>
<context position="1391" citStr="Ruland, 2000" startWordPosition="206" endWordPosition="207"> to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 1 Introduction The LR parsing strategy was originally devised for programming languages (Sippu and SoisalonSoininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (PDT). In this co</context>
<context position="26754" citStr="Ruland, 2000" startWordPosition="4852" endWordPosition="4853">this eliminates a considerable amount of nondeterminism. total # transitions 8,340,315 # push transitions 753,224 # swap transitions 589,811 # pop transitions 6,997,280 Table 1: Dimensions of PDT implementing LR strategy for CFG derived from WSJ, sect. 02-21. proper rev.-prop. 577,650 6,589,716 137,134 137,134 0.772 0.777 0.747 0.749 Table 2: The two methods of training, based on properness and reverse-properness. ing or back-off, as this could obscure properties inherent in the difference between the two discussed training methods. (Back-off for probabilistic LR parsing has been proposed by (Ruland, 2000).) All transitions that were not seen during training were given probability 0. The results are outlined in Table 2. Note that the number of free parameters in the case of reverseproperness is much larger than in the case of normal properness. Despite of this, the number of transitions that actually receive non-zero probabilities is (predictably) identical in both cases, viz. 137,134. However, the potential for fine-grained probability estimates and for smoothing and parameter-tying techniques is clearly greater in the case of reverseproperness. That in both cases the number of non-zero probab</context>
</contexts>
<marker>Ruland, 2000</marker>
<rawString>T. Ruland. 2000. A context-sensitive model for probabilistic LR parsing of spoken language with transformation-based postprocessing. In The 18th International Conference on Computational Linguistics, volume 2, pages 677–683, Saarbr¨ucken, Germany, July–August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-A S´anchez</author>
<author>J-M Benedi</author>
</authors>
<title>Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>9</issue>
<marker>S´anchez, Benedi, 1997</marker>
<rawString>J.-A. S´anchez and J.-M. Benedi. 1997. Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(9):1052–1055, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Santos</author>
</authors>
<title>Probabilistic grammars and automata.</title>
<date>1972</date>
<journal>Information and Control,</journal>
<pages>21--27</pages>
<contexts>
<context position="3213" citStr="Santos, 1972" startWordPosition="509" endWordPosition="510">rsers do not use lookahead to decide between alternative transitions, they are called LR(0) parsers. More generally, if LR parsers look ahead k symbols, they are called LR(k) parsers; some simplified LR parsing models that use lookahead are called SLR(k) and LALR(k) parsing (Sippu and Soisalon-Soininen, 1990). In order to simplify the discussion, we abstain from using lookahead in this article, and ‘LR parsing’ can further be read as ‘LR(0) parsing’. We would like to point out however that our observations carry over to LR parsing with lookahead. The theory of probabilistic pushdown automata (Santos, 1972) can be easily applied to LR parsing. A probability is then assigned to each transition, by a function that we will call the probability function pA, and the probability of an accepting computation of A is the product of the probabilities of the applied transitions. As each accepting computation produces a right-most derivation as output string, a probabilistic LR parser defines a probability distribution on the set of parses, and thereby also a probability distribution on the set of sentences generated by grammar G. Disambiguation of an ambiguous sentence can be achieved on the basis of a com</context>
</contexts>
<marker>Santos, 1972</marker>
<rawString>E.S. Santos. 1972. Probabilistic grammars and automata. Information and Control, 21:27–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sippu</author>
<author>E Soisalon-Soininen</author>
</authors>
<title>Parsing Theory, Vol. II: LR(k) and LL(k) Parsing,</title>
<date>1990</date>
<journal>of EATCS Monographs on Theoretical Computer Science.</journal>
<volume>20</volume>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="2910" citStr="Sippu and Soisalon-Soininen, 1990" startWordPosition="457" endWordPosition="460"> and apply it on an input sentence, then the set of output strings of A represents the set of all right-most derivations that G allows for that sentence. Such an output string enumerates the rules (or labels that identify the rules uniquely) that occur in the corresponding right-most derivation, in reversed order. If LR parsers do not use lookahead to decide between alternative transitions, they are called LR(0) parsers. More generally, if LR parsers look ahead k symbols, they are called LR(k) parsers; some simplified LR parsing models that use lookahead are called SLR(k) and LALR(k) parsing (Sippu and Soisalon-Soininen, 1990). In order to simplify the discussion, we abstain from using lookahead in this article, and ‘LR parsing’ can further be read as ‘LR(0) parsing’. We would like to point out however that our observations carry over to LR parsing with lookahead. The theory of probabilistic pushdown automata (Santos, 1972) can be easily applied to LR parsing. A probability is then assigned to each transition, by a function that we will call the probability function pA, and the probability of an accepting computation of A is the product of the probabilities of the applied transitions. As each accepting computation </context>
<context position="9361" citStr="Sippu and Soisalon-Soininen, 1990" startWordPosition="1529" endWordPosition="1532">e our formalization of LR parsing as a construction of PDTs from CFGs, making some superficial changes with respect to standard formulations. Properness and reverse-properness are discussed in Section 3, where we will show that reverse-properness does not restrict the space of probability distributions. Section 4 reports on experiments, and Section 5 concludes this article. 2 LR parsing As LR parsing has been extensively treated in existing literature, we merely recapitulate the main definitions here. For more explanation, the reader is referred to standard literature such as (Harrison, 1978; Sippu and Soisalon-Soininen, 1990). An LR parser is constructed on the basis of a CFG that is augmented with an additional rule 5† —* �- 5, where 5 is the former start symbol, and the new nonterminal 5† becomes the start symbol of the augmented grammar. The new terminal �- acts as an imaginary start-of-sentence marker. We denote the set of terminals by Σ and the set of nonterminals by N. We assume each rule has a unique label r. As explained before, we construct LR parsers as pushdown transducers. The main stack symbols of these automata are sets of dotted rules, which consist of rules from the augmented grammar with a disting</context>
</contexts>
<marker>Sippu, Soisalon-Soininen, 1990</marker>
<rawString>S. Sippu and E. Soisalon-Soininen. 1990. Parsing Theory, Vol. II: LR(k) and LL(k) Parsing, volume 20 of EATCS Monographs on Theoretical Computer Science. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sornlertlamvanich</author>
<author>K Inui</author>
<author>H Tanaka</author>
<author>T Tokunaga</author>
<author>T Takezawa</author>
</authors>
<title>Empirical support for new probabilistic generalized LR parsing.</title>
<date>1999</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="4348" citStr="Sornlertlamvanich et al., 1999" startWordPosition="696" endWordPosition="699">ated by grammar G. Disambiguation of an ambiguous sentence can be achieved on the basis of a comparison between the probabilities assigned to the respective parses by the probabilistic LR model. The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993) (see also (Su et al., 1991)). The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al., 2000). As we will not discuss lookahead here, this matter does not play a significant role in the current study. Noteworthy is that (Sornlertlamvanich et al., 1999) showed empirically that an LR parser may be more accurate than the original CFG, if both are trained on the basis of the same treebank. In other words, the resulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pg on rules of the original grammar. A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CF</context>
</contexts>
<marker>Sornlertlamvanich, Inui, Tanaka, Tokunaga, Takezawa, 1999</marker>
<rawString>V. Sornlertlamvanich, K. Inui, H. Tanaka, T. Tokunaga, and T. Takezawa. 1999. Empirical support for new probabilistic generalized LR parsing. Journal of Natural Language Processing, 6(3):3–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-Y Su</author>
<author>J-N Wang</author>
<author>M-H Su</author>
<author>J-S Chang</author>
</authors>
<title>GLR parsing with scoring.</title>
<date>1991</date>
<booktitle>Generalized LR Parsing, chapter 7,</booktitle>
<pages>93--112</pages>
<editor>In M. Tomita, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="4051" citStr="Su et al., 1991" startWordPosition="648" endWordPosition="651">he probabilities of the applied transitions. As each accepting computation produces a right-most derivation as output string, a probabilistic LR parser defines a probability distribution on the set of parses, and thereby also a probability distribution on the set of sentences generated by grammar G. Disambiguation of an ambiguous sentence can be achieved on the basis of a comparison between the probabilities assigned to the respective parses by the probabilistic LR model. The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993) (see also (Su et al., 1991)). The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al., 2000). As we will not discuss lookahead here, this matter does not play a significant role in the current study. Noteworthy is that (Sornlertlamvanich et al., 1999) showed empirically that an LR parser may be more accurate than the original CFG, if both are trained on the basis of the same treebank. In other words, the resulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pg on rules of the origin</context>
</contexts>
<marker>Su, Wang, Su, Chang, 1991</marker>
<rawString>K.-Y. Su, J.-N. Wang, M.-H. Su, and J.-S. Chang. 1991. GLR parsing with scoring. In M. Tomita, editor, Generalized LR Parsing, chapter 7, pages 93–112. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Tendeau</author>
</authors>
<title>Analyse syntaxique et s´emantique avec ´evaluation d’attributs dans un demi-anneau.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Orl´eans.</institution>
<contexts>
<context position="19990" citStr="Tendeau, 1997" startWordPosition="3619" endWordPosition="3620">g which are top-down parsing and left-corner parsing. This does not hold for PDTs constructed by the LR parsing strategy however, and in fact, properness for such automata may reduce the expressive power in terms of available probability distributions to strictly less than that offered by the original CFG. This was formally proven by (Nederhof and Satta, 2004), after (Ng and Tomita, 1991) and (Wright and Wrigley, 1991) had already suggested that creating a probabilistic LR parser that is equivalent to an input PCFG is difficult in general. The same difficulty for ELR parsing was suggested by (Tendeau, 1997). For this reason, we investigate a practical alternative, viz. reverse-properness. Now we have to assume that for each stack symbol R, we either have one or more transitions of the form P ��� �� R or Q P ��� �� R, or one or more transitions of the form P��� �� P R, but no combination thereof. In the first case, reverse-properness demands that the sum of probabilities of all transitions P ��� �� R or Q P ��� �� R is 1, and in the second case reverse-properness demands that the sum of probabilities of transitions P��� �� P R is 1 for each P. Again, our assumption above is without loss of genera</context>
</contexts>
<marker>Tendeau, 1997</marker>
<rawString>F. Tendeau. 1997. Analyse syntaxique et s´emantique avec ´evaluation d’attributs dans un demi-anneau. Ph.D. thesis, University of Orl´eans.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1986</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1824" citStr="Tomita, 1986" startWordPosition="276" endWordPosition="277">Soininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (PDT). In this context, the LR parsing strategy can be seen as a particular mapping from context-free grammars to PDTs. The acronym ‘LR’ stands for ‘Left-to-right processing of the input, producing a Right-most derivation (in reverse)’. When we construct a PDT A from a CFG G by the LR parsing strategy and apply it on an input sentence, then the set of output strings of A represents the set of all right-most derivations that G allows for that sent</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>M. Tomita. 1986. Efficient Parsing for Natural Language. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Wright</author>
<author>E N Wrigley</author>
</authors>
<title>GLR parsing with probability.</title>
<date>1991</date>
<booktitle>Generalized LR Parsing, chapter 8,</booktitle>
<pages>113--128</pages>
<editor>In M. Tomita, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="19798" citStr="Wright and Wrigley, 1991" startWordPosition="3585" endWordPosition="3588">ve frequency estimation. The resulting PPDT is proper. It has been shown that imposing properness is without loss of generality in the case of PDTs constructed by a wide range of parsing strategies, among which are top-down parsing and left-corner parsing. This does not hold for PDTs constructed by the LR parsing strategy however, and in fact, properness for such automata may reduce the expressive power in terms of available probability distributions to strictly less than that offered by the original CFG. This was formally proven by (Nederhof and Satta, 2004), after (Ng and Tomita, 1991) and (Wright and Wrigley, 1991) had already suggested that creating a probabilistic LR parser that is equivalent to an input PCFG is difficult in general. The same difficulty for ELR parsing was suggested by (Tendeau, 1997). For this reason, we investigate a practical alternative, viz. reverse-properness. Now we have to assume that for each stack symbol R, we either have one or more transitions of the form P ��� �� R or Q P ��� �� R, or one or more transitions of the form P��� �� P R, but no combination thereof. In the first case, reverse-properness demands that the sum of probabilities of all transitions P ��� �� R or Q P </context>
</contexts>
<marker>Wright, Wrigley, 1991</marker>
<rawString>J.H. Wright and E.N. Wrigley. 1991. GLR parsing with probability. In M. Tomita, editor, Generalized LR Parsing, chapter 8, pages 113–128. Kluwer Academic Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>