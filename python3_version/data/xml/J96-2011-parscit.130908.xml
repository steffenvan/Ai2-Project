<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000320">
<note confidence="0.5952155">
Letters to the Editor
From Eric Ristad:
</note>
<bodyText confidence="0.999927481481481">
The purpose of this letter is to advise the journal&apos;s readership of some misstatements
in Alexis Manaster Ramer &apos;s review of my book The Language Complexity Game (LCG)
[The MIT Press, 1993] in Computational Linguistics 21(1), March 1995, 124-131 (&amp;quot;the
review,&amp;quot; henceforth).
LCG presents a theory, at the highest level of computational abstraction, of the
internal computations performed by language users. The inputs to the LCG computa-
tions are mental representations, as are the outputs. Thus, the review is mistaken to
claim that &amp;quot;the problem studied [in LCG] is ... that of determining what meanings
are possible for a given set of sentences&amp;quot; (p. 124). No computational problem stud-
ied in LCG has sentences as its inputs or as its outputs. The input to the Anaphoric
Agreement problem considered in Chapter 3 consists of sets of anaphoric elements
and available antecedents. The output is a valid relation of immediate antecedence
(pp. 33-34). No sentences are involved. The input to the Referential Dependencies
problem considered in Chapter 4 is a linguistic representation lacking only relations
of obviation and antecedence. The output is the missing relations of obviation and
antecedence (pp. 49-50). Again, no sentences are involved. The same holds for the
Anaphoric Copying and Anaphoric Sharing problems considered in Chapter 5: no
sentences. This may help explain why the reviewer is unable to find even a &amp;quot;single
complete example&amp;quot; of an input sentence for the LCG complexity analysis of the Refer-
ential Dependencies problem (p. 127) or why &amp;quot;the translation from Boolean formulas
to English sentences is not fully specified&amp;quot; (p. 128).
The review&apos;s presentation of the LCG complexity analysis is not wholly accurate
either. For example, on page 125, the review states that examples such as (1) are
&amp;quot;completely irrelevant to the NP-hardness argument&amp;quot; and that &amp;quot;to get NP-hardness
one would need to consider examples in which pronouns with different features are
available.&amp;quot; Moreover, the review claims that &amp;quot;an infinite set of inflectional features&amp;quot;
are required &amp;quot;to get NP-hardness.&amp;quot;
</bodyText>
<listItem confidence="0.6025025">
(1) Before Bill, Tom, and Jack were friends, he wanted him to introduce him
to him.
</listItem>
<bodyText confidence="0.995245133333334">
As explained in section 4.1.1 of LCG, example (1) exemplifies a local c-command con-
figuration. Configurations of local c-command are essential in the reduction from 3SAT
to the Referential Dependencies problem (lemma 3, p. 64). They are used in both the
variable gadget (Figure 4.3) and the clause gadget (Figure 4.4). And contrary to the
review&apos;s second claim, absolutely no inflectional features are used in this reduction.
In the foreword to LCG, Robert Berwick correctly notes that examples such as (1) play
an important role in establishing the NP-hardness of the Anaphora Problem (p. xiii).
So the review&apos;s claim that &amp;quot;Berwick misinterprets what the result is all about&amp;quot; (p. 125)
is also unfounded.
Nor does the review accurately convey the statement of the relevant lemma. The
review claims &amp;quot;that [this] proof does not state what the reduction from 3-SAT is to,
which is a serious matter for a reduction proof&amp;quot; (p. 127). Turning to the middle of page
64 in LCG, we see &amp;quot;LEMMA 3 3SAT &lt;p Referential Dependencies.&amp;quot; As is standard
in computer science—and explained on page 135 in the appendix on mathematical
background—the symbol &lt;7, denotes the relation of polynomial time reducibility be-
</bodyText>
<note confidence="0.852852">
Letters to the Editor
</note>
<bodyText confidence="0.999877764705883">
tween computational problems. So Lemma 3 states that the 3SAT problem reduces to
the Referential Dependencies problem in polynomial time. A quick check of the index
reveals that 3SAT is defined on page 137 and Referential Dependencies is defined on
page 49.
The technical content of LCG is to define language computations involving ana-
phora, in the form of problem statements, and to analyze their computational complex-
ity The review misstates the computational problems involved and does not accurately
report the lemmas or their proofs. So I am concerned that the review may not have
accurately represented LCG to the journal&apos;s readership.
After raising these (and other) technical issues, the review questions the distinc-
tion between a computational theory and a generative theory. In particular, the review
states that &amp;quot;it is by no means clear . .. that this [computational approach] is .. . any dif-
ferent from, for example, Chomsky&apos;s [generative] approach&amp;quot; (p. 130). As summarized
on page 113 and pages 115-6 of LCG, a generative theory of language is an explicit
procedure for enumerating the set of linguistic representations. The computations spec-
ified by a generative theory enumerate linguistic representations; these computations
are called derivations. Derivations are not directly attributed to language users, contra
the Derivational Theory of Complexity. In contrast, a computational theory of language
is a theory of the mental computations performed in the comprehension, production,
and acquisition of human languages. Thus, the computations specified by a computa-
tional theory are indeed attributed to language users. For this reason, a computational
theory such as that of LCG offers a more internalized view of language than is offered
by a generative theory.
The review goes on to claim that LCG &amp;quot;does not attempt .. . to show that there are
factual reasons for preferring this [computational] view of language&amp;quot; to the generative
approach (p. 130). In Section 6.3, LCG argues that, unlike the generative approach, the
computational approach more naturally results in language models that are accurate
in computationally significant respects. Thus, the complexity analysis of anaphoric
agreement in Chapter 3 leads to an empirical critique of the standard &amp;quot;nondistinctness&amp;quot;
theory of agreement as well as a simpler theory of agreement. The complexity analysis
of referential dependencies in Chapter 4 leads to a better understanding of obviation
in elliptical contexts. The complexity analysis of ellipsis in Chapter 5 raises some
empirical problems for the popular &amp;quot;copy&amp;quot; theory of ellipsis, and proposes a simpler
alternative based on the sharing of thematic functions. And so the review&apos;s claim that
LCG &amp;quot;gives no basis for preferring this [computational] view of language to what [LCG]
takes to be Chomsky&apos;s version of the I-language perspective&amp;quot; (p. 130) is misleading.
In addition to these technical and conceptual issues, Manaster Ramer &apos;s review also
raises some questions of style and proportion.
The review opens with the statement that &amp;quot;Ristad consistently and mistakenly
uses the words prove and proof when speaking not only about the results concerning
well-defined mathematical objects .. . but also about the claims that (some) NLs are
properly modeled by these mathematical objects .. . &amp;quot; (p. 124). By my count, the words
prove and proof appear a total of 66 times in LCG. One instance is nontechnical: &amp;quot;prove
useful&amp;quot; (p. 11). The remaining 65 instances refer to mathematical proofs, typically to
proofs of the computational complexity of precisely stated computational problems
based on particular formal models of language. I am unable to find a single case
in LCG where either proof or prove is used in an empirical argument about human
languages. So the review is half right here: my usage is consistent, but not in the
manner that the review claims.
Shortly thereafter, the review states that &amp;quot;the whole argument about the NP-
hardness of the anaphora problem in agreement situations . .. was the centerpiece
</bodyText>
<page confidence="0.986352">
287
</page>
<note confidence="0.419119">
Computational Linguistics Volume 22, Number 2
</note>
<bodyText confidence="0.999306833333333">
of [Ristad&apos;s] dissertation&amp;quot; (p. 125). My doctoral dissertation includes complexity anal-
yses of segmental phonology, autosegmental phonology, morpho-syntax, ellipsis, and
anaphoric relations. The entire discussion of anaphoric agreement occupies only one
page out of more than 140 pages in the dissertation, less than one section out of the
more than 40 sections. So it does not seem entirely accurate to call the discussion of
anaphoric agreement the &amp;quot;centerpiece&amp;quot; of the dissertation.
</bodyText>
<figure confidence="0.980369833333333">
Eric Ristad
Department of Computer Science
Princeton University
Princeton, NJ 08544
ristad@cs.princeton.edu
From Alexis Manaster Ramer:
</figure>
<bodyText confidence="0.992915178571429">
Dr. Ristad claims that my review misrepresented both him and Robert C. Berwick,
the author of the effusive introduction to his book (Ristad 1993; henceforth LCG). In
my review, I praised LCG&apos;s insights into certain linguistic problems with agreement,
anaphora, and ellipsis, but I criticized some of its claims about English, its treatment of
mentalism, and its argumentation about the complexity-theoretic properties of natural
languages, specifically, the claims that English or even NLs in general are NP-complete
(that is, NP-hard and included in the class .A1-2). Dr. Ristad does not take issue with
the most important of my critical comments, dealing with the fundamentally empirical
nature of his claims and the extreme fragility of his claims about English. A laborious
examination of the points which he does appeal will show that I was right on those
points too.
1. The first point of contention concerns the term &apos;sentence&apos;. Dr. Ristad&apos;s complaint is
that I should never have mentioned &amp;quot;sentences,&amp;quot; since, for example, &amp;quot;[t]he input to
the Referential Dependencies problem considered in Chapter 4 is a linguistic represen-
tation lacking only relations of obviation and antecedence. The output is the missing
relations of obviation and antecedence.., no sentences are involved,&amp;quot; and likewise for
the other problems he studied. However, the examples in LCG of English structures
illustrating these problems are usually sentences, such as (1), and Ristad (1993) him-
self refers to them as &amp;quot;sentence&amp;quot;s (e.g., p. 54, lines 13, 15, 16, 18). The &amp;quot;proof&amp;quot; of the
NP-hardness of Referential Dependencies (p. 64) involves a reduction that translates
a Boolean formula into a &amp;quot;linguistic representation ... shown schematically in figure
4.2&amp;quot;; the figure shows us a highly schematic tree structure of an English sentence, a
part of which would be (1).
(1) True met him2, whoA hei expected him3 to want him4 framed, whoB he5
believed he6 did [e] with tB, after exposing himself5 to [e] for tA, before
telling himselfi to [e].
The only genuine issue I can see here, therefore, is whether the term &apos;sentence&apos;
may be used just for a bare string or whether it can also apply to a string provided
</bodyText>
<page confidence="0.986792">
288
</page>
<note confidence="0.554061">
Letters to the Editor
</note>
<bodyText confidence="0.999928925925926">
with some kind of structure. As it is, some of the example sentences in LCG are bare
strings (e.g., (5), p. 31), some contain bits of structure (such as the lers in (1) above),
and some have (unlabeled) brackets marking some constituent boundaries (e.g., (22),
p. 54). I see no reason to call these three different kinds of objects &apos;sentences&apos; and yet
to refuse this term to a more complete (labeled) bracketing, which is equivalent to a
tree structure, like those in Figures 4.2 and 4.4 in LCG. To me the term &apos;sentence&apos; is
systematically ambiguous on this point.
The real issue, of course, is not whether Dr. Ristad&apos;s arguments operate in terms
of bare strings, partial structures like (1), or complete ones, but rather that he did
not specify the set of linguistic objects (whatever we call them) that is involved in
his argument. Of course, if he had, I would not have complained about his failure to
define the set of &amp;quot;sentences&amp;quot; involved. For, given a set of trees, one can deduce the
relevant set of &amp;quot;sentences&amp;quot; (if, that is, we are going to insist that only a structure like
(1) or one with even less structure is to be called a &amp;quot;sentence&amp;quot;), by simply pruning as
much of the structural information as is desired.
In any event, the polemic about the term &apos;sentence&apos; (which LCG also uses, as
noted) and the rhetoric about &amp;quot;internal computations performed by language users,
at the highest level of computational abstraction&amp;quot; on &amp;quot;mental representations&amp;quot; (and
not on &amp;quot;sentences,&amp;quot; it is claimed) does not in the least &amp;quot;help explain why the re-
viewer is unable to find even a &apos;single complete example&apos; of an input sentence for the
LCG complexity analysis of the Referential Dependencies problem (p. 127) or why &apos;the
translation from Boolean formulas to English sentences is not fully specified&apos; (p. 128).&amp;quot;
The reason I could not find such an example or specification is that LCG never gave
a complete example of a relevant &amp;quot;linguistic representation&amp;quot; (since (1), even with its
complete tree structure, is just a part of what is required), not to mention a definition
of the relevant set of such &amp;quot;representations,&amp;quot; and as a result did not, and could not,
fully specify the reduction.
</bodyText>
<listItem confidence="0.79072825">
2. Dr. Ristad&apos;s second complaint has to do with my discussion of Berwick&apos;s use of (2)
as an example of the complexity that leads to the LCG NP-hardness result:
(2) Before Bill, Tom, and Jack were friends, he wanted him to introduce him
to him.
</listItem>
<bodyText confidence="0.9998165">
As noted in the review, (2) is not an example from LCG but from Ristad&apos;s dissertation
(1990, p. 69). What appears in LCG is a very similar but differently worded example
(p. 54, example (22)), and the reasons for these examples being irrelevant are given by
Ristad in both places (1990, p. 69; LCG, p. 55). I even quoted the statement from Ristad
(1990: 69), to wit, &amp;quot;The configurations used to construct the sentence [(2)] can only give
rise to very simple obviation graphs on their own, and therefore the proof of [NP-
hardness] must build obviation graphs using the agreement condition.&amp;quot; As Dr. Ristad
notes, I then went on to observe that the &amp;quot;agreement condition&amp;quot; in question—involving
(as do all agreement phenomena) a variety of contrasting inflectional features—does
not figure in this example, where all the pronouns are masculine singular. This, and
what I said about the number of features involved in the argument, was correct.
Dr. Ristad&apos;s equally correct observation that (2) is an example of local c-command,
which together with other phenomena is supposed to lead to NP-hardness, doesn&apos;t
change anything. The point was, and remains, that Berwick would have us believe
that Ristad&apos;s NP-hardness result explains why &amp;quot;the links between the different hims&amp;quot;
in (2) are difficult to &amp;quot;figure out,&amp;quot; and that this is untrue.
</bodyText>
<page confidence="0.986664">
289
</page>
<note confidence="0.616455">
Computational Linguistics Volume 22, Number 2
</note>
<bodyText confidence="0.999870555555555">
As noted in LCG (p. 55), &amp;quot;[t]he local c-command configurations used in (22) only
give rise to simple obviation graphs that are easily colored.&amp;quot; Since example (22) in LCG
is isomorphic to (2), and since &amp;quot;easily colored&amp;quot; means &amp;quot;in P,&amp;quot; Berwick&apos;s confidence
is misplaced. If such examples are hard for people to interpret, this cannot have any
connection with Ristad&apos;s result. Not even close. The sentences that illustrate the NP-
hardness are vastly more complicated than this—indeed so complicated that Ristad
(1993), as noted, never constructed a complete example. Sentence (1), which is already
so complicated as to be quite unintelligible, is just a part of a considerably longer
sentence that would have to be constructed as the shortest example of the kind of
complexity we are talking about.
The broader context here is that LCG (p. 31-48) refutes Ristad&apos;s 1990 attempt to
show the NP-hardness of English on the basis of agreement phenomena (which is now
presented as a straw man) and chooses to rest it on the interaction of c-command, con-
trol, strong crossover, and invisible obviation. Berwick&apos;s claim cannot be salvaged by
changing the reference from the 1990 argument (where these examples are irrelevant
because the pronouns are all the same) to the 1993 one (where they are again irrele-
vant because only local c-command is involved). Moreover, the confusion suggested
by Berwick&apos;s use of an example from the 1990 work is very real, and Berwick muddies
the waters still further by talking about &amp;quot;agreement and syntactic category ambiguity.&amp;quot;
This refers to yet another putative NP-hardness argument, entirely different from either
that in Ristad (1990) or that in LCG, and instead to be found in Chapter 3 (&amp;quot;Agreement
and Ambiguity&amp;quot;) of Barton, Berwick, and Ristad (1987). A potential reader (or pur-
chaser) of LCG has the right to be warned that there are three different NP-hardness
arguments floating around, and that Berwick&apos;s high-power endorsement of LCG&apos;s ar-
gument seems to confuse it with two earlier ones, the first of which (1987) was never
asserted to hold for any actual NL (Barton, Berwick, and Ristad 1987, p. 96) and the
second of which (1990) has now been given up.
</bodyText>
<listItem confidence="0.676089">
3. Next, Dr. Ristad returns to my criticism of his incompletely specified reduction
arguments:
</listItem>
<bodyText confidence="0.997668588235294">
Turning to the middle of page 64 in LCG, we see &amp;quot;LEMMA 3 3SAT &lt;p
Referential Dependencies.&amp;quot; As is standard in computer science—and
explained on page 135 in the appendix on mathematical background—
the symbol &lt;p denotes the relation of polynomial time reducibility
between computational problems. So Lemma 3 states that the 3SAT
problem reduces to the Referential Dependencies problem in polyno-
mial time. A quick check of the index reveals that 3SAT is defined on
page 137 and Referential Dependencies is defined on page 49.
Much of this is correct, but there is no precise definition of Referential Depen-
dencies on page 49 or anywhere else in LCG. &amp;quot;Referential Dependencies&amp;quot; are certain
dependencies that exist (or are claimed to exist) in English, but LCG never specifies
fully just what the claims about English are. The &amp;quot;definition&amp;quot; on pages 49-50 reads as
follows:
The input consists of a linguistic representation R lacking relations of
referential dependency but containing the 3-tuple (AA, AB, AC) of dis-
joint sets of arguments, where AA consists of reflexives and reciprocals,
AB consists of pronouns, and Ac consists of referring-expressions....
</bodyText>
<page confidence="0.991066">
290
</page>
<note confidence="0.785381">
Letters to the Editor
</note>
<bodyText confidence="0.995783195121951">
The output is a correct referential dependency graph G = (A, 0, L)
that is compatible with R.
We know from earlier on page 49 that 0 is the obviate relation and L is the link
relation, but we lack a precise definition of the conditions under which these relations
are supposed to hold in a well-defined set of sentences (or tree structures of sentences)
in some variety of English. It is only by extrapolating from examples and by reading the
relevant (informal) linguistic literature that one can gauge roughly what the extensions
of these relations are, and hence which referential dependency graphs are claimed to be
&amp;quot;correct&amp;quot; in English and which not. Perhaps all that LCG needs here is an empathetic
exegete, but it certainly needs something.
Dr. Ristad concludes his discussion of this point by saying:
The technical content of LCG is to define language computations in-
volving anaphora, in the form of problem statements, and to analyze
their computational complexity. The review misstates the computa-
tional problems involved and does not accurately report the lemmas
or their proofs. So I am concerned that the review may not have ac-
curately represented LCG to the journal&apos;s readership.
Since he gives no examples of such supposed inaccuracies, I do not know how to
respond to this beyond a flat, curt denial.
4. Next, Dr. Ristad raises the issue of how I handled his mentalist view of language.
His first complaint is:
... the review questions the distinction between a computational the-
ory and a generative theory. In particular, the review states that &amp;quot;it
is by no means clear ... that this [computational approach] is ...
any different from, for example, Chomsky&apos;s [generative] approach&amp;quot;
(p. 130).
This, however, is not what what I wrote, which was:
Now, it is by no means clear to me or to Chomsky (personal commu-
nication) that this is original, that is, any different from, for example,
Chomsky&apos;s approach.
The bracketed words &amp;quot;computational approach&amp;quot; and &amp;quot;generative,&amp;quot; which Dr. Ristad
sneaks into my quotation, misrepresent utterly what I said. There are three, and not
two, approaches at issue. What Dr. Ristad calls &amp;quot;generative&amp;quot; is what Chomsky calls the
&amp;quot;E-language&amp;quot; approach (found in some of Chomsky&apos;s writings between roughly 1956
and 1965). What he calls &amp;quot;computational&amp;quot; is basically the same as Chomsky&apos;s (e.g.,
1986) own I-language approach. What I questioned was not the difference between
the E-language (&amp;quot;generative&amp;quot;) and the I-language (&amp;quot;computational&amp;quot;) approaches, but
rather between Ristad&apos;s formulation of the I-language approach and Chomsky&apos;s, as I
made clear by saying, among other things:
Moreover, Ristad gives no basis for preferring this view of language to
what he takes to be Chomsky&apos;s version of the I-language perspective.
</bodyText>
<page confidence="0.991546">
291
</page>
<note confidence="0.715415">
Computational Linguistics Volume 22, Number 2
</note>
<bodyText confidence="0.999305857142858">
Thus, Dr. Ristad&apos;s complaint is groundless and misleading: Chomsky has not fol-
lowed the &amp;quot;generative approach&amp;quot; for decades, and I, having worked extensively on
characterizing Chomsky&apos;s notion of I-language computationally and mathematically
(Manaster Ramer 1993, to appear), would not very likely have said that he has.
Dr. Ristad&apos;s second complaint is that he did advance the kinds of reasons I claimed
he did not. Yet, even if there are differences between Chomsky&apos;s and Ristad&apos;s versions
of the I-language perspective, it is clear from Dr. Ristad&apos;s letter that his arguments,
rehashed here, refer not to these differences but to those between I-language and E-
language. Moreover, if Ristad&apos;s view of language is roughly the same as Chomsky&apos;s,
then there cannot logically be very many arguments for one over the other.
The only difference I actually found is that Chomsky continues to uphold the
competence/performance distinction, whereas Ristad claims not to, although he gives
no argument at all on this score, preferring to merely anathemize this distinction.
Moreover, the LCG results actually presuppose the competence/performance distinc-
tion, since, if there were no such distinction, then sentences like (1) could not be both
grammatical and unacceptable. Their status in the &amp;quot;language itself, exactly as it is and
how it operates&amp;quot; (LCG&apos;s description of what there is in place of competence and per-
formance, p. 117) would then have to be either unequivocally good (which surely no
speaker would assent to) or unequivocally bad (in which case they are not English,
and the NP-hardness argument collapses). Finally, LCG, (p. 13, 117-120) appears to
concede that its claims carry over to the E-language approach (as we would expect,
since complexity work is usually done in those terms anyway).
5. Next, Dr. Ristad objects to my having said that &amp;quot;as in far too many other books and
articles in our field • Ristad consistently and mistakenly uses the words prove and
proof when speaking not only about the results concerning well-defined mathematical
objects ... but also about the claims that (some) NLs are properly modeled by these
mathematical objects and hence share their properties.&amp;quot; Dr. Ristad does not appear to
question this distinction, which I consider so important because the status of a theorem
about a formal language has an entirely different status from an argument about an NL.
Results about formal languages, if done correctly, are presumably irrefutable. Claims
about NLs can, and often are, refuted when new discoveries occur (e.g., when we find
out that some sentence we thought was ungrammatical is fine, or that it can have a
meaning we thought impossible). To be sure, Dr. Ristad says that he never used the
words proof and prove in the way that I am objecting to. However, in LCG prove is used
this way (e.g., on page 48, line 4, and on page 71, line 1, and proof on page 66, line 12),
referring to arguments (as I would call them) that the &amp;quot;Anaphora Problem&amp;quot; and/or
the &amp;quot;Anaphoric Preference Problem&amp;quot; are NP-hard (or NP-complete). Now, these two
problems are the problems of finding, given a sentence with a number of anaphors
in it, those interpretations (specifically, relations of referential dependency) that it has
(or preferentially has), as a matter of linguistic fact, in English. The NP-hardness or
otherwise of these problems thus depends crucially on empirical claims about which
NPs can and which cannot antecede (or preferentially antecede) which anaphors in
certain English sentences (like (1) but more complex), although, as noted above, Ristad
never presents all this with sufficient rigor. But, whatever the claims are, they are up
to a linguist or an informant, not a mathematician, to evaluate. Which is precisely
what makes the use of the terms proof and prove inappropriate.
Of course, there must be a mathematical component to the arguments, namely, a
proof that the formal model (had this been fully specified) of the LCG theory of one
or the other of these problems is NP-hard (or NP-complete). But this does not alter
</bodyText>
<page confidence="0.98508">
292
</page>
<note confidence="0.831948">
Letters to the Editor
</note>
<bodyText confidence="0.970149957446809">
the fact that we may not say that anyone has proved, or given a proof, that any NL, or
a linguistically significant fragment or problem, is NP-hard. Such a result can never
be proved because it is always subject to empirical challenge. The complex story of
the attempts to show the NP-hardness of NLs shows how true and relevant this is. As
noted above, the 1987 argument appears to have been still-born, and the 1990 argument
was refuted in LCG, which now offers us a third argument. How can we accept the
latest result as a &amp;quot;proof,&amp;quot; knowing that the earlier ones failed, not because of any flaw
in the mathematics, but on empirical grounds? What happened each time is that it
turned out that some empirical claim or assumption about the relevant linguistic facts
did not hold. This can happen again, and indeed in my review I provided a detailed
discussion of the reasons why LCG&apos;s claims about English are to be rejected, vitiating
the latest argument as well.
I might add that it is particularly objectionable when we are told of a &amp;quot;proof&amp;quot;
of NP-completeness, as opposed to just NP-hardness (since the existence of a non-
deterministic polynomial time algorithm solving one problem in English can in no
imaginable way tell us that all linguistically significant problems in English have such
algorithms) or of a &amp;quot;proof&amp;quot; that NL tout court, rather than some specific NL like En-
glish, is NP-hard or NP-complete (since, even if English were, it would not follow
that other languages are). There are, one finds, more pernicious and less pernicious
misuses of the words proof and prove, although they are all pernicious.
6. Finally, Dr. Ristad contends that I misrepresented his 1990 dissertation by describing
the (now-refuted) anaphora/agreement argument as the &amp;quot;centerpiece&amp;quot; of that work.
He points out that he also dealt with various other problems in his dissertation, namely,
&amp;quot;complexity analyses of segmental phonology, autosegmental phonology, morpho-syn-
tax, [and] ellipsis,&amp;quot; in addition to anaphora and agreement, and that &amp;quot;[t]he entire
discussion of anaphoric agreement occupies only one page out of more than 140 pages
in my dissertation.&amp;quot;
While much of this is true, some of the other complexity analyses appear in an ap-
pendix, and the section of the dissertation dealing with the argument at issue (pp. 68-
70) is a bit longer than a page. More importantly, dissertations usually save the best
for last, and the problem of anaphoric agreement is indeed the last chapter before the
references and the appendices start (Chapter 4, entitled &amp;quot;Complexity of Pronoun An-
tecedence&amp;quot;). Even more importantly, the very first sentence of the dissertation reads:
&amp;quot;The central thesis of this dissertation is that human language is NP-complete&amp;quot; (Ris-
tad 1990, p. i), which can only refer to this chapter. There is a subtle but legitimate
issue Dr. Ristad could have raised here, but does not. He referred to the whole NP-
completeness result as the &amp;quot;central thesis,&amp;quot; whereas I described just the lower-bound
(NP-hardness) part of it as the &amp;quot;centerpiece.&amp;quot; This is not the issue he raises, but it may
be worth noting that the discussion of the upper-bound part of the argument still only
occupies a small fraction of the total page count (pp. 70-87). Moreover, given the fact,
alluded to above, that an upper-bound result of this nature is, by the nature of things,
much more fragile than a lower-bound one, I think I was right to focus on the latter
result. Had it stood the test of time, instead of getting refuted in LCG, this would
have been a revolutionary achievement—and one as robust as is possible given the
empirical nature of the underlying issues.
7. My evaluation of LCG stands. Berwick compares its claims to Chomsky&apos;s (1956)
classic center-embedding argument against finite-state models for NLs. What I said in
</bodyText>
<page confidence="0.993749">
293
</page>
<note confidence="0.69045">
Computational Linguistics Volume 22, Number 2
</note>
<bodyText confidence="0.996790571428571">
my review, and what I still believe, can perhaps best be summarized by saying that
Ristad&apos;s work is like what Chomsky&apos;s would have been (a) had the crucial grammat-
icality judgments Chomsky invoked been incorrect, (b) had Chomsky failed to make
it clear just what fragment of English he was talking about, (c) had Chomsky denied
the performance/competence distinction, and thus been unable to treat examples of
repeated center-embedding as sentences of English, and worst of all, (d) had Chomsky
then claimed to have &amp;quot;proved&amp;quot; that NLs are context-free on the grounds that he could
construct a context-free grammar handling those center-embeddings (a hypothetical
upper-bound result quite analogous to Ristad&apos;s).
Of course, it is entirely possible that English really is NP-hard and perhaps NP-
complete as well, but it is important to realize that these are not the kinds of things
we can ever know with mathematical certitude. And, that as of now, we still lack any
solid grounds for accepting either claim even as a working hypothesis or an empirical
theory.
</bodyText>
<reference confidence="0.906359974358974">
Alexis Manaster Ramer
Department of Computer Science
Wayne State University
Detroit, MI 48202
amr@cs.wayne.edu
References
Barton, G. Edward, Robert C. Berwick, and
Eric Sven Ristad (1987). Computational
Complexity and Natural Language.
Cambridge, MA: The MIT Press.
Chomsky, Noam (1956). &amp;quot;Three models for
the description of language.&amp;quot; IRE
Transactions on Information Theory IT-2,
113-124. Reprinted (1965) in: R. D. Luce,
R. R. Bush, and E. Galanter (editors),
Readings in Mathematical Psychology, v. 2,
105-124, New York: John Wiley.
Chomsky, Noam (1986). Knowledge of
Language: Its Nature, Origins, and Use. New
York: Praeger.
Manaster Ramer, Alexis (1993). &amp;quot;Towards
transductive linguistics.&amp;quot; In: Karen
Jensen, George E. Heidorn, and Stephen D.
Richardson (editors), Natural Language
Processing: The PLNLP Approach, 13-27.
Boston: Kluwer Academic Publishers.
Manaster Ramer, Alexis (To appear). The
Uses and Abuses of Mathematics in
Linguistics. Tarragona: Universitat Rovira i
Virgili.
Ristad, Eric Sven (1990). Computational
Structure of Human Language. Ph.D.
dissertation, Department of Electrical
Engineering and Computer Science,
Massachusetts Institute of Technology.
Ristad, Eric Sven (1993). The Language
Complexity Game. Cambridge,
Massachusetts, and London, England: The
MIT Press.
</reference>
<page confidence="0.99839">
294
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.188652">
<title confidence="0.812040666666667">Letters to the Editor From Eric Ristad: The purpose of this letter is to advise the journal&apos;s readership of some misstatements</title>
<author confidence="0.908353">Alexis Manaster Ramer &apos;s review of my book Language Complexity Game</author>
<abstract confidence="0.995365450980392">MIT Press, 1993] in Linguistics March 1995, 124-131 (&amp;quot;the review,&amp;quot; henceforth). a theory, at the highest level of computational abstraction, of the computations performed by language users. The inputs to the computations are mental representations, as are the outputs. Thus, the review is mistaken to claim that &amp;quot;the problem studied [in LCG] is ... that of determining what meanings are possible for a given set of sentences&amp;quot; (p. 124). No computational problem studin sentences as its inputs or as its outputs. The input to the Anaphoric Agreement problem considered in Chapter 3 consists of sets of anaphoric elements and available antecedents. The output is a valid relation of immediate antecedence (pp. 33-34). No sentences are involved. The input to the Referential Dependencies problem considered in Chapter 4 is a linguistic representation lacking only relations of obviation and antecedence. The output is the missing relations of obviation and antecedence (pp. 49-50). Again, no sentences are involved. The same holds for the Anaphoric Copying and Anaphoric Sharing problems considered in Chapter 5: no sentences. This may help explain why the reviewer is unable to find even a &amp;quot;single example&amp;quot; of an input sentence for the analysis of the Referential Dependencies problem (p. 127) or why &amp;quot;the translation from Boolean formulas to English sentences is not fully specified&amp;quot; (p. 128). review&apos;s presentation of the analysis is not wholly accurate either. For example, on page 125, the review states that examples such as (1) are &amp;quot;completely irrelevant to the NP-hardness argument&amp;quot; and that &amp;quot;to get NP-hardness one would need to consider examples in which pronouns with different features are available.&amp;quot; Moreover, the review claims that &amp;quot;an infinite set of inflectional features&amp;quot; are required &amp;quot;to get NP-hardness.&amp;quot; (1) Before Bill, Tom, and Jack were friends, he wanted him to introduce him to him. explained in section 4.1.1 of (1) exemplifies a local c-command configuration. Configurations of local c-command are essential in the reduction from 3SAT to the Referential Dependencies problem (lemma 3, p. 64). They are used in both the variable gadget (Figure 4.3) and the clause gadget (Figure 4.4). And contrary to the review&apos;s second claim, absolutely no inflectional features are used in this reduction. In the foreword to LCG, Robert Berwick correctly notes that examples such as (1) play an important role in establishing the NP-hardness of the Anaphora Problem (p. xiii). So the review&apos;s claim that &amp;quot;Berwick misinterprets what the result is all about&amp;quot; (p. 125) is also unfounded. Nor does the review accurately convey the statement of the relevant lemma. The review claims &amp;quot;that [this] proof does not state what the reduction from 3-SAT is to, which is a serious matter for a reduction proof&amp;quot; (p. 127). Turning to the middle of page in see &amp;quot;LEMMA 3 3SAT Dependencies.&amp;quot; As is standard in computer science—and explained on page 135 in the appendix on mathematical symbol denotes the relation of polynomial time reducibility be- Letters to the Editor tween computational problems. So Lemma 3 states that the 3SAT problem reduces to the Referential Dependencies problem in polynomial time. A quick check of the index reveals that 3SAT is defined on page 137 and Referential Dependencies is defined on page 49. technical content of to define language computations involving anaphora, in the form of problem statements, and to analyze their computational complexity The review misstates the computational problems involved and does not accurately report the lemmas or their proofs. So I am concerned that the review may not have represented the journal&apos;s readership. After raising these (and other) technical issues, the review questions the distinction between a computational theory and a generative theory. In particular, the review states that &amp;quot;it is by no means clear . .. that this [computational approach] is .. . any different from, for example, Chomsky&apos;s [generative] approach&amp;quot; (p. 130). As summarized page 113 and pages 115-6 of generative theory of language is an explicit procedure for enumerating the set of linguistic representations. The computations specified by a generative theory enumerate linguistic representations; these computations are called derivations. Derivations are not directly attributed to language users, contra the Derivational Theory of Complexity. In contrast, a computational theory of language is a theory of the mental computations performed in the comprehension, production, and acquisition of human languages. Thus, the computations specified by a computational theory are indeed attributed to language users. For this reason, a computational such as that of a more internalized view of language than is offered by a generative theory. review goes on to claim that not attempt .. . to show that there are factual reasons for preferring this [computational] view of language&amp;quot; to the generative (p. 130). In Section 6.3, that, unlike the generative approach, the computational approach more naturally results in language models that are accurate in computationally significant respects. Thus, the complexity analysis of anaphoric agreement in Chapter 3 leads to an empirical critique of the standard &amp;quot;nondistinctness&amp;quot; theory of agreement as well as a simpler theory of agreement. The complexity analysis of referential dependencies in Chapter 4 leads to a better understanding of obviation in elliptical contexts. The complexity analysis of ellipsis in Chapter 5 raises some empirical problems for the popular &amp;quot;copy&amp;quot; theory of ellipsis, and proposes a simpler alternative based on the sharing of thematic functions. And so the review&apos;s claim that no basis for preferring this [computational] view of language to what takes to be Chomsky&apos;s version of the I-language perspective&amp;quot; (p. 130) is misleading. In addition to these technical and conceptual issues, Manaster Ramer &apos;s review also raises some questions of style and proportion. The review opens with the statement that &amp;quot;Ristad consistently and mistakenly the words speaking not only about the results concerning well-defined mathematical objects .. . but also about the claims that (some) NLs are properly modeled by these mathematical objects .. . &amp;quot; (p. 124). By my count, the words a total of 66 times in instance is nontechnical: &amp;quot;prove useful&amp;quot; (p. 11). The remaining 65 instances refer to mathematical proofs, typically to proofs of the computational complexity of precisely stated computational problems based on particular formal models of language. I am unable to find a single case either used in an empirical argument about human languages. So the review is half right here: my usage is consistent, but not in the manner that the review claims. Shortly thereafter, the review states that &amp;quot;the whole argument about the NPhardness of the anaphora problem in agreement situations . .. was the centerpiece 287 Computational Linguistics Volume 22, Number 2 of [Ristad&apos;s] dissertation&amp;quot; (p. 125). My doctoral dissertation includes complexity analyses of segmental phonology, autosegmental phonology, morpho-syntax, ellipsis, and anaphoric relations. The entire discussion of anaphoric agreement occupies only one page out of more than 140 pages in the dissertation, less than one section out of the more than 40 sections. So it does not seem entirely accurate to call the discussion of anaphoric agreement the &amp;quot;centerpiece&amp;quot; of the dissertation.</abstract>
<author confidence="0.997545">Eric Ristad</author>
<affiliation confidence="0.999933">Department of Computer Science Princeton University</affiliation>
<address confidence="0.999931">Princeton, NJ 08544</address>
<email confidence="0.999437">ristad@cs.princeton.edu</email>
<author confidence="0.769129">From Alexis Manaster Ramer</author>
<affiliation confidence="0.615969">Dr. Ristad claims that my review misrepresented both him and Robert C. Berwick,</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<pages>48202</pages>
<institution>Alexis Manaster Ramer Department of Computer Science Wayne State University</institution>
<location>Detroit, MI</location>
<marker></marker>
<rawString>Alexis Manaster Ramer Department of Computer Science Wayne State University Detroit, MI 48202</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Edward Barton</author>
<author>Robert C Berwick</author>
</authors>
<title>and Eric Sven Ristad</title>
<date>1987</date>
<booktitle>Computational Complexity and Natural Language.</booktitle>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Barton, Berwick, 1987</marker>
<rawString>Barton, G. Edward, Robert C. Berwick, and Eric Sven Ristad (1987). Computational Complexity and Natural Language. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Three models for the description of language.&amp;quot;</title>
<date>1956</date>
<booktitle>IRE Transactions on Information Theory IT-2,</booktitle>
<volume>2</volume>
<pages>113--124</pages>
<editor>in: R. D. Luce, R. R. Bush, and E. Galanter (editors),</editor>
<publisher>John Wiley.</publisher>
<location>New York:</location>
<note>Reprinted</note>
<marker>Chomsky, 1956</marker>
<rawString>Chomsky, Noam (1956). &amp;quot;Three models for the description of language.&amp;quot; IRE Transactions on Information Theory IT-2, 113-124. Reprinted (1965) in: R. D. Luce, R. R. Bush, and E. Galanter (editors), Readings in Mathematical Psychology, v. 2, 105-124, New York: John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Knowledge of Language: Its Nature, Origins, and Use.</title>
<date>1986</date>
<publisher>Praeger.</publisher>
<location>New York:</location>
<marker>Chomsky, 1986</marker>
<rawString>Chomsky, Noam (1986). Knowledge of Language: Its Nature, Origins, and Use. New York: Praeger.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaster Ramer</author>
</authors>
<title>Alexis</title>
<date>1993</date>
<publisher>Karen</publisher>
<contexts>
<context position="20715" citStr="Ramer 1993" startWordPosition="3342" endWordPosition="3343">(&amp;quot;computational&amp;quot;) approaches, but rather between Ristad&apos;s formulation of the I-language approach and Chomsky&apos;s, as I made clear by saying, among other things: Moreover, Ristad gives no basis for preferring this view of language to what he takes to be Chomsky&apos;s version of the I-language perspective. 291 Computational Linguistics Volume 22, Number 2 Thus, Dr. Ristad&apos;s complaint is groundless and misleading: Chomsky has not followed the &amp;quot;generative approach&amp;quot; for decades, and I, having worked extensively on characterizing Chomsky&apos;s notion of I-language computationally and mathematically (Manaster Ramer 1993, to appear), would not very likely have said that he has. Dr. Ristad&apos;s second complaint is that he did advance the kinds of reasons I claimed he did not. Yet, even if there are differences between Chomsky&apos;s and Ristad&apos;s versions of the I-language perspective, it is clear from Dr. Ristad&apos;s letter that his arguments, rehashed here, refer not to these differences but to those between I-language and Elanguage. Moreover, if Ristad&apos;s view of language is roughly the same as Chomsky&apos;s, then there cannot logically be very many arguments for one over the other. The only difference I actually found is t</context>
</contexts>
<marker>Ramer, 1993</marker>
<rawString>Manaster Ramer, Alexis (1993). &amp;quot;Towards transductive linguistics.&amp;quot; In: Karen</rawString>
</citation>
<citation valid="false">
<authors>
<author>George E Heidorn Jensen</author>
<author>D Stephen</author>
</authors>
<booktitle>Natural Language Processing: The PLNLP Approach,</booktitle>
<pages>13--27</pages>
<editor>Richardson (editors),</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Boston:</location>
<marker>Jensen, Stephen, </marker>
<rawString>Jensen, George E. Heidorn, and Stephen D. Richardson (editors), Natural Language Processing: The PLNLP Approach, 13-27. Boston: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Manaster Ramer</author>
</authors>
<title>Alexis (To appear). The Uses and Abuses of Mathematics in Linguistics. Tarragona: Universitat Rovira i Virgili.</title>
<marker>Ramer, </marker>
<rawString>Manaster Ramer, Alexis (To appear). The Uses and Abuses of Mathematics in Linguistics. Tarragona: Universitat Rovira i Virgili.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
</authors>
<title>Computational Structure of Human Language.</title>
<date>1990</date>
<tech>Ph.D. dissertation,</tech>
<institution>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology.</institution>
<contexts>
<context position="13149" citStr="Ristad (1990" startWordPosition="2124" endWordPosition="2125"> the reduction. 2. Dr. Ristad&apos;s second complaint has to do with my discussion of Berwick&apos;s use of (2) as an example of the complexity that leads to the LCG NP-hardness result: (2) Before Bill, Tom, and Jack were friends, he wanted him to introduce him to him. As noted in the review, (2) is not an example from LCG but from Ristad&apos;s dissertation (1990, p. 69). What appears in LCG is a very similar but differently worded example (p. 54, example (22)), and the reasons for these examples being irrelevant are given by Ristad in both places (1990, p. 69; LCG, p. 55). I even quoted the statement from Ristad (1990: 69), to wit, &amp;quot;The configurations used to construct the sentence [(2)] can only give rise to very simple obviation graphs on their own, and therefore the proof of [NPhardness] must build obviation graphs using the agreement condition.&amp;quot; As Dr. Ristad notes, I then went on to observe that the &amp;quot;agreement condition&amp;quot; in question—involving (as do all agreement phenomena) a variety of contrasting inflectional features—does not figure in this example, where all the pronouns are masculine singular. This, and what I said about the number of features involved in the argument, was correct. Dr. Ristad&apos;s e</context>
<context position="15836" citStr="Ristad (1990)" startWordPosition="2554" endWordPosition="2555">, strong crossover, and invisible obviation. Berwick&apos;s claim cannot be salvaged by changing the reference from the 1990 argument (where these examples are irrelevant because the pronouns are all the same) to the 1993 one (where they are again irrelevant because only local c-command is involved). Moreover, the confusion suggested by Berwick&apos;s use of an example from the 1990 work is very real, and Berwick muddies the waters still further by talking about &amp;quot;agreement and syntactic category ambiguity.&amp;quot; This refers to yet another putative NP-hardness argument, entirely different from either that in Ristad (1990) or that in LCG, and instead to be found in Chapter 3 (&amp;quot;Agreement and Ambiguity&amp;quot;) of Barton, Berwick, and Ristad (1987). A potential reader (or purchaser) of LCG has the right to be warned that there are three different NP-hardness arguments floating around, and that Berwick&apos;s high-power endorsement of LCG&apos;s argument seems to confuse it with two earlier ones, the first of which (1987) was never asserted to hold for any actual NL (Barton, Berwick, and Ristad 1987, p. 96) and the second of which (1990) has now been given up. 3. Next, Dr. Ristad returns to my criticism of his incompletely specifi</context>
<context position="27308" citStr="Ristad 1990" startWordPosition="4429" endWordPosition="4431">tion.&amp;quot; While much of this is true, some of the other complexity analyses appear in an appendix, and the section of the dissertation dealing with the argument at issue (pp. 68- 70) is a bit longer than a page. More importantly, dissertations usually save the best for last, and the problem of anaphoric agreement is indeed the last chapter before the references and the appendices start (Chapter 4, entitled &amp;quot;Complexity of Pronoun Antecedence&amp;quot;). Even more importantly, the very first sentence of the dissertation reads: &amp;quot;The central thesis of this dissertation is that human language is NP-complete&amp;quot; (Ristad 1990, p. i), which can only refer to this chapter. There is a subtle but legitimate issue Dr. Ristad could have raised here, but does not. He referred to the whole NPcompleteness result as the &amp;quot;central thesis,&amp;quot; whereas I described just the lower-bound (NP-hardness) part of it as the &amp;quot;centerpiece.&amp;quot; This is not the issue he raises, but it may be worth noting that the discussion of the upper-bound part of the argument still only occupies a small fraction of the total page count (pp. 70-87). Moreover, given the fact, alluded to above, that an upper-bound result of this nature is, by the nature of thin</context>
</contexts>
<marker>Ristad, 1990</marker>
<rawString>Ristad, Eric Sven (1990). Computational Structure of Human Language. Ph.D. dissertation, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ristad</author>
</authors>
<title>Eric Sven</title>
<date>1993</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, Massachusetts, and London, England:</location>
<contexts>
<context position="8301" citStr="Ristad 1993" startWordPosition="1304" endWordPosition="1305"> ellipsis, and anaphoric relations. The entire discussion of anaphoric agreement occupies only one page out of more than 140 pages in the dissertation, less than one section out of the more than 40 sections. So it does not seem entirely accurate to call the discussion of anaphoric agreement the &amp;quot;centerpiece&amp;quot; of the dissertation. Eric Ristad Department of Computer Science Princeton University Princeton, NJ 08544 ristad@cs.princeton.edu From Alexis Manaster Ramer: Dr. Ristad claims that my review misrepresented both him and Robert C. Berwick, the author of the effusive introduction to his book (Ristad 1993; henceforth LCG). In my review, I praised LCG&apos;s insights into certain linguistic problems with agreement, anaphora, and ellipsis, but I criticized some of its claims about English, its treatment of mentalism, and its argumentation about the complexity-theoretic properties of natural languages, specifically, the claims that English or even NLs in general are NP-complete (that is, NP-hard and included in the class .A1-2). Dr. Ristad does not take issue with the most important of my critical comments, dealing with the fundamentally empirical nature of his claims and the extreme fragility of his </context>
<context position="9624" citStr="Ristad (1993)" startWordPosition="1507" endWordPosition="1508"> those points too. 1. The first point of contention concerns the term &apos;sentence&apos;. Dr. Ristad&apos;s complaint is that I should never have mentioned &amp;quot;sentences,&amp;quot; since, for example, &amp;quot;[t]he input to the Referential Dependencies problem considered in Chapter 4 is a linguistic representation lacking only relations of obviation and antecedence. The output is the missing relations of obviation and antecedence.., no sentences are involved,&amp;quot; and likewise for the other problems he studied. However, the examples in LCG of English structures illustrating these problems are usually sentences, such as (1), and Ristad (1993) himself refers to them as &amp;quot;sentence&amp;quot;s (e.g., p. 54, lines 13, 15, 16, 18). The &amp;quot;proof&amp;quot; of the NP-hardness of Referential Dependencies (p. 64) involves a reduction that translates a Boolean formula into a &amp;quot;linguistic representation ... shown schematically in figure 4.2&amp;quot;; the figure shows us a highly schematic tree structure of an English sentence, a part of which would be (1). (1) True met him2, whoA hei expected him3 to want him4 framed, whoB he5 believed he6 did [e] with tB, after exposing himself5 to [e] for tA, before telling himselfi to [e]. The only genuine issue I can see here, therefor</context>
<context position="14689" citStr="Ristad (1993)" startWordPosition="2368" endWordPosition="2369">ifficult to &amp;quot;figure out,&amp;quot; and that this is untrue. 289 Computational Linguistics Volume 22, Number 2 As noted in LCG (p. 55), &amp;quot;[t]he local c-command configurations used in (22) only give rise to simple obviation graphs that are easily colored.&amp;quot; Since example (22) in LCG is isomorphic to (2), and since &amp;quot;easily colored&amp;quot; means &amp;quot;in P,&amp;quot; Berwick&apos;s confidence is misplaced. If such examples are hard for people to interpret, this cannot have any connection with Ristad&apos;s result. Not even close. The sentences that illustrate the NPhardness are vastly more complicated than this—indeed so complicated that Ristad (1993), as noted, never constructed a complete example. Sentence (1), which is already so complicated as to be quite unintelligible, is just a part of a considerably longer sentence that would have to be constructed as the shortest example of the kind of complexity we are talking about. The broader context here is that LCG (p. 31-48) refutes Ristad&apos;s 1990 attempt to show the NP-hardness of English on the basis of agreement phenomena (which is now presented as a straw man) and chooses to rest it on the interaction of c-command, control, strong crossover, and invisible obviation. Berwick&apos;s claim canno</context>
</contexts>
<marker>Ristad, 1993</marker>
<rawString>Ristad, Eric Sven (1993). The Language Complexity Game. Cambridge, Massachusetts, and London, England: The MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>