<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004058">
<title confidence="0.960508">
Automatic Chinese Abbreviation Generation Using Conditional Random
Field
</title>
<author confidence="0.990444">
Dong Yang, Yi-cheng Pan, and Sadaoki Furui
</author>
<affiliation confidence="0.9998135">
Department of Computer Science
Tokyo Institute of Technology
</affiliation>
<address confidence="0.596141">
Tokyo 152-8552 Japan
</address>
<email confidence="0.999483">
{raymond,thomas,furui}@furui.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.996671" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998219">
This paper presents a new method for au-
tomatically generating abbreviations for Chi-
nese organization names. Abbreviations are
commonly used in spoken Chinese, especially
for organization names. The generation of
Chinese abbreviation is much more complex
than English abbreviations, most of which are
acronyms and truncations. The abbreviation
generation process is formulated as a character
tagging problem and the conditional random
field (CRF) is used as the tagging model. A
carefully selected group of features is used in
the CRF model. After generating a list of ab-
breviation candidates using the CRF, a length
model is incorporated to re-rank the candi-
dates. Finally the full-name and abbreviation
co-occurrence information from a web search
engine is utilized to further improve the per-
formance. We achieved top-10 coverage of
88.3% by the proposed method.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895076923077">
Long named entities are frequently abbreviated in
oral Chinese language for efficiency and simplic-
ity. Therefore, abbreviation modeling is an impor-
tant building component for many systems that ac-
cept spoken input, such as directory assistance and
voice search systems.
While English abbreviations are usually formed
as acronyms, Chinese abbreviations are much more
complex, as shown in Figure 1. Most of the Chi-
nese abbreviations are formed by selecting several
characters from full-names, which are not necessar-
ily the first character of each word. Usually the orig-
inal character order in the full-name is preserved in
</bodyText>
<figureCaption confidence="0.999075">
Figure 1: Chinese abbreviation examples
</figureCaption>
<bodyText confidence="0.999958923076923">
the abbreviation. However, re-ordering of charac-
ters as shown in the third example in Figure 1 where
characters “_” and “R” are swapped in the abbre-
viation, also happens.
There has been a considerable amount of research
on extracting full-name and abbreviation pairs in
the same document for obtaining abbreviations (Li
and Yarowsky, 2008; Sun et al., 2006; Fu et al.,
2006). However, generation of abbreviations given
a full-name is still a non-trivial problem. Chang
and Lai (Chang and Lai, 2004) have proposed using
a hidden Markov model to generate abbreviations
from full-names. However, their method assumes
that there is no word-to-null mapping, which means
that every word in the full-name has to contribute at
least one character to the abbreviation. This assump-
tion does not hold for organizations’ names which
have many word skips in the abbreviation genera-
tion.
The CRF was first introduced to natural language
processing (NLP) by (Lafferty et al., 2001) and has
been widely used in word segmentation, part-of-
speech (POS) tagging, and some other NLP tasks.
In this paper, we convert the Chinese abbreviation
generation process to a CRF tagging problem. The
key problem here is how to find a group of discrim-
</bodyText>
<equation confidence="0.995003">
࣫Ҁ ໻ᄺ ㄀ϝ ए䰶 ࣫एϝ䰶
⏙ढ ໻ᄺ ⏙ढ
Ё೑ Ё༂ ⬉㾚ৄ ༂㾚
Ͳ
</equation>
<page confidence="0.966638">
273
</page>
<note confidence="0.3542405">
Proceedings of NAACL HLT 2009: Short Papers, pages 273–276,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999542">
inant and robust features. After using the CRF, we
get a list of abbreviation candidates with associate
probability scores. We also use the prior condi-
tional probability of the length of the abbreviations
given the length of the full-names to complement the
CRF probability scores. Such global information is
hard to include in the CRF model. In addition, we
apply the full-name and abbreviation candidate co-
occurrence statistics obtained on the web to increase
the correctness of the abbreviation candidates.
</bodyText>
<sectionHeader confidence="0.935353" genericHeader="method">
2 Chinese Abbreviation Introduction
</sectionHeader>
<bodyText confidence="0.99961665">
Chinese abbreviations are generated by three meth-
ods (Lee, 2005): reduction, elimination, and gener-
alization.
Both in the reduction and elimination methods,
characters are selected from the full-name, and the
order of the characters is sometime changed. Note
that this paper does not cover the case when the or-
der is changed. The elimination means that one or
more words in the full-name are ignored completely,
while the reduction requires that at least one char-
acter is selected from each word. All the three ex-
amples in Figure 1 are produced by the elimination,
where at least one word is skipped.
Generalization, which is used to abbreviate a list
of similar terms, is usually composed of the number
of terms and a shared character across the terms. A
example is “—E ” (three forces) for “ax,$*,X,
��” (land force, sea force, air force). This is the
most difficult scenario for the abbreviations and is
not considered in this paper.
</bodyText>
<sectionHeader confidence="0.996414" genericHeader="method">
3 CRF Model for Abbreviation Modeling
</sectionHeader>
<subsectionHeader confidence="0.980535">
3.1 CRF model
</subsectionHeader>
<bodyText confidence="0.9980935">
A CRF is an undirected graphical model and assigns
the following probability to a label sequence L =
</bodyText>
<equation confidence="0.9874018">
l1l2 ... lT, given an input sequence C = c1c2 ... cT,
T
P(L|C) = Z(C) exp(1:1:λkfk(lt, lt−1, C, t))
t=1 k
(1)
</equation>
<bodyText confidence="0.997212888888889">
Here, fk is the feature function for the k-th fea-
ture, λk is the parameter which controls the weight
of the k-th feature in the model, and Z(C) is the nor-
malization term that makes the summation of the
probability of all label sequences to 1. CRF training
is usually performed through the typical L-BFGS al-
gorithm (Wallach, 2002) and decoding is performed
by Viterbi algorithm (Viterbi, 1967). In this paper,
we use an open source toolkit “crf++”.
</bodyText>
<subsectionHeader confidence="0.7504825">
3.2 Abbreviation modeling as a tagging
problem
</subsectionHeader>
<bodyText confidence="0.999976375">
In order to use the CRF method in abbreviation gen-
eration, the abbreviation generation problem was
converted to a tagging problem. The character is
used as a tagging unit and each character in a full-
name is tagged by a binary variable with the values
of either Y or N: Y stands for a character used in the
abbreviation and N means not. An example is given
in Figure 2.
</bodyText>
<subsectionHeader confidence="0.994957">
3.3 Feature selection for the CRF
</subsectionHeader>
<bodyText confidence="0.999307571428571">
In the CRF method, feature function describes
a co-occurrence relation, and it is defined as
fk(lt, lt−1, C, t) (Eq. 1). fk is usually a binary func-
tion, and takes the value 1 when both observation ct
and transition lt−1 → lt are observed. In our ab-
breviation generation model, we use the following
features:
</bodyText>
<listItem confidence="0.936282681818182">
1. Current character The character itself is the
most important feature for abbreviation as it will be
either retained or discarded. For example, “A” (bu-
reau) and “�” (institue), indicating a government
department, are very common characters used in ab-
breviations. When they appear in full-names, they
are likely to be kept in abbreviations.
2. Current word In the full name of “ 094LJ&amp;
k*” (China Agricultural university), the word “41
1J” (China) is usually ignored in the abbreviation,
but the word “4L3h” (agriculture) is usually kept.
The length (the number of characters) is also an im-
portant feature of the current word.
3. Position of the current character in the cur-
rent word Previous work (Chang and Lai, 2004)
showed that the first character of a word has high
possibility to form part of the abbreviation and this
is also true for the last character of a three-character
word.
4. Combination of feature 2. and 3. above
Combination of the features 2 and 3 is expected to
improve the performance, since the position infor-
</listItem>
<equation confidence="0.931535">
Ё೑ Ё༂ ⬉㾚ৄ ༂㾚
Ё ೑ Ё ༂ ⬉ 㾚 ৄ
</equation>
<figureCaption confidence="0.99722">
Figure 2: Abbreviation in the CRF tagging format
</figureCaption>
<page confidence="0.985288">
274
</page>
<bodyText confidence="0.999981">
mation affects the abbreviation along with the cur-
rent word. For example, ending character in “k*”
(university) and that in “RftR” (research institute)
have very different possibilities to be selected for ab-
breviations.
Besides the features above, we have examined
context information (previous word, previous char-
acter, next character, etc.) and other local features
like the length of the word, but these features did
not improve the performance. The reason may be
due to the sparseness of the training data.
</bodyText>
<sectionHeader confidence="0.8188525" genericHeader="method">
4 Improvement by a Length Model and a
Web Search Engine
</sectionHeader>
<subsectionHeader confidence="0.989996">
4.1 Length model
</subsectionHeader>
<bodyText confidence="0.9989624">
There is a strong correlation between the length of
organizations’ full-names and their abbreviations.
We use the length modeling based on discrete prob-
ability of P(M|L), in which the variables M and
L are lengths of abbreviations and full-names, re-
spectively. Since it is difficult to incorporate length
information into the CRF model explicitly, we use
P(M|L) to rescore the output of the CRF.
In order to use the length information, we model
the abbreviation process with two steps:
</bodyText>
<listItem confidence="0.93166925">
• 1st step: evaluate the length in abbreviation ac-
cording to the length model P(M|L);
• 2nd step: choose the abbreviation, given the
length and full-name.
</listItem>
<bodyText confidence="0.815418">
We assume the following approximation:
</bodyText>
<equation confidence="0.998639">
P(A|F) —_ P(M|L) · P(A|M, F) (2)
</equation>
<bodyText confidence="0.9892025">
in which variable A is the abbreviation and F is the
full-name; P(M|L) is the length model, and the sec-
ond probability can be calculated according to the
Bayesian rule:
</bodyText>
<equation confidence="0.457484">
(3)
</equation>
<bodyText confidence="0.994625666666667">
It is obvious that P(A, M|F) = P(A|F) (as A
contains the information M implicitly) and P(A|F)
can be obtained from the output of the CRF.
</bodyText>
<subsectionHeader confidence="0.991201">
4.2 Web search engine
</subsectionHeader>
<bodyText confidence="0.999983944444444">
Co-occurrence of a full-name and an abbreviation
candidate can be a clue of the correctness of the ab-
breviation. We use the “abbreviation candidate”+
“full-name” as queries and input them to the most
popular Chinese search engine (www.baidu.com),
and then we use the number of hits as the metric
to perform re-ranking. The hits is theoretically re-
lated to the number of pages which contain both the
full-name and abbreviation. The bigger the value of
hits, the higher probability that the abbreviation is
correct.
We then simply multiply the previous probability
score, obtained from Eq. 2, by the number of hits
and re-rank the top-30 candidates accordingly.
There are some other ways to use information re-
trieval methods (Mandala et al., 2000). Our method
has an advantage that the access load to the web
search engine is relatively small.
</bodyText>
<sectionHeader confidence="0.999522" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.994483">
5.1 Data introduction
</subsectionHeader>
<bodyText confidence="0.999990875">
The corpus we use in this paper comes from two
sources: one is the book “modern Chinese abbre-
viation dictionary” (Yuan and Ruan, 2002) and the
other is the wikipedia. Altogether we collected 1945
pairs of organization full-names and their abbrevia-
tions.
The data is randomly divided into two parts, a
training set with 1298 pairs and a test set with 647
pairs. Table 1 shows the length mapping statistics
of the training set. It can be seen that the average
length of full-names is about 7.29. We know that for
a full-name with length N, the number of abbrevia-
tion candidates is about 2N − 2 − N (exclude length
of 0, 1, and N) and we can conclude that the average
number of candidates for organization names in this
corpus is more than 100.
</bodyText>
<subsectionHeader confidence="0.88571">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999990166666667">
The abbreviation method described is part of a
project to develop a voice-based search application.
For our name abbreviation system we plan to add 10
abbreviation candidates for each organization name
into the vocabulary of our voice search application,
hence here we consider top-10 coverage.
</bodyText>
<equation confidence="0.9651382">
P(A, M|F )
P(A|M, F) = P (M|F )
P(A, M|F)
E
length(A′)=Nr P(A′, M|F)
</equation>
<page confidence="0.991396">
275
</page>
<table confidence="0.994250846153846">
length of length of abbreviation sum
full-name
2 3 4 5 &gt;5
4 107 1 0 0 0 108
5 89 140 0 0 0 229
6 96 45 46 0 0 187
7 60 189 49 16 0 314
8 48 29 60 3 6 146
9 10 47 35 12 2 106
10 18 11 29 8 6 73
others 21 43 38 17 14 133
average length of the full-name 7.27
average length of the abbreviation 3.01
</table>
<tableCaption confidence="0.998771">
Table 1: Length statistics on the training set
</tableCaption>
<figureCaption confidence="0.990925545454545">
Figure 3: Contribution of features in CRF
Figure 3 shows the result for various combina-
tions of features introduced in Section 3.3.
Figure 4 displays the coverage results obtained
using the CRF method and the improvements gained
from the inclusion of the length feature and the web
search hits. As we can see the CRF gives a coverage
79.9%. Both length model and web search engine
show significant improvement over the CRF base-
line and the coverage increases to 88.3%.
Figure 4: Results of different methods
</figureCaption>
<sectionHeader confidence="0.986591" genericHeader="conclusions">
6 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.999982133333334">
The CRF works well in generating abbreviations for
organization names, while both length model and
web search engine further improve the performance.
We are going to perform word clustering or char-
acter clustering to alleviate the data sparseness prob-
lem. Also we notice that multiple abbreviations for
single full-name is very common, such as “111411
A*,AQ” (China central television) with abbrevi-
ations “-A4v and “11Aa”. We plan to collect
multiple abbreviations for reference. After that we
are going to combine the abbreviation modeling in
the voice search system to alleviate the weakness of
speech recognition for unknown abbreviation words,
which are unlikely to be correctly recognized due to
the out of vocabulary problem.
</bodyText>
<sectionHeader confidence="0.997689" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998004315789474">
Jing-shin Chang and Yu-Tso Lai 2004. A Preliminary
Study on Probabilistic Models for Chinese Abbrevia-
tions. Proceedings of ACL SIGHAN Workshop 2004,
pages 9-16.
Guohong Fu, Kang-Kwong Luke, GuoDong Zhou and
Ruifeng Xu 2006. Automatic Expansion of Abbre-
viations in Chinese News Text. Lecture Notes in Com-
puter Science, Washington, DC.
John Lafferty, Andrew McCallum, and Fernando Pereira
2001. Conditional Random Fields: Probabilistic
Models for Segmenting and Labeling Sequence Data.,
In Proceedings of International Conference on Ma-
chine Learning 2001, pages 282-289
Hiu Wing Doris Lee 2005. A Study of Automatic Ex-
pansion of Chinese Abbreviations. MA Thesis, The
University of Hong Kong.
Zhifei Li and David Yarowsky. 2008. Unsupervised
Translation Induction for Chinese Abbreviations us-
ing Monolingual Corpora. Proceedings of ACL 2008,
pages 425-433.
Rila Mandala, Takenobu Tokunaga, and Hozumi Tanaka
2000. Query expansion using heterogeneous thesauri.,
In Information Processing and Management Volume
36 , Issue 3 2000,Pages 361 - 378
Xu Sun, Houfeng Wang and Yu Zhang 2006. Chi-
nese Abbreviation-Definition Identification: A SVM
Approach Using Context Information. Lecture Notes
in Computer Science,Volume 4182/2006, pages 530-
536.
Andrew J. Viterbi 1967. Error Boundsfor Convolutional
Codes and an Asymptotically Optimum Decoding Al-
gorithm. in IEEE Transactions on Information Theory,
Volume IT-13, in April, 1967,pages 260-269,
Hanna Wallach 2002. Efficient Training of Conditional
Random Fields. M. Thesis, University of Edinburgh,
2002.
Hui Yuan and Xianzhong Ruan 2002. Modern Chinese
abbreviation dictionary. Yuwen press, Beijing, China.
</reference>
<figure confidence="0.999690375">
7RS��
FRYHUDJH
���
��
��
iL
�� i
��
��
��
��
��
sO
i
)HDWXUH �
)HDWXUH ���
)HDWXUH �����
)HDWXUH �������
����
����
����
,`I.}I
7RSii
FRYHUDJH
���
��
��
��
��
~~
ri
.n
r
1
&amp;5)
&amp;5)/HJW
&amp;5)/HJWK:HE
�it.t)
�i.i
����
</figure>
<page confidence="0.976152">
276
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748339">
<title confidence="0.999772">Automatic Chinese Abbreviation Generation Using Conditional Random</title>
<author confidence="0.9393115">Field Dong Yang</author>
<author confidence="0.9393115">Yi-cheng Pan</author>
<author confidence="0.9393115">Sadaoki</author>
<affiliation confidence="0.9980975">Department of Computer Tokyo Institute of</affiliation>
<address confidence="0.925213">Tokyo 152-8552</address>
<abstract confidence="0.995857047619048">This paper presents a new method for automatically generating abbreviations for Chinese organization names. Abbreviations are commonly used in spoken Chinese, especially for organization names. The generation of Chinese abbreviation is much more complex than English abbreviations, most of which are acronyms and truncations. The abbreviation generation process is formulated as a character tagging problem and the conditional random field (CRF) is used as the tagging model. A carefully selected group of features is used in the CRF model. After generating a list of abbreviation candidates using the CRF, a length model is incorporated to re-rank the candidates. Finally the full-name and abbreviation co-occurrence information from a web search engine is utilized to further improve the performance. We achieved top-10 coverage of 88.3% by the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jing-shin Chang</author>
<author>Yu-Tso Lai</author>
</authors>
<title>A Preliminary Study on Probabilistic Models for Chinese Abbreviations.</title>
<date>2004</date>
<booktitle>Proceedings of ACL SIGHAN Workshop</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2289" citStr="Chang and Lai, 2004" startWordPosition="340" endWordPosition="343"> each word. Usually the original character order in the full-name is preserved in Figure 1: Chinese abbreviation examples the abbreviation. However, re-ordering of characters as shown in the third example in Figure 1 where characters “_” and “R” are swapped in the abbreviation, also happens. There has been a considerable amount of research on extracting full-name and abbreviation pairs in the same document for obtaining abbreviations (Li and Yarowsky, 2008; Sun et al., 2006; Fu et al., 2006). However, generation of abbreviations given a full-name is still a non-trivial problem. Chang and Lai (Chang and Lai, 2004) have proposed using a hidden Markov model to generate abbreviations from full-names. However, their method assumes that there is no word-to-null mapping, which means that every word in the full-name has to contribute at least one character to the abbreviation. This assumption does not hold for organizations’ names which have many word skips in the abbreviation generation. The CRF was first introduced to natural language processing (NLP) by (Lafferty et al., 2001) and has been widely used in word segmentation, part-ofspeech (POS) tagging, and some other NLP tasks. In this paper, we convert the</context>
<context position="6867" citStr="Chang and Lai, 2004" startWordPosition="1119" endWordPosition="1122">be either retained or discarded. For example, “A” (bureau) and “�” (institue), indicating a government department, are very common characters used in abbreviations. When they appear in full-names, they are likely to be kept in abbreviations. 2. Current word In the full name of “ 094LJ&amp; k*” (China Agricultural university), the word “41 1J” (China) is usually ignored in the abbreviation, but the word “4L3h” (agriculture) is usually kept. The length (the number of characters) is also an important feature of the current word. 3. Position of the current character in the current word Previous work (Chang and Lai, 2004) showed that the first character of a word has high possibility to form part of the abbreviation and this is also true for the last character of a three-character word. 4. Combination of feature 2. and 3. above Combination of the features 2 and 3 is expected to improve the performance, since the position inforЁ Ё༂ ⬉㾚ৄ ༂㾚 Ё  Ё ༂ ⬉ 㾚 ৄ Figure 2: Abbreviation in the CRF tagging format 274 mation affects the abbreviation along with the current word. For example, ending character in “k*” (university) and that in “RftR” (research institute) have very different possibilities to be selected for abbr</context>
</contexts>
<marker>Chang, Lai, 2004</marker>
<rawString>Jing-shin Chang and Yu-Tso Lai 2004. A Preliminary Study on Probabilistic Models for Chinese Abbreviations. Proceedings of ACL SIGHAN Workshop 2004, pages 9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guohong Fu</author>
<author>Kang-Kwong Luke</author>
</authors>
<title>GuoDong Zhou and Ruifeng Xu</title>
<date>2006</date>
<booktitle>Automatic Expansion of Abbreviations in Chinese News Text. Lecture Notes in Computer Science,</booktitle>
<location>Washington, DC.</location>
<marker>Fu, Luke, 2006</marker>
<rawString>Guohong Fu, Kang-Kwong Luke, GuoDong Zhou and Ruifeng Xu 2006. Automatic Expansion of Abbreviations in Chinese News Text. Lecture Notes in Computer Science, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data., In</title>
<date>2001</date>
<booktitle>Proceedings of International Conference on Machine Learning</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2757" citStr="Lafferty et al., 2001" startWordPosition="414" endWordPosition="417">et al., 2006; Fu et al., 2006). However, generation of abbreviations given a full-name is still a non-trivial problem. Chang and Lai (Chang and Lai, 2004) have proposed using a hidden Markov model to generate abbreviations from full-names. However, their method assumes that there is no word-to-null mapping, which means that every word in the full-name has to contribute at least one character to the abbreviation. This assumption does not hold for organizations’ names which have many word skips in the abbreviation generation. The CRF was first introduced to natural language processing (NLP) by (Lafferty et al., 2001) and has been widely used in word segmentation, part-ofspeech (POS) tagging, and some other NLP tasks. In this paper, we convert the Chinese abbreviation generation process to a CRF tagging problem. The key problem here is how to find a group of discrimҀ ᄺ ϝ ए䰶 एϝ䰶 ⏙ढ ᄺ ⏙ढ Ё Ё༂ ⬉㾚ৄ ༂㾚  273 Proceedings of NAACL HLT 2009: Short Papers, pages 273–276, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics inant and robust features. After using the CRF, we get a list of abbreviation candidates with associate probability scores. We also use the prior conditional proba</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data., In Proceedings of International Conference on Machine Learning 2001, pages 282-289</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiu Wing Doris Lee</author>
</authors>
<title>A Study of Automatic Expansion of Chinese Abbreviations.</title>
<date>2005</date>
<tech>MA Thesis,</tech>
<institution>The University of Hong Kong.</institution>
<contexts>
<context position="3804" citStr="Lee, 2005" startWordPosition="586" endWordPosition="587">inant and robust features. After using the CRF, we get a list of abbreviation candidates with associate probability scores. We also use the prior conditional probability of the length of the abbreviations given the length of the full-names to complement the CRF probability scores. Such global information is hard to include in the CRF model. In addition, we apply the full-name and abbreviation candidate cooccurrence statistics obtained on the web to increase the correctness of the abbreviation candidates. 2 Chinese Abbreviation Introduction Chinese abbreviations are generated by three methods (Lee, 2005): reduction, elimination, and generalization. Both in the reduction and elimination methods, characters are selected from the full-name, and the order of the characters is sometime changed. Note that this paper does not cover the case when the order is changed. The elimination means that one or more words in the full-name are ignored completely, while the reduction requires that at least one character is selected from each word. All the three examples in Figure 1 are produced by the elimination, where at least one word is skipped. Generalization, which is used to abbreviate a list of similar t</context>
</contexts>
<marker>Lee, 2005</marker>
<rawString>Hiu Wing Doris Lee 2005. A Study of Automatic Expansion of Chinese Abbreviations. MA Thesis, The University of Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Translation Induction for Chinese Abbreviations using Monolingual Corpora.</title>
<date>2008</date>
<booktitle>Proceedings of ACL</booktitle>
<pages>425--433</pages>
<contexts>
<context position="2129" citStr="Li and Yarowsky, 2008" startWordPosition="313" endWordPosition="316">shown in Figure 1. Most of the Chinese abbreviations are formed by selecting several characters from full-names, which are not necessarily the first character of each word. Usually the original character order in the full-name is preserved in Figure 1: Chinese abbreviation examples the abbreviation. However, re-ordering of characters as shown in the third example in Figure 1 where characters “_” and “R” are swapped in the abbreviation, also happens. There has been a considerable amount of research on extracting full-name and abbreviation pairs in the same document for obtaining abbreviations (Li and Yarowsky, 2008; Sun et al., 2006; Fu et al., 2006). However, generation of abbreviations given a full-name is still a non-trivial problem. Chang and Lai (Chang and Lai, 2004) have proposed using a hidden Markov model to generate abbreviations from full-names. However, their method assumes that there is no word-to-null mapping, which means that every word in the full-name has to contribute at least one character to the abbreviation. This assumption does not hold for organizations’ names which have many word skips in the abbreviation generation. The CRF was first introduced to natural language processing (NLP</context>
</contexts>
<marker>Li, Yarowsky, 2008</marker>
<rawString>Zhifei Li and David Yarowsky. 2008. Unsupervised Translation Induction for Chinese Abbreviations using Monolingual Corpora. Proceedings of ACL 2008, pages 425-433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rila Mandala</author>
<author>Takenobu Tokunaga</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Query expansion using heterogeneous thesauri.,</title>
<date>2000</date>
<booktitle>In Information Processing and Management Volume 36 , Issue</booktitle>
<volume>3</volume>
<pages>2000--361</pages>
<contexts>
<context position="9633" citStr="Mandala et al., 2000" startWordPosition="1588" endWordPosition="1591">“abbreviation candidate”+ “full-name” as queries and input them to the most popular Chinese search engine (www.baidu.com), and then we use the number of hits as the metric to perform re-ranking. The hits is theoretically related to the number of pages which contain both the full-name and abbreviation. The bigger the value of hits, the higher probability that the abbreviation is correct. We then simply multiply the previous probability score, obtained from Eq. 2, by the number of hits and re-rank the top-30 candidates accordingly. There are some other ways to use information retrieval methods (Mandala et al., 2000). Our method has an advantage that the access load to the web search engine is relatively small. 5 Experiment 5.1 Data introduction The corpus we use in this paper comes from two sources: one is the book “modern Chinese abbreviation dictionary” (Yuan and Ruan, 2002) and the other is the wikipedia. Altogether we collected 1945 pairs of organization full-names and their abbreviations. The data is randomly divided into two parts, a training set with 1298 pairs and a test set with 647 pairs. Table 1 shows the length mapping statistics of the training set. It can be seen that the average length of </context>
</contexts>
<marker>Mandala, Tokunaga, Tanaka, 2000</marker>
<rawString>Rila Mandala, Takenobu Tokunaga, and Hozumi Tanaka 2000. Query expansion using heterogeneous thesauri., In Information Processing and Management Volume 36 , Issue 3 2000,Pages 361 - 378</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
</authors>
<title>Houfeng Wang and Yu Zhang</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science,Volume</journal>
<volume>4182</volume>
<pages>530--536</pages>
<marker>Sun, 2006</marker>
<rawString>Xu Sun, Houfeng Wang and Yu Zhang 2006. Chinese Abbreviation-Definition Identification: A SVM Approach Using Context Information. Lecture Notes in Computer Science,Volume 4182/2006, pages 530-536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error Boundsfor Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.</title>
<date>1967</date>
<booktitle>in IEEE Transactions on Information Theory, Volume IT-13, in April, 1967,pages</booktitle>
<pages>260--269</pages>
<contexts>
<context position="5332" citStr="Viterbi, 1967" startWordPosition="853" endWordPosition="854"> CRF model A CRF is an undirected graphical model and assigns the following probability to a label sequence L = l1l2 ... lT, given an input sequence C = c1c2 ... cT, T P(L|C) = Z(C) exp(1:1:λkfk(lt, lt−1, C, t)) t=1 k (1) Here, fk is the feature function for the k-th feature, λk is the parameter which controls the weight of the k-th feature in the model, and Z(C) is the normalization term that makes the summation of the probability of all label sequences to 1. CRF training is usually performed through the typical L-BFGS algorithm (Wallach, 2002) and decoding is performed by Viterbi algorithm (Viterbi, 1967). In this paper, we use an open source toolkit “crf++”. 3.2 Abbreviation modeling as a tagging problem In order to use the CRF method in abbreviation generation, the abbreviation generation problem was converted to a tagging problem. The character is used as a tagging unit and each character in a fullname is tagged by a binary variable with the values of either Y or N: Y stands for a character used in the abbreviation and N means not. An example is given in Figure 2. 3.3 Feature selection for the CRF In the CRF method, feature function describes a co-occurrence relation, and it is defined as f</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Andrew J. Viterbi 1967. Error Boundsfor Convolutional Codes and an Asymptotically Optimum Decoding Algorithm. in IEEE Transactions on Information Theory, Volume IT-13, in April, 1967,pages 260-269,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
</authors>
<title>Efficient Training of Conditional Random Fields.</title>
<date>2002</date>
<tech>M. Thesis,</tech>
<institution>University of Edinburgh,</institution>
<contexts>
<context position="5269" citStr="Wallach, 2002" startWordPosition="844" endWordPosition="845">idered in this paper. 3 CRF Model for Abbreviation Modeling 3.1 CRF model A CRF is an undirected graphical model and assigns the following probability to a label sequence L = l1l2 ... lT, given an input sequence C = c1c2 ... cT, T P(L|C) = Z(C) exp(1:1:λkfk(lt, lt−1, C, t)) t=1 k (1) Here, fk is the feature function for the k-th feature, λk is the parameter which controls the weight of the k-th feature in the model, and Z(C) is the normalization term that makes the summation of the probability of all label sequences to 1. CRF training is usually performed through the typical L-BFGS algorithm (Wallach, 2002) and decoding is performed by Viterbi algorithm (Viterbi, 1967). In this paper, we use an open source toolkit “crf++”. 3.2 Abbreviation modeling as a tagging problem In order to use the CRF method in abbreviation generation, the abbreviation generation problem was converted to a tagging problem. The character is used as a tagging unit and each character in a fullname is tagged by a binary variable with the values of either Y or N: Y stands for a character used in the abbreviation and N means not. An example is given in Figure 2. 3.3 Feature selection for the CRF In the CRF method, feature func</context>
</contexts>
<marker>Wallach, 2002</marker>
<rawString>Hanna Wallach 2002. Efficient Training of Conditional Random Fields. M. Thesis, University of Edinburgh, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yuan</author>
<author>Xianzhong Ruan</author>
</authors>
<title>Modern Chinese abbreviation dictionary. Yuwen press,</title>
<date>2002</date>
<location>Beijing, China.</location>
<contexts>
<context position="9899" citStr="Yuan and Ruan, 2002" startWordPosition="1634" endWordPosition="1637">oth the full-name and abbreviation. The bigger the value of hits, the higher probability that the abbreviation is correct. We then simply multiply the previous probability score, obtained from Eq. 2, by the number of hits and re-rank the top-30 candidates accordingly. There are some other ways to use information retrieval methods (Mandala et al., 2000). Our method has an advantage that the access load to the web search engine is relatively small. 5 Experiment 5.1 Data introduction The corpus we use in this paper comes from two sources: one is the book “modern Chinese abbreviation dictionary” (Yuan and Ruan, 2002) and the other is the wikipedia. Altogether we collected 1945 pairs of organization full-names and their abbreviations. The data is randomly divided into two parts, a training set with 1298 pairs and a test set with 647 pairs. Table 1 shows the length mapping statistics of the training set. It can be seen that the average length of full-names is about 7.29. We know that for a full-name with length N, the number of abbreviation candidates is about 2N − 2 − N (exclude length of 0, 1, and N) and we can conclude that the average number of candidates for organization names in this corpus is more th</context>
</contexts>
<marker>Yuan, Ruan, 2002</marker>
<rawString>Hui Yuan and Xianzhong Ruan 2002. Modern Chinese abbreviation dictionary. Yuwen press, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>