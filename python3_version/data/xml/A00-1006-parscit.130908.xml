<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.962363">
Translation using Information on Dialogue Participants
</title>
<author confidence="0.901512">
Setsuo Yamada, Eiichiro Sumita and Hideki Kashioka
</author>
<affiliation confidence="0.794315">
ATR Interpreting Telecommunications Research Laboratories*
</affiliation>
<address confidence="0.8859665">
2-2, Hikaridai, Seika-cho, Soraku-gun,
Kyoto, 619-0288, JAPAN
</address>
<email confidence="0.969667">
{syamada, sumita, kashioka}@itl.atr.cojpt
</email>
<sectionHeader confidence="0.993866" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957833333333">
This paper proposes a way to improve the trans-
lation quality by using information on dialogue
participants that is easily obtained from out-
side the translation component. We incorpo-
rated information on participants&apos; social roles
and genders into transfer rules and dictionary
entries. An experiment with 23 unseen dia-
logues demonstrated a recall of 65% and a preci-
sion of 86%. These results showed that our sim-
ple and easy-to-implement method is effective,
and is a key technology enabling smooth con-
versation with a dialogue translation system.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987655">
Recently, various dialogue translation systems
have been proposed (Bub and others, 1997;
Kurematsu and Morimoto, 1996; Rayner and
Carter, 1997; Rosé and Levin, 1998; Sumita
and others, 1999; Yang and Park, 1997; Vi-
dal, 1997). If we want to make a conversation
proceed smoothly using these translation sys-
tems, it is important to use not only linguis-
tic information, which comes from the source
language, but also extra-linguistic information,
which does not come from the source language,
but, is shared between the participants of the
conversation.
Several dialogue translation methods that
use extra-linguistic information have been pro-
posed. Horiguchi outlined how &amp;quot;spoken lan-
guage pragmatic information&amp;quot; can be trans-
lated (Horiguchi, 1997). However, she did not
apply this idea to a dialogue translation system.
LuperFoy et al. proposed a software architec-
</bodyText>
<footnote confidence="0.5675216">
•Current affiliation is ATR Spoken Language Trans-
lation Research Laboratories
ICurrent mail addresses are
Isetsuo.yamada, elichiro.surnita, hideki.kashiokal
@slt.atr.co.jp
</footnote>
<bodyText confidence="0.997969390243902">
ture that uses &amp;quot;a pragmatic adaptation&amp;quot; (Lu-
perFoy and others, 1998), and Mima et al. pro-
posed a method that uses &amp;quot;situational informa-
tion&amp;quot; (Mima and others, 1997). LuperFoy et al.
simulated their method on man-machine inter-
faces and Mima et al. preliminarily evaluated
their method. Neither study, however, applied
its proposals to an actual dialogue translation
system.
The above mentioned methods will need time
to work in practice, since it is hard to obtain
the extra-linguistic information on which they
depend.
We have been paying special attention to &amp;quot;po-
liteness,&amp;quot; because a lack of politeness can inter-
fere with a smooth conversation between two
participants, such as a clerk and a customer. It
is easy for a dialogue translation system to know
which participant is the clerk and which is the
customer from the interface (such as the wires
to the microphones).
This paper describes a method of &amp;quot;polite-
ness&amp;quot; selection according to a participant&apos;s so-
cial role (a clerk or a customer), which is eas-
ily obtained from the extra-linguistic environ-
ment. We incorporated each participant&apos;s so-
cial role into transfer rules and transfer dictio-
nary entries. We then conducted an experiment
with 23 unseen dialogues (344 utterances). Our
method achieved a recall of 65% and a preci-
sion of 86%. These rates could be improved to
86% and 96%, respectively (see Section 4). It
is therefore possible to use a &amp;quot;participant&apos;s so-
cial role&amp;quot; (a clerk or a customer in this case)
to appropriately make the translation results
&amp;quot;polite,&amp;quot; and to make the conversation proceed
smoothly with a dialogue translation system.
Section 2 analyzes the relationship between a
particular participant&apos;s social role (a clerk) and
politeness in Japanese. Section 3 describes our
proposal in detail using an English-to-Japanese
</bodyText>
<page confidence="0.998909">
37
</page>
<bodyText confidence="0.999633">
translation system. Section 4 shows an exper-
iment and results, followed by a discussion in
Section 5. Finally, Section 6 concludes this pa-
per.
</bodyText>
<sectionHeader confidence="0.7483455" genericHeader="method">
2 A Participant&apos;s Social Role and
Politeness
</sectionHeader>
<bodyText confidence="0.999743395348837">
This section focuses on one participant&apos;s social
role. We investigated Japanese outputs of a di-
alogue translation system to see how many ut-
terances should be polite expressions in a cur-
rent translation system for travel arrangement.
We input 1,409 clerk utterances into a Transfer
Driven Machine Translation system (Sumita
and others, 1999) (TDMT for short). The in-
puts were closed utterances, meaning the sys-
tem already knew the utterances, enabling the
utterances to be transferred at a good quality.
Therefore, we used closed utterances as the in-
puts to avoid translation errors.
As a result, it was shown that about 70%
(952) of all utterances should be improved to use
polite expressions. This result shows that a cur-
rent translation system is not enough to make
a conversation smoothly. Not surprisingly, if all
expressions were polite, some Japanese speakers
would feel insulted. Therefore, Japanese speak-
ers do not have to use polite expression in all
utterances.
We classified the investigated data into dif-
ferent types of English expressions for Japanese
politeness, i.e., into honorific titles, parts of
speech such as verbs, and canned phrases,
as shown in Table 1; however, not all types
appeared in the data. For example, when
the clerk said &amp;quot;How will you be paying, Mr.
Suzuki,&amp;quot; the Japanese translation was made
polite as &amp;quot;donoyouni oshiharaininarimasu-ka
suzuki-sama&amp;quot; in place of the standard expres-
sion &amp;quot;donoyouni shiharaimasu-ka suzuki-san.&amp;quot;
Table 1 shows that there is a difference in
how expressions should be made more polite ac-
cording to the type, and that many polite ex-
pressions can be translated by using only local
information, i.e., transfer rules and dictionary
entries. In the next section, we describe how to
incorporate the information on dialogue partic-
ipants, such as roles and genders, into transfer
rules and dictionary entries in a dialogue trans-
lation system.
</bodyText>
<sectionHeader confidence="0.9639835" genericHeader="method">
3 A Method of Using Information
on Dialogue Participants
</sectionHeader>
<bodyText confidence="0.999950571428571">
This section describes how to use information
on dialogue participants, such as participants&apos;
social roles and genders. First, we describe
TDMT, which we also used in our experiment.
Second, we mention how to modify transfer
rules and transfer dictionary entries according
to information on dialogue participants.
</bodyText>
<subsectionHeader confidence="0.995797">
3.1 Transfer Driven Machine
Translation
</subsectionHeader>
<bodyText confidence="0.999616625">
TDMT uses bottom-up left-to-right chart pars-
ing with transfer rules as shown in Figure 1.
The parsing determines the best structure and
best transferred result locally by performing
structural disambiguation using semantic dis-
tance calculations, in parallel with the deriva-
tion of possible structures. The semantic dis-
tance is defined by a thesaurus.
</bodyText>
<figure confidence="0.641854666666667">
(source pattern)
((target pattern 1)
((source example 1)
(source example 2)
)
(target pattern 2)
</figure>
<figureCaption confidence="0.999734">
Figure 1: Transfer rule format
</figureCaption>
<bodyText confidence="0.999877523809524">
A transfer rule consists of a source pattern,
a target pattern, and a source example. The
source pattern consists of variables and con-
stituent boundaries (Furuse and lida, 1996).
A constituent boundary is either a functional
word or the part-of-speech of a left constituent&apos;s
last word and the part-of-speech of a right con-
stituent&apos;s first word. In Example (1), the con-
stituent boundary (V—CN) is inserted between
&amp;quot;accept&amp;quot; and &amp;quot;payment,&amp;quot; because &amp;quot;accept&amp;quot; is
a Verb and &amp;quot;payment&amp;quot; is a Common Noun.
The target pattern consists of variables that cor-
respond to variables in the source pattern and
words of the target language. The source exam-
ple consists of words that come from utterances
referred to when a person creates transfer rules
(we call such utterances closed utterances).
Figure 2 shows a transfer rule whose source
pattern is (X (V—CN) Y). Variable X corre-
sponds to x, which is used in the target pat-
tern, and Y corresponds to y, which is also
</bodyText>
<page confidence="0.999395">
38
</page>
<tableCaption confidence="0.99981">
Table 1: Examples of polite expressions
</tableCaption>
<table confidence="0.926064288888889">
Type: verb, title
Eng: How will you be paying, Mr. Suzuki
Standard:
Polite:
Gloss:
donoyouni shiharaimasu-ka suzuki-san
donoyouni oshiharaininarimasu-ka suzuki-sama
How pay-QUESTION suzuki-Mr. .
Type: verb, common noun
Eng: We have two types of rooms available
Standard:
Polite:
Gloss:
aiteiru ni-shurui-no heya-ga arimasu
aiteiru ni-shurui-no oheya-ga gozaimasu
available two-types-of room-TOP have
Type: auxiliary verb
Eng: You can shop for hours
Standard:
Polite:
Gloss:
suujikan kaimono-wo surukotogadekimasu
suujikan kaimono-wo shiteitadakemasu
for hours make-OBJ can
Type: pronoun
Eng: Your room number, please
Standard:
Polite:
Gloss:
anatano heya bangou-wo onegaishimasu
okyakusamano heya ban gou-wo one gaishimasu
Your room number-sc obj please
Type: canned phrase
Eng: How can I help you
Standard:
Polite:
Gloss:
dou shimashitaka
douitta goyoukendeshouka
How can I help you
Example (1)
Eng: We accept payment by credit card
Standard: watashitachi-wa kurejitto-kaado-deno shiharai-wo uketsukemasu
Polite: watashidomo-wa kurejitto-kaado-deno oshiharai-wo oukeshimasu
Gloss: We-TOP credit-card-by payment-OBJ accept
</table>
<bodyText confidence="0.999187277777778">
used in the target pattern. The source exam-
ple ((&amp;quot;accept&amp;quot;) (&amp;quot;payment&amp;quot;)) comes from Ex-
ample (1), and the other source examples come
from the other closed utterances. This transfer
rule means that if the source pattern is (X (V-
C N) Y) then (y &amp;quot;wo&amp;quot; x) or (y &amp;quot;ni&amp;quot; x) is selected
as the target pattern, where an input word pair
corresponding to X and Y is semantically the
most similar in a thesaurus to, or exactly the
same as, the source example. For example, if
an input word pair corresponding to X and Y
is semantically the most similar in a thesaurus
to, or exactly the same as, ((&amp;quot;accept&amp;quot;) (&amp;quot;pay-
ment&amp;quot;)), then the target pattern (y &amp;quot;wo&amp;quot; x) is
selected in Figure 2. As a result, an appropriate
target pattern is selected.
After a target pattern is selected, TDMT cre-
ates a target structure according to the pattern
</bodyText>
<equation confidence="0.999401571428571">
(X (V-CN)Y)
((y &amp;quot;wo&amp;quot; x)
(((&amp;quot;accept&amp;quot;) (&amp;quot;payment&amp;quot;))
((&amp;quot;take&amp;quot;) (&amp;quot;picture&amp;quot;)))
(y &amp;quot;ni&amp;quot; x)
(((&amp;quot;take&amp;quot;) (&amp;quot;bus&amp;quot;))
((&amp;quot;get&amp;quot;) (&amp;quot;sunstroke&amp;quot;)))
</equation>
<figureCaption confidence="0.993962">
Figure 2: Transfer rule example
</figureCaption>
<bodyText confidence="0.999961714285714">
by referring to a transfer dictionary, as shown
in Figure 3. If the input is &amp;quot;accept (V-CN)
payment,&amp;quot; then this part is translated into &amp;quot;shi-
harai wo uketsukeru.&amp;quot; &amp;quot;wo&amp;quot; is derived from the
target pattern (y &amp;quot;wo&amp;quot; x), and &amp;quot;shiharai&amp;quot; and
&amp;quot;uketsukeru&amp;quot; are derived from the transfer dic-
tionary, as shown in Figure 4.
</bodyText>
<page confidence="0.992862">
39
</page>
<figure confidence="0.919520083333333">
(source pattern)
(((target pattern 11) :pattern-cond 11
(target pattern 12) :pattern-cond 12
(target pattern 1,2) :default)
((source example 1)
)
(((source example 1) (target word 11) :word-cond ii
(source example 1) —+ (target word 12) :word-cond 12
(source example 1) (target word 1,,,) :default)
••• )
(((target pattern 21) :pattern-cond 21
•-• )))
</figure>
<figureCaption confidence="0.99428">
Figure 5: Transfer rule format with information on dialogue participants
</figureCaption>
<figure confidence="0.9012365">
(((source word 1) --+ (target word 11) :cond 11
(source word 1) --+ (target word 12) :cond 12
(source word 1) (target word lk) :default)
•-• )
</figure>
<figureCaption confidence="0.987369">
Figure 6: Dictionary format with information on dialogue participants
</figureCaption>
<figure confidence="0.9207075">
((source word) (target word)
)
</figure>
<figureCaption confidence="0.992097">
Figure 3: Transfer dictionary format
</figureCaption>
<figure confidence="0.4735055">
((&amp;quot;accept&amp;quot;) (&amp;quot;uketsukeru&amp;quot;)
(&amp;quot;payment&amp;quot;) (&amp;quot;shiharai&amp;quot;))
</figure>
<figureCaption confidence="0.985135">
Figure 4: Transfer dictionary example
</figureCaption>
<figure confidence="0.717000833333333">
(X &amp;quot;sama&amp;quot;)
(((&amp;quot;Mr.&amp;quot; x) :h-gender male
(&amp;quot;Ms.&amp;quot; x) :h-gender female
(&amp;quot;Mr-ms.&amp;quot; x))
(((&amp;quot;case&amp;quot;))
((&amp;quot;room number&amp;quot;)))
</figure>
<figureCaption confidence="0.948336">
Figure 7: Transfer rule example with the par-
ticipant&apos;s gender
</figureCaption>
<subsectionHeader confidence="0.928886">
3.2 Transfer Rules and Entries
</subsectionHeader>
<bodyText confidence="0.95801640625">
according to Information on
Dialogue Participants
For this research, we modified the transfer rules
and the transfer dictionary entries, as shown in
Figures 5 and 6. In Figure 5, the target pattern
&amp;quot;target pattern 11&amp;quot; and the source word &amp;quot;source
example 1&amp;quot; are used to change the translation
according to information on dialogue partici-
pants. For example, if &amp;quot;:pattern-cond 11&amp;quot; is de-
fined as &amp;quot;:h-gender male&amp;quot; as shown in Figure 7,
then &amp;quot;target pattern 11&amp;quot; is selected when the
hearer is a male, that is, &amp;quot;(&amp;quot;Mr.&amp;quot; x)&amp;quot; is selected.
Moreover, if &amp;quot;:word-cond 11&amp;quot; is defined as &amp;quot;:s-
role clerk&amp;quot; as shown in Figure 8, then &amp;quot;source
example 1&amp;quot; is translated into &amp;quot;target word 11&amp;quot;
when the speaker is a clerk, that is, &amp;quot;accept&amp;quot; is
translated into &amp;quot;oukesuru.&amp;quot; Translations such
as &amp;quot;target word 11&amp;quot; are valid only in the source
pattern; that is, a source example might not
always be translated into one of these target
words. If we always want to produce transla-
tions according to information on dialogue par-
ticipants, then we need to modify the entries
in the transfer dictionary like Figure 6 shows.
Conversely, if we do not want to always change
the translation, then we should not modify the
entries but modify the transfer rules. Several
conditions can also be given to &amp;quot;:word-cond&amp;quot;
and &amp;quot;:pattern-cond.&amp;quot; For example, &amp;quot;:s-role cus-
tomer and :s-gender female,&amp;quot; which means the
speaker is a customer and a female, can be
given. In Figure 5, &amp;quot;:default&amp;quot; means the de-
</bodyText>
<page confidence="0.996312">
40
</page>
<bodyText confidence="0.99466925">
fault target pattern or word if no condition is
matched. The condition is checked from up to
down in order; that is, first, &amp;quot;:pattern-cond 11,&amp;quot;
second, &amp;quot;:pattern-cond 12,&amp;quot; ... and so on.
</bodyText>
<equation confidence="0.997775">
(X (V—CN) Y)
((y &amp;quot;wo&amp;quot; x)
(((&amp;quot;accept&amp;quot;) (&amp;quot;payment&amp;quot;))
((&amp;quot;take&amp;quot;) (&amp;quot;picture&amp;quot;)))
(((&amp;quot;accept&amp;quot;) (&amp;quot;oukesuru&amp;quot;) :s-role clerk
(&amp;quot;accept&amp;quot;) (&amp;quot;uketsukeru&amp;quot;)))
</equation>
<figureCaption confidence="0.885046">
Figure 8: Transfer rule example with a partici-
pant&apos;s role
</figureCaption>
<equation confidence="0.85035375">
(((&amp;quot;payment&amp;quot;) --+ (&amp;quot;oshiharai&amp;quot;) :s-role clerk
(&amp;quot;payment&amp;quot;) (&amp;quot;shiharai&amp;quot;))
((&amp;quot;we&amp;quot;) (&amp;quot;watashidomo&amp;quot;) :s-role clerk
(&amp;quot;we&amp;quot;) --+ (&amp;quot;watashitac.hi&amp;quot;)))
</equation>
<figureCaption confidence="0.9489975">
Figure 9: Transfer dictionary example with a
speaker&apos;s role
</figureCaption>
<bodyText confidence="0.9999633">
Even though we do not have rules and en-
tries for pattern conditions and word condi-
tions according to another participant&apos;s infor-
mation, such as &amp;quot;:s-role customer&amp;quot;(which means
the speaker&apos;s role is a customer) and &amp;quot;:s-gender
male&amp;quot; (which means the speaker&apos;s gender is
male), TDMT can translate expressions corre-
sponding to this information too. For example,
&amp;quot;Very good, please let me confirm them&amp;quot; will
be translated into &amp;quot;shouchiitashimasita kakunin
sasete itadakimasu&amp;quot; when the speaker is a clerk
or &amp;quot;soredekekkoudesu kakunin sasete kudasai&amp;quot;
when the speaker is a customer, as shown in
Example (2).
By making a rule and an entry like the ex-
amples shown in Figures 8 and 9, the utter-
ance of Example (1) will be translated into
&amp;quot;watashidomo wa kurejitto kaado deno oshi-
harai wo oukeshimasu&amp;quot; when the speaker is a
clerk.
</bodyText>
<sectionHeader confidence="0.987502" genericHeader="method">
4 An Experiment
</sectionHeader>
<bodyText confidence="0.99848840625">
The TDMT system for English-to-Japanese at
the time of the experiment had about 1,500
transfer rules and 8,000 transfer dictionary en-
tries. In other words, this TDMT system was
capable of translating 8,000 English words into
Japanese words. About 300 transfer rules and
40 transfer dictionary entries were modified to
improve the level of &amp;quot;politeness.&amp;quot;
We conducted an experiment using the trans-
fer rules and transfer dictionary for a clerk with
23 unseen dialogues (344 utterances). Our input
was off-line, i.e., a transcription of dialogues,
which was encoded with the participant&apos;s social
role. In the on-line situation, our system can
not infer whether the participant&apos;s social role is
a clerk or a customer, but can instead determine
the role without error from the interface (such
as a microphone or a button).
In order to evaluate the experiment, we clas-
sified the Japanese translation results obtained
for the 23 unseen dialogues (199 utterances from
a clerk, and 145 utterances from a customer,
making 344 utterances in total) into two types:
expressions that had to be changed to more po-
lite expressions, and expressions that did not.
Table 2 shows the number of utterances that in-
cluded an expression which had to be changed
into a more polite one (indicated by &amp;quot;Yes&amp;quot;) and
those that did not (indicated by &amp;quot;No&amp;quot;). We ne-
glected 74 utterances whose translations were
too poor to judge whether to assign a &amp;quot;Yes&amp;quot; or
&amp;quot;No.&amp;quot;
</bodyText>
<tableCaption confidence="0.927241">
Table 2: The number of utterances to be
changed or not
</tableCaption>
<table confidence="0.798305">
Necessity The number
of change of utterances
Yes 104
No 166
Out of scope
Total 344
</table>
<tableCaption confidence="0.646815333333333">
* 74 translations were too poor to handle for the
&amp;quot;politeness&amp;quot; problem, and so they are ignored in this
paper.
</tableCaption>
<bodyText confidence="0.999983333333333">
The translation results were evaluated to see
whether the impressions of the translated re-
sults were improved or not with/without mod-
ification for the clerk from the viewpoint of
&amp;quot;politeness.&amp;quot; Table 3 shows the impressions
obtained according to the necessity of change
shown in Table 2.
The evaluation criteria are recall and preci-
sion, which are defined as follows:
</bodyText>
<equation confidence="0.789854">
Recall =
</equation>
<bodyText confidence="0.963224">
number of utterances whose impression is better
number of utterances which should be more polite
</bodyText>
<page confidence="0.922418">
74
41
</page>
<table confidence="0.968675833333333">
Example (2)
Eng: Very good, please let me confirm them
Standard: wakariznasita kakunin sasete kudasai
Clerk: shouchiitashimasita kakunin sasete itadakimasu
Customer: soredekekkoudesu kakunin sasete kudasai
Gloss: very good confirm let me please
</table>
<tableCaption confidence="0.999231">
Table 3: Evaluation on using the speaker&apos;s role
</tableCaption>
<table confidence="0.9924038">
Necessity Impression The number
of change of utterances
Yes better 68
(104) same 5
worse 3
no-sit 8
No aetter 0
(166) same 3
worse 0
no- i ,
</table>
<bodyText confidence="0.4961818">
better: Impression of a translation is better.
same: Impression of a translation has not changed.
worse: Impression of a translation is worse.
no-clilf: There is no difference between the two
translations.
</bodyText>
<equation confidence="0.789071">
Precision =
</equation>
<bodyText confidence="0.998884782608696">
number of utterances whose impression is better
number of utterances whose expression has been
changed by the modified rules and entries
The recall was 65% (= 68 ÷ (68 + 5 + 3 -I- 28))
and the precision was 86% (= 68 ÷ (68 + 5 + 3 +
0 + 3 -I- 0)).
There are two main reasons which bring down
these rates. One reason is that TDMT does not
know who or what the agent of the action in
the utterance is; agents are also needed to se-
lect polite expressions. The other reason is that
there are not enough rules and transfer dictio-
nary entries for the clerk.
It is easier to take care of the latter problem
than the former problem. If we resolve the lat-
ter problem, that is, if we expand the transfer
rules and the transfer dictionary entries accord-
ing to the &amp;quot;participant&apos;s social role&amp;quot; (a clerk and
a customer), then the recall rate and the preci-
sion rate can be improved (to 86% and 96%,
respectively, as we have found). As a result, we
can say that our method is effective for smooth
conversation with a dialogue translation system.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.977004">
In general, extra-linguistic information is hard
to obtain. However, some extra-linguistic infor-
mation can be easily obtained:
(1) One piece of information is the participant&apos;s
social role, which can be obtained from the in-
terface such as the microphone used. It was
proven that a clerk and customer as the social
roles of participants are useful for translation
into Japanese. However, more research is re-
quired on another participant&apos;s social role.
(2) Another piece of information is the par-
ticipant&apos;s gender, which can be obtained by a
speech recognizer with high accuracy (Takezawa
and others, 1998; Naito and others, 1998). We
have considered how expressions can be useful
by using the hearer&apos;s gender for Japanese-to-
English translation.
Let us consider the Japanese honorific title
&amp;quot;sama&amp;quot; or &amp;quot;san.&amp;quot; If the hearer&apos;s gender is male,
then it should be translated &amp;quot;Mr.&amp;quot; and if the
hearer&apos;s gender is female, then it should be
translated &amp;quot;Ms.&amp;quot; as shown in Figure 7. Ad-
ditionally, the participant&apos;s gender is useful for
translating typical expressions for males or fe-
males. For example, Japanese &amp;quot;wa&amp;quot; is often at-
tached at the end of the utterance by females.
It is also important for a dialogue translation
system to use extra-linguistic information which
the system can obtain easily, in order to make
a conversation proceed smoothly and comfort-
ably for humans using the translation system.
We expect that other pieces of usable informa-
tion can be easily obtained in the future. For
example, age might be obtained from a cellular
telephone if it were always carried by the same
person and provided with personal information.
In this case, if the system knew the hearer was a
child, it could change complex expressions into
easier ones.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9983455">
We have proposed a method of translation us-
ing information on dialogue participants, which
</bodyText>
<page confidence="0.996996">
42
</page>
<bodyText confidence="0.999973575757576">
is easily obtained from outside the translation
component, and applied it to a dialogue trans-
lation system for travel arrangement. This
method can select a polite expression for an
utterance according to the &amp;quot;participant&apos;s social
role,&amp;quot; which is easily determined by the inter-
face (such as the wires to the microphones). For
example, if the microphone is for the clerk (the
speaker is a clerk), then the dialogue translation
system can select a more polite expression.
In an English-to-Japanese translation system,
we added additional transfer rules and transfer
dictionary entries for the clerk to be more po-
lite than the customer. Then, we conducted an
experiment with 23 unseen dialogues (344 ut-
terances). We evaluated the translation results
to see whether the impressions of the results im-
proved or not. Our method achieved a recall of
65% and a precision of 86%. These rates could
easily be improved to 86% and 96%, respec-
tively. Therefore, we can say that our method
is effective for smooth conversation with a dia-
logue translation system.
Our proposal has a limitation in that if the
system does not know who or what the agent
of an action in an utterance is, it cannot ap-
propriately select a polite expression. We are
considering ways to enable identification of the
agent of an action in an utterance and to ex-
pand the current framework to improve the level
of politeness even more. In addition, we intend
to apply other extra-linguistic information to a
dialogue translation system.
</bodyText>
<sectionHeader confidence="0.996709" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982539234375">
Thomas Bub et al. 1997. Verbmobil: The
combination of deep and shallow processing
for spontaneous speech translation. In the
1997 International Conference on Acoustics,
Speech, and Signal Processing: ICASSP 97,
pages 71-74, Munich.
Osamu Furuse and Hitoshi lida. 1996. In-
cremental translation utilizing constituent
boundary patterns. In Proceedings of
COLING-96, pages 412-417, Copenhagen.
Keiko Horiguchi. 1997. Towards translating
spoken language pragmatics in an analogical
framework. In Proceedings of A CL/EACL-97
workshop on Spoken Language Translation,
pages 16-23, Madrid.
Akira Kurematsu and Tsuyoshi Morimoto.
1996. Automatic Speech Translation. Gordon
and Breach Publishers.
Susann LuperFoy et al. 1998. An architecture
for dialogue management, context tracking,
and pragmatic adaptation in spoken dialogue
system. In Proceedings of COLING-ACL&apos;98,
pages 794-801, Montreal.
Hideki Mima et al. 1997. A situation-based
approach to spoken dialogue translation be-
tween different social roles. In Proceedings of
TMI-97, pages 176-183, Santa Fe.
Masaki Naito et al. 1998. Acoustic and lan-
guage model for speech translation system
ATR-MATRIX. In the Proceedings of the
1998 Spring Meeting of the Acoustical Soci-
ety of Japan, pages 159-160 (in Japanese).
Manny Rayner and David Carter. 1997. Hy-
brid language processing in the spoken lan-
guage translator. In the 1997 International
Conference on Acoustics, Speech, and Signal
Processing: ICASSP 97, pages 107-110, Mu-
nich.
Carolyn Penstein Rose and Lori S. Levin. 1998.
An interactive domain independent approach
to robust dialogue interpretation. In Proceed-
ings of COLING-ACL&apos;98, pages 1129-1135,
Montreal.
Eiichiro Sumita et al. 1999. Solutions to prob-
lems inherent in spoken-language translation:
The ATR-MATRIX approach. In the Ma-
chine Translation Summit VII, pages 229-
235, Singapore.
Toshiyuki Takezawa et al. 1998. A Japanese-
to-English speech translation system: ATR-
MATRIX. In the 5th International Con-
ference On Spoken Language Processing:
ICSLP-98, pages 2779-2782, Sydney.
Enrique Vidal. 1997. Finite-state speech-to-
speech translation. In the 1997 International
Conference on Acoustics, Speech, and Signal
Processing: ICASSP 97, pages 111-114, Mu-
nich.
Jae-Woo Yang and Jun Park. 1997. An exper-
iment on Korean-to-English and Korean-to-
Japanese spoken language translation. In the
1997 International Conference on Acoustics,
Speech, and Signal Processing: ICASSP 97,
pages 87-90, Munich.
</reference>
<page confidence="0.999833">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930702">
<title confidence="0.99911">Translation using Information on Dialogue Participants</title>
<author confidence="0.988319">Setsuo Yamada</author>
<author confidence="0.988319">Eiichiro Sumita</author>
<author confidence="0.988319">Hideki Kashioka</author>
<affiliation confidence="0.999648">ATR Interpreting Telecommunications Research Laboratories*</affiliation>
<address confidence="0.991504">2-2, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288, JAPAN</address>
<email confidence="0.992502">kashioka}@itl.atr.cojpt</email>
<abstract confidence="0.997311384615385">This paper proposes a way to improve the translation quality by using information on dialogue participants that is easily obtained from outside the translation component. We incorporated information on participants&apos; social roles and genders into transfer rules and dictionary entries. An experiment with 23 unseen dialogues demonstrated a recall of 65% and a preciof results showed that our simple and easy-to-implement method is effective, and is a key technology enabling smooth conversation with a dialogue translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thomas Bub</author>
</authors>
<title>Verbmobil: The combination of deep and shallow processing for spontaneous speech translation.</title>
<date>1997</date>
<booktitle>In the 1997 International Conference on Acoustics, Speech, and Signal Processing: ICASSP 97,</booktitle>
<pages>71--74</pages>
<location>Munich.</location>
<marker>Bub, 1997</marker>
<rawString>Thomas Bub et al. 1997. Verbmobil: The combination of deep and shallow processing for spontaneous speech translation. In the 1997 International Conference on Acoustics, Speech, and Signal Processing: ICASSP 97, pages 71-74, Munich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osamu Furuse</author>
<author>Hitoshi lida</author>
</authors>
<title>Incremental translation utilizing constituent boundary patterns.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96,</booktitle>
<pages>412--417</pages>
<location>Copenhagen.</location>
<contexts>
<context position="6823" citStr="Furuse and lida, 1996" startWordPosition="1052" endWordPosition="1055">chart parsing with transfer rules as shown in Figure 1. The parsing determines the best structure and best transferred result locally by performing structural disambiguation using semantic distance calculations, in parallel with the derivation of possible structures. The semantic distance is defined by a thesaurus. (source pattern) ((target pattern 1) ((source example 1) (source example 2) ) (target pattern 2) Figure 1: Transfer rule format A transfer rule consists of a source pattern, a target pattern, and a source example. The source pattern consists of variables and constituent boundaries (Furuse and lida, 1996). A constituent boundary is either a functional word or the part-of-speech of a left constituent&apos;s last word and the part-of-speech of a right constituent&apos;s first word. In Example (1), the constituent boundary (V—CN) is inserted between &amp;quot;accept&amp;quot; and &amp;quot;payment,&amp;quot; because &amp;quot;accept&amp;quot; is a Verb and &amp;quot;payment&amp;quot; is a Common Noun. The target pattern consists of variables that correspond to variables in the source pattern and words of the target language. The source example consists of words that come from utterances referred to when a person creates transfer rules (we call such utterances closed utterances</context>
</contexts>
<marker>Furuse, lida, 1996</marker>
<rawString>Osamu Furuse and Hitoshi lida. 1996. Incremental translation utilizing constituent boundary patterns. In Proceedings of COLING-96, pages 412-417, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiko Horiguchi</author>
</authors>
<title>Towards translating spoken language pragmatics in an analogical framework.</title>
<date>1997</date>
<booktitle>In Proceedings of A CL/EACL-97 workshop on Spoken Language Translation,</booktitle>
<pages>16--23</pages>
<location>Madrid.</location>
<contexts>
<context position="1582" citStr="Horiguchi, 1997" startWordPosition="228" endWordPosition="229">nd Carter, 1997; Rosé and Levin, 1998; Sumita and others, 1999; Yang and Park, 1997; Vidal, 1997). If we want to make a conversation proceed smoothly using these translation systems, it is important to use not only linguistic information, which comes from the source language, but also extra-linguistic information, which does not come from the source language, but, is shared between the participants of the conversation. Several dialogue translation methods that use extra-linguistic information have been proposed. Horiguchi outlined how &amp;quot;spoken language pragmatic information&amp;quot; can be translated (Horiguchi, 1997). However, she did not apply this idea to a dialogue translation system. LuperFoy et al. proposed a software architec•Current affiliation is ATR Spoken Language Translation Research Laboratories ICurrent mail addresses are Isetsuo.yamada, elichiro.surnita, hideki.kashiokal @slt.atr.co.jp ture that uses &amp;quot;a pragmatic adaptation&amp;quot; (LuperFoy and others, 1998), and Mima et al. proposed a method that uses &amp;quot;situational information&amp;quot; (Mima and others, 1997). LuperFoy et al. simulated their method on man-machine interfaces and Mima et al. preliminarily evaluated their method. Neither study, however, appl</context>
</contexts>
<marker>Horiguchi, 1997</marker>
<rawString>Keiko Horiguchi. 1997. Towards translating spoken language pragmatics in an analogical framework. In Proceedings of A CL/EACL-97 workshop on Spoken Language Translation, pages 16-23, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Kurematsu</author>
<author>Tsuyoshi Morimoto</author>
</authors>
<title>Automatic Speech Translation.</title>
<date>1996</date>
<publisher>Gordon and Breach Publishers.</publisher>
<contexts>
<context position="956" citStr="Kurematsu and Morimoto, 1996" startWordPosition="130" endWordPosition="133">on quality by using information on dialogue participants that is easily obtained from outside the translation component. We incorporated information on participants&apos; social roles and genders into transfer rules and dictionary entries. An experiment with 23 unseen dialogues demonstrated a recall of 65% and a precision of 86%. These results showed that our simple and easy-to-implement method is effective, and is a key technology enabling smooth conversation with a dialogue translation system. 1 Introduction Recently, various dialogue translation systems have been proposed (Bub and others, 1997; Kurematsu and Morimoto, 1996; Rayner and Carter, 1997; Rosé and Levin, 1998; Sumita and others, 1999; Yang and Park, 1997; Vidal, 1997). If we want to make a conversation proceed smoothly using these translation systems, it is important to use not only linguistic information, which comes from the source language, but also extra-linguistic information, which does not come from the source language, but, is shared between the participants of the conversation. Several dialogue translation methods that use extra-linguistic information have been proposed. Horiguchi outlined how &amp;quot;spoken language pragmatic information&amp;quot; can be tr</context>
</contexts>
<marker>Kurematsu, Morimoto, 1996</marker>
<rawString>Akira Kurematsu and Tsuyoshi Morimoto. 1996. Automatic Speech Translation. Gordon and Breach Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susann LuperFoy</author>
</authors>
<title>An architecture for dialogue management, context tracking, and pragmatic adaptation in spoken dialogue system.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL&apos;98,</booktitle>
<pages>794--801</pages>
<location>Montreal.</location>
<marker>LuperFoy, 1998</marker>
<rawString>Susann LuperFoy et al. 1998. An architecture for dialogue management, context tracking, and pragmatic adaptation in spoken dialogue system. In Proceedings of COLING-ACL&apos;98, pages 794-801, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Mima</author>
</authors>
<title>A situation-based approach to spoken dialogue translation between different social roles.</title>
<date>1997</date>
<booktitle>In Proceedings of TMI-97,</booktitle>
<pages>176--183</pages>
<location>Santa Fe.</location>
<marker>Mima, 1997</marker>
<rawString>Hideki Mima et al. 1997. A situation-based approach to spoken dialogue translation between different social roles. In Proceedings of TMI-97, pages 176-183, Santa Fe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Naito</author>
</authors>
<title>Acoustic and language model for speech translation system ATR-MATRIX.</title>
<date>1998</date>
<booktitle>In the Proceedings of the 1998 Spring Meeting of the Acoustical Society of Japan,</booktitle>
<pages>159--160</pages>
<note>(in Japanese).</note>
<marker>Naito, 1998</marker>
<rawString>Masaki Naito et al. 1998. Acoustic and language model for speech translation system ATR-MATRIX. In the Proceedings of the 1998 Spring Meeting of the Acoustical Society of Japan, pages 159-160 (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>David Carter</author>
</authors>
<title>Hybrid language processing in the spoken language translator.</title>
<date>1997</date>
<booktitle>In the 1997 International Conference on Acoustics, Speech, and Signal Processing: ICASSP 97,</booktitle>
<pages>107--110</pages>
<location>Munich.</location>
<contexts>
<context position="981" citStr="Rayner and Carter, 1997" startWordPosition="134" endWordPosition="137">n on dialogue participants that is easily obtained from outside the translation component. We incorporated information on participants&apos; social roles and genders into transfer rules and dictionary entries. An experiment with 23 unseen dialogues demonstrated a recall of 65% and a precision of 86%. These results showed that our simple and easy-to-implement method is effective, and is a key technology enabling smooth conversation with a dialogue translation system. 1 Introduction Recently, various dialogue translation systems have been proposed (Bub and others, 1997; Kurematsu and Morimoto, 1996; Rayner and Carter, 1997; Rosé and Levin, 1998; Sumita and others, 1999; Yang and Park, 1997; Vidal, 1997). If we want to make a conversation proceed smoothly using these translation systems, it is important to use not only linguistic information, which comes from the source language, but also extra-linguistic information, which does not come from the source language, but, is shared between the participants of the conversation. Several dialogue translation methods that use extra-linguistic information have been proposed. Horiguchi outlined how &amp;quot;spoken language pragmatic information&amp;quot; can be translated (Horiguchi, 1997</context>
</contexts>
<marker>Rayner, Carter, 1997</marker>
<rawString>Manny Rayner and David Carter. 1997. Hybrid language processing in the spoken language translator. In the 1997 International Conference on Acoustics, Speech, and Signal Processing: ICASSP 97, pages 107-110, Munich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Penstein Rose</author>
<author>Lori S Levin</author>
</authors>
<title>An interactive domain independent approach to robust dialogue interpretation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL&apos;98,</booktitle>
<pages>1129--1135</pages>
<location>Montreal.</location>
<marker>Rose, Levin, 1998</marker>
<rawString>Carolyn Penstein Rose and Lori S. Levin. 1998. An interactive domain independent approach to robust dialogue interpretation. In Proceedings of COLING-ACL&apos;98, pages 1129-1135, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiichiro Sumita</author>
</authors>
<title>Solutions to problems inherent in spoken-language translation: The ATR-MATRIX approach.</title>
<date>1999</date>
<booktitle>In the Machine Translation Summit VII,</booktitle>
<pages>229--235</pages>
<marker>Sumita, 1999</marker>
<rawString>Eiichiro Sumita et al. 1999. Solutions to problems inherent in spoken-language translation: The ATR-MATRIX approach. In the Machine Translation Summit VII, pages 229-235, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
</authors>
<title>A Japaneseto-English speech translation system: ATRMATRIX.</title>
<date>1998</date>
<booktitle>In the 5th International Conference On Spoken Language Processing: ICSLP-98,</booktitle>
<pages>2779--2782</pages>
<location>Sydney.</location>
<marker>Takezawa, 1998</marker>
<rawString>Toshiyuki Takezawa et al. 1998. A Japaneseto-English speech translation system: ATRMATRIX. In the 5th International Conference On Spoken Language Processing: ICSLP-98, pages 2779-2782, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
</authors>
<title>Finite-state speech-tospeech translation.</title>
<date>1997</date>
<booktitle>In the 1997 International Conference on Acoustics, Speech, and Signal Processing: ICASSP 97,</booktitle>
<pages>111--114</pages>
<location>Munich.</location>
<contexts>
<context position="1063" citStr="Vidal, 1997" startWordPosition="150" endWordPosition="152"> incorporated information on participants&apos; social roles and genders into transfer rules and dictionary entries. An experiment with 23 unseen dialogues demonstrated a recall of 65% and a precision of 86%. These results showed that our simple and easy-to-implement method is effective, and is a key technology enabling smooth conversation with a dialogue translation system. 1 Introduction Recently, various dialogue translation systems have been proposed (Bub and others, 1997; Kurematsu and Morimoto, 1996; Rayner and Carter, 1997; Rosé and Levin, 1998; Sumita and others, 1999; Yang and Park, 1997; Vidal, 1997). If we want to make a conversation proceed smoothly using these translation systems, it is important to use not only linguistic information, which comes from the source language, but also extra-linguistic information, which does not come from the source language, but, is shared between the participants of the conversation. Several dialogue translation methods that use extra-linguistic information have been proposed. Horiguchi outlined how &amp;quot;spoken language pragmatic information&amp;quot; can be translated (Horiguchi, 1997). However, she did not apply this idea to a dialogue translation system. LuperFoy</context>
</contexts>
<marker>Vidal, 1997</marker>
<rawString>Enrique Vidal. 1997. Finite-state speech-tospeech translation. In the 1997 International Conference on Acoustics, Speech, and Signal Processing: ICASSP 97, pages 111-114, Munich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jae-Woo Yang</author>
<author>Jun Park</author>
</authors>
<title>An experiment on Korean-to-English and Korean-toJapanese spoken language translation.</title>
<date>1997</date>
<booktitle>In the 1997 International Conference on Acoustics, Speech, and Signal Processing: ICASSP 97,</booktitle>
<pages>87--90</pages>
<location>Munich.</location>
<contexts>
<context position="1049" citStr="Yang and Park, 1997" startWordPosition="146" endWordPosition="149">slation component. We incorporated information on participants&apos; social roles and genders into transfer rules and dictionary entries. An experiment with 23 unseen dialogues demonstrated a recall of 65% and a precision of 86%. These results showed that our simple and easy-to-implement method is effective, and is a key technology enabling smooth conversation with a dialogue translation system. 1 Introduction Recently, various dialogue translation systems have been proposed (Bub and others, 1997; Kurematsu and Morimoto, 1996; Rayner and Carter, 1997; Rosé and Levin, 1998; Sumita and others, 1999; Yang and Park, 1997; Vidal, 1997). If we want to make a conversation proceed smoothly using these translation systems, it is important to use not only linguistic information, which comes from the source language, but also extra-linguistic information, which does not come from the source language, but, is shared between the participants of the conversation. Several dialogue translation methods that use extra-linguistic information have been proposed. Horiguchi outlined how &amp;quot;spoken language pragmatic information&amp;quot; can be translated (Horiguchi, 1997). However, she did not apply this idea to a dialogue translation sy</context>
</contexts>
<marker>Yang, Park, 1997</marker>
<rawString>Jae-Woo Yang and Jun Park. 1997. An experiment on Korean-to-English and Korean-toJapanese spoken language translation. In the 1997 International Conference on Acoustics, Speech, and Signal Processing: ICASSP 97, pages 87-90, Munich.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>