<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.982739">
Experimental Support for a Categorical Compositional
Distributional Model of Meaning
</title>
<author confidence="0.996429">
Edward Grefenstette
</author>
<affiliation confidence="0.999792">
University of Oxford
Department of Computer Science
</affiliation>
<address confidence="0.992471">
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
</address>
<email confidence="0.998544">
edward.grefenstette@cs.ox.ac.uk
</email>
<author confidence="0.957566">
Mehrnoosh Sadrzadeh
</author>
<affiliation confidence="0.9996095">
University of Oxford
Department of Computer Science
</affiliation>
<address confidence="0.9924745">
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
</address>
<email confidence="0.998994">
mehrs@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.99673" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997509666666667">
Modelling compositional meaning for sen-
tences using empirical distributional methods
has been a challenge for computational lin-
guists. We implement the abstract categorical
model of Coecke et al. (2010) using data from
the BNC and evaluate it. The implementation
is based on unsupervised learning of matrices
for relational words and applying them to the
vectors of their arguments. The evaluation is
based on the word disambiguation task devel-
oped by Mitchell and Lapata (2008) for intran-
sitive sentences, and on a similar new experi-
ment designed for transitive sentences. Our
model matches the results of its competitors
in the first experiment, and betters them in the
second. The general improvement in results
with increase in syntactic complexity show-
cases the compositional power of our model.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9989869375">
As competent language speakers, we humans can al-
most trivially make sense of sentences we’ve never
seen or heard before. We are naturally good at un-
derstanding ambiguous words given a context, and
forming the meaning of a sentence from the mean-
ing of its parts. But while human beings seem
comfortable doing this, machines fail to deliver.
Search engines such as Google either fall back on
bag of words models—ignoring syntax and lexical
relations—or exploit superficial models of lexical
semantics to retrieve pages with terms related to
those in the query (Manning et al., 2008).
However, such models fail to shine when it comes
to processing the semantics of phrases and sen-
tences. Discovering the process of meaning as-
signment in natural language is among the most
challenging and foundational questions of linguis-
tics and computer science. The findings thereof will
increase our understanding of cognition and intelli-
gence and shall assist in applications to automating
language-related tasks such as document search.
Compositional type-logical approaches (Mon-
tague, 1974; Lambek, 2008) and distributional mod-
els of lexical semantics (Schutze, 1998; Firth, 1957)
have provided two partial orthogonal solutions to the
question. Compositional formal semantic models
stem from classical ideas from mathematical logic,
mainly Frege’s principle that the meaning of a sen-
tence is a function of the meaning of its parts (Frege,
1892). Distributional models are more recent and
can be related to Wittgenstein’s later philosophy of
‘meaning is use’, whereby meanings of words can be
determined from their context (Wittgenstein, 1953).
The logical models relate to well known and robust
logical formalisms, hence offering a scalable theory
of meaning which can be used to reason inferen-
tially. The distributional models have found their
way into real world applications such as thesaurus
extraction (Grefenstette, 1994; Curran, 2004) or au-
tomated essay marking (Landauer, 1997), and have
connections to semantically motivated information
retrieval (Manning et al., 2008). This two-sortedness
of defining properties of meaning: ‘logical form’
versus ‘contextual use’, has left the quest for ‘what is
the foundational structure of meaning?’ even more
of a challenge.
Recently, Coecke et al. (2010) used high level
cross-disciplinary techniques from logic, category
</bodyText>
<page confidence="0.953678">
1394
</page>
<note confidence="0.957746">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999878607142857">
theory, and physics to bring the above two ap-
proaches together. They developed a unified mathe-
matical framework whereby a sentence vector is by
definition a function of the Kronecker product of its
word vectors. A concrete instantiation of this the-
ory was exemplified on a toy hand crafted corpus
by Grefenstette et al. (2011). In this paper we imple-
ment it by training the model over the entire BNC.
The highlight of our implementation is that words
with relational types, such as verbs, adjectives, and
adverbs are matrices that act on their arguments. We
provide a general algorithm for building (or indeed
learning) these matrices from the corpus.
The implementation is evaluated against the task
provided by Mitchell and Lapata (2008) for disam-
biguating intransitive verbs, as well as a similar new
experiment for transitive verbs. Our model improves
on the best method evaluated in Mitchell and Lapata
(2008) and offers promising results for the transitive
case, demonstrating its scalability in comparison to
that of other models. But we still feel there is need
for a different class of experiments to showcase mer-
its of compositionality in a statistically significant
manner. Our work shows that the categorical com-
positional distributional model of meaning permits
a practical implementation and that this opens the
way to the production of large scale compositional
models.
</bodyText>
<sectionHeader confidence="0.93161" genericHeader="introduction">
2 Two Orthogonal Semantic Models
</sectionHeader>
<bodyText confidence="0.999428117647059">
Formal Semantics To compute the meaning of a
sentence consisting of n words, meanings of these
words must interact with one another. In formal se-
mantics, this further interaction is represented as a
function derived from the grammatical structure of
the sentence, but meanings of words are amorphous
objects of the domain: no distinction is made be-
tween words that have the same type. Such models
consist of a pairing of syntactic interpretation rules
(in the form of a grammar) with semantic interpreta-
tion rules, as exemplified by the simple model pre-
sented in Figure 1.
The parse of a sentence such as “cats like milk”
typically produces its semantic interpretation by
substituting semantic representation for their gram-
matical constituents and applying β-reduction where
needed. Such a derivation is shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.904491">
Syntactic Analysis Semantic Interpretation
</subsectionHeader>
<equation confidence="0.983722">
|V P|(|NP|)
|cats|, |milk|, ...
|Vt|(|NP|)
Ayx.|like|(x, y), ...
</equation>
<figureCaption confidence="0.913577">
Figure 1: A simple model of formal semantics.
</figureCaption>
<equation confidence="0.862919">
|like|(|cats|, |milk|)
|cats |Ax.|like|(x, |milk|)
|milk |Ayx.|like|(x, y)
</equation>
<figureCaption confidence="0.998723">
Figure 2: A parse tree showing a semantic derivation.
</figureCaption>
<bodyText confidence="0.99450384375">
This methodology is used to translate sentences
of natural language into logical formulae, then use
computer-aided automation tools to reason about
them (Alshawi, 1992). One major drawback is that
the result of such analysis can only deal with truth
or falsity as the meaning of a sentence, and says
nothing about the closeness in meaning or topic of
expressions beyond their truth-conditions and what
models satisfy them, hence do not perform well on
language tasks such as search. Furthermore, an un-
derlying domain of objects and a valuation function
must be provided, as with any logic, leaving open
the question of how we might learn the meaning of
language using such a model, rather than just use it.
Distributional Models Distributional models of
semantics, on the other hand, dismiss the interaction
between syntactically linked words and are solely
concerned with lexical semantics. Word meaning
is obtained empirically by examining the contexts1
in which a word appears, and equating the meaning
of a word with the distribution of contexts it shares.
The intuition is that context of use is what we ap-
peal to in learning the meaning of a word, and that
words that frequently have the same sort of context
in common are likely to be semantically related.
For instance, beer and sherry are both drinks, al-
coholic, and often cause a hangover. We expect
these facts to be reflected in a sufficiently large cor-
pus: the words ‘beer’ and ‘sherry’ occur within the
1E.g. words which appear in the same sentence or n-word
window, or words which hold particular grammatical or depen-
dency relations to the word being learned.
</bodyText>
<equation confidence="0.99150375">
S → NP VP
NP → cats, milk, etc.
VP → Vt NP
Vt → like, hug, etc.
</equation>
<page confidence="0.940923">
1395
</page>
<bodyText confidence="0.96793575">
context of identifying words such as ‘drink’, ‘alco-
holic’ and ‘hangover’ more frequently than they oc-
cur with other content words.
Such context distributions can be encoded as vec-
tors in a high dimensional space with contexts as
−−→
basis vectors. For any word vector word, the scalar
weight cword iassociated with each context basis vec-
tor →−ni is a function of the number of times the
word has appeared in that context. Semantic vectors
(Ci ord, c2 ord , cnord) are also denoted by sums
lof such weight/basis vector pairs:
</bodyText>
<equation confidence="0.7670215">
−−→ word =
i
</equation>
<bodyText confidence="0.999662357142857">
Learning a semantic vector is just learning its ba-
sis weights from the corpus. This setting offers ge-
ometric means to reason about semantic similarity
(e.g. via cosine measure or k-means clustering), as
discussed in Widdows (2005).
The principal drawback of such models is their
non-compositional nature: they ignore grammatical
structure and logical words, and hence cannot com-
pute the meanings of phrases and sentences in the
same efficient way that they do for words. Com-
mon operations discussed in (Mitchell and Lapata,
2008) such as vector addition (+) and component-
wise multiplication (O, cf. §4 for details) are com-
mutative, hence if vw−→ = →− v + →−w or →−v O →−w , then
</bodyText>
<equation confidence="0.558027333333333">
vw = −→
−→ wv, leading to unwelcome equalities such as
−−−−−−−−−−−−−→ −−−−−−−−−−−−−→
</equation>
<bodyText confidence="0.981589909090909">
the dog bit the man = the man bit the dog
Non-commutative operations, such as the Kronecker
product (cf. §4 for definition) can take word-order
into account (Smolensky, 1990) or even some more
complex syntactic relations, as described in Clark
and Pulman (2007). However, the dimensionality of
sentence vectors produced in this manner differs for
sentences of different length, barring all sentences
from being compared in the same vector space, and
growing exponentially with sentence length hence
quickly becoming computationally intractable.
</bodyText>
<sectionHeader confidence="0.983898" genericHeader="method">
3 A Hybrid Logico-Distributional Model
</sectionHeader>
<bodyText confidence="0.993753404761905">
Whereas semantic compositional mechanisms for
set-theoretic constructions are well understood,
there are no obvious corresponding methods for vec-
tor spaces. To solve this problem, Coecke et al.
−−→
milk. The logical recipe tells us to apply the mean-
ing of the verb to the meanings of subject and object.
But how can a vector apply to other vectors? The so-
lution proposed above implies that one needs to have
different levels of meaning for words with different
types. This is similar to logical models where verbs
are relations and nouns are atomic sets. So verb vec-
tors should be built differently from noun vectors,
for instance as matrices.
The general information as to which words should
be matrices and which words atomic vectors is in
fact encoded in the type-logical representation of the
grammatical structure of the sentence. This is the
linear map with word vectors as input and sentence
vectors as output. Hence, at least theoretically, one
should be able to build sentence vectors and com-
pare their synonymity in exactly the same way as
one measures word synonymity.
Pregroup Grammars The aforementioned linear
maps turn out to be the grammatical reductions
of a type-logic called a Lambek pregroup gram-
mar (Lambek, 2008)2. Pregroups and vector spaces
share the same high level mathematical structure, re-
ferred to as a compact closed category, for a proof
and details of this claim see Coecke et al. (2010); for
a friendly introduction to category theory, see Co-
ecke and Paquette (2011). One consequence of this
parity is that the grammatical reductions of a pre-
group grammar can be directly transformed into lin-
ear maps that act on vectors.
In a nutshell, pregroup types are either atomic
or compound. Atomic types can be simple (e.g. n
for noun phrases, s for statements) or left/right
superscripted—referred to as adjoint types (e.g. nr
and nl). An example of a compound type is that of
a verb nrsnl. The superscripted types express that
the verb is a relation with two arguments of type n,
</bodyText>
<footnote confidence="0.736137666666667">
2The usage of pregroup types is not essential, the types of
any other logic, for instance CCG can be used, but should be
translated into the language of pregroups.
</footnote>
<equation confidence="0.7512965">
cword n
Z i
</equation>
<bodyText confidence="0.953684222222222">
(2010) use the abstract setting of category theory to
turn the grammatical structure of a sentence into a
morphism compatible with the higher level logical
structure of vector spaces.
One pragmatic consequence of this abstract idea
is as follows. In distributional models, there is a
−→
−→
meaning vector for each word, e.g. cats, like, and
</bodyText>
<page confidence="0.730344">
1396
</page>
<bodyText confidence="0.998567285714286">
which have to occur to the right and to the left of
it, and that it outputs an argument of the type s. A
transitive sentence has types as shown in Figure 3.
Each type n cancels out with its right adjoint nr
from the right and its left adjoint nl from the left;
mathematically speaking these mean3
nln &lt; 1 and nnr &lt; 1
Here 1 is the unit of concatenation: 1n = n1 =
n. The corresponding grammatical reduction of a
transitive sentence is nnrsnl &lt; 1s1 = s. Each such
reduction can be depicted as a wire diagram. The
diagram of a transitive sentence is shown in Figure 3.
Cats like milk.
n nr s nl n
</bodyText>
<figureCaption confidence="0.9977675">
Figure 3: The pregroup types and reduction diagram for
a transitive sentence.
</figureCaption>
<bodyText confidence="0.97028948">
Syntax-guided Semantic Composition Accord-
ing to Coecke et al. (2010) and based on a general
completeness theorem between compact categories,
wire diagrams, and vector spaces, the meaning of
sentences can be canonically reduced to linear alge-
braic formulae. The following is the meaning vector
of our transitive sentence:
++++++++- cats like = (f ) (cats ®1 like ® �) (I)
Here f is the linear map that encodes the grammati-
cal structure. The categorical morphism correspond-
ing to it is denoted by the tensor product of 3 compo-
nents: eV ®1S ®eW, where V and W are subject and
object spaces, S is the sentence space, the E’s are the
cups, and 1S is the straight line in the diagram. The
cups stand for taking inner products, which when
done with the basis vectors imitate substitution. The
straight line stands for the identity map that does
nothing. By the rules of the category, equation (I) re-
duces to the following linear algebraic formula with
3The relation ≤ is the partial order of the pregroup. It corre-
sponds to implication =⇒ in a logical reading thereof. If these
inequalities are replaced by equalities, i.e. if nln = 1 = nnr,
then the pregroup collapses into a group where nl = nr.
lower dimensions, hence the dimensional explosion
problem for Kronecker products is avoided:
</bodyText>
<equation confidence="0.835737285714286">
E citj(+-
catsJ+-vi)+-st (+- wjJ++-
milk) E S (II)
itj
-+vi , wj +-are basis vectors of V and W. The inner
product (+-
cats J -+vi) substitutes the weights of +-
</equation>
<bodyText confidence="0.995554428571429">
cats
into the first argument place of the verb (similarly
for object and second argument place). -+st is a basis
vector of the sentence space S in which meanings of
sentences live, regardless of their grammatical struc-
ture.
The degree of synonymity of sentences is ob-
tained by taking the cosine measure of their vectors.
S is an abstract space: it needs to be instantiated
to provide concrete meanings and synonymity mea-
sures. For instance, a truth-theoretic model is ob-
tained by taking the sentence space S to be the 2-
dimensional space with basis vectors J1) (True) and
J0) (False).
</bodyText>
<sectionHeader confidence="0.875584" genericHeader="method">
4 Building Matrices for Relational Words
</sectionHeader>
<bodyText confidence="0.882680695652174">
In this section we present a general scheme to build
matrices for relational words. Recall that given
a vector space A with basis {+-ni}i, the Kronecker
product of two vectors -+v = E i can and =
Ei cbi+-ni is defined as follows: i z
-+ v ® �-+ w = caicbj (+-ni ® -+nj)
ij
where (+-ni ® +-nj) is just the pairing of the basis of A,
i.e. (+-ni, n-+j). The Kronecker product vectors belong
in the tensor product of A with itself: A ® A, hence
if A has dimension r, these will be of dimensionality
r xr. The point-wise multiplication of these vectors
is defined as follows
-+v O �-+ w = cai cbi ni
-+
i
The intuition behind having a matrix for a rela-
tional word is that any relation R on sets X and Y ,
i.e. R C_ X x Y can be represented as a matrix,
namely one that has as row-bases x E X and as
column-bases y E Y , with weight cxy = 1 where
(x, y) E R and 0 otherwise. In a distributional set-
ting, the weights, which are natural or real numbers,
</bodyText>
<page confidence="0.967045">
1397
</page>
<bodyText confidence="0.999513659090909">
will represent more: ‘the extent according to which
x and y are related’. This can be determined in dif-
ferent ways.
Suppose X is the set of animals, and ‘chase’ is a
relation on it: chase ⊆ X × X. Take x = ‘dog’
and y = ‘cat’: with our type-logical glasses on, the
obvious choice would be to take cxy to be the num-
ber of times ‘dog’ has chased ‘cat’, i.e. the number
of times the sentence ‘the dog chases the cat’ has
appeared in the corpus. But in the distributional set-
ting, this method will be too syntactic and dismissive
of the actual meaning of ‘cat’ and ‘dog’. If instead
the corpus contains the sentence ‘the hound hunted
the wild cat’, cxy will be 0, restricting us to only
assign meaning to sentences that have directly ap-
peared in the corpus. We propose to, instead, use a
level of abstraction by taking words such as verbs to
be distributions over the semantic information in the
vectors of their context words, rather than over the
context words themselves.
Start with an r-dimensional vector space N with
basis {−→n i}i, in which meaning vectors of atomic
words, such as nouns, live. The basis vectors of N
are in principle all the words from the corpus, how-
ever in practice and following Mitchell and Lapata
(2008) we had to restrict these to a subset of the
most occurring words. These basis vectors are not
restricted to nouns: they can as well be verbs, adjec-
tives, and adverbs, so that we can define the mean-
ing of a noun in all possible contexts—as is usual
in context-based models—and not only in the con-
text of other nouns. Note that basis words with re-
lational types are treated as pure lexical items rather
than as semantic objects represented as matrices. In
short, we count how many times a noun has occurred
close to words of other syntactic types such as ‘elect’
and ‘scientific’, rather than count how many times it
has occurred close to their corresponding matrices:
it is the lexical tokens that form the context, not their
meaning.
Each relational word P with grammatical type 7r
and m adjoint types α1, α2, · · · , αm is encoded as
an (r × ... × r) matrix with m dimensions. Since
our vector space N has a fixed basis, each such ma-
</bodyText>
<equation confidence="0.928727875">
trix is represented in vector form as follows:
cij···ζ (−→n i ⊗ →−n j ⊗ ··· ⊗ →−n ζ)
{z
m
This vector lives in the tensor space
N ⊗ N ⊗ ··· ⊗ N . Each cij···ζ is computed
 |{z }
m
</equation>
<bodyText confidence="0.881475">
according to the procedure described in Figure 4.
</bodyText>
<figureCaption confidence="0.9960325">
Figure 4: Procedure for learning weights for matrices of
words ‘P’ with relational types 7r of m arguments.
</figureCaption>
<bodyText confidence="0.9959165">
Linear algebraically, this procedure corresponds to
computing the following
</bodyText>
<equation confidence="0.983545">
�−→ �
w 1 ⊗ →− w 2 ⊗ · · · ⊗ →− w m k
</equation>
<bodyText confidence="0.9983772">
Type-logical examples of relational words are
verbs, adjectives, and adverbs. A transitive verb is
represented as a 2 dimensional matrix since its type
is nrsnl with two adjoint types nr and nl. The cor-
responding vector of this matrix is
</bodyText>
<listItem confidence="0.681252666666667">
−−→ Xverb = cij (−→n i ⊗ →−n j)
ij
1) Consider a sequence of words containing a re-
</listItem>
<bodyText confidence="0.846121777777778">
lational word ‘P’ and its arguments w1, w2, · · · ,
wm, occurring in the same order as described in
P’s grammatical type 7r. Refer to these sequences
as ‘P’-relations. Suppose there are k of them.
2) Retrieve the vector →−wl of each argument wl.
3) Suppose w1 has weight c1i on basis vector →−n i,
w2 has weight c2j on basis vector →−n j, · · · , and
wm has weight cmζ on basis vector →−n ζ. Multiply
these weights
</bodyText>
<equation confidence="0.810472">
c1i × c2j × ··· × cmζ
</equation>
<bodyText confidence="0.815384">
4) Repeat the above steps for all the k ‘P’-
relations, and suma the corresponding weights
</bodyText>
<equation confidence="0.974401">
Xcij···ζ = �c1 �
k i × c2 j × · · · × cm ζ k
</equation>
<bodyText confidence="0.998007333333333">
aWe also experimented with multiplication, but the spar-
sity of noun vectors resulted in most verb matrices being
empty.
</bodyText>
<equation confidence="0.9982126">
→− XP =
 |{z }
ij · · · ζ
→− XP =
k
</equation>
<page confidence="0.863274">
1398
</page>
<bodyText confidence="0.9999435">
The weight cij corresponding to basis vector ten i®
te n j, is the extent according to which words that have
co-occurred with ten i have been the subject of the
‘verb’ and words that have co-occurred with ten j
have been the object of the ‘verb’. This example
computation is demonstrated in Figure 5.
</bodyText>
<listItem confidence="0.914703125">
1) Consider phrases containing ‘verb’, its subject
w1 and object w2. Suppose there are k of them.
2) Retrieve vectors tew 1 and tew2.
3) Suppose tew 1 has weight c1i on ten i and tew 2 has
c2j on ten j. Multiply these weights c1i x c2j.
4) Repeat the above steps for all k ‘verb’-
relations and sum the corresponding weights
Ek(c1i x c2j)k
</listItem>
<figureCaption confidence="0.99673">
Figure 5: Procedure for learning weights for matrices of
transitive verbs.
</figureCaption>
<equation confidence="0.913902333333333">
Linear algebraically, we are computing
(et �
w 1 ® te w 2 k
</equation>
<bodyText confidence="0.834768818181818">
As an example, consider the verb ‘show’ and sup-
pose there are two ‘show’-relations in the corpus:
s1 = table show result
s2 = map show location
The vector of ‘show’ is
+ map eet ®eeeet
location
Consider an N space with four basis vectors ‘far’,
‘room’, ‘scientific’, and ‘elect’. The TF/IDF-
weighted values for vectors of the above four nouns
(built from the BNC) are as shown in Table 1.
</bodyText>
<table confidence="0.963875">
i te table map result location
ni
1 far 6.6 5.6 7 5.9
2 room 27 7.4 0.99 7.3
3 scientific 0 5.4 13 6.1
4 elect 0 0 4.2 0
</table>
<tableCaption confidence="0.999755">
Table 1: Sample weights for selected noun vectors.
</tableCaption>
<bodyText confidence="0.91761625">
Part of the matrix of ‘show’ is presented in Table 2.
As a sample computation, the weight c11 for
vector (1, 1), i.e. (etfar, etfar) is computed by multiply-
ing weights of ‘table’ and ‘result’ on far, i.e. 6.6x7,
</bodyText>
<table confidence="0.963127">
far room scientific elect
far 79.24 47.41 119.96 27.72
room 232.66 80.75 396.14 113.2
scientific 32.94 31.86 32.94 0
elect 0 0 0 0
</table>
<tableCaption confidence="0.997238">
Table 2: Sample semantic matrix for ‘show’.
</tableCaption>
<bodyText confidence="0.9538045">
et
multiplying weights of ‘map’ and ‘location’ on far,
i.e. 5.6 x 5.9 then adding these 46.2 + 33.04 and
obtaining the total weight 79.24.
The same method is applied to build matrices for di-
transitive verbs, which will have 3 dimensions, and
adjectives and adverbs, which will be of 1 dimension
each.
</bodyText>
<sectionHeader confidence="0.977182" genericHeader="method">
5 Computing Sentence Vectors
</sectionHeader>
<bodyText confidence="0.9906438">
Meaning of sentences are vectors computed by tak-
ing the variables of the categorical prescription of
meaning (the linear map f obtained from the gram-
matical reduction of the sentence) to be determined
by the matrices of the relational words. For instance
the meaning of the transitive sentence ‘sub verb obj’
is:
eeeeeeeet � (et
sub verb obj = sub  |te v i)(et w j  |etobj) citjetst
itj
We take V := W := N and S = N ® N, then
Eitj citjets t is determined by the matrix of the verb,
i.e. substitute it by Eij cij(etn i ® ten j)4. Hence
eeeeeeeet
sub verb obj becomes:
</bodyText>
<equation confidence="0.93497825">
(et
sub |teni)(etnj  |etobj)cij(etni ®ten j) =
sub obj (-n-+i ® �&apos;)
ci cj cij 7
</equation>
<bodyText confidence="0.9820955">
This can be decomposed to point-wise multiplica-
tion of two vectors as follows:
</bodyText>
<equation confidence="0.41395">
� � ciubj bj(etn i ® n j)) O M �cij(et n i ®te n j)
ij ij
</equation>
<bodyText confidence="0.7011045">
4Note that by doing so we are also reducing the verb space
from N ® (N ® N) ® N to N ® N, since for our construction
we only need tuples of the form _it i ® ��n i ® _nj ® _it j which
are isomorphic to pairs (��n i ® _# j).
</bodyText>
<equation confidence="0.585703142857143">
eet �verb =
k
eet eet eeet
show = table ® result
�
ij
I
</equation>
<page confidence="0.977421">
1399
</page>
<bodyText confidence="0.999943">
The left argument is the Kronecker product of sub-
ject and object vectors and the right argument is the
vector of the verb, so we obtain
</bodyText>
<equation confidence="0.685086">
(−→
sub ® −→) O −−→
obj verb
</equation>
<bodyText confidence="0.934519">
Since O is commutative, this provides us with a dis-
tributional version of the type-logical meaning of the
sentence: point-wise multiplication of the meaning
of the verb to the Kronecker product of its subject
and object:
−−−−−−−−→ −−→sub verb obj = verb O (−→
sub ® obj)
This mathematical operation can be informally de-
scribed as a structured ‘mixing’ of the information
of the subject and object, followed by it being ‘fil-
tered’ through the information of the verb applied
to them, in order to produce the information of the
sentence.
In the transitive case, 5 = N ® N, hence →−s t =
→−n i ® →−n j. More generally, the vector space cor-
responding to the abstract sentence space 5 is the
concrete tensor space (N ® ... ® N) for m the di-
mension of the matrix of the ‘verb’. As we have
seen above, in practice we do not need to build this
tensor space, as the computations thereof reduce to
point-wise multiplications and summations.
Similar computations yield meanings of sentences
with adjectives and adverbs. For instance the mean-
ing of a transitive sentence with a modified subject
and a modified verb we have
−−−−−−−−−−−−−→
adj sub verb obj adv =
(adv O verb) O ( (adj O sub) ® o j)
After building vectors for sentences, we can com-
pare their meaning and measure their degree of syn-
onymy by taking their cosine measure.
</bodyText>
<sectionHeader confidence="0.998869" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999793137254902">
Evaluating such a framework is no easy task. What
to evaluate depends heavily on what sort of applica-
tion a practical instantiation of the model is geared
towards. In (Grefenstette et al., 2011), it is sug-
gested that the simplified model we presented and
expanded here could be evaluated in the same way as
lexical semantic models, measuring compositionally
built sentence vectors against a benchmark dataset
such as that provided by Mitchell and Lapata (2008).
In this section, we briefly describe the evaluation of
our model against this dataset. Following this, we
present a new evaluation task extending the experi-
mental methodology of Mitchell and Lapata (2008)
to transitive verb-centric sentences, and compare our
model to those discussed by Mitchell and Lapata
(2008) within this new experiment.
First Dataset Description The first experiment,
described in detail by Mitchell and Lapata (2008),
evaluates how well compositional models disam-
biguate ambiguous words given the context of a po-
tentially disambiguating noun. Each entry of the
dataset provides a noun, a target verb and landmark
verb (both intransitive). The noun must be com-
posed with both verbs to produce short phrase vec-
tors the similarity of which is measured by the can-
didate. Also provided with each entry is a classifi-
cation (“High” or “Low”) indicating whether or not
the verbs are indeed semantically close within the
context of the noun, as well as an evaluator-set simi-
larity score between 1 and 7 (along with an evaluator
identifier), where 1 is low similarity and 7 is high.
Evaluation Methodology Candidate models pro-
vide a similarity score for each entry. The scores
of high similarity entries and low similarity entries
are averaged to produce a mean High score and
mean Low score for the model. The correlation of
the model’s similarity judgements with the human
judgements is also calculated using Spearman’s p, a
metric which is deemed to be more scrupulous, and
ultimately that by which models should be ranked,
by Mitchell and Lapata (2008). The mean for each
model is on a [0, 1] scale, except for UpperBound
which is on the same [1, 7] scale the annotators used.
The p scores are on a [−1, 1] scale. It is assumed
that inter-annotator agreement provides the theoret-
ical maximum p for any model for this experiment.
The cosine measure of the verb vectors, ignoring the
noun, is taken to be the baseline (no composition).
Other Models The other models we compare
ours to are those evaluated by Mitchell and Lap-
ata (2008). We provide a selection of the results
</bodyText>
<page confidence="0.973993">
1400
</page>
<bodyText confidence="0.999954078947369">
from that paper for the worst (Add) and best5 (Mul-
tiply) performing models, as well as the previous
second-best performing model (Kintsch). The ad-
ditive and multiplicative models are simply applica-
tions of vector addition and component-wise multi-
plication. We invite the reader to consult (Mitchell
and Lapata, 2008) for the description of Kintsch’s
additive model and parametric choices.
Model Parameters To provide the most accurate
comparison with the existing multiplicative model,
and exploiting the aforementioned feature that the
categorical model can be built “on top of” existing
lexical distributional models, we used the parame-
ters described by Mitchell and Lapata (2008) to re-
produce the vectors evaluated in the original exper-
iment as our noun vectors. All vectors were built
from a lemmatised version of the BNC. The noun
basis was the 2000 most common context words,
basis weights were the probability of context words
given the target word divided by the overall proba-
bility of the context word. Intransitive verb function-
vectors were trained using the procedure presented
in §4. Since the dataset only contains intransitive
verbs and nouns, we used S = N. The cosine mea-
sure of vectors was used as a similarity metric.
First Experiment Results In Table 3 we present
the comparison of the selected models. Our categor-
ical model performs significantly better than the ex-
isting second-place (Kintsch) and obtains a ρ quasi-
identical to the multiplicative model, indicating sig-
nificant correlation with the annotator scores.
There is not a large difference between the mean
High score and mean Low score, but the distri-
bution in Figure 6 shows that our model makes a
non-negligible distinction between high similarity
phrases and low similarity phrases, despite the ab-
solute scores not being different by more than a few
percentiles.
</bodyText>
<footnote confidence="0.969331333333333">
5The multiplicative model presented here is what is quali-
fied as best in (Mitchell and Lapata, 2008). However, they also
present a slightly better performing (ρ = 0.19) model which
is a combination of their multiplicative model and a weighted
additive model. The difference in ρ is qualified as “not sta-
tistically significant” in the original paper, and furthermore the
mixed model requires parametric optimisation hence was not
evaluated against the entire test set. For these reasons, we chose
not to include it in the comparison.
</footnote>
<table confidence="0.999128">
Model High Low ρ
Baseline 0.27 0.26 0.08
Add 0.59 0.59 0.04
Kintsch 0.47 0.45 0.09
Multiply 0.42 0.28 0.17
Categorical 0.84 0.79 0.17
UpperBound 4.94 3.25 0.40
</table>
<tableCaption confidence="0.98786825">
Table 3: Selected model means for High and Low similar-
ity items and correlation coefficients with human judge-
ments, first experiment (Mitchell and Lapata, 2008). p &lt;
0.05 for each ρ.
</tableCaption>
<figure confidence="0.9971715">
1.0
0.9
0.8
0.7
0.6
0.5
0.4
High Low
</figure>
<figureCaption confidence="0.997541333333333">
Figure 6: Distribution of predicted similarities for the cat-
egorical distributional model on High and Low similarity
items.
</figureCaption>
<bodyText confidence="0.9978605">
Second Dataset Description The second dataset6,
developed by the authors, follows the format of the
(Mitchell and Lapata, 2008) dataset used for the first
experiment, with the exception that the target and
landmark verbs are transitive, and an object noun
is provided in addition to the subject noun, hence
forming a small transitive sentence. The dataset
comprises 200 entries consisting of sentence pairs
(hence a total of 400 sentences) constructed by fol-
lowing the procedure outlined in §4 of (Mitchell and
Lapata, 2008), using transitive verbs from CELEX7.
For examples of these sentences, see Table 4. The
dataset was split into four sections of 100 entries
each, with guaranteed 50% exclusive overlap with
</bodyText>
<footnote confidence="0.999921666666667">
6http://www.cs.ox.ac.uk/activities/CompD
istMeaning/GS2011data.txt
7http://celex.mpi.nl/
</footnote>
<page confidence="0.993239">
1401
</page>
<bodyText confidence="0.999375571428571">
exactly two other datasets. Each section was given
to a group of evaluators, with a total of 25, who were
asked to form simple transitive sentence pairs from
the verbs, subject and object provided in each entry;
for instance ‘the table showed the result’ from ‘table
show result’. The evaluators were then asked to rate
the semantic similarity of each verb pair within the
context of those sentences, and offer a score between
1 and 7 for each entry. Each entry was given an arbi-
trary classification of HIGH or LOW by the authors,
for the purpose of calculating mean high/low scores
for each model. For example, the first two pairs in
table 4 were classified as HIGH, whereas the second
two pairs as LOW.
</bodyText>
<tableCaption confidence="0.813787714285714">
Sentence 1 Sentence 2
table show result table express result
map show location map picture location
table show result table picture result
map show location map express location
Table 4: Example entries from the transitive dataset with-
out annotator score, second experiment.
</tableCaption>
<bodyText confidence="0.949596678571429">
Evaluation Methodology The evaluation
methodology for the second experiment was
identical to that of the first, as are the scales for
means and scores. Here also, Spearman’s ρ is
deemed a more rigorous way of determining how
well a model tracks difference in meaning. This is
both because of the imprecise nature of the classifi-
cation of verb pairs as HIGH or LOW; and since the
objective similarity scores produced by a model that
distinguishes sentences of different meaning from
those of similar meaning can be renormalised in
practice. Therefore the delta between HIGH means
and LOW mean cannot serve as a definite indication
of the practical applicability (or lack thereof) of
semantic models; the means are provided just to aid
comparison with the results of the first experiment.
Model Parameters As in the first experiment, the
lexical vectors from (Mitchell and Lapata, 2008)
were used for the other models evaluated (additive,
multiplicative and baseline)8 and for the noun vec-
8Kintsch was not evaluated as it required optimising model
parameters against a held-out segment of the test set, and we
could not replicate the methodology of Mitchell and Lapata
tors of our categorical model. Transitive verb vec-
tors were trained as described in §4 with S = N⊗N.
Second Experiment Results The results for the
models evaluated against the second dataset are pre-
sented in Table 5.
</bodyText>
<table confidence="0.9995055">
Model High Low ρ
Baseline 0.47 0.44 0.16
Add 0.90 0.90 0.05
Multiply 0.67 0.59 0.17
Categorical 0.73 0.72 0.21
UpperBound 4.80 2.49 0.62
</table>
<tableCaption confidence="0.866518666666667">
Table 5: Selected model means for High and Low similar-
ity items and correlation coefficients with human judge-
ments, second experiment. p &lt; 0.05 for each ρ.
</tableCaption>
<bodyText confidence="0.9999872">
We observe a significant (according to p &lt; 0.0.5)
improvement in the alignment of our categorical
model with the human judgements, from 0.17 to
0.21. The additive model continues to make lit-
tle distinction between senses of the verb during
composition, and the multiplicative model’s align-
ment does not change, but becomes statistically in-
distinguishable from the non-compositional baseline
model.
Once again we note that the high-low means are
not very indicative of model performance, as the dif-
ference between high mean and the low mean of the
categorical model is much smaller than that of the
both the baseline model and multiplicative model,
despite better alignment with annotator judgements.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.959856583333333">
In this paper, we described an implementation of the
categorical model of meaning (Coecke et al., 2010),
which combines the formal logical and the empiri-
cal distributional frameworks into a unified seman-
tic model. The implementation is based on build-
ing matrices for words with relational types (ad-
jectives, verbs), and vectors for words with atomic
types (nouns), based on data from the BNC. We
then show how to apply verbs to their subject/object,
in order to compute the meaning of intransitive and
transitive sentences.
(2008) with full confidence.
</bodyText>
<page confidence="0.991699">
1402
</page>
<bodyText confidence="0.999995">
Other work uses matrices to model meaning (Ba-
roni and Zamparelli, 2010; Guevara, 2010), but only
for adjective-noun phrases. Our approach easily ap-
plies to such compositions, as well as to sentences
containing combinations of adjectives, nouns, verbs,
and adverbs. The other key difference is that they
learn their matrices in a top-down fashion, i.e. by re-
gression from the composite adjective-noun context
vectors, whereas our model is bottom-up: it learns
sentence/phrase meaning compositionally from the
vectors of the compartments of the composites. Fi-
nally, very similar functions, for example a verb with
argument alternations such as ‘break’ in ‘Y breaks’
and ‘X breaks Y’, are not treated as unrelated. The
matrix of the intransitive ‘break’ uses the corpus-
observed information about the subject of break, in-
cluding that of ‘Y’, similarly the matrix of the tran-
sitive ‘break’ uses information about its subject and
object, including that of ‘X’ and ‘Y’. We leave a
thorough study of these phenomena, which fall un-
der providing a modular representation of passive-
active similarities, to future work.
We evaluated our model in two ways: first against
the word disambiguation task of Mitchell and Lap-
ata (2008) for intransitive verbs, and then against a
similar new experiment for transitive verbs, which
we developed.
Our findings in the first experiment show that
the categorical method performs on par with the
leading existing approaches. This should not sur-
prise us given that the context is so small and our
method becomes similar to the multiplicative model
of Mitchell and Lapata (2008). However, our ap-
proach is sensitive to grammatical structure, lead-
ing us to develop a second experiment taking this
into account and differentiating it from models with
commutative composition operations.
The second experiment’s results deliver the ex-
pected qualitative difference between models, with
our categorical model outperforming the others and
showing an increase in alignment with human judge-
ments in correlation with the increase in sentence
complexity. We use this second evaluation princi-
pally to show that there is a strong case for the devel-
opment of more complex experiments measuring not
only the disambiguating qualities of compositional
models, but also their syntactic sensitivity, which is
not directly measured in the existing experiments.
These results show that the high level categori-
cal distributional model, uniting empirical data with
logical form, can be implemented just like any other
concrete model. Furthermore it shows better results
in experiments involving higher syntactic complex-
ity. This is just the tip of the iceberg: the mathe-
matics underlying the implementation ensures that
it uniformly scales to larger, more complicated sen-
tences and enables it to compare synonymity of sen-
tences that are of different grammatical structure.
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.99993019047619">
Treatment of function words such as ‘that’, ‘who’,
as well as logical words such as quantifiers and con-
junctives are left to future work. This will build
alongside the general guidelines of Coecke et al.
(2010) and concrete insights from the work of Wid-
dows (2005). It is not yet entirely clear how ex-
isting set-theoretic approaches, for example that of
discourse representation and generalised quantifiers,
apply to our setting. Preliminary work on integration
of the two has been presented by Preller (2007) and
more recently also by Preller and Sadrzadeh ( 2009).
As mentioned by one of the reviewers, our pre-
group approach to grammar flattens the sentence
representation, in that the verb is applied to its sub-
ject and object at the same time; whereas in other
approaches such as CCG, it is first applied to the
object to produce a verb phrase, then applied to the
subject to produce the sentence. The advantages and
disadvantages of this method and comparisons with
other systems, in particular CCG, constitutes ongo-
ing work.
</bodyText>
<sectionHeader confidence="0.992806" genericHeader="acknowledgments">
9 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998464">
We wish to thank P. Blunsom, S. Clark, B. Coecke,
S. Pulman, and the anonymous EMNLP review-
ers for discussions and comments. Support from
EPSRC grant EP/F042728/1 is gratefully acknowl-
edged by M. Sadrzadeh.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99959225">
H. Alshawi (ed). 1992. The Core Language Engine.
MIT Press.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices. Proceedings of Conference
</reference>
<page confidence="0.565612">
1403
</page>
<reference confidence="0.999802794871795">
on Empirical Methods in Natural Language Processing
(EMNLP).
S. Clark and S. Pulman. 2007. Combining Symbolic
and Distributional Models of Meaning. Proceedings
of AAAI Spring Symposium on Quantum Interaction.
AAAI Press.
B. Coecke, and E. Paquette. 2011. Categories for the
Practicing Physicist. New Structures for Physics, 167-
271. B. Coecke (ed.). Lecture Notes in Physics 813.
Springer.
B. Coecke, M. Sadrzadeh and S. Clark. 2010. Mathemat-
ical Foundations for Distributed Compositional Model
of Meaning. Lambek Festschrift. Linguistic Analysis
36, 345–384. J. van Benthem, M. Moortgat and W.
Buszkowski (eds.).
J. Curran. 2004. From Distributional to Semantic Simi-
larity. PhD Thesis, University of Edinburgh.
K. Erk and S. Pad´o. 2004. A Structured Vector Space
Model for Word Meaning in Context. Proceedings
of Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 897–906.
G. Frege 1892. ¨Uber Sinn und Bedeutung. Zeitschrift
f¨ur Philosophie und philosophische Kritik 100.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis.
E. Grefenstette, M. Sadrzadeh, S. Clark, B. Coecke,
S. Pulman. 2011. Concrete Compositional Sentence
Spaces for a Compositional Distributional Model of
Meaning. International Conference on Computational
Semantics (IWCS’11). Oxford.
G. Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer.
E. Guevara. 2010. A Regression Model of Adjective-
Noun Compositionality in Distributional Semantics.
Proceedings of the ACL GEMS Workshop.
Z. S. Harris. 1966. A Cycling Cancellation-Automaton
for Sentence Well-Formedness. International Compu-
tation Centre Bulletin 5, 69–94.
R. Hudson. 1984. Word Grammar. Blackwell.
J. Lambek. 2008. From Word to Sentence. Polimetrica,
Milan.
T. Landauer, and S. Dumais. 2008. A solution to Platos
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological review.
C. D. Manning, P. Raghavan, and H. Sch¨utze. 2008. In-
troduction to information retrieval. Cambridge Uni-
versity Press.
J. Mitchell and M. Lapata. 2008. Vector-based mod-
els of semantic composition. Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, 236–244.
R. Montague. 1974. English as a formal language. For-
mal Philosophy, 189–223.
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT).
A. Preller. Towards Discourse Representation via Pre-
group Grammars. Journal of Logic Language Infor-
mation 16 173–194.
A. Preller and M. Sadrzadeh. Semantic Vector Mod-
els and Functional Models for Pregroup Grammars.
Journal of Logic Language Information. DOI:
10.1007/s10849-011-9132-2. to appear.
J. Saffron, E. Newport, R. Asling. 1999. Word Segmenta-
tion: The role of distributional cues. Journal of Mem-
ory and Language 35, 606–621.
H. Schuetze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics 24, 97–123.
P. Smolensky. 1990. Tensor product variable binding
and the representation of symbolic structures in con-
nectionist systems. Computational Linguistics 46, 1–
2, 159–216.
M. Steedman. 2000. The Syntactic Process. MIT Press.
D. Widdows. 2005. Geometry and Meaning. University
of Chicago Press.
L. Wittgenstein. 1953. Philosophical Investigations.
Blackwell.
</reference>
<page confidence="0.996486">
1404
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.086474">
<title confidence="0.9953875">Experimental Support for a Categorical Distributional Model of Meaning</title>
<author confidence="0.983789">Edward</author>
<affiliation confidence="0.9991245">University of Department of Computer</affiliation>
<address confidence="0.5823575">Wolfson Building, Parks Oxford OX1 3QD,</address>
<email confidence="0.997411">edward.grefenstette@cs.ox.ac.uk</email>
<author confidence="0.567375">Mehrnoosh</author>
<affiliation confidence="0.76378975">University of Department of Computer Wolfson Building, Parks Oxford OX1 3QD,</affiliation>
<email confidence="0.996837">mehrs@cs.ox.ac.uk</email>
<abstract confidence="0.999752789473684">Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>The Core Language Engine.</title>
<date>1992</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6452" citStr="Alshawi, 1992" startWordPosition="978" endWordPosition="979">retation by substituting semantic representation for their grammatical constituents and applying β-reduction where needed. Such a derivation is shown in Figure 2. Syntactic Analysis Semantic Interpretation |V P|(|NP|) |cats|, |milk|, ... |Vt|(|NP|) Ayx.|like|(x, y), ... Figure 1: A simple model of formal semantics. |like|(|cats|, |milk|) |cats |Ax.|like|(x, |milk|) |milk |Ayx.|like|(x, y) Figure 2: A parse tree showing a semantic derivation. This methodology is used to translate sentences of natural language into logical formulae, then use computer-aided automation tools to reason about them (Alshawi, 1992). One major drawback is that the result of such analysis can only deal with truth or falsity as the meaning of a sentence, and says nothing about the closeness in meaning or topic of expressions beyond their truth-conditions and what models satisfy them, hence do not perform well on language tasks such as search. Furthermore, an underlying domain of objects and a valuation function must be provided, as with any logic, leaving open the question of how we might learn the meaning of language using such a model, rather than just use it. Distributional Models Distributional models of semantics, on </context>
</contexts>
<marker>Alshawi, 1992</marker>
<rawString>H. Alshawi (ed). 1992. The Core Language Engine. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices.</title>
<date>2010</date>
<booktitle>Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="34620" citStr="Baroni and Zamparelli, 2010" startWordPosition="5936" endWordPosition="5940">n this paper, we described an implementation of the categorical model of meaning (Coecke et al., 2010), which combines the formal logical and the empirical distributional frameworks into a unified semantic model. The implementation is based on building matrices for words with relational types (adjectives, verbs), and vectors for words with atomic types (nouns), based on data from the BNC. We then show how to apply verbs to their subject/object, in order to compute the meaning of intransitive and transitive sentences. (2008) with full confidence. 1402 Other work uses matrices to model meaning (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases. Our approach easily applies to such compositions, as well as to sentences containing combinations of adjectives, nouns, verbs, and adverbs. The other key difference is that they learn their matrices in a top-down fashion, i.e. by regression from the composite adjective-noun context vectors, whereas our model is bottom-up: it learns sentence/phrase meaning compositionally from the vectors of the compartments of the composites. Finally, very similar functions, for example a verb with argument alternations such as ‘break’ in ‘Y breaks’ and ‘X</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>M. Baroni and R. Zamparelli. 2010. Nouns are vectors, adjectives are matrices. Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>S Pulman</author>
</authors>
<title>Combining Symbolic and Distributional Models of Meaning.</title>
<date>2007</date>
<booktitle>Proceedings of AAAI Spring Symposium on Quantum Interaction.</booktitle>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="9541" citStr="Clark and Pulman (2007)" startWordPosition="1508" endWordPosition="1511">ases and sentences in the same efficient way that they do for words. Common operations discussed in (Mitchell and Lapata, 2008) such as vector addition (+) and componentwise multiplication (O, cf. §4 for details) are commutative, hence if vw−→ = →− v + →−w or →−v O →−w , then vw = −→ −→ wv, leading to unwelcome equalities such as −−−−−−−−−−−−−→ −−−−−−−−−−−−−→ the dog bit the man = the man bit the dog Non-commutative operations, such as the Kronecker product (cf. §4 for definition) can take word-order into account (Smolensky, 1990) or even some more complex syntactic relations, as described in Clark and Pulman (2007). However, the dimensionality of sentence vectors produced in this manner differs for sentences of different length, barring all sentences from being compared in the same vector space, and growing exponentially with sentence length hence quickly becoming computationally intractable. 3 A Hybrid Logico-Distributional Model Whereas semantic compositional mechanisms for set-theoretic constructions are well understood, there are no obvious corresponding methods for vector spaces. To solve this problem, Coecke et al. −−→ milk. The logical recipe tells us to apply the meaning of the verb to the meani</context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>S. Clark and S. Pulman. 2007. Combining Symbolic and Distributional Models of Meaning. Proceedings of AAAI Spring Symposium on Quantum Interaction. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>E Paquette</author>
</authors>
<title>Categories for the Practicing Physicist. New Structures</title>
<date>2011</date>
<booktitle>Lecture Notes in Physics 813.</booktitle>
<pages>167--271</pages>
<editor>B. Coecke (ed.).</editor>
<publisher>for Physics,</publisher>
<contexts>
<context position="11363" citStr="Coecke and Paquette (2011)" startWordPosition="1799" endWordPosition="1803">ors as input and sentence vectors as output. Hence, at least theoretically, one should be able to build sentence vectors and compare their synonymity in exactly the same way as one measures word synonymity. Pregroup Grammars The aforementioned linear maps turn out to be the grammatical reductions of a type-logic called a Lambek pregroup grammar (Lambek, 2008)2. Pregroups and vector spaces share the same high level mathematical structure, referred to as a compact closed category, for a proof and details of this claim see Coecke et al. (2010); for a friendly introduction to category theory, see Coecke and Paquette (2011). One consequence of this parity is that the grammatical reductions of a pregroup grammar can be directly transformed into linear maps that act on vectors. In a nutshell, pregroup types are either atomic or compound. Atomic types can be simple (e.g. n for noun phrases, s for statements) or left/right superscripted—referred to as adjoint types (e.g. nr and nl). An example of a compound type is that of a verb nrsnl. The superscripted types express that the verb is a relation with two arguments of type n, 2The usage of pregroup types is not essential, the types of any other logic, for instance CC</context>
</contexts>
<marker>Coecke, Paquette, 2011</marker>
<rawString>B. Coecke, and E. Paquette. 2011. Categories for the Practicing Physicist. New Structures for Physics, 167-271. B. Coecke (ed.). Lecture Notes in Physics 813. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
</authors>
<date>2010</date>
<booktitle>Mathematical Foundations for Distributed Compositional Model of Meaning. Lambek Festschrift. Linguistic Analysis 36,</booktitle>
<pages>345--384</pages>
<editor>J. van Benthem, M. Moortgat and W. Buszkowski (eds.).</editor>
<contexts>
<context position="3490" citStr="Coecke et al. (2010)" startWordPosition="520" endWordPosition="523"> and robust logical formalisms, hence offering a scalable theory of meaning which can be used to reason inferentially. The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al., 2008). This two-sortedness of defining properties of meaning: ‘logical form’ versus ‘contextual use’, has left the quest for ‘what is the foundational structure of meaning?’ even more of a challenge. Recently, Coecke et al. (2010) used high level cross-disciplinary techniques from logic, category 1394 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics theory, and physics to bring the above two approaches together. They developed a unified mathematical framework whereby a sentence vector is by definition a function of the Kronecker product of its word vectors. A concrete instantiation of this theory was exemplified on a toy hand crafted corpus by Grefenstette et al. (2011). In</context>
<context position="11283" citStr="Coecke et al. (2010)" startWordPosition="1787" endWordPosition="1790">ammatical structure of the sentence. This is the linear map with word vectors as input and sentence vectors as output. Hence, at least theoretically, one should be able to build sentence vectors and compare their synonymity in exactly the same way as one measures word synonymity. Pregroup Grammars The aforementioned linear maps turn out to be the grammatical reductions of a type-logic called a Lambek pregroup grammar (Lambek, 2008)2. Pregroups and vector spaces share the same high level mathematical structure, referred to as a compact closed category, for a proof and details of this claim see Coecke et al. (2010); for a friendly introduction to category theory, see Coecke and Paquette (2011). One consequence of this parity is that the grammatical reductions of a pregroup grammar can be directly transformed into linear maps that act on vectors. In a nutshell, pregroup types are either atomic or compound. Atomic types can be simple (e.g. n for noun phrases, s for statements) or left/right superscripted—referred to as adjoint types (e.g. nr and nl). An example of a compound type is that of a verb nrsnl. The superscripted types express that the verb is a relation with two arguments of type n, 2The usage o</context>
<context position="13134" citStr="Coecke et al. (2010)" startWordPosition="2121" endWordPosition="2124">nce has types as shown in Figure 3. Each type n cancels out with its right adjoint nr from the right and its left adjoint nl from the left; mathematically speaking these mean3 nln &lt; 1 and nnr &lt; 1 Here 1 is the unit of concatenation: 1n = n1 = n. The corresponding grammatical reduction of a transitive sentence is nnrsnl &lt; 1s1 = s. Each such reduction can be depicted as a wire diagram. The diagram of a transitive sentence is shown in Figure 3. Cats like milk. n nr s nl n Figure 3: The pregroup types and reduction diagram for a transitive sentence. Syntax-guided Semantic Composition According to Coecke et al. (2010) and based on a general completeness theorem between compact categories, wire diagrams, and vector spaces, the meaning of sentences can be canonically reduced to linear algebraic formulae. The following is the meaning vector of our transitive sentence: ++++++++- cats like = (f ) (cats ®1 like ® �) (I) Here f is the linear map that encodes the grammatical structure. The categorical morphism corresponding to it is denoted by the tensor product of 3 components: eV ®1S ®eW, where V and W are subject and object spaces, S is the sentence space, the E’s are the cups, and 1S is the straight line in th</context>
<context position="34095" citStr="Coecke et al., 2010" startWordPosition="5852" endWordPosition="5855">stinction between senses of the verb during composition, and the multiplicative model’s alignment does not change, but becomes statistically indistinguishable from the non-compositional baseline model. Once again we note that the high-low means are not very indicative of model performance, as the difference between high mean and the low mean of the categorical model is much smaller than that of the both the baseline model and multiplicative model, despite better alignment with annotator judgements. 7 Discussion In this paper, we described an implementation of the categorical model of meaning (Coecke et al., 2010), which combines the formal logical and the empirical distributional frameworks into a unified semantic model. The implementation is based on building matrices for words with relational types (adjectives, verbs), and vectors for words with atomic types (nouns), based on data from the BNC. We then show how to apply verbs to their subject/object, in order to compute the meaning of intransitive and transitive sentences. (2008) with full confidence. 1402 Other work uses matrices to model meaning (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases. Our approach easily </context>
<context position="37646" citStr="Coecke et al. (2010)" startWordPosition="6411" endWordPosition="6414">, can be implemented just like any other concrete model. Furthermore it shows better results in experiments involving higher syntactic complexity. This is just the tip of the iceberg: the mathematics underlying the implementation ensures that it uniformly scales to larger, more complicated sentences and enables it to compare synonymity of sentences that are of different grammatical structure. 8 Future Work Treatment of function words such as ‘that’, ‘who’, as well as logical words such as quantifiers and conjunctives are left to future work. This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005). It is not yet entirely clear how existing set-theoretic approaches, for example that of discourse representation and generalised quantifiers, apply to our setting. Preliminary work on integration of the two has been presented by Preller (2007) and more recently also by Preller and Sadrzadeh ( 2009). As mentioned by one of the reviewers, our pregroup approach to grammar flattens the sentence representation, in that the verb is applied to its subject and object at the same time; whereas in other approaches such as CCG, it is first applied t</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>B. Coecke, M. Sadrzadeh and S. Clark. 2010. Mathematical Foundations for Distributed Compositional Model of Meaning. Lambek Festschrift. Linguistic Analysis 36, 345–384. J. van Benthem, M. Moortgat and W. Buszkowski (eds.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>PhD Thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="3128" citStr="Curran, 2004" startWordPosition="469" endWordPosition="470">gic, mainly Frege’s principle that the meaning of a sentence is a function of the meaning of its parts (Frege, 1892). Distributional models are more recent and can be related to Wittgenstein’s later philosophy of ‘meaning is use’, whereby meanings of words can be determined from their context (Wittgenstein, 1953). The logical models relate to well known and robust logical formalisms, hence offering a scalable theory of meaning which can be used to reason inferentially. The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al., 2008). This two-sortedness of defining properties of meaning: ‘logical form’ versus ‘contextual use’, has left the quest for ‘what is the foundational structure of meaning?’ even more of a challenge. Recently, Coecke et al. (2010) used high level cross-disciplinary techniques from logic, category 1394 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Associatio</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>J. Curran. 2004. From Distributional to Semantic Similarity. PhD Thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Pad´o</author>
</authors>
<title>A Structured Vector Space Model for Word Meaning in Context.</title>
<date>2004</date>
<booktitle>Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>897--906</pages>
<marker>Erk, Pad´o, 2004</marker>
<rawString>K. Erk and S. Pad´o. 2004. A Structured Vector Space Model for Word Meaning in Context. Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 897–906.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Frege</author>
</authors>
<title>1892. ¨Uber Sinn und Bedeutung. Zeitschrift f¨ur Philosophie und philosophische Kritik 100.</title>
<marker>Frege, </marker>
<rawString>G. Frege 1892. ¨Uber Sinn und Bedeutung. Zeitschrift f¨ur Philosophie und philosophische Kritik 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955. Studies in Linguistic Analysis.</title>
<date>1957</date>
<contexts>
<context position="2367" citStr="Firth, 1957" startWordPosition="354" endWordPosition="355">ry (Manning et al., 2008). However, such models fail to shine when it comes to processing the semantics of phrases and sentences. Discovering the process of meaning assignment in natural language is among the most challenging and foundational questions of linguistics and computer science. The findings thereof will increase our understanding of cognition and intelligence and shall assist in applications to automating language-related tasks such as document search. Compositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question. Compositional formal semantic models stem from classical ideas from mathematical logic, mainly Frege’s principle that the meaning of a sentence is a function of the meaning of its parts (Frege, 1892). Distributional models are more recent and can be related to Wittgenstein’s later philosophy of ‘meaning is use’, whereby meanings of words can be determined from their context (Wittgenstein, 1953). The logical models relate to well known and robust logical formalisms, hence offering a scalable theory of meaning which can be used to </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth. 1957. A synopsis of linguistic theory 1930-1955. Studies in Linguistic Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
<author>B Coecke</author>
<author>S Pulman</author>
</authors>
<date>2011</date>
<booktitle>Concrete Compositional Sentence Spaces for a Compositional Distributional Model of Meaning. International Conference on Computational Semantics (IWCS’11).</booktitle>
<location>Oxford.</location>
<contexts>
<context position="4086" citStr="Grefenstette et al. (2011)" startWordPosition="609" endWordPosition="612">Recently, Coecke et al. (2010) used high level cross-disciplinary techniques from logic, category 1394 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics theory, and physics to bring the above two approaches together. They developed a unified mathematical framework whereby a sentence vector is by definition a function of the Kronecker product of its word vectors. A concrete instantiation of this theory was exemplified on a toy hand crafted corpus by Grefenstette et al. (2011). In this paper we implement it by training the model over the entire BNC. The highlight of our implementation is that words with relational types, such as verbs, adjectives, and adverbs are matrices that act on their arguments. We provide a general algorithm for building (or indeed learning) these matrices from the corpus. The implementation is evaluated against the task provided by Mitchell and Lapata (2008) for disambiguating intransitive verbs, as well as a similar new experiment for transitive verbs. Our model improves on the best method evaluated in Mitchell and Lapata (2008) and offers </context>
<context position="24570" citStr="Grefenstette et al., 2011" startWordPosition="4300" endWordPosition="4303">tiplications and summations. Similar computations yield meanings of sentences with adjectives and adverbs. For instance the meaning of a transitive sentence with a modified subject and a modified verb we have −−−−−−−−−−−−−→ adj sub verb obj adv = (adv O verb) O ( (adj O sub) ® o j) After building vectors for sentences, we can compare their meaning and measure their degree of synonymy by taking their cosine measure. 6 Evaluation Evaluating such a framework is no easy task. What to evaluate depends heavily on what sort of application a practical instantiation of the model is geared towards. In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). In this section, we briefly describe the evaluation of our model against this dataset. Following this, we present a new evaluation task extending the experimental methodology of Mitchell and Lapata (2008) to transitive verb-centric sentences, and compare our model to those discussed by Mitchell and Lapata (2008) within this new ex</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, Clark, Coecke, Pulman, 2011</marker>
<rawString>E. Grefenstette, M. Sadrzadeh, S. Clark, B. Coecke, S. Pulman. 2011. Concrete Compositional Sentence Spaces for a Compositional Distributional Model of Meaning. International Conference on Computational Semantics (IWCS’11). Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer.</publisher>
<contexts>
<context position="3113" citStr="Grefenstette, 1994" startWordPosition="467" endWordPosition="468">from mathematical logic, mainly Frege’s principle that the meaning of a sentence is a function of the meaning of its parts (Frege, 1892). Distributional models are more recent and can be related to Wittgenstein’s later philosophy of ‘meaning is use’, whereby meanings of words can be determined from their context (Wittgenstein, 1953). The logical models relate to well known and robust logical formalisms, hence offering a scalable theory of meaning which can be used to reason inferentially. The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al., 2008). This two-sortedness of defining properties of meaning: ‘logical form’ versus ‘contextual use’, has left the quest for ‘what is the foundational structure of meaning?’ even more of a challenge. Recently, Coecke et al. (2010) used high level cross-disciplinary techniques from logic, category 1394 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland, UK, July 27–31, 2011. c�</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Guevara</author>
</authors>
<title>A Regression Model of AdjectiveNoun Compositionality in Distributional Semantics.</title>
<date>2010</date>
<booktitle>Proceedings of the ACL GEMS Workshop.</booktitle>
<contexts>
<context position="34636" citStr="Guevara, 2010" startWordPosition="5941" endWordPosition="5942"> implementation of the categorical model of meaning (Coecke et al., 2010), which combines the formal logical and the empirical distributional frameworks into a unified semantic model. The implementation is based on building matrices for words with relational types (adjectives, verbs), and vectors for words with atomic types (nouns), based on data from the BNC. We then show how to apply verbs to their subject/object, in order to compute the meaning of intransitive and transitive sentences. (2008) with full confidence. 1402 Other work uses matrices to model meaning (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases. Our approach easily applies to such compositions, as well as to sentences containing combinations of adjectives, nouns, verbs, and adverbs. The other key difference is that they learn their matrices in a top-down fashion, i.e. by regression from the composite adjective-noun context vectors, whereas our model is bottom-up: it learns sentence/phrase meaning compositionally from the vectors of the compartments of the composites. Finally, very similar functions, for example a verb with argument alternations such as ‘break’ in ‘Y breaks’ and ‘X breaks Y’, are </context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>E. Guevara. 2010. A Regression Model of AdjectiveNoun Compositionality in Distributional Semantics. Proceedings of the ACL GEMS Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>A Cycling Cancellation-Automaton for Sentence Well-Formedness.</title>
<date>1966</date>
<journal>International Computation Centre Bulletin</journal>
<volume>5</volume>
<pages>69--94</pages>
<marker>Harris, 1966</marker>
<rawString>Z. S. Harris. 1966. A Cycling Cancellation-Automaton for Sentence Well-Formedness. International Computation Centre Bulletin 5, 69–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hudson</author>
</authors>
<date>1984</date>
<publisher>Word Grammar. Blackwell.</publisher>
<marker>Hudson, 1984</marker>
<rawString>R. Hudson. 1984. Word Grammar. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>From Word to Sentence.</title>
<date>2008</date>
<location>Polimetrica, Milan.</location>
<contexts>
<context position="2291" citStr="Lambek, 2008" startWordPosition="343" endWordPosition="344">of lexical semantics to retrieve pages with terms related to those in the query (Manning et al., 2008). However, such models fail to shine when it comes to processing the semantics of phrases and sentences. Discovering the process of meaning assignment in natural language is among the most challenging and foundational questions of linguistics and computer science. The findings thereof will increase our understanding of cognition and intelligence and shall assist in applications to automating language-related tasks such as document search. Compositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question. Compositional formal semantic models stem from classical ideas from mathematical logic, mainly Frege’s principle that the meaning of a sentence is a function of the meaning of its parts (Frege, 1892). Distributional models are more recent and can be related to Wittgenstein’s later philosophy of ‘meaning is use’, whereby meanings of words can be determined from their context (Wittgenstein, 1953). The logical models relate to well known and robust logical f</context>
<context position="11098" citStr="Lambek, 2008" startWordPosition="1757" endWordPosition="1758"> instance as matrices. The general information as to which words should be matrices and which words atomic vectors is in fact encoded in the type-logical representation of the grammatical structure of the sentence. This is the linear map with word vectors as input and sentence vectors as output. Hence, at least theoretically, one should be able to build sentence vectors and compare their synonymity in exactly the same way as one measures word synonymity. Pregroup Grammars The aforementioned linear maps turn out to be the grammatical reductions of a type-logic called a Lambek pregroup grammar (Lambek, 2008)2. Pregroups and vector spaces share the same high level mathematical structure, referred to as a compact closed category, for a proof and details of this claim see Coecke et al. (2010); for a friendly introduction to category theory, see Coecke and Paquette (2011). One consequence of this parity is that the grammatical reductions of a pregroup grammar can be directly transformed into linear maps that act on vectors. In a nutshell, pregroup types are either atomic or compound. Atomic types can be simple (e.g. n for noun phrases, s for statements) or left/right superscripted—referred to as adjo</context>
</contexts>
<marker>Lambek, 2008</marker>
<rawString>J. Lambek. 2008. From Word to Sentence. Polimetrica, Milan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>2008</date>
<note>Psychological review.</note>
<marker>Landauer, Dumais, 2008</marker>
<rawString>T. Landauer, and S. Dumais. 2008. A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Sch¨utze</author>
</authors>
<title>Introduction to information retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>C. D. Manning, P. Raghavan, and H. Sch¨utze. 2008. Introduction to information retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="862" citStr="Mitchell and Lapata (2008)" startWordPosition="117" endWordPosition="120"> Mehrnoosh Sadrzadeh University of Oxford Department of Computer Science Wolfson Building, Parks Road Oxford OX1 3QD, UK mehrs@cs.ox.ac.uk Abstract Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model. 1 Introduction As competent language speakers, we humans can almost trivially make sense of sentences we’ve never seen or heard before. We are naturally good at understanding ambiguous words given a context, and forming the meaning of a sentence from the meaning of its parts. </context>
<context position="4499" citStr="Mitchell and Lapata (2008)" startWordPosition="676" endWordPosition="679">hereby a sentence vector is by definition a function of the Kronecker product of its word vectors. A concrete instantiation of this theory was exemplified on a toy hand crafted corpus by Grefenstette et al. (2011). In this paper we implement it by training the model over the entire BNC. The highlight of our implementation is that words with relational types, such as verbs, adjectives, and adverbs are matrices that act on their arguments. We provide a general algorithm for building (or indeed learning) these matrices from the corpus. The implementation is evaluated against the task provided by Mitchell and Lapata (2008) for disambiguating intransitive verbs, as well as a similar new experiment for transitive verbs. Our model improves on the best method evaluated in Mitchell and Lapata (2008) and offers promising results for the transitive case, demonstrating its scalability in comparison to that of other models. But we still feel there is need for a different class of experiments to showcase merits of compositionality in a statistically significant manner. Our work shows that the categorical compositional distributional model of meaning permits a practical implementation and that this opens the way to the pr</context>
<context position="9045" citStr="Mitchell and Lapata, 2008" startWordPosition="1421" endWordPosition="1424">i ord, c2 ord , cnord) are also denoted by sums lof such weight/basis vector pairs: −−→ word = i Learning a semantic vector is just learning its basis weights from the corpus. This setting offers geometric means to reason about semantic similarity (e.g. via cosine measure or k-means clustering), as discussed in Widdows (2005). The principal drawback of such models is their non-compositional nature: they ignore grammatical structure and logical words, and hence cannot compute the meanings of phrases and sentences in the same efficient way that they do for words. Common operations discussed in (Mitchell and Lapata, 2008) such as vector addition (+) and componentwise multiplication (O, cf. §4 for details) are commutative, hence if vw−→ = →− v + →−w or →−v O →−w , then vw = −→ −→ wv, leading to unwelcome equalities such as −−−−−−−−−−−−−→ −−−−−−−−−−−−−→ the dog bit the man = the man bit the dog Non-commutative operations, such as the Kronecker product (cf. §4 for definition) can take word-order into account (Smolensky, 1990) or even some more complex syntactic relations, as described in Clark and Pulman (2007). However, the dimensionality of sentence vectors produced in this manner differs for sentences of diffe</context>
<context position="17319" citStr="Mitchell and Lapata (2008)" startWordPosition="2896" endWordPosition="2899">sentence ‘the hound hunted the wild cat’, cxy will be 0, restricting us to only assign meaning to sentences that have directly appeared in the corpus. We propose to, instead, use a level of abstraction by taking words such as verbs to be distributions over the semantic information in the vectors of their context words, rather than over the context words themselves. Start with an r-dimensional vector space N with basis {−→n i}i, in which meaning vectors of atomic words, such as nouns, live. The basis vectors of N are in principle all the words from the corpus, however in practice and following Mitchell and Lapata (2008) we had to restrict these to a subset of the most occurring words. These basis vectors are not restricted to nouns: they can as well be verbs, adjectives, and adverbs, so that we can define the meaning of a noun in all possible contexts—as is usual in context-based models—and not only in the context of other nouns. Note that basis words with relational types are treated as pure lexical items rather than as semantic objects represented as matrices. In short, we count how many times a noun has occurred close to words of other syntactic types such as ‘elect’ and ‘scientific’, rather than count ho</context>
<context position="24836" citStr="Mitchell and Lapata (2008)" startWordPosition="4342" endWordPosition="4345">dj O sub) ® o j) After building vectors for sentences, we can compare their meaning and measure their degree of synonymy by taking their cosine measure. 6 Evaluation Evaluating such a framework is no easy task. What to evaluate depends heavily on what sort of application a practical instantiation of the model is geared towards. In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). In this section, we briefly describe the evaluation of our model against this dataset. Following this, we present a new evaluation task extending the experimental methodology of Mitchell and Lapata (2008) to transitive verb-centric sentences, and compare our model to those discussed by Mitchell and Lapata (2008) within this new experiment. First Dataset Description The first experiment, described in detail by Mitchell and Lapata (2008), evaluates how well compositional models disambiguate ambiguous words given the context of a potentially disambiguating noun. Each entry of the dataset provid</context>
<context position="26407" citStr="Mitchell and Lapata (2008)" startWordPosition="4598" endWordPosition="4601">of the noun, as well as an evaluator-set similarity score between 1 and 7 (along with an evaluator identifier), where 1 is low similarity and 7 is high. Evaluation Methodology Candidate models provide a similarity score for each entry. The scores of high similarity entries and low similarity entries are averaged to produce a mean High score and mean Low score for the model. The correlation of the model’s similarity judgements with the human judgements is also calculated using Spearman’s p, a metric which is deemed to be more scrupulous, and ultimately that by which models should be ranked, by Mitchell and Lapata (2008). The mean for each model is on a [0, 1] scale, except for UpperBound which is on the same [1, 7] scale the annotators used. The p scores are on a [−1, 1] scale. It is assumed that inter-annotator agreement provides the theoretical maximum p for any model for this experiment. The cosine measure of the verb vectors, ignoring the noun, is taken to be the baseline (no composition). Other Models The other models we compare ours to are those evaluated by Mitchell and Lapata (2008). We provide a selection of the results 1400 from that paper for the worst (Add) and best5 (Multiply) performing models,</context>
<context position="28878" citStr="Mitchell and Lapata, 2008" startWordPosition="5006" endWordPosition="5009">. Our categorical model performs significantly better than the existing second-place (Kintsch) and obtains a ρ quasiidentical to the multiplicative model, indicating significant correlation with the annotator scores. There is not a large difference between the mean High score and mean Low score, but the distribution in Figure 6 shows that our model makes a non-negligible distinction between high similarity phrases and low similarity phrases, despite the absolute scores not being different by more than a few percentiles. 5The multiplicative model presented here is what is qualified as best in (Mitchell and Lapata, 2008). However, they also present a slightly better performing (ρ = 0.19) model which is a combination of their multiplicative model and a weighted additive model. The difference in ρ is qualified as “not statistically significant” in the original paper, and furthermore the mixed model requires parametric optimisation hence was not evaluated against the entire test set. For these reasons, we chose not to include it in the comparison. Model High Low ρ Baseline 0.27 0.26 0.08 Add 0.59 0.59 0.04 Kintsch 0.47 0.45 0.09 Multiply 0.42 0.28 0.17 Categorical 0.84 0.79 0.17 UpperBound 4.94 3.25 0.40 Table 3</context>
<context position="30339" citStr="Mitchell and Lapata, 2008" startWordPosition="5244" endWordPosition="5247"> of predicted similarities for the categorical distributional model on High and Low similarity items. Second Dataset Description The second dataset6, developed by the authors, follows the format of the (Mitchell and Lapata, 2008) dataset used for the first experiment, with the exception that the target and landmark verbs are transitive, and an object noun is provided in addition to the subject noun, hence forming a small transitive sentence. The dataset comprises 200 entries consisting of sentence pairs (hence a total of 400 sentences) constructed by following the procedure outlined in §4 of (Mitchell and Lapata, 2008), using transitive verbs from CELEX7. For examples of these sentences, see Table 4. The dataset was split into four sections of 100 entries each, with guaranteed 50% exclusive overlap with 6http://www.cs.ox.ac.uk/activities/CompD istMeaning/GS2011data.txt 7http://celex.mpi.nl/ 1401 exactly two other datasets. Each section was given to a group of evaluators, with a total of 25, who were asked to form simple transitive sentence pairs from the verbs, subject and object provided in each entry; for instance ‘the table showed the result’ from ‘table show result’. The evaluators were then asked to ra</context>
<context position="32486" citStr="Mitchell and Lapata, 2008" startWordPosition="5589" endWordPosition="5592"> meaning. This is both because of the imprecise nature of the classification of verb pairs as HIGH or LOW; and since the objective similarity scores produced by a model that distinguishes sentences of different meaning from those of similar meaning can be renormalised in practice. Therefore the delta between HIGH means and LOW mean cannot serve as a definite indication of the practical applicability (or lack thereof) of semantic models; the means are provided just to aid comparison with the results of the first experiment. Model Parameters As in the first experiment, the lexical vectors from (Mitchell and Lapata, 2008) were used for the other models evaluated (additive, multiplicative and baseline)8 and for the noun vec8Kintsch was not evaluated as it required optimising model parameters against a held-out segment of the test set, and we could not replicate the methodology of Mitchell and Lapata tors of our categorical model. Transitive verb vectors were trained as described in §4 with S = N⊗N. Second Experiment Results The results for the models evaluated against the second dataset are presented in Table 5. Model High Low ρ Baseline 0.47 0.44 0.16 Add 0.90 0.90 0.05 Multiply 0.67 0.59 0.17 Categorical 0.73</context>
<context position="35766" citStr="Mitchell and Lapata (2008)" startWordPosition="6118" endWordPosition="6122">r example a verb with argument alternations such as ‘break’ in ‘Y breaks’ and ‘X breaks Y’, are not treated as unrelated. The matrix of the intransitive ‘break’ uses the corpusobserved information about the subject of break, including that of ‘Y’, similarly the matrix of the transitive ‘break’ uses information about its subject and object, including that of ‘X’ and ‘Y’. We leave a thorough study of these phenomena, which fall under providing a modular representation of passiveactive similarities, to future work. We evaluated our model in two ways: first against the word disambiguation task of Mitchell and Lapata (2008) for intransitive verbs, and then against a similar new experiment for transitive verbs, which we developed. Our findings in the first experiment show that the categorical method performs on par with the leading existing approaches. This should not surprise us given that the context is so small and our method becomes similar to the multiplicative model of Mitchell and Lapata (2008). However, our approach is sensitive to grammatical structure, leading us to develop a second experiment taking this into account and differentiating it from models with commutative composition operations. The second</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>J. Mitchell and M. Lapata. 2008. Vector-based models of semantic composition. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>English as a formal language. Formal Philosophy,</title>
<date>1974</date>
<pages>189--223</pages>
<marker>Montague, 1974</marker>
<rawString>R. Montague. 1974. English as a formal language. Formal Philosophy, 189–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 8th International Workshop on Parsing Technologies (IWPT).</booktitle>
<marker>Nivre, 2003</marker>
<rawString>J. Nivre. 2003. An efficient algorithm for projective dependency parsing. Proceedings of the 8th International Workshop on Parsing Technologies (IWPT).</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Preller</author>
</authors>
<title>Towards Discourse Representation via Pregroup Grammars.</title>
<journal>Journal of Logic Language Information</journal>
<volume>16</volume>
<pages>173--194</pages>
<marker>Preller, </marker>
<rawString>A. Preller. Towards Discourse Representation via Pregroup Grammars. Journal of Logic Language Information 16 173–194.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Preller</author>
<author>M Sadrzadeh</author>
</authors>
<title>Semantic Vector Models and Functional Models for Pregroup Grammars.</title>
<journal>Journal of Logic Language Information. DOI:</journal>
<pages>10--1007</pages>
<note>to appear.</note>
<marker>Preller, Sadrzadeh, </marker>
<rawString>A. Preller and M. Sadrzadeh. Semantic Vector Models and Functional Models for Pregroup Grammars. Journal of Logic Language Information. DOI: 10.1007/s10849-011-9132-2. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Saffron</author>
<author>E Newport</author>
<author>R Asling</author>
</authors>
<title>Word Segmentation: The role of distributional cues.</title>
<date>1999</date>
<journal>Journal of Memory and Language</journal>
<volume>35</volume>
<pages>606--621</pages>
<marker>Saffron, Newport, Asling, 1999</marker>
<rawString>J. Saffron, E. Newport, R. Asling. 1999. Word Segmentation: The role of distributional cues. Journal of Memory and Language 35, 606–621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schuetze</author>
</authors>
<title>Automatic Word Sense Discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<pages>97--123</pages>
<marker>Schuetze, 1998</marker>
<rawString>H. Schuetze. 1998. Automatic Word Sense Discrimination. Computational Linguistics 24, 97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<volume>46</volume>
<pages>159--216</pages>
<contexts>
<context position="9454" citStr="Smolensky, 1990" startWordPosition="1496" endWordPosition="1497">atical structure and logical words, and hence cannot compute the meanings of phrases and sentences in the same efficient way that they do for words. Common operations discussed in (Mitchell and Lapata, 2008) such as vector addition (+) and componentwise multiplication (O, cf. §4 for details) are commutative, hence if vw−→ = →− v + →−w or →−v O →−w , then vw = −→ −→ wv, leading to unwelcome equalities such as −−−−−−−−−−−−−→ −−−−−−−−−−−−−→ the dog bit the man = the man bit the dog Non-commutative operations, such as the Kronecker product (cf. §4 for definition) can take word-order into account (Smolensky, 1990) or even some more complex syntactic relations, as described in Clark and Pulman (2007). However, the dimensionality of sentence vectors produced in this manner differs for sentences of different length, barring all sentences from being compared in the same vector space, and growing exponentially with sentence length hence quickly becoming computationally intractable. 3 A Hybrid Logico-Distributional Model Whereas semantic compositional mechanisms for set-theoretic constructions are well understood, there are no obvious corresponding methods for vector spaces. To solve this problem, Coecke et </context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>P. Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Computational Linguistics 46, 1– 2, 159–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT</publisher>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The Syntactic Process. MIT Press. D. Widdows. 2005. Geometry and Meaning. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wittgenstein</author>
</authors>
<title>Philosophical Investigations.</title>
<date>1953</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="2829" citStr="Wittgenstein, 1953" startWordPosition="424" endWordPosition="425">ment search. Compositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question. Compositional formal semantic models stem from classical ideas from mathematical logic, mainly Frege’s principle that the meaning of a sentence is a function of the meaning of its parts (Frege, 1892). Distributional models are more recent and can be related to Wittgenstein’s later philosophy of ‘meaning is use’, whereby meanings of words can be determined from their context (Wittgenstein, 1953). The logical models relate to well known and robust logical formalisms, hence offering a scalable theory of meaning which can be used to reason inferentially. The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al., 2008). This two-sortedness of defining properties of meaning: ‘logical form’ versus ‘contextual use’, has left the quest for ‘what is the foundational structure of meani</context>
</contexts>
<marker>Wittgenstein, 1953</marker>
<rawString>L. Wittgenstein. 1953. Philosophical Investigations. Blackwell.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>