<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000501">
<title confidence="0.98852">
A Sentimental Education: Sentiment Analysis Using Subjectivity
Summarization Based on Minimum Cuts
</title>
<author confidence="0.9985">
Bo Pang and Lillian Lee
</author>
<affiliation confidence="0.9971525">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.72269">
Ithaca, NY 14853-7501
</address>
<email confidence="0.998373">
1pabo,llee}@cs.cornell.edu
</email>
<sectionHeader confidence="0.998594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999839818181818">
Sentiment analysis seeks to identify the view-
point(s) underlying a text span; an example appli-
cation is classifying a movie review as “thumbs up”
or “thumbs down”. To determine this sentiment po-
larity, we propose a novel machine-learning method
that applies text-categorization techniques to just
the subjective portions of the document. Extracting
these portions can be implemented using efficient
techniques for finding minimum cuts in graphs; this
greatly facilitates incorporation of cross-sentence
contextual constraints.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992512244898">
The computational treatment of opinion, sentiment,
and subjectivity has recently attracted a great deal
of attention (see references), in part because of its
potential applications. For instance, information-
extraction and question-answering systems could
flag statements and queries regarding opinions
rather than facts (Cardie et al., 2003). Also, it
has proven useful for companies, recommender sys-
tems, and editorial sites to create summaries of peo-
ple’s experiences and opinions that consist of sub-
jective expressions extracted from reviews (as is
commonly done in movie ads) or even just a re-
view’s polarity — positive (“thumbs up”) or neg-
ative (“thumbs down”).
Document polarity classification poses a signifi-
cant challenge to data-driven methods, resisting tra-
ditional text-categorization techniques (Pang, Lee,
and Vaithyanathan, 2002). Previous approaches fo-
cused on selecting indicative lexical features (e.g.,
the word “good”), classifying a document accord-
ing to the number of such features that occur any-
where within it. In contrast, we propose the follow-
ing process: (1) label the sentences in the document
as either subjective or objective, discarding the lat-
ter; and then (2) apply a standard machine-learning
classifier to the resulting extract. This can prevent
the polarity classifier from considering irrelevant or
even potentially misleading text: for example, al-
though the sentence “The protagonist tries to pro-
tect her good name” contains the word “good”, it
tells us nothing about the author’s opinion and in
fact could well be embedded in a negative movie
review. Also, as mentioned above, subjectivity ex-
tracts can be provided to users as a summary of the
sentiment-oriented content of the document.
Our results show that the subjectivity extracts
we create accurately represent the sentiment in-
formation of the originating documents in a much
more compact form: depending on choice of down-
stream polarity classifier, we can achieve highly sta-
tistically significant improvement (from 82.8% to
86.4%) or maintain the same level of performance
for the polarity classification task while retaining
only 60% of the reviews’ words. Also, we ex-
plore extraction methods based on a minimum cut
formulation, which provides an efficient, intuitive,
and effective means for integrating inter-sentence-
level contextual information with traditional bag-of-
words features.
</bodyText>
<sectionHeader confidence="0.998343" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.994034">
2.1 Architecture
</subsectionHeader>
<bodyText confidence="0.999906157894737">
One can consider document-level polarity classi-
fication to be just a special (more difficult) case
of text categorization with sentiment- rather than
topic-based categories. Hence, standard machine-
learning classification techniques, such as support
vector machines (SVMs), can be applied to the en-
tire documents themselves, as was done by Pang,
Lee, and Vaithyanathan (2002). We refer to such
classification techniques as default polarity classi-
fiers.
However, as noted above, we may be able to im-
prove polarity classification by removing objective
sentences (such as plot summaries in a movie re-
view). We therefore propose, as depicted in Figure
1, to first employ a subjectivity detector that deter-
mines whether each sentence is subjective or not:
discarding the objective ones creates an extract that
should better represent a review’s subjective content
to a default polarity classifier.
</bodyText>
<figureCaption confidence="0.996179">
Figure 1: Polarity classification via subjectivity detec-
tion.
</figureCaption>
<bodyText confidence="0.999914916666667">
To our knowledge, previous work has not in-
tegrated sentence-level subjectivity detection with
document-level sentiment polarity. Yu and Hatzi-
vassiloglou (2003) provide methods for sentence-
level analysis and for determining whether a doc-
ument is subjective or not, but do not combine these
two types of algorithms or consider document polar-
ity classification. The motivation behind the single-
sentence selection method of Beineke et al. (2004)
is to reveal a document’s sentiment polarity, but they
do not evaluate the polarity-classification accuracy
that results.
</bodyText>
<subsectionHeader confidence="0.998451">
2.2 Context and Subjectivity Detection
</subsectionHeader>
<bodyText confidence="0.99998265625">
As with document-level polarity classification, we
could perform subjectivity detection on individual
sentences by applying a standard classification algo-
rithm on each sentence in isolation. However, mod-
eling proximity relationships between sentences
would enable us to leverage coherence: text spans
occurring near each other (within discourse bound-
aries) may share the same subjectivity status, other
things being equal (Wiebe, 1994).
We would therefore like to supply our algorithms
with pair-wise interaction information, e.g., to spec-
ify that two particular sentences should ideally re-
ceive the same subjectivity label but not state which
label this should be. Incorporating such informa-
tion is somewhat unnatural for classifiers whose in-
put consists simply of individual feature vectors,
such as Naive Bayes or SVMs, precisely because
such classifiers label each test item in isolation.
One could define synthetic features or feature vec-
tors to attempt to overcome this obstacle. However,
we propose an alternative that avoids the need for
such feature engineering: we use an efficient and
intuitive graph-based formulation relying on find-
ing minimum cuts. Our approach is inspired by
Blum and Chawla (2001), although they focused on
similarity between items (the motivation being to
combine labeled and unlabeled data), whereas we
are concerned with physical proximity between the
items to be classified; indeed, in computer vision,
modeling proximity information via graph cuts has
led to very effective classification (Boykov, Veksler,
and Zabih, 1999).
</bodyText>
<subsectionHeader confidence="0.995287">
2.3 Cut-based classification
</subsectionHeader>
<bodyText confidence="0.9995438">
Figure 2 shows a worked example of the concepts
in this section.
Suppose we have n items x1, ... , xn to divide
into two classes C1 and C2, and we have access to
two types of information:
</bodyText>
<listItem confidence="0.983888166666667">
• Individual scores indj(xi): non-negative esti-
mates of each xi’s preference for being in Cj based
on just the features of xi alone; and
• Association scores assoc(xi, xk): non-negative
estimates of how important it is that xi and xk be in
the same class.1
</listItem>
<bodyText confidence="0.999627625">
We would like to maximize each item’s “net hap-
piness”: its individual score for the class it is as-
signed to, minus its individual score for the other
class. But, we also want to penalize putting tightly-
associated items into different classes. Thus, after
some algebra, we arrive at the following optimiza-
tion problem: assign the xis to C1 and C2 so as to
minimize the partition cost
</bodyText>
<equation confidence="0.998115333333333">
� ind2(x)+ � ind1(x)+ � assoc(xi, xk).
x∈C1 x∈C2 xi∈C1,
xk∈C2
</equation>
<bodyText confidence="0.95719725">
The problem appears intractable, since there are
2n possible binary partitions of the xi’s. How-
ever, suppose we represent the situation in the fol-
lowing manner. Build an undirected graph G with
vertices {v1, ... , vn, s, t}; the last two are, respec-
tively, the source and sink. Add n edges (s, vi), each
with weight ind1(xi), and n edges (vi, t), each with
weight ind2(xi). Finally, add (n ) edges (vi, vk),
</bodyText>
<page confidence="0.580257">
2
</page>
<bodyText confidence="0.992786142857143">
each with weight assoc(xi, xk). Then, cuts in G
are defined as follows:
Definition 1 A cut (S, T) of G is a partition of its
nodes into sets S = {s} U S0 and T = {t} U T0,
where s ∈� S0, t ∈� T0. Its cost cost(S, T) is the sum
of the weights of all edges crossing from S to T. A
minimum cut of G is one of minimum cost.
</bodyText>
<footnote confidence="0.968821">
1Asymmetry is allowed, but we used symmetric scores.
</footnote>
<figure confidence="0.998939511111111">
subjective
n−sentence review sentence? m−sentence extract
(m&lt;=n)
positive or negative
review?
yes
s1
default
classifier
polarity
subjectivity
detector
s2
s3
s4
s_n
no
no
yes
s1
s4
+/−
subjectivity extraction
s ind1(M) [.5] M ind2(M) [.5] t
ind (Y) [.8] ind (Y) [.2]
1 2
ind (N) [.1] ind (N) [.9]
1 2
assoc(Y,M)
assoc(M,N)
[1.0]
[.2]
Y
N
assoc(Y,N) [.1]
C1 Individual Association Cost
penalties penalties
{Y,M} .2 + .5 + .1 .1 + .2 1.1
(none) .8 + .5 + .1 0 1.4
{Y,M,N} .2 + .5 + .9 0 1.6
{Y} .2 + .5 + .1 1.0 + .1 1.9
{N} .8 + .5 + .9 .1 + .2 2.5
{M} .8 + .5 + .1 1.0 + .2 2.6
{Y,N} .2 + .5 + .9 1.0 + .2 2.8
{M,N} .8 + .5 + .9 1.0 + .1 3.3
</figure>
<figureCaption confidence="0.9931135">
Figure 2: Graph for classifying three items. Brackets enclose example values; here, the individual scores happen to
be probabilities. Based on individual scores alone, we would put Y (“yes”) in C1, N (“no”) in C2, and be undecided
about M (“maybe”). But the association scores favor cuts that put Y and M in the same class, as shown in the table.
Thus, the minimum cut, indicated by the dashed line, places M together with Y in C1.
</figureCaption>
<bodyText confidence="0.9998948125">
Observe that every cut corresponds to a partition of
the items and has cost equal to the partition cost.
Thus, our optimization problem reduces to finding
minimum cuts.
Practical advantages As we have noted, formulat-
ing our subjectivity-detection problem in terms of
graphs allows us to model item-specific and pair-
wise information independently. Note that this is
a very flexible paradigm. For instance, it is per-
fectly legitimate to use knowledge-rich algorithms
employing deep linguistic knowledge about sen-
timent indicators to derive the individual scores.
And we could also simultaneously use knowledge-
lean methods to assign the association scores. In-
terestingly, Yu and Hatzivassiloglou (2003) com-
pared an individual-preference classifier against a
relationship-based method, but didn’t combine the
two; the ability to coordinate such algorithms is
precisely one of the strengths of our approach.
But a crucial advantage specific to the utilization
of a minimum-cut-based approach is that we can use
maximumfflow algorithms with polynomial asymp-
totic running times — and near-linear running times
in practice — to exactly compute the minimum-
cost cut(s), despite the apparent intractability of
the optimization problem (Cormen, Leiserson, and
Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2
In contrast, other graph-partitioning problems that
have been previously used to formulate NLP clas-
sification problems3 are NP-complete (Hatzivassi-
loglou and McKeown, 1997; Agrawal et al., 2003;
Joachims, 2003).
</bodyText>
<footnote confidence="0.929566666666667">
2Code available at http://www.avglab.com/andrew/soft.html.
3Graph-based approaches to general clustering problems
are too numerous to mention here.
</footnote>
<sectionHeader confidence="0.998054" genericHeader="method">
3 Evaluation Framework
</sectionHeader>
<bodyText confidence="0.9999220625">
Our experiments involve classifying movie reviews
as either positive or negative, an appealing task for
several reasons. First, as mentioned in the intro-
duction, providing polarity information about re-
views is a useful service: witness the popularity of
www.rottentomatoes.com. Second, movie reviews
are apparently harder to classify than reviews of
other products (Turney, 2002; Dave, Lawrence, and
Pennock, 2003). Third, the correct label can be ex-
tracted automatically from rating information (e.g.,
number of stars). Our data4 contains 1000 positive
and 1000 negative reviews all written before 2002,
with a cap of 20 reviews per author (312 authors
total) per category. We refer to this corpus as the
polarity dataset.
Default polarity classifiers We tested support vec-
tor machines (SVMs) and Naive Bayes (NB). Fol-
lowing Pang et al. (2002), we use unigram-presence
features: the ith coordinate of a feature vector is
1 if the corresponding unigram occurs in the input
text, 0 otherwise. (For SVMs, the feature vectors
are length-normalized). Each default document-
level polarity classifier is trained and tested on the
extracts formed by applying one of the sentence-
level subjectivity detectors to reviews in the polarity
dataset.
Subjectivity dataset To train our detectors, we
need a collection of labeled sentences. Riloff and
Wiebe (2003) state that “It is [very hard] to ob-
tain collections of individual sentences that can be
easily identified as subjective or objective”; the
polarity-dataset sentences, for example, have not
</bodyText>
<footnote confidence="0.9985275">
4Available at www.cs.cornell.edu/people/pabo/movie-
review-data/ (review corpus version 2.0).
</footnote>
<bodyText confidence="0.989151136363637">
been so annotated.5 Fortunately, we were able
to mine the Web to create a large, automatically-
labeled sentence corpus6. To gather subjective
sentences (or phrases), we collected 5000 movie-
review snippets (e.g., “bold, imaginative, and im-
possible to resist”) from www.rottentomatoes.com.
To obtain (mostly) objective data, we took 5000 sen-
tences from plot summaries available from the In-
ternet Movie Database (www.imdb.com). We only
selected sentences or snippets at least ten words
long and drawn from reviews or plot summaries of
movies released post-2001, which prevents overlap
with the polarity dataset.
Subjectivity detectors As noted above, we can use
our default polarity classifiers as “basic” sentence-
level subjectivity detectors (after retraining on the
subjectivity dataset) to produce extracts of the orig-
inal reviews. We also create a family of cut-based
subjectivity detectors; these take as input the set of
sentences appearing in a single document and de-
termine the subjectivity status of all the sentences
simultaneously using per-item and pairwise rela-
tionship information. Specifically, for a given doc-
ument, we use the construction in Section 2.2 to
build a graph wherein the source s and sink t cor-
respond to the class of subjective and objective sen-
tences, respectively, and each internal node vi cor-
responds to the document’s ith sentence si. We can
set the individual scores ind1(si) to PrNB
sub (si) and
ind2(si) to 1 − PrNB
sub (si), as shown in Figure 3,
where PrNB
sub (s) denotes Naive Bayes’ estimate of
the probability that sentence s is subjective; or, we
can use the weights produced by the SVM classi-
fier instead.7 If we set all the association scores
to zero, then the minimum-cut classification of the
sentences is the same as that of the basic subjectiv-
ity detector. Alternatively, we incorporate the de-
gree of proximity between pairs of sentences, con-
trolled by three parameters. The threshold T spec-
ifies the maximum distance two sentences can be
separated by and still be considered proximal. The
</bodyText>
<footnote confidence="0.989466428571429">
5We therefore could not directly evaluate sentence-
classification accuracy on the polarity dataset.
6Available at www.cs.cornell.edu/people/pabo/movie-
review-data/ , sentence corpus version 1.0.
7We converted SVM output di, which is a signed distance
(negative=objective) from the separating hyperplane, to non-
negative numbers by
</footnote>
<equation confidence="0.793234">
def r1 di &gt; 2;
ind1(si) = S (2 + di)/4 −2 &lt;_ di &lt;_ 2;
l 0 di &lt; −2.
</equation>
<bodyText confidence="0.987226272727273">
and ind2(si) = 1 − ind1(si). Note that scaling is employed
only for consistency; the algorithm itself does not require prob-
abilities for individual scores.
non-increasing function f(d) specifies how the in-
fluence of proximal sentences decays with respect to
distance d; in our experiments, we tried f(d) = 1,
e1−d, and 1/d2. The constant c controls the relative
influence of the association scores: a larger c makes
the minimum-cut algorithm more loath to put prox-
imal sentences in different classes. With these in
hand8, we set (for j &gt; i)
</bodyText>
<equation confidence="0.547028">
assoc(si, sj)=l
def f f(j − i) · c if (j − i) ≤ T;
0 otherwise.
</equation>
<sectionHeader confidence="0.994155" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999930190476191">
Below, we report average accuracies computed by
ten-fold cross-validation over the polarity dataset.
Section 4.1 examines our basic subjectivity extrac-
tion algorithms, which are based on individual-
sentence predictions alone. Section 4.2 evaluates
the more sophisticated form of subjectivity extrac-
tion that incorporates context information via the
minimum-cut paradigm.
As we will see, the use of subjectivity extracts
can in the best case provide satisfying improve-
ment in polarity classification, and otherwise can
at least yield polarity-classification accuracies indis-
tinguishable from employing the full review. At the
same time, the extracts we create are both smaller
on average than the original document and more
effective as input to a default polarity classifier
than the same-length counterparts produced by stan-
dard summarization tactics (e.g., first- or last-N sen-
tences). We therefore conclude that subjectivity ex-
traction produces effective summaries of document
sentiment.
</bodyText>
<subsectionHeader confidence="0.998132">
4.1 Basic subjectivity extraction
</subsectionHeader>
<bodyText confidence="0.959842538461539">
As noted in Section 3, both Naive Bayes and SVMs
can be trained on our subjectivity dataset and then
used as a basic subjectivity detector. The former has
somewhat better average ten-fold cross-validation
performance on the subjectivity dataset (92% vs.
90%), and so for space reasons, our initial discus-
sions will focus on the results attained via NB sub-
jectivity detection.
Employing Naive Bayes as a subjectivity detec-
tor (ExtractNB) in conjunction with a Naive Bayes
document-level polarity classifier achieves 86.4%
accuracy.9 This is a clear improvement over the
82.8% that results when no extraction is applied
</bodyText>
<footnote confidence="0.9993355">
8Parameter training is driven by optimizing the performance
of the downstream polarity classifier rather than the detector
itself because the subjectivity dataset’s sentences come from
different reviews, and so are never proximal.
9This result and others are depicted in Figure 5; for now,
consider only the y-axis in those plots.
</footnote>
<figure confidence="0.7990805">
n−sentence review
proximity link
</figure>
<figureCaption confidence="0.982393">
Figure 3: Graph-cut-based creation of subjective extracts.
</figureCaption>
<figure confidence="0.989209">
edge crossing the cut
individual subjectivity−probability link
NB NB
Pr (s1) v1 1−Pr (s1)
sub sub
v2
min. cut s t � �extract
� � � � � �
v3
vn
s4
��
v1
s
t
compute
v2
create
v3
vn
m−sentence extract
(m&lt;=n)
s1
s4
...
...
s1
s2
construct
graph � �
s_n
s3
</figure>
<bodyText confidence="0.992332082191781">
(Full review); indeed, the difference is highly sta-
tistically significant (p &lt; 0.01, paired t-test). With
SVMs as the polarity classifier instead, the Full re-
view performance rises to 87.15%, but comparison
via the paired t-test reveals that this is statistically
indistinguishable from the 86.4% that is achieved by
running the SVM polarity classifier on ExtractNB
input. (More improvements to extraction perfor-
mance are reported later in this section.)
These findings indicate10 that the extracts pre-
serve (and, in the NB polarity-classifier case, appar-
ently clarify) the sentiment information in the orig-
inating documents, and thus are good summaries
from the polarity-classification point of view. Fur-
ther support comes from a “flipping” experiment:
if we give as input to the default polarity classifier
an extract consisting of the sentences labeled ob-
jective, accuracy drops dramatically to 71% for NB
and 67% for SVMs. This confirms our hypothesis
that sentences discarded by the subjectivity extrac-
tion process are indeed much less indicative of sen-
timent polarity.
Moreover, the subjectivity extracts are much
more compact than the original documents (an im-
portant feature for a summary to have): they contain
on average only about 60% of the source reviews’
words. (This word preservation rate is plotted along
the x-axis in the graphs in Figure 5.) This prompts
us to study how much reduction of the original doc-
uments subjectivity detectors can perform and still
accurately represent the texts’ sentiment informa-
tion.
We can create subjectivity extracts of varying
lengths by taking just the N most subjective sen-
tences11 from the originating review. As one base-
10Recall that direct evidence is not available because the po-
larity dataset’s sentences lack subjectivity labels.
11These are the N sentences assigned the highest probability
by the basic NB detector, regardless of whether their probabil-
line to compare against, we take the canonical sum-
marization standard of extracting the first N sen-
tences — in general settings, authors often be-
gin documents with an overview. We also con-
sider the last N sentences: in many documents,
concluding material may be a good summary, and
www.rottentomatoes.com tends to select “snippets”
from the end of movie reviews (Beineke et al.,
2004). Finally, as a sanity check, we include results
from the N least subjective sentences according to
Naive Bayes.
Figure 4 shows the polarity classifier results as
N ranges between 1 and 40. Our first observation
is that the NB detector provides very good “bang
for the buck”: with subjectivity extracts containing
as few as 15 sentences, accuracy is quite close to
what one gets if the entire review is used. In fact,
for the NB polarity classifier, just using the 5 most
subjective sentences is almost as informative as the
Full review while containing on average only about
22% of the source reviews’ words.
Also, it so happens that at N = 30, performance
is actually slightly better than (but statistically in-
distinguishable from) Full review even when the
SVM default polarity classifier is used (87.2% vs.
87.15%).12 This suggests potentially effective ex-
traction alternatives other than using a fixed proba-
bility threshold (which resulted in the lower accu-
racy of 86.4% reported above).
Furthermore, we see in Figure 4 that the N most-
subjective-sentences method generally outperforms
the other baseline summarization methods (which
perhaps suggests that sentiment summarization can-
not be treated the same as topic-based summariza-
</bodyText>
<footnote confidence="0.846741666666667">
ities exceed 50% and so would actually be classified as subjec-
tive by Naive Bayes. For reviews with fewer than N sentences,
the entire review will be returned.
12Note that roughly half of the documents in the polarity
dataset contain more than 30 sentences (average=32.3, standard
deviation 15).
</footnote>
<note confidence="0.426049">
Accuracy for N-sentence abstracts (def = NB)
</note>
<figure confidence="0.990165696969697">
1 5 10 15 20 25 30 35 40
N
Accuracy for N-sentence abstracts (def = SVM)
1 5 10 15 20 25 30 35 40
N
Average accuracy
90
85
80
75
70
65
60
55
most subjective N sentences
last N sentences
first N sentences
least subjective N sentences
Full review
Average accuracy
90
85
80
75
70
65
60
55
most subjective N sentences
last N sentences
first N sentences
least subjective N sentences
Full review
</figure>
<figureCaption confidence="0.995291">
Figure 4: Accuracies using N-sentence extracts for NB (left) and SVM (right) default polarity classifiers.
</figureCaption>
<figure confidence="0.928430046511628">
Accuracy for subjective abstracts (def = NB)
0.6 0.7 0.8 0.9 1 1.1
% of words extracted
Accuracy for subjective abstracts (def = SVM)
0.6 0.7 0.8 0.9 1 1.1
% of words extracted
Average accuracy
86.5
85.5
84.5
83.5
87
86
85
84
83
ExtractNB
ExtractSVM
indicates statistically significant
improvement in accuracy
ExtractNB+Prox
ExtractSVM+Prox
difference in accuracy
not statistically significant
Full Review
Average accuracy
86.5
85.5
84.5
83.5
87
86
85
84
83
not statistically significant
ExtractNB ExtractSVM+Prox
ExtractSVM
indicates statistically significant
improvement in accuracy
ExtractNB+Prox
difference in accuracy
Full Review
</figure>
<figureCaption confidence="0.850653">
Figure 5: Word preservation rate vs. accuracy, NB (left) and SVMs (right) as default polarity classifiers.
Also indicated are results for some statistical significance tests.
</figureCaption>
<bodyText confidence="0.9999345">
tion, although this conjecture would need to be veri-
fied on other domains and data). It’s also interesting
to observe how much better the last N sentences are
than the first N sentences; this may reflect a (hardly
surprising) tendency for movie-review authors to
place plot descriptions at the beginning rather than
the end of the text and conclude with overtly opin-
ionated statements.
</bodyText>
<subsectionHeader confidence="0.996333">
4.2 Incorporating context information
</subsectionHeader>
<bodyText confidence="0.994083898550725">
The previous section demonstrated the value of
subjectivity detection. We now examine whether
context information, particularly regarding sentence
proximity, can further improve subjectivity extrac-
tion. As discussed in Section 2.2 and 3, con-
textual constraints are easily incorporated via the
minimum-cut formalism but are not natural inputs
for standard Naive Bayes and SVMs.
Figure 5 shows the effect of adding in
proximity information. ExtractNB+Prox and
ExtractSVM+Prox are the graph-based subjectivity
detectors using Naive Bayes and SVMs, respec-
tively, for the individual scores; we depict the
best performance achieved by a single setting of
the three proximity-related edge-weight parameters
over all ten data folds13 (parameter selection was
not a focus of the current work). The two compar-
isons we are most interested in are ExtractNB+Prox
versus ExtractNB and ExtractSVM+Prox versus
ExtractSVM.
We see that the context-aware graph-based sub-
jectivity detectors tend to create extracts that are
more informative (statistically significant so (paired
t-test) for SVM subjectivity detectors only), al-
though these extracts are longer than their context-
blind counterparts. We note that the performance
13Parameters are chosen from T E {1, 2, 3}, f(d) E
{1, e1−d, 1/d2}, and c E [0, 1] at intervals of 0.1.
enhancements cannot be attributed entirely to the
mere inclusion of more sentences regardless of
whether they are subjective or not — one counter-
argument is that Full review yielded substantially
worse results for the NB default polarity classifier—
and at any rate, the graph-derived extracts are still
substantially more concise than the full texts.
Now, while incorporating a bias for assigning
nearby sentences to the same category into NB and
SVM subjectivity detectors seems to require some
non-obvious feature engineering, we also wish
to investigate whether our graph-based paradigm
makes better use of contextual constraints that can
be (more or less) easily encoded into the input of
standard classifiers. For illustrative purposes, we
consider paragraph-boundary information, looking
only at SVM subjectivity detection for simplicity’s
sake.
It seems intuitively plausible that paragraph
boundaries (an approximation to discourse bound-
aries) loosen coherence constraints between nearby
sentences. To capture this notion for minimum-cut-
based classification, we can simply reduce the as-
sociation scores for all pairs of sentences that oc-
cur in different paragraphs by multiplying them by
a cross-paragraph-boundary weight w ∈ [0, 1]. For
standard classifiers, we can employ the trick of hav-
ing the detector treat paragraphs, rather than sen-
tences, as the basic unit to be labeled. This en-
ables the standard classifier to utilize coherence be-
tween sentences in the same paragraph; on the other
hand, it also (probably unavoidably) poses a hard
constraint that all of a paragraph’s sentences get the
same label, which increases noise sensitivity.14 Our
experiments reveal the graph-cut formulation to be
the better approach: for both default polarity clas-
sifiers (NB and SVM), some choice of parameters
(including w) for ExtractSVM+Prox yields statisti-
cally significant improvement over its paragraph-
unit non-graph counterpart (NB: 86.4% vs. 85.2%;
SVM: 86.15% vs. 85.45%).
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999978875">
We examined the relation between subjectivity de-
tection and polarity classification, showing that sub-
jectivity detection can compress reviews into much
shorter extracts that still retain polarity information
at a level comparable to that of the full review. In
fact, for the Naive Bayes polarity classifier, the sub-
jectivity extracts are shown to be more effective in-
put than the originating document, which suggests
</bodyText>
<footnote confidence="0.5632395">
14For example, in the data we used, boundaries may have
been missed due to malformed html.
</footnote>
<bodyText confidence="0.999822769230769">
that they are not only shorter, but also “cleaner” rep-
resentations of the intended polarity.
We have also shown that employing the
minimum-cut framework results in the develop-
ment of efficient algorithms for sentiment analy-
sis. Utilizing contextual information via this frame-
work can lead to statistically significant improve-
ment in polarity-classification accuracy. Directions
for future research include developing parameter-
selection techniques, incorporating other sources of
contextual cues besides sentence proximity, and in-
vestigating other means for modeling such informa-
tion.
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993139071428571">
We thank Eric Breck, Claire Cardie, Rich Caruana,
Yejin Choi, Shimon Edelman, Thorsten Joachims,
Jon Kleinberg, Oren Kurland, Art Munson, Vincent
Ng, Fernando Pereira, Ves Stoyanov, Ramin Zabih,
and the anonymous reviewers for helpful comments.
This paper is based upon work supported in part
by the National Science Foundation under grants
ITR/IM IIS-0081334 and IIS-0329064, a Cornell
Graduate Fellowship in Cognitive Studies, and by
an Alfred P. Sloan Research Fellowship. Any opin-
ions, findings, and conclusions or recommendations
expressed above are those of the authors and do not
necessarily reflect the views of the National Science
Foundation or Sloan Foundation.
</bodyText>
<sectionHeader confidence="0.99783" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983716733333334">
Agrawal, Rakesh, Sridhar Rajagopalan, Ramakrish-
nan Srikant, and Yirong Xu. 2003. Mining news-
groups using networks arising from social behav-
ior. In WWW, pages 529–535.
Ahuja, Ravindra, Thomas L. Magnanti, and
James B. Orlin. 1993. Network Flows: Theory,
Algorithms, and Applications. Prentice Hall.
Beineke, Philip, Trevor Hastie, Christopher Man-
ning, and Shivakumar Vaithyanathan. 2004.
Exploring sentiment summarization. In AAAI
Spring Symposium on Exploring Attitude and Af-
fect in Text: Theories and Applications (AAAI
tech report SS-04-07).
Blum, Avrim and Shuchi Chawla. 2001. Learning
from labeled and unlabeled data using graph min-
cuts. In Intl. Conf. on Machine Learning (ICML),
pages 19–26.
Boykov, Yuri, Olga Veksler, and Ramin Zabih.
1999. Fast approximate energy minimization via
graph cuts. In Intl. Conf. on Computer Vision
(ICCV), pages 377–384. Journal version in IEEE
Trans. Pattern Analysis and Machine Intelligence
(PAMI) 23(11):1222–1239, 2001.
Cardie, Claire, Janyce Wiebe, Theresa Wilson, and
Diane Litman. 2003. Combining low-level and
summary representations of opinions for multi-
perspective question answering. In AAAI Spring
Symposium on New Directions in Question An-
swering, pages 20–27.
Cormen, Thomas H., Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to Algo-
rithms. MIT Press.
Das, Sanjiv and Mike Chen. 2001. Yahoo! for
Amazon: Extracting market sentiment from stock
message boards. In Asia Pacific Finance Associ-
ation Annual Conf. (APFA).
Dave, Kushal, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of product
reviews. In WWW, pages 519–528.
Dini, Luca and Giampaolo Mazzini. 2002. Opin-
ion classification through information extraction.
In Intl. Conf. on Data Mining Methods and
Databases for Engineering, Finance and Other
Fields, pages 299–310.
Durbin, Stephen D., J. Neal Richter, and Doug
Warner. 2003. A system for affective rating of
texts. In KDD Wksp. on Operational Text Classi-
fication Systems (OTC-3).
Hatzivassiloglou, Vasileios and Kathleen Mc-
Keown. 1997. Predicting the semantic orienta-
tion of adjectives. In 35th ACL/8th EACL, pages
174–181.
Joachims, Thorsten. 2003. Transductive learning
via spectral graph partitioning. In Intl. Conf. on
Machine Learning (ICML).
Liu, Hugo, Henry Lieberman, and Ted Selker.
2003. A model of textual affect sensing using
real-world knowledge. In Intelligent User Inter-
faces (IUI), pages 125–132.
Montes-y-G´omez, Manuel, Aurelio L´opez-L´opez,
and Alexander Gelbukh. 1999. Text mining as a
social thermometer. In IJCAI Wksp. on Text Min-
ing, pages 103–107.
Morinaga, Satoshi, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining prod-
uct reputations on the web. In KDD, pages 341–
349. Industry track.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Senti-
ment classification using machine learning
techniques. In EMNLP, pages 79–86.
Qu, Yan, James Shanahan, and Janyce Wiebe, edi-
tors. 2004. AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Ap-
plications. AAAI technical report SS-04-07.
Riloff, Ellen and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP.
Riloff, Ellen, Janyce Wiebe, and Theresa Wilson.
2003. Learning subjective nouns using extraction
pattern bootstrapping. In Conf. on Natural Lan-
guage Learning (CoNLL), pages 25–32.
Subasic, Pero and Alison Huettner. 2001. Af-
fect analysis of text using fuzzy semantic typing.
IEEE Trans. Fuzzy Systems, 9(4):483–496.
Tong, Richard M. 2001. An operational system for
detecting and tracking opinions in on-line discus-
sion. SIGIR Wksp. on Operational Text Classifi-
cation.
Turney, Peter. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised
classification of reviews. In ACL, pages 417–424.
Wiebe, Janyce M. 1994. Tracking point of view in
narrative. Computational Linguistics, 20(2):233–
287.
Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu,
and Wayne Niblack. 2003. Sentiment analyzer:
Extracting sentiments about a given topic using
natural language processing techniques. In IEEE
Intl. Conf. on Data Mining (ICDM).
Yu, Hong and Vasileios Hatzivassiloglou. 2003.
Towards answering opinion questions: Separat-
ing facts from opinions and identifying the polar-
ity of opinion sentences. In EMNLP.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958428">
<title confidence="0.9967135">A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts</title>
<author confidence="0.999478">Pang Lee</author>
<affiliation confidence="0.999447">Department of Computer Science Cornell University</affiliation>
<address confidence="0.999861">Ithaca, NY 14853-7501</address>
<abstract confidence="0.997171166666666">analysis to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as “thumbs up” “thumbs down”. To determine this powe propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient for finding cuts in this greatly facilitates incorporation of cross-sentence contextual constraints.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Sridhar Rajagopalan</author>
<author>Ramakrishnan Srikant</author>
<author>Yirong Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior. In</title>
<date>2003</date>
<booktitle>WWW,</booktitle>
<pages>529--535</pages>
<contexts>
<context position="10568" citStr="Agrawal et al., 2003" startWordPosition="1686" endWordPosition="1689"> the strengths of our approach. But a crucial advantage specific to the utilization of a minimum-cut-based approach is that we can use maximumfflow algorithms with polynomial asymptotic running times — and near-linear running times in practice — to exactly compute the minimumcost cut(s), despite the apparent intractability of the optimization problem (Cormen, Leiserson, and Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2 In contrast, other graph-partitioning problems that have been previously used to formulate NLP classification problems3 are NP-complete (Hatzivassiloglou and McKeown, 1997; Agrawal et al., 2003; Joachims, 2003). 2Code available at http://www.avglab.com/andrew/soft.html. 3Graph-based approaches to general clustering problems are too numerous to mention here. 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave, Lawrence, and Pennock, 2</context>
</contexts>
<marker>Agrawal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>Agrawal, Rakesh, Sridhar Rajagopalan, Ramakrishnan Srikant, and Yirong Xu. 2003. Mining newsgroups using networks arising from social behavior. In WWW, pages 529–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravindra Ahuja</author>
<author>Thomas L Magnanti</author>
<author>James B Orlin</author>
</authors>
<title>Network Flows: Theory, Algorithms, and Applications.</title>
<date>1993</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="10371" citStr="Ahuja, Magnanti, and Orlin, 1993" startWordPosition="1658" endWordPosition="1662">ingly, Yu and Hatzivassiloglou (2003) compared an individual-preference classifier against a relationship-based method, but didn’t combine the two; the ability to coordinate such algorithms is precisely one of the strengths of our approach. But a crucial advantage specific to the utilization of a minimum-cut-based approach is that we can use maximumfflow algorithms with polynomial asymptotic running times — and near-linear running times in practice — to exactly compute the minimumcost cut(s), despite the apparent intractability of the optimization problem (Cormen, Leiserson, and Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2 In contrast, other graph-partitioning problems that have been previously used to formulate NLP classification problems3 are NP-complete (Hatzivassiloglou and McKeown, 1997; Agrawal et al., 2003; Joachims, 2003). 2Code available at http://www.avglab.com/andrew/soft.html. 3Graph-based approaches to general clustering problems are too numerous to mention here. 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarity information about reviews is a u</context>
</contexts>
<marker>Ahuja, Magnanti, Orlin, 1993</marker>
<rawString>Ahuja, Ravindra, Thomas L. Magnanti, and James B. Orlin. 1993. Network Flows: Theory, Algorithms, and Applications. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Beineke</author>
<author>Trevor Hastie</author>
<author>Christopher Manning</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Exploring sentiment summarization.</title>
<date>2004</date>
<booktitle>In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications (AAAI tech report</booktitle>
<pages>04--07</pages>
<contexts>
<context position="4577" citStr="Beineke et al. (2004)" startWordPosition="671" endWordPosition="674"> ones creates an extract that should better represent a review’s subjective content to a default polarity classifier. Figure 1: Polarity classification via subjectivity detection. To our knowledge, previous work has not integrated sentence-level subjectivity detection with document-level sentiment polarity. Yu and Hatzivassiloglou (2003) provide methods for sentencelevel analysis and for determining whether a document is subjective or not, but do not combine these two types of algorithms or consider document polarity classification. The motivation behind the singlesentence selection method of Beineke et al. (2004) is to reveal a document’s sentiment polarity, but they do not evaluate the polarity-classification accuracy that results. 2.2 Context and Subjectivity Detection As with document-level polarity classification, we could perform subjectivity detection on individual sentences by applying a standard classification algorithm on each sentence in isolation. However, modeling proximity relationships between sentences would enable us to leverage coherence: text spans occurring near each other (within discourse boundaries) may share the same subjectivity status, other things being equal (Wiebe, 1994). W</context>
<context position="20060" citStr="Beineke et al., 2004" startWordPosition="3175" endWordPosition="3178">10Recall that direct evidence is not available because the polarity dataset’s sentences lack subjectivity labels. 11These are the N sentences assigned the highest probability by the basic NB detector, regardless of whether their probabilline to compare against, we take the canonical summarization standard of extracting the first N sentences — in general settings, authors often begin documents with an overview. We also consider the last N sentences: in many documents, concluding material may be a good summary, and www.rottentomatoes.com tends to select “snippets” from the end of movie reviews (Beineke et al., 2004). Finally, as a sanity check, we include results from the N least subjective sentences according to Naive Bayes. Figure 4 shows the polarity classifier results as N ranges between 1 and 40. Our first observation is that the NB detector provides very good “bang for the buck”: with subjectivity extracts containing as few as 15 sentences, accuracy is quite close to what one gets if the entire review is used. In fact, for the NB polarity classifier, just using the 5 most subjective sentences is almost as informative as the Full review while containing on average only about 22% of the source review</context>
</contexts>
<marker>Beineke, Hastie, Manning, Vaithyanathan, 2004</marker>
<rawString>Beineke, Philip, Trevor Hastie, Christopher Manning, and Shivakumar Vaithyanathan. 2004. Exploring sentiment summarization. In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications (AAAI tech report SS-04-07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<title>Learning from labeled and unlabeled data using graph mincuts.</title>
<date>2001</date>
<booktitle>In Intl. Conf. on Machine Learning (ICML),</booktitle>
<pages>pages</pages>
<contexts>
<context position="5952" citStr="Blum and Chawla (2001)" startWordPosition="874" endWordPosition="877">eive the same subjectivity label but not state which label this should be. Incorporating such information is somewhat unnatural for classifiers whose input consists simply of individual feature vectors, such as Naive Bayes or SVMs, precisely because such classifiers label each test item in isolation. One could define synthetic features or feature vectors to attempt to overcome this obstacle. However, we propose an alternative that avoids the need for such feature engineering: we use an efficient and intuitive graph-based formulation relying on finding minimum cuts. Our approach is inspired by Blum and Chawla (2001), although they focused on similarity between items (the motivation being to combine labeled and unlabeled data), whereas we are concerned with physical proximity between the items to be classified; indeed, in computer vision, modeling proximity information via graph cuts has led to very effective classification (Boykov, Veksler, and Zabih, 1999). 2.3 Cut-based classification Figure 2 shows a worked example of the concepts in this section. Suppose we have n items x1, ... , xn to divide into two classes C1 and C2, and we have access to two types of information: • Individual scores indj(xi): non</context>
</contexts>
<marker>Blum, Chawla, 2001</marker>
<rawString>Blum, Avrim and Shuchi Chawla. 2001. Learning from labeled and unlabeled data using graph mincuts. In Intl. Conf. on Machine Learning (ICML), pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Boykov</author>
<author>Olga Veksler</author>
<author>Ramin Zabih</author>
</authors>
<title>Fast approximate energy minimization via graph cuts.</title>
<date>1999</date>
<journal>Journal version in IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI)</journal>
<booktitle>In Intl. Conf. on Computer Vision (ICCV),</booktitle>
<volume>23</volume>
<issue>11</issue>
<pages>377--384</pages>
<contexts>
<context position="6299" citStr="Boykov, Veksler, and Zabih, 1999" startWordPosition="923" endWordPosition="927"> or feature vectors to attempt to overcome this obstacle. However, we propose an alternative that avoids the need for such feature engineering: we use an efficient and intuitive graph-based formulation relying on finding minimum cuts. Our approach is inspired by Blum and Chawla (2001), although they focused on similarity between items (the motivation being to combine labeled and unlabeled data), whereas we are concerned with physical proximity between the items to be classified; indeed, in computer vision, modeling proximity information via graph cuts has led to very effective classification (Boykov, Veksler, and Zabih, 1999). 2.3 Cut-based classification Figure 2 shows a worked example of the concepts in this section. Suppose we have n items x1, ... , xn to divide into two classes C1 and C2, and we have access to two types of information: • Individual scores indj(xi): non-negative estimates of each xi’s preference for being in Cj based on just the features of xi alone; and • Association scores assoc(xi, xk): non-negative estimates of how important it is that xi and xk be in the same class.1 We would like to maximize each item’s “net happiness”: its individual score for the class it is assigned to, minus its indi</context>
</contexts>
<marker>Boykov, Veksler, Zabih, 1999</marker>
<rawString>Boykov, Yuri, Olga Veksler, and Ramin Zabih. 1999. Fast approximate energy minimization via graph cuts. In Intl. Conf. on Computer Vision (ICCV), pages 377–384. Journal version in IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI) 23(11):1222–1239, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Diane Litman</author>
</authors>
<title>Combining low-level and summary representations of opinions for multiperspective question answering.</title>
<date>2003</date>
<booktitle>In AAAI Spring Symposium on New Directions in Question Answering,</booktitle>
<pages>20--27</pages>
<contexts>
<context position="1114" citStr="Cardie et al., 2003" startWordPosition="146" endWordPosition="149">categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints. 1 Introduction The computational treatment of opinion, sentiment, and subjectivity has recently attracted a great deal of attention (see references), in part because of its potential applications. For instance, informationextraction and question-answering systems could flag statements and queries regarding opinions rather than facts (Cardie et al., 2003). Also, it has proven useful for companies, recommender systems, and editorial sites to create summaries of people’s experiences and opinions that consist of subjective expressions extracted from reviews (as is commonly done in movie ads) or even just a review’s polarity — positive (“thumbs up”) or negative (“thumbs down”). Document polarity classification poses a significant challenge to data-driven methods, resisting traditional text-categorization techniques (Pang, Lee, and Vaithyanathan, 2002). Previous approaches focused on selecting indicative lexical features (e.g., the word “good”), cl</context>
</contexts>
<marker>Cardie, Wiebe, Wilson, Litman, 2003</marker>
<rawString>Cardie, Claire, Janyce Wiebe, Theresa Wilson, and Diane Litman. 2003. Combining low-level and summary representations of opinions for multiperspective question answering. In AAAI Spring Symposium on New Directions in Question Answering, pages 20–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10337" citStr="Cormen, Leiserson, and Rivest, 1990" startWordPosition="1653" endWordPosition="1657">sign the association scores. Interestingly, Yu and Hatzivassiloglou (2003) compared an individual-preference classifier against a relationship-based method, but didn’t combine the two; the ability to coordinate such algorithms is precisely one of the strengths of our approach. But a crucial advantage specific to the utilization of a minimum-cut-based approach is that we can use maximumfflow algorithms with polynomial asymptotic running times — and near-linear running times in practice — to exactly compute the minimumcost cut(s), despite the apparent intractability of the optimization problem (Cormen, Leiserson, and Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2 In contrast, other graph-partitioning problems that have been previously used to formulate NLP classification problems3 are NP-complete (Hatzivassiloglou and McKeown, 1997; Agrawal et al., 2003; Joachims, 2003). 2Code available at http://www.avglab.com/andrew/soft.html. 3Graph-based approaches to general clustering problems are too numerous to mention here. 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarit</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Cormen, Thomas H., Charles E. Leiserson, and Ronald L. Rivest. 1990. Introduction to Algorithms. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjiv Das</author>
<author>Mike Chen</author>
</authors>
<title>Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Asia Pacific Finance Association Annual Conf.</title>
<date>2001</date>
<publisher>(APFA).</publisher>
<marker>Das, Chen, 2001</marker>
<rawString>Das, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Asia Pacific Finance Association Annual Conf. (APFA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In WWW,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="11171" citStr="Dave, Lawrence, and Pennock, 2003" startWordPosition="1765" endWordPosition="1769">wn, 1997; Agrawal et al., 2003; Joachims, 2003). 2Code available at http://www.avglab.com/andrew/soft.html. 3Graph-based approaches to general clustering problems are too numerous to mention here. 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave, Lawrence, and Pennock, 2003). Third, the correct label can be extracted automatically from rating information (e.g., number of stars). Our data4 contains 1000 positive and 1000 negative reviews all written before 2002, with a cap of 20 reviews per author (312 authors total) per category. We refer to this corpus as the polarity dataset. Default polarity classifiers We tested support vector machines (SVMs) and Naive Bayes (NB). Following Pang et al. (2002), we use unigram-presence features: the ith coordinate of a feature vector is 1 if the corresponding unigram occurs in the input text, 0 otherwise. (For SVMs, the featur</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Dave, Kushal, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In WWW, pages 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luca Dini</author>
<author>Giampaolo Mazzini</author>
</authors>
<title>Opinion classification through information extraction.</title>
<date>2002</date>
<booktitle>In Intl. Conf. on Data Mining Methods and Databases for Engineering, Finance and Other Fields,</booktitle>
<pages>299--310</pages>
<marker>Dini, Mazzini, 2002</marker>
<rawString>Dini, Luca and Giampaolo Mazzini. 2002. Opinion classification through information extraction. In Intl. Conf. on Data Mining Methods and Databases for Engineering, Finance and Other Fields, pages 299–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen D Durbin</author>
<author>J Neal Richter</author>
<author>Doug Warner</author>
</authors>
<title>A system for affective rating of texts.</title>
<date>2003</date>
<booktitle>In KDD Wksp. on Operational Text Classification Systems (OTC-3).</booktitle>
<marker>Durbin, Richter, Warner, 2003</marker>
<rawString>Durbin, Stephen D., J. Neal Richter, and Doug Warner. 2003. A system for affective rating of texts. In KDD Wksp. on Operational Text Classification Systems (OTC-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In 35th ACL/8th EACL,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="10546" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="1681" endWordPosition="1685"> such algorithms is precisely one of the strengths of our approach. But a crucial advantage specific to the utilization of a minimum-cut-based approach is that we can use maximumfflow algorithms with polynomial asymptotic running times — and near-linear running times in practice — to exactly compute the minimumcost cut(s), despite the apparent intractability of the optimization problem (Cormen, Leiserson, and Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2 In contrast, other graph-partitioning problems that have been previously used to formulate NLP classification problems3 are NP-complete (Hatzivassiloglou and McKeown, 1997; Agrawal et al., 2003; Joachims, 2003). 2Code available at http://www.avglab.com/andrew/soft.html. 3Graph-based approaches to general clustering problems are too numerous to mention here. 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave, La</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Hatzivassiloglou, Vasileios and Kathleen McKeown. 1997. Predicting the semantic orientation of adjectives. In 35th ACL/8th EACL, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive learning via spectral graph partitioning.</title>
<date>2003</date>
<booktitle>In Intl. Conf. on Machine Learning (ICML).</booktitle>
<contexts>
<context position="10585" citStr="Joachims, 2003" startWordPosition="1690" endWordPosition="1691">approach. But a crucial advantage specific to the utilization of a minimum-cut-based approach is that we can use maximumfflow algorithms with polynomial asymptotic running times — and near-linear running times in practice — to exactly compute the minimumcost cut(s), despite the apparent intractability of the optimization problem (Cormen, Leiserson, and Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2 In contrast, other graph-partitioning problems that have been previously used to formulate NLP classification problems3 are NP-complete (Hatzivassiloglou and McKeown, 1997; Agrawal et al., 2003; Joachims, 2003). 2Code available at http://www.avglab.com/andrew/soft.html. 3Graph-based approaches to general clustering problems are too numerous to mention here. 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave, Lawrence, and Pennock, 2003). Third, the </context>
</contexts>
<marker>Joachims, 2003</marker>
<rawString>Joachims, Thorsten. 2003. Transductive learning via spectral graph partitioning. In Intl. Conf. on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Liu</author>
<author>Henry Lieberman</author>
<author>Ted Selker</author>
</authors>
<title>A model of textual affect sensing using real-world knowledge.</title>
<date>2003</date>
<booktitle>In Intelligent User Interfaces (IUI),</booktitle>
<pages>125--132</pages>
<marker>Liu, Lieberman, Selker, 2003</marker>
<rawString>Liu, Hugo, Henry Lieberman, and Ted Selker. 2003. A model of textual affect sensing using real-world knowledge. In Intelligent User Interfaces (IUI), pages 125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Montes-y-G´omez</author>
<author>Aurelio L´opez-L´opez</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Text mining as a social thermometer.</title>
<date>1999</date>
<booktitle>In IJCAI Wksp. on Text Mining,</booktitle>
<pages>103--107</pages>
<marker>Montes-y-G´omez, L´opez-L´opez, Gelbukh, 1999</marker>
<rawString>Montes-y-G´omez, Manuel, Aurelio L´opez-L´opez, and Alexander Gelbukh. 1999. Text mining as a social thermometer. In IJCAI Wksp. on Text Mining, pages 103–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Morinaga</author>
<author>Kenji Yamanishi</author>
<author>Kenji Tateishi</author>
<author>Toshikazu Fukushima</author>
</authors>
<title>Mining product reputations on the web. In</title>
<date>2002</date>
<booktitle>KDD,</booktitle>
<pages>341--349</pages>
<note>Industry track.</note>
<marker>Morinaga, Yamanishi, Tateishi, Fukushima, 2002</marker>
<rawString>Morinaga, Satoshi, Kenji Yamanishi, Kenji Tateishi, and Toshikazu Fukushima. 2002. Mining product reputations on the web. In KDD, pages 341– 349. Industry track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1615" citStr="Pang, Lee, and Vaithyanathan, 2002" startWordPosition="220" endWordPosition="224">xtraction and question-answering systems could flag statements and queries regarding opinions rather than facts (Cardie et al., 2003). Also, it has proven useful for companies, recommender systems, and editorial sites to create summaries of people’s experiences and opinions that consist of subjective expressions extracted from reviews (as is commonly done in movie ads) or even just a review’s polarity — positive (“thumbs up”) or negative (“thumbs down”). Document polarity classification poses a significant challenge to data-driven methods, resisting traditional text-categorization techniques (Pang, Lee, and Vaithyanathan, 2002). Previous approaches focused on selecting indicative lexical features (e.g., the word “good”), classifying a document according to the number of such features that occur anywhere within it. In contrast, we propose the following process: (1) label the sentences in the document as either subjective or objective, discarding the latter; and then (2) apply a standard machine-learning classifier to the resulting extract. This can prevent the polarity classifier from considering irrelevant or even potentially misleading text: for example, although the sentence “The protagonist tries to protect her </context>
<context position="11602" citStr="Pang et al. (2002)" startWordPosition="1837" endWordPosition="1840">: witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave, Lawrence, and Pennock, 2003). Third, the correct label can be extracted automatically from rating information (e.g., number of stars). Our data4 contains 1000 positive and 1000 negative reviews all written before 2002, with a cap of 20 reviews per author (312 authors total) per category. We refer to this corpus as the polarity dataset. Default polarity classifiers We tested support vector machines (SVMs) and Naive Bayes (NB). Following Pang et al. (2002), we use unigram-presence features: the ith coordinate of a feature vector is 1 if the corresponding unigram occurs in the input text, 0 otherwise. (For SVMs, the feature vectors are length-normalized). Each default documentlevel polarity classifier is trained and tested on the extracts formed by applying one of the sentencelevel subjectivity detectors to reviews in the polarity dataset. Subjectivity dataset To train our detectors, we need a collection of labeled sentences. Riloff and Wiebe (2003) state that “It is [very hard] to obtain collections of individual sentences that can be easily id</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Qu</author>
<author>James Shanahan</author>
<author>Janyce Wiebe</author>
<author>editors</author>
</authors>
<date>2004</date>
<booktitle>Symposium on Exploring Attitude and Affect in Text: Theories and Applications. AAAI technical report</booktitle>
<pages>04--07</pages>
<publisher>AAAI Spring</publisher>
<marker>Qu, Shanahan, Wiebe, editors, 2004</marker>
<rawString>Qu, Yan, James Shanahan, and Janyce Wiebe, editors. 2004. AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications. AAAI technical report SS-04-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="12104" citStr="Riloff and Wiebe (2003)" startWordPosition="1914" endWordPosition="1917"> Default polarity classifiers We tested support vector machines (SVMs) and Naive Bayes (NB). Following Pang et al. (2002), we use unigram-presence features: the ith coordinate of a feature vector is 1 if the corresponding unigram occurs in the input text, 0 otherwise. (For SVMs, the feature vectors are length-normalized). Each default documentlevel polarity classifier is trained and tested on the extracts formed by applying one of the sentencelevel subjectivity detectors to reviews in the polarity dataset. Subjectivity dataset To train our detectors, we need a collection of labeled sentences. Riloff and Wiebe (2003) state that “It is [very hard] to obtain collections of individual sentences that can be easily identified as subjective or objective”; the polarity-dataset sentences, for example, have not 4Available at www.cs.cornell.edu/people/pabo/moviereview-data/ (review corpus version 2.0). been so annotated.5 Fortunately, we were able to mine the Web to create a large, automaticallylabeled sentence corpus6. To gather subjective sentences (or phrases), we collected 5000 moviereview snippets (e.g., “bold, imaginative, and impossible to resist”) from www.rottentomatoes.com. To obtain (mostly) objective da</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Riloff, Ellen and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Conf. on Natural Language Learning (CoNLL),</booktitle>
<pages>25--32</pages>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Riloff, Ellen, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Conf. on Natural Language Learning (CoNLL), pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pero Subasic</author>
<author>Alison Huettner</author>
</authors>
<title>Affect analysis of text using fuzzy semantic typing.</title>
<date>2001</date>
<journal>IEEE Trans. Fuzzy Systems,</journal>
<volume>9</volume>
<issue>4</issue>
<marker>Subasic, Huettner, 2001</marker>
<rawString>Subasic, Pero and Alison Huettner. 2001. Affect analysis of text using fuzzy semantic typing. IEEE Trans. Fuzzy Systems, 9(4):483–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard M Tong</author>
</authors>
<title>An operational system for detecting and tracking opinions in on-line discussion. SIGIR Wksp. on Operational Text Classification.</title>
<date>2001</date>
<marker>Tong, 2001</marker>
<rawString>Tong, Richard M. 2001. An operational system for detecting and tracking opinions in on-line discussion. SIGIR Wksp. on Operational Text Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="11136" citStr="Turney, 2002" startWordPosition="1763" endWordPosition="1764">glou and McKeown, 1997; Agrawal et al., 2003; Joachims, 2003). 2Code available at http://www.avglab.com/andrew/soft.html. 3Graph-based approaches to general clustering problems are too numerous to mention here. 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave, Lawrence, and Pennock, 2003). Third, the correct label can be extracted automatically from rating information (e.g., number of stars). Our data4 contains 1000 positive and 1000 negative reviews all written before 2002, with a cap of 20 reviews per author (312 authors total) per category. We refer to this corpus as the polarity dataset. Default polarity classifiers We tested support vector machines (SVMs) and Naive Bayes (NB). Following Pang et al. (2002), we use unigram-presence features: the ith coordinate of a feature vector is 1 if the corresponding unigram occurs in the input text,</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, Peter. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<pages>287</pages>
<contexts>
<context position="5174" citStr="Wiebe, 1994" startWordPosition="755" endWordPosition="756">e et al. (2004) is to reveal a document’s sentiment polarity, but they do not evaluate the polarity-classification accuracy that results. 2.2 Context and Subjectivity Detection As with document-level polarity classification, we could perform subjectivity detection on individual sentences by applying a standard classification algorithm on each sentence in isolation. However, modeling proximity relationships between sentences would enable us to leverage coherence: text spans occurring near each other (within discourse boundaries) may share the same subjectivity status, other things being equal (Wiebe, 1994). We would therefore like to supply our algorithms with pair-wise interaction information, e.g., to specify that two particular sentences should ideally receive the same subjectivity label but not state which label this should be. Incorporating such information is somewhat unnatural for classifiers whose input consists simply of individual feature vectors, such as Naive Bayes or SVMs, precisely because such classifiers label each test item in isolation. One could define synthetic features or feature vectors to attempt to overcome this obstacle. However, we propose an alternative that avoids th</context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>Wiebe, Janyce M. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2):233– 287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeonghee Yi</author>
<author>Tetsuya Nasukawa</author>
<author>Razvan Bunescu</author>
<author>Wayne Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques.</title>
<date>2003</date>
<booktitle>In IEEE Intl. Conf. on Data Mining (ICDM).</booktitle>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu, and Wayne Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In IEEE Intl. Conf. on Data Mining (ICDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4295" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="625" endWordPosition="629"> be able to improve polarity classification by removing objective sentences (such as plot summaries in a movie review). We therefore propose, as depicted in Figure 1, to first employ a subjectivity detector that determines whether each sentence is subjective or not: discarding the objective ones creates an extract that should better represent a review’s subjective content to a default polarity classifier. Figure 1: Polarity classification via subjectivity detection. To our knowledge, previous work has not integrated sentence-level subjectivity detection with document-level sentiment polarity. Yu and Hatzivassiloglou (2003) provide methods for sentencelevel analysis and for determining whether a document is subjective or not, but do not combine these two types of algorithms or consider document polarity classification. The motivation behind the singlesentence selection method of Beineke et al. (2004) is to reveal a document’s sentiment polarity, but they do not evaluate the polarity-classification accuracy that results. 2.2 Context and Subjectivity Detection As with document-level polarity classification, we could perform subjectivity detection on individual sentences by applying a standard classification algori</context>
<context position="9776" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="1572" endWordPosition="1575">s cost equal to the partition cost. Thus, our optimization problem reduces to finding minimum cuts. Practical advantages As we have noted, formulating our subjectivity-detection problem in terms of graphs allows us to model item-specific and pairwise information independently. Note that this is a very flexible paradigm. For instance, it is perfectly legitimate to use knowledge-rich algorithms employing deep linguistic knowledge about sentiment indicators to derive the individual scores. And we could also simultaneously use knowledgelean methods to assign the association scores. Interestingly, Yu and Hatzivassiloglou (2003) compared an individual-preference classifier against a relationship-based method, but didn’t combine the two; the ability to coordinate such algorithms is precisely one of the strengths of our approach. But a crucial advantage specific to the utilization of a minimum-cut-based approach is that we can use maximumfflow algorithms with polynomial asymptotic running times — and near-linear running times in practice — to exactly compute the minimumcost cut(s), despite the apparent intractability of the optimization problem (Cormen, Leiserson, and Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2 I</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>