<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000877">
<note confidence="0.76926375">
Practical very large scale CRFs
Thomas Lavergne Olivier Capp´e Franc¸ois Yvon
LIMSI – CNRS T´el´ecom ParisTech Universit´e Paris-Sud 11
lavergne@limsi.fr LTCI – CNRS LIMSI – CNRS
</note>
<email confidence="0.93913">
cappe@enst.fr yvon@limsi.fr
</email>
<sectionHeader confidence="0.992924" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997968782608696">
Conditional Random Fields (CRFs) are
a widely-used approach for supervised
sequence labelling, notably due to their
ability to handle large description spaces
and to integrate structural dependency be-
tween labels. Even for the simple linear-
chain model, taking structure into account
implies a number of parameters and a
computational effort that grows quadrati-
cally with the cardinality of the label set.
In this paper, we address the issue of train-
ing very large CRFs, containing up to hun-
dreds output labels and several billion fea-
tures. Efficiency stems here from the spar-
sity induced by the use of a il penalty
term. Based on our own implementa-
tion, we compare three recent proposals
for implementing this regularization strat-
egy. Our experiments demonstrate that
very large CRFs can be trained efficiently
and that very large models are able to im-
prove the accuracy, while delivering com-
pact parameter sets.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995583941176471">
Conditional Random Fields (CRFs) (Lafferty et
al., 2001; Sutton and McCallum, 2006) constitute
a widely-used and effective approach for super-
vised structure learning tasks involving the map-
ping between complex objects such as strings and
trees. An important property of CRFs is their abil-
ity to handle large and redundant feature sets and
to integrate structural dependency between out-
put labels. However, even for simple linear chain
CRFs, the complexity of learning and inference
This work was partly supported by ANR projects CroTaL
(ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311-
02).
grows quadratically with respect to the number of
output labels and so does the number of structural
features, ie. features testing adjacent pairs of la-
bels. Most empirical studies on CRFs thus ei-
ther consider tasks with a restricted output space
(typically in the order of few dozens of output la-
bels), heuristically reduce the use of features, es-
pecially of features that test pairs of adjacent la-
bels1, and/or propose heuristics to simulate con-
textual dependencies, via extended tests on the ob-
servations (see discussions in, eg., (Punyakanok
et al., 2005; Liang et al., 2008)). Limitating the
feature set or the number of output labels is how-
ever frustrating for many NLP tasks, where the
type and number of potentially relevant features
are very large. A number of studies have tried to
alleviate this problem. Pal et al. (2006) propose
to use a “sparse” version of the forward-backward
algorithm during training, where sparsity is en-
forced through beam pruning. Related ideas are
discussed by Dietterich et al. (2004); by Cohn
(2006), who considers “generalized” feature func-
tions; and by Jeong et al. (2009), who use approx-
imations to simplify the forward-backward recur-
sions. In this paper, we show that the sparsity that
is induced by il-penalized estimation of CRFs can
be used to reduce the total training time, while
yielding extremely compact models. The benefits
of sparsity are even greater during inference: less
features need to be extracted and included in the
potential functions, speeding up decoding with a
lesser memory footprint. We study and compare
three different ways to implement il penalty for
CRFs that have been introduced recently: orthant-
wise Quasi Newton (Andrew and Gao, 2007),
stochastic gradient descent (Tsuruoka et al., 2009)
and coordinate descent (Sokolovska et al., 2010),
concluding that these methods have complemen-
</bodyText>
<footnote confidence="0.956424">
1In CRFsuite (Okazaki, 2007), it is even impossible to
jointly test a pair of labels and a test on the observation, bi-
grams feature are only of the form f(yt−1, yt).
</footnote>
<page confidence="0.913236">
504
</page>
<note confidence="0.945797">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999965709677419">
tary strengths and weaknesses. Based on an effi-
cient implementation of these algorithms, we were
able to train very large CRFs containing more than
a hundred of output labels and up to several billion
features, yielding results that are as good or better
than the best reported results for two NLP bench-
marks, text phonetization and part-of-speech tag-
ging.
Our contribution is therefore twofold: firstly a
detailed analysis of these three algorithms, dis-
cussing implementation, convergence and com-
paring the effect of various speed-ups. This
comparison is made fair and reliable thanks to
the reimplementation of these techniques in the
same software package. Second, the experimen-
tal demonstration that using large output label sets
is doable and that very large feature sets actually
help improve prediction accuracy. In addition, we
show how sparsity in structured feature sets can
be used in incremental training regimes, where
long-range features are progressively incorporated
in the model insofar as the shorter range features
have proven useful.
The rest of the paper is organized as follows: we
first recall the basics of CRFs in Section 2, and dis-
cuss three ways to train CRFs with a `1 penalty in
Section 3. We then detail several implementation
issues that need to be addressed when dealing with
massive feature sets in Section 4. Our experiments
are reported in Section 5. The main conclusions of
this study are drawn in Section 6.
</bodyText>
<sectionHeader confidence="0.984527" genericHeader="method">
2 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.99986775">
In this section, we recall the basics of Conditional
Random Fields (CRFs) (Lafferty et al., 2001; Sut-
ton and McCallum, 2006) and introduce the nota-
tions that will be used throughout.
</bodyText>
<subsectionHeader confidence="0.974215">
2.1 Basics
</subsectionHeader>
<bodyText confidence="0.936891">
CRFs are based on the following model
</bodyText>
<equation confidence="0.994691">
( K
pθ(y|x) = Zθ(x) exp X θkFk(x,y) (1)
k=1
</equation>
<bodyText confidence="0.999886">
where x = (x1, ...,xT) and y = (y1, ... , yT)
are, respectively, the input and output sequences2,
and Fk(x, y) is equal to PTt=1 fk(yt−1, yt, xt),
where {fk}1≤k≤K is an arbitrary set of feature
</bodyText>
<footnote confidence="0.9521625">
2Our implementation also includes a special label y0, that
is always observed and marks the beginning of a sequence.
</footnote>
<bodyText confidence="0.9982375">
functions and {θk}1≤k≤K are the associated pa-
rameter values. We denote by Y and X, respec-
tively, the sets in which yt and xt take their values.
The normalization factor in (1) is defined by
</bodyText>
<equation confidence="0.99871">
( K
exp X θkFk(x, y) . (2)
k=1
</equation>
<bodyText confidence="0.999886666666667">
The most common choice of feature functions is to
use binary tests. In the sequel, we distinguish be-
tween two types of feature functions: unigram fea-
tures fy,x, associated with parameters µy,x, and bi-
gram features fyl,y,x, associated with parameters
λy/,y,x. These are defined as
</bodyText>
<equation confidence="0.94767">
fy,x(yt−1, yt, xt) = 1(yt = y, xt = x)
fy/,y,x(yt−1, yt, xt) = 1(yt−1 = y0, yt = y, xt = x)
</equation>
<bodyText confidence="0.999851684210527">
where 1(cond.) is equal to 1 when the condition
is verified and to 0 otherwise. In this setting, the
number of parameters K is equal to |Y |2x|X|train,
where |· |denotes the cardinal and |X|train refers to
the number of configurations of xt observed dur-
ing training. Thus, even in moderate size applica-
tions, the number of parameters can be very large,
mostly due to the introduction of sequential de-
pendencies in the model. This also explains why it
is hard to train CRFs with dependencies spanning
more than two adjacent labels. Using only uni-
gram features {fy,x}(y,x)∈Y ×X results in a model
equivalent to a simple bag-of-tokens position-
by-position logistic regression model. On the
other hand, bigram features {fy/,y,x}(y,x)∈Y 2×X
are helpful in modelling dependencies between
successive labels. The motivations for using si-
multaneously both types of feature functions are
evaluated experimentally in Section 5.
</bodyText>
<subsectionHeader confidence="0.999206">
2.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.9992482">
Given N independent sequences {x(i), y(i)}Ni=1,
where x(i) and y(i) contain T(i) symbols, condi-
tional maximum likelihood estimation is based on
the minimization, with respect to θ, of the negated
conditional log-likelihood of the observations
</bodyText>
<equation confidence="0.999350333333333">
l(θ) = − XN log pθ(y(i)|x(i)) (3)
i=1
XN( )
K
log Zθ(x(i)) − XθkFk(x(i),y(i))
i=1 k=1
</equation>
<bodyText confidence="0.6996125">
This term is usually complemented with an addi-
tional regularization term so as to avoid overfitting
</bodyText>
<equation confidence="0.9967365">
XZθ(x) =
Y∈Y T
</equation>
<page confidence="0.982148">
505
</page>
<bodyText confidence="0.990395333333333">
(see Section 3.1 below). The gradient of l(θ) is
where Epθ(y|x) denotes the conditional expecta-
tion given the observation sequence, i.e.
</bodyText>
<equation confidence="0.999666">
Epθ(y|x) fk(yt−1, yt, x(i)
t ) =
X fk(y, y0, xt) Pθ(yt−1 = y0, yt = y|x) (5)
(y,,y)∈Y 2
</equation>
<bodyText confidence="0.976275">
Although l(θ) is a smooth convex function, its op-
timum cannot be computed in closed form, and
l(θ) has to be optimized numerically. The com-
putation of its gradient implies to repeatedly com-
pute the conditional expectation in (5) for all in-
put sequences x(i) and all positions t. The stan-
dard approach for computing these expectations
is inspired by the forward-backward algorithm for
hidden Markov models: using the notations intro-
duced above, the algorithm implies the computa-
tion of the forward
</bodyText>
<equation confidence="0.974538333333333">
(
α1(y) = exp(µy,x1 + λy0,y,x1)
αt+1(y) = Py, αt(y0) exp(µy,xt+1 + λy,,y,xt+1)
</equation>
<bodyText confidence="0.653824">
and backward recursions
</bodyText>
<equation confidence="0.992474833333333">
( βTi(y) = 1
βt(y0) = P y βt+1(y) exp(µy,xt+1 + λy,,y,xt+1),
for all indices 1 ≤ t ≤ T and all labels y ∈ Y .
Then, Zθ(x) = Py αT (y) and the pairwise prob-
abilities Pθ(yt = y0, yt+1 = y|x) are given by
αt(y0) exp(µy,xt+1 + λy,,y,xt+1)βt+1(y)/Zθ(x)
</equation>
<bodyText confidence="0.995616">
These recursions require a number of operations
that grows quadratically with |Y |.
</bodyText>
<sectionHeader confidence="0.96013" genericHeader="method">
3 V Regularization in CRFs
</sectionHeader>
<subsectionHeader confidence="0.989031">
3.1 Regularization
</subsectionHeader>
<bodyText confidence="0.998182733333334">
The standard approach for parameter estimation in
CRFs consists in minimizing the logarithmic loss
l(θ) defined by (3) with an additional `2 penalty
term ρ22 kθk22, where ρ2 is a regularization parame-
ter. The objective function is then a smooth convex
function to be minimized over an unconstrained
parameter space. Hence, any numerical optimiza-
tion strategy may be used and practical solutions
include limited memory BFGS (L-BFGS) (Liu
and Nocedal, 1989), which is used in the popu-
lar CRF++ (Kudo, 2005) and CRFsuite (Okazaki,
2007) packages; conjugate gradient (Nocedal and
Wright, 2006) and Stochastic Gradient Descent
(SGD) (Bottou, 2004; Vishwanathan et al., 2006),
used in CRFsgd (Bottou, 2007). The only caveat
is to avoid numerical optimizers that require the
full Hessian matrix (e.g., Newton’s algorithm) due
to the size of the parameter vector in usual appli-
cations of CRFs.
The most significant alternative to `2 regulariza-
tion is to use a `1 penalty term ρ1kθk1: such regu-
larizers are able to yield sparse parameter vectors
in which many component have been zeroed (Tib-
shirani, 1996). Using a `1 penalty term thus im-
plicitly performs feature selection, where ρ1 con-
trols the amount of regularization and the number
of extracted features. In the following, we will
jointly use both penalty terms, yielding the so-
called elastic net penalty (Zhou and Hastie, 2005)
which corresponds to the objective function
</bodyText>
<equation confidence="0.973197">
l(θ) + ρ1kθk1 + ρ22 kθk2 (6)
2
</equation>
<bodyText confidence="0.999990583333333">
The use of both penalty terms makes it possible
to control the number of non zero coefficients and
to avoid the numerical problems that might occur
in large dimensional parameter settings (see also
(Chen, 2009)). However, the introduction of a `1
penalty term makes the optimization of (6) more
problematic, as the objective function is no longer
differentiable in 0. Various strategies have been
proposed to handle this difficulty. We will only
consider here exact approaches and will not dis-
cuss heuristic strategies such as grafting (Perkins
et al., 2003; Riezler and Vasserman, 2004).
</bodyText>
<subsectionHeader confidence="0.999369">
3.2 Quasi Newton Methods
</subsectionHeader>
<bodyText confidence="0.9999791">
To deal with `1 penalties, a simple idea is that of
(Kazama and Tsujii, 2003), originally introduced
for maxent models. It amounts to reparameteriz-
ing θk as θk = θ+k −θ−k , where θ+k and θ−k are pos-
itive. The `1 penalty thus becomes ρ1(θ+ − θ−).
In this formulation, the objective function recovers
its smoothness and can be optimized with conven-
tional algorithms, subject to domain constraints.
Optimization is straightforward, but the number
of parameters is doubled and convergence is slow
</bodyText>
<equation confidence="0.999668529411765">
Epθ(y|x(i)) fk(yt−1, yt, x(i)
t )
fk(y(i)
t−1, y(i)
t , x(i)
t ) (4)
XN
i=1
∂l(θ)
∂θk
TX(i)
t=1
−
TX(i)
t=1
XN
i=1
</equation>
<page confidence="0.9836">
506
</page>
<bodyText confidence="0.99982152173913">
(Andrew and Gao, 2007): the procedure lacks a
mechanism for zeroing out useless parameters.
A more efficient strategy is the orthant-wise
quasi-Newton (OWL-QN) algorithm introduced in
(Andrew and Gao, 2007). The method is based on
the observation that the `1 norm is differentiable
when restricted to a set of points in which each
coordinate never changes its sign (an “orthant”),
and that its second derivative is then zero, mean-
ing that the `1 penalty does not change the Hessian
of the objective on each orthant. An OWL-QN
update then simply consists in (i) computing the
Newton update in a well-chosen orthant; (ii) per-
forming the update, which might cause some com-
ponent of the parameter vector to change sign; and
(iii) projecting back the parameter value onto the
initial orthant, thereby zeroing out those compo-
nents. In (Gao et al., 2007), the authors show that
OWL-QN is faster than the algorithm proposed by
Kazama and Tsujii (2003) and can perform model
selection even in very high-dimensional problems,
with no loss of performance compared to the use
of `2 penalty terms.
</bodyText>
<subsectionHeader confidence="0.996585">
3.3 Stochastic Gradient Descent
</subsectionHeader>
<bodyText confidence="0.9998924">
Stochastic gradient (SGD) approaches update the
parameter vector based on an crude approximation
of the gradient (4), where the computation of ex-
pectations only includes a small batch of observa-
tions. SGD updates have the following form
</bodyText>
<equation confidence="0.948290666666667">
∂l(θ)
θk ← θk + η ,(7)
∂θk
</equation>
<bodyText confidence="0.9998989">
where η is the learning rate. In (Tsuruoka et al.,
2009), various ways of adapting this update to `1-
penalized likelihood functions are discussed. Two
effective ideas are proposed: (i) only update pa-
rameters that correspond to active features in the
current observation, (ii) keep track of the cumu-
lated penalty zk that θk should have received, had
the gradient been computed exactly, and use this
value to “clip” the parameter value. This is imple-
mented by patching the update (7) as follows
</bodyText>
<construct confidence="0.765336333333333">
�
if (θk &gt; 0) θk ← max(0, θk − zk) (8)
else if (θk &lt; 0) θk ← min(0, θk − zk)
</construct>
<bodyText confidence="0.9998142">
Based on a study of three NLP benchmarks, the
authors of (Tsuruoka et al., 2009) claim this ap-
proach to be much faster than the orthant-wise ap-
proach and yet to yield very comparable perfor-
mance, while selecting slightly larger feature sets.
</bodyText>
<subsectionHeader confidence="0.990786">
3.4 Block Coordinate Descent
</subsectionHeader>
<bodyText confidence="0.9484925">
The coordinate descent approach of Dudik et
al. (2004) and Friedman et al. (2008) uses the
fact that optimizing a mono-dimensional quadratic
function augmented with a `1 penalty can be per-
formed analytically. For arbitrary functions, this
idea can be adapted by considering quadratic ap-
proximations of the objective around the current
value θ¯
</bodyText>
<equation confidence="0.974730230769231">
1∂2l(¯θ)
¯θk) + (θk − ¯θk)2
2 ∂θ2 k
+ ρ1|θk |+ ρ22 θ2k + Cst (9)
The minimizer of the approximation (9) is simply
s{∂a8B)θk − al( ), ρ1}
∂2l( � + ρ2
θ)
∂θ2
where s is the soft-thresholding function
{ z − ρ if z &gt; ρ
z + ρ if z &lt; −ρ
0 otherwise
</equation>
<bodyText confidence="0.999358291666667">
Coordinate descent is ported to CRFs in
(Sokolovska et al., 2010). Making this scheme
practical requires a number of adaptations,
including (i) approximating the second order
term in (10), (ii) performing updates in block,
where a block contains the |Y  |× |Y + 1 |fea-
tures νy1,y,x and λy,x for a fixed test x on the
observation sequence and (iii) approximating the
Hessian for a block by its diagonal terms. (ii)
is specially critical, as repeatedly cycling over
individual features to perform the update (10)
is only possible with restricted sets of features.
The block update schemes uses the fact that
all features within a block appear in the same
set of sequences, which means that most of the
computations needed to perform theses updates
can be shared within the block. One advantage
of the resulting algorithm, termed BCD in the
following, is that the update of θk only involves
carrying out the forward-backward recursions for
the set of sequences that contain symbols x such
that at least one {fk(y&apos;, y, x)}(y y1)EY 2 is non
null, which can be much smaller than the whole
training set.
</bodyText>
<equation confidence="0.9991358">
∂l( θ)
lk,¯θ(θk) = (θk −
∂θk
s(z, ρ) =
θk =
</equation>
<page confidence="0.989724">
507
</page>
<sectionHeader confidence="0.995075" genericHeader="method">
4 Implementation Issues
</sectionHeader>
<bodyText confidence="0.99996725">
Efficiently processing very-large feature and ob-
servation sets requires to pay attention to many
implementation details. In this section, we present
several optimizations devised to speed up training.
</bodyText>
<subsectionHeader confidence="0.996908">
4.1 Sparse Forward-Backward Recursions
</subsectionHeader>
<bodyText confidence="0.976667">
For all algorithms, the computation time is domi-
nated by the evaluations of the gradient: our im-
plementation takes advantage of the sparsity to ac-
celerate these computations. Assume the set of bi-
gram features {λy0,y,xt+1}(y0,y)∈Y 2 is sparse with
only r(xt+1) « |Y |2 non null values and define
</bodyText>
<equation confidence="0.974533555555556">
the |Y  |x |Y  |sparse matrix
Mt(y0, y) = exp(λy0,y,xt) − 1.
Using M, the forward-backward recursions are
�αt(y) = ut−1(y0) + � ut−1(y0)Mt(y0, y)
y0 y0
βt(y0) = � vt+1(y) + � Mt+1(y0, y)vt+1(y)
y y
with ut−1(y) = exp(µy,xt)αt−1(y) and
vt+1(y) = exp(µy,xt+1)βt+1(y). (Sokolovska et
</equation>
<bodyText confidence="0.998742205882353">
al., 2010) explains how computational savings can
be obtained using the fact that the vector/matrix
products in the recursions above only involve
the sparse matrix Mt+1(y0, y). They can thus be
computed with exactly r(xt+1) multiplications
instead of |Y |2. The same idea can be used
when the set {µy,xt+1}y∈Y of unigram features is
sparse. Using this implementation, the complexity
of the forward-backward procedure for x(�) can be
made proportional to the average number of active
features per position, which can be much smaller
than the number of potentially active features.
For BCD, forward-backward can even be made
slightly faster. When computing the gradient wrt.
features λy,x and µy0,y,x (for all the values of y
and y0) for sequence x(�), assuming that x only
occurs once in x(�) at position t, all that is needed
is α0t(y), bt0 &lt; t and β0t(y), bt0 &gt; t. ZB(x) is then
recovered as Ey αt(y)βt(y). Forward-backward
recursions can thus be truncated: in our experi-
ments, this divided the computational cost by 1,8
on average.
Note finally that forward-backward is per-
formed on a per-observation basis and is easily
parallelized (see also (Mann et al., 2009) for more
powerful ways to distribute the computation when
dealing with very large datasets). In our imple-
mentation, it is distributed on all available cores,
resulting in significant speed-ups for OWL-QN
and L-BFGS; for BCD the gain is less acute, as
parallelization only helps when updating the pa-
rameters for a block of features that are occur in
many sequences; for SGD, with batches of size
one, this parallelization policy is useless.
</bodyText>
<subsectionHeader confidence="0.986883">
4.2 Scaling
</subsectionHeader>
<bodyText confidence="0.999878444444444">
Most existing implementations of CRFs, eg.
CRF++ and CRFsgd perform the forward-
backward recursions in the log-domain, which
guarantees that numerical over/underflows are
avoided no matter the length T(�) of the sequence.
It is however very inefficient from an implementa-
tion point of view, due to the repeated calls to the
exp() and log() functions. As an alternative way
of avoiding numerical problems, our implementa-
tion, like crfSuite’s, resorts to “scaling”, a solution
commonly used for HMMs. Scaling amounts to
normalizing the values of αt and βt to one, making
sure to keep track of the cumulated normalization
factors so as to compute ZB(x) and the conditional
expectations Epo(y|x). Also note that in our imple-
mentation, all the computations of exp(x) are vec-
torized, which provides an additional speed up of
about 20%.
</bodyText>
<subsectionHeader confidence="0.99453">
4.3 Optimization in Large Parameter Spaces
</subsectionHeader>
<bodyText confidence="0.999495545454545">
Processing very large feature vectors, up to bil-
lions of components, is problematic in many ways.
Sparsity has been used here to speed up forward-
backward, but we have made no attempt to accel-
erate the computation of the OWL-QN updates,
which are linear in the size of the parameter vector.
Of the three algorithms, BCD is the most affected
by increases in the number of features, or more
precisely, in the number of features blocks, where
one block correspond to a specific test of the ob-
servation. In the worst case scenario, each block
may require to visit all the training instances,
yielding terrible computational wastes. In prac-
tice though, most blocks only require to process
a small fraction of the training set, and the ac-
tual complexity depends on the average number of
blocks per observations. Various strategies have
been tried to further accelerate BCD, such as pro-
cessing blocks that only visit one observation in
parallel and updating simultaneously all the blocks
that visit all the training instances, leading to a
small speed-up on the POS-tagging task.
</bodyText>
<page confidence="0.988836">
508
</page>
<bodyText confidence="0.999935142857143">
Working with billions of features finally re-
quires to worry also about memory usage. In this
respect, BCD is the most efficient, as it only re-
quires to store one K-dimensional vector for the
parameter itself. SGD requires two such vectors,
one for the parameter and one for storing the zk
(see Eq. (8)). In comparison, OWL-QN requires
much more memory, due to the internals of the
update routines, which require several histories of
the parameter vector and of its gradient. Typi-
cally, our implementation necessitates in the order
of a dozen K-dimensional vectors. Parallelization
only makes things worse, as each core will also
need to maintain its own copy of the gradient.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999996878787879">
Our experiments use two standard NLP tasks,
phonetization and part-of-speech tagging, chosen
here to illustrate two very different situations, and
to allow for comparison with results reported else-
where in the literature. Unless otherwise men-
tioned, the experiments use the same protocol: 10
fold cross validation, where eight folds are used
for training, one for development, and one for test-
ing. Results are reported in terms of phoneme er-
ror rates or tag error rates on the test set.
Comparing run-times can be a tricky matter, es-
pecially when different software packages are in-
volved. As discussed above, the observed run-
times depend on many small implementation de-
tails. As the three algorithms share as much code
as possible, we believe the comparison reported
hereafter to be fair and reliable. All experiments
were performed on a server with 64G of memory
and two Xeon processors with 4 cores at 2.27 Ghz.
For comparison, all measures of run-times include
the cumulated activity of all cores and give very
pessimistic estimates of the wall time, which can
be up to 7 times smaller. For OWL-QN, we use 5
past values of the gradient to approximate the in-
verse of the Hessian matrix: increasing this value
had no effect on accuracy or convergence and was
detrimental to speed; for SGD, the learning rate
parameter was tuned manually.
Note that we have not spent much time optimiz-
ing the values of p1 and p2. Based on a pilot study
on Nettalk, we found that taking p1 = .5 and p2 in
the order of 10−5 to yield nearly optimal perfor-
mance, and have used these values throughout.
</bodyText>
<subsectionHeader confidence="0.8914465">
5.1 Tasks and Settings
5.1.1 Nettalk
</subsectionHeader>
<bodyText confidence="0.97719187755102">
Our first benchmark is the word phonetization
task, using the Nettalk dictionary (Sejnowski and
Rosenberg, 1987). This dataset contains approxi-
mately 20,000 English word forms, their pronun-
ciation, plus some prosodic information (stress
markers for vowels, syllabic parsing for con-
sonants). Grapheme and phoneme strings are
aligned at the character level, thanks to the use of
a “null sound” in the latter string when it is shorter
than the former; likewise, each prosodic mark is
aligned with the corresponding letter. We have de-
rived two test conditions from this database. The
first one is standard and aims at predicting the pro-
nunciation information only. In this setting, the set
of observations (X) contains 26 graphemes, and
the output label set contains |Y  |= 51 phonemes.
The second condition aims at jointly predict-
ing phonemic and prosodic information3. The rea-
sons for designing this new condition are twofold:
firstly, it yields a large set of composite labels
(|Y  |= 114) and makes the problem computation-
ally challenging. Second, it allows to quantify how
much the information provided by the prosodic
marks help predict the phonemic labels. Both in-
formation are quite correlated, as the stress mark
and the syllable openness, for instance, greatly in-
fluence the realization of some archi-phonemes.
The features used in Nettalk experiments take
the form fy,,,, (unigram) and fy0,y,,,, (bigram),
where w is a n-gram of letters. The n-grm feature
sets (n = 11, 3, 5, 7}) includes all features testing
embedded windows of k letters, for all 0 &lt; k &lt; n;
the n-grm- setting is similar, but only includes
the window of length n; in the n-grm+ setting,
we add features for odd-size windows; in the n-
grm++ setting, we add all sequences of letters up
to size n occurring in current window. For in-
stance, the active bigram features at position t = 2
in the sequence x=’lemma’ are as follows: the 3-
grm feature set contains fy,y0, fy,y0,e and fy0,y,lem;
only the latter appears in the 3-grm- setting. In
the 3-grm+ feature set, we also have fy0,y,le and
fy0,y,em. The 3-grm++ feature set additionally in-
cludes fy0,y,l and fy0,y,m. The number of features
ranges from 360 thousands (1-grm setting) to 1.6
billion (7-grm).
3Given the design of the Nettalk dictionary, this experi-
ment required to modify the original database so as to reas-
sign prosodic marks to phonemes, rather than to letters.
</bodyText>
<page confidence="0.994338">
509
</page>
<table confidence="0.999438">
Features With Without
Nettalk
3-grm 10.74% 14.3M 14.59% 0.3M
5-grm 8.48% 132.5M 11.54% 2.5M
POS tagging
base 2.91% 436.7M 3.47% 70.2M
</table>
<tableCaption confidence="0.941253666666667">
Table 1: Features jointly testing label pairs and
the observation are useful (error rates and features
counts.)
</tableCaption>
<table confidence="0.998054">
E2 E1-sparse E1 % zero
1-grm 84min 41min 57min 44.6%
3-grm- 65min 16min 44min 99.6%
3-grm 72min 48min 58min 19.9%
</table>
<tableCaption confidence="0.944272">
Table 2: Sparse vs standard forward-backward
(training times and percentages of sparsity of M)
</tableCaption>
<subsubsectionHeader confidence="0.983396">
5.1.2 Part-of-Speech Tagging
</subsubsectionHeader>
<bodyText confidence="0.999979315789474">
Our second benchmark is a part-of-speech (POS)
tagging task using the PennTreeBank corpus
(Marcus et al., 1993), which provides us with a
quite different condition. For this task, the number
of labels is smaller (|Y  |= 45) than for Nettalk,
and the set of observations is much larger (IXI =
43207). This benchmark, which has been used in
many studies, allows for direct comparisons with
other published work. We thus use a standard ex-
perimental set-up, where sections 0-18 of the Wall
Street Journal are used for training, sections 19-21
for development, and sections 22-24 for testing.
Features are also standard and follow the design
of (Suzuki and Isozaki, 2008) and test the current
words (as written and lowercased), prefixes and
suffixes up to length 4, and typographical charac-
teristics (case, etc.) of the words. Our baseline
feature set also contains tests on individual and
pairs of words in a window of 5 words.
</bodyText>
<subsectionHeader confidence="0.999915">
5.2 Using Large Feature Sets
</subsectionHeader>
<bodyText confidence="0.999882875">
The first important issue is to assess the benefits
of using large feature sets, notably including fea-
tures testing both a bigram of labels and an obser-
vation. Table 1 compares the results obtained with
and without these features for various setting (us-
ing OWL-QN to perform the optimization), sug-
gesting that for the tasks at hand, these features
are actually helping.
</bodyText>
<table confidence="0.992492">
E2 E1 Elastic-net
1-grm 17.81% 17.86% 17.79%
3-grm 10.62% 10.74% 10.70%
5-grm 8.50% 8.45% 8.48%
</table>
<tableCaption confidence="0.982077">
Table 3: Error rates of the three regularizers on the
Nettalk task.
</tableCaption>
<subsectionHeader confidence="0.992897">
5.3 Speed, Sparsity, Convergence
</subsectionHeader>
<bodyText confidence="0.9999704">
The training speed depends of two main factors:
the number of iterations needed to achieve conver-
gence and the computational cost of one iteration.
In this section, we analyze and compare the run-
time efficiency of the three optimizers.
</bodyText>
<subsectionHeader confidence="0.927552">
5.3.1 Convergence
</subsectionHeader>
<bodyText confidence="0.999831692307692">
As far as convergence is concerned, the two forms
of regularization (E2 and E1) yield the same per-
formance (see Table 3), and the three algorithms
exhibit more or less the same behavior. They
quickly reach an acceptable set of active param-
eters, which is often several orders of magnitude
smaller than the whole parameter set (see results
below in Table 4 and 5). Full convergence, re-
flected by a stabilization of the objective function,
is however not so easily achieved. We have of-
ten observed a slow, yet steady, decrease of the
log-loss, accompanied with a diminution of the
number of active features as the number of iter-
ations increases. Based on this observation, we
have chosen to stop all algorithms based on their
performance on an independent development set,
allowing a fair comparison of the overall training
time; for OWL-QN, it allowed to divide the total
training time by almost 2.
It has finally often been found useful to fine
tune the non-zero parameters by running a final
handful of L-BFGS iterations using only a small
E2 penalty; at this stage, all the other features are
removed from the model. This had a small impact
BCD and SGD’s performance and allowed them to
catch up with OWL-QN’s performance.
</bodyText>
<subsubsectionHeader confidence="0.691028">
5.3.2 Sparsity and the Forward-Backward
</subsubsectionHeader>
<bodyText confidence="0.999939375">
As explained in section 4.1, the forward-backward
algorithm can be written so as to use the sparsity
of the matrix My�y,�,,. To evaluate the resulting
speed-up, we ran a series of experiments using
Nettalk (see Table 2). In this table, the 3-grm- set-
ting corresponds to maximum sparsity for M, and
training with the sparse algorithm is three times
faster than with the non-sparse version. Throwing
</bodyText>
<page confidence="0.988108">
510
</page>
<table confidence="0.999431222222222">
Method Iter. # Feat. Error Time
OWL-QN 1-grm 63.4 4684 17.79% 11min
7-grm 140.2 38214 8.12% 1h02min
5-grm+ 141.0 43429 7.89% 1h37min
SGD 1-grm 21.4 3540 18.21% 9min
5-grm+ 28.5 34319 8.01% 45min
BCD 1-grm 28.2 5017 18.27% 27min
7-grm 9.2 3692 8.21% 1h22min
5-grm+ 8.7 47675 7.91% 2h18min
</table>
<tableCaption confidence="0.999832">
Table 4: Performance on Nettalk
</tableCaption>
<bodyText confidence="0.999914777777778">
in more features has the effect of making M much
more dense, mitigating the benefits of the sparse
recursions. Nevertheless, even for very large fea-
ture sets, the percentage of zeros in M averages
20% to 30%, and the sparse version remains 10 to
20% faster than the non-sparse one. Note that the
non-sparse version is faster with a E&apos; penalty term
than with only the E2 term: this is because exp(0)
is faster to evaluate than exp(x) when x 7� 0.
</bodyText>
<subsectionHeader confidence="0.967569">
5.3.3 Training Speed and Test Accuracy
</subsectionHeader>
<bodyText confidence="0.999591034482759">
Table 4 displays the results achieved on the Nettalk
task. The three algorithms yield very compara-
ble accuracy results, and deliver compact models:
for the 5-gram+ setting, only 50,000 out of 250
million features are selected. SGD is the fastest
of the three, up to twice as fast as OWL-QN and
BCD depending on the feature set. The perfor-
mance it achieves are consistently slightly worst
than the other optimizers, and only catch up when
the parameters are fine-tuned (see above). There
are not so many comparisons for Nettalk with
CRFs, due to the size of the label set. Our results
compare favorably with those reported in (Pal et
al., 2006), where the accuracy attains 91.7% us-
ing 19075 examples for training and 934 for test-
ing, and with those in (Jeong et al., 2009) (88.4%
accuracy with 18,000 (2,000) training (test) in-
stances). Table 5 gives the results obtained for
the larger Nettalk+prosody task. Here, we only
report the results obtained with SGD and BCD.
For OWL-QN, the largest model we could han-
dle was the 3-grm model, which contained 69 mil-
lion features, and took 48min to train. Here again,
performance steadily increase with the number of
features, showing the benefits of large-scale mod-
els. We lack comparisons for this task, which
seems considerably harder than the sole phone-
tization task, and all systems seem to plateau
around 13.5% accuracy. Interestingly, simulta-
</bodyText>
<table confidence="0.99932">
Method Error Time
SGD 5-grm 14.71% / 8.11% 55min
5-grm+ 13.91% / 7.51% 2h45min
BCD 5-grm 14.57% / 8.06% 2h46min
7-grm 14.12% / 7.86% 3h02min
5-grm+ 13.85% / 7.47% 7h14min
5-grm++ 13.69% / 7.36% 16h03min
</table>
<tableCaption confidence="0.958645">
Table 5: Performance on Nettalk+prosody. Error
is given for both joint labels and phonemic labels.
</tableCaption>
<bodyText confidence="0.9997335">
neously predicting the phoneme and its prosodic
markers allows to improve the accuracy on the pre-
diction of phonemes, which improves of almost a
half point as compared to the best Nettalk system.
For the POS tagging task, BCD appears to be
unpractically slower to train than the others ap-
proaches (SGD takes about 40min to train, OWL-
QN about 1 hour) due the simultaneous increase
in the sequence length and in the number of ob-
servations. As a result, one iteration of BCD typi-
cally requires to repeatedly process over and over
the same sequences: on average, each sequence is
visited 380 times when we use the baseline fea-
ture set. This technique should reserved for tasks
where the number of blocks is small, or, as below,
when memory usage is an issue.
</bodyText>
<subsectionHeader confidence="0.990883">
5.4 Structured Feature Sets
</subsectionHeader>
<bodyText confidence="0.999889608695652">
In many tasks, the ambiguity of tokens can be re-
duced by looking up increasingly large windows
of local context. This strategy however quickly
runs into a combinatorial increase of the number
of features. A side note of the Nettalk experiments
is that when using embedded features, the active
feature set tends to reflect this hierarchical organi-
zation. This means that when a feature testing a
n-gram is active, in most cases, the features for all
embedded k-grams are also selected.
Based on this observation, we have designed
an incremental training strategy for the POS tag-
ging task, where more specific features are pro-
gressively incorporated into the model if the cor-
responding less specific feature is active. This ex-
periment used BCD, which is the most memory ef-
ficient algorithm. The first iteration only includes
tests on the current word. During the second it-
eration, we add tests on bigram of words, on suf-
fixes and prefixes up to length 4. After four itera-
tions, we throw in features testing word trigrams,
subject to the corresponding unigram block being
active. After 6 iterations, we finally augment the
</bodyText>
<page confidence="0.990582">
511
</page>
<bodyText confidence="0.999647333333333">
model with windows of length 5, subject to the
corresponding trigram being active. After 10 iter-
ations, the model contains about 4 billion features,
out of which 400,000 are active. It achieves an
error rate of 2.63% (resp. 2.78%) on the develop-
ment (resp. test) data, which compares favorably
with some of the best results for this task (for in-
stance (Toutanova et al., 2003; Shen et al., 2007;
Suzuki and Isozaki, 2008)).
</bodyText>
<sectionHeader confidence="0.985858" genericHeader="conclusions">
6 Conclusion and Perspectives
</sectionHeader>
<bodyText confidence="0.99993696875">
In this paper, we have discussed various ways to
train extremely large CRFs with a &apos; penalty term
and compared experimentally the results obtained,
both in terms of training speed and of accuracy.
The algorithms studied in this paper have com-
plementary strength and weaknesses: OWL-QN is
probably the method of choice in small or moder-
ate size applications while BCD is most efficient
when using very large feature sets combined with
limited-size observation alphabets; SGD comple-
mented with fine tuning appears to be the preferred
choice in most large-scale applications. Our anal-
ysis demonstrate that training large-scale sparse
models can be done efficiently and allows to im-
prove over the performance of smaller models.
The CRF package developed in the course of this
study implements many algorithmic optimizations
and allows to design innovative training strategies,
such as the one presented in section 5.4. This
package is released as open-source software and
is available at http://wapiti.limsi.fr.
In the future, we intend to study how spar-
sity can be used to speed-up training in the face
of more complex dependency patterns (such as
higher-order CRFs or hierarchical dependency
structures (Rozenknop, 2002; Finkel et al., 2008).
From a performance point of view, it might also
be interesting to combine the use of large-scale
feature sets with other recent improvements such
as the use of semi-supervised learning techniques
(Suzuki and Isozaki, 2008) or variable-length de-
pendencies (Qian et al., 2009).
</bodyText>
<sectionHeader confidence="0.998463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999615360655738">
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proceed-
ings of the International Conference on Machine
Learning, pages 33–40, Corvalis, Oregon.
L´eon Bottou. 2004. Stochastic learning. In Olivier
Bousquet and Ulrike von Luxburg, editors, Ad-
vanced Lectures on Machine Learning, Lecture
Notes in Artificial Intelligence, LNAI 3176, pages
146–168. Springer Verlag, Berlin.
L´eon Bottou. 2007. Stochastic gradient descent (sgd)
implementation. http://leon.bottou.org/projects/sgd.
Stanley Chen. 2009. Performance prediction for ex-
ponential language models. In Proceedings of the
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 450–458, Boulder, Colorado, June.
Trevor Cohn. 2006. Efficient inference in large con-
ditional random fields. In Proceedings of the 17th
European Conference on Machine Learning, pages
606–613, Berlin, September.
Thomas G. Dietterich, Adam Ashenfelter, and Yaroslav
Bulatov. 2004. Training conditional random fields
via gradient tree boosting. In Proceedings of
the International Conference on Machine Learning,
Banff, Canada.
Miroslav Dud´ık, Steven J. Phillips, and Robert E.
Schapire. 2004. Performance guarantees for reg-
ularized maximum entropy density estimation. In
John Shawe-Taylor and Yoram Singer, editors, Pro-
ceedings of the 17th annual Conference on Learning
Theory, volume 3120 of Lecture Notes in Computer
Science, pages 472–486. Springer.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics, pages 959–967, Columbus, Ohio.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2008. Regularization paths for generalized linear
models via coordinate descent. Technical report,
Department of Statistics, Stanford University.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 824–831, Prague, Czech republic.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Efficient inference of crfs for large-scale nat-
ural language data. In Proceedings of the Joint Con-
ference of the Annual Meeting of the Association
for Computational Linguistics and the International
Joint Conference on Natural Language Processing,
pages 281–284, Suntec, Singapore.
Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evalua-
tion and extension of maximum entropy models with
inequality constraints. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 137–144.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net/.
</reference>
<page confidence="0.968859">
512
</page>
<reference confidence="0.999915863636363">
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282–289.
Morgan Kaufmann, San Francisco, CA.
Percy Liang, Hal Daum´e, III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning, pages 592–599.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503–528.
Gideon Mann, Ryan McDonald, Mehryar Mohri,
Nathan Silberman, and Dan Walker. 2009. Efficient
large-scale distributed training of conditional maxi-
mum entropy models. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A.Culotta, editors,
Advances in Neural Information Processing Systems
22, pages 1231–1239.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn treebank. Com-
putational Linguistics, 19(2):313–330.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer.
Naoaki Okazaki. 2007. CRFsuite: A fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Chris Pal, Charles Sutton, and Andrew McCallum.
2006. Sparse forward-backward using minimum di-
vergence beams for fast training of conditional ran-
dom fields. In Proceedings of the International Con-
ference on Acoustics, Speech, and Signal Process-
ing, Toulouse, France.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: Fast, incremental feature selection
by gradient descent in function space. Journal of
Machine Learning Research, 3:1333–1356.
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav
Zimak. 2005. Learning and inference over con-
strained output. In Proceedings of the International
Joint Conference on Artificial Intelligence, pages
1124–1129.
Xian Qian, Xiaoqian Jiang, Qi Zhang, Xuanjing
Huang, and Lide Wu. 2009. Sparse higher order
conditional random fields for improved sequence la-
beling. In Proceedings of the Annual International
Conference on Machine Learning, pages 849–856.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and l1 regularization for re-
laxed maximum-entropy modeling. In Dekang Lin
and Dekai Wu, editors, Proceedings of the confer-
ence on Empirical Methods in Natural Language
Processing, pages 174–181, Barcelona, Spain, July.
Antoine Rozenknop. 2002. Mod`eles syntaxiques
probabilistes non-g´en´eratifs. Ph.D. thesis, Dpt.
d’informatique, ´Ecole Polytechnique F´ed´erale de
Lausanne.
Terrence J. Sejnowski and Charles R. Rosenberg.
1987. Parallel networks that learn to pronounce en-
glish text. Complex Systems, 1.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760–767, Prague, Czech Republic.
Nataliya Sokolovska, Thomas Lavergne, Olivier
Capp´e, and Franc¸ois Yvon. 2010. Efficient learning
of sparse conditional random fields for supervised
sequence labelling. IEEE Selected Topics in Signal
Processing.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors, In-
troduction to Statistical Relational Learning, Cam-
bridge, MA. The MIT Press.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of the
Conference of the Association for Computational
Linguistics on Human Language Technology, pages
665–673, Columbus, Ohio.
Robert Tibshirani. 1996. Regression shrinkage and
selection via the lasso. J.R.Statist.Soc.B, 58(1):267–
288.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
173–180.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumula-
tive penalty. In Proceedings of the Joint Conference
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing, pages
477–485, Suntec, Singapore.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark
Schmidt, and Kevin Murphy. 2006. Accelerated
training of conditional random fields with stochas-
tic gradient methods. In Proceedings of the 23th In-
ternational Conference on Machine Learning, pages
969–976. ACM Press, New York, NY, USA.
Hui Zhou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. J. Royal. Stat.
Soc. B., 67(2):301–320.
</reference>
<page confidence="0.998856">
513
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.127648">
<title confidence="0.997283">Practical very large scale CRFs</title>
<author confidence="0.864402">Lavergne Olivier Capp´e Yvon</author>
<note confidence="0.227354">LIMSI – CNRS T´el´ecom ParisTech Universit´e Paris-Sud 11 – CNRS LIMSI – CNRS cappe@enst.fr yvon@limsi.fr</note>
<abstract confidence="0.999342833333333">Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparinduced by the use of a term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of l1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>33--40</pages>
<location>Corvalis, Oregon.</location>
<contexts>
<context position="3441" citStr="Andrew and Gao, 2007" startWordPosition="542" endWordPosition="545">d by Jeong et al. (2009), who use approximations to simplify the forward-backward recursions. In this paper, we show that the sparsity that is induced by il-penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models. The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint. We study and compare three different ways to implement il penalty for CRFs that have been introduced recently: orthantwise Quasi Newton (Andrew and Gao, 2007), stochastic gradient descent (Tsuruoka et al., 2009) and coordinate descent (Sokolovska et al., 2010), concluding that these methods have complemen1In CRFsuite (Okazaki, 2007), it is even impossible to jointly test a pair of labels and a test on the observation, bigrams feature are only of the form f(yt−1, yt). 504 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics tary strengths and weaknesses. Based on an efficient implementation of these algorithms, we wer</context>
<context position="11889" citStr="Andrew and Gao, 2007" startWordPosition="1952" endWordPosition="1955">`1 penalties, a simple idea is that of (Kazama and Tsujii, 2003), originally introduced for maxent models. It amounts to reparameterizing θk as θk = θ+k −θ−k , where θ+k and θ−k are positive. The `1 penalty thus becomes ρ1(θ+ − θ−). In this formulation, the objective function recovers its smoothness and can be optimized with conventional algorithms, subject to domain constraints. Optimization is straightforward, but the number of parameters is doubled and convergence is slow Epθ(y|x(i)) fk(yt−1, yt, x(i) t ) fk(y(i) t−1, y(i) t , x(i) t ) (4) XN i=1 ∂l(θ) ∂θk TX(i) t=1 − TX(i) t=1 XN i=1 506 (Andrew and Gao, 2007): the procedure lacks a mechanism for zeroing out useless parameters. A more efficient strategy is the orthant-wise quasi-Newton (OWL-QN) algorithm introduced in (Andrew and Gao, 2007). The method is based on the observation that the `1 norm is differentiable when restricted to a set of points in which each coordinate never changes its sign (an “orthant”), and that its second derivative is then zero, meaning that the `1 penalty does not change the Hessian of the objective on each orthant. An OWL-QN update then simply consists in (i) computing the Newton update in a well-chosen orthant; (ii) pe</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of l1-regularized log-linear models. In Proceedings of the International Conference on Machine Learning, pages 33–40, Corvalis, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Stochastic learning.</title>
<date>2004</date>
<booktitle>Advanced Lectures on Machine Learning, Lecture Notes in Artificial Intelligence, LNAI 3176,</booktitle>
<pages>146--168</pages>
<editor>In Olivier Bousquet and Ulrike von Luxburg, editors,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="9832" citStr="Bottou, 2004" startWordPosition="1607" endWordPosition="1608">ameter estimation in CRFs consists in minimizing the logarithmic loss l(θ) defined by (3) with an additional `2 penalty term ρ22 kθk22, where ρ2 is a regularization parameter. The objective function is then a smooth convex function to be minimized over an unconstrained parameter space. Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popular CRF++ (Kudo, 2005) and CRFsuite (Okazaki, 2007) packages; conjugate gradient (Nocedal and Wright, 2006) and Stochastic Gradient Descent (SGD) (Bottou, 2004; Vishwanathan et al., 2006), used in CRFsgd (Bottou, 2007). The only caveat is to avoid numerical optimizers that require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs. The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many component have been zeroed (Tibshirani, 1996). Using a `1 penalty term thus implicitly performs feature selection, where ρ1 controls the amount of regularization and the number of extracted </context>
</contexts>
<marker>Bottou, 2004</marker>
<rawString>L´eon Bottou. 2004. Stochastic learning. In Olivier Bousquet and Ulrike von Luxburg, editors, Advanced Lectures on Machine Learning, Lecture Notes in Artificial Intelligence, LNAI 3176, pages 146–168. Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Stochastic gradient descent (sgd) implementation.</title>
<date>2007</date>
<note>http://leon.bottou.org/projects/sgd.</note>
<contexts>
<context position="9891" citStr="Bottou, 2007" startWordPosition="1616" endWordPosition="1617">ithmic loss l(θ) defined by (3) with an additional `2 penalty term ρ22 kθk22, where ρ2 is a regularization parameter. The objective function is then a smooth convex function to be minimized over an unconstrained parameter space. Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popular CRF++ (Kudo, 2005) and CRFsuite (Okazaki, 2007) packages; conjugate gradient (Nocedal and Wright, 2006) and Stochastic Gradient Descent (SGD) (Bottou, 2004; Vishwanathan et al., 2006), used in CRFsgd (Bottou, 2007). The only caveat is to avoid numerical optimizers that require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs. The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many component have been zeroed (Tibshirani, 1996). Using a `1 penalty term thus implicitly performs feature selection, where ρ1 controls the amount of regularization and the number of extracted features. In the following, we will jointly use both penalt</context>
</contexts>
<marker>Bottou, 2007</marker>
<rawString>L´eon Bottou. 2007. Stochastic gradient descent (sgd) implementation. http://leon.bottou.org/projects/sgd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
</authors>
<title>Performance prediction for exponential language models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>450--458</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="10851" citStr="Chen, 2009" startWordPosition="1778" endWordPosition="1779">y component have been zeroed (Tibshirani, 1996). Using a `1 penalty term thus implicitly performs feature selection, where ρ1 controls the amount of regularization and the number of extracted features. In the following, we will jointly use both penalty terms, yielding the socalled elastic net penalty (Zhou and Hastie, 2005) which corresponds to the objective function l(θ) + ρ1kθk1 + ρ22 kθk2 (6) 2 The use of both penalty terms makes it possible to control the number of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)). However, the introduction of a `1 penalty term makes the optimization of (6) more problematic, as the objective function is no longer differentiable in 0. Various strategies have been proposed to handle this difficulty. We will only consider here exact approaches and will not discuss heuristic strategies such as grafting (Perkins et al., 2003; Riezler and Vasserman, 2004). 3.2 Quasi Newton Methods To deal with `1 penalties, a simple idea is that of (Kazama and Tsujii, 2003), originally introduced for maxent models. It amounts to reparameterizing θk as θk = θ+k −θ−k , where θ+k and θ−k are p</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>Stanley Chen. 2009. Performance prediction for exponential language models. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 450–458, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
</authors>
<title>Efficient inference in large conditional random fields.</title>
<date>2006</date>
<booktitle>In Proceedings of the 17th European Conference on Machine Learning,</booktitle>
<pages>606--613</pages>
<location>Berlin,</location>
<contexts>
<context position="2769" citStr="Cohn (2006)" startWordPosition="437" endWordPosition="438">ics to simulate contextual dependencies, via extended tests on the observations (see discussions in, eg., (Punyakanok et al., 2005; Liang et al., 2008)). Limitating the feature set or the number of output labels is however frustrating for many NLP tasks, where the type and number of potentially relevant features are very large. A number of studies have tried to alleviate this problem. Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning. Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers “generalized” feature functions; and by Jeong et al. (2009), who use approximations to simplify the forward-backward recursions. In this paper, we show that the sparsity that is induced by il-penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models. The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint. We study and compare three different ways to implement il penalty for CRFs that have b</context>
</contexts>
<marker>Cohn, 2006</marker>
<rawString>Trevor Cohn. 2006. Efficient inference in large conditional random fields. In Proceedings of the 17th European Conference on Machine Learning, pages 606–613, Berlin, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
<author>Adam Ashenfelter</author>
<author>Yaroslav Bulatov</author>
</authors>
<title>Training conditional random fields via gradient tree boosting.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<location>Banff, Canada.</location>
<contexts>
<context position="2753" citStr="Dietterich et al. (2004)" startWordPosition="432" endWordPosition="435">bels1, and/or propose heuristics to simulate contextual dependencies, via extended tests on the observations (see discussions in, eg., (Punyakanok et al., 2005; Liang et al., 2008)). Limitating the feature set or the number of output labels is however frustrating for many NLP tasks, where the type and number of potentially relevant features are very large. A number of studies have tried to alleviate this problem. Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning. Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers “generalized” feature functions; and by Jeong et al. (2009), who use approximations to simplify the forward-backward recursions. In this paper, we show that the sparsity that is induced by il-penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models. The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint. We study and compare three different ways to implement il penalty for </context>
</contexts>
<marker>Dietterich, Ashenfelter, Bulatov, 2004</marker>
<rawString>Thomas G. Dietterich, Adam Ashenfelter, and Yaroslav Bulatov. 2004. Training conditional random fields via gradient tree boosting. In Proceedings of the International Conference on Machine Learning, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miroslav Dud´ık</author>
<author>Steven J Phillips</author>
<author>Robert E Schapire</author>
</authors>
<title>Performance guarantees for regularized maximum entropy density estimation.</title>
<date>2004</date>
<booktitle>In John Shawe-Taylor and Yoram Singer, editors, Proceedings of the 17th annual Conference on Learning Theory,</booktitle>
<volume>3120</volume>
<pages>472--486</pages>
<publisher>Springer.</publisher>
<marker>Dud´ık, Phillips, Schapire, 2004</marker>
<rawString>Miroslav Dud´ık, Steven J. Phillips, and Robert E. Schapire. 2004. Performance guarantees for regularized maximum entropy density estimation. In John Shawe-Taylor and Yoram Singer, editors, Proceedings of the 17th annual Conference on Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 472–486. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>959--967</pages>
<location>Columbus, Ohio.</location>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 959–967, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Friedman</author>
<author>Trevor Hastie</author>
<author>Rob Tibshirani</author>
</authors>
<title>Regularization paths for generalized linear models via coordinate descent.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>Department of Statistics, Stanford University.</institution>
<contexts>
<context position="14171" citStr="Friedman et al. (2008)" startWordPosition="2343" endWordPosition="2346">ted penalty zk that θk should have received, had the gradient been computed exactly, and use this value to “clip” the parameter value. This is implemented by patching the update (7) as follows � if (θk &gt; 0) θk ← max(0, θk − zk) (8) else if (θk &lt; 0) θk ← min(0, θk − zk) Based on a study of three NLP benchmarks, the authors of (Tsuruoka et al., 2009) claim this approach to be much faster than the orthant-wise approach and yet to yield very comparable performance, while selecting slightly larger feature sets. 3.4 Block Coordinate Descent The coordinate descent approach of Dudik et al. (2004) and Friedman et al. (2008) uses the fact that optimizing a mono-dimensional quadratic function augmented with a `1 penalty can be performed analytically. For arbitrary functions, this idea can be adapted by considering quadratic approximations of the objective around the current value θ¯ 1∂2l(¯θ) ¯θk) + (θk − ¯θk)2 2 ∂θ2 k + ρ1|θk |+ ρ22 θ2k + Cst (9) The minimizer of the approximation (9) is simply s{∂a8B)θk − al( ), ρ1} ∂2l( � + ρ2 θ) ∂θ2 where s is the soft-thresholding function { z − ρ if z &gt; ρ z + ρ if z &lt; −ρ 0 otherwise Coordinate descent is ported to CRFs in (Sokolovska et al., 2010). Making this scheme practica</context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2008</marker>
<rawString>Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2008. Regularization paths for generalized linear models via coordinate descent. Technical report, Department of Statistics, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Galen Andrew</author>
<author>Mark Johnson</author>
<author>Kristina Toutanova</author>
</authors>
<title>A comparative study of parameter estimation methods for statistical natural language processing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>824--831</pages>
<location>Prague, Czech republic.</location>
<contexts>
<context position="12714" citStr="Gao et al., 2007" startWordPosition="2089" endWordPosition="2092"> the observation that the `1 norm is differentiable when restricted to a set of points in which each coordinate never changes its sign (an “orthant”), and that its second derivative is then zero, meaning that the `1 penalty does not change the Hessian of the objective on each orthant. An OWL-QN update then simply consists in (i) computing the Newton update in a well-chosen orthant; (ii) performing the update, which might cause some component of the parameter vector to change sign; and (iii) projecting back the parameter value onto the initial orthant, thereby zeroing out those components. In (Gao et al., 2007), the authors show that OWL-QN is faster than the algorithm proposed by Kazama and Tsujii (2003) and can perform model selection even in very high-dimensional problems, with no loss of performance compared to the use of `2 penalty terms. 3.3 Stochastic Gradient Descent Stochastic gradient (SGD) approaches update the parameter vector based on an crude approximation of the gradient (4), where the computation of expectations only includes a small batch of observations. SGD updates have the following form ∂l(θ) θk ← θk + η ,(7) ∂θk where η is the learning rate. In (Tsuruoka et al., 2009), various </context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>Jianfeng Gao, Galen Andrew, Mark Johnson, and Kristina Toutanova. 2007. A comparative study of parameter estimation methods for statistical natural language processing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824–831, Prague, Czech republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Chin-Yew Lin</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Efficient inference of crfs for large-scale natural language data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing,</booktitle>
<pages>281--284</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2844" citStr="Jeong et al. (2009)" startWordPosition="447" endWordPosition="450">observations (see discussions in, eg., (Punyakanok et al., 2005; Liang et al., 2008)). Limitating the feature set or the number of output labels is however frustrating for many NLP tasks, where the type and number of potentially relevant features are very large. A number of studies have tried to alleviate this problem. Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning. Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers “generalized” feature functions; and by Jeong et al. (2009), who use approximations to simplify the forward-backward recursions. In this paper, we show that the sparsity that is induced by il-penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models. The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint. We study and compare three different ways to implement il penalty for CRFs that have been introduced recently: orthantwise Quasi Newton (Andrew and Gao, 2007), s</context>
<context position="30434" citStr="Jeong et al., 2009" startWordPosition="5081" endWordPosition="5084">the 5-gram+ setting, only 50,000 out of 250 million features are selected. SGD is the fastest of the three, up to twice as fast as OWL-QN and BCD depending on the feature set. The performance it achieves are consistently slightly worst than the other optimizers, and only catch up when the parameters are fine-tuned (see above). There are not so many comparisons for Nettalk with CRFs, due to the size of the label set. Our results compare favorably with those reported in (Pal et al., 2006), where the accuracy attains 91.7% using 19075 examples for training and 934 for testing, and with those in (Jeong et al., 2009) (88.4% accuracy with 18,000 (2,000) training (test) instances). Table 5 gives the results obtained for the larger Nettalk+prosody task. Here, we only report the results obtained with SGD and BCD. For OWL-QN, the largest model we could handle was the 3-grm model, which contained 69 million features, and took 48min to train. Here again, performance steadily increase with the number of features, showing the benefits of large-scale models. We lack comparisons for this task, which seems considerably harder than the sole phonetization task, and all systems seem to plateau around 13.5% accuracy. Int</context>
</contexts>
<marker>Jeong, Lin, Lee, 2009</marker>
<rawString>Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee. 2009. Efficient inference of crfs for large-scale natural language data. In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, pages 281–284, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="11332" citStr="Kazama and Tsujii, 2003" startWordPosition="1854" endWordPosition="1857">er of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)). However, the introduction of a `1 penalty term makes the optimization of (6) more problematic, as the objective function is no longer differentiable in 0. Various strategies have been proposed to handle this difficulty. We will only consider here exact approaches and will not discuss heuristic strategies such as grafting (Perkins et al., 2003; Riezler and Vasserman, 2004). 3.2 Quasi Newton Methods To deal with `1 penalties, a simple idea is that of (Kazama and Tsujii, 2003), originally introduced for maxent models. It amounts to reparameterizing θk as θk = θ+k −θ−k , where θ+k and θ−k are positive. The `1 penalty thus becomes ρ1(θ+ − θ−). In this formulation, the objective function recovers its smoothness and can be optimized with conventional algorithms, subject to domain constraints. Optimization is straightforward, but the number of parameters is doubled and convergence is slow Epθ(y|x(i)) fk(yt−1, yt, x(i) t ) fk(y(i) t−1, y(i) t , x(i) t ) (4) XN i=1 ∂l(θ) ∂θk TX(i) t=1 − TX(i) t=1 XN i=1 506 (Andrew and Gao, 2007): the procedure lacks a mechanism for zeroi</context>
<context position="12810" citStr="Kazama and Tsujii (2003)" startWordPosition="2105" endWordPosition="2108"> which each coordinate never changes its sign (an “orthant”), and that its second derivative is then zero, meaning that the `1 penalty does not change the Hessian of the objective on each orthant. An OWL-QN update then simply consists in (i) computing the Newton update in a well-chosen orthant; (ii) performing the update, which might cause some component of the parameter vector to change sign; and (iii) projecting back the parameter value onto the initial orthant, thereby zeroing out those components. In (Gao et al., 2007), the authors show that OWL-QN is faster than the algorithm proposed by Kazama and Tsujii (2003) and can perform model selection even in very high-dimensional problems, with no loss of performance compared to the use of `2 penalty terms. 3.3 Stochastic Gradient Descent Stochastic gradient (SGD) approaches update the parameter vector based on an crude approximation of the gradient (4), where the computation of expectations only includes a small batch of observations. SGD updates have the following form ∂l(θ) θk ← θk + η ,(7) ∂θk where η is the learning rate. In (Tsuruoka et al., 2009), various ways of adapting this update to `1- penalized likelihood functions are discussed. Two effective </context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<date>2005</date>
<note>CRF++: Yet another CRF toolkit. http://crfpp.sourceforge.net/.</note>
<contexts>
<context position="9695" citStr="Kudo, 2005" startWordPosition="1589" endWordPosition="1590">e a number of operations that grows quadratically with |Y |. 3 V Regularization in CRFs 3.1 Regularization The standard approach for parameter estimation in CRFs consists in minimizing the logarithmic loss l(θ) defined by (3) with an additional `2 penalty term ρ22 kθk22, where ρ2 is a regularization parameter. The objective function is then a smooth convex function to be minimized over an unconstrained parameter space. Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popular CRF++ (Kudo, 2005) and CRFsuite (Okazaki, 2007) packages; conjugate gradient (Nocedal and Wright, 2006) and Stochastic Gradient Descent (SGD) (Bottou, 2004; Vishwanathan et al., 2006), used in CRFsgd (Bottou, 2007). The only caveat is to avoid numerical optimizers that require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs. The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many component have been zeroed (Tibshirani, 1996). Using </context>
</contexts>
<marker>Kudo, 2005</marker>
<rawString>Taku Kudo. 2005. CRF++: Yet another CRF toolkit. http://crfpp.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="1199" citStr="Lafferty et al., 2001" startWordPosition="183" endWordPosition="186">lly with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparsity induced by the use of a il penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets. 1 Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) constitute a widely-used and effective approach for supervised structure learning tasks involving the mapping between complex objects such as strings and trees. An important property of CRFs is their ability to handle large and redundant feature sets and to integrate structural dependency between output labels. However, even for simple linear chain CRFs, the complexity of learning and inference This work was partly supported by ANR projects CroTaL (ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311- 02). grows quadratically with respect to the number of output labels and s</context>
<context position="5515" citStr="Lafferty et al., 2001" startWordPosition="872" endWordPosition="875">res are progressively incorporated in the model insofar as the shorter range features have proven useful. The rest of the paper is organized as follows: we first recall the basics of CRFs in Section 2, and discuss three ways to train CRFs with a `1 penalty in Section 3. We then detail several implementation issues that need to be addressed when dealing with massive feature sets in Section 4. Our experiments are reported in Section 5. The main conclusions of this study are drawn in Section 6. 2 Conditional Random Fields In this section, we recall the basics of Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) and introduce the notations that will be used throughout. 2.1 Basics CRFs are based on the following model ( K pθ(y|x) = Zθ(x) exp X θkFk(x,y) (1) k=1 where x = (x1, ...,xT) and y = (y1, ... , yT) are, respectively, the input and output sequences2, and Fk(x, y) is equal to PTt=1 fk(yt−1, yt, xt), where {fk}1≤k≤K is an arbitrary set of feature 2Our implementation also includes a special label y0, that is always observed and marks the beginning of a sequence. functions and {θk}1≤k≤K are the associated parameter values. We denote by Y and X, respectively, the sets in </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning, pages 282–289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daum´e</author>
<author>Dan Klein</author>
</authors>
<title>Structure compilation: trading structure for features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>592--599</pages>
<marker>Liang, Daum´e, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daum´e, III, and Dan Klein. 2008. Structure compilation: trading structure for features. In Proceedings of the 25th international conference on Machine learning, pages 592–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="9646" citStr="Liu and Nocedal, 1989" startWordPosition="1577" endWordPosition="1580">p(µy,xt+1 + λy,,y,xt+1)βt+1(y)/Zθ(x) These recursions require a number of operations that grows quadratically with |Y |. 3 V Regularization in CRFs 3.1 Regularization The standard approach for parameter estimation in CRFs consists in minimizing the logarithmic loss l(θ) defined by (3) with an additional `2 penalty term ρ22 kθk22, where ρ2 is a regularization parameter. The objective function is then a smooth convex function to be minimized over an unconstrained parameter space. Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popular CRF++ (Kudo, 2005) and CRFsuite (Okazaki, 2007) packages; conjugate gradient (Nocedal and Wright, 2006) and Stochastic Gradient Descent (SGD) (Bottou, 2004; Vishwanathan et al., 2006), used in CRFsgd (Bottou, 2007). The only caveat is to avoid numerical optimizers that require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs. The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many comp</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Mann</author>
<author>Ryan McDonald</author>
<author>Mehryar Mohri</author>
<author>Nathan Silberman</author>
<author>Dan Walker</author>
</authors>
<title>Efficient large-scale distributed training of conditional maximum entropy models.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 22,</booktitle>
<pages>1231--1239</pages>
<editor>In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A.Culotta, editors,</editor>
<contexts>
<context position="17824" citStr="Mann et al., 2009" startWordPosition="2966" endWordPosition="2969"> of potentially active features. For BCD, forward-backward can even be made slightly faster. When computing the gradient wrt. features λy,x and µy0,y,x (for all the values of y and y0) for sequence x(�), assuming that x only occurs once in x(�) at position t, all that is needed is α0t(y), bt0 &lt; t and β0t(y), bt0 &gt; t. ZB(x) is then recovered as Ey αt(y)βt(y). Forward-backward recursions can thus be truncated: in our experiments, this divided the computational cost by 1,8 on average. Note finally that forward-backward is performed on a per-observation basis and is easily parallelized (see also (Mann et al., 2009) for more powerful ways to distribute the computation when dealing with very large datasets). In our implementation, it is distributed on all available cores, resulting in significant speed-ups for OWL-QN and L-BFGS; for BCD the gain is less acute, as parallelization only helps when updating the parameters for a block of features that are occur in many sequences; for SGD, with batches of size one, this parallelization policy is useless. 4.2 Scaling Most existing implementations of CRFs, eg. CRF++ and CRFsgd perform the forwardbackward recursions in the log-domain, which guarantees that numeric</context>
</contexts>
<marker>Mann, McDonald, Mohri, Silberman, Walker, 2009</marker>
<rawString>Gideon Mann, Ryan McDonald, Mehryar Mohri, Nathan Silberman, and Dan Walker. 2009. Efficient large-scale distributed training of conditional maximum entropy models. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A.Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1231–1239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="25529" citStr="Marcus et al., 1993" startWordPosition="4246" endWordPosition="4249">her than to letters. 509 Features With Without Nettalk 3-grm 10.74% 14.3M 14.59% 0.3M 5-grm 8.48% 132.5M 11.54% 2.5M POS tagging base 2.91% 436.7M 3.47% 70.2M Table 1: Features jointly testing label pairs and the observation are useful (error rates and features counts.) E2 E1-sparse E1 % zero 1-grm 84min 41min 57min 44.6% 3-grm- 65min 16min 44min 99.6% 3-grm 72min 48min 58min 19.9% Table 2: Sparse vs standard forward-backward (training times and percentages of sparsity of M) 5.1.2 Part-of-Speech Tagging Our second benchmark is a part-of-speech (POS) tagging task using the PennTreeBank corpus (Marcus et al., 1993), which provides us with a quite different condition. For this task, the number of labels is smaller (|Y |= 45) than for Nettalk, and the set of observations is much larger (IXI = 43207). This benchmark, which has been used in many studies, allows for direct comparisons with other published work. We thus use a standard experimental set-up, where sections 0-18 of the Wall Street Journal are used for training, sections 19-21 for development, and sections 22-24 for testing. Features are also standard and follow the design of (Suzuki and Isozaki, 2008) and test the current words (as written and lo</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="9780" citStr="Nocedal and Wright, 2006" startWordPosition="1598" endWordPosition="1601">rization in CRFs 3.1 Regularization The standard approach for parameter estimation in CRFs consists in minimizing the logarithmic loss l(θ) defined by (3) with an additional `2 penalty term ρ22 kθk22, where ρ2 is a regularization parameter. The objective function is then a smooth convex function to be minimized over an unconstrained parameter space. Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popular CRF++ (Kudo, 2005) and CRFsuite (Okazaki, 2007) packages; conjugate gradient (Nocedal and Wright, 2006) and Stochastic Gradient Descent (SGD) (Bottou, 2004; Vishwanathan et al., 2006), used in CRFsgd (Bottou, 2007). The only caveat is to avoid numerical optimizers that require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs. The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many component have been zeroed (Tibshirani, 1996). Using a `1 penalty term thus implicitly performs feature selection, where ρ1 controls the a</context>
</contexts>
<marker>Nocedal, Wright, 2006</marker>
<rawString>Jorge Nocedal and Stephen Wright. 2006. Numerical Optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: A fast implementation of conditional random fields</title>
<date>2007</date>
<contexts>
<context position="3617" citStr="Okazaki, 2007" startWordPosition="568" endWordPosition="569">Fs can be used to reduce the total training time, while yielding extremely compact models. The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint. We study and compare three different ways to implement il penalty for CRFs that have been introduced recently: orthantwise Quasi Newton (Andrew and Gao, 2007), stochastic gradient descent (Tsuruoka et al., 2009) and coordinate descent (Sokolovska et al., 2010), concluding that these methods have complemen1In CRFsuite (Okazaki, 2007), it is even impossible to jointly test a pair of labels and a test on the observation, bigrams feature are only of the form f(yt−1, yt). 504 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics tary strengths and weaknesses. Based on an efficient implementation of these algorithms, we were able to train very large CRFs containing more than a hundred of output labels and up to several billion features, yielding results that are as good or better than the best re</context>
<context position="9724" citStr="Okazaki, 2007" startWordPosition="1593" endWordPosition="1594">hat grows quadratically with |Y |. 3 V Regularization in CRFs 3.1 Regularization The standard approach for parameter estimation in CRFs consists in minimizing the logarithmic loss l(θ) defined by (3) with an additional `2 penalty term ρ22 kθk22, where ρ2 is a regularization parameter. The objective function is then a smooth convex function to be minimized over an unconstrained parameter space. Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popular CRF++ (Kudo, 2005) and CRFsuite (Okazaki, 2007) packages; conjugate gradient (Nocedal and Wright, 2006) and Stochastic Gradient Descent (SGD) (Bottou, 2004; Vishwanathan et al., 2006), used in CRFsgd (Bottou, 2007). The only caveat is to avoid numerical optimizers that require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs. The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many component have been zeroed (Tibshirani, 1996). Using a `1 penalty term thus implic</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. CRFsuite: A fast implementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Pal</author>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Sparse forward-backward using minimum divergence beams for fast training of conditional random fields.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="2563" citStr="Pal et al. (2006)" startWordPosition="403" endWordPosition="406">with a restricted output space (typically in the order of few dozens of output labels), heuristically reduce the use of features, especially of features that test pairs of adjacent labels1, and/or propose heuristics to simulate contextual dependencies, via extended tests on the observations (see discussions in, eg., (Punyakanok et al., 2005; Liang et al., 2008)). Limitating the feature set or the number of output labels is however frustrating for many NLP tasks, where the type and number of potentially relevant features are very large. A number of studies have tried to alleviate this problem. Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning. Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers “generalized” feature functions; and by Jeong et al. (2009), who use approximations to simplify the forward-backward recursions. In this paper, we show that the sparsity that is induced by il-penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models. The benefits of sparsity are even greater during inference: less feat</context>
<context position="30306" citStr="Pal et al., 2006" startWordPosition="5057" endWordPosition="5060">ts achieved on the Nettalk task. The three algorithms yield very comparable accuracy results, and deliver compact models: for the 5-gram+ setting, only 50,000 out of 250 million features are selected. SGD is the fastest of the three, up to twice as fast as OWL-QN and BCD depending on the feature set. The performance it achieves are consistently slightly worst than the other optimizers, and only catch up when the parameters are fine-tuned (see above). There are not so many comparisons for Nettalk with CRFs, due to the size of the label set. Our results compare favorably with those reported in (Pal et al., 2006), where the accuracy attains 91.7% using 19075 examples for training and 934 for testing, and with those in (Jeong et al., 2009) (88.4% accuracy with 18,000 (2,000) training (test) instances). Table 5 gives the results obtained for the larger Nettalk+prosody task. Here, we only report the results obtained with SGD and BCD. For OWL-QN, the largest model we could handle was the 3-grm model, which contained 69 million features, and took 48min to train. Here again, performance steadily increase with the number of features, showing the benefits of large-scale models. We lack comparisons for this ta</context>
</contexts>
<marker>Pal, Sutton, McCallum, 2006</marker>
<rawString>Chris Pal, Charles Sutton, and Andrew McCallum. 2006. Sparse forward-backward using minimum divergence beams for fast training of conditional random fields. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Perkins</author>
<author>Kevin Lacker</author>
<author>James Theiler</author>
</authors>
<title>Grafting: Fast, incremental feature selection by gradient descent in function space.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1333</pages>
<contexts>
<context position="11198" citStr="Perkins et al., 2003" startWordPosition="1831" endWordPosition="1834">ponds to the objective function l(θ) + ρ1kθk1 + ρ22 kθk2 (6) 2 The use of both penalty terms makes it possible to control the number of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)). However, the introduction of a `1 penalty term makes the optimization of (6) more problematic, as the objective function is no longer differentiable in 0. Various strategies have been proposed to handle this difficulty. We will only consider here exact approaches and will not discuss heuristic strategies such as grafting (Perkins et al., 2003; Riezler and Vasserman, 2004). 3.2 Quasi Newton Methods To deal with `1 penalties, a simple idea is that of (Kazama and Tsujii, 2003), originally introduced for maxent models. It amounts to reparameterizing θk as θk = θ+k −θ−k , where θ+k and θ−k are positive. The `1 penalty thus becomes ρ1(θ+ − θ−). In this formulation, the objective function recovers its smoothness and can be optimized with conventional algorithms, subject to domain constraints. Optimization is straightforward, but the number of parameters is doubled and convergence is slow Epθ(y|x(i)) fk(yt−1, yt, x(i) t ) fk(y(i) t−1, y(i</context>
</contexts>
<marker>Perkins, Lacker, Theiler, 2003</marker>
<rawString>Simon Perkins, Kevin Lacker, and James Theiler. 2003. Grafting: Fast, incremental feature selection by gradient descent in function space. Journal of Machine Learning Research, 3:1333–1356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Learning and inference over constrained output.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1124--1129</pages>
<contexts>
<context position="2288" citStr="Punyakanok et al., 2005" startWordPosition="355" endWordPosition="358">rojects CroTaL (ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311- 02). grows quadratically with respect to the number of output labels and so does the number of structural features, ie. features testing adjacent pairs of labels. Most empirical studies on CRFs thus either consider tasks with a restricted output space (typically in the order of few dozens of output labels), heuristically reduce the use of features, especially of features that test pairs of adjacent labels1, and/or propose heuristics to simulate contextual dependencies, via extended tests on the observations (see discussions in, eg., (Punyakanok et al., 2005; Liang et al., 2008)). Limitating the feature set or the number of output labels is however frustrating for many NLP tasks, where the type and number of potentially relevant features are very large. A number of studies have tried to alleviate this problem. Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning. Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers “generalized” feature functions; and by Jeong et al. (2009), who use approximations to simplify the for</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2005</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav Zimak. 2005. Learning and inference over constrained output. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 1124–1129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Xiaoqian Jiang</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Sparse higher order conditional random fields for improved sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual International Conference on Machine Learning,</booktitle>
<pages>849--856</pages>
<marker>Qian, Jiang, Zhang, Huang, Wu, 2009</marker>
<rawString>Xian Qian, Xiaoqian Jiang, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Sparse higher order conditional random fields for improved sequence labeling. In Proceedings of the Annual International Conference on Machine Learning, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
</authors>
<title>Incremental feature selection and l1 regularization for relaxed maximum-entropy modeling.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of the conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>174--181</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="11228" citStr="Riezler and Vasserman, 2004" startWordPosition="1835" endWordPosition="1838"> function l(θ) + ρ1kθk1 + ρ22 kθk2 (6) 2 The use of both penalty terms makes it possible to control the number of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)). However, the introduction of a `1 penalty term makes the optimization of (6) more problematic, as the objective function is no longer differentiable in 0. Various strategies have been proposed to handle this difficulty. We will only consider here exact approaches and will not discuss heuristic strategies such as grafting (Perkins et al., 2003; Riezler and Vasserman, 2004). 3.2 Quasi Newton Methods To deal with `1 penalties, a simple idea is that of (Kazama and Tsujii, 2003), originally introduced for maxent models. It amounts to reparameterizing θk as θk = θ+k −θ−k , where θ+k and θ−k are positive. The `1 penalty thus becomes ρ1(θ+ − θ−). In this formulation, the objective function recovers its smoothness and can be optimized with conventional algorithms, subject to domain constraints. Optimization is straightforward, but the number of parameters is doubled and convergence is slow Epθ(y|x(i)) fk(yt−1, yt, x(i) t ) fk(y(i) t−1, y(i) t , x(i) t ) (4) XN i=1 ∂l(θ</context>
</contexts>
<marker>Riezler, Vasserman, 2004</marker>
<rawString>Stefan Riezler and Alexander Vasserman. 2004. Incremental feature selection and l1 regularization for relaxed maximum-entropy modeling. In Dekang Lin and Dekai Wu, editors, Proceedings of the conference on Empirical Methods in Natural Language Processing, pages 174–181, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Rozenknop</author>
</authors>
<title>Mod`eles syntaxiques probabilistes non-g´en´eratifs.</title>
<date>2002</date>
<booktitle>Ph.D. thesis, Dpt. d’informatique, ´Ecole Polytechnique F´ed´erale de Lausanne.</booktitle>
<marker>Rozenknop, 2002</marker>
<rawString>Antoine Rozenknop. 2002. Mod`eles syntaxiques probabilistes non-g´en´eratifs. Ph.D. thesis, Dpt. d’informatique, ´Ecole Polytechnique F´ed´erale de Lausanne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terrence J Sejnowski</author>
<author>Charles R Rosenberg</author>
</authors>
<title>Parallel networks that learn to pronounce english text.</title>
<date>1987</date>
<journal>Complex Systems,</journal>
<volume>1</volume>
<contexts>
<context position="22645" citStr="Sejnowski and Rosenberg, 1987" startWordPosition="3770" endWordPosition="3773">, we use 5 past values of the gradient to approximate the inverse of the Hessian matrix: increasing this value had no effect on accuracy or convergence and was detrimental to speed; for SGD, the learning rate parameter was tuned manually. Note that we have not spent much time optimizing the values of p1 and p2. Based on a pilot study on Nettalk, we found that taking p1 = .5 and p2 in the order of 10−5 to yield nearly optimal performance, and have used these values throughout. 5.1 Tasks and Settings 5.1.1 Nettalk Our first benchmark is the word phonetization task, using the Nettalk dictionary (Sejnowski and Rosenberg, 1987). This dataset contains approximately 20,000 English word forms, their pronunciation, plus some prosodic information (stress markers for vowels, syllabic parsing for consonants). Grapheme and phoneme strings are aligned at the character level, thanks to the use of a “null sound” in the latter string when it is shorter than the former; likewise, each prosodic mark is aligned with the corresponding letter. We have derived two test conditions from this database. The first one is standard and aims at predicting the pronunciation information only. In this setting, the set of observations (X) contai</context>
</contexts>
<marker>Sejnowski, Rosenberg, 1987</marker>
<rawString>Terrence J. Sejnowski and Charles R. Rosenberg. 1987. Parallel networks that learn to pronounce english text. Complex Systems, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>760--767</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="33656" citStr="Shen et al., 2007" startWordPosition="5626" endWordPosition="5629"> of words, on suffixes and prefixes up to length 4. After four iterations, we throw in features testing word trigrams, subject to the corresponding unigram block being active. After 6 iterations, we finally augment the 511 model with windows of length 5, subject to the corresponding trigram being active. After 10 iterations, the model contains about 4 billion features, out of which 400,000 are active. It achieves an error rate of 2.63% (resp. 2.78%) on the development (resp. test) data, which compares favorably with some of the best results for this task (for instance (Toutanova et al., 2003; Shen et al., 2007; Suzuki and Isozaki, 2008)). 6 Conclusion and Perspectives In this paper, we have discussed various ways to train extremely large CRFs with a &apos; penalty term and compared experimentally the results obtained, both in terms of training speed and of accuracy. The algorithms studied in this paper have complementary strength and weaknesses: OWL-QN is probably the method of choice in small or moderate size applications while BCD is most efficient when using very large feature sets combined with limited-size observation alphabets; SGD complemented with fine tuning appears to be the preferred choice i</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta, and Aravind Joshi. 2007. Guided learning for bidirectional sequence classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nataliya Sokolovska</author>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Efficient learning of sparse conditional random fields for supervised sequence labelling.</title>
<date>2010</date>
<journal>IEEE Selected Topics in Signal Processing.</journal>
<marker>Sokolovska, Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Nataliya Sokolovska, Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. 2010. Efficient learning of sparse conditional random fields for supervised sequence labelling. IEEE Selected Topics in Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An introduction to conditional random fields for relational learning.</title>
<date>2006</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning,</booktitle>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1227" citStr="Sutton and McCallum, 2006" startWordPosition="187" endWordPosition="190">y of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparsity induced by the use of a il penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets. 1 Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) constitute a widely-used and effective approach for supervised structure learning tasks involving the mapping between complex objects such as strings and trees. An important property of CRFs is their ability to handle large and redundant feature sets and to integrate structural dependency between output labels. However, even for simple linear chain CRFs, the complexity of learning and inference This work was partly supported by ANR projects CroTaL (ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311- 02). grows quadratically with respect to the number of output labels and so does the number of structu</context>
<context position="5543" citStr="Sutton and McCallum, 2006" startWordPosition="876" endWordPosition="880">ncorporated in the model insofar as the shorter range features have proven useful. The rest of the paper is organized as follows: we first recall the basics of CRFs in Section 2, and discuss three ways to train CRFs with a `1 penalty in Section 3. We then detail several implementation issues that need to be addressed when dealing with massive feature sets in Section 4. Our experiments are reported in Section 5. The main conclusions of this study are drawn in Section 6. 2 Conditional Random Fields In this section, we recall the basics of Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) and introduce the notations that will be used throughout. 2.1 Basics CRFs are based on the following model ( K pθ(y|x) = Zθ(x) exp X θkFk(x,y) (1) k=1 where x = (x1, ...,xT) and y = (y1, ... , yT) are, respectively, the input and output sequences2, and Fk(x, y) is equal to PTt=1 fk(yt−1, yt, xt), where {fk}1≤k≤K is an arbitrary set of feature 2Our implementation also includes a special label y0, that is always observed and marks the beginning of a sequence. functions and {θk}1≤k≤K are the associated parameter values. We denote by Y and X, respectively, the sets in which yt and xt take their v</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>Charles Sutton and Andrew McCallum. 2006. An introduction to conditional random fields for relational learning. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning, Cambridge, MA. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>665--673</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="26083" citStr="Suzuki and Isozaki, 2008" startWordPosition="4339" endWordPosition="4342"> (POS) tagging task using the PennTreeBank corpus (Marcus et al., 1993), which provides us with a quite different condition. For this task, the number of labels is smaller (|Y |= 45) than for Nettalk, and the set of observations is much larger (IXI = 43207). This benchmark, which has been used in many studies, allows for direct comparisons with other published work. We thus use a standard experimental set-up, where sections 0-18 of the Wall Street Journal are used for training, sections 19-21 for development, and sections 22-24 for testing. Features are also standard and follow the design of (Suzuki and Isozaki, 2008) and test the current words (as written and lowercased), prefixes and suffixes up to length 4, and typographical characteristics (case, etc.) of the words. Our baseline feature set also contains tests on individual and pairs of words in a window of 5 words. 5.2 Using Large Feature Sets The first important issue is to assess the benefits of using large feature sets, notably including features testing both a bigram of labels and an observation. Table 1 compares the results obtained with and without these features for various setting (using OWL-QN to perform the optimization), suggesting that for</context>
<context position="33683" citStr="Suzuki and Isozaki, 2008" startWordPosition="5630" endWordPosition="5633">xes and prefixes up to length 4. After four iterations, we throw in features testing word trigrams, subject to the corresponding unigram block being active. After 6 iterations, we finally augment the 511 model with windows of length 5, subject to the corresponding trigram being active. After 10 iterations, the model contains about 4 billion features, out of which 400,000 are active. It achieves an error rate of 2.63% (resp. 2.78%) on the development (resp. test) data, which compares favorably with some of the best results for this task (for instance (Toutanova et al., 2003; Shen et al., 2007; Suzuki and Isozaki, 2008)). 6 Conclusion and Perspectives In this paper, we have discussed various ways to train extremely large CRFs with a &apos; penalty term and compared experimentally the results obtained, both in terms of training speed and of accuracy. The algorithms studied in this paper have complementary strength and weaknesses: OWL-QN is probably the method of choice in small or moderate size applications while BCD is most efficient when using very large feature sets combined with limited-size observation alphabets; SGD complemented with fine tuning appears to be the preferred choice in most large-scale applicat</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data. In Proceedings of the Conference of the Association for Computational Linguistics on Human Language Technology, pages 665–673, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>J.R.Statist.Soc.B,</journal>
<volume>58</volume>
<issue>1</issue>
<pages>288</pages>
<contexts>
<context position="10287" citStr="Tibshirani, 1996" startWordPosition="1682" endWordPosition="1684">opular CRF++ (Kudo, 2005) and CRFsuite (Okazaki, 2007) packages; conjugate gradient (Nocedal and Wright, 2006) and Stochastic Gradient Descent (SGD) (Bottou, 2004; Vishwanathan et al., 2006), used in CRFsgd (Bottou, 2007). The only caveat is to avoid numerical optimizers that require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs. The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many component have been zeroed (Tibshirani, 1996). Using a `1 penalty term thus implicitly performs feature selection, where ρ1 controls the amount of regularization and the number of extracted features. In the following, we will jointly use both penalty terms, yielding the socalled elastic net penalty (Zhou and Hastie, 2005) which corresponds to the objective function l(θ) + ρ1kθk1 + ρ22 kθk2 (6) 2 The use of both penalty terms makes it possible to control the number of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)). However, the introduction of a `1</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. J.R.Statist.Soc.B, 58(1):267– 288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="33637" citStr="Toutanova et al., 2003" startWordPosition="5622" endWordPosition="5625">, we add tests on bigram of words, on suffixes and prefixes up to length 4. After four iterations, we throw in features testing word trigrams, subject to the corresponding unigram block being active. After 6 iterations, we finally augment the 511 model with windows of length 5, subject to the corresponding trigram being active. After 10 iterations, the model contains about 4 billion features, out of which 400,000 are active. It achieves an error rate of 2.63% (resp. 2.78%) on the development (resp. test) data, which compares favorably with some of the best results for this task (for instance (Toutanova et al., 2003; Shen et al., 2007; Suzuki and Isozaki, 2008)). 6 Conclusion and Perspectives In this paper, we have discussed various ways to train extremely large CRFs with a &apos; penalty term and compared experimentally the results obtained, both in terms of training speed and of accuracy. The algorithms studied in this paper have complementary strength and weaknesses: OWL-QN is probably the method of choice in small or moderate size applications while BCD is most efficient when using very large feature sets combined with limited-size observation alphabets; SGD complemented with fine tuning appears to be the</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing,</booktitle>
<pages>477--485</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="3494" citStr="Tsuruoka et al., 2009" startWordPosition="549" endWordPosition="552">simplify the forward-backward recursions. In this paper, we show that the sparsity that is induced by il-penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models. The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint. We study and compare three different ways to implement il penalty for CRFs that have been introduced recently: orthantwise Quasi Newton (Andrew and Gao, 2007), stochastic gradient descent (Tsuruoka et al., 2009) and coordinate descent (Sokolovska et al., 2010), concluding that these methods have complemen1In CRFsuite (Okazaki, 2007), it is even impossible to jointly test a pair of labels and a test on the observation, bigrams feature are only of the form f(yt−1, yt). 504 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics tary strengths and weaknesses. Based on an efficient implementation of these algorithms, we were able to train very large CRFs containing more than </context>
<context position="13304" citStr="Tsuruoka et al., 2009" startWordPosition="2188" endWordPosition="2191">omponents. In (Gao et al., 2007), the authors show that OWL-QN is faster than the algorithm proposed by Kazama and Tsujii (2003) and can perform model selection even in very high-dimensional problems, with no loss of performance compared to the use of `2 penalty terms. 3.3 Stochastic Gradient Descent Stochastic gradient (SGD) approaches update the parameter vector based on an crude approximation of the gradient (4), where the computation of expectations only includes a small batch of observations. SGD updates have the following form ∂l(θ) θk ← θk + η ,(7) ∂θk where η is the learning rate. In (Tsuruoka et al., 2009), various ways of adapting this update to `1- penalized likelihood functions are discussed. Two effective ideas are proposed: (i) only update parameters that correspond to active features in the current observation, (ii) keep track of the cumulated penalty zk that θk should have received, had the gradient been computed exactly, and use this value to “clip” the parameter value. This is implemented by patching the update (7) as follows � if (θk &gt; 0) θk ← max(0, θk − zk) (8) else if (θk &lt; 0) θk ← min(0, θk − zk) Based on a study of three NLP benchmarks, the authors of (Tsuruoka et al., 2009) clai</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, pages 477–485, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Nicol N Schraudolph</author>
<author>Mark Schmidt</author>
<author>Kevin Murphy</author>
</authors>
<title>Accelerated training of conditional random fields with stochastic gradient methods.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23th International Conference on Machine Learning,</booktitle>
<pages>969--976</pages>
<publisher>ACM Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9860" citStr="Vishwanathan et al., 2006" startWordPosition="1609" endWordPosition="1612">ion in CRFs consists in minimizing the logarithmic loss l(θ) defined by (3) with an additional `2 penalty term ρ22 kθk22, where ρ2 is a regularization parameter. The objective function is then a smooth convex function to be minimized over an unconstrained parameter space. Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popular CRF++ (Kudo, 2005) and CRFsuite (Okazaki, 2007) packages; conjugate gradient (Nocedal and Wright, 2006) and Stochastic Gradient Descent (SGD) (Bottou, 2004; Vishwanathan et al., 2006), used in CRFsgd (Bottou, 2007). The only caveat is to avoid numerical optimizers that require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs. The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many component have been zeroed (Tibshirani, 1996). Using a `1 penalty term thus implicitly performs feature selection, where ρ1 controls the amount of regularization and the number of extracted features. In the following, </context>
</contexts>
<marker>Vishwanathan, Schraudolph, Schmidt, Murphy, 2006</marker>
<rawString>S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark Schmidt, and Kevin Murphy. 2006. Accelerated training of conditional random fields with stochastic gradient methods. In Proceedings of the 23th International Conference on Machine Learning, pages 969–976. ACM Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhou</author>
<author>Trevor Hastie</author>
</authors>
<title>Regularization and variable selection via the elastic net.</title>
<date>2005</date>
<journal>J. Royal. Stat. Soc. B.,</journal>
<volume>67</volume>
<issue>2</issue>
<contexts>
<context position="10565" citStr="Zhou and Hastie, 2005" startWordPosition="1727" endWordPosition="1730">hat require the full Hessian matrix (e.g., Newton’s algorithm) due to the size of the parameter vector in usual applications of CRFs. The most significant alternative to `2 regularization is to use a `1 penalty term ρ1kθk1: such regularizers are able to yield sparse parameter vectors in which many component have been zeroed (Tibshirani, 1996). Using a `1 penalty term thus implicitly performs feature selection, where ρ1 controls the amount of regularization and the number of extracted features. In the following, we will jointly use both penalty terms, yielding the socalled elastic net penalty (Zhou and Hastie, 2005) which corresponds to the objective function l(θ) + ρ1kθk1 + ρ22 kθk2 (6) 2 The use of both penalty terms makes it possible to control the number of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)). However, the introduction of a `1 penalty term makes the optimization of (6) more problematic, as the objective function is no longer differentiable in 0. Various strategies have been proposed to handle this difficulty. We will only consider here exact approaches and will not discuss heuristic strategies such </context>
</contexts>
<marker>Zhou, Hastie, 2005</marker>
<rawString>Hui Zhou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net. J. Royal. Stat. Soc. B., 67(2):301–320.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>