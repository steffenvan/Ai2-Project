<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.990933">
Automatically Labeling Semantic Classes
</title>
<author confidence="0.993305">
Patrick Pantel and Deepak Ravichandran
</author>
<affiliation confidence="0.9961655">
Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.845122">
4676 Admiralty Way
Marina del Rey, CA 90292
</address>
<email confidence="0.999204">
{pantel,ravichan}@isi.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999696777777778">
Systems that automatically discover semantic
classes have emerged in part to address the
limitations of broad-coverage lexical re-
sources such as WordNet and Cyc. The cur-
rent state of the art discovers many semantic
classes but fails to label their concepts. We
propose an algorithm labeling semantic
classes and for leveraging them to extract is-a
relationships using a top-down approach.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955275">
The natural language literature is rich in theories of se-
mantics (Barwise and Perry 1985; Schank and Abelson
1977). However, WordNet (Miller 1990) and Cyc (Le-
nat 1995) aside, the community has had little success in
actually building large semantic repositories. Such
broad-coverage lexical resources are extremely useful in
applications such as word sense disambiguation (Lea-
cock, Chodorow and Miller 1998) and question answer-
ing (Pasca and Harabagiu 2001).
Current manually constructed ontologies such as
WordNet and Cyc have important limitations. First, they
often contain rare senses. For example, WordNet in-
cludes a rare sense of computer that means `the person
who computes&apos;. Using WordNet to expand queries to an
information retrieval system, the expansion of computer
will include words like estimator and reckoner. Also,
the words dog, computer and company all have a sense
that is a hyponym of person. Such rare senses make it
difficult for a coreference resolution system to use
WordNet to enforce the constraint that personal pro-
nouns (e.g. he or she) must refer to a person. The second
problem with these lexicons is that they miss many do-
main specific senses. For example, WordNet misses the
user-interface-object sense of the word dialog (as often
used in software manuals). WordNet also contains a
very poor coverage of proper nouns.
There is a need for (semi-) automatic approaches to
building and extending ontologies as well as for validat-
ing the structure and content of existing ones. With the
advent of the Web, we have access to enormous
amounts of text. The future of ontology growing lies in
leveraging this data by harvesting it for concepts and
semantic relationships. Moreover, once such knowledge
is discovered, mechanisms must be in place to enrich
current ontologies with this new knowledge.
To address some of the coverage and specificity
problems in WordNet and Cyc, Pantel and Lin (2002)
proposed and algorithm, called CBC, for automatically
extracting semantic classes. Their classes consist of
clustered instances like the three shown below:
</bodyText>
<keyword confidence="0.670046304347826">
(A) multiple sclerosis, diabetes,
osteoporosis, cardiovascular disease,
Parkinson&apos;s, rheumatoid arthritis, heart
disease, asthma, cancer, hypertension,
lupus, high blood pressure, arthritis,
emphysema, epilepsy, cystic fibrosis,
leukemia, hemophilia, Alzheimer, myeloma,
glaucoma, schizophrenia, ...
(S) Mike Richter, Tommy Salo, John
Vanbiesbrouck, Curtis Joseph, Chris Osgood,
Steve Shields, Tom Barrasso, Guy Hebert,
Arturs Irbe, Byron Dafoe, Patrick Roy, Bill
Ranford, Ed Belfour, Grant Fuhr, Dominik
Hasek, Martin Brodeur, Mike Vernon, Ron
Tugnutt, Sean Burke, Zach Thornton, Jocelyn
Thibault, Kevin Hartman, Felix Potvin, ...
(C) pink, red, turquoise, blue, purple,
green, yellow, beige, orange, taupe, white,
lavender, fuchsia, brown, gray, black,
mauve, royal blue, violet, chartreuse,
teal, gold, burgundy, lilac, crimson,
garnet, coral, grey, silver, olive green,
cobalt blue, scarlet, tan, amber, ...
</keyword>
<bodyText confidence="0.999965694444444">
A limitation of these concepts is that CBC does not
discover their actual names. That is, CBC discovers a
semantic class of Canadian provinces such as Manitoba,
Alberta, and Ontario, but stops short of labeling the
concept as Canadian Provinces. Some applications such
as question answering would benefit from class labels.
For example, given the concept list (B) and a label
goalie/goaltender, a QA system could look for answers
to the question &amp;quot;Which goaltender won the most Hart
Trophys?&amp;quot; in the concept.
In this paper, we propose an algorithm for automati-
cally inducing names for semantic classes and for find-
ing instance/concept (is-a) relationships. Using concept
signatures (templates describing the prototypical syntac-
tic behavior of instances of a concept), we extract con-
cept names by searching for simple syntactic patterns
such as &amp;quot;concept apposition-of instance&amp;quot;. Searching
concept signatures is more robust than searching the
syntactic features of individual instances since many
instances suffer from sparse features or multiple senses.
Once labels are assigned to concepts, we can extract
a hyponym relationship between each instance of a con-
cept and its label. For example, once our system labels
list (C) as color, we may extract relationships such as:
pink is a color, red is a color, turquoise is a color, etc.
Our results show that of the 159,000 hyponyms we ex-
tract using this simple method, 68% are correct. Of the
65,000 proper name hyponyms we discover, 81.5% are
correct.
The remainder of this paper is organized as follows.
In the next section, we review previous algorithms for
extracting semantic classes and hyponym relationships.
Section 3 describes our algorithm for labeling concepts
and for extracting hyponym relationships. Experimental
results are presented in Section 4 and finally, we con-
clude with a discussion and future work.
</bodyText>
<sectionHeader confidence="0.995812" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.989689254545454">
There have been several approaches to automatically
discovering lexico-semantic information from text
(Hearst 1992; Riloff and Shepherd 1997; Riloff and
Jones 1999; Berland and Charniak 1999; Pantel and Lin
2002; Fleischman et al. 2003; Girju et al. 2003). One
approach constructs automatic thesauri by computing
the similarity between words based on their distribution
in a corpus (Hindle 1990; Lin 1998). The output of
these programs is a ranked list of similar words to each
word. For example, Lin&apos;s approach outputs the follow-
ing top-20 similar words of orange:
(D) peach, grapefruit, yellow, lemon, pink,
avocado, tangerine, banana, purple, Santa
Ana, strawberry, tomato, red, pineapple,
pear, Apricot, apple, green, citrus, mango
A common problem of such lists is that they do not
discriminate between the senses of polysemous words.
For example, in (D), the color and fruit senses of orange
are mixed up.
Lin and Pantel (2001) proposed a clustering algo-
rithm, UNICON, which generates similar lists but
discriminates between senses of words. Later, Pantel
and Lin (2002) improved the precision and recall of
UNICON clusters with CBC (Clustering by Commit-
tee). Using sets of representative elements called com-
mittees, CBC discovers cluster centroids that
unambiguously describe the members of a possible
class. The algorithm initially discovers committees that
are well scattered in the similarity space. It then pro-
ceeds by assigning elements to their most similar com-
mittees. After assigning an element to a cluster, CBC
removes their overlapping features from the element
before assigning it to another cluster. This allows CBC
to discover the less frequent senses of a word and to
avoid discovering duplicate senses.
CBC discovered both the color sense of orange, as
shown in list (C) of Section 1, and the fruit sense shown
below:
(E) peach, pear, apricot, strawberry, ba-
nana, mango, melon, apple, pineapple,
cherry, plum, lemon, grapefruit, orange,
berry, raspberry, blueberry, kiwi, ...
There have also been several approaches to discov-
ering hyponym (is-a) relationships from text. Hearst
(1992) used seven lexico-syntactic patterns, for example
&amp;quot;such NP as {NP,}*{(or|and)} NP&amp;quot; and &amp;quot;NP {, NP}*{,}
or other NP&amp;quot;. Berland and Charniak (1999) used similar
pattern-based techniques and other heuristics to extract
meronymy (part-whole) relations. They reported an
accuracy of about 55% precision on a corpus of 100,000
words. Girju, Badulescu and Moldovan (2003)
improved upon this work by using a machine learning
filter. Mann (2002) and Fleischman et al. (2003) used
part of speech patterns to extract a subset of hyponym
relations involing proper nouns.
</bodyText>
<sectionHeader confidence="0.952131" genericHeader="method">
3 Labeling Classes
</sectionHeader>
<bodyText confidence="0.999960774193548">
The research discussed above on discovering hyponym
relationships all take a bottom up approach. That is,
they use patterns to independently discover semantic
relationships of words. However, for infrequent words,
these patterns do not match or, worse yet, generate in-
correct relationships.
Ours is a top down approach. We make use of co-
occurrence statistics of semantic classes discovered by
algorithms like CBC to label their concepts. Hyponym
relationships may then be extracted easily: one hypo-
nym per instance/concept label pair. For example, if we
labeled concept (A) from Section 1 with disease, then
we could extract is-a relationships such as: diabetes is a
disease, cancer is a disease, and lupus is a disease. A
concept instance such as lupus is assigned a hypernym
disease not because it necessarily occurs in any particu-
lar syntactic relationship with disease, but because it
belongs to the class of instances that does.
The input to our labeling algorithm is a list of se-
mantic classes, in the form of clusters of words, which
may be generated from any source. In our experiments,
we used the clustering outputs of CBC (Pantel and Lin
2002). The output of the system is a ranked list of con-
cept names for each semantic class.
In the first phase of the algorithm, we extract feature
vectors for each word that occurs in a semantic class.
Phase II then uses these features to compute grammati-
cal signatures of concepts using the CBC algorithm.
Finally, we use simple syntactic patterns to discover
class names from each class&apos; signature. Below, we de-
scribe these phases in detail.
</bodyText>
<subsectionHeader confidence="0.998297">
3.1 Phase I
</subsectionHeader>
<bodyText confidence="0.999994666666667">
We represent each word (concept instance) by a feature
vector. Each feature corresponds to a context in which
the word occurs. For example, &amp;quot;catch _&amp;quot; is a verb-
object context. If the word wave occurred in this con-
text, then the context is a feature of wave.
We first construct a frequency count vector C(e) =
(ce1, ce2, �, cem), where m is the total number of features
and cef is the frequency count of feature f occurring in
word e. Here, cef is the number of times word e occurred
in a grammatical context f. For example, if the word
wave occurred 217 times as the object of the verb catch,
then the feature vector for wave will have value 217 for
its &amp;quot;object-of catch&amp;quot; feature. In Section 4.1, we describe
how we obtain these features.
We then construct a mutual information vector
MI(e) = (mie1, mie2, ..., miem) for each word e, where mief
is the pointwise mutual information between word e and
feature f, which is defined as:
</bodyText>
<equation confidence="0.963881">
cef
mief = log n N m (1)
N N
</equation>
<bodyText confidence="0.99958875">
Following (Pantel and Lin 2002), we construct a com-
mittee for each semantic class. A committee is a set of
representative elements that unambiguously describe the
members of a possible class.
For each class c, we construct a matrix containing
the similarity between each pair of words ei and ej in c
using the cosine coefficient of their mutual information
vectors (Salton and McGill 1983):
</bodyText>
<equation confidence="0.9669208">
f
∑ ∑
mief2 ×
i
f f
</equation>
<bodyText confidence="0.9967074">
For each word e, we then cluster its most similar in-
stances using group-average clustering (Han and Kam-
ber 2001) and we store as a candidate committee the
highest scoring cluster c&apos; according to the following
metric:
</bodyText>
<equation confidence="0.967972">
 |c&apos; |x avgsim(c&apos;) (4)
</equation>
<bodyText confidence="0.999897">
where |c&apos; |is the number of elements in c&apos; and avgsim(c&apos;)
is the average pairwise similarity between words in c&apos;.
The assumption is that the best representative for a con-
cept is a large set of very similar instances. The commit-
tee for class c is then the highest scoring candidate
committee containing only words from c. For example,
below are the committee members discovered for the
semantic classes (A), (B), and (C) from Section 1:
</bodyText>
<equation confidence="0.712998060606061">
1) cardiovascular disease, diabetes,
multiple sclerosis, osteoporosis,
Parkinson&apos;s, rheumatoid arthritis
 n m 
 ∑∑
cei, c 
jf i = 1 j = 1 
min
cef
1
 n m 
 c c 
ei jf
 i = 1 j = 1 
+ ∑ ∑
min , +
1
3.2 Phase II
∑mi m
e f ×
i
i
ef
j
∑= × ∑ =
c c
if ej
i 1 j 1
(2)
( )
sim e e
, =
i j
</equation>
<page confidence="0.199124">
2
</page>
<figure confidence="0.959513133333333">
mi
ef
j
(3)
2) Curtis Joseph, John Vanbiesbrouck, Mike
Richter, Tommy Salo
3) blue, pink, red, yellow
c
is the
∑∑
1
ij
i= =
1 j
total fr
</figure>
<bodyText confidence="0.975661857142857">
equency count of all features of all words.
Mutual information is commonly used to measure
the association strength between two words (Church and
Hanks 1989). A well-known problem is that mutual
information is biased towards infrequent ele-
ments/features. We therefore multiply mief with the fol-
lowing discounting factor:
</bodyText>
<subsectionHeader confidence="0.989987">
3.3 Phase III
</subsectionHeader>
<bodyText confidence="0.968046666666667">
n m
where n is the number of words and N =
cef
×
By averaging the feature vectors of the committee
members of a particular semantic class, we obtain a
grammatical template, or signature, for that class. For
example, Figure 1 shows an excerpt of the grammatical
signature for concept (B) in Section 1. The vector is
obtained by averaging the feature vectors for the words
Curtis Joseph, John Vanbiesbrouck, Mike Richter, and
Tommy Salo (the committee of this concept). The
</bodyText>
<table confidence="0.9982225">
{Curtis Joseph, John Vanbiesbrouck,
Mike Richter, Tommy Salo}
-N:gen:N
pad 57 11.19
backup 29 9.95
crease 7 9.69
glove 52 9.57
stick 20 9.15
shutout 17 8.80
-N:conj:N
Hasek 15 12.36
Martin Brodeur 12 12.26
Belfour 13 12.22
Patrick Roy 10 11.90
Dominik Hasek 7 11.20
Roy 6 10.01
-V:subj:N 11 6.69
sprawl
misplay 6 6.55
smother 10 6.54
skate 28 6.43
turn back 10 6.28
stop 453 6.19
N:appo:N 449 10.79
goaltender
goalie 1641 10.76
netminder 57 10.39
goalkeeper 487 9.69
N:conj:N
Martin Brodeur 11 12.49
Dominik Hasek 11 12.33
Ed Belfour 10 12.04
Curtis Joseph 7 11.46
Tom Barrasso 5 10.85
Byron Dafoe 5 10.80
Chris Osgood 4 10.25
</table>
<figureCaption confidence="0.9982655">
Figure 1. Excerpt of the grammatical signature for the
goalie/goaltender concept.
</figureCaption>
<bodyText confidence="0.99986925">
&amp;quot;-V:subj:N:sprawl� feature indicates a subject-verb re-
lationship between the concept and the verb sprawl
while &amp;quot;N:appo:N:goaltender&amp;quot; indicates an apposition
relationship between the concept and the noun goal-
tender. The (-) in a relationship means that the right
hand side of the relationship is the head (e.g. sprawl is
the head of the subject-verb relationship). The two col-
umns of numbers indicate the frequency and mutual
information score for each feature respectively.
In order to discover the characteristics of human
naming conventions, we manually named 50 concepts
discovered by CBC. For each concept, we extracted the
relationships between the concept committee and the
assigned label. We then added the mutual information
scores for each extracted relationship among the 50
concepts. The top-4 highest scoring relationships are:
</bodyText>
<figure confidence="0.671514454545455">
• Apposition (N:appo:N)
e.g. ... Oracle, a company known
for its progressive employment
policies, ...
• Nominal subject (-N:subj:N)
e.g. ... Apple was a hot young com-
pany, with Steve Jobs in charge.
• Such as (-N:such as:N)
e.g. ... companies such as IBM must
be weary ...
• Like (-N:like:N)
</figure>
<figureCaption confidence="0.734574666666667">
e.g. ... companies like Sun Micro-
systems do no shy away from such
challenges, ...
</figureCaption>
<bodyText confidence="0.963425875">
To name a class, we simply search for these syntac-
tic relationships in the signature of a concept. We sum
up the mutual information scores for each term that oc-
curs in these relationships with a committee of a class.
The highest scoring term is the name of the class. For
example, the top-5 scoring terms that occurred in these
relationships with the signature of the concept repre-
sented by the committee {Curtis Joseph, John
Vanbiesbrouck, Mike Richter, Tommy Salo} are:
goalie 40.37
goaltender 33.64
goalkeeper 19.22
player 14.55
backup 9.40
The numbers are the total mutual information scores
of each name in the four syntactic relationships.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999784333333333">
In this section, we present an evaluation of the class
labeling algorithm and of the hyponym relationships
discovered by our system.
</bodyText>
<subsectionHeader confidence="0.978857">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9997988">
We used Minipar (Lin 1994), a broad coverage parser,
to parse 3GB of newspaper text from the Aquaint
(TREC-9) collection. We collected the frequency counts
of the grammatical relationships (contexts) output by
Minipar and used them to compute the pointwise mutual
information vectors described in Section 3.1.
We used the 1432 noun clusters extracted by CBC1
as the list of concepts to name. For each concept, we
then used our algorithm described in Section 3 to extract
the top-20 names for each concept.
</bodyText>
<footnote confidence="0.393022">
1 Available at http://www.isi.edu/~pantel/demos.htm
</footnote>
<tableCaption confidence="0.999413">
Table 1. Labels assigned to 10 randomly selected concepts (each represented by three committee members.
</tableCaption>
<table confidence="0.738513375">
CBC CONCEPT HUMAN LABEL WORDNET LABELS SYSTEM LABELS (RANKED)
BMG, EMI, Sony record label none label / company / album /
machine / studio
Preakness Stakes, Preakness, Belmont horse race none race / event / run / victory /
Stakes start
Olympia Snowe, Susan Collins, James US senator none republican / senator / chair-
Jeffords man / supporter / conservative
Eldoret, Kisumu, Mombasa African city none city / port / cut off / town /
southeast
Bronze Star, Silver Star, Purple Heart medal decoration / laurel distinction / set / honor / sym-
wreath / medal / medal- bol
lion / palm
NHL goalie none goalie / goaltender / goal-
keeper / player / backup
Dodoma, Mwanza, Mbeya African city none facilitator / town
fresco, wall painting, Mural art painting / picture painting / world / piece / floor
</table>
<figure confidence="0.852647666666667">
/ symbol
university none university / institution / stock-
holder / college / school
Federal Bureau of Investigation, Drug governmental depart- law enforcement agency agency / police / investigation
Enforcement Administration, FBI ment / department / FBI
Mike Richter, Tommy Salo, John
Vanbiesbrouck
Qinghua University, Fudan University,
Beijing University
</figure>
<subsectionHeader confidence="0.991242">
4.2 Labeling Precision
</subsectionHeader>
<bodyText confidence="0.999982354166666">
Out of the 1432 noun concepts, we were unable to name
21 (1.5%) of them. This occurs when a concept&apos;s com-
mittee members do not occur in any of the four syntactic
relationships described in Section 0. We performed a
manual evaluation of the remaining 1411 concepts.
We randomly selected 125 concepts and their top-5
highest ranking names according to our algorithm. Ta-
ble 1 shows the first 10 randomly selected concepts
(each concept is represented by three of its committee
members).
For each concept, we added to the list of names a
human generated name (obtained from an annotator
looking at only the concept instances). We also ap-
pended concept names extracted from WordNet. For
each concept that contains at least five instances in the
WordNet hierarchy, we named the concept with the
most frequent common ancestor of each pair of in-
stances. Up to five names were generated by WordNet
for each concept. Because of the low coverage of proper
nouns in WordNet, only 33 of the 125 concepts we
evaluated had WordNet generated labels.
We presented to three human judges the 125 ran-
domly selected concepts together with the system, hu-
man, and WordNet generated names randomly ordered.
That way, there was no way for a judge to know the
source of a label nor the system&apos;s ranking of the labels.
For each name, we asked the judges to assign a score of
correct, partially correct, or incorrect. We then com-
puted the mean reciprocal rank (MRR) of the system,
human, and WordNet labels. For each concept, a nam-
ing scheme receives a score of 1 / M where M is the
rank of the first name judged correct. Table 2 shows the
results. Table 3 shows similar results for a more lenient
evaluation where M is the rank of the first name judged
correct or partially correct.
Our system achieved an overall MRR score of
77.1%. We performed much better than the baseline
WordNet (19.9%) because of the lack of coverage
(mostly proper nouns) in the hierarchy. For the 33 con-
cepts that WordNet named, it achieved a score of 75.3%
and a lenient score of 82.7%, which is high considering
the simple algorithm we used to extract labels using
WordNet.
The Kappa statistic (Siegel and Castellan Jr. 1988)
measures the agreements between a set of judges&apos; as-
sessments correcting for chance agreements:
where P(A) is the probability of agreement between the
judges and P(E) is the probability that the judges agree
</bodyText>
<equation confidence="0.987346833333334">
P( A)− P(E)
1
−
P(E)
=
K
</equation>
<page confidence="0.865263">
(5)
</page>
<tableCaption confidence="0.87429275">
Table 2. MRR scores for the human evaluation of naming 125
random concepts.
Table 3. Lenient MRR scores for the human evaluation of
naming 125 random concepts.
</tableCaption>
<table confidence="0.9998905">
JUDGE HUMAN WordNet System
LABELS Labels Labels
1 100% 18.1% 74.4%
2 91.2% 20.0% 78.1%
3 89.6% 21.6% 78.8%
Combined 93.6% 19.9% 77.1%
</table>
<tableCaption confidence="0.984625">
Table 4. Percentage of concepts with a correct name in the
top-5 ranks returned by our system.
</tableCaption>
<table confidence="0.999970454545454">
JUDGE TOP-1 TOP-2 TOP-3 TOP-4 TOP-5
1 68.8% 75.2% 78.4% 83.2% 84.0%
2 73.6% 80.0% 81.6% 83.2% 84.8%
3 73.6% 80.0% 82.4% 84.0% 88.8%
Combined 72.0% 78.4% 80.8% 83.5% 85.6%
JUDGE HUMAN WordNet System
LABELS Labels Labels
1 100% 22.8% 85.0%
2 96.0% 20.8% 86.5%
3 92.0% 21.8% 85.2%
Combined 96.0% 21.8% 85.6%
</table>
<tableCaption confidence="0.963077">
Table 5. Accuracy of 159,000 extracted hyponyms and a sub-
set of 65,000 proper noun hyponyms.
</tableCaption>
<table confidence="0.9994004">
JUDGE All Nouns Proper Nouns
Strict Lenient Strict Lenient
1 62.0% 68.0% 79.0% 82.0%
2 74.0% 76.5% 84.0% 85.5%
Combined 68.0% 72.2% 81.5% 83.8%
</table>
<bodyText confidence="0.9994065625">
by chance on an assessment. An experiment with K ≥
0.8 is generally viewed as reliable and 0.67 &lt; K &lt; 0.8
allows tentative conclusions. The Kappa statistic for our
experiment is K = 0.72.
The human labeling is at a disadvantage since only
one label was generated per concept. Therefore, the
human scores either 1 or 0 for each concept. Our sys-
tem&apos;s highest ranking name was correct 72% of the
time. Table 4 shows the percentage of semantic classes
with a correct label in the top 1-5 ranks returned by our
system.
Overall, 41.8% of the top-5 names extracted by our
system were judged correct. The overall accuracy for
the top-4, top-3, top-2, and top-1 names are 44.4%,
48.8%, 58.5%, and 72% respectively. Hence, the name
ranking of our algorithm is effective.
</bodyText>
<subsectionHeader confidence="0.998551">
4.3 Hyponym Precision
</subsectionHeader>
<bodyText confidence="0.902838">
The 1432 CBC concepts contain 18,000 unique words.
For each concept to which a word belongs, we extracted
up to 3 hyponyms, one for each of the top-3 labels for
the concept. The result was 159,000 hyponym relation-
ships. 24 are shown in the Appendix.
Two judges annotated two random samples of 100
relationships: one from all 159,000 hyponyms and one
from the subset of 65,000 proper nouns. For each in-
stance, the judges were asked to decide whether the
hyponym relationship was correct, partially correct or
incorrect. Table 5 shows the results. The strict measure
counts a score of 1 for each correctly judged instance
and 0 otherwise. The lenient measure also gives a score
of 0.5 for each instance judged partially correct.
Many of the CBC concepts contain noise. For ex-
ample, the wine cluster:
Zinfandel, merlot, Pinot noir, Chardonnay,
Cabernet Sauvignon, cabernet, riesling,
Sauvignon blanc, Chenin blanc, sangiovese,
syrah, Grape, Chianti ...
contains some incorrect instances such as grape, appe-
lation, and milk chocolate. Each of these instances will
generate incorrect hyponyms such as grape is wine and
milk chocolate is wine. This hyponym extraction task
would likely serve well for evaluating the accuracy of
lists of semantic classes.
Table 5 shows that the hyponyms involving proper
nouns are much more reliable than common nouns.
Since WordNet contains poor coverage of proper nouns,
these relationships could be useful to enrich it.
</bodyText>
<subsectionHeader confidence="0.950963">
4.4 Recall
</subsectionHeader>
<bodyText confidence="0.9992446">
Semantic extraction tasks are notoriously difficult to
evaluate for recall. To approximate recall, we conducted
two question answering (QA) tasks: answering
definition questions and performing QA information
retrieval.
</bodyText>
<tableCaption confidence="0.958781">
Table 6. Percentage of correct answers in the Top-1 and
Top-5 returned answers on 50 definition questions.
</tableCaption>
<table confidence="0.9998708">
SYSTEM Top-1 Top-5
Strict Lenient Strict Lenient
WordNet 38% 38% 38% 38%
Fleischman 36% 40% 42% 44%
Our System 36% 44% 60% 62%
</table>
<tableCaption confidence="0.991295333333333">
Table 7. Percentage of questions where the passage retrieval
module returns a correct answer in the Top-1 and Top-100
ranked passages (with and without semantic indexing).
</tableCaption>
<table confidence="0.944162">
CORRECT TOP-1 Correct Top-100
With semantic 43 / 179 134 / 179
indexing
Without semantic 36 / 179 131 / 179
indexing
</table>
<subsectionHeader confidence="0.5654">
Definition Questions
</subsectionHeader>
<bodyText confidence="0.999990533333333">
We chose the 50 definition questions that appeared in
the QA track of TREC2003 (Voorhees, 2003). For ex-
ample: &amp;quot;Who is Aaron Copland?&amp;quot; and &amp;quot;What is the
Kama Sutra?&amp;quot; For each question we looked for at most
five corresponding concepts in our hyponym list. For
example, for Aaron Copland, we found the following
hypernyms: composer, music, and gift. We compared
our system with the concepts in WordNet and Fleisch-
man et al.&apos;s instance/concept relations (Fleischman et al.
2003). Table 6 shows the percentage of correct answers
in the top-1 and top-5 returned answers from each sys-
tem. All systems seem to have similar performance on
the top-1 answers, but our system has many more an-
swers in the top-5. This shows that our system has com-
paratively higher recall for this task.
</bodyText>
<sectionHeader confidence="0.547235" genericHeader="method">
Information (Passage) Retrieval
</sectionHeader>
<bodyText confidence="0.999975612903226">
Passage retrieval is used in QA to supply relevant in-
formation to an answer pinpointing module. The higher
the performance of the passage retrieval module, the
higher will be the performance of the answer pinpoint-
ing module.
The passage retrieval module can make use of the
hyponym relationships that are discovered by our sys-
tem. Given a question such as &amp;quot;What color ...&amp;quot;, the like-
lihood of a correct answer being present in a retrieved
passage is greatly increased if we know the set of all
possible colors and index them in the document collec-
tion appropriately.
We used the hyponym relations learned by our sys-
tem to perform semantic indexing on a QA passage re-
trieval task. We selected the 179 questions from the QA
track of TREC-2003 that had an explicit semantic an-
swer type (e.g. &amp;quot;What band was Jerry Garcia with?&amp;quot;
and &amp;quot;What color is the top stripe on the U.S. flag?&amp;quot;).
For each expected semantic answer type corresponding
to a given question (e.g. band and color), we indexed
the entire TREC-2002 IR collection with our system&apos;s
hyponyms.
We compared the passages returned by the passage
retrieval module with and without the semantic index-
ing. We counted how many of the 179 questions had a
correct answer returned in the top-1 and top-100 pas-
sages. Table 7 shows the results.
Our system shows small gains in the performance of
the IR output. In the top-1 category, the performance
improved by 20%. This may lead to better answer selec-
tions.
</bodyText>
<sectionHeader confidence="0.998859" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999970806451613">
Current state of the art concept discovery algorithms
generate lists of instances of semantic classes but stop
short of labeling the classes with concept names. Class
labels would serve useful in applications such as ques-
tion answering to map a question concept into a seman-
tic class and then search for answers within that class.
We propose here an algorithm for automatically label-
ing concepts that searches for syntactic patterns within a
grammatical template for a class. Of the 1432 noun con-
cepts discovered by CBC, our system labelled 98.5% of
them with an NRR score of 77.1% in a human evalua-
tion.
Hyponym relationships were then easily extracted,
one for each instance/concept label pair. We extracted
159,000 hyponyms and achieved a precision of 68%. On
a subset of 65,000 proper names, our performance was
81.5%.
This work forms an important attempt to building
large-scale semantic knowledge bases. Without being
able to automatically name a cluster and extract hypo-
nym/hypernym relationships, the utility of automatically
generated clusters or manually compiled lists of terms is
limited. Of course, it is a serious open question how
many names each cluster (concept) should have, and
how good each name is. Our method begins to address
this thorny issue by quantifying the name assigned to a
class and by simultaneously assigning a number that can
be interpreted to reflect the strength of membership of
each element to the class. This is potentially a signifi-
cant step away from traditional all-or-nothing seman-
tic/ontology representations to a concept representation
</bodyText>
<table confidence="0.986702111111111">
Appendix. Sample hyponyms discovered by our system.
INSTANCE CONCEPT INSTANCE CONCEPT
actor hero price support benefit
Ameritrade brokerage republican politician
Arthur pitcher Royal Air force
Rhodes Force
bebop MUSIC Rwanda city
Buccaneer team Santa Ana city
Congressional agency shot-blocker player
Research
Service
Cuba country slavery issue
Dan Petrescu midfielder spa facility
Hercules aircraft taxi vehicle
Moscow city Terrence director
Malick
Nokia COMPANY verbena tree
nominee candidate Wagner composer
</table>
<bodyText confidence="0.9933445">
scheme that is more nuanced and admits multiple names
and graded set memberships.
</bodyText>
<sectionHeader confidence="0.998406" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999877666666667">
The authors wish to thank the reviewers for their helpful
comments. This research was partly supported by NSF
grant #EIA-0205111.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849194029851">
Barwise, J. and Perry, J. 1985. Semantic innocence and un-
compromising situations. In: Martinich, A. P. (ed.) The
Philosophy of Language. New York: Oxford University
Press. pp. 401413.
Berland, M. and E. Charniak, 1999. Finding parts in very large
corpora. In ACL-1999. pp. 5764. College Park, MD.
Church, K. and Hanks, P. 1989. Word association norms, mu-
tual information, and lexicography. In Proceedings of ACL-
89. pp. 7683. Vancouver, Canada.
Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline
strategies for online question answering: Answering ques-
tions before they are asked. In Proceedings of ACL-03. pp.
1-7. Sapporo, Japan.
Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning
semantic constraints for the automatic discovery of part-
whole relations. In Proceedings of HLT/NAACL-03. pp.
8087. Edmonton, Canada.
Han, J. and Kamber, M. 2001. Data Mining - Concepts and
Techniques. Morgan Kaufmann.
Hearst, M. 1992. Automatic acquisition of hyponyms from
large text corpora. In COLING-92. pp. 539545. Nantes,
France.
Hindle, D. 1990. Noun classification from predicate-argument
structures. In Proceedings of ACL-90. pp. 268275. Pitts-
burgh, PA.
Leacock, C.; Chodorow, M.; and Miller; G. A. 1998. Using
corpus statistics and WordNet relations for sense identifica-
tion. Computational Linguistics, 24(1):147-165.
Lenat, D. 1995. CYC: A large-scale investment in knowledge
infrastructure. Communications of the ACM, 38(11):33-38.
Lin, D. 1994. Principar - an efficient, broad-coverage, princi-
ple-based parser. Proceedings of COLING-94. pp. 4248.
Kyoto, Japan.
Lin, D. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of COLING/ACL-98. pp. 768774.
Montreal, Canada.
Lin, D. and Pantel, P. 2001. Induction of semantic classes
from natural language text. In Proceedings of SIGKDD-01.
pp. 317322. San Francisco, CA.
Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies
for Question Answering. SemaNet&apos; 02: Building and
Using Semantic Networks, Taipei, Taiwan.
Miller, G. 1990. WordNet: An online lexical database. Inter-
national Journal of Lexicography, 3(4).
Pasca, M. and Harabagiu, S. 2001. The informative role of
WordNet in Open-Domain Question Answering. In Pro-
ceedings of NAACL-01 Workshop on WordNet and Other
Lexical Resources. pp. 138143. Pittsburgh, PA.
Pantel, P. and Lin, D. 2002. Discovering Word Senses from
Text. In Proceedings of SIGKDD-02. pp. 613619. Edmon-
ton, Canada.
Riloff, E. and Shepherd, J. 1997. A corpus-based approach for
building semantic lexicons. In Proceedings of EMNLP-
1997.
Riloff, E. and Jones, R. 1999. Learning dictionaries for infor-
mation extraction by multi-level bootstrapping. In Proceed-
ings of AAAI-99. pp. 474479. Orlando, FL.
Salton, G. and McGill, M. J. 1983. Introduction to Modern
Information Retrieval. McGraw Hill
Schank, R. and Abelson, R. 1977. Scripts, Plans, Goals and
Understanding: An Inquiry into Human Knowledge Struc-
tures. Lawrence Erlbaum Associates.
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statis-
tics for the Behavioral Sciences. McGraw-Hill.
Voorhees, E. 2003. Overview of the question answering track.
To appear in Proceedings of TREC-12 Conference. NIST,
Gaithersburg, MD.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912204">
<title confidence="0.999959">Automatically Labeling Semantic Classes</title>
<author confidence="0.984758">Patrick Pantel</author>
<author confidence="0.984758">Deepak</author>
<affiliation confidence="0.998584">Information Sciences University of Southern</affiliation>
<address confidence="0.98667">4676 Admiralty</address>
<author confidence="0.946794">Marina del Rey</author>
<author confidence="0.946794">CA</author>
<email confidence="0.998984">pantel@isi.edu</email>
<email confidence="0.998984">ravichan@isi.edu</email>
<abstract confidence="0.9993621">Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc. The current state of the art discovers many semantic classes but fails to label their concepts. We propose an algorithm labeling semantic and for leveraging them to extract relationships using a top-down approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>J Perry</author>
</authors>
<title>Semantic innocence and uncompromising situations.</title>
<date>1985</date>
<booktitle>The Philosophy of Language.</booktitle>
<pages>401--413</pages>
<editor>In: Martinich, A. P. (ed.)</editor>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="715" citStr="Barwise and Perry 1985" startWordPosition="99" endWordPosition="102">iences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 {pantel,ravichan}@isi.edu Abstract Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc. The current state of the art discovers many semantic classes but fails to label their concepts. We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach. 1 Introduction The natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977). However, WordNet (Miller 1990) and Cyc (Lenat 1995) aside, the community has had little success in actually building large semantic repositories. Such broad-coverage lexical resources are extremely useful in applications such as word sense disambiguation (Leacock, Chodorow and Miller 1998) and question answering (Pasca and Harabagiu 2001). Current manually constructed ontologies such as WordNet and Cyc have important limitations. First, they often contain rare senses. For example, WordNet includes a rare sense of computer that means `the person who computes&apos;. Using </context>
</contexts>
<marker>Barwise, Perry, 1985</marker>
<rawString>Barwise, J. and Perry, J. 1985. Semantic innocence and uncompromising situations. In: Martinich, A. P. (ed.) The Philosophy of Language. New York: Oxford University Press. pp. 401413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In ACL-1999.</booktitle>
<pages>57--64</pages>
<location>College Park, MD.</location>
<contexts>
<context position="5690" citStr="Berland and Charniak 1999" startWordPosition="854" endWordPosition="857">per name hyponyms we discover, 81.5% are correct. The remainder of this paper is organized as follows. In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships. Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not dis</context>
<context position="7751" citStr="Berland and Charniak (1999)" startWordPosition="1176" endWordPosition="1179"> allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. CBC discovered both the color sense of orange, as shown in list (C) of Section 1, and the fruit sense shown below: (E) peach, pear, apricot, strawberry, banana, mango, melon, apple, pineapple, cherry, plum, lemon, grapefruit, orange, berry, raspberry, blueberry, kiwi, ... There have also been several approaches to discovering hyponym (is-a) relationships from text. Hearst (1992) used seven lexico-syntactic patterns, for example &amp;quot;such NP as {NP,}*{(or|and)} NP&amp;quot; and &amp;quot;NP {, NP}*{,} or other NP&amp;quot;. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations. They reported an accuracy of about 55% precision on a corpus of 100,000 words. Girju, Badulescu and Moldovan (2003) improved upon this work by using a machine learning filter. Mann (2002) and Fleischman et al. (2003) used part of speech patterns to extract a subset of hyponym relations involing proper nouns. 3 Labeling Classes The research discussed above on discovering hyponym relationships all take a bottom up approach. That is, they use patterns to independently discover semantic relation</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Berland, M. and E. Charniak, 1999. Finding parts in very large corpora. In ACL-1999. pp. 5764. College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of ACL89.</booktitle>
<pages>76--83</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="12425" citStr="Church and Hanks 1989" startWordPosition="2027" endWordPosition="2030">), (B), and (C) from Section 1: 1) cardiovascular disease, diabetes, multiple sclerosis, osteoporosis, Parkinson&apos;s, rheumatoid arthritis  n m   ∑∑ cei, c  jf i = 1 j = 1  min cef 1  n m   c c  ei jf  i = 1 j = 1  + ∑ ∑ min , + 1 3.2 Phase II ∑mi m e f × i i ef j ∑= × ∑ = c c if ej i 1 j 1 (2) ( ) sim e e , = i j 2 mi ef j (3) 2) Curtis Joseph, John Vanbiesbrouck, Mike Richter, Tommy Salo 3) blue, pink, red, yellow c is the ∑∑ 1 ij i= = 1 j total fr equency count of all features of all words. Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989). A well-known problem is that mutual information is biased towards infrequent elements/features. We therefore multiply mief with the following discounting factor: 3.3 Phase III n m where n is the number of words and N = cef × By averaging the feature vectors of the committee members of a particular semantic class, we obtain a grammatical template, or signature, for that class. For example, Figure 1 shows an excerpt of the grammatical signature for concept (B) in Section 1. The vector is obtained by averaging the feature vectors for the words Curtis Joseph, John Vanbiesbrouck, Mike Richter, an</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Church, K. and Hanks, P. 1989. Word association norms, mutual information, and lexicography. In Proceedings of ACL89. pp. 7683. Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>E Hovy</author>
<author>A Echihabi</author>
</authors>
<title>Offline strategies for online question answering: Answering questions before they are asked.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-03.</booktitle>
<pages>1--7</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5735" citStr="Fleischman et al. 2003" startWordPosition="862" endWordPosition="865">. The remainder of this paper is organized as follows. In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships. Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous wo</context>
<context position="8071" citStr="Fleischman et al. (2003)" startWordPosition="1224" endWordPosition="1227">uit, orange, berry, raspberry, blueberry, kiwi, ... There have also been several approaches to discovering hyponym (is-a) relationships from text. Hearst (1992) used seven lexico-syntactic patterns, for example &amp;quot;such NP as {NP,}*{(or|and)} NP&amp;quot; and &amp;quot;NP {, NP}*{,} or other NP&amp;quot;. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations. They reported an accuracy of about 55% precision on a corpus of 100,000 words. Girju, Badulescu and Moldovan (2003) improved upon this work by using a machine learning filter. Mann (2002) and Fleischman et al. (2003) used part of speech patterns to extract a subset of hyponym relations involing proper nouns. 3 Labeling Classes The research discussed above on discovering hyponym relationships all take a bottom up approach. That is, they use patterns to independently discover semantic relationships of words. However, for infrequent words, these patterns do not match or, worse yet, generate incorrect relationships. Ours is a top down approach. We make use of cooccurrence statistics of semantic classes discovered by algorithms like CBC to label their concepts. Hyponym relationships may then be extracted easil</context>
<context position="24463" citStr="Fleischman et al. 2003" startWordPosition="4028" endWordPosition="4031"> indexing). CORRECT TOP-1 Correct Top-100 With semantic 43 / 179 134 / 179 indexing Without semantic 36 / 179 131 / 179 indexing Definition Questions We chose the 50 definition questions that appeared in the QA track of TREC2003 (Voorhees, 2003). For example: &amp;quot;Who is Aaron Copland?&amp;quot; and &amp;quot;What is the Kama Sutra?&amp;quot; For each question we looked for at most five corresponding concepts in our hyponym list. For example, for Aaron Copland, we found the following hypernyms: composer, music, and gift. We compared our system with the concepts in WordNet and Fleischman et al.&apos;s instance/concept relations (Fleischman et al. 2003). Table 6 shows the percentage of correct answers in the top-1 and top-5 returned answers from each system. All systems seem to have similar performance on the top-1 answers, but our system has many more answers in the top-5. This shows that our system has comparatively higher recall for this task. Information (Passage) Retrieval Passage retrieval is used in QA to supply relevant information to an answer pinpointing module. The higher the performance of the passage retrieval module, the higher will be the performance of the answer pinpointing module. The passage retrieval module can make use o</context>
</contexts>
<marker>Fleischman, Hovy, Echihabi, 2003</marker>
<rawString>Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline strategies for online question answering: Answering questions before they are asked. In Proceedings of ACL-03. pp. 1-7. Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of partwhole relations.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL-03.</booktitle>
<pages>80--87</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5755" citStr="Girju et al. 2003" startWordPosition="866" endWordPosition="869">paper is organized as follows. In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships. Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous words. For example, in</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning semantic constraints for the automatic discovery of partwhole relations. In Proceedings of HLT/NAACL-03. pp. 8087. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Han</author>
<author>M Kamber</author>
</authors>
<title>Data Mining - Concepts and Techniques.</title>
<date>2001</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="11272" citStr="Han and Kamber 2001" startWordPosition="1784" endWordPosition="1788">tual information between word e and feature f, which is defined as: cef mief = log n N m (1) N N Following (Pantel and Lin 2002), we construct a committee for each semantic class. A committee is a set of representative elements that unambiguously describe the members of a possible class. For each class c, we construct a matrix containing the similarity between each pair of words ei and ej in c using the cosine coefficient of their mutual information vectors (Salton and McGill 1983): f ∑ ∑ mief2 × i f f For each word e, we then cluster its most similar instances using group-average clustering (Han and Kamber 2001) and we store as a candidate committee the highest scoring cluster c&apos; according to the following metric: |c&apos; |x avgsim(c&apos;) (4) where |c&apos; |is the number of elements in c&apos; and avgsim(c&apos;) is the average pairwise similarity between words in c&apos;. The assumption is that the best representative for a concept is a large set of very similar instances. The committee for class c is then the highest scoring candidate committee containing only words from c. For example, below are the committee members discovered for the semantic classes (A), (B), and (C) from Section 1: 1) cardiovascular disease, diabetes, </context>
</contexts>
<marker>Han, Kamber, 2001</marker>
<rawString>Han, J. and Kamber, M. 2001. Data Mining - Concepts and Techniques. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In COLING-92.</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="5614" citStr="Hearst 1992" startWordPosition="844" endWordPosition="845">t using this simple method, 68% are correct. Of the 65,000 proper name hyponyms we discover, 81.5% are correct. The remainder of this paper is organized as follows. In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships. Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple,</context>
<context position="7607" citStr="Hearst (1992)" startWordPosition="1156" endWordPosition="1157">ning an element to a cluster, CBC removes their overlapping features from the element before assigning it to another cluster. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. CBC discovered both the color sense of orange, as shown in list (C) of Section 1, and the fruit sense shown below: (E) peach, pear, apricot, strawberry, banana, mango, melon, apple, pineapple, cherry, plum, lemon, grapefruit, orange, berry, raspberry, blueberry, kiwi, ... There have also been several approaches to discovering hyponym (is-a) relationships from text. Hearst (1992) used seven lexico-syntactic patterns, for example &amp;quot;such NP as {NP,}*{(or|and)} NP&amp;quot; and &amp;quot;NP {, NP}*{,} or other NP&amp;quot;. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations. They reported an accuracy of about 55% precision on a corpus of 100,000 words. Girju, Badulescu and Moldovan (2003) improved upon this work by using a machine learning filter. Mann (2002) and Fleischman et al. (2003) used part of speech patterns to extract a subset of hyponym relations involing proper nouns. 3 Labeling Classes The research discussed </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING-92. pp. 539545. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90.</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="5894" citStr="Hindle 1990" startWordPosition="888" endWordPosition="889">on 3 describes our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous words. For example, in (D), the color and fruit senses of orange are mixed up. Lin and Pantel (2001) proposed a clustering algorithm, UNICON, which generates sim</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, D. 1990. Noun classification from predicate-argument structures. In Proceedings of ACL-90. pp. 268275. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>G A Miller</author>
</authors>
<title>Using corpus statistics and WordNet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>Leacock, C.; Chodorow, M.; and Miller; G. A. 1998. Using corpus statistics and WordNet relations for sense identification. Computational Linguistics, 24(1):147-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lenat</author>
</authors>
<title>CYC: A large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>38--11</pages>
<contexts>
<context position="794" citStr="Lenat 1995" startWordPosition="113" endWordPosition="115">0292 {pantel,ravichan}@isi.edu Abstract Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc. The current state of the art discovers many semantic classes but fails to label their concepts. We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach. 1 Introduction The natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977). However, WordNet (Miller 1990) and Cyc (Lenat 1995) aside, the community has had little success in actually building large semantic repositories. Such broad-coverage lexical resources are extremely useful in applications such as word sense disambiguation (Leacock, Chodorow and Miller 1998) and question answering (Pasca and Harabagiu 2001). Current manually constructed ontologies such as WordNet and Cyc have important limitations. First, they often contain rare senses. For example, WordNet includes a rare sense of computer that means `the person who computes&apos;. Using WordNet to expand queries to an information retrieval system, the expansion of </context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Lenat, D. 1995. CYC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principar - an efficient, broad-coverage, principle-based parser.</title>
<date>1994</date>
<booktitle>Proceedings of COLING-94.</booktitle>
<pages>42--48</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="15839" citStr="Lin 1994" startWordPosition="2588" endWordPosition="2589">ring term is the name of the class. For example, the top-5 scoring terms that occurred in these relationships with the signature of the concept represented by the committee {Curtis Joseph, John Vanbiesbrouck, Mike Richter, Tommy Salo} are: goalie 40.37 goaltender 33.64 goalkeeper 19.22 player 14.55 backup 9.40 The numbers are the total mutual information scores of each name in the four syntactic relationships. 4 Evaluation In this section, we present an evaluation of the class labeling algorithm and of the hyponym relationships discovered by our system. 4.1 Experimental Setup We used Minipar (Lin 1994), a broad coverage parser, to parse 3GB of newspaper text from the Aquaint (TREC-9) collection. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1. We used the 1432 noun clusters extracted by CBC1 as the list of concepts to name. For each concept, we then used our algorithm described in Section 3 to extract the top-20 names for each concept. 1 Available at http://www.isi.edu/~pantel/demos.htm Table 1. Labels assigned to 10 randomly selected concepts (each repr</context>
</contexts>
<marker>Lin, 1994</marker>
<rawString>Lin, D. 1994. Principar - an efficient, broad-coverage, principle-based parser. Proceedings of COLING-94. pp. 4248. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-98.</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="5905" citStr="Lin 1998" startWordPosition="890" endWordPosition="891">s our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous words. For example, in (D), the color and fruit senses of orange are mixed up. Lin and Pantel (2001) proposed a clustering algorithm, UNICON, which generates similar lists </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL-98. pp. 768774. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Induction of semantic classes from natural language text.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGKDD-01.</booktitle>
<pages>317--322</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="6433" citStr="Lin and Pantel (2001)" startWordPosition="974" endWordPosition="977"> the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous words. For example, in (D), the color and fruit senses of orange are mixed up. Lin and Pantel (2001) proposed a clustering algorithm, UNICON, which generates similar lists but discriminates between senses of words. Later, Pantel and Lin (2002) improved the precision and recall of UNICON clusters with CBC (Clustering by Committee). Using sets of representative elements called committees, CBC discovers cluster centroids that unambiguously describe the members of a possible class. The algorithm initially discovers committees that are well scattered in the similarity space. It then proceeds by assigning elements to their most similar committees. After assigning an element to a cluster, CBC remov</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, D. and Pantel, P. 2001. Induction of semantic classes from natural language text. In Proceedings of SIGKDD-01. pp. 317322. San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
</authors>
<title>Fine-Grained Proper Noun Ontologies for Question Answering. SemaNet&apos; 02: Building and Using Semantic Networks,</title>
<date>2002</date>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="8042" citStr="Mann (2002)" startWordPosition="1221" endWordPosition="1222">, lemon, grapefruit, orange, berry, raspberry, blueberry, kiwi, ... There have also been several approaches to discovering hyponym (is-a) relationships from text. Hearst (1992) used seven lexico-syntactic patterns, for example &amp;quot;such NP as {NP,}*{(or|and)} NP&amp;quot; and &amp;quot;NP {, NP}*{,} or other NP&amp;quot;. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations. They reported an accuracy of about 55% precision on a corpus of 100,000 words. Girju, Badulescu and Moldovan (2003) improved upon this work by using a machine learning filter. Mann (2002) and Fleischman et al. (2003) used part of speech patterns to extract a subset of hyponym relations involing proper nouns. 3 Labeling Classes The research discussed above on discovering hyponym relationships all take a bottom up approach. That is, they use patterns to independently discover semantic relationships of words. However, for infrequent words, these patterns do not match or, worse yet, generate incorrect relationships. Ours is a top down approach. We make use of cooccurrence statistics of semantic classes discovered by algorithms like CBC to label their concepts. Hyponym relationship</context>
</contexts>
<marker>Mann, 2002</marker>
<rawString>Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies for Question Answering. SemaNet&apos; 02: Building and Using Semantic Networks, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: An online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="773" citStr="Miller 1990" startWordPosition="109" endWordPosition="110">y Marina del Rey, CA 90292 {pantel,ravichan}@isi.edu Abstract Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc. The current state of the art discovers many semantic classes but fails to label their concepts. We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach. 1 Introduction The natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977). However, WordNet (Miller 1990) and Cyc (Lenat 1995) aside, the community has had little success in actually building large semantic repositories. Such broad-coverage lexical resources are extremely useful in applications such as word sense disambiguation (Leacock, Chodorow and Miller 1998) and question answering (Pasca and Harabagiu 2001). Current manually constructed ontologies such as WordNet and Cyc have important limitations. First, they often contain rare senses. For example, WordNet includes a rare sense of computer that means `the person who computes&apos;. Using WordNet to expand queries to an information retrieval syst</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>Miller, G. 1990. WordNet: An online lexical database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>S Harabagiu</author>
</authors>
<title>The informative role of WordNet in Open-Domain Question Answering.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources.</booktitle>
<pages>138--143</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1083" citStr="Pasca and Harabagiu 2001" startWordPosition="153" endWordPosition="156"> label their concepts. We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach. 1 Introduction The natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977). However, WordNet (Miller 1990) and Cyc (Lenat 1995) aside, the community has had little success in actually building large semantic repositories. Such broad-coverage lexical resources are extremely useful in applications such as word sense disambiguation (Leacock, Chodorow and Miller 1998) and question answering (Pasca and Harabagiu 2001). Current manually constructed ontologies such as WordNet and Cyc have important limitations. First, they often contain rare senses. For example, WordNet includes a rare sense of computer that means `the person who computes&apos;. Using WordNet to expand queries to an information retrieval system, the expansion of computer will include words like estimator and reckoner. Also, the words dog, computer and company all have a sense that is a hyponym of person. Such rare senses make it difficult for a coreference resolution system to use WordNet to enforce the constraint that personal pronouns (e.g. he </context>
</contexts>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>Pasca, M. and Harabagiu, S. 2001. The informative role of WordNet in Open-Domain Question Answering. In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources. pp. 138143. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGKDD-02.</booktitle>
<pages>613--619</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2548" citStr="Pantel and Lin (2002)" startWordPosition="392" endWordPosition="395">so contains a very poor coverage of proper nouns. There is a need for (semi-) automatic approaches to building and extending ontologies as well as for validating the structure and content of existing ones. With the advent of the Web, we have access to enormous amounts of text. The future of ontology growing lies in leveraging this data by harvesting it for concepts and semantic relationships. Moreover, once such knowledge is discovered, mechanisms must be in place to enrich current ontologies with this new knowledge. To address some of the coverage and specificity problems in WordNet and Cyc, Pantel and Lin (2002) proposed and algorithm, called CBC, for automatically extracting semantic classes. Their classes consist of clustered instances like the three shown below: (A) multiple sclerosis, diabetes, osteoporosis, cardiovascular disease, Parkinson&apos;s, rheumatoid arthritis, heart disease, asthma, cancer, hypertension, lupus, high blood pressure, arthritis, emphysema, epilepsy, cystic fibrosis, leukemia, hemophilia, Alzheimer, myeloma, glaucoma, schizophrenia, ... (S) Mike Richter, Tommy Salo, John Vanbiesbrouck, Curtis Joseph, Chris Osgood, Steve Shields, Tom Barrasso, Guy Hebert, Arturs Irbe, Byron Dafo</context>
<context position="5711" citStr="Pantel and Lin 2002" startWordPosition="858" endWordPosition="861">er, 81.5% are correct. The remainder of this paper is organized as follows. In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships. Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the</context>
<context position="9338" citStr="Pantel and Lin 2002" startWordPosition="1432" endWordPosition="1435"> For example, if we labeled concept (A) from Section 1 with disease, then we could extract is-a relationships such as: diabetes is a disease, cancer is a disease, and lupus is a disease. A concept instance such as lupus is assigned a hypernym disease not because it necessarily occurs in any particular syntactic relationship with disease, but because it belongs to the class of instances that does. The input to our labeling algorithm is a list of semantic classes, in the form of clusters of words, which may be generated from any source. In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002). The output of the system is a ranked list of concept names for each semantic class. In the first phase of the algorithm, we extract feature vectors for each word that occurs in a semantic class. Phase II then uses these features to compute grammatical signatures of concepts using the CBC algorithm. Finally, we use simple syntactic patterns to discover class names from each class&apos; signature. Below, we describe these phases in detail. 3.1 Phase I We represent each word (concept instance) by a feature vector. Each feature corresponds to a context in which the word occurs. For example, &amp;quot;catch _&amp;quot;</context>
<context position="10780" citStr="Pantel and Lin 2002" startWordPosition="1697" endWordPosition="1700"> cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in a grammatical context f. For example, if the word wave occurred 217 times as the object of the verb catch, then the feature vector for wave will have value 217 for its &amp;quot;object-of catch&amp;quot; feature. In Section 4.1, we describe how we obtain these features. We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miem) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: cef mief = log n N m (1) N N Following (Pantel and Lin 2002), we construct a committee for each semantic class. A committee is a set of representative elements that unambiguously describe the members of a possible class. For each class c, we construct a matrix containing the similarity between each pair of words ei and ej in c using the cosine coefficient of their mutual information vectors (Salton and McGill 1983): f ∑ ∑ mief2 × i f f For each word e, we then cluster its most similar instances using group-average clustering (Han and Kamber 2001) and we store as a candidate committee the highest scoring cluster c&apos; according to the following metric: |c&apos;</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In Proceedings of SIGKDD-02. pp. 613619. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A corpus-based approach for building semantic lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP1997.</booktitle>
<contexts>
<context position="5640" citStr="Riloff and Shepherd 1997" startWordPosition="846" endWordPosition="849">simple method, 68% are correct. Of the 65,000 proper name hyponyms we discover, 81.5% are correct. The remainder of this paper is organized as follows. In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships. Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A co</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Riloff, E. and Shepherd, J. 1997. A corpus-based approach for building semantic lexicons. In Proceedings of EMNLP1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI-99.</booktitle>
<pages>474--479</pages>
<location>Orlando, FL.</location>
<contexts>
<context position="5663" citStr="Riloff and Jones 1999" startWordPosition="850" endWordPosition="853">rect. Of the 65,000 proper name hyponyms we discover, 81.5% are correct. The remainder of this paper is organized as follows. In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships. Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin&apos;s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such li</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Riloff, E. and Jones, R. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of AAAI-99. pp. 474479. Orlando, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw Hill</publisher>
<contexts>
<context position="11138" citStr="Salton and McGill 1983" startWordPosition="1757" endWordPosition="1760">e features. We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miem) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: cef mief = log n N m (1) N N Following (Pantel and Lin 2002), we construct a committee for each semantic class. A committee is a set of representative elements that unambiguously describe the members of a possible class. For each class c, we construct a matrix containing the similarity between each pair of words ei and ej in c using the cosine coefficient of their mutual information vectors (Salton and McGill 1983): f ∑ ∑ mief2 × i f f For each word e, we then cluster its most similar instances using group-average clustering (Han and Kamber 2001) and we store as a candidate committee the highest scoring cluster c&apos; according to the following metric: |c&apos; |x avgsim(c&apos;) (4) where |c&apos; |is the number of elements in c&apos; and avgsim(c&apos;) is the average pairwise similarity between words in c&apos;. The assumption is that the best representative for a concept is a large set of very similar instances. The committee for class c is then the highest scoring candidate committee containing only words from c. For example, below</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. and McGill, M. J. 1983. Introduction to Modern Information Retrieval. McGraw Hill</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
<author>R Abelson</author>
</authors>
<title>Scripts, Plans, Goals and Understanding: An Inquiry into Human Knowledge Structures. Lawrence Erlbaum Associates.</title>
<date>1977</date>
<contexts>
<context position="741" citStr="Schank and Abelson 1977" startWordPosition="103" endWordPosition="106">ity of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 {pantel,ravichan}@isi.edu Abstract Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc. The current state of the art discovers many semantic classes but fails to label their concepts. We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach. 1 Introduction The natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977). However, WordNet (Miller 1990) and Cyc (Lenat 1995) aside, the community has had little success in actually building large semantic repositories. Such broad-coverage lexical resources are extremely useful in applications such as word sense disambiguation (Leacock, Chodorow and Miller 1998) and question answering (Pasca and Harabagiu 2001). Current manually constructed ontologies such as WordNet and Cyc have important limitations. First, they often contain rare senses. For example, WordNet includes a rare sense of computer that means `the person who computes&apos;. Using WordNet to expand queries </context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Schank, R. and Abelson, R. 1977. Scripts, Plans, Goals and Understanding: An Inquiry into Human Knowledge Structures. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siegel</author>
<author>Castellan Jr</author>
<author>N J</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill.</publisher>
<marker>Siegel, Jr, J, 1988</marker>
<rawString>Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the question answering track. To appear in</title>
<date>2003</date>
<booktitle>Proceedings of TREC-12 Conference.</booktitle>
<location>NIST, Gaithersburg, MD.</location>
<contexts>
<context position="24085" citStr="Voorhees, 2003" startWordPosition="3968" endWordPosition="3969">orrect answers in the Top-1 and Top-5 returned answers on 50 definition questions. SYSTEM Top-1 Top-5 Strict Lenient Strict Lenient WordNet 38% 38% 38% 38% Fleischman 36% 40% 42% 44% Our System 36% 44% 60% 62% Table 7. Percentage of questions where the passage retrieval module returns a correct answer in the Top-1 and Top-100 ranked passages (with and without semantic indexing). CORRECT TOP-1 Correct Top-100 With semantic 43 / 179 134 / 179 indexing Without semantic 36 / 179 131 / 179 indexing Definition Questions We chose the 50 definition questions that appeared in the QA track of TREC2003 (Voorhees, 2003). For example: &amp;quot;Who is Aaron Copland?&amp;quot; and &amp;quot;What is the Kama Sutra?&amp;quot; For each question we looked for at most five corresponding concepts in our hyponym list. For example, for Aaron Copland, we found the following hypernyms: composer, music, and gift. We compared our system with the concepts in WordNet and Fleischman et al.&apos;s instance/concept relations (Fleischman et al. 2003). Table 6 shows the percentage of correct answers in the top-1 and top-5 returned answers from each system. All systems seem to have similar performance on the top-1 answers, but our system has many more answers in the top</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Voorhees, E. 2003. Overview of the question answering track. To appear in Proceedings of TREC-12 Conference. NIST, Gaithersburg, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>