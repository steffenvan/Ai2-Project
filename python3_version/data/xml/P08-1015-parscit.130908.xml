<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003040">
<title confidence="0.999578">
Grounded Language Modeling for
Automatic Speech Recognition of Sports Video
</title>
<author confidence="0.856663">
Michael Fleischman Deb Roy
</author>
<affiliation confidence="0.83598">
Massachusetts Institute of Technology Massachusetts Institute of Technology
Media Laboratory Media Laboratory
</affiliation>
<email confidence="0.998832">
mbf@mit.edu dkroy@media.mit.edu
</email>
<sectionHeader confidence="0.99351" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.874976454545455">
Grounded language models represent the rela-
tionship between words and the non-linguistic
context in which they are said. This paper de-
scribes how they are learned from large cor-
pora of unlabeled video, and are applied to the
task of automatic speech recognition of sports
video. Results show that grounded language
models improve perplexity and word error
rate over text based language models, and fur-
ther, support video information retrieval better
than human generated speech transcriptions.
</bodyText>
<sectionHeader confidence="0.997823" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988575862069">
Recognizing speech in broadcast video is a neces-
sary precursor to many multimodal applications
such as video search and summarization (Snoek
and Worring, 2005;). Although performance is
often reasonable in controlled environments (such
as studio news rooms), automatic speech recogni-
tion (ASR) systems have significant difficulty in
noisier settings (such as those found in live sports
broadcasts) (Wactlar et al., 1996). While many
researches have examined how to compensate for
such noise using acoustic techniques, few have
attempted to leverage information in the visual
stream to improve speech recognition performance
(for an exception see Murkherjee and Roy, 2003).
In many types of video, however, visual context
can provide valuable clues as to what has been
said. For example, in video of Major League
Baseball games, the likelihood of the phrase “home
run” increases dramatically when a home run has
actually been hit. This paper describes a method
for incorporating such visual information in an
ASR system for sports video. The method is based
on the use of grounded language models to repre-
sent the relationship between words and the non-
linguistic context to which they refer (Fleischman
and Roy, 2007).
Grounded language models are based on re-
search from cognitive science on grounded models
of meaning. (for a review see Roy, 2005, and Roy
and Reiter, 2005). In such models, the meaning of
a word is defined by its relationship to representa-
tions of the language users’ environment. Thus,
for a robot operating in a laboratory setting, words
for colors and shapes may be grounded in the out-
puts of its computer vision system (Roy &amp; Pent-
land, 2002); while for a simulated agent operating
in a virtual world, words for actions and events
may be mapped to representations of the agent’s
plans or goals (Fleischman &amp; Roy, 2005).
This paper extends previous work on grounded
models of meaning by learning a grounded lan-
guage model from naturalistic data collected from
broadcast video of Major League Baseball games.
A large corpus of unlabeled sports videos is col-
lected and paired with closed captioning transcrip-
tions of the announcers’ speech. 1 This corpus is
used to train the grounded language model, which
like traditional language models encode the prior
probability of words for an ASR system. Unlike
traditional language models, however, grounded
language models represent the probability of a
word conditioned not only on the previous word(s),
but also on features of the non-linguistic context in
which the word was uttered.
Our approach to learning grounded language
models operates in two phases. In the first phase,
events that occur in the video are represented using
hierarchical temporal pattern automatically mined
</bodyText>
<note confidence="0.9102265">
1 Closed captioning refers to human transcriptions of speech
embedded in the video stream primarily for the hearing im-
paired. Closed captioning is reasonably accurate (although not
perfect) and available on some, but not all, video broadcasts.
</note>
<page confidence="0.956706">
121
</page>
<note confidence="0.689108">
Proceedings ofACL-08: HLT, pages 121–129,
</note>
<page confidence="0.372183">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<figureCaption confidence="0.98777325">
Figure 1. Representing events in video. a) Events are represented by first abstracting the raw video into visual con-
text, camera motion, and audio context features. b) Temporal data mining is then used to discover hierarchical tem-
poral patterns in the parallel streams of features. c) Temporal patterns found significant in each iteration are stored
in a codebook that is used to represent high level events in video.
</figureCaption>
<bodyText confidence="0.999731083333333">
from low level features. In the second phase, a
conditional probability distribution is estimated
that describes the probability that a word was ut-
tered given such event representations. In the fol-
lowing sections we describe these two aspects of
our approach and evaluate the performance of our
grounded language model on a speech recognition
task using video highlights from Major League
Baseball games. Results indicate improved per-
formance using three metrics: perplexity, word
error rate, and precision on an information retrieval
task.
</bodyText>
<sectionHeader confidence="0.846781" genericHeader="method">
2 Representing Events in Sports Video
</sectionHeader>
<bodyText confidence="0.999929153846154">
Recent work in video surveillance has demon-
strated the benefit of representing complex events
as temporal relations between lower level sub-
events (Hongen et al., 2004). Thus, to represent
events in the sports domain, we would ideally first
represent the basic sub events that occur in sports
video (e.g., hitting, throwing, catching, running,
etc.) and then build up complex events (such as
home run) as a set of temporal relations between
these basic events. Unfortunately, due to the limi-
tations of computer vision techniques, reliably
identifying such basic events in video is not feasi-
ble. However, sports video does have characteris-
tics that can be exploited to effectively represent
complex events.
Like much broadcast video, sports video is
highly produced, exploiting many different camera
angles and a human director who selects which
camera is most appropriate given what is happen-
ing on the field. The styles that different directors
employ are extremely consistent within a sport and
make up a “language of film” which the machine
can take advantage of in order to represent the
events taking place in the video.
Thus, even though it is not easy to automati-
cally identify a player hitting a ball in video, it is
easy to detect features that correlate with hitting,
e.g., when a scene focusing on the pitching mound
immediately jumps to one zooming in on the field
(see Figure 1). Although these correlations are not
perfect, experiments have shown that baseball
events can be classified using such features
(Fleischman et al., 2007).
We exploit the language of film to represent
events in sports video in two phases. First, low
level features that correlate with basic events in
sports are extracted from the video stream. Then,
temporal data mining is used to find patterns
within this low level event stream.
</bodyText>
<subsectionHeader confidence="0.989544">
2.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999533666666667">
We extract three types of features: visual con-
text features, camera motion features, and audio
context features.
</bodyText>
<page confidence="0.995608">
122
</page>
<subsectionHeader confidence="0.940411">
Visual Context Features
</subsectionHeader>
<bodyText confidence="0.999841896551724">
Visual context features encode general proper-
ties of the visual scene in a video segment. Super-
vised classifiers are trained to identify these
features, which are relatively simple to classify in
comparison to high level events (like home runs)
that require more training data and achieve lower
accuracy. The first step in classifying visual con-
text features is to segment the video into shots (or
scenes) based on changes in the visual scene due to
editing (e.g. jumping from a close up to a wide
shot of the field). Shot detection and segmentation
is a well studied problem; in this work we use the
method of Tardini et al. (2005).
After the video is segmented into shots, indi-
vidual frames (called key frames) are selected and
represented as a vector of low level features that
describe the key frame’s color distribution, en-
tropy, etc. (see Fleischman and Roy, 2007 for the
full list of low level features used). The WEKA
machine learning package is used to train a boosted
decision tree to classify these frames into one of
three categories: pitching-scene, field-scene, other
(Witten and Frank, 2005). Those shots whose key
frames are classified as field-scenes are then sub-
categorized (using boosted decision trees) into one
of the following categories: infield, outfield, wall,
base, running, and misc. Performance of these
classification tasks is approximately 96% and 90%
accuracy respectively.
</bodyText>
<subsectionHeader confidence="0.947529">
Camera Motion Features
</subsectionHeader>
<bodyText confidence="0.99996726923077">
In addition to visual context features, we also
examine the camera motion that occurs within a
video. Unlike visual context features, which pro-
vide information about the global situation that is
being observed, camera motion features represent
more precise information about the actions occur-
ring in a video. The intuition here is that the cam-
era is a stand in for a viewer’s focus of attention.
As actions occur in a video, the camera moves to
follow it; this camera motion thus mirrors the ac-
tions themselves, providing informative features
for event representation.
Like shot boundary detection, detecting the mo-
tion of the camera in a video (i.e., the amount it
pans left to right, tilts up and down, and zooms in
and out) is a well-studied problem. We use the
system of Bouthemy et al. (1999) which computes
the camera motion using the parameters of a two-
dimensional affine model to fit every pair of se-
quential frames in a video. A 15 state 1st order
Hidden Markov Model, implemented with the
Graphical Modeling Toolkit,2 then converts the
output of the Bouthemy system into a stream of
clustered characteristic camera motions (e.g. state
12 clusters together motions of zooming in fast
while panning slightly left).
</bodyText>
<subsectionHeader confidence="0.813284">
Audio Context
</subsectionHeader>
<bodyText confidence="0.999972444444444">
The audio stream of a video can also provide use-
ful information for representing non-linguistic con-
text. We use boosted decision trees to classify
audio into segments of speech, excited_speech,
cheering, and music. Classification operates on a
sequence of overlapping 30 ms frames extracted
from the audio stream. For each frame, a feature
vector is computed using, MFCCs (often used in
speaker identification and speech detection tasks),
as well as energy, the number of zero crossings,
spectral entropy, and relative power between dif-
ferent frequency bands. The classifier is applied to
each frame, producing a sequence of class labels.
These labels are then smoothed using a dynamic
programming cost minimization algorithm (similar
to those used in Hidden Markov Models). Per-
formance of this system achieves between 78%
and 94% accuracy.
</bodyText>
<subsectionHeader confidence="0.998069">
2.2 Temporal Pattern Mining
</subsectionHeader>
<bodyText confidence="0.999978058823529">
Given a set of low level features that correlate with
the basic events in sports, we can now focus on
building up representations of complex events.
Unlike previous work (Hongen et al., 2005) in
which representations of the temporal relations
between low level events are built up by hand, we
employ temporal data mining techniques to auto-
matically discover such relations from a large cor-
pus of unannotated video.
As described above, ideal basic events (such as
hitting and catching) cannot be identified easily in
sports video. By finding temporal patterns between
audio, visual and camera motion features, how-
ever, we can produce representations that are
highly correlated with sports events. Importantly,
such temporal patterns are not strictly sequential,
but rather, are composed of features that can occur
</bodyText>
<footnote confidence="0.874932">
2 http://ssli.ee.washington.edu/~bilmes/gmtk/
</footnote>
<page confidence="0.997731">
123
</page>
<bodyText confidence="0.999891016949153">
in complex and varied temporal relations to each
other.
To find such patterns automatically, we follow
previous work in video content classification in
which temporal data mining techniques are used to
discover event patterns within streams of lower
level features. The algorithm we use is fully unsu-
pervised and proceeds by examining the relations
that occur between features in multiple streams
within a moving time window. Any two features
that occur within this window must be in one of
seven temporal relations with each other (e.g. be-
fore, during, etc.) (Allen, 1984). The algorithm
keeps track of how often each of these relations is
observed, and after the entire video corpus is ana-
lyzed, uses chi-square analyses to determine which
relations are significant. The algorithm iterates
through the data, and relations between individual
features that are found significant in one iteration
(e.g. [OVERLAP, field-scene, cheer]), are them-
selves treated as individual features in the next.
This allows the system to build up higher-order
nested relations in each iteration (e.g. [BEFORE,
[OVERLAP, field-scene, cheer], field scene]]).
The temporal patterns found significant in this
way make up a codebook which can then be used
as a basis for representing a video. The term code-
book is often used in image analysis to describe a
set of features (stored in the codebook) that are
used to encode raw data (images or video). Such
codebooks are used to represent raw video using
features that are more easily processed by the
computer.
Our framework follows a similar approach in
which raw video is encoded (using a codebook of
temporal patterns) as follows. First, the raw video
is abstracted into the visual context, camera mo-
tion, and audio context feature streams (as de-
scribed in Section 2.1). These feature streams are
then scanned, looking for any temporal patterns
(and nested sub-patterns) that match those found in
the codebook. For each pattern, the duration for
which it occurs in the feature streams is treated as
the value of an element in the vector representation
for that video.
Thus, a video is represented as an n length vec-
tor, where n is the total number of temporal pat-
terns in the codebook. The value of each element
of this vector is the duration for which the pattern
associated with that element was observed in the
video. So, if a pattern was not observed in a video
at all, it would have a value of 0, while if it was
observed for the entire length of the video, it would
have a value equal to the number of frames present
in that video.
Given this method for representing the non-
linguistic context of a video, we can now examine
how to model the relationship between such con-
text and the words used to describe it.
</bodyText>
<sectionHeader confidence="0.980331" genericHeader="method">
3 Linguistic Mapping
</sectionHeader>
<bodyText confidence="0.999368564102564">
Modeling the relationship between words and non-
linguistic context assumes that the speech uttered
in a video refers consistently (although not exclu-
sively) to the events being represented by the tem-
poral pattern features. We model this relationship,
much like traditional language models, using con-
ditional probability distributions. Unlike tradi-
tional language models, however, our grounded
language models condition the probability of a
word not only on the word(s) uttered before it, but
also on the temporal pattern features that describe
the non-linguistic context in which it was uttered.
We estimate these conditional distributions using a
framework similar that used for training acoustic
models in ASR and translation models in Machine
Translation (MT).
We generate a training corpus of utterances
paired with representations of the non-linguistic
context in which they were uttered. The first step
in generating this corpus is to generate the low
level features described in Section 2.1 for each
video in our training set. We then segment each
video into a set of independent events based on the
visual context features we have extracted. We fol-
low previous work in sports video processing
(Gong et al., 2004) and define an event in a base-
ball video as any sequence of shots starting with a
pitching-scene and continuing for four subsequent
shots. This definition follows from the fact that the
vast majority of events in baseball start with a
pitch and do not last longer than four shots. For
each of these events in our corpus, a temporal pat-
tern feature vector is generated as described in sec-
tion 2.2. These events are then paired with all the
words from the closed captioning transcription that
occur during each event (plus or minus 10 sec-
onds). Because these transcriptions are not neces-
sarily time synched with the audio, we use the
method described in Hauptmann and Witbrock
</bodyText>
<page confidence="0.993201">
124
</page>
<bodyText confidence="0.998522225806451">
(1998) to align the closed captioning to the an-
nouncers’ speech.
Previous work has examined applying models
often used in MT to the paired corpus described
above (Fleischman and Roy, 2006). Recent work
in automatic image annotation (Barnard et al.,
2003; Blei and Jordan, 2003) and natural language
processing (Steyvers et al., 2004), however, have
demonstrated the advantages of using hierarchical
Bayesian models for related tasks. In this work we
follow closely the Author-Topic (AT) model (Stey-
vers et al., 2004) which is a generalization of La-
tent Dirichlet Allocation (LDA) (Blei et al., 2005).3
LDA is a technique that was developed to
model the distribution of topics discussed in a large
corpus of documents. The model assumes that
every document is made up of a mixture of topics,
and that each word in a document is generated
from a probability distribution associated with one
of those topics. The AT model generalizes LDA,
saying that the mixture of topics is not dependent
on the document itself, but rather on the authors
who wrote it. According to this model, for each
word (or phrase) in a document, an author is cho-
sen uniformly from the set of the authors of the
document. Then, a topic is chosen from a distribu-
tion of topics associated with that particular author.
Finally, the word is generated from the distribution
associated with that chosen topic. We can express
the probability of the words in a document (W)
given its authors (A) as:
</bodyText>
<equation confidence="0.899414666666667">
p(W  |A) _ r1 1 EE p m z p z x
(  |) (  |) (1)
meW Ad xeAzeT
</equation>
<bodyText confidence="0.91018612">
where T is the set of latent topics that are induced
given a large set of training data.
We use the AT model to estimate our grounded
language model by making an analogy between
documents and events in video. In our framework,
the words in a document correspond to the words
in the closed captioning transcript associated with
an event. The authors of a document correspond to
the temporal patterns representing the non-
linguistic context of that event. We modify the AT
model slightly, such that, instead of selecting from
3 In the discussion that follows, we describe a method for es-
timating unigram grounded language models. Estimating
bigram and trigram models can be done by processing on
word pairs or triples, and performing normalization on the
resulting conditional distributions.
a uniform distribution (as is done with authors of
documents), we select patterns from a multinomial
distribution based upon the duration of the pattern.
The intuition here is that patterns that occur for a
longer duration are more salient and thus, should
be given greater weight in the generative process.
We can now rewrite (1) to give the probability of
words during an event (W) given the vector of ob-
served temporal patterns (P) as:
</bodyText>
<equation confidence="0.9831072">
p W P
(  |) _ r1EE p m z p z x p x
(  |) (  |) ( ) (2)
m W x P z T
e e e
</equation>
<bodyText confidence="0.999981583333334">
In the experiments described below we follow
Steyver et al., (2004) and train our AT model using
Gibbs sampling, a Markov Chain Monte Carlo
technique for obtaining parameter estimates. We
run the sampler on a single chain for 200 iterations.
We set the number of topics to 15, and normalize
the pattern durations first by individual pattern
across all events, and then for all patterns within an
event. The resulting parameter estimates are
smoothed using a simple add N smoothing tech-
nique, where N=1 for the word by topic counts and
N=.01 for the pattern by topic counts.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999987916666667">
In order to evaluate our grounded language model-
ing approach, a parallel data set of 99 Major
League Baseball games with corresponding closed
captioning transcripts was recorded from live tele-
vision. These games represent data totaling ap-
proximately 275 hours and 20,000 distinct events
from 25 teams in 23 stadiums, broadcast on five
different television stations. From this set, six
games were held out for testing (15 hours, 1200
events, nine teams, four stations). From this test
set, baseball highlights (i.e., events which termi-
nate with the player either out or safe) were hand
annotated for use in evaluation, and manually tran-
scribed in order to get clean text transcriptions for
gold standard comparisons. Of the 1200 events in
the test set, 237 were highlights with a total word
count of 12,626 (vocabulary of 1800 words).
The remaining 93 unlabeled games are used to
train unigram, bigram, and trigram grounded lan-
guage models. Only unigrams, bigrams, and tri-
grams that are not proper names, appear greater
than three times, and are not composed only of
stop words were used. These grounded language
models are then combined in a backoff strategy
</bodyText>
<page confidence="0.997318">
125
</page>
<bodyText confidence="0.999972285714286">
with traditional unigram, bigram, and trigram lan-
guage models generated from a combination of the
closed captioning transcripts of all training games
and data from the switchboard corpus (see below).
This backoff is necessary to account for the words
not included in the grounded language model itself
(i.e. stop words, proper names, low frequency
words). The traditional text-only language models
(which are also used below as baseline compari-
sons) are generated with the SRI language model-
ing toolkit (Stolcke, 2002) using Chen and
Goodman&apos;s modified Kneser-Ney discounting and
interpolation (Chen and Goodman, 1998). The
backoff strategy we employ here is very simple: if
the ngram appears in the GLM then it is used, oth-
erwise the traditional LM is used. In future work
we will examine more complex backoff strategies
(Hsu, in review).
We evaluate our grounded language modeling
approach using 3 metrics: perplexity, word error
rate, and precision on an information retrieval task.
</bodyText>
<subsectionHeader confidence="0.982048">
4.1 Perplexity
</subsectionHeader>
<bodyText confidence="0.999956515151515">
Perplexity is an information theoretic measure of
how well a model predicts a held out test set. We
use perplexity to compare our grounded language
model to two baseline language models: a lan-
guage model generated from the switchboard cor-
pus, a commonly used corpus of spontaneous
speech in the telephony domain (3.65M words; 27k
vocab); and a language model that interpolates
(with equal weight given to both) between the
switchboard model and a language model trained
only on the baseball-domain closed captioning
(1.65M words; 17k vocab). The results of calculat-
ing perplexity on the test set highlights for these
three models is presented in Table 1 (lower is bet-
ter).
Not surprisingly, the switchboard language
model performs far worse than both the interpo-
lated text baseline and the grounded language
model. This is due to the large discrepancy be-
tween both the style and vocabulary of language
about sports compared to the domain of telephony
sampled by the switchboard corpus. Of more in-
terest is the decrease in perplexity seen when using
the grounded language model compared to the in-
terpolated model. Note that these two language
models are generated using the same speech tran-
scriptions, i.e. the closed captioning from the train-
ing games and the switchboard corpus. However,
whereas the baseline model remains the same for
each of the 237 test highlights, the grounded lan-
guage model generates different word distributions
for each highlight depending on the event features
extracted from the highlight video.
</bodyText>
<tableCaption confidence="0.998693142857143">
Table 1. Perplexity measures for three different lan-
guage models on a held out test set of baseball high-
lights (12,626 words). We compare the grounded
language model to two text based language models: one
trained on the switchboard corpus alone; and interpo-
lated with one trained on closed captioning transcrip-
tions of baseball video.
</tableCaption>
<subsectionHeader confidence="0.997215">
4.2 Word Accuracy and Error Rate
</subsectionHeader>
<bodyText confidence="0.999900696969697">
Word error rate (WER) is a normalized measure of
the number of word insertions, substitutions, and
deletions required to transform the output tran-
scription of an ASR system to a human generated
gold standard transcription of the same utterance.
Word accuracy is simply the number of words in
the gold standard that they system correctly recog-
nized. Unlike perplexity which only evaluates the
performance of language models, examining word
accuracy and error rate requires running an entire
ASR system, i.e. both the language and acoustic
models.
We use the Sphinx system to train baseball specific
acoustic models using parallel acoustic/text data
automatically mined from our training set. Follow-
ing Jang and Hauptman (1999), we use an off the
shelf acoustic model (the hub4 model) to generate
an extremely noisy speech transcript of each game
in our training set, and use dynamic programming
to align these noisy outputs to the closed caption-
ing stream for those same games. Given these two
transcriptions, we then generate a paired acous-
tic/text corpus by sampling the audio at the time
codes where the ASR transcription matches the
closed captioning transcription.
For example, if the ASR output contains the
term sequence “... and farther home run for David
forty says...” and the closed captioning contains
the sequence “...another home run for David
Ortiz...,” the matched phrase “home run for
David” is assumed a correct transcription for the
audio at the time codes given by the ASR system.
Only looking at sequences of three words or more,
</bodyText>
<figure confidence="0.990708">
Switchboard
1404
Interpolated
(Switch+CC)
145.27
Grounded
83.88
ppl
</figure>
<page confidence="0.855744">
126
</page>
<figureCaption confidence="0.9727876">
Figure 3. Word accuracy and error rates for ASR sys-
tems using a grounded language model, a text based
language model trained on the switchboard corpus, and
the switchboard model interpolated with a text based
model trained on baseball closed captions.
</figureCaption>
<bodyText confidence="0.999988863636364">
we extract approximately 18 hours of clean paired
data from our 275 hour training corpus. A con-
tinuous acoustic model with 8 gaussians and 6000
ties states is trained on this data using the Sphinx
speech recognizer.4
Figure 3 shows the WERs and accuracy for
three ASR systems run using the Sphinx decoder
with the acoustic model described above and either
the grounded language model or the two baseline
models described in section 4.1. Note that per-
formance for all of these systems is very poor due
to limited acoustic data and the large amount of
background crowd noise present in sports video
(and particularly in sports highlights). Even with
this noise, however, results indicate that the word
accuracy and error rates when using the grounded
language model is significantly better than both the
switchboard model (absolute WER reduction of
13%; absolute accuracy increase of 15.2%) and the
switchboard interpolated with the baseball specific
text based language model (absolute WER reduc-
tion of 3.7%; absolute accuracy increase of 5.9%).
</bodyText>
<footnote confidence="0.544966">
4 http://cmusphinx.sourceforge.net/html/cmusphinx.php
</footnote>
<bodyText confidence="0.9997992">
Drawing conclusions about the usefulness of
grounded language models using word accuracy or
error rate alone is difficult. As it is defined, these
measures penalizes a system that mistakes “a” for
“uh” as much as one that mistakes “run” for “rum.”
When using ASR to support multimedia applica-
tions (such as search), though, such substitutions
are not of equal importance. Further, while visual
information may be useful for distinguishing the
latter error, it is unlikely to assist with the former.
Thus, in the next section we examine an extrinsic
evaluation in which grounded language models are
judged not directly on their effect on word accu-
racy or error rate, but based on their ability to sup-
port video information retrieval.
</bodyText>
<subsectionHeader confidence="0.99987">
4.3 Precision of Information Retrieval
</subsectionHeader>
<bodyText confidence="0.999763470588235">
One of the most commonly used applications of
ASR for video is to support information retrieval
(IR). Such video IR systems often use speech tran-
scriptions to index segments of video in much the
same way that words are used to index text docu-
ments (Wactlar et al., 1996). For example, in the
domain of baseball, if a video IR system were is-
sued the query “home run,” it would typically re-
turn a set of video clips by searching its database
for events in which someone uttered the phrase
“home run.” Because such systems rely on ASR
output to search video, the performance of a video
IR system gives an indirect evaluation of the
ASR’s quality. Further, unlike the case with word
accuracy or error rate, such evaluations highlight a
systems ability to recognize the more relevant con-
tent words without being distracted by the more
common stop words.
Our metric for evaluation is the precision with
which baseball highlights are returned in a video
IR system. We examine three systems: one that
uses ASR with the grounded language model, a
baseline system that uses ASR with the text only
interpolated language model, and finally a system
that uses human produced closed caption transcrip-
tions to index events.
For each system, all 1200 events from the test
set (not just the highlights) are indexed. Queries
are generated artificially using a method similar to
Berger and Lafferty (1999) and used in Fleischman
and Roy (2007). First, each highlight is labeled
with the event’s type (e.g. fly ball), the event’s lo-
cation (e.g. left field) and the event’s result (e.g.
double play): 13 labels total. Log likelihood ratios
</bodyText>
<figure confidence="0.9944175">
Word Error Rate (WER)
Word Accuracy (%)
95
90
85
80
75
70
35
30
25
20
15
10
5
0
switchboard interpolated grounded
switchboard interpolated grounded
89.6
15.1
80.3
25.4
76.6
31.3
</figure>
<page confidence="0.979706">
127
</page>
<bodyText confidence="0.999917448979592">
are then used to find the phrases (unigram, trigram,
and bigram) most indicative of each label (e.g. “fly
ball” for category fly ball). For each label, the
three most indicative phrases are issued as queries
to the system, which ranks its results using the lan-
guage modeling approach of Ponte and Croft
(1998). Precision is measured on how many of the
top five returned events are of the correct category.
Figure 4 shows the precision of the video IR
systems based on ASR with the grounded language
model, ASR with the text-only interpolated lan-
guage model, and closed captioning transcriptions.
As with our previous evaluations, the IR results
show that the system using ASR with the grounded
language model performed better than the one us-
ing ASR with the text-only language model (5.1%
absolute improvement). More notably, though,
Figure 4 shows that the system using the grounded
language model performed better than the system
using the hand generated closed captioning tran-
scriptions (4.6% absolute improvement). Although
this is somewhat counterintuitive given that hand
transcriptions are typically considered gold stan-
dards, these results follow from a limitation of us-
ing text-based methods to index video.
Unlike the case with text documents, the occur-
rence of a query term in a video is often not
enough to assume the video’s relevance to that
query. For example, when searching through
video of baseball games, returning all clips in
which the phrase “home run” occurs, results pri-
marily in video of events where a home run does
not actually occur. This follows from the fact that
in sports, as in life, people often talk not about
what is currently happening, but rather, they talk
about what did, might, or will happen in the future.
By taking into account non-linguistic context
during speech recognition, the grounded language
model system indirectly circumvents some of these
false positive results. This follows from the fact
that an effect of using the grounded language
model is that when an announcer utters a phrase
(e.g., “fly ball”), the system is more likely to rec-
ognize that phrase correctly if the event it refers to
is actually occurring (e.g. if someone actually hit a
fly ball). Because the grounded language model
system is biased to recognize phrases that describe
what is currently happening, it returns fewer false
positives and gets higher precision.
</bodyText>
<figureCaption confidence="0.801156">
Figure 4. Precision of top five results of a video IR sys-
</figureCaption>
<tableCaption confidence="0.934233428571429">
tem based on speech transcriptions. Three different
transcriptions are compared: ASR-LM uses ASR with a
text-only interpolated language model (trained on base-
ball closed captioning and the switchboard corpus);
ASR-GLM uses ASR with a grounded language model;
CC uses human generated closed captioning transcrip-
tions (i.e., no ASR).
</tableCaption>
<sectionHeader confidence="0.999001" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999965185185185">
We have described a method for improving speech
recognition in video. The method uses grounded
language modeling, an extension of tradition lan-
guage modeling in which the probability of a word
is conditioned not only on the previous word(s) but
also on the non-linguistic context in which the
word is uttered. Context is represented using hier-
archical temporal patterns of low level features
which are mined automatically from a large unla-
beled video corpus. Hierarchical Bayesian models
are then used to map these representations to
words. Initial results show grounded language
models improve performance on measures of per-
plexity, word accuracy and error rate, and preci-
sion on an information retrieval task.
In future work, we will examine the ability of
grounded language models to improve perform-
ance for other natural language tasks that exploit
text based language models, such as Machine
Translation. Also, we are examining extending
this approach to other sports domains such as
American football. In theory, however, our ap-
proach is applicable to any domain in which there
is discussion of the here-and-now (e.g., cooking
shows, etc.). In future work, we will examine the
strengths and limitations of grounded language
modeling in these domains.
</bodyText>
<figure confidence="0.986309416666667">
ASR-LM CC ASR-GLM
0.35
0.34
Precision of Top 5
0.33
0.32
0.31
0.3
0.29
0.28
0.27
0.26
</figure>
<page confidence="0.979839">
128
</page>
<sectionHeader confidence="0.994157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883979591837">
Allen, J.F. (1984). A General Model of Action and
Time. Artificial Intelligence. 23(2).
Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei,
D, and Jordan, M. (2003), Matching Words and
Pictures, Journal of Machine Learning Research,
Vol 3.
Berger, A. and Lafferty, J. (1999). Information
Retrieval as Statistical Translation. In Proceed-
ings of SIGIR-99.
Blei, D. and Jordan, M. (2003). Modeling annotated
data. Proceedings of the 26th International Confer-
ence on Research and Development in Information
Retrieval, ACM Press, 127–134.
Blei, D. Ng, A., and Jordan, M (2003). “Latent Dirichlet
allocation.” Journal of Machine Learning Research
3:993–1022.
Bouthemy, P., Gelgon, M., Ganansia, F. (1999). A uni-
fied approach to shot change detection and cam-
era motion characterization. IEEE Trans. on
Circuits and Systems for Video Technology,
9(7).
Chen, S. F. and Goodman, J., (1998). An Empirical
Study of Smoothing Techniques for Language Mod-
eling, Tech. Report TR-10-98, Computer Science
Group, Harvard U., Cambridge, MA.
Fleischman M, Roy, D. (2007). Situated Models of
Meaning for Sports Video Retrieval. HLT/NAACL.
Rochester, NY.
Fleischman, M. and Roy, D. (2007). Unsupervised Con-
tent-Based Indexing of Sports Video Retrieval. 9th
ACM Workshop on Multimedia Information Retrieval
(MIR). Augsburg, Germany.
Fleischman, M. B. and Roy, D. (2005) Why Verbs are
Harder to Learn than Nouns: Initial Insights from a
Computational Model of Intention Recognition in
Situated Word Learning. 27th Annual Meeting of the
Cognitive Science Society, Stresa, Italy.
Fleischman, M., DeCamp, P. Roy, D. (2006). Mining
Temporal Patterns of Movement for Video Content
Classification. ACM Workshop on Multimedia In-
formation Retrieval.
Fleischman, M., Roy, B., and Roy, D. (2007). Tempo-
ral Feature Induction for Sports Highlight Classifica-
tion. In Proceedings of ACM Multimedia.
Augsburg, Germany.
Gong, Y., Han, M., Hua, W., Xu, W. (2004). Maximum
entropy model-based baseball highlight detection and
classification. Computer Vision and Image Un-
derstanding. 96(2).
Hauptmann, A. , Witbrock, M., (1998) Story Segmenta-
tion and Detection of Commercials in Broadcast
News Video, Advances in Digital Libraries.
Hongen, S., Nevatia, R. Bremond, F. (2004).
Video-based event recognition: activity repre-
sentation and probabilistic recognition methods.
Computer Vision and Image Understanding.
96(2).
Hsu , Bo-June (Paul). (in review). Generalized Linear
Interpolation of Language Models.
Jang, P., Hauptmann, A. (1999). Learning to Recognize
Speech by Watching Television. IEEE Intelligent
Systems Magazine, 14(5), pp. 51-58.
Mukherjee, N. and Roy, D.. (2003). A Visual Context-
Aware Multimodal System for Spoken Language
Processing. Proc. Eurospeech, 4 pages.
Ponte, J.M., and Croft, W.B. (1998). A Language Mod-
eling Approach to Information Retrieval. In Proc. of
SIGIR’98.
Roy, D. (2005). . Grounding Words in Perception and
Action: Insights from Computational Models. TICS.
Roy, D. and Pentland, A. (2002). Learning Words from
Sights and Sounds: A Computational Model. Cogni-
tive Science, 26(1).
Roy. D. and Reiter, E. (2005). . Connecting Language to
the World. Artificial Intelligence, 167(1-2), 1-12.
Snoek, C.G.M. and Worring, M.. (2005). Multimodal
video indexing: A review of the state-of-the-art.
Multimedia Tools and Applications, 25(1):5-35.
Steyvers, M., Smyth, P., Rosen-Zvi, M., &amp; Griffiths, T.
(2004). Probabilistic Author-Topic Models for In-
formation Discovery. The Tenth ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining. Seattle, Washington.
Stolcke, A., (2002). SRILM - An Extensible Language
Modeling Toolkit, in Proc. Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005).
Shot Detection and Motion Analysis for Automatic
MPEG-7 Annotation of Sports Videos. In 13th In-
ternational Conference on Image Analysis and Proc-
essing.
Wactlar, H., Witbrock, M., Hauptmann, A., (1996 ).
Informedia: News-on-Demand Experiments in
Speech Recognition. ARPA Speech Recognition
Workshop, Arden House, Harriman, NY.
Witten, I. and Frank, E. (2005). Data Mining: Practical
machine learning tools and techniques. 2nd Edition,
Morgan Kaufmann. San Francisco, CA.
</reference>
<page confidence="0.998573">
129
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986739">
<title confidence="0.9954245">Grounded Language Modeling for Automatic Speech Recognition of Sports Video</title>
<author confidence="0.999843">Michael Fleischman Deb Roy</author>
<affiliation confidence="0.9997765">Massachusetts Institute of Technology Massachusetts Institute of Technology Media Laboratory Media Laboratory</affiliation>
<email confidence="0.998396">mbf@mit.edudkroy@media.mit.edu</email>
<abstract confidence="0.9998235">Grounded language models represent the relationship between words and the non-linguistic context in which they are said. This paper describes how they are learned from large corpora of unlabeled video, and are applied to the task of automatic speech recognition of sports video. Results show that grounded language models improve perplexity and word error rate over text based language models, and further, support video information retrieval better than human generated speech transcriptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
</authors>
<title>A General Model of Action and Time.</title>
<date>1984</date>
<journal>Artificial Intelligence.</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="11834" citStr="Allen, 1984" startWordPosition="1873" endWordPosition="1874">li.ee.washington.edu/~bilmes/gmtk/ 123 in complex and varied temporal relations to each other. To find such patterns automatically, we follow previous work in video content classification in which temporal data mining techniques are used to discover event patterns within streams of lower level features. The algorithm we use is fully unsupervised and proceeds by examining the relations that occur between features in multiple streams within a moving time window. Any two features that occur within this window must be in one of seven temporal relations with each other (e.g. before, during, etc.) (Allen, 1984). The algorithm keeps track of how often each of these relations is observed, and after the entire video corpus is analyzed, uses chi-square analyses to determine which relations are significant. The algorithm iterates through the data, and relations between individual features that are found significant in one iteration (e.g. [OVERLAP, field-scene, cheer]), are themselves treated as individual features in the next. This allows the system to build up higher-order nested relations in each iteration (e.g. [BEFORE, [OVERLAP, field-scene, cheer], field scene]]). The temporal patterns found signifi</context>
</contexts>
<marker>Allen, 1984</marker>
<rawString>Allen, J.F. (1984). A General Model of Action and Time. Artificial Intelligence. 23(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>P Duygulu</author>
<author>N de Freitas</author>
<author>D Forsyth</author>
<author>D Blei</author>
<author>M Jordan</author>
</authors>
<title>Matching Words and Pictures,</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<marker>Barnard, Duygulu, de Freitas, Forsyth, Blei, Jordan, 2003</marker>
<rawString>Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei, D, and Jordan, M. (2003), Matching Words and Pictures, Journal of Machine Learning Research, Vol 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Information Retrieval as Statistical Translation.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR-99.</booktitle>
<contexts>
<context position="28516" citStr="Berger and Lafferty (1999)" startWordPosition="4650" endWordPosition="4653"> the more relevant content words without being distracted by the more common stop words. Our metric for evaluation is the precision with which baseball highlights are returned in a video IR system. We examine three systems: one that uses ASR with the grounded language model, a baseline system that uses ASR with the text only interpolated language model, and finally a system that uses human produced closed caption transcriptions to index events. For each system, all 1200 events from the test set (not just the highlights) are indexed. Queries are generated artificially using a method similar to Berger and Lafferty (1999) and used in Fleischman and Roy (2007). First, each highlight is labeled with the event’s type (e.g. fly ball), the event’s location (e.g. left field) and the event’s result (e.g. double play): 13 labels total. Log likelihood ratios Word Error Rate (WER) Word Accuracy (%) 95 90 85 80 75 70 35 30 25 20 15 10 5 0 switchboard interpolated grounded switchboard interpolated grounded 89.6 15.1 80.3 25.4 76.6 31.3 127 are then used to find the phrases (unigram, trigram, and bigram) most indicative of each label (e.g. “fly ball” for category fly ball). For each label, the three most indicative phrases</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>Berger, A. and Lafferty, J. (1999). Information Retrieval as Statistical Translation. In Proceedings of SIGIR-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>M Jordan</author>
</authors>
<title>Modeling annotated data.</title>
<date>2003</date>
<booktitle>Proceedings of the 26th International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>127--134</pages>
<publisher>ACM Press,</publisher>
<contexts>
<context position="16201" citStr="Blei and Jordan, 2003" startWordPosition="2602" endWordPosition="2605">ttern feature vector is generated as described in section 2.2. These events are then paired with all the words from the closed captioning transcription that occur during each event (plus or minus 10 seconds). Because these transcriptions are not necessarily time synched with the audio, we use the method described in Hauptmann and Witbrock 124 (1998) to align the closed captioning to the announcers’ speech. Previous work has examined applying models often used in MT to the paired corpus described above (Fleischman and Roy, 2006). Recent work in automatic image annotation (Barnard et al., 2003; Blei and Jordan, 2003) and natural language processing (Steyvers et al., 2004), however, have demonstrated the advantages of using hierarchical Bayesian models for related tasks. In this work we follow closely the Author-Topic (AT) model (Steyvers et al., 2004) which is a generalization of Latent Dirichlet Allocation (LDA) (Blei et al., 2005).3 LDA is a technique that was developed to model the distribution of topics discussed in a large corpus of documents. The model assumes that every document is made up of a mixture of topics, and that each word in a document is generated from a probability distribution associat</context>
</contexts>
<marker>Blei, Jordan, 2003</marker>
<rawString>Blei, D. and Jordan, M. (2003). Modeling annotated data. Proceedings of the 26th International Conference on Research and Development in Information Retrieval, ACM Press, 127–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ng Blei</author>
<author>A</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.”</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<pages>3--993</pages>
<marker>Blei, A, Jordan, 2003</marker>
<rawString>Blei, D. Ng, A., and Jordan, M (2003). “Latent Dirichlet allocation.” Journal of Machine Learning Research 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bouthemy</author>
<author>M Gelgon</author>
<author>F Ganansia</author>
</authors>
<title>A unified approach to shot change detection and camera motion characterization.</title>
<date>1999</date>
<booktitle>IEEE Trans. on Circuits and Systems for Video Technology,</booktitle>
<volume>9</volume>
<issue>7</issue>
<contexts>
<context position="9089" citStr="Bouthemy et al. (1999)" startWordPosition="1443" endWordPosition="1446">l situation that is being observed, camera motion features represent more precise information about the actions occurring in a video. The intuition here is that the camera is a stand in for a viewer’s focus of attention. As actions occur in a video, the camera moves to follow it; this camera motion thus mirrors the actions themselves, providing informative features for event representation. Like shot boundary detection, detecting the motion of the camera in a video (i.e., the amount it pans left to right, tilts up and down, and zooms in and out) is a well-studied problem. We use the system of Bouthemy et al. (1999) which computes the camera motion using the parameters of a twodimensional affine model to fit every pair of sequential frames in a video. A 15 state 1st order Hidden Markov Model, implemented with the Graphical Modeling Toolkit,2 then converts the output of the Bouthemy system into a stream of clustered characteristic camera motions (e.g. state 12 clusters together motions of zooming in fast while panning slightly left). Audio Context The audio stream of a video can also provide useful information for representing non-linguistic context. We use boosted decision trees to classify audio into se</context>
</contexts>
<marker>Bouthemy, Gelgon, Ganansia, 1999</marker>
<rawString>Bouthemy, P., Gelgon, M., Ganansia, F. (1999). A unified approach to shot change detection and camera motion characterization. IEEE Trans. on Circuits and Systems for Video Technology, 9(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling,</title>
<date>1998</date>
<tech>Tech. Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard U.,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="21110" citStr="Chen and Goodman, 1998" startWordPosition="3444" endWordPosition="3447"> 125 with traditional unigram, bigram, and trigram language models generated from a combination of the closed captioning transcripts of all training games and data from the switchboard corpus (see below). This backoff is necessary to account for the words not included in the grounded language model itself (i.e. stop words, proper names, low frequency words). The traditional text-only language models (which are also used below as baseline comparisons) are generated with the SRI language modeling toolkit (Stolcke, 2002) using Chen and Goodman&apos;s modified Kneser-Ney discounting and interpolation (Chen and Goodman, 1998). The backoff strategy we employ here is very simple: if the ngram appears in the GLM then it is used, otherwise the traditional LM is used. In future work we will examine more complex backoff strategies (Hsu, in review). We evaluate our grounded language modeling approach using 3 metrics: perplexity, word error rate, and precision on an information retrieval task. 4.1 Perplexity Perplexity is an information theoretic measure of how well a model predicts a held out test set. We use perplexity to compare our grounded language model to two baseline language models: a language model generated fro</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Chen, S. F. and Goodman, J., (1998). An Empirical Study of Smoothing Techniques for Language Modeling, Tech. Report TR-10-98, Computer Science Group, Harvard U., Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>D Roy</author>
</authors>
<title>Situated Models of Meaning for Sports Video Retrieval.</title>
<date>2007</date>
<location>HLT/NAACL. Rochester, NY.</location>
<contexts>
<context position="1979" citStr="Fleischman and Roy, 2007" startWordPosition="293" endWordPosition="296">e speech recognition performance (for an exception see Murkherjee and Roy, 2003). In many types of video, however, visual context can provide valuable clues as to what has been said. For example, in video of Major League Baseball games, the likelihood of the phrase “home run” increases dramatically when a home run has actually been hit. This paper describes a method for incorporating such visual information in an ASR system for sports video. The method is based on the use of grounded language models to represent the relationship between words and the nonlinguistic context to which they refer (Fleischman and Roy, 2007). Grounded language models are based on research from cognitive science on grounded models of meaning. (for a review see Roy, 2005, and Roy and Reiter, 2005). In such models, the meaning of a word is defined by its relationship to representations of the language users’ environment. Thus, for a robot operating in a laboratory setting, words for colors and shapes may be grounded in the outputs of its computer vision system (Roy &amp; Pentland, 2002); while for a simulated agent operating in a virtual world, words for actions and events may be mapped to representations of the agent’s plans or goals (</context>
<context position="7733" citStr="Fleischman and Roy, 2007" startWordPosition="1223" endWordPosition="1226">that require more training data and achieve lower accuracy. The first step in classifying visual context features is to segment the video into shots (or scenes) based on changes in the visual scene due to editing (e.g. jumping from a close up to a wide shot of the field). Shot detection and segmentation is a well studied problem; in this work we use the method of Tardini et al. (2005). After the video is segmented into shots, individual frames (called key frames) are selected and represented as a vector of low level features that describe the key frame’s color distribution, entropy, etc. (see Fleischman and Roy, 2007 for the full list of low level features used). The WEKA machine learning package is used to train a boosted decision tree to classify these frames into one of three categories: pitching-scene, field-scene, other (Witten and Frank, 2005). Those shots whose key frames are classified as field-scenes are then subcategorized (using boosted decision trees) into one of the following categories: infield, outfield, wall, base, running, and misc. Performance of these classification tasks is approximately 96% and 90% accuracy respectively. Camera Motion Features In addition to visual context features, w</context>
<context position="28554" citStr="Fleischman and Roy (2007)" startWordPosition="4657" endWordPosition="4660">t being distracted by the more common stop words. Our metric for evaluation is the precision with which baseball highlights are returned in a video IR system. We examine three systems: one that uses ASR with the grounded language model, a baseline system that uses ASR with the text only interpolated language model, and finally a system that uses human produced closed caption transcriptions to index events. For each system, all 1200 events from the test set (not just the highlights) are indexed. Queries are generated artificially using a method similar to Berger and Lafferty (1999) and used in Fleischman and Roy (2007). First, each highlight is labeled with the event’s type (e.g. fly ball), the event’s location (e.g. left field) and the event’s result (e.g. double play): 13 labels total. Log likelihood ratios Word Error Rate (WER) Word Accuracy (%) 95 90 85 80 75 70 35 30 25 20 15 10 5 0 switchboard interpolated grounded switchboard interpolated grounded 89.6 15.1 80.3 25.4 76.6 31.3 127 are then used to find the phrases (unigram, trigram, and bigram) most indicative of each label (e.g. “fly ball” for category fly ball). For each label, the three most indicative phrases are issued as queries to the system, </context>
</contexts>
<marker>Fleischman, Roy, 2007</marker>
<rawString>Fleischman M, Roy, D. (2007). Situated Models of Meaning for Sports Video Retrieval. HLT/NAACL. Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>D Roy</author>
</authors>
<title>Unsupervised Content-Based Indexing of Sports Video Retrieval.</title>
<date>2007</date>
<booktitle>9th ACM Workshop on Multimedia Information Retrieval (MIR).</booktitle>
<location>Augsburg, Germany.</location>
<contexts>
<context position="1979" citStr="Fleischman and Roy, 2007" startWordPosition="293" endWordPosition="296">e speech recognition performance (for an exception see Murkherjee and Roy, 2003). In many types of video, however, visual context can provide valuable clues as to what has been said. For example, in video of Major League Baseball games, the likelihood of the phrase “home run” increases dramatically when a home run has actually been hit. This paper describes a method for incorporating such visual information in an ASR system for sports video. The method is based on the use of grounded language models to represent the relationship between words and the nonlinguistic context to which they refer (Fleischman and Roy, 2007). Grounded language models are based on research from cognitive science on grounded models of meaning. (for a review see Roy, 2005, and Roy and Reiter, 2005). In such models, the meaning of a word is defined by its relationship to representations of the language users’ environment. Thus, for a robot operating in a laboratory setting, words for colors and shapes may be grounded in the outputs of its computer vision system (Roy &amp; Pentland, 2002); while for a simulated agent operating in a virtual world, words for actions and events may be mapped to representations of the agent’s plans or goals (</context>
<context position="7733" citStr="Fleischman and Roy, 2007" startWordPosition="1223" endWordPosition="1226">that require more training data and achieve lower accuracy. The first step in classifying visual context features is to segment the video into shots (or scenes) based on changes in the visual scene due to editing (e.g. jumping from a close up to a wide shot of the field). Shot detection and segmentation is a well studied problem; in this work we use the method of Tardini et al. (2005). After the video is segmented into shots, individual frames (called key frames) are selected and represented as a vector of low level features that describe the key frame’s color distribution, entropy, etc. (see Fleischman and Roy, 2007 for the full list of low level features used). The WEKA machine learning package is used to train a boosted decision tree to classify these frames into one of three categories: pitching-scene, field-scene, other (Witten and Frank, 2005). Those shots whose key frames are classified as field-scenes are then subcategorized (using boosted decision trees) into one of the following categories: infield, outfield, wall, base, running, and misc. Performance of these classification tasks is approximately 96% and 90% accuracy respectively. Camera Motion Features In addition to visual context features, w</context>
<context position="28554" citStr="Fleischman and Roy (2007)" startWordPosition="4657" endWordPosition="4660">t being distracted by the more common stop words. Our metric for evaluation is the precision with which baseball highlights are returned in a video IR system. We examine three systems: one that uses ASR with the grounded language model, a baseline system that uses ASR with the text only interpolated language model, and finally a system that uses human produced closed caption transcriptions to index events. For each system, all 1200 events from the test set (not just the highlights) are indexed. Queries are generated artificially using a method similar to Berger and Lafferty (1999) and used in Fleischman and Roy (2007). First, each highlight is labeled with the event’s type (e.g. fly ball), the event’s location (e.g. left field) and the event’s result (e.g. double play): 13 labels total. Log likelihood ratios Word Error Rate (WER) Word Accuracy (%) 95 90 85 80 75 70 35 30 25 20 15 10 5 0 switchboard interpolated grounded switchboard interpolated grounded 89.6 15.1 80.3 25.4 76.6 31.3 127 are then used to find the phrases (unigram, trigram, and bigram) most indicative of each label (e.g. “fly ball” for category fly ball). For each label, the three most indicative phrases are issued as queries to the system, </context>
</contexts>
<marker>Fleischman, Roy, 2007</marker>
<rawString>Fleischman, M. and Roy, D. (2007). Unsupervised Content-Based Indexing of Sports Video Retrieval. 9th ACM Workshop on Multimedia Information Retrieval (MIR). Augsburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Fleischman</author>
<author>D Roy</author>
</authors>
<title>Why Verbs are Harder to Learn than Nouns: Initial Insights from a Computational Model of Intention Recognition</title>
<date>2005</date>
<booktitle>in Situated Word Learning. 27th Annual Meeting of the Cognitive Science Society,</booktitle>
<location>Stresa, Italy.</location>
<contexts>
<context position="2602" citStr="Fleischman &amp; Roy, 2005" startWordPosition="402" endWordPosition="405">. Grounded language models are based on research from cognitive science on grounded models of meaning. (for a review see Roy, 2005, and Roy and Reiter, 2005). In such models, the meaning of a word is defined by its relationship to representations of the language users’ environment. Thus, for a robot operating in a laboratory setting, words for colors and shapes may be grounded in the outputs of its computer vision system (Roy &amp; Pentland, 2002); while for a simulated agent operating in a virtual world, words for actions and events may be mapped to representations of the agent’s plans or goals (Fleischman &amp; Roy, 2005). This paper extends previous work on grounded models of meaning by learning a grounded language model from naturalistic data collected from broadcast video of Major League Baseball games. A large corpus of unlabeled sports videos is collected and paired with closed captioning transcriptions of the announcers’ speech. 1 This corpus is used to train the grounded language model, which like traditional language models encode the prior probability of words for an ASR system. Unlike traditional language models, however, grounded language models represent the probability of a word conditioned not on</context>
</contexts>
<marker>Fleischman, Roy, 2005</marker>
<rawString>Fleischman, M. B. and Roy, D. (2005) Why Verbs are Harder to Learn than Nouns: Initial Insights from a Computational Model of Intention Recognition in Situated Word Learning. 27th Annual Meeting of the Cognitive Science Society, Stresa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>P Roy DeCamp</author>
<author>D</author>
</authors>
<title>Mining Temporal Patterns of Movement for Video Content Classification.</title>
<date>2006</date>
<booktitle>ACM Workshop on Multimedia Information Retrieval.</booktitle>
<marker>Fleischman, DeCamp, D, 2006</marker>
<rawString>Fleischman, M., DeCamp, P. Roy, D. (2006). Mining Temporal Patterns of Movement for Video Content Classification. ACM Workshop on Multimedia Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>B Roy</author>
<author>D Roy</author>
</authors>
<title>Temporal Feature Induction for Sports Highlight Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACM Multimedia.</booktitle>
<location>Augsburg, Germany.</location>
<contexts>
<context position="6421" citStr="Fleischman et al., 2007" startWordPosition="1003" endWordPosition="1006">that different directors employ are extremely consistent within a sport and make up a “language of film” which the machine can take advantage of in order to represent the events taking place in the video. Thus, even though it is not easy to automatically identify a player hitting a ball in video, it is easy to detect features that correlate with hitting, e.g., when a scene focusing on the pitching mound immediately jumps to one zooming in on the field (see Figure 1). Although these correlations are not perfect, experiments have shown that baseball events can be classified using such features (Fleischman et al., 2007). We exploit the language of film to represent events in sports video in two phases. First, low level features that correlate with basic events in sports are extracted from the video stream. Then, temporal data mining is used to find patterns within this low level event stream. 2.1 Feature Extraction We extract three types of features: visual context features, camera motion features, and audio context features. 122 Visual Context Features Visual context features encode general properties of the visual scene in a video segment. Supervised classifiers are trained to identify these features, whic</context>
</contexts>
<marker>Fleischman, Roy, Roy, 2007</marker>
<rawString>Fleischman, M., Roy, B., and Roy, D. (2007). Temporal Feature Induction for Sports Highlight Classification. In Proceedings of ACM Multimedia. Augsburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gong</author>
<author>M Han</author>
<author>W Hua</author>
<author>W Xu</author>
</authors>
<title>Maximum entropy model-based baseball highlight detection and classification.</title>
<date>2004</date>
<journal>Computer Vision and Image Understanding.</journal>
<volume>96</volume>
<issue>2</issue>
<contexts>
<context position="15245" citStr="Gong et al., 2004" startWordPosition="2438" endWordPosition="2441">imate these conditional distributions using a framework similar that used for training acoustic models in ASR and translation models in Machine Translation (MT). We generate a training corpus of utterances paired with representations of the non-linguistic context in which they were uttered. The first step in generating this corpus is to generate the low level features described in Section 2.1 for each video in our training set. We then segment each video into a set of independent events based on the visual context features we have extracted. We follow previous work in sports video processing (Gong et al., 2004) and define an event in a baseball video as any sequence of shots starting with a pitching-scene and continuing for four subsequent shots. This definition follows from the fact that the vast majority of events in baseball start with a pitch and do not last longer than four shots. For each of these events in our corpus, a temporal pattern feature vector is generated as described in section 2.2. These events are then paired with all the words from the closed captioning transcription that occur during each event (plus or minus 10 seconds). Because these transcriptions are not necessarily time syn</context>
</contexts>
<marker>Gong, Han, Hua, Xu, 2004</marker>
<rawString>Gong, Y., Han, M., Hua, W., Xu, W. (2004). Maximum entropy model-based baseball highlight detection and classification. Computer Vision and Image Understanding. 96(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hauptmann</author>
</authors>
<date>1998</date>
<booktitle>Story Segmentation and Detection of Commercials in Broadcast News Video, Advances in Digital Libraries.</booktitle>
<marker>Hauptmann, 1998</marker>
<rawString>Hauptmann, A. , Witbrock, M., (1998) Story Segmentation and Detection of Commercials in Broadcast News Video, Advances in Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hongen</author>
<author>R Bremond Nevatia</author>
<author>F</author>
</authors>
<title>Video-based event recognition: activity representation and probabilistic recognition methods.</title>
<date>2004</date>
<journal>Computer Vision and Image Understanding.</journal>
<volume>96</volume>
<issue>2</issue>
<contexts>
<context position="5044" citStr="Hongen et al., 2004" startWordPosition="777" endWordPosition="780">ty that a word was uttered given such event representations. In the following sections we describe these two aspects of our approach and evaluate the performance of our grounded language model on a speech recognition task using video highlights from Major League Baseball games. Results indicate improved performance using three metrics: perplexity, word error rate, and precision on an information retrieval task. 2 Representing Events in Sports Video Recent work in video surveillance has demonstrated the benefit of representing complex events as temporal relations between lower level subevents (Hongen et al., 2004). Thus, to represent events in the sports domain, we would ideally first represent the basic sub events that occur in sports video (e.g., hitting, throwing, catching, running, etc.) and then build up complex events (such as home run) as a set of temporal relations between these basic events. Unfortunately, due to the limitations of computer vision techniques, reliably identifying such basic events in video is not feasible. However, sports video does have characteristics that can be exploited to effectively represent complex events. Like much broadcast video, sports video is highly produced, ex</context>
</contexts>
<marker>Hongen, Nevatia, F, 2004</marker>
<rawString>Hongen, S., Nevatia, R. Bremond, F. (2004). Video-based event recognition: activity representation and probabilistic recognition methods. Computer Vision and Image Understanding. 96(2).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bo-June</author>
</authors>
<title>(in review). Generalized Linear Interpolation of Language Models.</title>
<marker>Bo-June, </marker>
<rawString>Hsu , Bo-June (Paul). (in review). Generalized Linear Interpolation of Language Models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jang</author>
<author>A Hauptmann</author>
</authors>
<title>Learning to Recognize Speech by Watching Television.</title>
<date>1999</date>
<journal>IEEE Intelligent Systems Magazine,</journal>
<volume>14</volume>
<issue>5</issue>
<pages>51--58</pages>
<marker>Jang, Hauptmann, 1999</marker>
<rawString>Jang, P., Hauptmann, A. (1999). Learning to Recognize Speech by Watching Television. IEEE Intelligent Systems Magazine, 14(5), pp. 51-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Mukherjee</author>
<author>D Roy</author>
</authors>
<title>A Visual ContextAware Multimodal System for Spoken Language Processing.</title>
<date>2003</date>
<booktitle>Proc. Eurospeech,</booktitle>
<volume>4</volume>
<pages>pages.</pages>
<marker>Mukherjee, Roy, 2003</marker>
<rawString>Mukherjee, N. and Roy, D.. (2003). A Visual ContextAware Multimodal System for Spoken Language Processing. Proc. Eurospeech, 4 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>A Language Modeling Approach to Information Retrieval.</title>
<date>1998</date>
<booktitle>In Proc. of SIGIR’98.</booktitle>
<contexts>
<context position="29240" citStr="Ponte and Croft (1998)" startWordPosition="4776" endWordPosition="4779">ly ball), the event’s location (e.g. left field) and the event’s result (e.g. double play): 13 labels total. Log likelihood ratios Word Error Rate (WER) Word Accuracy (%) 95 90 85 80 75 70 35 30 25 20 15 10 5 0 switchboard interpolated grounded switchboard interpolated grounded 89.6 15.1 80.3 25.4 76.6 31.3 127 are then used to find the phrases (unigram, trigram, and bigram) most indicative of each label (e.g. “fly ball” for category fly ball). For each label, the three most indicative phrases are issued as queries to the system, which ranks its results using the language modeling approach of Ponte and Croft (1998). Precision is measured on how many of the top five returned events are of the correct category. Figure 4 shows the precision of the video IR systems based on ASR with the grounded language model, ASR with the text-only interpolated language model, and closed captioning transcriptions. As with our previous evaluations, the IR results show that the system using ASR with the grounded language model performed better than the one using ASR with the text-only language model (5.1% absolute improvement). More notably, though, Figure 4 shows that the system using the grounded language model performed </context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Ponte, J.M., and Croft, W.B. (1998). A Language Modeling Approach to Information Retrieval. In Proc. of SIGIR’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roy</author>
</authors>
<title>Grounding Words in Perception and Action: Insights from Computational Models.</title>
<date>2005</date>
<tech>TICS.</tech>
<contexts>
<context position="2109" citStr="Roy, 2005" startWordPosition="317" endWordPosition="318">able clues as to what has been said. For example, in video of Major League Baseball games, the likelihood of the phrase “home run” increases dramatically when a home run has actually been hit. This paper describes a method for incorporating such visual information in an ASR system for sports video. The method is based on the use of grounded language models to represent the relationship between words and the nonlinguistic context to which they refer (Fleischman and Roy, 2007). Grounded language models are based on research from cognitive science on grounded models of meaning. (for a review see Roy, 2005, and Roy and Reiter, 2005). In such models, the meaning of a word is defined by its relationship to representations of the language users’ environment. Thus, for a robot operating in a laboratory setting, words for colors and shapes may be grounded in the outputs of its computer vision system (Roy &amp; Pentland, 2002); while for a simulated agent operating in a virtual world, words for actions and events may be mapped to representations of the agent’s plans or goals (Fleischman &amp; Roy, 2005). This paper extends previous work on grounded models of meaning by learning a grounded language model from</context>
</contexts>
<marker>Roy, 2005</marker>
<rawString>Roy, D. (2005). . Grounding Words in Perception and Action: Insights from Computational Models. TICS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roy</author>
<author>A Pentland</author>
</authors>
<title>Learning Words from Sights and Sounds: A Computational Model.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="2426" citStr="Roy &amp; Pentland, 2002" startWordPosition="371" endWordPosition="375">ethod is based on the use of grounded language models to represent the relationship between words and the nonlinguistic context to which they refer (Fleischman and Roy, 2007). Grounded language models are based on research from cognitive science on grounded models of meaning. (for a review see Roy, 2005, and Roy and Reiter, 2005). In such models, the meaning of a word is defined by its relationship to representations of the language users’ environment. Thus, for a robot operating in a laboratory setting, words for colors and shapes may be grounded in the outputs of its computer vision system (Roy &amp; Pentland, 2002); while for a simulated agent operating in a virtual world, words for actions and events may be mapped to representations of the agent’s plans or goals (Fleischman &amp; Roy, 2005). This paper extends previous work on grounded models of meaning by learning a grounded language model from naturalistic data collected from broadcast video of Major League Baseball games. A large corpus of unlabeled sports videos is collected and paired with closed captioning transcriptions of the announcers’ speech. 1 This corpus is used to train the grounded language model, which like traditional language models encod</context>
</contexts>
<marker>Roy, Pentland, 2002</marker>
<rawString>Roy, D. and Pentland, A. (2002). Learning Words from Sights and Sounds: A Computational Model. Cognitive Science, 26(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D</author>
<author>E Reiter</author>
</authors>
<title>Connecting Language to the World.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>167</volume>
<issue>1</issue>
<pages>1--12</pages>
<marker>D, Reiter, 2005</marker>
<rawString>Roy. D. and Reiter, E. (2005). . Connecting Language to the World. Artificial Intelligence, 167(1-2), 1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G M Snoek</author>
<author>M Worring</author>
</authors>
<title>Multimodal video indexing: A review of the state-of-the-art. Multimedia Tools and Applications,</title>
<date>2005</date>
<pages>25--1</pages>
<contexts>
<context position="921" citStr="Snoek and Worring, 2005" startWordPosition="127" endWordPosition="130">ionship between words and the non-linguistic context in which they are said. This paper describes how they are learned from large corpora of unlabeled video, and are applied to the task of automatic speech recognition of sports video. Results show that grounded language models improve perplexity and word error rate over text based language models, and further, support video information retrieval better than human generated speech transcriptions. 1 Introduction Recognizing speech in broadcast video is a necessary precursor to many multimodal applications such as video search and summarization (Snoek and Worring, 2005;). Although performance is often reasonable in controlled environments (such as studio news rooms), automatic speech recognition (ASR) systems have significant difficulty in noisier settings (such as those found in live sports broadcasts) (Wactlar et al., 1996). While many researches have examined how to compensate for such noise using acoustic techniques, few have attempted to leverage information in the visual stream to improve speech recognition performance (for an exception see Murkherjee and Roy, 2003). In many types of video, however, visual context can provide valuable clues as to what</context>
</contexts>
<marker>Snoek, Worring, 2005</marker>
<rawString>Snoek, C.G.M. and Worring, M.. (2005). Multimodal video indexing: A review of the state-of-the-art. Multimedia Tools and Applications, 25(1):5-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>P Smyth</author>
<author>M Rosen-Zvi</author>
<author>T Griffiths</author>
</authors>
<title>Probabilistic Author-Topic Models for Information Discovery.</title>
<date>2004</date>
<booktitle>The Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="16257" citStr="Steyvers et al., 2004" startWordPosition="2610" endWordPosition="2613">n 2.2. These events are then paired with all the words from the closed captioning transcription that occur during each event (plus or minus 10 seconds). Because these transcriptions are not necessarily time synched with the audio, we use the method described in Hauptmann and Witbrock 124 (1998) to align the closed captioning to the announcers’ speech. Previous work has examined applying models often used in MT to the paired corpus described above (Fleischman and Roy, 2006). Recent work in automatic image annotation (Barnard et al., 2003; Blei and Jordan, 2003) and natural language processing (Steyvers et al., 2004), however, have demonstrated the advantages of using hierarchical Bayesian models for related tasks. In this work we follow closely the Author-Topic (AT) model (Steyvers et al., 2004) which is a generalization of Latent Dirichlet Allocation (LDA) (Blei et al., 2005).3 LDA is a technique that was developed to model the distribution of topics discussed in a large corpus of documents. The model assumes that every document is made up of a mixture of topics, and that each word in a document is generated from a probability distribution associated with one of those topics. The AT model generalizes LD</context>
</contexts>
<marker>Steyvers, Smyth, Rosen-Zvi, Griffiths, 2004</marker>
<rawString>Steyvers, M., Smyth, P., Rosen-Zvi, M., &amp; Griffiths, T. (2004). Probabilistic Author-Topic Models for Information Discovery. The Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit,</title>
<date>2002</date>
<booktitle>in Proc. Intl. Conf. Spoken Language Processing,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="21010" citStr="Stolcke, 2002" startWordPosition="3433" endWordPosition="3434">top words were used. These grounded language models are then combined in a backoff strategy 125 with traditional unigram, bigram, and trigram language models generated from a combination of the closed captioning transcripts of all training games and data from the switchboard corpus (see below). This backoff is necessary to account for the words not included in the grounded language model itself (i.e. stop words, proper names, low frequency words). The traditional text-only language models (which are also used below as baseline comparisons) are generated with the SRI language modeling toolkit (Stolcke, 2002) using Chen and Goodman&apos;s modified Kneser-Ney discounting and interpolation (Chen and Goodman, 1998). The backoff strategy we employ here is very simple: if the ngram appears in the GLM then it is used, otherwise the traditional LM is used. In future work we will examine more complex backoff strategies (Hsu, in review). We evaluate our grounded language modeling approach using 3 metrics: perplexity, word error rate, and precision on an information retrieval task. 4.1 Perplexity Perplexity is an information theoretic measure of how well a model predicts a held out test set. We use perplexity to</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, A., (2002). SRILM - An Extensible Language Modeling Toolkit, in Proc. Intl. Conf. Spoken Language Processing, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grana C Tardini</author>
<author>R Marchi</author>
<author>R Cucchiara</author>
</authors>
<title>Shot Detection and Motion Analysis for Automatic MPEG-7 Annotation of Sports Videos.</title>
<date>2005</date>
<booktitle>In 13th International Conference on Image Analysis and Processing.</booktitle>
<contexts>
<context position="7496" citStr="Tardini et al. (2005)" startWordPosition="1184" endWordPosition="1187">ext features encode general properties of the visual scene in a video segment. Supervised classifiers are trained to identify these features, which are relatively simple to classify in comparison to high level events (like home runs) that require more training data and achieve lower accuracy. The first step in classifying visual context features is to segment the video into shots (or scenes) based on changes in the visual scene due to editing (e.g. jumping from a close up to a wide shot of the field). Shot detection and segmentation is a well studied problem; in this work we use the method of Tardini et al. (2005). After the video is segmented into shots, individual frames (called key frames) are selected and represented as a vector of low level features that describe the key frame’s color distribution, entropy, etc. (see Fleischman and Roy, 2007 for the full list of low level features used). The WEKA machine learning package is used to train a boosted decision tree to classify these frames into one of three categories: pitching-scene, field-scene, other (Witten and Frank, 2005). Those shots whose key frames are classified as field-scenes are then subcategorized (using boosted decision trees) into one </context>
</contexts>
<marker>Tardini, Marchi, Cucchiara, 2005</marker>
<rawString>Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005). Shot Detection and Motion Analysis for Automatic MPEG-7 Annotation of Sports Videos. In 13th International Conference on Image Analysis and Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wactlar</author>
<author>M Witbrock</author>
<author>A Hauptmann</author>
</authors>
<title>Informedia: News-on-Demand Experiments in Speech Recognition. ARPA Speech Recognition Workshop,</title>
<date>1996</date>
<location>Arden House, Harriman, NY.</location>
<contexts>
<context position="1183" citStr="Wactlar et al., 1996" startWordPosition="164" endWordPosition="167">nguage models improve perplexity and word error rate over text based language models, and further, support video information retrieval better than human generated speech transcriptions. 1 Introduction Recognizing speech in broadcast video is a necessary precursor to many multimodal applications such as video search and summarization (Snoek and Worring, 2005;). Although performance is often reasonable in controlled environments (such as studio news rooms), automatic speech recognition (ASR) systems have significant difficulty in noisier settings (such as those found in live sports broadcasts) (Wactlar et al., 1996). While many researches have examined how to compensate for such noise using acoustic techniques, few have attempted to leverage information in the visual stream to improve speech recognition performance (for an exception see Murkherjee and Roy, 2003). In many types of video, however, visual context can provide valuable clues as to what has been said. For example, in video of Major League Baseball games, the likelihood of the phrase “home run” increases dramatically when a home run has actually been hit. This paper describes a method for incorporating such visual information in an ASR system f</context>
<context position="27400" citStr="Wactlar et al., 1996" startWordPosition="4461" endWordPosition="4464">or distinguishing the latter error, it is unlikely to assist with the former. Thus, in the next section we examine an extrinsic evaluation in which grounded language models are judged not directly on their effect on word accuracy or error rate, but based on their ability to support video information retrieval. 4.3 Precision of Information Retrieval One of the most commonly used applications of ASR for video is to support information retrieval (IR). Such video IR systems often use speech transcriptions to index segments of video in much the same way that words are used to index text documents (Wactlar et al., 1996). For example, in the domain of baseball, if a video IR system were issued the query “home run,” it would typically return a set of video clips by searching its database for events in which someone uttered the phrase “home run.” Because such systems rely on ASR output to search video, the performance of a video IR system gives an indirect evaluation of the ASR’s quality. Further, unlike the case with word accuracy or error rate, such evaluations highlight a systems ability to recognize the more relevant content words without being distracted by the more common stop words. Our metric for evalua</context>
</contexts>
<marker>Wactlar, Witbrock, Hauptmann, 1996</marker>
<rawString>Wactlar, H., Witbrock, M., Hauptmann, A., (1996 ). Informedia: News-on-Demand Experiments in Speech Recognition. ARPA Speech Recognition Workshop, Arden House, Harriman, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques. 2nd Edition,</title>
<date>2005</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="7970" citStr="Witten and Frank, 2005" startWordPosition="1261" endWordPosition="1264">e up to a wide shot of the field). Shot detection and segmentation is a well studied problem; in this work we use the method of Tardini et al. (2005). After the video is segmented into shots, individual frames (called key frames) are selected and represented as a vector of low level features that describe the key frame’s color distribution, entropy, etc. (see Fleischman and Roy, 2007 for the full list of low level features used). The WEKA machine learning package is used to train a boosted decision tree to classify these frames into one of three categories: pitching-scene, field-scene, other (Witten and Frank, 2005). Those shots whose key frames are classified as field-scenes are then subcategorized (using boosted decision trees) into one of the following categories: infield, outfield, wall, base, running, and misc. Performance of these classification tasks is approximately 96% and 90% accuracy respectively. Camera Motion Features In addition to visual context features, we also examine the camera motion that occurs within a video. Unlike visual context features, which provide information about the global situation that is being observed, camera motion features represent more precise information about the</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten, I. and Frank, E. (2005). Data Mining: Practical machine learning tools and techniques. 2nd Edition, Morgan Kaufmann. San Francisco, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>