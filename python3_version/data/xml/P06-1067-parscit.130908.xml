<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000834">
<title confidence="0.988389">
Distortion Models For Statistical Machine Translation
</title>
<author confidence="0.703277">
Yaser Al-Onaizan and Kishore Papineni
</author>
<affiliation confidence="0.550249">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.8781945">
1101 Kitchawan Road
Yorktown Heights, NY 10598, USA
</address>
<email confidence="0.998383">
{onaizan, papineni}@us.ibm.com
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969692307692">
In this paper, we argue that n-gram lan-
guage models are not sufficient to address
word reordering required for Machine Trans-
lation. We propose a new distortion model
that can be used with existing phrase-based
SMT decoders to address those n-gram lan-
guage model limitations. We present empirical
results in Arabic to English Machine Transla-
tion that show statistically significant improve-
ments when our proposed model is used. We
also propose a novel metric to measure word
order similarity (or difference) between any
pair of languages based on word alignments.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999384222222222">
A language model is a statistical model that gives
a probability distribution over possible sequences of
words. It computes the probability of producing a given
word w1 given all the words that precede it in the sen-
tence. An n-gram language model is an n-th order
Markov model where the probability of generating a
given word depends only on the last n − 1 words im-
mediately preceding it and is given by the following
equation:
</bodyText>
<equation confidence="0.9995615">
P(wk1) = P(w1)P(w2|w1) ··· P(wn|wn�1
1 ) (1)
</equation>
<bodyText confidence="0.999809725490196">
where k &gt;= n.
N-gram language models have been successfully
used in Automatic Speech Recognition (ASR) as was
first proposed by (Bahl et al., 1983). They play an im-
portant role in selecting among several candidate word
realization of a given acoustic signal. N-gram lan-
guage models have also been used in Statistical Ma-
chine Translation (SMT) as proposed by (Brown et al.,
1990; Brown et al., 1993). The run-time search pro-
cedure used to find the most likely translation (or tran-
scription in the case of Speech Recognition) is typically
referred to as decoding.
There is a fundamental difference between decoding
for machine translation and decoding for speech recog-
nition. When decoding a speech signal, words are gen-
erated in the same order in which their corresponding
acoustic signal is consumed. However, that is not nec-
essarily the case in MT due to the fact that different
languages have different word order requirements. For
example, in Spanish and Arabic adjectives are mainly
noun post-modifiers, whereas in English adjectives are
noun pre-modifiers. Therefore, when translating be-
tween Spanish and English, words must usually be re-
ordered.
Existing statistical machine translation decoders
have mostly relied on language models to select the
proper word order among many possible choices when
translating between two languages. In this paper, we
argue that a language model is not sufficient to ade-
quately address this issue, especially when translating
between languages that have very different word orders
as suggested by our experimental results in Section 5.
We propose a new distortion model that can be used
as an additional component in SMT decoders. This
new model leads to significant improvements in MT
quality as measured by BLEU (Papineni et al., 2002).
The experimental results we report in this paper are for
Arabic-English machine translation of news stories.
We also present a novel method for measuring word
order similarity (or differences) between any given pair
of languages based on word alignments as described in
Section 3.
The rest of this paper is organized as follows. Sec-
tion 2 presents a review of related work. In Section 3
we propose a method for measuring the distortion be-
tween any given pair of languages. In Section 4, we
present our proposed distortion model. In Section 5,
we present some empirical results that show the utility
of our distortion model for statistical machine trans-
lation systems. Then, we conclude this paper with a
discussion in Section 6.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996545333333333">
Different languages have different word order require-
ments. SMT decoders attempt to generate translations
in the proper word order by attempting many possible
</bodyText>
<page confidence="0.993282">
529
</page>
<note confidence="0.5331325">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 529–536,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999962214285714">
word reorderings during the translation process. Trying
all possible word reordering is an NP-Complete prob-
lem as shown in (Knight, 1999), which makes search-
ing for the optimal solution among all possible permu-
tations computationally intractable. Therefore, SMT
decoders typically limit the number of permutations
considered for efficiency reasons by placing reorder-
ing restrictions. Reordering restrictions for word-based
SMT decoders were introduced by (Berger et al., 1996)
and (Wu, 1996). (Berger et al., 1996) allow only re-
ordering of at most n words at any given time. (Wu,
1996) propose using contiguity restrictions on the re-
ordering. For a comparison and a more detailed discus-
sion of the two approaches see (Zens and Ney, 2003).
A different approach to allow for a limited reorder-
ing is to reorder the input sentence such that the source
and the target sentences have similar word order and
then proceed to monotonically decode the reordered
source sentence.
Monotone decoding translates words in the same or-
der they appear in the source language. Hence, the
input and output sentences have the same word order.
Monotone decoding is very efficient since the optimal
decoding can be found in polynomial time. (Tillmann
et al., 1997) proposed a DP-based monotone search al-
gorithm for SMT. Their proposed solution to address
the necessary word reordering is to rewrite the input
sentence such that it has a similar word order to the de-
sired target sentence. The paper suggests that reorder-
ing the input reduces the translation error rate. How-
ever, it does not provide a methodology on how to per-
form this reordering.
(Xia and McCord, 2004) propose a method to auto-
matically acquire rewrite patterns that can be applied
to any given input sentence so that the rewritten source
and target sentences have similar word order. These
rewrite patterns are automatically extracted by pars-
ing the source and target sides of the training parallel
corpus. Their approach show a statistically-significant
improvement over a phrase-based monotone decoder.
Their experiments also suggest that allowing the de-
coder to consider some word order permutations in
addition to the rewrite patterns already applied to the
source sentence actually decreases the BLEU score.
Rewriting the input sentence whether using syntactic
rules or heuristics makes hard decisions that can not
be undone by the decoder. Hence, reordering is better
handled during the search algorithm and as part of the
optimization function.
Phrase-based monotone decoding does not directly
address word order issues. Indirectly, however, the
phrase dictionary1 in phrase-based decoders typically
captures local reorderings that were seen in the training
data. However, it fails to generalize to word reorder-
ings that were never seen in the training data. For ex-
ample, a phrase-based decoder might translate the Ara-
</bodyText>
<footnote confidence="0.929252">
1Also referred to in the literature as the set of blocks or
clumps.
</footnote>
<bodyText confidence="0.999955">
bic phrase AlwlAyAt AlmtHdp2 correctly into English
as the United States if it was seen in its training data,
was aligned correctly, and was added to the phrase dic-
tionary. However, if the phrase Almmlkp AlmtHdp is
not in the phrase dictionary, it will not be translated
correctly by a monotone phrase decoder even if the in-
dividual units of the phrase Almmlkp and AlmtHdp, and
their translations (Kingdom and United, respectively)
are in the phrase dictionary since that would require
swapping the order of the two words.
(Och et al., 1999; Tillmann and Ney, 2003) relax
the monotonicity restriction in their phrase-based de-
coder by allowing a restricted set of word reorderings.
For their translation task, word reordering is done only
for words belonging to the verb group. The context in
which they report their results is a Speech-to-Speech
translation from German to English.
(Yamada and Knight, 2002) propose a syntax-based
decoder that restrict word reordering based on reorder-
ing operations on syntactic parse-trees of the input
sentence. They reported results that are better than
word-based IBM4-like decoder. However, their de-
coder is outperformed by phrase-based decoders such
as (Koehn, 2004), (Och et al., 1999), and (Tillmann and
Ney, 2003) . Phrase-based SMT decoders mostly rely
on the language model to select among possible word
order choices. However, in our experiments we show
that the language model is not reliable enough to make
the choices that lead to a better MT quality. This obser-
vation is also reported by (Xia and McCord, 2004).We
argue that the distortion model we propose leads to a
better translation as measured by BLEU.
Distortion models were first proposed by (Brown et
al., 1993) in the so-called IBM Models. IBM Mod-
els 2 and 3 define the distortion parameters in terms of
the word positions in the sentence pair, not the actual
words at those positions. Distortion probability is also
conditioned on the source and target sentence lengths.
These models do not generalize well since their param-
eters are tied to absolute word position within sentences
which tend to be different for the same words across
sentences. IBM Models 4 and 5 alleviate this limita-
tion by replacing absolute word positions with relative
positions. The latter models define the distortion pa-
rameters for a cept (one or more words). This models
phrasal movement better since words tend to move in
blocks and not independently. The distortion is con-
ditioned on classes of the aligned source and target
words. The entire source and target vocabularies are
reduced to a small number of classes (e.g., 50) for the
purpose of estimating those parameters.
Similarly, (Koehn et al., 2003) propose a relative dis-
tortion model to be used with a phrase decoder. The
model is defined in terms of the difference between the
position of the current phrase and the position of the
previous phrase in the source sentence. It does not con-
</bodyText>
<footnote confidence="0.96987">
2Arabic text appears throughout this paper in Tim Buck-
walter’s Romanization.
</footnote>
<page confidence="0.977607">
530
</page>
<table confidence="0.9995798">
Arabic Ezp1 AbrAhym2 ystqbl3 ms&amp;wlA4 AqtSAdyA5 sEwdyA6 fy7 bgdAd8
English Izzet1 Ibrahim2 Meets3 Saudi4 Trade5 official6 in7 Baghdad8
Word Alignment (Ezp1,Izzet1) (AbrAhym2,Ibrahim2) (ystqbl3,Meets3) ( ms&amp;wlA4,official6)
(AqtSAdyA5,Trade5) (sEwdyA6,Saudi4) (fy7,in7) (bgdAd8,Baghdad8)
Reordered English Izzet1 Ibrahim2 Meets3 official6 Trade5 Saudi4 in7 Baghdad8
</table>
<tableCaption confidence="0.999769">
Table 1: Alignment-based word reordering. The indices are not part of the sentence pair, they are only used to
</tableCaption>
<bodyText confidence="0.963603916666667">
illustrate word positions in the sentence. The indices in the reordered English denote word position in the original
English order.
sider the words in those positions.
The distortion model we propose assigns a proba-
bility distribution over possible relative jumps condi-
tioned on source words. Conditioning on the source
words allows for a much more fine-grained model. For
instance, words that tend to act as modifers (e.g., adjec-
tives) would have a different distribution than verbs or
nouns. Our model’s parameters are directly estimated
from word alignments as we will further explain in Sec-
tion 4. We will also show how to generalize this word
distortion model to a phrase-based model.
(Och et al., 2004; Tillman, 2004) propose
orientation-based distortion models lexicalized on the
phrase level. There are two important distinctions be-
tween their models and ours. First, they lexicalize their
model on the phrases, which have many more param-
eters and hence would require much more data to esti-
mate reliably. Second, their models consider only the
direction (i.e., orientation) and not the relative jump.
We are not aware of any work on measuring word
order differences between a given language pair in the
context of statistical machine translation.
</bodyText>
<sectionHeader confidence="0.8233545" genericHeader="method">
3 Measuring Word Order Similarity
Between Two Language
</sectionHeader>
<bodyText confidence="0.945625351351351">
In this section, we propose a simple, novel method for
measuring word order similarity (or differences) be-
tween any given language pair. This method is based
on word-alignments and the BLEU metric.
We assume that we have word-alignments for a set
of sentence pairs. We first reorder words in the target
sentence (e.g., English when translating from Arabic
to English) according to the order in which they are
aligned to the source words as shown in Table 1. If
a target word is not aligned, then, we assume that it
is aligned to the same source word that the preceding
aligned target word is aligned to.
Once the reordered target (here English) sentences
are generated, we measure the distortion between the
language pair by computing the BLEU3 score between
the original target and reordered target, treating the
original target as the reference.
Table 2 shows these scores for Arabic-English and
3the BLEU scores reported throughout this paper are for
case-sensitive BLEU. The number of references used is also
reported (e.g., BLEUr1n4c: r1 means 1 reference, n4 means
upto 4-gram are considred, c means case sensitive).
Chinese-English. The word alignments we use are both
annotated manually by human annotators. The Arabic-
English test set is the NIST MT Evaluation 2003 test
set. It contains 663 segments (i.e., sentences). The
Arabic side consists of 16,652 tokens and the English
consists of 19,908 tokens. The Chinese-English test set
contains 260 segments. The Chinese side is word seg-
mented and consists of 4,319 tokens and the English
consists of 5,525 tokens.
As suggested by the BLEU scores reported in Ta-
ble 2, Arabic-English has more word order differences
than Chinese-English. The difference in n-gPrec is big-
ger for smaller values of n, which suggests that Arabic-
English has more local word order differences than in
Chinese-English.
</bodyText>
<sectionHeader confidence="0.997771" genericHeader="method">
4 Proposed Distortion Model
</sectionHeader>
<bodyText confidence="0.999896">
The distortion model we are proposing consists of three
components: outbound, inbound, and pair distortion.
Intuitively our distortion models attempt to capture the
order in which source words need to be translated. For
instance, the outbound distortion component attempts
to capture what is typically translated immediately after
the word that has just been translated. Do we tend to
translate words that precede it or succeed it? Which
word position to translate next?
Our distortion parameters are directly estimated
from word alignments by simple counting over align-
ment links in the training data. Any aligner such as
(Al-Onaizan et al., 1999) or (Vogel et al., 1996) can
be used to obtain word alignments. For the results
reported in this paper word alignments were obtained
using a maximum-posterior word aligner4 described in
(Ge, 2004).
We will illustrate the components of our model with
a partial word alignment. Let us assume that our
source sentences is (f10, f250
</bodyText>
<equation confidence="0.823959">
, f30
0
</equation>
<bodyText confidence="0.702245">
)6, and our target
sentence is (e410, e20), and their word alignment is
a = ((f10, e410), (f300, e20)). Word Alignment a can
</bodyText>
<footnote confidence="0.999374444444444">
4We also estimated distortion parameters using a Maxi-
mum Entropy aligner and the differences were negligible.
5In practice, we add special symbols at the start and end of
the source and target sentences, we also assume that the start
symbols in the source and target are aligned, and similarly
for the end symbols. Those special symbols are omitted in
our example for ease of presentation.
6The indices here represent source and target vocabulary
ids.
</footnote>
<page confidence="0.962278">
531
</page>
<table confidence="0.99996875">
N-gram Precision Arabic-English Chinese-English
1-gPrec 1 1
2-gPrec 0.6192 0.7378
3-gPrec 0.4547 0.5382
4-gPrec 0.3535 0.3990
5-gPrec 0.2878 0.3075
6-gPrec 0.2378 0.2406
7-gPrec 0.1977 0.1930
8-gPrec 0.1653 0.1614
9-gPrec 0.1380 0.1416
BLEUr1n4c 0.3152 0.3340
95% Confidence σ 0.0180 0.0370
</table>
<tableCaption confidence="0.875648">
Table 2: Word order similarity for two language pairs: Arabic-English and Chinese-English. n-gPrec is the n-gram
precision as defined in BLEU.
</tableCaption>
<bodyText confidence="0.995386857142857">
be rewritten as a1 = 1 and a2 = 3 (i.e., the second tar-
get word is aligned to the third source word). From this
partial alignment we increase the counts for the follow-
ing outbound, inbound, and pair distortions: Po(δ =
+2|f10), Pi(δ = +2|f300). and Pp(δ = +2|f10, f300).
Formally, our distortion model components are de-
fined as follows:
</bodyText>
<equation confidence="0.95037425">
Outbound Distortion:
Po(δ|fi) = (EC(6k fi)
|fi) (2)
k
</equation>
<bodyText confidence="0.99996175">
where fi is a foreign word (i.e., Arabic in our case),
δ is the step size, and C(δ|fi) is the observed count of
this parameter over all word alignments in the training
data. The value for δ, in theory, ranges from −max to
+max (where max is the maximum source sentence
length observed), but in practice only a small number
of those step sizes are observed in the training data,
and hence, have non-zero value).
</bodyText>
<equation confidence="0.9877424">
Inbound Distortion:
C(δ|fj)
Pi(δ|fj) = (3)
EC(δk|fj)
k
Pairwise Distortion:
C(δ|fi, fj)
Pp(δ|fi, fj) = (4)
E C(δk|fi, fj)
k
</equation>
<bodyText confidence="0.999939333333333">
In order to use these probability distributions in our
decoder, they are then turned into costs. The outbound
distortion cost is defined as:
</bodyText>
<equation confidence="0.990702">
Co(δ|fi) = log {αPo(δ|fi) + (1 − α)P3(δ)} (5)
</equation>
<bodyText confidence="0.9941805">
where P3(δ) is a smoothing distribution 7 and α is a
linear-mixture parameter 8.
</bodyText>
<footnote confidence="0.997947">
7The smoothing we use is a geometrically decreasing dis-
tribution as the step size increases.
8For the experiments reported here we use α = 0.1,
which is set empirically.
</footnote>
<bodyText confidence="0.9996999375">
The inbound and pair costs (Ci(δ|fi) and
Cp(δ|fi, fj)) can be defined in a similar fashion.
So far, our distortion cost is defined in terms of
words, not phrases. Therefore, we need to general-
ize the distortion cost in order to use it in a phrase-
based decoder. This generalization is defined in terms
of the internal word alignment within phrases (we used
the Viterbi word alignment). We illustrate this with
an example: Suppose the last position translated in the
source sentence so far is n and we are to cover a source
phrase p=wlAyp wA$nTn that begins at position m in
the source sentence. Also, suppose that our phrase dic-
tionary provided the translation Washington State, with
internal word alignment a = (a1 = 2, a2 = 1) (i.e.,
a=(&lt;Washington,wA$nTn&gt;, &lt;State,wlAyp&gt;), then the
outbound phrase cost is defined as:
</bodyText>
<equation confidence="0.993387666666667">
Co(p, n, m, a) =Co(δ = (m − n)|fn)+
�t − 1 Co(δ = (ai+1 − ai) |fa,) (6)
i=1
</equation>
<bodyText confidence="0.999950583333333">
where l is the length of the target phrase, a is the
internal word alignment, fn is source word at position
n (in the sentence), and fa, is the source word that is
aligned to the i-th word in the target side of the phrase
(not the sentence).
The inbound and pair distortion costs (i..e,
Ci(p, n, m, a) and Cp(p, n, m, a)) can be defined
in a similar fashion.
The above distortion costs are used in conjunction
with other cost components used in our decoder. The
ultimate word order choice made is influenced by both
the language model cost as well as the distortion cost.
</bodyText>
<sectionHeader confidence="0.997644" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999278">
The phrase-based decoder we use is inspired by the de-
coder described in (Tillmann and Ney, 2003) and sim-
ilar to that described in (Koehn, 2004). It is a multi-
stack, multi-beam search decoder with n stacks (where
n is the length of the source sentence being decoded)
</bodyText>
<page confidence="0.993529">
532
</page>
<table confidence="0.999772333333333">
s 0 1 1 1 1 1 2 2 2 2
w 0 4 6 8 10 12 4 6 8 10
BLEUr1n4c 0.5617 0.6507 0.6443 0.6430 0.6461 0.6456 0.6831 0.6706 0.6609 0.6596
2 3 3 3 3 3 4 4 4 4 4
12 4 6 8 10 12 4 6 8 10 12
0.6626 0.6919 0.6751 0.6580 0.6505 0.6490 0.6851 0.6592 0.6317 0.6237 0.6081
</table>
<tableCaption confidence="0.997891">
Table 3: BLEU scores for the word order restoration task. The BLEU scores reported here are with 1 reference.
</tableCaption>
<bodyText confidence="0.987675318181818">
The input is the reordered English in the reference. The 95% Confidence u ranges from 0.011 to 0.016
and a beam associated with each stack as described
in (Al-Onaizan, 2005). The search is done in n time
steps. In time step i, only hypotheses that cover ex-
actly i source words are extended. The beam search
algorithm attempts to find the translation (i.e., hypoth-
esis that covers all source words) with the minimum
cost as in (Tillmann and Ney, 2003) and (Koehn, 2004)
. The distortion cost is added to the log-linear mixture
of the hypothesis extension in a fashion similar to the
language model cost.
A hypothesis covers a subset of the source words.
The final translation is a hypothesis that covers all
source words and has the minimum cost among all pos-
sible 9 hypotheses that cover all source words. A hy-
pothesis h is extended by matching the phrase dictio-
nary against source word sequences in the input sen-
tence that are not covered in h. The cost of the new
hypothesis C(hn w) = C(h) + C(e), where C(e) is
the cost of this extension. The main components of
the cost of extension e can be defined by the following
equation:
</bodyText>
<equation confidence="0.998207">
C(e) = A1CLM(e) + A2CTM(e) + A3CD(e)
</equation>
<bodyText confidence="0.998938421052632">
where CLM(e) is the language model cost, CTM (e)
is the translation model cost, and CD(e) is the distor-
tion cost. The extension cost depends on the hypothesis
being extended, the phrase being used in the extension,
and the source word positions being covered.
The word reorderings that are explored by the search
algorithm are controlled by two parameters s and w as
described in (Tillmann and Ney, 2003). The first pa-
rameter s denotes the number of source words that are
temporarily skipped (i.e., temporarily left uncovered)
during the search to cover a source word to the right of
the skipped words. The second parameter is the win-
dow width w, which is defined as the distance (in num-
ber of source words) between the left-most uncovered
source word and the right-most covered source word.
To illustrate these restrictions, let us assume the
input sentence consists of the following sequence
(f1, f2, f3, f4). For s=1 and w=2, the permissi-
ble permutations are (f1,f2, f3, f4), (f2, f1, f3, f4),
</bodyText>
<footnote confidence="0.9496648">
9Exploring all possible hypothesis with all possible word
permutations is computationally intractable. Therefore, the
search algorithm gives an approximation to the optimal so-
lution. All possible hypotheses refers to all hypotheses that
were explored by the decoder.
</footnote>
<equation confidence="0.7761725">
(f2, f3, f1, f4), (f1, f3, f2, f4),(f1, f3, f4, f2), and
(f1, f2, f4, f3).
</equation>
<subsectionHeader confidence="0.948176">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99998125">
The experiments reported in this section are in the con-
text of SMT from Arabic into English. The training
data is a 500K sentence-pairs subsample of the 2005
Large Track Arabic-English Data for NIST MT Evalu-
ation.
The language model used is an interpolated trigram
model described in (Bahl et al., 1983). The language
model is trained on the LDC English GigaWord Cor-
pus.
The test set used in the experiments in this section
is the 2003 NIST MT Evaluation test set (which is not
part of the training data).
</bodyText>
<subsectionHeader confidence="0.999973">
5.2 Reordering with Perfect Translations
</subsectionHeader>
<bodyText confidence="0.99914772">
In the experiments in this section, we show the util-
ity of a trigram language model in restoring the correct
word order for English. The task is a simplified transla-
tion task, where the input is reordered English (English
written in Arabic word order) and the output is English
in the correct order. The source sentence is a reordered
English sentence in the same manner we described in
Section 3. The objective of the decoder is to recover
the correct English order.
We use the same phrase-based decoder we use for
our SMT experiments, except that only the language
model cost is used here. Also, the phrase dictionary
used is a one-to-one function that maps every English
word in our vocabulary to itself. The language model
we use for the experiments reported here is the same
as the one used for other experiments reported in this
paper.
The results in Table 3 illustrate how the language
model performs reasonably well for local reorderings
(e.g., for s = 3 and w = 4), but its perfromance de-
teriorates as we relax the reordering restrictions by in-
creasing the reordering window size (w).
Table 4 shows some examples of original English,
English in Arabic order, and the decoder output for two
different sets of reordering parameters.
</bodyText>
<subsectionHeader confidence="0.991485">
5.3 SMT Experiments
</subsectionHeader>
<bodyText confidence="0.997906">
The phrases in the phrase dictionary we use in
the experiments reported here are a combination
</bodyText>
<page confidence="0.997526">
533
</page>
<table confidence="0.993167666666667">
Eng Ar Opposition Iraqi Prepares for Meeting mid - January in Kurdistan
Orig. Eng. Iraqi Opposition Prepares for mid - January Meeting in Kurdistan
Output1 Iraqi Opposition Meeting Prepares for mid - January in Kurdistan
Output2 Opposition Meeting Prepares for Iraqi Kurdistan in mid - January
Eng Ar Head of Congress National Iraqi Visits Kurdistan Iraqi
Orig. Eng. Head of Iraqi National Congress Visits Iraqi Kurdistan
Output1 Head of Iraqi National Congress Visits Iraqi Kurdistan
Output2 Head Visits Iraqi National Congress of Iraqi Kurdistan
Eng Ar House White Confirms Presence of Tape New Bin Laden
Orig. Eng. White House Confirms Presence of New Bin Laden Tape
Output1 White House Confirms Presence of Bin Laden Tape New
Output2 White House of Bin Laden Tape Confirms Presence New
</table>
<tableCaption confidence="0.68538875">
Table 4: Examples of reordering with perfect translations. The examples show English in Arabic order (Eng Ar.),
English in its original order (Orig. Eng.) and decoding with two different parameter settings. Output1 is decoding
with (s=3,w=4). Output2 is decoding with (s=4,w=12). The sentence lengths of the examples presented here are
much shorter than the average in our test set (— 28.5).
</tableCaption>
<table confidence="0.9997634">
s w Distortion Used? BLEUr4n4c
0 0 NO 0.4468
1 8 NO 0.4346
1 8 YES 0.4715
2 8 NO 0.4309
2 8 YES 0.4775
3 8 NO 0.4283
3 8 YES 0.4792
4 8 NO 0.4104
4 8 YES 0.4782
</table>
<tableCaption confidence="0.784046">
Table 5: BLEU scores for the Arabic-English machine translation task. The 95% Confidence u ranges from 0.0158
to 0.0176. s is the number of words temporarily skipped, and w is the word permutation window size.
</tableCaption>
<bodyText confidence="0.999812514285714">
of phrases automatically extracted from maximum-
posterior alignments and maximum entropy align-
ments. Only phrases that conform to the so-called con-
sistent alignment restrictions (Och et al., 1999) are ex-
tracted.
Table 5 shows BLEU scores for our SMT decoder
with different parameter settings for skip s, window
width w, with and without our distortion model. The
BLEU scores reported in this table are based on 4 refer-
ence translations. The language model, phrase dictio-
nary, and other decoder tuning parameters remain the
same in all experiments reported in this table.
Table 5 clearly shows that as we open the search and
consider wider range of word reorderings, the BLEU
score decreases in the absence of our distortion model
when we rely solely on the language model. Wrong
reorderings look attractive to the decoder via the lan-
guage model which suggests that we need a richer
model with more parameter. In the absence of richer
models such as the proposed distortion model, our re-
sults suggest that it is best to decode monotonically and
only allow local reorderings that are captured in our
phrase dictionary.
However, when the distortion model is used, we see
statistically significant increases in the BLEU score as
we consider more word reorderings. The best BLEU
score achieved when using the distortion model is
0.4792 , compared to a best BLEU score of 0.4468
when the distortion model is not used.
Our results on the 2004 and 2005 NIST MT Evalua-
tion test sets using the distortion model are 0.4497 and
0.464610, respectively.
Table 6 shows some Arabic-English translation ex-
amples using our decoder with and without the distor-
tion model.
</bodyText>
<sectionHeader confidence="0.997621" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9996296">
We presented a new distortion model that can be in-
tegrated with existing phrase-based SMT decoders.
The proposed model shows statistically significant im-
provement over a state-of-the-art phrase-based SMT
decoder. We also showed that n-gram language mod-
</bodyText>
<footnote confidence="0.929321333333333">
10The MT05 BLEU score is the from the official NIST
evaluation. The MT04 BLEU score is only our second run
on MT04.
</footnote>
<page confidence="0.987257">
534
</page>
<table confidence="0.95361565">
Input (Ar) kwryA Al$mAlyp mstEdp llsmAH lwA$nTn bAltHqq mn AnhA lA tSnE AslHp nwwyp
Ref. (En) North Korea Prepared to allow Washington to check it is not Manufacturing Nuclear
Weapons
Out1 North Korea to Verify Washington That It Was Not Prepared to Make Nuclear Weapons
Out2 North Korea Is Willing to Allow Washington to Verify It Does Not Make Nuclear Weapons
Input (Ar) wAkd AldblwmAsy An ”AnsHAb (kwryA Al$mAlyp mn AlmEAhdp) ybd&gt; AEtbArA mn
Alywm”.
Ref. (En) The diplomat confirmed that ”North Korea’s withdrawal from the treaty starts as of today.”
Out1 The diplomat said that ” the withdrawal of the Treaty (start) North Korea as of today. ”
Out2 The diplomat said that the ” withdrawal of (North Korea of the treaty) will start as of
today ”.
Input (Ar) snrfE *lk AmAm Almjls Aldstwry”.
Ref. (En) We will bring this before the Constitutional Assembly.”
Out1 The Constitutional Council to lift it. ”
Out2 This lift before the Constitutional Council ”.
Input (Ar) wAkd AlbrAdEy An mjls AlAmn ”ytfhm” An 27 kAnwn AlvAny/ynAyr lys mhlp nhA}yp.
Ref. (En) Baradei stressed that the Security Council ”appreciates” that January 27 is not a final
ultimatum.
Out1 Elbaradei said that the Security Council ” understand ” that is not a final period January 27.
Out2 Elbaradei said that the Security Council ” understand ” that 27 January is not a final period.
</table>
<tableCaption confidence="0.948903">
Table 6: Selected examples of our Arabic-English SMT output. The English is one of the human reference trans-
</tableCaption>
<bodyText confidence="0.993084545454545">
lations. Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEU
score. Output 2 is decoding with the distortion model and (s=3, w=8), which corresponds to 0.4792 BLEU score.
The sentences presented here are much shorter than the average in our test set. The average length of the arabic
sentence in the MT03 test set is — 24.7.
els are not sufficient to model word movement in trans-
lation. Our proposed distortion model addresses this
weakness of the n-gram language model.
We also propose a novel metric to measure word or-
der similarity (or differences) between any pair of lan-
guages based on word alignments. Our metric shows
that Chinese-English have a closer word order than
Arabic-English.
Our proposed distortion model relies solely on word
alignments and is conditioned on the source words.
The majority of word movement in translation is
mainly due to syntactic differences between the source
and target language. For example, Arabic is verb-initial
for the most part. So, when translating into English,
one needs to move the verb after the subject, which is
often a long compounded phrase. Therefore, we would
like to incorporate syntactic or part-of-speech informa-
tion in our distortion model.
</bodyText>
<sectionHeader confidence="0.959257" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9992302">
This work was partially supported by DARPA GALE
program under contract number HR0011-06-2-0001. It
was also partially supported by DARPA TIDES pro-
gram monitored by SPAWAR under contract number
N66001-99-2-8916.
</bodyText>
<sectionHeader confidence="0.998041" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997034518518519">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-
Josef Och, David Purdy, Noah Smith, and David
Yarowsky. 1999. Statistical Machine Translation:
Final Report, Johns Hopkins University Summer
Workshop (WS 99) on Language Engineering, Cen-
ter for Language and Speech Processing, Baltimore,
MD.
Yaser Al-Onaizan. 2005. IBM Arabic-to-English MT
Submission. Presentation given at DARPA/TIDES
NIST MT Evaluation workshop.
Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.
1983. A Maximum Likelihood Approach to Con-
tinuous Speech Recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, PAMI-
5(2):179–190.
Adam L. Berger, Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, Andrew S. Kehler,
and Robert L. Mercer. 1996. Language Transla-
tion Apparatus and Method of Using Context-Based
Translation Models. United States Patent, Patent
Number 5510981, April.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Frederick Jelinek, John D
Lafferty, Robert L Mercer, and Paul S Roossin.
1990. A Statistical Approach to Machine Transla-
tion. Computational Linguistics, 16(2):79–85.
</reference>
<page confidence="0.982341">
535
</page>
<reference confidence="0.999883472527472">
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2):263–311.
Niyu Ge. 2004. Improvements in Word Alignments.
Presentation given at DARPA/TIDES NIST MT Eval-
uation workshop.
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational
Linguistics, 25(4):607–615.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Marti Hearst
and Mari Ostendorf, editors, HLT-NAACL 2003:
Main Proceedings, pages 127–133, Edmonton, Al-
berta, Canada, May 27 – June 1. Association for
Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Trans-
lation Models. In Proceedings of the 6th Con-
ference of the Association for Machine Translation
in the Americas, pages 115–124, Washington DC,
September-October. The Association for Machine
Translation in the Americas (AMTA).
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved Alignment Models for Statis-
tical Machine Translation. In Joint Conf. of Empir-
ical Methods in Natural Language Processing and
Very Large Corpora, pages 20–28, College Park,
Maryland.
Franz Josef Och, Daniel Gildea, Sanjeev Khudan-
pur, Anoop Sarkar, Kenji Yamada, Alex Fraser,
Shankar Kumar, Libin Shen, David Smith, Kather-
ine Eng, Viren Jain, Zhen Jin, and Dragomir Radev.
2004. A Smorgasbord of Features for Statistical
Machine Translation. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Main Proceedings, pages 161–168, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of machine translation. In 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 02), pages 311–318, Philadelphia, PA,
July.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101–
104, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Christoph Tillmann and Hermann Ney. 2003. Word
Re-ordering and a DP Beam Search Algorithm for
Statistical Machine Translation. Computational Lin-
guistics, 29(1):97–133.
Christoph Tillmann, Stephan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-Based Search Using
Monotone Alignments in Statistical Translation. In
Proceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics and 8th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 289–296,
Madrid. Association for Computational Linguistics.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-BasedWord Alignment in Statisti-
cal Machine Translation. In Proc. of the 16th
Int. Conf. on Computational Linguistics (COLING
1996), pages 836–841, Copenhagen, Denmark, Au-
gust.
Dekai Wu. 1996. A Polynomial-Time Algorithm for
Statistical Machine Translation. In Proc. of the 34th
Annual Conf. of the Association for Computational
Linguistics (ACL 96), pages 152–158, Santa Cruz,
CA, June.
Fei Xia and Michael McCord. 2004. Improving a
Statistical MT System with Automatically Learned
Rewrite Patterns. In Proc. of the 20th International
Conference on Computational Linguistics (COLING
2004), Geneva, Switzerland.
Kenji Yamada and Kevin Knight. 2002. A Decoder for
Syntax-based Statistical MT. In Proc. of the 40th
Annual Conf. of the Association for Computational
Linguistics (ACL 02), pages 303–310, Philadelphia,
PA, July.
Richard Zens and Hermann Ney. 2003. A Compar-
ative Study on Reordering Constraints in Statistical
Machine Translation. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meet-
ing ofthe Associationfor Computational Linguistics,
pages 144–151, Sapporo, Japan.
</reference>
<page confidence="0.998537">
536
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972346">
<title confidence="0.9999">Distortion Models For Statistical Machine Translation</title>
<author confidence="0.993108">Yaser Al-Onaizan</author>
<author confidence="0.993108">Kishore Papineni</author>
<affiliation confidence="0.999953">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.9942285">1101 Kitchawan Road Yorktown Heights, NY 10598, USA</address>
<abstract confidence="0.999319071428571">In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation. We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations. We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used. We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>FranzJosef Och</author>
<author>David Purdy</author>
<author>Noah Smith</author>
<author>David Yarowsky</author>
</authors>
<date>1999</date>
<booktitle>Statistical Machine Translation: Final Report, Johns Hopkins University Summer Workshop (WS 99) on Language Engineering, Center for Language and Speech Processing,</booktitle>
<location>Baltimore, MD.</location>
<contexts>
<context position="14415" citStr="Al-Onaizan et al., 1999" startWordPosition="2295" endWordPosition="2298">are proposing consists of three components: outbound, inbound, and pair distortion. Intuitively our distortion models attempt to capture the order in which source words need to be translated. For instance, the outbound distortion component attempts to capture what is typically translated immediately after the word that has just been translated. Do we tend to translate words that precede it or succeed it? Which word position to translate next? Our distortion parameters are directly estimated from word alignments by simple counting over alignment links in the training data. Any aligner such as (Al-Onaizan et al., 1999) or (Vogel et al., 1996) can be used to obtain word alignments. For the results reported in this paper word alignments were obtained using a maximum-posterior word aligner4 described in (Ge, 2004). We will illustrate the components of our model with a partial word alignment. Let us assume that our source sentences is (f10, f250 , f30 0 )6, and our target sentence is (e410, e20), and their word alignment is a = ((f10, e410), (f300, e20)). Word Alignment a can 4We also estimated distortion parameters using a Maximum Entropy aligner and the differences were negligible. 5In practice, we add specia</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, FranzJosef Och, David Purdy, Noah Smith, and David Yarowsky. 1999. Statistical Machine Translation: Final Report, Johns Hopkins University Summer Workshop (WS 99) on Language Engineering, Center for Language and Speech Processing, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
</authors>
<date>2005</date>
<journal>IBM Arabic-to-English MT Submission. Presentation given at DARPA/TIDES NIST MT Evaluation workshop.</journal>
<contexts>
<context position="19429" citStr="Al-Onaizan, 2005" startWordPosition="3187" endWordPosition="3188">r with n stacks (where n is the length of the source sentence being decoded) 532 s 0 1 1 1 1 1 2 2 2 2 w 0 4 6 8 10 12 4 6 8 10 BLEUr1n4c 0.5617 0.6507 0.6443 0.6430 0.6461 0.6456 0.6831 0.6706 0.6609 0.6596 2 3 3 3 3 3 4 4 4 4 4 12 4 6 8 10 12 4 6 8 10 12 0.6626 0.6919 0.6751 0.6580 0.6505 0.6490 0.6851 0.6592 0.6317 0.6237 0.6081 Table 3: BLEU scores for the word order restoration task. The BLEU scores reported here are with 1 reference. The input is the reordered English in the reference. The 95% Confidence u ranges from 0.011 to 0.016 and a beam associated with each stack as described in (Al-Onaizan, 2005). The search is done in n time steps. In time step i, only hypotheses that cover exactly i source words are extended. The beam search algorithm attempts to find the translation (i.e., hypothesis that covers all source words) with the minimum cost as in (Tillmann and Ney, 2003) and (Koehn, 2004) . The distortion cost is added to the log-linear mixture of the hypothesis extension in a fashion similar to the language model cost. A hypothesis covers a subset of the source words. The final translation is a hypothesis that covers all source words and has the minimum cost among all possible 9 hypothe</context>
</contexts>
<marker>Al-Onaizan, 2005</marker>
<rawString>Yaser Al-Onaizan. 2005. IBM Arabic-to-English MT Submission. Presentation given at DARPA/TIDES NIST MT Evaluation workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A Maximum Likelihood Approach to Continuous Speech Recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="1414" citStr="Bahl et al., 1983" startWordPosition="224" endWordPosition="227">language model is a statistical model that gives a probability distribution over possible sequences of words. It computes the probability of producing a given word w1 given all the words that precede it in the sentence. An n-gram language model is an n-th order Markov model where the probability of generating a given word depends only on the last n − 1 words immediately preceding it and is given by the following equation: P(wk1) = P(w1)P(w2|w1) ··· P(wn|wn�1 1 ) (1) where k &gt;= n. N-gram language models have been successfully used in Automatic Speech Recognition (ASR) as was first proposed by (Bahl et al., 1983). They play an important role in selecting among several candidate word realization of a given acoustic signal. N-gram language models have also been used in Statistical Machine Translation (SMT) as proposed by (Brown et al., 1990; Brown et al., 1993). The run-time search procedure used to find the most likely translation (or transcription in the case of Speech Recognition) is typically referred to as decoding. There is a fundamental difference between decoding for machine translation and decoding for speech recognition. When decoding a speech signal, words are generated in the same order in w</context>
<context position="22089" citStr="Bahl et al., 1983" startWordPosition="3646" endWordPosition="3649">le word permutations is computationally intractable. Therefore, the search algorithm gives an approximation to the optimal solution. All possible hypotheses refers to all hypotheses that were explored by the decoder. (f2, f3, f1, f4), (f1, f3, f2, f4),(f1, f3, f4, f2), and (f1, f2, f4, f3). 5.1 Experimental Setup The experiments reported in this section are in the context of SMT from Arabic into English. The training data is a 500K sentence-pairs subsample of the 2005 Large Track Arabic-English Data for NIST MT Evaluation. The language model used is an interpolated trigram model described in (Bahl et al., 1983). The language model is trained on the LDC English GigaWord Corpus. The test set used in the experiments in this section is the 2003 NIST MT Evaluation test set (which is not part of the training data). 5.2 Reordering with Perfect Translations In the experiments in this section, we show the utility of a trigram language model in restoring the correct word order for English. The task is a simplified translation task, where the input is reordered English (English written in Arabic word order) and the output is English in the correct order. The source sentence is a reordered English sentence in t</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A Maximum Likelihood Approach to Continuous Speech Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI5(2):179–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Andrew S Kehler</author>
<author>Robert L Mercer</author>
</authors>
<title>Language Translation Apparatus and Method of Using Context-Based Translation Models. United States Patent, Patent Number 5510981,</title>
<date>1996</date>
<contexts>
<context position="4626" citStr="Berger et al., 1996" startWordPosition="726" endWordPosition="729">omputational Linguistics and 44th Annual Meeting of the ACL, pages 529–536, Sydney, July 2006. c�2006 Association for Computational Linguistics word reorderings during the translation process. Trying all possible word reordering is an NP-Complete problem as shown in (Knight, 1999), which makes searching for the optimal solution among all possible permutations computationally intractable. Therefore, SMT decoders typically limit the number of permutations considered for efficiency reasons by placing reordering restrictions. Reordering restrictions for word-based SMT decoders were introduced by (Berger et al., 1996) and (Wu, 1996). (Berger et al., 1996) allow only reordering of at most n words at any given time. (Wu, 1996) propose using contiguity restrictions on the reordering. For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). A different approach to allow for a limited reordering is to reorder the input sentence such that the source and the target sentences have similar word order and then proceed to monotonically decode the reordered source sentence. Monotone decoding translates words in the same order they appear in the source language. Hence, the input a</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Kehler, Mercer, 1996</marker>
<rawString>Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Andrew S. Kehler, and Robert L. Mercer. 1996. Language Translation Apparatus and Method of Using Context-Based Translation Models. United States Patent, Patent Number 5510981, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1644" citStr="Brown et al., 1990" startWordPosition="263" endWordPosition="266">language model is an n-th order Markov model where the probability of generating a given word depends only on the last n − 1 words immediately preceding it and is given by the following equation: P(wk1) = P(w1)P(w2|w1) ··· P(wn|wn�1 1 ) (1) where k &gt;= n. N-gram language models have been successfully used in Automatic Speech Recognition (ASR) as was first proposed by (Bahl et al., 1983). They play an important role in selecting among several candidate word realization of a given acoustic signal. N-gram language models have also been used in Statistical Machine Translation (SMT) as proposed by (Brown et al., 1990; Brown et al., 1993). The run-time search procedure used to find the most likely translation (or transcription in the case of Speech Recognition) is typically referred to as decoding. There is a fundamental difference between decoding for machine translation and decoding for speech recognition. When decoding a speech signal, words are generated in the same order in which their corresponding acoustic signal is consumed. However, that is not necessarily the case in MT due to the fact that different languages have different word order requirements. For example, in Spanish and Arabic adjectives a</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, John D Lafferty, Robert L Mercer, and Paul S Roossin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1665" citStr="Brown et al., 1993" startWordPosition="267" endWordPosition="270"> n-th order Markov model where the probability of generating a given word depends only on the last n − 1 words immediately preceding it and is given by the following equation: P(wk1) = P(w1)P(w2|w1) ··· P(wn|wn�1 1 ) (1) where k &gt;= n. N-gram language models have been successfully used in Automatic Speech Recognition (ASR) as was first proposed by (Bahl et al., 1983). They play an important role in selecting among several candidate word realization of a given acoustic signal. N-gram language models have also been used in Statistical Machine Translation (SMT) as proposed by (Brown et al., 1990; Brown et al., 1993). The run-time search procedure used to find the most likely translation (or transcription in the case of Speech Recognition) is typically referred to as decoding. There is a fundamental difference between decoding for machine translation and decoding for speech recognition. When decoding a speech signal, words are generated in the same order in which their corresponding acoustic signal is consumed. However, that is not necessarily the case in MT due to the fact that different languages have different word order requirements. For example, in Spanish and Arabic adjectives are mainly noun post-m</context>
<context position="8807" citStr="Brown et al., 1993" startWordPosition="1405" endWordPosition="1408">d IBM4-like decoder. However, their decoder is outperformed by phrase-based decoders such as (Koehn, 2004), (Och et al., 1999), and (Tillmann and Ney, 2003) . Phrase-based SMT decoders mostly rely on the language model to select among possible word order choices. However, in our experiments we show that the language model is not reliable enough to make the choices that lead to a better MT quality. This observation is also reported by (Xia and McCord, 2004).We argue that the distortion model we propose leads to a better translation as measured by BLEU. Distortion models were first proposed by (Brown et al., 1993) in the so-called IBM Models. IBM Models 2 and 3 define the distortion parameters in terms of the word positions in the sentence pair, not the actual words at those positions. Distortion probability is also conditioned on the source and target sentence lengths. These models do not generalize well since their parameters are tied to absolute word position within sentences which tend to be different for the same words across sentences. IBM Models 4 and 5 alleviate this limitation by replacing absolute word positions with relative positions. The latter models define the distortion parameters for a</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
</authors>
<title>Improvements in Word Alignments. Presentation given at DARPA/TIDES NIST MT Evaluation workshop.</title>
<date>2004</date>
<contexts>
<context position="14611" citStr="Ge, 2004" startWordPosition="2329" endWordPosition="2330">utbound distortion component attempts to capture what is typically translated immediately after the word that has just been translated. Do we tend to translate words that precede it or succeed it? Which word position to translate next? Our distortion parameters are directly estimated from word alignments by simple counting over alignment links in the training data. Any aligner such as (Al-Onaizan et al., 1999) or (Vogel et al., 1996) can be used to obtain word alignments. For the results reported in this paper word alignments were obtained using a maximum-posterior word aligner4 described in (Ge, 2004). We will illustrate the components of our model with a partial word alignment. Let us assume that our source sentences is (f10, f250 , f30 0 )6, and our target sentence is (e410, e20), and their word alignment is a = ((f10, e410), (f300, e20)). Word Alignment a can 4We also estimated distortion parameters using a Maximum Entropy aligner and the differences were negligible. 5In practice, we add special symbols at the start and end of the source and target sentences, we also assume that the start symbols in the source and target are aligned, and similarly for the end symbols. Those special symb</context>
</contexts>
<marker>Ge, 2004</marker>
<rawString>Niyu Ge. 2004. Improvements in Word Alignments. Presentation given at DARPA/TIDES NIST MT Evaluation workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding Complexity in WordReplacement Translation Models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="4287" citStr="Knight, 1999" startWordPosition="682" endWordPosition="683">tatistical machine translation systems. Then, we conclude this paper with a discussion in Section 6. 2 Related Work Different languages have different word order requirements. SMT decoders attempt to generate translations in the proper word order by attempting many possible 529 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 529–536, Sydney, July 2006. c�2006 Association for Computational Linguistics word reorderings during the translation process. Trying all possible word reordering is an NP-Complete problem as shown in (Knight, 1999), which makes searching for the optimal solution among all possible permutations computationally intractable. Therefore, SMT decoders typically limit the number of permutations considered for efficiency reasons by placing reordering restrictions. Reordering restrictions for word-based SMT decoders were introduced by (Berger et al., 1996) and (Wu, 1996). (Berger et al., 1996) allow only reordering of at most n words at any given time. (Wu, 1996) propose using contiguity restrictions on the reordering. For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003)</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding Complexity in WordReplacement Translation Models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>HLT-NAACL 2003: Main Proceedings,</booktitle>
<volume>27</volume>
<pages>127--133</pages>
<editor>In Marti Hearst and Mari Ostendorf, editors,</editor>
<location>Edmonton, Alberta, Canada,</location>
<contexts>
<context position="9782" citStr="Koehn et al., 2003" startWordPosition="1567" endWordPosition="1570">within sentences which tend to be different for the same words across sentences. IBM Models 4 and 5 alleviate this limitation by replacing absolute word positions with relative positions. The latter models define the distortion parameters for a cept (one or more words). This models phrasal movement better since words tend to move in blocks and not independently. The distortion is conditioned on classes of the aligned source and target words. The entire source and target vocabularies are reduced to a small number of classes (e.g., 50) for the purpose of estimating those parameters. Similarly, (Koehn et al., 2003) propose a relative distortion model to be used with a phrase decoder. The model is defined in terms of the difference between the position of the current phrase and the position of the previous phrase in the source sentence. It does not con2Arabic text appears throughout this paper in Tim Buckwalter’s Romanization. 530 Arabic Ezp1 AbrAhym2 ystqbl3 ms&amp;wlA4 AqtSAdyA5 sEwdyA6 fy7 bgdAd8 English Izzet1 Ibrahim2 Meets3 Saudi4 Trade5 official6 in7 Baghdad8 Word Alignment (Ezp1,Izzet1) (AbrAhym2,Ibrahim2) (ystqbl3,Meets3) ( ms&amp;wlA4,official6) (AqtSAdyA5,Trade5) (sEwdyA6,Saudi4) (fy7,in7) (bgdAd8,Bag</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Marti Hearst and Mari Ostendorf, editors, HLT-NAACL 2003: Main Proceedings, pages 127–133, Edmonton, Alberta, Canada, May 27 – June 1. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>115--124</pages>
<location>Washington</location>
<contexts>
<context position="8294" citStr="Koehn, 2004" startWordPosition="1319" endWordPosition="1320">iction in their phrase-based decoder by allowing a restricted set of word reorderings. For their translation task, word reordering is done only for words belonging to the verb group. The context in which they report their results is a Speech-to-Speech translation from German to English. (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. They reported results that are better than word-based IBM4-like decoder. However, their decoder is outperformed by phrase-based decoders such as (Koehn, 2004), (Och et al., 1999), and (Tillmann and Ney, 2003) . Phrase-based SMT decoders mostly rely on the language model to select among possible word order choices. However, in our experiments we show that the language model is not reliable enough to make the choices that lead to a better MT quality. This observation is also reported by (Xia and McCord, 2004).We argue that the distortion model we propose leads to a better translation as measured by BLEU. Distortion models were first proposed by (Brown et al., 1993) in the so-called IBM Models. IBM Models 2 and 3 define the distortion parameters in te</context>
<context position="18766" citStr="Koehn, 2004" startWordPosition="3049" endWordPosition="3050">), and fa, is the source word that is aligned to the i-th word in the target side of the phrase (not the sentence). The inbound and pair distortion costs (i..e, Ci(p, n, m, a) and Cp(p, n, m, a)) can be defined in a similar fashion. The above distortion costs are used in conjunction with other cost components used in our decoder. The ultimate word order choice made is influenced by both the language model cost as well as the distortion cost. 5 Experimental Results The phrase-based decoder we use is inspired by the decoder described in (Tillmann and Ney, 2003) and similar to that described in (Koehn, 2004). It is a multistack, multi-beam search decoder with n stacks (where n is the length of the source sentence being decoded) 532 s 0 1 1 1 1 1 2 2 2 2 w 0 4 6 8 10 12 4 6 8 10 BLEUr1n4c 0.5617 0.6507 0.6443 0.6430 0.6461 0.6456 0.6831 0.6706 0.6609 0.6596 2 3 3 3 3 3 4 4 4 4 4 12 4 6 8 10 12 4 6 8 10 12 0.6626 0.6919 0.6751 0.6580 0.6505 0.6490 0.6851 0.6592 0.6317 0.6237 0.6081 Table 3: BLEU scores for the word order restoration task. The BLEU scores reported here are with 1 reference. The input is the reordered English in the reference. The 95% Confidence u ranges from 0.011 to 0.016 and a bea</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas, pages 115–124, Washington DC, September-October. The Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Alignment Models for Statistical Machine Translation.</title>
<date>1999</date>
<booktitle>In Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="7628" citStr="Och et al., 1999" startWordPosition="1217" endWordPosition="1220"> the Ara1Also referred to in the literature as the set of blocks or clumps. bic phrase AlwlAyAt AlmtHdp2 correctly into English as the United States if it was seen in its training data, was aligned correctly, and was added to the phrase dictionary. However, if the phrase Almmlkp AlmtHdp is not in the phrase dictionary, it will not be translated correctly by a monotone phrase decoder even if the individual units of the phrase Almmlkp and AlmtHdp, and their translations (Kingdom and United, respectively) are in the phrase dictionary since that would require swapping the order of the two words. (Och et al., 1999; Tillmann and Ney, 2003) relax the monotonicity restriction in their phrase-based decoder by allowing a restricted set of word reorderings. For their translation task, word reordering is done only for words belonging to the verb group. The context in which they report their results is a Speech-to-Speech translation from German to English. (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. They reported results that are better than word-based IBM4-like decoder. However, their deco</context>
<context position="25440" citStr="Och et al., 1999" startWordPosition="4214" endWordPosition="4217">shorter than the average in our test set (— 28.5). s w Distortion Used? BLEUr4n4c 0 0 NO 0.4468 1 8 NO 0.4346 1 8 YES 0.4715 2 8 NO 0.4309 2 8 YES 0.4775 3 8 NO 0.4283 3 8 YES 0.4792 4 8 NO 0.4104 4 8 YES 0.4782 Table 5: BLEU scores for the Arabic-English machine translation task. The 95% Confidence u ranges from 0.0158 to 0.0176. s is the number of words temporarily skipped, and w is the word permutation window size. of phrases automatically extracted from maximumposterior alignments and maximum entropy alignments. Only phrases that conform to the so-called consistent alignment restrictions (Och et al., 1999) are extracted. Table 5 shows BLEU scores for our SMT decoder with different parameter settings for skip s, window width w, with and without our distortion model. The BLEU scores reported in this table are based on 4 reference translations. The language model, phrase dictionary, and other decoder tuning parameters remain the same in all experiments reported in this table. Table 5 clearly shows that as we open the search and consider wider range of word reorderings, the BLEU score decreases in the absence of our distortion model when we rely solely on the language model. Wrong reorderings look </context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved Alignment Models for Statistical Machine Translation. In Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28, College Park, Maryland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date></date>
<booktitle>and Dragomir Radev. 2004. A Smorgasbord of Features for Statistical Machine Translation. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>161--168</pages>
<editor>Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,</editor>
<location>Libin</location>
<marker>Och, Gildea, </marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A Smorgasbord of Features for Statistical Machine Translation. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 161–168, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics (ACL 02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="3044" citStr="Papineni et al., 2002" startWordPosition="484" endWordPosition="487">sting statistical machine translation decoders have mostly relied on language models to select the proper word order among many possible choices when translating between two languages. In this paper, we argue that a language model is not sufficient to adequately address this issue, especially when translating between languages that have very different word orders as suggested by our experimental results in Section 5. We propose a new distortion model that can be used as an additional component in SMT decoders. This new model leads to significant improvements in MT quality as measured by BLEU (Papineni et al., 2002). The experimental results we report in this paper are for Arabic-English machine translation of news stories. We also present a novel method for measuring word order similarity (or differences) between any given pair of languages based on word alignments as described in Section 3. The rest of this paper is organized as follows. Section 2 presents a review of related work. In Section 3 we propose a method for measuring the distortion between any given pair of languages. In Section 4, we present our proposed distortion model. In Section 5, we present some empirical results that show the utility</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of machine translation. In 40th Annual Meeting of the Association for Computational Linguistics (ACL 02), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Short Papers,</booktitle>
<volume>2</volume>
<pages>101--104</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="11301" citStr="Tillman, 2004" startWordPosition="1800" endWordPosition="1801">riginal English order. sider the words in those positions. The distortion model we propose assigns a probability distribution over possible relative jumps conditioned on source words. Conditioning on the source words allows for a much more fine-grained model. For instance, words that tend to act as modifers (e.g., adjectives) would have a different distribution than verbs or nouns. Our model’s parameters are directly estimated from word alignments as we will further explain in Section 4. We will also show how to generalize this word distortion model to a phrase-based model. (Och et al., 2004; Tillman, 2004) propose orientation-based distortion models lexicalized on the phrase level. There are two important distinctions between their models and ours. First, they lexicalize their model on the phrases, which have many more parameters and hence would require much more data to estimate reliably. Second, their models consider only the direction (i.e., orientation) and not the relative jump. We are not aware of any work on measuring word order differences between a given language pair in the context of statistical machine translation. 3 Measuring Word Order Similarity Between Two Language In this secti</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers, pages 101– 104, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word Re-ordering and a DP Beam Search Algorithm for Statistical Machine Translation.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7653" citStr="Tillmann and Ney, 2003" startWordPosition="1221" endWordPosition="1224">rred to in the literature as the set of blocks or clumps. bic phrase AlwlAyAt AlmtHdp2 correctly into English as the United States if it was seen in its training data, was aligned correctly, and was added to the phrase dictionary. However, if the phrase Almmlkp AlmtHdp is not in the phrase dictionary, it will not be translated correctly by a monotone phrase decoder even if the individual units of the phrase Almmlkp and AlmtHdp, and their translations (Kingdom and United, respectively) are in the phrase dictionary since that would require swapping the order of the two words. (Och et al., 1999; Tillmann and Ney, 2003) relax the monotonicity restriction in their phrase-based decoder by allowing a restricted set of word reorderings. For their translation task, word reordering is done only for words belonging to the verb group. The context in which they report their results is a Speech-to-Speech translation from German to English. (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. They reported results that are better than word-based IBM4-like decoder. However, their decoder is outperformed by ph</context>
<context position="18719" citStr="Tillmann and Ney, 2003" startWordPosition="3038" endWordPosition="3041">lignment, fn is source word at position n (in the sentence), and fa, is the source word that is aligned to the i-th word in the target side of the phrase (not the sentence). The inbound and pair distortion costs (i..e, Ci(p, n, m, a) and Cp(p, n, m, a)) can be defined in a similar fashion. The above distortion costs are used in conjunction with other cost components used in our decoder. The ultimate word order choice made is influenced by both the language model cost as well as the distortion cost. 5 Experimental Results The phrase-based decoder we use is inspired by the decoder described in (Tillmann and Ney, 2003) and similar to that described in (Koehn, 2004). It is a multistack, multi-beam search decoder with n stacks (where n is the length of the source sentence being decoded) 532 s 0 1 1 1 1 1 2 2 2 2 w 0 4 6 8 10 12 4 6 8 10 BLEUr1n4c 0.5617 0.6507 0.6443 0.6430 0.6461 0.6456 0.6831 0.6706 0.6609 0.6596 2 3 3 3 3 3 4 4 4 4 4 12 4 6 8 10 12 4 6 8 10 12 0.6626 0.6919 0.6751 0.6580 0.6505 0.6490 0.6851 0.6592 0.6317 0.6237 0.6081 Table 3: BLEU scores for the word order restoration task. The BLEU scores reported here are with 1 reference. The input is the reordered English in the reference. The 95% Co</context>
<context position="20828" citStr="Tillmann and Ney, 2003" startWordPosition="3436" endWordPosition="3439"> cost of the new hypothesis C(hn w) = C(h) + C(e), where C(e) is the cost of this extension. The main components of the cost of extension e can be defined by the following equation: C(e) = A1CLM(e) + A2CTM(e) + A3CD(e) where CLM(e) is the language model cost, CTM (e) is the translation model cost, and CD(e) is the distortion cost. The extension cost depends on the hypothesis being extended, the phrase being used in the extension, and the source word positions being covered. The word reorderings that are explored by the search algorithm are controlled by two parameters s and w as described in (Tillmann and Ney, 2003). The first parameter s denotes the number of source words that are temporarily skipped (i.e., temporarily left uncovered) during the search to cover a source word to the right of the skipped words. The second parameter is the window width w, which is defined as the distance (in number of source words) between the left-most uncovered source word and the right-most covered source word. To illustrate these restrictions, let us assume the input sentence consists of the following sequence (f1, f2, f3, f4). For s=1 and w=2, the permissible permutations are (f1,f2, f3, f4), (f2, f1, f3, f4), 9Explor</context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2003. Word Re-ordering and a DP Beam Search Algorithm for Statistical Machine Translation. Computational Linguistics, 29(1):97–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Alex Zubiaga</author>
</authors>
<title>A DP-Based Search Using Monotone Alignments in Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>289--296</pages>
<institution>Madrid. Association for Computational Linguistics.</institution>
<contexts>
<context position="5391" citStr="Tillmann et al., 1997" startWordPosition="857" endWordPosition="860">ions on the reordering. For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). A different approach to allow for a limited reordering is to reorder the input sentence such that the source and the target sentences have similar word order and then proceed to monotonically decode the reordered source sentence. Monotone decoding translates words in the same order they appear in the source language. Hence, the input and output sentences have the same word order. Monotone decoding is very efficient since the optimal decoding can be found in polynomial time. (Tillmann et al., 1997) proposed a DP-based monotone search algorithm for SMT. Their proposed solution to address the necessary word reordering is to rewrite the input sentence such that it has a similar word order to the desired target sentence. The paper suggests that reordering the input reduces the translation error rate. However, it does not provide a methodology on how to perform this reordering. (Xia and McCord, 2004) propose a method to automatically acquire rewrite patterns that can be applied to any given input sentence so that the rewritten source and target sentences have similar word order. These rewrit</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, 1997</marker>
<rawString>Christoph Tillmann, Stephan Vogel, Hermann Ney, and Alex Zubiaga. 1997. A DP-Based Search Using Monotone Alignments in Statistical Translation. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 289–296, Madrid. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-BasedWord Alignment in Statistical Machine Translation.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th Int. Conf. on Computational Linguistics (COLING</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="14439" citStr="Vogel et al., 1996" startWordPosition="2300" endWordPosition="2303">ee components: outbound, inbound, and pair distortion. Intuitively our distortion models attempt to capture the order in which source words need to be translated. For instance, the outbound distortion component attempts to capture what is typically translated immediately after the word that has just been translated. Do we tend to translate words that precede it or succeed it? Which word position to translate next? Our distortion parameters are directly estimated from word alignments by simple counting over alignment links in the training data. Any aligner such as (Al-Onaizan et al., 1999) or (Vogel et al., 1996) can be used to obtain word alignments. For the results reported in this paper word alignments were obtained using a maximum-posterior word aligner4 described in (Ge, 2004). We will illustrate the components of our model with a partial word alignment. Let us assume that our source sentences is (f10, f250 , f30 0 )6, and our target sentence is (e410, e20), and their word alignment is a = ((f10, e410), (f300, e20)). Word Alignment a can 4We also estimated distortion parameters using a Maximum Entropy aligner and the differences were negligible. 5In practice, we add special symbols at the start a</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-BasedWord Alignment in Statistical Machine Translation. In Proc. of the 16th Int. Conf. on Computational Linguistics (COLING 1996), pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A Polynomial-Time Algorithm for Statistical Machine Translation.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Conf. of the Association for Computational Linguistics (ACL 96),</booktitle>
<pages>152--158</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="4641" citStr="Wu, 1996" startWordPosition="731" endWordPosition="732">nd 44th Annual Meeting of the ACL, pages 529–536, Sydney, July 2006. c�2006 Association for Computational Linguistics word reorderings during the translation process. Trying all possible word reordering is an NP-Complete problem as shown in (Knight, 1999), which makes searching for the optimal solution among all possible permutations computationally intractable. Therefore, SMT decoders typically limit the number of permutations considered for efficiency reasons by placing reordering restrictions. Reordering restrictions for word-based SMT decoders were introduced by (Berger et al., 1996) and (Wu, 1996). (Berger et al., 1996) allow only reordering of at most n words at any given time. (Wu, 1996) propose using contiguity restrictions on the reordering. For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). A different approach to allow for a limited reordering is to reorder the input sentence such that the source and the target sentences have similar word order and then proceed to monotonically decode the reordered source sentence. Monotone decoding translates words in the same order they appear in the source language. Hence, the input and output sente</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A Polynomial-Time Algorithm for Statistical Machine Translation. In Proc. of the 34th Annual Conf. of the Association for Computational Linguistics (ACL 96), pages 152–158, Santa Cruz, CA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a Statistical MT System with Automatically Learned Rewrite Patterns.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="5796" citStr="Xia and McCord, 2004" startWordPosition="927" endWordPosition="930">y appear in the source language. Hence, the input and output sentences have the same word order. Monotone decoding is very efficient since the optimal decoding can be found in polynomial time. (Tillmann et al., 1997) proposed a DP-based monotone search algorithm for SMT. Their proposed solution to address the necessary word reordering is to rewrite the input sentence such that it has a similar word order to the desired target sentence. The paper suggests that reordering the input reduces the translation error rate. However, it does not provide a methodology on how to perform this reordering. (Xia and McCord, 2004) propose a method to automatically acquire rewrite patterns that can be applied to any given input sentence so that the rewritten source and target sentences have similar word order. These rewrite patterns are automatically extracted by parsing the source and target sides of the training parallel corpus. Their approach show a statistically-significant improvement over a phrase-based monotone decoder. Their experiments also suggest that allowing the decoder to consider some word order permutations in addition to the rewrite patterns already applied to the source sentence actually decreases the </context>
<context position="8648" citStr="Xia and McCord, 2004" startWordPosition="1379" endWordPosition="1382">that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. They reported results that are better than word-based IBM4-like decoder. However, their decoder is outperformed by phrase-based decoders such as (Koehn, 2004), (Och et al., 1999), and (Tillmann and Ney, 2003) . Phrase-based SMT decoders mostly rely on the language model to select among possible word order choices. However, in our experiments we show that the language model is not reliable enough to make the choices that lead to a better MT quality. This observation is also reported by (Xia and McCord, 2004).We argue that the distortion model we propose leads to a better translation as measured by BLEU. Distortion models were first proposed by (Brown et al., 1993) in the so-called IBM Models. IBM Models 2 and 3 define the distortion parameters in terms of the word positions in the sentence pair, not the actual words at those positions. Distortion probability is also conditioned on the source and target sentence lengths. These models do not generalize well since their parameters are tied to absolute word position within sentences which tend to be different for the same words across sentences. IBM </context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite Patterns. In Proc. of the 20th International Conference on Computational Linguistics (COLING 2004), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A Decoder for Syntax-based Statistical MT.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02),</booktitle>
<pages>303--310</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="7995" citStr="Yamada and Knight, 2002" startWordPosition="1274" endWordPosition="1277"> a monotone phrase decoder even if the individual units of the phrase Almmlkp and AlmtHdp, and their translations (Kingdom and United, respectively) are in the phrase dictionary since that would require swapping the order of the two words. (Och et al., 1999; Tillmann and Ney, 2003) relax the monotonicity restriction in their phrase-based decoder by allowing a restricted set of word reorderings. For their translation task, word reordering is done only for words belonging to the verb group. The context in which they report their results is a Speech-to-Speech translation from German to English. (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. They reported results that are better than word-based IBM4-like decoder. However, their decoder is outperformed by phrase-based decoders such as (Koehn, 2004), (Och et al., 1999), and (Tillmann and Ney, 2003) . Phrase-based SMT decoders mostly rely on the language model to select among possible word order choices. However, in our experiments we show that the language model is not reliable enough to make the choices that lead to a better MT quality. This o</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Kenji Yamada and Kevin Knight. 2002. A Decoder for Syntax-based Statistical MT. In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02), pages 303–310, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A Comparative Study on Reordering Constraints in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting ofthe Associationfor Computational Linguistics,</booktitle>
<pages>144--151</pages>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4887" citStr="Zens and Ney, 2003" startWordPosition="774" endWordPosition="777">n in (Knight, 1999), which makes searching for the optimal solution among all possible permutations computationally intractable. Therefore, SMT decoders typically limit the number of permutations considered for efficiency reasons by placing reordering restrictions. Reordering restrictions for word-based SMT decoders were introduced by (Berger et al., 1996) and (Wu, 1996). (Berger et al., 1996) allow only reordering of at most n words at any given time. (Wu, 1996) propose using contiguity restrictions on the reordering. For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). A different approach to allow for a limited reordering is to reorder the input sentence such that the source and the target sentences have similar word order and then proceed to monotonically decode the reordered source sentence. Monotone decoding translates words in the same order they appear in the source language. Hence, the input and output sentences have the same word order. Monotone decoding is very efficient since the optimal decoding can be found in polynomial time. (Tillmann et al., 1997) proposed a DP-based monotone search algorithm for SMT. Their proposed solution to address the n</context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. 2003. A Comparative Study on Reordering Constraints in Statistical Machine Translation. In Erhard Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting ofthe Associationfor Computational Linguistics, pages 144–151, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>