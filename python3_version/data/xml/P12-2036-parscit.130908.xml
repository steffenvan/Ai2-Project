<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020795">
<title confidence="0.959914">
Corpus-based interpretation of instructions in virtual environments
</title>
<author confidence="0.986153">
Luciana Benotti1 Martin Villalba1 Tessa Lau2 Juli´an Cerruti3
</author>
<affiliation confidence="0.647416">
1 FaMAF, Medina Allende s/n, Universidad Nacional de C´ordoba, C´ordoba, Argentina
2IBM Research – Almaden, 650 Harry Road, San Jose, CA 95120 USA
3IBM Argentina, Ing. Butty 275, C1001AFA, Buenos Aires, Argentina
</affiliation>
<email confidence="0.991843">
lbenotti,villalbal@famaf.unc.edu.ar, tessalau@us.ibm.com, jcerruti@ar.ibm.com
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999419785714286">
Previous approaches to instruction interpre-
tation have required either extensive domain
adaptation or manually annotated corpora.
This paper presents a novel approach to in-
struction interpretation that leverages a large
amount of unannotated, easy-to-collect data
from humans interacting with a virtual world.
We compare several algorithms for automat-
ically segmenting and discretizing this data
into (utterance, reaction) pairs and training a
classifier to predict reactions given the next ut-
terance. Our empirical analysis shows that the
best algorithm achieves 70% accuracy on this
task, with no manual annotation required.
</bodyText>
<sectionHeader confidence="0.98147" genericHeader="categories and subject descriptors">
1 Introduction and motivation
</sectionHeader>
<bodyText confidence="0.988373843137255">
Mapping instructions into automatically executable
actions would enable the creation of natural lan-
guage interfaces to many applications (Lau et al.,
2009; Branavan et al., 2009; Orkin and Roy, 2009).
In this paper, we focus on the task of navigation and
manipulation of a virtual environment (Vogel and
Jurafsky, 2010; Chen and Mooney, 2011).
Current symbolic approaches to the problem are
brittle to the natural language variation present in in-
structions and require intensive rule authoring to be
fit for a new task (Dzikovska et al., 2008). Current
statistical approaches require extensive manual an-
notations of the corpora used for training (MacMa-
hon et al., 2006; Matuszek et al., 2010; Gorniak and
Roy, 2007; Rieser and Lemon, 2010). Manual anno-
tation and rule authoring by natural language engi-
neering experts are bottlenecks for developing con-
versational systems for new domains.
This paper proposes a fully automated approach
to interpreting natural language instructions to com-
plete a task in a virtual world based on unsupervised
recordings of human-human interactions perform-
ing that task in that virtual world. Given unanno-
tated corpora collected from humans following other
humans’ instructions, our system automatically seg-
ments the corpus into labeled training data for a clas-
sification algorithm. Our interpretation algorithm is
based on the observation that similar instructions ut-
tered in similar contexts should lead to similar ac-
tions being taken in the virtual world. Given a previ-
ously unseen instruction, our system outputs actions
that can be directly executed in the virtual world,
based on what humans did when given similar in-
structions in the past.
2 Corpora situated in virtual worlds
Our environment consists of six virtual worlds de-
signed for the natural language generation shared
task known as the GIVE Challenge (Koller et al.,
2010), where a pair of partners must collaborate to
solve a task in a 3D space (Figure 1). The “instruc-
tion follower” (IF) can move around in the virtual
world, but has no knowledge of the task. The “in-
struction giver” (IG) types instructions to the IF in
order to guide him to accomplish the task. Each cor-
pus contains the IF’s actions and position recorded
every 200 milliseconds, as well as the IG’s instruc-
tions with their timestamps.
We used two corpora for our experiments. The
Cm corpus (Gargett et al., 2010) contains instruc-
tions given by multiple people, consisting of 37
games spanning 2163 instructions over 8:17 hs. The
</bodyText>
<page confidence="0.98036">
181
</page>
<note confidence="0.902231">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 181–186,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.997807">
Figure 1: A screenshot of a virtual world. The world
consists of interconnecting hallways, rooms and objects
</figureCaption>
<bodyText confidence="0.997859454545455">
Cs corpus (Benotti and Denis, 2011), gathered using
a single IG, is composed of 63 games and 3417 in-
structions, and was recorded in a span of 6:09 hs. It
took less than 15 hours to collect the corpora through
the web and the subjects reported that the experi-
ment was fun.
While the environment is restricted, people de-
scribe the same route and the same objects in ex-
tremely different ways. Below are some examples of
instructions from our corpus all given for the same
route shown in Figure 1.
</bodyText>
<listItem confidence="0.994053">
1) out
2) walk down the passage
3) nowgo [sic] to the pink room
4) back to the room with the plant
S) Go through the door on the left
6) go through opening with yellow wallpaper
</listItem>
<bodyText confidence="0.996510857142857">
People describe routes using landmarks (4) or
specific actions (2). They may describe the same
object differently (5 vs 6). Instructions also differ
in their scope (3 vs 1). Thus, even ignoring spelling
and grammatical errors, navigation instructions con-
tain considerable variation which makes interpreting
them a challenging problem.
</bodyText>
<sectionHeader confidence="0.939793" genericHeader="method">
3 Learning from previous interpretations
</sectionHeader>
<bodyText confidence="0.999680857142857">
Our algorithm consists of two phases: annotation
and interpretation. Annotation is performed only
once and consists of automatically associating each
IG instruction to an IF reaction. Interpretation is
performed every time the system receives an instruc-
tion and consists of predicting an appropriate reac-
tion given reactions observed in the corpus.
Our method is based on the assumption that a re-
action captures the semantics of the instruction that
caused it. Therefore, if two utterances result in the
same reaction, they are paraphrases of each other,
and similar utterances should generate the same re-
action. This approach enables us to predict reactions
for previously-unseen instructions.
</bodyText>
<subsectionHeader confidence="0.999898">
3.1 Annotation phase
</subsectionHeader>
<bodyText confidence="0.995139928571429">
The key challenge in learning from massive amounts
of easily-collected data is to automatically annotate
an unannotated corpus. Our annotation method con-
sists of two parts: first, segmenting a low-level in-
teraction trace into utterances and corresponding re-
actions, and second, discretizing those reactions into
canonical action sequences.
Segmentation enables our algorithm to learn from
traces of IFs interacting directly with a virtual world.
Since the IF can move freely in the virtual world, his
actions are a stream of continuous behavior. Seg-
mentation divides these traces into reactions that fol-
low from each utterance of the IG. Consider the fol-
lowing example starting at the situation shown in
</bodyText>
<figureCaption confidence="0.817591">
Figure 1:
</figureCaption>
<bodyText confidence="0.939707380952381">
IG(1): go through the yellow opening
IF(2): [walks out of the room]
IF(3): [turns left at the intersection]
IF(4): [enters the room with the sofa]
IG(S): stop
It is not clear whether the IF is doing (3,4) be-
cause he is reacting to 1 or because he is being
proactive. While one could manually annotate this
data to remove extraneous actions, our goal is to de-
velop automated solutions that enable learning from
massive amounts of data.
We decided to approach this problem by experi-
menting with two alternative formal definitions: 1) a
strict definition that considers the maximum reaction
according to the IF behavior, and 2) a loose defini-
tion based on the empirical observation that, in sit-
uated interaction, most instructions are constrained
by the current visually perceived affordances (Gib-
son, 1979; Stoia et al., 2006).
We formally define behavior segmentation (Bhv)
as follows. A reaction rk to an instruction uk begins
</bodyText>
<page confidence="0.991296">
182
</page>
<bodyText confidence="0.99994684">
right after the instruction uk is uttered and ends right
before the next instruction uk+1 is uttered. In the
example, instruction 1 corresponds to (2, 3, 4). We
formally define visibility segmentation (Vis) as fol-
lows. A reaction rk to an instruction uk begins right
after the instruction uk is uttered and ends right be-
fore the next instruction uk+1 is uttered or right after
the IF leaves the area visible at 360◦ from where uk
was uttered. In the example, instruction 1’s reaction
would be limited to (2) because the intersection is
not visible from where the instruction was uttered.
The Bhv and Vis methods define how to segment
an interaction trace into utterances and their corre-
sponding reactions. However, users frequently per-
form noisy behavior that is irrelevant to the goal of
the task. For example, after hearing an instruction,
an IF might go into the wrong room, realize the er-
ror, and leave the room. A reaction should not in-
clude such irrelevant actions. In addition, IFs may
accomplish the same goal using different behaviors:
two different IFs may interpret “go to the pink room”
by following different paths to the same destination.
We would like to be able to generalize both reactions
into one canonical reaction.
As a result, our approach discretizes reactions into
higher-level action sequences with less noise and
less variation. Our discretization algorithm uses an
automated planner and a planning representation of
the task. This planning representation includes: (1)
the task goal, (2) the actions which can be taken in
the virtual world, and (3) the current state of the
virtual world. Using the planning representation,
the planner calculates an optimal path between the
starting and ending states of the reaction, eliminat-
ing all unnecessary actions. While we use the clas-
sical planner FF (Hoffmann, 2003), our technique
could also work with classical planning (Nau et al.,
2004) or other techniques such as probabilistic plan-
ning (Bonet and Geffner, 2005). It is also not de-
pendent on a particular discretization of the world in
terms of actions.
Now we are ready to define canonical reaction ck
formally. Let Sk be the state of the virtual world
when instruction uk was uttered, Sk+1 be the state of
the world where the reaction ends (as defined by Bhv
or Vis segmentation), and D be the planning domain
representation of the virtual world. The canonical
reaction to uk is defined as the sequence of actions
returned by the planner with Sk as initial state, Sk+1
as goal state and D as planning domain.
</bodyText>
<subsectionHeader confidence="0.998534">
3.2 Interpretation phase
</subsectionHeader>
<bodyText confidence="0.999801181818182">
The annotation phase results in a collection of (uk,
ck) pairs. The interpretation phase uses these pairs to
interpret new utterances in three steps. First, we fil-
ter the set of pairs into those whose reactions can be
directly executed from the current IF position. Sec-
ond, we group the filtered pairs according to their
reactions. Third, we select the group with utterances
most similar to the new utterance, and output that
group’s reaction. Figure 2 shows the output of the
first two steps: three groups of pairs whose reactions
can all be executed from the IF’s current position.
</bodyText>
<figureCaption confidence="0.998343">
Figure 2: Utterance groups for this situation. Colored
arrows show the reaction associated with each group.
</figureCaption>
<bodyText confidence="0.999966555555555">
We treat the third step, selecting the most similar
group for a new utterance, as a classification prob-
lem. We compare three different classification meth-
ods. One method uses nearest-neighbor classifica-
tion with three different similarity metrics: Jaccard
and Overlap coefficients (both of which measure the
degree of overlap between two sets, differing only
in the normalization of the final value (Nikravesh et
al., 2005)), and Levenshtein Distance (a string met-
ric for measuring the amount of differences between
two sequences of words (Levenshtein, 1966)). Our
second classification method employs a strategy in
which we considered each group as a set of pos-
sible machine translations of our utterance, using
the BLEU measure (Papineni et al., 2002) to select
which group could be considered the best translation
of our utterance. Finally, we trained an SVM clas-
sifier (Cortes and Vapnik, 1995) using the unigrams
</bodyText>
<page confidence="0.997664">
183
</page>
<table confidence="0.998476714285714">
Algorithm Corpus Cm Corpus Cs
Bhv Vis Bhv Vis
Jaccard 47% 54% 54% 70%
Overlap 43% 53% 45% 60%
BLEU 44% 52% 54% 50%
SVM 33% 29% 45% 29%
Levenshtein 21% 20% 8% 17%
</table>
<tableCaption confidence="0.964701">
Table 1: Accuracy comparison between C,,,, and C3 for
Bhv and Vis segmentation
</tableCaption>
<bodyText confidence="0.998210666666667">
of each paraphrase and the position of the IF as fea-
tures, and setting their group as the output class us-
ing a libSVM wrapper (Chang and Lin, 2011).
When the system misinterprets an instruction we
use a similar approach to what people do in order
to overcome misunderstandings. If the system exe-
cutes an incorrect reaction, the IG can tell the system
to cancel its current interpretation and try again us-
ing a paraphrase, selecting a different reaction.
</bodyText>
<sectionHeader confidence="0.998734" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999949868421053">
For the evaluation phase, we annotated both the Cm
and Cs corpora entirely, and then we split them in
an 80/20 proportion; the first 80% of data collected
in each virtual world was used for training, while
the remaining 20% was used for testing. For each
pair (uk, ck) in the testing set, we used our algorithm
to predict the reaction to the selected utterance, and
then compared this result against the automatically
annotated reaction. Table 1 shows the results.
Comparing the Bhv and Vis segmentation strate-
gies, Vis tends to obtain better results than Bhv. In
addition, accuracy on the Cs corpus was generally
higher than Cm. Given that Cs contained only one
IG, we believe this led to less variability in the in-
structions and less noise in the training data.
We evaluated the impact of user corrections by
simulating them using the existing corpus. In case
of a wrong response, the algorithm receives a second
utterance with the same reaction (a paraphrase of the
previous one). Then the new utterance is tested over
the same set of possible groups, except for the one
which was returned before. If the correct reaction
is not predicted after four tries, or there are no ut-
terances with the same reaction, the predictions are
registered as wrong. To measure the effects of user
corrections vs. without, we used a different evalu-
ation process for this algorithm: first, we split the
corpus in a 50/50 proportion, and then we moved
correctly predicted utterances from the testing set to-
wards training, until either there was nothing more
to learn or the training set reached 80% of the entire
corpus size.
As expected, user corrections significantly im-
prove accuracy, as shown in Figure 3. The worst
algorithm’s results improve linearly with each try,
while the best ones behave asymptotically, barely
improving after the second try. The best algorithm
reaches 92% with just one correction from the IG.
</bodyText>
<sectionHeader confidence="0.965177" genericHeader="conclusions">
5 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999769428571429">
We presented an approach to instruction interpreta-
tion which learns from non-annotated logs of hu-
man behavior. Our empirical analysis shows that
our best algorithm achieves 70% accuracy on this
task, with no manual annotation required. When
corrections are added, accuracy goes up to 92%
for just one correction. We consider our results
promising since state of the art semi-unsupervised
approaches to instruction interpretation (Chen and
Mooney, 2011) reports a 55% accuracy on manually
segmented data.
We plan to compare our system’s performance
against human performance in comparable situa-
tions. Our informal observations of the GIVE cor-
pus indicate that humans often follow instructions
incorrectly, so our automated system’s performance
may be on par with human performance.
Although we have presented our approach in the
context of 3D virtual worlds, we believe our tech-
nique is also applicable to other domains such as the
web, video games, or Human Robot Interaction.
</bodyText>
<figureCaption confidence="0.987129">
Figure 3: Accuracy values with corrections over C3
</figureCaption>
<page confidence="0.997552">
184
</page>
<sectionHeader confidence="0.913183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987877037735849">
Luciana Benotti and Alexandre Denis. 2011. CL system:
Giving instructions by corpus based selection. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 296–301, Nancy, France, September. As-
sociation for Computational Linguistics.
Blai Bonet and H´ector Geffner. 2005. mGPT: a proba-
bilistic planner based on heuristic search. Journal of
Artificial Intelligence Research, 24:933–944.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
82–90, Suntec, Singapore, August. Association for
Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1–
27:27. Software available at http://www.csie.
ntu.edu.tw/˜cjlin/libsvm.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011), pages 859–865, August.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20:273–297.
Myroslava O. Dzikovska, James F. Allen, and Mary D.
Swift. 2008. Linking semantic and knowledge repre-
sentations in a multi-domain dialogue system. Journal
of Logic and Computation, 18:405–430, June.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus
of giving instructions in virtual environments. In Pro-
ceedings of the 7th Conference on International Lan-
guage Resources and Evaluation (LREC), Malta.
James J. Gibson. 1979. The Ecological Approach to Vi-
sual Perception, volume 40. Houghton Mifflin.
Peter Gorniak and Deb Roy. 2007. Situated language
understanding as filtering perceived affordances. Cog-
nitive Science, 31(2):197–231.
J¨org Hoffmann. 2003. The Metric-FF planning sys-
tem: Translating ”ignoring delete lists” to numeric
state variables. Journal of Artificial Intelligence Re-
search (JAIR), 20:291–341.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010. Report on the sec-
ond challenge on generating instructions in virtual en-
vironments (GIVE-2). In Proceedings of the 6th In-
ternational Natural Language Generation Conference
(INLG), Dublin.
Tessa Lau, Clemens Drews, and Jeffrey Nichols. 2009.
Interpreting written how-to instructions. In Proceed-
ings of the 21st International Joint Conference on Ar-
tificial Intelligence, pages 1433–1438, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Techni-
cal Report 8.
Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: connecting language,
knowledge, and action in route instructions. In Pro-
ceedings of the 21st National Conference on Artifi-
cial Intelligence - Volume 2, pages 1475–1482. AAAI
Press.
Cynthia Matuszek, Dieter Fox, and Karl Koscher. 2010.
Following directions using statistical machine trans-
lation. In Proceedings of the 5th ACM/IEEE inter-
national conference on Human-robot interaction, HRI
’10, pages 251–258, New York, NY, USA. ACM.
Dana Nau, Malik Ghallab, and Paolo Traverso. 2004.
Automated Planning: Theory &amp; Practice. Morgan
Kaufmann Publishers Inc., California, USA.
Masoud Nikravesh, Tomohiro Takagi, Masanori Tajima,
Akiyoshi Shinmura, Ryosuke Ohgaya, Koji Taniguchi,
Kazuyosi Kawahara, Kouta Fukano, and Akiko
Aizawa. 2005. Soft computing for perception-based
decision processing and analysis: Web-based BISC-
DSS. In Masoud Nikravesh, Lotfi Zadeh, and Janusz
Kacprzyk, editors, Soft Computing for Information
Processing and Analysis, volume 164 of Studies in
Fuzziness and Soft Computing, chapter 4, pages 93–
188. Springer Berlin / Heidelberg.
Jeff Orkin and Deb Roy. 2009. Automatic learning
and generation of social behavior from collective hu-
man gameplay. In Proceedings of The 8th Interna-
tional Conference on Autonomous Agents and Mul-
tiagent SystemsVolume 1, volume 1, pages 385–392.
International Foundation for Autonomous Agents and
Multiagent Systems, International Foundation for Au-
tonomous Agents and Multiagent Systems.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Verena Rieser and Oliver Lemon. 2010. Learning hu-
man multimodal dialogue strategies. Natural Lan-
guage Engineering, 16:3–23.
Laura Stoia, Donna K. Byron, Darla Magdalene Shock-
ley, and Eric Fosler-Lussier. 2006. Sentence planning
</reference>
<page confidence="0.987511">
185
</page>
<reference confidence="0.9755039">
for realtime navigational instructions. In Proceedings
of the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, NAACL-
Short ’06, pages 157–160, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 806–814, Stroudsburg,
PA, USA. Association for Computational Linguistics.
</reference>
<page confidence="0.99878">
186
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.605578">
<title confidence="0.810227">Corpus-based interpretation of instructions in virtual environments</title>
<author confidence="0.654127">Medina Allende sn</author>
<author confidence="0.654127">Universidad Nacional de_C´ordoba</author>
<author confidence="0.654127">C´ordoba</author>
<address confidence="0.990567">Research – Almaden, 650 Harry Road, San Jose, CA 95120 Argentina, Ing. Butty 275, C1001AFA, Buenos Aires, Argentina</address>
<email confidence="0.999682">lbenotti,villalbal@famaf.unc.edu.ar,tessalau@us.ibm.com,jcerruti@ar.ibm.com</email>
<abstract confidence="0.998060933333333">Previous approaches to instruction interpretation have required either extensive domain adaptation or manually annotated corpora. This paper presents a novel approach to instruction interpretation that leverages a large amount of unannotated, easy-to-collect data from humans interacting with a virtual world. We compare several algorithms for automatically segmenting and discretizing this data into (utterance, reaction) pairs and training a classifier to predict reactions given the next utterance. Our empirical analysis shows that the best algorithm achieves 70% accuracy on this task, with no manual annotation required.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciana Benotti</author>
<author>Alexandre Denis</author>
</authors>
<title>CL system: Giving instructions by corpus based selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>296--301</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="3926" citStr="Benotti and Denis, 2011" startWordPosition="594" endWordPosition="597">ecorded every 200 milliseconds, as well as the IG’s instructions with their timestamps. We used two corpora for our experiments. The Cm corpus (Gargett et al., 2010) contains instructions given by multiple people, consisting of 37 games spanning 2163 instructions over 8:17 hs. The 181 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 181–186, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: A screenshot of a virtual world. The world consists of interconnecting hallways, rooms and objects Cs corpus (Benotti and Denis, 2011), gathered using a single IG, is composed of 63 games and 3417 instructions, and was recorded in a span of 6:09 hs. It took less than 15 hours to collect the corpora through the web and the subjects reported that the experiment was fun. While the environment is restricted, people describe the same route and the same objects in extremely different ways. Below are some examples of instructions from our corpus all given for the same route shown in Figure 1. 1) out 2) walk down the passage 3) nowgo [sic] to the pink room 4) back to the room with the plant S) Go through the door on the left 6) go t</context>
</contexts>
<marker>Benotti, Denis, 2011</marker>
<rawString>Luciana Benotti and Alexandre Denis. 2011. CL system: Giving instructions by corpus based selection. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 296–301, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blai Bonet</author>
<author>H´ector Geffner</author>
</authors>
<title>mGPT: a probabilistic planner based on heuristic search.</title>
<date>2005</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>24--933</pages>
<contexts>
<context position="9288" citStr="Bonet and Geffner, 2005" startWordPosition="1471" endWordPosition="1474">etization algorithm uses an automated planner and a planning representation of the task. This planning representation includes: (1) the task goal, (2) the actions which can be taken in the virtual world, and (3) the current state of the virtual world. Using the planning representation, the planner calculates an optimal path between the starting and ending states of the reaction, eliminating all unnecessary actions. While we use the classical planner FF (Hoffmann, 2003), our technique could also work with classical planning (Nau et al., 2004) or other techniques such as probabilistic planning (Bonet and Geffner, 2005). It is also not dependent on a particular discretization of the world in terms of actions. Now we are ready to define canonical reaction ck formally. Let Sk be the state of the virtual world when instruction uk was uttered, Sk+1 be the state of the world where the reaction ends (as defined by Bhv or Vis segmentation), and D be the planning domain representation of the virtual world. The canonical reaction to uk is defined as the sequence of actions returned by the planner with Sk as initial state, Sk+1 as goal state and D as planning domain. 3.2 Interpretation phase The annotation phase resul</context>
</contexts>
<marker>Bonet, Geffner, 2005</marker>
<rawString>Blai Bonet and H´ector Geffner. 2005. mGPT: a probabilistic planner based on heuristic search. Journal of Artificial Intelligence Research, 24:933–944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>82--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1264" citStr="Branavan et al., 2009" startWordPosition="166" endWordPosition="169"> large amount of unannotated, easy-to-collect data from humans interacting with a virtual world. We compare several algorithms for automatically segmenting and discretizing this data into (utterance, reaction) pairs and training a classifier to predict reactions given the next utterance. Our empirical analysis shows that the best algorithm achieves 70% accuracy on this task, with no manual annotation required. 1 Introduction and motivation Mapping instructions into automatically executable actions would enable the creation of natural language interfaces to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annotation and rule authoring</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 82–90, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<pages>27--27</pages>
<note>Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="11864" citStr="Chang and Lin, 2011" startWordPosition="1910" endWordPosition="1913">f our utterance, using the BLEU measure (Papineni et al., 2002) to select which group could be considered the best translation of our utterance. Finally, we trained an SVM classifier (Cortes and Vapnik, 1995) using the unigrams 183 Algorithm Corpus Cm Corpus Cs Bhv Vis Bhv Vis Jaccard 47% 54% 54% 70% Overlap 43% 53% 45% 60% BLEU 44% 52% 54% 50% SVM 33% 29% 45% 29% Levenshtein 21% 20% 8% 17% Table 1: Accuracy comparison between C,,,, and C3 for Bhv and Vis segmentation of each paraphrase and the position of the IF as features, and setting their group as the output class using a libSVM wrapper (Chang and Lin, 2011). When the system misinterprets an instruction we use a similar approach to what people do in order to overcome misunderstandings. If the system executes an incorrect reaction, the IG can tell the system to cancel its current interpretation and try again using a paraphrase, selecting a different reaction. 4 Evaluation For the evaluation phase, we annotated both the Cm and Cs corpora entirely, and then we split them in an 80/20 proportion; the first 80% of data collected in each virtual world was used for training, while the remaining 20% was used for testing. For each pair (uk, ck) in the test</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1– 27:27. Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI2011),</booktitle>
<pages>859--865</pages>
<contexts>
<context position="1429" citStr="Chen and Mooney, 2011" startWordPosition="194" endWordPosition="197">retizing this data into (utterance, reaction) pairs and training a classifier to predict reactions given the next utterance. Our empirical analysis shows that the best algorithm achieves 70% accuracy on this task, with no manual annotation required. 1 Introduction and motivation Mapping instructions into automatically executable actions would enable the creation of natural language interfaces to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annotation and rule authoring by natural language engineering experts are bottlenecks for developing conversational systems for new domains. This paper proposes a fully automated approach to int</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI2011), pages 859–865, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="11452" citStr="Cortes and Vapnik, 1995" startWordPosition="1830" endWordPosition="1833">cients (both of which measure the degree of overlap between two sets, differing only in the normalization of the final value (Nikravesh et al., 2005)), and Levenshtein Distance (a string metric for measuring the amount of differences between two sequences of words (Levenshtein, 1966)). Our second classification method employs a strategy in which we considered each group as a set of possible machine translations of our utterance, using the BLEU measure (Papineni et al., 2002) to select which group could be considered the best translation of our utterance. Finally, we trained an SVM classifier (Cortes and Vapnik, 1995) using the unigrams 183 Algorithm Corpus Cm Corpus Cs Bhv Vis Bhv Vis Jaccard 47% 54% 54% 70% Overlap 43% 53% 45% 60% BLEU 44% 52% 54% 50% SVM 33% 29% 45% 29% Levenshtein 21% 20% 8% 17% Table 1: Accuracy comparison between C,,,, and C3 for Bhv and Vis segmentation of each paraphrase and the position of the IF as features, and setting their group as the output class using a libSVM wrapper (Chang and Lin, 2011). When the system misinterprets an instruction we use a similar approach to what people do in order to overcome misunderstandings. If the system executes an incorrect reaction, the IG can </context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine Learning, 20:273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>James F Allen</author>
<author>Mary D Swift</author>
</authors>
<title>Linking semantic and knowledge representations in a multi-domain dialogue system.</title>
<date>2008</date>
<journal>Journal of Logic and Computation,</journal>
<volume>18</volume>
<contexts>
<context position="1630" citStr="Dzikovska et al., 2008" startWordPosition="227" endWordPosition="230">on this task, with no manual annotation required. 1 Introduction and motivation Mapping instructions into automatically executable actions would enable the creation of natural language interfaces to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annotation and rule authoring by natural language engineering experts are bottlenecks for developing conversational systems for new domains. This paper proposes a fully automated approach to interpreting natural language instructions to complete a task in a virtual world based on unsupervised recordings of human-human interactions performing that task in that virtual world. Given unannotated </context>
</contexts>
<marker>Dzikovska, Allen, Swift, 2008</marker>
<rawString>Myroslava O. Dzikovska, James F. Allen, and Mary D. Swift. 2008. Linking semantic and knowledge representations in a multi-domain dialogue system. Journal of Logic and Computation, 18:405–430, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gargett</author>
<author>Konstantina Garoufi</author>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
</authors>
<title>The GIVE-2 corpus of giving instructions in virtual environments.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th Conference on International Language Resources and Evaluation (LREC),</booktitle>
<contexts>
<context position="3467" citStr="Gargett et al., 2010" startWordPosition="526" endWordPosition="529">lds designed for the natural language generation shared task known as the GIVE Challenge (Koller et al., 2010), where a pair of partners must collaborate to solve a task in a 3D space (Figure 1). The “instruction follower” (IF) can move around in the virtual world, but has no knowledge of the task. The “instruction giver” (IG) types instructions to the IF in order to guide him to accomplish the task. Each corpus contains the IF’s actions and position recorded every 200 milliseconds, as well as the IG’s instructions with their timestamps. We used two corpora for our experiments. The Cm corpus (Gargett et al., 2010) contains instructions given by multiple people, consisting of 37 games spanning 2163 instructions over 8:17 hs. The 181 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 181–186, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: A screenshot of a virtual world. The world consists of interconnecting hallways, rooms and objects Cs corpus (Benotti and Denis, 2011), gathered using a single IG, is composed of 63 games and 3417 instructions, and was recorded in a span of 6:09 hs. It took less than 15 hour</context>
</contexts>
<marker>Gargett, Garoufi, Koller, Striegnitz, 2010</marker>
<rawString>Andrew Gargett, Konstantina Garoufi, Alexander Koller, and Kristina Striegnitz. 2010. The GIVE-2 corpus of giving instructions in virtual environments. In Proceedings of the 7th Conference on International Language Resources and Evaluation (LREC), Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James J Gibson</author>
</authors>
<title>The Ecological Approach to Visual Perception, volume 40.</title>
<date>1979</date>
<publisher>Houghton Mifflin.</publisher>
<contexts>
<context position="7172" citStr="Gibson, 1979" startWordPosition="1124" endWordPosition="1126">s doing (3,4) because he is reacting to 1 or because he is being proactive. While one could manually annotate this data to remove extraneous actions, our goal is to develop automated solutions that enable learning from massive amounts of data. We decided to approach this problem by experimenting with two alternative formal definitions: 1) a strict definition that considers the maximum reaction according to the IF behavior, and 2) a loose definition based on the empirical observation that, in situated interaction, most instructions are constrained by the current visually perceived affordances (Gibson, 1979; Stoia et al., 2006). We formally define behavior segmentation (Bhv) as follows. A reaction rk to an instruction uk begins 182 right after the instruction uk is uttered and ends right before the next instruction uk+1 is uttered. In the example, instruction 1 corresponds to (2, 3, 4). We formally define visibility segmentation (Vis) as follows. A reaction rk to an instruction uk begins right after the instruction uk is uttered and ends right before the next instruction uk+1 is uttered or right after the IF leaves the area visible at 360◦ from where uk was uttered. In the example, instruction 1</context>
</contexts>
<marker>Gibson, 1979</marker>
<rawString>James J. Gibson. 1979. The Ecological Approach to Visual Perception, volume 40. Houghton Mifflin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gorniak</author>
<author>Deb Roy</author>
</authors>
<title>Situated language understanding as filtering perceived affordances.</title>
<date>2007</date>
<journal>Cognitive Science,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="1801" citStr="Gorniak and Roy, 2007" startWordPosition="254" endWordPosition="257">al language interfaces to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annotation and rule authoring by natural language engineering experts are bottlenecks for developing conversational systems for new domains. This paper proposes a fully automated approach to interpreting natural language instructions to complete a task in a virtual world based on unsupervised recordings of human-human interactions performing that task in that virtual world. Given unannotated corpora collected from humans following other humans’ instructions, our system automatically segments the corpus into labeled training data for a classification algorithm.</context>
</contexts>
<marker>Gorniak, Roy, 2007</marker>
<rawString>Peter Gorniak and Deb Roy. 2007. Situated language understanding as filtering perceived affordances. Cognitive Science, 31(2):197–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Hoffmann</author>
</authors>
<title>The Metric-FF planning system: Translating ”ignoring delete lists” to numeric state variables.</title>
<date>2003</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>20--291</pages>
<contexts>
<context position="9137" citStr="Hoffmann, 2003" startWordPosition="1449" endWordPosition="1450">cal reaction. As a result, our approach discretizes reactions into higher-level action sequences with less noise and less variation. Our discretization algorithm uses an automated planner and a planning representation of the task. This planning representation includes: (1) the task goal, (2) the actions which can be taken in the virtual world, and (3) the current state of the virtual world. Using the planning representation, the planner calculates an optimal path between the starting and ending states of the reaction, eliminating all unnecessary actions. While we use the classical planner FF (Hoffmann, 2003), our technique could also work with classical planning (Nau et al., 2004) or other techniques such as probabilistic planning (Bonet and Geffner, 2005). It is also not dependent on a particular discretization of the world in terms of actions. Now we are ready to define canonical reaction ck formally. Let Sk be the state of the virtual world when instruction uk was uttered, Sk+1 be the state of the world where the reaction ends (as defined by Bhv or Vis segmentation), and D be the planning domain representation of the virtual world. The canonical reaction to uk is defined as the sequence of act</context>
</contexts>
<marker>Hoffmann, 2003</marker>
<rawString>J¨org Hoffmann. 2003. The Metric-FF planning system: Translating ”ignoring delete lists” to numeric state variables. Journal of Artificial Intelligence Research (JAIR), 20:291–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
<author>Andrew Gargett</author>
<author>Donna Byron</author>
<author>Justine Cassell</author>
<author>Robert Dale</author>
<author>Johanna Moore</author>
<author>Jon Oberlander</author>
</authors>
<title>Report on the second challenge on generating instructions in virtual environments (GIVE-2).</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Natural Language Generation Conference (INLG),</booktitle>
<location>Dublin.</location>
<contexts>
<context position="2956" citStr="Koller et al., 2010" startWordPosition="434" endWordPosition="437">corpus into labeled training data for a classification algorithm. Our interpretation algorithm is based on the observation that similar instructions uttered in similar contexts should lead to similar actions being taken in the virtual world. Given a previously unseen instruction, our system outputs actions that can be directly executed in the virtual world, based on what humans did when given similar instructions in the past. 2 Corpora situated in virtual worlds Our environment consists of six virtual worlds designed for the natural language generation shared task known as the GIVE Challenge (Koller et al., 2010), where a pair of partners must collaborate to solve a task in a 3D space (Figure 1). The “instruction follower” (IF) can move around in the virtual world, but has no knowledge of the task. The “instruction giver” (IG) types instructions to the IF in order to guide him to accomplish the task. Each corpus contains the IF’s actions and position recorded every 200 milliseconds, as well as the IG’s instructions with their timestamps. We used two corpora for our experiments. The Cm corpus (Gargett et al., 2010) contains instructions given by multiple people, consisting of 37 games spanning 2163 ins</context>
</contexts>
<marker>Koller, Striegnitz, Gargett, Byron, Cassell, Dale, Moore, Oberlander, 2010</marker>
<rawString>Alexander Koller, Kristina Striegnitz, Andrew Gargett, Donna Byron, Justine Cassell, Robert Dale, Johanna Moore, and Jon Oberlander. 2010. Report on the second challenge on generating instructions in virtual environments (GIVE-2). In Proceedings of the 6th International Natural Language Generation Conference (INLG), Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tessa Lau</author>
<author>Clemens Drews</author>
<author>Jeffrey Nichols</author>
</authors>
<title>Interpreting written how-to instructions.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1433--1438</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1241" citStr="Lau et al., 2009" startWordPosition="162" endWordPosition="165">n that leverages a large amount of unannotated, easy-to-collect data from humans interacting with a virtual world. We compare several algorithms for automatically segmenting and discretizing this data into (utterance, reaction) pairs and training a classifier to predict reactions given the next utterance. Our empirical analysis shows that the best algorithm achieves 70% accuracy on this task, with no manual annotation required. 1 Introduction and motivation Mapping instructions into automatically executable actions would enable the creation of natural language interfaces to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annota</context>
</contexts>
<marker>Lau, Drews, Nichols, 2009</marker>
<rawString>Tessa Lau, Clemens Drews, and Jeffrey Nichols. 2009. Interpreting written how-to instructions. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, pages 1433–1438, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<tech>Technical Report 8.</tech>
<contexts>
<context position="11112" citStr="Levenshtein, 1966" startWordPosition="1776" endWordPosition="1777">olored arrows show the reaction associated with each group. We treat the third step, selecting the most similar group for a new utterance, as a classification problem. We compare three different classification methods. One method uses nearest-neighbor classification with three different similarity metrics: Jaccard and Overlap coefficients (both of which measure the degree of overlap between two sets, differing only in the normalization of the final value (Nikravesh et al., 2005)), and Levenshtein Distance (a string metric for measuring the amount of differences between two sequences of words (Levenshtein, 1966)). Our second classification method employs a strategy in which we considered each group as a set of possible machine translations of our utterance, using the BLEU measure (Papineni et al., 2002) to select which group could be considered the best translation of our utterance. Finally, we trained an SVM classifier (Cortes and Vapnik, 1995) using the unigrams 183 Algorithm Corpus Cm Corpus Cs Bhv Vis Bhv Vis Jaccard 47% 54% 54% 70% Overlap 43% 53% 45% 60% BLEU 44% 52% 54% 50% SVM 33% 29% 45% 29% Levenshtein 21% 20% 8% 17% Table 1: Accuracy comparison between C,,,, and C3 for Bhv and Vis segmenta</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Technical Report 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt MacMahon</author>
<author>Brian Stankiewicz</author>
<author>Benjamin Kuipers</author>
</authors>
<title>Walk the talk: connecting language, knowledge, and action in route instructions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence -</booktitle>
<volume>2</volume>
<pages>1475--1482</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1755" citStr="MacMahon et al., 2006" startWordPosition="245" endWordPosition="249">ble actions would enable the creation of natural language interfaces to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annotation and rule authoring by natural language engineering experts are bottlenecks for developing conversational systems for new domains. This paper proposes a fully automated approach to interpreting natural language instructions to complete a task in a virtual world based on unsupervised recordings of human-human interactions performing that task in that virtual world. Given unannotated corpora collected from humans following other humans’ instructions, our system automatically segments the corpus into labeled</context>
</contexts>
<marker>MacMahon, Stankiewicz, Kuipers, 2006</marker>
<rawString>Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. 2006. Walk the talk: connecting language, knowledge, and action in route instructions. In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 2, pages 1475–1482. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Dieter Fox</author>
<author>Karl Koscher</author>
</authors>
<title>Following directions using statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction, HRI ’10,</booktitle>
<pages>251--258</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1778" citStr="Matuszek et al., 2010" startWordPosition="250" endWordPosition="253">e the creation of natural language interfaces to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annotation and rule authoring by natural language engineering experts are bottlenecks for developing conversational systems for new domains. This paper proposes a fully automated approach to interpreting natural language instructions to complete a task in a virtual world based on unsupervised recordings of human-human interactions performing that task in that virtual world. Given unannotated corpora collected from humans following other humans’ instructions, our system automatically segments the corpus into labeled training data for a cl</context>
</contexts>
<marker>Matuszek, Fox, Koscher, 2010</marker>
<rawString>Cynthia Matuszek, Dieter Fox, and Karl Koscher. 2010. Following directions using statistical machine translation. In Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction, HRI ’10, pages 251–258, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Nau</author>
<author>Malik Ghallab</author>
<author>Paolo Traverso</author>
</authors>
<title>Automated Planning: Theory &amp; Practice.</title>
<date>2004</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>California, USA.</location>
<contexts>
<context position="9211" citStr="Nau et al., 2004" startWordPosition="1459" endWordPosition="1462">r-level action sequences with less noise and less variation. Our discretization algorithm uses an automated planner and a planning representation of the task. This planning representation includes: (1) the task goal, (2) the actions which can be taken in the virtual world, and (3) the current state of the virtual world. Using the planning representation, the planner calculates an optimal path between the starting and ending states of the reaction, eliminating all unnecessary actions. While we use the classical planner FF (Hoffmann, 2003), our technique could also work with classical planning (Nau et al., 2004) or other techniques such as probabilistic planning (Bonet and Geffner, 2005). It is also not dependent on a particular discretization of the world in terms of actions. Now we are ready to define canonical reaction ck formally. Let Sk be the state of the virtual world when instruction uk was uttered, Sk+1 be the state of the world where the reaction ends (as defined by Bhv or Vis segmentation), and D be the planning domain representation of the virtual world. The canonical reaction to uk is defined as the sequence of actions returned by the planner with Sk as initial state, Sk+1 as goal state </context>
</contexts>
<marker>Nau, Ghallab, Traverso, 2004</marker>
<rawString>Dana Nau, Malik Ghallab, and Paolo Traverso. 2004. Automated Planning: Theory &amp; Practice. Morgan Kaufmann Publishers Inc., California, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Masoud Nikravesh</author>
</authors>
<title>Tomohiro Takagi, Masanori Tajima, Akiyoshi Shinmura, Ryosuke Ohgaya, Koji Taniguchi, Kazuyosi Kawahara, Kouta Fukano, and Akiko Aizawa.</title>
<date>2005</date>
<booktitle>Soft Computing for Information Processing and Analysis, volume 164 of Studies in Fuzziness and Soft Computing, chapter 4,</booktitle>
<pages>93--188</pages>
<editor>In Masoud Nikravesh, Lotfi Zadeh, and Janusz Kacprzyk, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>Nikravesh, 2005</marker>
<rawString>Masoud Nikravesh, Tomohiro Takagi, Masanori Tajima, Akiyoshi Shinmura, Ryosuke Ohgaya, Koji Taniguchi, Kazuyosi Kawahara, Kouta Fukano, and Akiko Aizawa. 2005. Soft computing for perception-based decision processing and analysis: Web-based BISCDSS. In Masoud Nikravesh, Lotfi Zadeh, and Janusz Kacprzyk, editors, Soft Computing for Information Processing and Analysis, volume 164 of Studies in Fuzziness and Soft Computing, chapter 4, pages 93– 188. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Orkin</author>
<author>Deb Roy</author>
</authors>
<title>Automatic learning and generation of social behavior from collective human gameplay.</title>
<date>2009</date>
<booktitle>In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent SystemsVolume</booktitle>
<volume>1</volume>
<pages>385--392</pages>
<contexts>
<context position="1286" citStr="Orkin and Roy, 2009" startWordPosition="170" endWordPosition="173">tated, easy-to-collect data from humans interacting with a virtual world. We compare several algorithms for automatically segmenting and discretizing this data into (utterance, reaction) pairs and training a classifier to predict reactions given the next utterance. Our empirical analysis shows that the best algorithm achieves 70% accuracy on this task, with no manual annotation required. 1 Introduction and motivation Mapping instructions into automatically executable actions would enable the creation of natural language interfaces to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annotation and rule authoring by natural language e</context>
</contexts>
<marker>Orkin, Roy, 2009</marker>
<rawString>Jeff Orkin and Deb Roy. 2009. Automatic learning and generation of social behavior from collective human gameplay. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent SystemsVolume 1, volume 1, pages 385–392. International Foundation for Autonomous Agents and Multiagent Systems, International Foundation for Autonomous Agents and Multiagent Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11307" citStr="Papineni et al., 2002" startWordPosition="1806" endWordPosition="1809">ent classification methods. One method uses nearest-neighbor classification with three different similarity metrics: Jaccard and Overlap coefficients (both of which measure the degree of overlap between two sets, differing only in the normalization of the final value (Nikravesh et al., 2005)), and Levenshtein Distance (a string metric for measuring the amount of differences between two sequences of words (Levenshtein, 1966)). Our second classification method employs a strategy in which we considered each group as a set of possible machine translations of our utterance, using the BLEU measure (Papineni et al., 2002) to select which group could be considered the best translation of our utterance. Finally, we trained an SVM classifier (Cortes and Vapnik, 1995) using the unigrams 183 Algorithm Corpus Cm Corpus Cs Bhv Vis Bhv Vis Jaccard 47% 54% 54% 70% Overlap 43% 53% 45% 60% BLEU 44% 52% 54% 50% SVM 33% 29% 45% 29% Levenshtein 21% 20% 8% 17% Table 1: Accuracy comparison between C,,,, and C3 for Bhv and Vis segmentation of each paraphrase and the position of the IF as features, and setting their group as the output class using a libSVM wrapper (Chang and Lin, 2011). When the system misinterprets an instruct</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Learning human multimodal dialogue strategies.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<pages>16--3</pages>
<contexts>
<context position="1826" citStr="Rieser and Lemon, 2010" startWordPosition="258" endWordPosition="261">to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annotation and rule authoring by natural language engineering experts are bottlenecks for developing conversational systems for new domains. This paper proposes a fully automated approach to interpreting natural language instructions to complete a task in a virtual world based on unsupervised recordings of human-human interactions performing that task in that virtual world. Given unannotated corpora collected from humans following other humans’ instructions, our system automatically segments the corpus into labeled training data for a classification algorithm. Our interpretation algor</context>
</contexts>
<marker>Rieser, Lemon, 2010</marker>
<rawString>Verena Rieser and Oliver Lemon. 2010. Learning human multimodal dialogue strategies. Natural Language Engineering, 16:3–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Stoia</author>
<author>Donna K Byron</author>
<author>Darla Magdalene Shockley</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>Sentence planning for realtime navigational instructions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACLShort ’06,</booktitle>
<pages>157--160</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7193" citStr="Stoia et al., 2006" startWordPosition="1127" endWordPosition="1130">because he is reacting to 1 or because he is being proactive. While one could manually annotate this data to remove extraneous actions, our goal is to develop automated solutions that enable learning from massive amounts of data. We decided to approach this problem by experimenting with two alternative formal definitions: 1) a strict definition that considers the maximum reaction according to the IF behavior, and 2) a loose definition based on the empirical observation that, in situated interaction, most instructions are constrained by the current visually perceived affordances (Gibson, 1979; Stoia et al., 2006). We formally define behavior segmentation (Bhv) as follows. A reaction rk to an instruction uk begins 182 right after the instruction uk is uttered and ends right before the next instruction uk+1 is uttered. In the example, instruction 1 corresponds to (2, 3, 4). We formally define visibility segmentation (Vis) as follows. A reaction rk to an instruction uk begins right after the instruction uk is uttered and ends right before the next instruction uk+1 is uttered or right after the IF leaves the area visible at 360◦ from where uk was uttered. In the example, instruction 1’s reaction would be </context>
</contexts>
<marker>Stoia, Byron, Shockley, Fosler-Lussier, 2006</marker>
<rawString>Laura Stoia, Donna K. Byron, Darla Magdalene Shockley, and Eric Fosler-Lussier. 2006. Sentence planning for realtime navigational instructions. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACLShort ’06, pages 157–160, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Dan Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>806--814</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1405" citStr="Vogel and Jurafsky, 2010" startWordPosition="190" endWordPosition="193">ically segmenting and discretizing this data into (utterance, reaction) pairs and training a classifier to predict reactions given the next utterance. Our empirical analysis shows that the best algorithm achieves 70% accuracy on this task, with no manual annotation required. 1 Introduction and motivation Mapping instructions into automatically executable actions would enable the creation of natural language interfaces to many applications (Lau et al., 2009; Branavan et al., 2009; Orkin and Roy, 2009). In this paper, we focus on the task of navigation and manipulation of a virtual environment (Vogel and Jurafsky, 2010; Chen and Mooney, 2011). Current symbolic approaches to the problem are brittle to the natural language variation present in instructions and require intensive rule authoring to be fit for a new task (Dzikovska et al., 2008). Current statistical approaches require extensive manual annotations of the corpora used for training (MacMahon et al., 2006; Matuszek et al., 2010; Gorniak and Roy, 2007; Rieser and Lemon, 2010). Manual annotation and rule authoring by natural language engineering experts are bottlenecks for developing conversational systems for new domains. This paper proposes a fully a</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Dan Jurafsky. 2010. Learning to follow navigational directions. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 806–814, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>