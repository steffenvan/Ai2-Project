<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000917">
<title confidence="0.993695">
High-precision Identification of Discourse New and Unique Noun Phrases
</title>
<author confidence="0.986177">
Olga Uryupina
</author>
<affiliation confidence="0.982459">
Computational Linguistics, Saarland University
</affiliation>
<address confidence="0.941414333333333">
Building 17
Postfach 15 11 50
66041 Saarbr¨ucken, Germany
</address>
<email confidence="0.998575">
ourioupi@coli.uni-sb.de
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996832">
Coreference resolution systems usually at-
tempt to find a suitable antecedent for (al-
most) every noun phrase. Recent studies,
however, show that many definite NPs are
not anaphoric. The same claim, obviously,
holds for the indefinites as well.
We expect our classifiers to provide a good
prefiltering for coreference resolution sys-
tems, improving both their speed and per-
formance.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999432704225352">
Most coreference resolution systems proceed in
the following way: they first identify all the
possible markables (for example, noun phrases)
and then check one by one candidate pairs
, trying to find out whether
the members of those pairs can be coreferent. As the
final step, the pairs are ranked using a scoring algo-
rithm in order to find an appropriate partition of all
the markables into coreference classes.
Those approaches require substantial processing:
in the worst case one has to check candi-
date pairs, where is the total number of mark-
ables found by the system. However, R. Vieira
and M. Poesio have recently shown in (Vieira and
Poesio, 2000) that such an exhaustive search is
not needed, because many noun phrases are not
anaphoric at all — about of definite NPs in their
corpus have no prior referents. Obviously, this num-
ber is even higher if one takes into account all the
other types of NPs — for example, indefinites are
almost always non-anaphoric.
We can conclude that a coreference resolution en-
gine might benefit a lot from a pre-filtering algo-
rithm for identifying non-anaphoric entities. First,
we save much processing time by discarding at least
half of the markables. Second, we can hope to re-
duce the number of mistakes: without pre-filtering,
our coreference resolution system might misclassify
a discourse new entity as coreferent to some previ-
ous one.
However, such a pre-filtering can also decrease
the system’s performance if too many anaphoric NPs
are classified as discourse new: as those NPs are
not processed by the main coreference resolution
module at all, we cannot find correct antecedents
for them. Therefore, we are interested in an algo-
rithm with a good precision, possibly sacrificing its
recall to a reasonable extent. V. Ng and C. Cardie
analysed in (Ng and Cardie, 2002) the impact of
such a prefiltering on their coreference resolution
engine. It turned out that an automatically induced
classifier did not help to improve
the overall performance and even decreased it. How-
ever, when more NPs were considered anaphoric
(that is, the precision for the class
In this study we try to learn automatically
two classifications, and
, relevant for this problem. We
use a small training corpus (MUC-7), but
also acquire some data from the Internet.
Combining our classifiers sequentially, we
achieve 88.9% precision and 84.6% recall
for discourse new entities.
increased and the recall decreased), the prefiltering
resulted in improving the coreference resolution.
Several algorithms for identifying discourse new
entities have been proposed in the literature.
R. Vieira and M. Poesio use hand-crafted heuris-
tics, encoding syntactic information. For exam-
ple, the noun phrase “the inequities of the current
land-ownership system” is classified by their sys-
tem as , because it contains the
restrictive postmodification “of the current land-
ownership system”. This approach leads to 72%
precision and 69% recall for definite discourse new
NPs.
The system described in (Bean and Riloff, 1999)
also makes use of syntactic heuristics. But in ad-
dition the authors mine discourse new entities from
the corpus. Four types of entities can be classified as
non-anaphoric:
</bodyText>
<listItem confidence="0.98661875">
1. having specific syntactic structure,
2. appearing in the first sentence of some text in
the training corpus,
3. exhibiting the same pattern as several expres-
sions of type (2),
4. appearing in the corpus at least 5 times and
always with the definite article (“definites-
only”).
</listItem>
<bodyText confidence="0.977524242424243">
Using various combinations of these methods,
D. Bean and E. Riloff achieved an accuracy for def-
inite non-anaphoric NPs of about (F-
measure), with various combinations of precision
and recall.&apos; This algorithm, however, has two lim-
itations. First, one needs a corpus consisting of
many small texts. Otherwise it is impossible to
find enough non-anaphoric entities of type (2) and,
hence, to collect enough patterns for the entities of
type (3). Second, for an entity to be recognized
as “definite-only”, it should be found in the corpus
at least 5 times. This automatically results in the
data sparseness problem, excluding many infrequent
nouns and NPs.
&apos;Bean and Riloff’s non-anaphoric NPs do not correspond to
our +discourse new ones, but rather to the union of our +dis-
course new and +unique classes.
In our approach we use machine learning to iden-
tify non-anaphoric noun-phrases. We combine syn-
tactic heuristics with the “definite probability”. Un-
like Bean and Riloff, we model definite probability
using the Internet instead of the training corpus it-
self. This helps us to overcome the data sparseness
problem to a large extent. As it has been shown re-
cently in (Keller et al., 2002), Internet counts pro-
duce reliable data for linguistic analysis, correlating
well with corpus counts and plausibility judgements.
The rest of the paper is organised as follows: first
we discuss our NPs classification. In Section 3, we
describe briefly various data sources we used. Sec-
tion 4 provides an explanation of our learning strat-
egy and evaluation results. The approach is sum-
marised in Section 5.
</bodyText>
<sectionHeader confidence="0.994907" genericHeader="method">
2 NP Classification
</sectionHeader>
<bodyText confidence="0.999713545454546">
In our study we follow mainly E. Prince’s classifi-
cation of NPs (Prince, 1981). Prince distinguishes
between the discourse and the hearer givenness.The
resulting taxonomy is summarised below:
brand new NPs introduce entities which are
both discourse and hearer new (“a bus”), sub-
class of them, brand new anchored NPs con-
tain explicit link to some given discourse entity
(“a guy I work with”),
unused NPs introduce discourse new, but
hearer old entities (“Noam Chomsky”),
evoked NPs introduce entities already present
in the discourse model and thus discourse and
hearer old: textually evoked NPs refer to enti-
ties which have already been mentioned in the
previous discourse (“he” in “A guy I worked
with says he knows your sister”), whereas situ-
ationally evoked are known for situational rea-
sons (“you” in “Would you have change of a
quarter?”),
inferrables are not discourse or hearer old,
however, the speaker assumes the hearer can
infer them via logical reasoning from evoked
entities or other inferrables (“the driver” in
“I got on a bus yesterday and the driver was
drunk”), containing inferrables make this in-
ference link explicit (“one of these eggs”).
For our present study we do not need such an elab-
orate classification. Moreover, various experiments
of Vieira and Poesio show that even humans have
difficulties distinguishing, for example, between in-
ferrables and new NPs, or trying to find an anchor
for an inferrable. So, we developed a simple taxon-
omy following the main Prince’s distinction between
the discourse and the hearer givenness.
First, we distinguish between discourse new and
discourse old entities. An entity is considered dis-
course old ( ) if it refers to an ob-
ject or a person mentioned in the previous discourse.
For example, in “The Navy is considering a new
ship that [..] The Navy would like to spend about
S 200 million a year on the arsenal ship..” the
first occurrence of “The Navy” and “a new ship”
are classified as , whereas the sec-
ond occurrence of “The Navy” and “the arsenal
ship” are classified as . It must be
noted that many researchers, in particular, Bean and
Riloff, would consider the second “the Navy” non-
anaphoric, because it fully specifies its referent and
does not require information on the first NP to be in-
terpreted successfully. However, we think that a link
between two instances of “the Navy” can be very
helpful, for example, in the Information Extraction
task. Therefore we treat those NPs as discourse old.
Our class corresponds to Prince’s
textually evoked NPs.
Second, we distinguish between uniquely and
non-uniquely referring expressions. Uniquely refer-
ring expressions ( ) fully specify their refer-
ents and can be successfully interpreted without any
local supportive context. Main part of the
class constitute entities, known to the hearer (reader)
already at the moment when she starts processing
the text, for example “The Mount Everest”. In ad-
dition, an NP (unknown to the reader in the very be-
ginning) is considered unique if it fully specifies its
referent due to its own content only and thus can be
added as it is (maybe, for a very short time) to the
reader’s World knowledge base after the processing
of the text, for example, ”John Smith, chief exec-
utive of John Smith Gmbh” or “the fact that John
Smith is a chief executive of John Smith Gmbh”. In
Prince’s terms our class corresponds to the
unused and, partially, new. In our Navy example (cf.
above) both occurrences of “The Navy” are consid-
ered , whereas “a new ship” and “the ar-
senal ship” are classified as .
</bodyText>
<sectionHeader confidence="0.99391" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.996161043478261">
In our research we use 20 texts from the MUC-
7 corpus (Hirschman and Chinchor, 1997). The
texts were parsed by E. Charniak’s parser (Char-
niak, 2000). Parsing errors were not corrected man-
ually. After this preprocessing step we have 20 lists
of noun phrases.
There are discrepancies between our lists and the
MUC-7 annotations. First, we consider only noun
phrases, whereas MUC-7 takes into account more
types of entities (for example, “his” in “his posi-
tion” should be annotated according to the MUC-7
scheme, but is not included in our lists). Second, the
MUC-7 annotation identifies only markables, partic-
ipating in some coreference chain. Our lists are pro-
duced automatically and thus include all the NPs.
We annotated automatically our NPs as
using the following simple
rule: an NP is considered if and
only if
it is marked in the original MUC-7 corpus, and
it has an antecedent in the MUC-7 corpus (even
if this antecedent does not correspond to any
NP in our corpus).
</bodyText>
<equation confidence="0.720308">
ered :
</equation>
<bodyText confidence="0.999640690476191">
fully specifying the referent without any local
or global context (the chairman of Microsoft
Corporation, 1998, or Washington). We do not
take homonymy into account, so, for example,
Washington is annotated as although
it can refer to many different entities: various
persons, cities, counties, towns, islands, a state,
the government and many others.
time expressions that can be interpreted
uniquely once some starting time point (global
context) is specified. The MUC-7 corpus con-
sists of New York Times News Service articles.
Obviously, they were designed to be read on
some particular day. Thus, for a reader of such
In addition, we annotated our NPs manually as
. The following expressions were consid-
a text, the expressions on Thursday or tomor-
row fully specify their referents. Moreover, the
information on the starting time point can be
easily extracted from the header of the text.
expressions, denoting political or administra-
tive objects (for example, “the Army”). Al-
though such expressions do not fully specify
their referents without an appropriate global
context (many countries have armies), in
an U.S. newspaper they can be interpreted
uniquely.
Overall, we have 3710 noun phrases. 2628 of
them were annotated as and 1082
— as . 2651 NPs were classified
as and 1059 — as . We provide
these data to a machine learning system (Ripper).
Another source of data for our experiments is
the World Wide Web. To model “definite probabil-
ity” for a given NP, we construct various phrases,
for example, “the NP”, and send them to the Al-
taVista search engine. Obtained counts (number of
pages worldwide written in English and containing
the phrases) are used to calculate values for several
“definite probability” features (see Section 4.1 be-
low). We do not use morphological variants in this
study.
</bodyText>
<sectionHeader confidence="0.9836185" genericHeader="method">
4 Identifying Discourse New and Unique
Expressions
</sectionHeader>
<bodyText confidence="0.997238210526316">
In our experiments we want to learn both classifica-
tions and automatically.
However, not every learning algorithm would be ap-
propriate due to the specific requirements we have.
First, we need an algorithm that does not always
require all the features to be specified. For exam-
ple, we might want to calculate “definite probabil-
ity” for a definite NP, but not for a pronoun. We
also don’t want to decide a priori, which features are
important and which ones are not in any particular
case. This requirement rules out such approaches
as Memory-based Learning, Naive Bayes, and many
others. On the contrary, algorithms, providing tree-
or rule-based classifications (for example, C4.5 and
Ripper) would fulfil our first requirement ideally.
Second, we want to control precision-recall trade-
off, at least for the task. For these
reasons we have finally chosen the Ripper learner
(Cohen, 1995).
</bodyText>
<subsectionHeader confidence="0.856391">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.999508">
Our feature set consists currently of 32 features.
They can be divided into three groups:
</bodyText>
<listItem confidence="0.536007272727273">
1. Syntactic Features. We encode part of speech
of the head word and type of the determiner.
Several features contain information on the
characters, constituting the NP’s string (dig-
its, capital and low case letters, special sym-
bols). We use several heuristics for restrictive
postmodification. Two types of appositions are
identified: with and without commas (“Rupert
Murdoch, News Corp.’s chairman and chief ex-
ecutive of�cer,” and “News Corp.’s chairman
and chief executive of�cer Rupert Murdoch”).
</listItem>
<bodyText confidence="0.984856166666667">
In the MUC-7 corpus, appositions of the latter
type are usually annotated as a whole. Char-
niak’s parser, however, analyses these construc-
tions as two NPs ([News Corp.’s chairman
and chief executive of�cer] [Rupert Murdoch]).
Therefore those cases require special treatment.
</bodyText>
<listItem confidence="0.88720575">
2. Context Features. For every NP we calculate
the distance (in NPs and in sentences) to the
previous NP with the same head if such an NP
exists. Obtaining values for these features does
not require exhaustive search when heads are
stored in an appropriate data structure, for ex-
ample, in a trie.
3. “Definite probability” features. Suppose
</listItem>
<bodyText confidence="0.988743666666667">
is a noun phrase, is the same noun phrase
without a determiner, and is its head. We
obtain Internet counts for “Det Y” and “Det
H”, where stays for “the”, “a(n)”, or the
empty string. Then the following ratios are
used as features:
We expect our NPs to behave w.r.t. the “defi-
nite probability” as follows: pronouns and long
proper names are seldom used with any article:
</bodyText>
<table confidence="0.7350413">
” ”
”
”
”
”
Features P R F
All the All 88.5 84.3 86.3
entities Synt+Context 87.9 86 86.9
Definite All 84.8 82.3 83.5
NPs only Synt+Context 82.5 79.3 80.8
</table>
<tableCaption confidence="0.9364965">
Table 1: Precision, Recall, and F-score for the
class
</tableCaption>
<bodyText confidence="0.999673357142857">
“he” was found on the Web 44681672 times,
“the he” — 134978 times (0.3%), and “a he”
— 154204 times (0.3%). Uniques (including
short proper names) and plural non-uniques are
used with the definite article much more of-
ten than with the indefinite one: “government”
was found 23197407 times, “the government”
— 5539661 times (23.9%), and “a govern-
ment” — 1109574 times (4.8%). Singular
non-unique expressions are used only slightly
(if at all) more often with the definite article:
“retailer” was found 1759272 times, “the re-
tailer” — 204551 times (11.6%), and “a re-
tailer” — 309392 times (17.6%).
</bodyText>
<subsectionHeader confidence="0.989823">
4.2 Discourse New entities
</subsectionHeader>
<bodyText confidence="0.998970681818182">
We use Ripper to learn the clas-
sification from the feature representations described
above. The experiment is designed in the follow-
ing way: one text is reserved for testing (we do not
want to split our texts and always process them as
a whole). The remaining 19 texts are first used to
optimise Ripper parameters — class ordering, pos-
sibility of negative tests, hypothesis simplification,
and minimal number of training examples to be cov-
ered by a rule. We perform 5-fold cross-validation
on these 19 texts in order to find the settings with the
best precision for the class. These
settings are then used to train Ripper on all the 19
files and test on the reserved one. The whole proce-
dure is repeated for all the 20 test files and the aver-
age precision and recall are calculated. The parame-
ter “Loss Ratio” (ratio of the cost of a false negative
to the cost of a false positive) is adjusted separately
— we decreased it as much as possible (to 0.3) to
have a classification with a good precision and a rea-
sonable recall.
The automatically induced classifier includes, for
</bodyText>
<table confidence="0.999736714285714">
Optimisation Features P R F
Best prec. All 95.0 83.5 88.9
Synt+Cont. 94.0 84.0 88.7
Best recall All 87.2 97.0 91.8
Synt+Cont. 86.7 96.0 91.1
Best accur. All 87.8 96.6 92.0
Synt+Cont. 87.7 95.6 91.5
</table>
<tableCaption confidence="0.6842895">
Table 2: Precision, Recall, and F-score for the
class
example, the following rules:
R2: (applicable to such NPs as “you”)
</tableCaption>
<figure confidence="0.7699258">
IF an NP is a pronoun,
CLASSIFY it as discourse old.
R14: (applicable to such NPs as “Mexico” or
“the Shuttle”)
IF an NP has no premodifiers,
is more often used with “the” than with “a(n)”
(the ratio is between 2 and 10),
and a same head NP is found within the 18-NPs
window,
CLASSIFY it as discourse old.
</figure>
<bodyText confidence="0.717782">
The performance is shown in table 1.
</bodyText>
<subsectionHeader confidence="0.99652">
4.3 Uniquely Referring Expressions
</subsectionHeader>
<bodyText confidence="0.999954333333333">
Although the “definite probability” features
could not help us much to classify NPs as
, we expect them to be useful for
identifying unique expressions.
We conducted a similar experiment trying to learn
a classifier. The only difference was in
the optimisation strategy: as we did not know a pri-
ori, what was more important, we looked for set-
tings with the best precision for non-uniques, recall
for non-uniques, and overall accuracy (number of
correctly classified items of both classes) separately.
The results are summarised in table 2.
</bodyText>
<subsectionHeader confidence="0.99905">
4.4 Combining two approaches
</subsectionHeader>
<bodyText confidence="0.999532">
Unique and non-unique NPs demonstrate different
behaviour w.r.t. the coreference: discourse entities
are seldom introduced by vague descriptions and
then referred to by fully specifying NPs. Therefore
</bodyText>
<table confidence="0.999631">
P R F
Uniques 85.2 68.8 76.1
Non-uniques 90.4 88.9 89.6
All 88.9 84.6 86.7
</table>
<tableCaption confidence="0.999731">
Table 3: Accuracy of classifica-
</tableCaption>
<bodyText confidence="0.999757916666667">
tion for unique and non-unique NPs separately, all
the features are used
we can expect a unique NP to be discourse new,
if obvious checks for coreference fail. The “obvi-
ous checks” include in our case looking for same
head expressions and appositive constructions, both
of them requiring only constant time.
On the other hand, unique expressions always
have the same or similar form: “The Navy” can
be either discourse new or discourse old. Non-
unique NPs, on the contrary, look differently when
introducing entities (for example, “a company” or
“the company that... ”) and referring to the previ-
ous ones (“it” or “the company” without postmod-
ifiers). Therefore our syntactic features should be
much more helpful when classifying non-uniques as
.
To investigate this difference we conducted an-
other experiment. We split our data into two parts
and . Then we learn the
classification for both parts sepa-
rately as described in section 4.2. Finally the rules
are combined, producing a classifier for all the NPs.
The results are summarised in table 3.
</bodyText>
<subsectionHeader confidence="0.757175">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999964534883721">
As far as the task is concerned,
our system performed slightly, if at all, better with
the definite probability features than without them:
the improvement in precision (our main criterion) is
compensated by the loss in recall. However, when
only definite NPs are taken into account, the im-
provement becomes significant. It’s not surprising,
as these features bring much more information for
definites than for other NPs.
For the classification our definite prob-
ability features were more important, leading to sig-
nificantly better results compared to the case when
only syntactic and context features were used. Al-
though the improvement is only about 0.5%, it must
be taken into account that overall figures are high:
1% improvement on 90% and on 70% accuracy is
not the same. We conducted the t-test to check the
significance of these improvements, using weighted
means and weighted standard deviations, as all the
texts have different sizes. Table 2 shows in bold
performance measures (precision, recall, or F-score)
that improve significantly ( ) when we use
the definite probability features.
As our third experiment shows, non-unique
entities can be classified very reliably into
classes. Uniques, however,
have shown quite poor performance, although
we expected them to be resolved successfully by
heuristics for appositions and same heads. Such a
low performance is mainly due to the fact that many
objects can be referred to by very similar, but not
the same unique NPs: “Lockheed Martin Corp.”,
“Lockheed Martin”, and “Lockheed”, for example,
introduce the same object. We hope to improve
the accuracy by developing more sophisticated
matching rules for unique descriptions.
Although uniques currently perform poorly, the
overall classification still benefits from the sequen-
tial processing (identify first, then learn
classifiers for uniques and non-
uniques separately, and then combine them). And
we hope to get a better overall accuracy once our
matching rules are improved.
</bodyText>
<sectionHeader confidence="0.993533" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99976375">
We have implemented a system for automatic iden-
tification of discourse new and unique entities. To
learn the classification we use a small training cor-
pus (MUC-7). However, much bigger corpus (the
WWW, as indexed by AltaVista) is used to obtain
values for some features. Combining heuristics and
Internet counts we are able to achieve 88.9% preci-
sion and 84.6% recall for discourse new entities.
Our system can also reliably classify NPs as
. The accuracy of this clas-
sification is about 89–92% with various preci-
sion/recall combinations. The classifier provide use-
ful information for coreference resolution in general,
as and descriptions exhibit dif-
ferent behaviour w.r.t. the anaphoricity. This fact
is partially reflected by the performance of our se-
quential classifier (table 3): the context information
is not sufficient to determine whether a unique NP is
a first-mention or not, one has to develop sophisti-
cated names matching techniques instead.
We expect our algorithms to improve both the
speed and the performance of the main corefer-
ence resolution module: once many NPs are dis-
carded, the system can proceed quicker and make
fewer mistakes (for example, almost all the pars-
ing errors were classified by our algorithm as
).
Some issues are still open. First, we need sophis-
ticated rules to compare unique expressions. At the
present stage our system looks only for full matches
and for same head expressions. Thus, “China and
Taiwan” and “Taiwan” (or “China”, depending on
the rules one uses for coordinates’ heads) have much
better chances to be considered coreferent, than
“World Trade Organisation” and “WTO”.
We also plan to conduct more experiments on
the interaction between the and
classifications, treating, for example, time
expressions as , or exploring the influence
of various optimisation strategies for
</bodyText>
<sectionHeader confidence="0.999269" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995159">
David L. Bean and Ellen Riloff. 1999. Corpus-based
Identification of Non-Anaphoric Noun Phrases. Pro-
ceedings ofthe 37th Annual Meeting of the Association
for Computational Linguistics (ACL-99), 373–380.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. Proceedings of the 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-2000), 132–139.
William W. Cohen. 1995. Fast effective rule induction.
Proceedings of the 12th International Conference on
Machine Learning (ICML-95), 115–123.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-7
Coreference Task Definition. Message Understanding
Conference Proceedings.
Frank Keller, Maria Lapata, and Olga Ourioupina. 2002.
Using the Web to Overcome Data Sparseness. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-2002), 230–
237.
Vincent Ng and Claire Cardie. 2002. Identifying
Anaphoric and Non-Anaphoric Noun Phrases to Im-
prove Coreference Resolution. Proceedings of the
Nineteenth International Conference on Computa-
tional Linguistics (COLING-2002), 730–736.
Ellen F. Prince. 1981. Toward a Taxonomy of given-new
information. Radical Pragmatics, 223–256.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539–
594.
</reference>
<bodyText confidence="0.99715">
on
the overall performance of the sequential classifier.
Finally, we still have to estimate the impact of
our pre-filtering algorithm on the overall corefer-
ence resolution performance. Although we expect
the coreference resolution system to benefit from the
and classifiers, this hy-
pothesis has to be verified.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.419088">
<title confidence="0.999942">High-precision Identification of Discourse New and Unique Noun Phrases</title>
<author confidence="0.990057">Olga Uryupina</author>
<affiliation confidence="0.999827">Computational Linguistics, Saarland University</affiliation>
<address confidence="0.818836333333333">Building 17 Postfach 15 11 50 66041 Saarbr¨ucken, Germany</address>
<email confidence="0.999146">ourioupi@coli.uni-sb.de</email>
<abstract confidence="0.979922">Coreference resolution systems usually attempt to find a suitable antecedent for (almost) every noun phrase. Recent studies, however, show that many definite NPs are not anaphoric. The same claim, obviously, holds for the indefinites as well. We expect our classifiers to provide a good prefiltering for coreference resolution systems, improving both their speed and performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David L Bean</author>
<author>Ellen Riloff</author>
</authors>
<title>Corpus-based Identification of Non-Anaphoric Noun Phrases.</title>
<date>1999</date>
<booktitle>Proceedings ofthe 37th Annual Meeting of the Association for Computational Linguistics (ACL-99),</booktitle>
<pages>373--380</pages>
<contexts>
<context position="3646" citStr="Bean and Riloff, 1999" startWordPosition="572" endWordPosition="575">eased and the recall decreased), the prefiltering resulted in improving the coreference resolution. Several algorithms for identifying discourse new entities have been proposed in the literature. R. Vieira and M. Poesio use hand-crafted heuristics, encoding syntactic information. For example, the noun phrase “the inequities of the current land-ownership system” is classified by their system as , because it contains the restrictive postmodification “of the current landownership system”. This approach leads to 72% precision and 69% recall for definite discourse new NPs. The system described in (Bean and Riloff, 1999) also makes use of syntactic heuristics. But in addition the authors mine discourse new entities from the corpus. Four types of entities can be classified as non-anaphoric: 1. having specific syntactic structure, 2. appearing in the first sentence of some text in the training corpus, 3. exhibiting the same pattern as several expressions of type (2), 4. appearing in the corpus at least 5 times and always with the definite article (“definitesonly”). Using various combinations of these methods, D. Bean and E. Riloff achieved an accuracy for definite non-anaphoric NPs of about (Fmeasure), with var</context>
</contexts>
<marker>Bean, Riloff, 1999</marker>
<rawString>David L. Bean and Ellen Riloff. 1999. Corpus-based Identification of Non-Anaphoric Noun Phrases. Proceedings ofthe 37th Annual Meeting of the Association for Computational Linguistics (ACL-99), 373–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2000),</booktitle>
<pages>132--139</pages>
<contexts>
<context position="9409" citStr="Charniak, 2000" startWordPosition="1541" endWordPosition="1543"> it is (maybe, for a very short time) to the reader’s World knowledge base after the processing of the text, for example, ”John Smith, chief executive of John Smith Gmbh” or “the fact that John Smith is a chief executive of John Smith Gmbh”. In Prince’s terms our class corresponds to the unused and, partially, new. In our Navy example (cf. above) both occurrences of “The Navy” are considered , whereas “a new ship” and “the arsenal ship” are classified as . 3 Data In our research we use 20 texts from the MUC7 corpus (Hirschman and Chinchor, 1997). The texts were parsed by E. Charniak’s parser (Charniak, 2000). Parsing errors were not corrected manually. After this preprocessing step we have 20 lists of noun phrases. There are discrepancies between our lists and the MUC-7 annotations. First, we consider only noun phrases, whereas MUC-7 takes into account more types of entities (for example, “his” in “his position” should be annotated according to the MUC-7 scheme, but is not included in our lists). Second, the MUC-7 annotation identifies only markables, participating in some coreference chain. Our lists are produced automatically and thus include all the NPs. We annotated automatically our NPs as u</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2000), 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>Proceedings of the 12th International Conference on Machine Learning (ICML-95),</booktitle>
<pages>115--123</pages>
<contexts>
<context position="12982" citStr="Cohen, 1995" startWordPosition="2128" endWordPosition="2129">xample, we might want to calculate “definite probability” for a definite NP, but not for a pronoun. We also don’t want to decide a priori, which features are important and which ones are not in any particular case. This requirement rules out such approaches as Memory-based Learning, Naive Bayes, and many others. On the contrary, algorithms, providing treeor rule-based classifications (for example, C4.5 and Ripper) would fulfil our first requirement ideally. Second, we want to control precision-recall tradeoff, at least for the task. For these reasons we have finally chosen the Ripper learner (Cohen, 1995). 4.1 Features Our feature set consists currently of 32 features. They can be divided into three groups: 1. Syntactic Features. We encode part of speech of the head word and type of the determiner. Several features contain information on the characters, constituting the NP’s string (digits, capital and low case letters, special symbols). We use several heuristics for restrictive postmodification. Two types of appositions are identified: with and without commas (“Rupert Murdoch, News Corp.’s chairman and chief executive of�cer,” and “News Corp.’s chairman and chief executive of�cer Rupert Murdo</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William W. Cohen. 1995. Fast effective rule induction. Proceedings of the 12th International Conference on Machine Learning (ICML-95), 115–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Nancy Chinchor</author>
</authors>
<date>1997</date>
<booktitle>MUC-7 Coreference Task Definition. Message Understanding Conference Proceedings.</booktitle>
<contexts>
<context position="9345" citStr="Hirschman and Chinchor, 1997" startWordPosition="1529" endWordPosition="1532">ly specifies its referent due to its own content only and thus can be added as it is (maybe, for a very short time) to the reader’s World knowledge base after the processing of the text, for example, ”John Smith, chief executive of John Smith Gmbh” or “the fact that John Smith is a chief executive of John Smith Gmbh”. In Prince’s terms our class corresponds to the unused and, partially, new. In our Navy example (cf. above) both occurrences of “The Navy” are considered , whereas “a new ship” and “the arsenal ship” are classified as . 3 Data In our research we use 20 texts from the MUC7 corpus (Hirschman and Chinchor, 1997). The texts were parsed by E. Charniak’s parser (Charniak, 2000). Parsing errors were not corrected manually. After this preprocessing step we have 20 lists of noun phrases. There are discrepancies between our lists and the MUC-7 annotations. First, we consider only noun phrases, whereas MUC-7 takes into account more types of entities (for example, “his” in “his position” should be annotated according to the MUC-7 scheme, but is not included in our lists). Second, the MUC-7 annotation identifies only markables, participating in some coreference chain. Our lists are produced automatically and t</context>
</contexts>
<marker>Hirschman, Chinchor, 1997</marker>
<rawString>Lynette Hirschman and Nancy Chinchor. 1997. MUC-7 Coreference Task Definition. Message Understanding Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Maria Lapata</author>
<author>Olga Ourioupina</author>
</authors>
<title>Using the Web to Overcome Data Sparseness.</title>
<date>2002</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2002),</booktitle>
<pages>230--237</pages>
<contexts>
<context position="5284" citStr="Keller et al., 2002" startWordPosition="843" endWordPosition="846">ally results in the data sparseness problem, excluding many infrequent nouns and NPs. &apos;Bean and Riloff’s non-anaphoric NPs do not correspond to our +discourse new ones, but rather to the union of our +discourse new and +unique classes. In our approach we use machine learning to identify non-anaphoric noun-phrases. We combine syntactic heuristics with the “definite probability”. Unlike Bean and Riloff, we model definite probability using the Internet instead of the training corpus itself. This helps us to overcome the data sparseness problem to a large extent. As it has been shown recently in (Keller et al., 2002), Internet counts produce reliable data for linguistic analysis, correlating well with corpus counts and plausibility judgements. The rest of the paper is organised as follows: first we discuss our NPs classification. In Section 3, we describe briefly various data sources we used. Section 4 provides an explanation of our learning strategy and evaluation results. The approach is summarised in Section 5. 2 NP Classification In our study we follow mainly E. Prince’s classification of NPs (Prince, 1981). Prince distinguishes between the discourse and the hearer givenness.The resulting taxonomy is </context>
</contexts>
<marker>Keller, Lapata, Ourioupina, 2002</marker>
<rawString>Frank Keller, Maria Lapata, and Olga Ourioupina. 2002. Using the Web to Overcome Data Sparseness. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2002), 230– 237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying Anaphoric and Non-Anaphoric Noun Phrases to Improve Coreference Resolution.</title>
<date>2002</date>
<booktitle>Proceedings of the Nineteenth International Conference on Computational Linguistics (COLING-2002),</booktitle>
<pages>730--736</pages>
<contexts>
<context position="2436" citStr="Ng and Cardie, 2002" startWordPosition="387" endWordPosition="390">ond, we can hope to reduce the number of mistakes: without pre-filtering, our coreference resolution system might misclassify a discourse new entity as coreferent to some previous one. However, such a pre-filtering can also decrease the system’s performance if too many anaphoric NPs are classified as discourse new: as those NPs are not processed by the main coreference resolution module at all, we cannot find correct antecedents for them. Therefore, we are interested in an algorithm with a good precision, possibly sacrificing its recall to a reasonable extent. V. Ng and C. Cardie analysed in (Ng and Cardie, 2002) the impact of such a prefiltering on their coreference resolution engine. It turned out that an automatically induced classifier did not help to improve the overall performance and even decreased it. However, when more NPs were considered anaphoric (that is, the precision for the class In this study we try to learn automatically two classifications, and , relevant for this problem. We use a small training corpus (MUC-7), but also acquire some data from the Internet. Combining our classifiers sequentially, we achieve 88.9% precision and 84.6% recall for discourse new entities. increased and th</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Identifying Anaphoric and Non-Anaphoric Noun Phrases to Improve Coreference Resolution. Proceedings of the Nineteenth International Conference on Computational Linguistics (COLING-2002), 730–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen F Prince</author>
</authors>
<title>Toward a Taxonomy of given-new information.</title>
<date>1981</date>
<booktitle>Radical Pragmatics,</booktitle>
<pages>223--256</pages>
<contexts>
<context position="5788" citStr="Prince, 1981" startWordPosition="927" endWordPosition="928">vercome the data sparseness problem to a large extent. As it has been shown recently in (Keller et al., 2002), Internet counts produce reliable data for linguistic analysis, correlating well with corpus counts and plausibility judgements. The rest of the paper is organised as follows: first we discuss our NPs classification. In Section 3, we describe briefly various data sources we used. Section 4 provides an explanation of our learning strategy and evaluation results. The approach is summarised in Section 5. 2 NP Classification In our study we follow mainly E. Prince’s classification of NPs (Prince, 1981). Prince distinguishes between the discourse and the hearer givenness.The resulting taxonomy is summarised below: brand new NPs introduce entities which are both discourse and hearer new (“a bus”), subclass of them, brand new anchored NPs contain explicit link to some given discourse entity (“a guy I work with”), unused NPs introduce discourse new, but hearer old entities (“Noam Chomsky”), evoked NPs introduce entities already present in the discourse model and thus discourse and hearer old: textually evoked NPs refer to entities which have already been mentioned in the previous discourse (“he</context>
</contexts>
<marker>Prince, 1981</marker>
<rawString>Ellen F. Prince. 1981. Toward a Taxonomy of given-new information. Radical Pragmatics, 223–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renata Vieira</author>
<author>Massimo Poesio</author>
</authors>
<title>An empirically-based system for processing definite descriptions.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<pages>594</pages>
<contexts>
<context position="1273" citStr="Vieira and Poesio, 2000" startWordPosition="194" endWordPosition="197">tems proceed in the following way: they first identify all the possible markables (for example, noun phrases) and then check one by one candidate pairs , trying to find out whether the members of those pairs can be coreferent. As the final step, the pairs are ranked using a scoring algorithm in order to find an appropriate partition of all the markables into coreference classes. Those approaches require substantial processing: in the worst case one has to check candidate pairs, where is the total number of markables found by the system. However, R. Vieira and M. Poesio have recently shown in (Vieira and Poesio, 2000) that such an exhaustive search is not needed, because many noun phrases are not anaphoric at all — about of definite NPs in their corpus have no prior referents. Obviously, this number is even higher if one takes into account all the other types of NPs — for example, indefinites are almost always non-anaphoric. We can conclude that a coreference resolution engine might benefit a lot from a pre-filtering algorithm for identifying non-anaphoric entities. First, we save much processing time by discarding at least half of the markables. Second, we can hope to reduce the number of mistakes: withou</context>
</contexts>
<marker>Vieira, Poesio, 2000</marker>
<rawString>Renata Vieira and Massimo Poesio. 2000. An empirically-based system for processing definite descriptions. Computational Linguistics, 26(4):539– 594.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>