<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021855">
<title confidence="0.9970925">
Easy Web Search Results Clustering:
When Baselines Can Reach State-of-the-Art Algorithms
</title>
<author confidence="0.968059">
Jose G. Moreno
</author>
<affiliation confidence="0.970542">
Normandie University
</affiliation>
<address confidence="0.6997005">
UNICAEN, GREYC CNRS
F-14032 Caen, France
</address>
<email confidence="0.998882">
jose.moreno@unicaen.fr
</email>
<sectionHeader confidence="0.993895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999617">
This work discusses the evaluation of
baseline algorithms for Web search re-
sults clustering. An analysis is performed
over frequently used baseline algorithms
and standard datasets. Our work shows
that competitive results can be obtained by
either fine tuning or performing cascade
clustering over well-known algorithms. In
particular, the latter strategy can lead to
a scalable and real-world solution, which
evidences comparative results to recent
text-based state-of-the-art algorithms.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999806304347826">
Visualizing Web search results remains an open
problem in Information Retrieval (IR). For exam-
ple, in order to deal with ambiguous or multi-
faceted queries, many works present Web page re-
sults using groups of correlated contents instead
of long flat lists of relevant documents. Among
existing techniques, Web Search Results Cluster-
ing (SRC) is a commonly studied area, which
consists in clustering “on-the-fly” Web page re-
sults based on their Web snippets. Therefore,
many works have been recently presented includ-
ing task adapted clustering (Moreno et al., 2013),
meta clustering (Carpineto and Romano, 2010)
and knowledge-based clustering (Scaiella et al.,
2012).
Evaluation is also a hot topic both in Natural
Language Processing (NLP) and IR. Within the
specific case of SRC, different metrics have been
used such as F1-measure (F1), k55L1 and Fbs-
measure (Fbs) over different standard datasets:
ODP-239 (Carpineto and Romano, 2010) and
Moresque (Navigli and Crisafulli, 2010). Unfor-
tunately, comparative results are usually biased as
</bodyText>
<footnote confidence="0.999287">
1This metric is based on subjective label evaluation and as
such is out of the scope of this paper.
</footnote>
<note confidence="0.798831">
Ga¨el Dias
Normandie University
UNICAEN, GREYC CNRS
F-14032 Caen, France
</note>
<email confidence="0.98296">
gael.dias@unicaen.fr
</email>
<bodyText confidence="0.999533133333333">
baseline algorithms are run with default parame-
ters whereas proposed methodologies are usually
tuned to increase performance over the studied
datasets. Moreover, evaluation metrics tend to cor-
relate with the number of produced clusters.
In this paper, we focus on deep understand-
ing of the evaluation task within the context of
SRC. First, we provide the results of baseline algo-
rithms with their best parameter settings. Second,
we show that a simple cascade strategy of base-
line algorithms can lead to a scalable and real-
world solution, which evidences comparative re-
sults to recent text-based algorithms. Finally, we
draw some conclusions about evaluation metrics
and their bias to the number of output clusters.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.957454625">
Search results clustering is an active research area.
Two main streams have been proposed so far:
text-based strategies such as (Hearst and Peder-
sen, 1996; Zamir and Etzioni, 1998; Zeng et al.,
2004; Osinski et al., 2004; Carpineto and Romano,
2010; Carpineto et al., 2011; Moreno et al., 2013)
and knowledge-based ones (Ferragina and Gulli,
2008; Scaiella et al., 2012; Di Marco and Nav-
igli, 2013). Successful results have been obtained
by recent works compared to STC (Zamir and Et-
zioni, 1998) and LINGO (Osinski et al., 2004)
which provide publicly available implementations,
and as a consequence, are often used as state-
of-the-art baselines. On the one hand, STC pro-
poses a monothetic methodology which merges
base clusters with high string overlap relying on
suffix trees. On the other hand, LINGO is a poly-
thetic solution which reduces a term-document
matrix using single value decomposition and as-
signs documents to each discovered latent topic.
All solutions have been evaluated on differ-
ent datasets and evaluation measures. The well-
known F1 has been used as the standard evaluation
metric. More recently, (Carpineto and Romano,
</bodyText>
<page confidence="0.71644">
1
</page>
<note confidence="0.8166625">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 1–5,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.995611571428571">
Moresque ODP-239
F1 Fb3 F1 Fb3
Algo. Stand. k Tuned k Stand. k Tuned k Stand. k Tuned k Stand. k Tuned k
STC 0.4550 12.7 0.6000 2.9 0.4602 12.7 0.4987 2.9 0.3238 12.4 0.3350 3.0 0.4027 12.4 0.4046 14.5
LINGO 0.3258 26.7 0.6034 3.0 0.3989 26.7 0.5004 5.8 0.2029 27.7 0.3320 3.0 0.3461 27.7 0.4459 8.7
BiKm 0.3165 9.7 0.5891 2.1 0.3145 9.7 0.4240 2.1 0.1995 12.1 0.3381 2.2 0.3074 12.1 0.3751 2.2
Random - - 0.5043 2 - - 0.3548 2 - - 0.2980 2 - - 0.3212 2
</table>
<tableCaption confidence="0.999855">
Table 1: Standard, Tuned and Random Results for Moresque and ODP-239 datasets.
</tableCaption>
<bodyText confidence="0.998764115384615">
2010) evidenced more complete results with the
general definition of the Fβ-measure for Q =
{1, 2, 5}, (Navigli and Crisafulli, 2010) introduced
the Rand Index metric and (Moreno et al., 2013)
used Fb3 introduced by (Amig´o et al., 2009) as a
more adequate metric for clustering.
Different standard datasets have been built such
as AMBIENT2 (Carpineto and Romano, 2009),
ODP-2393 (Carpineto and Romano, 2010) and
Moresque4 (Navigli and Crisafulli, 2010). ODP-
239, an improved version of AMBIENT, is based
on DMOZ5 where each query, over 239 ones, is a
selected category in DMOZ and its associated sub-
categories are considered as the respective clus-
ter results. The small text description included in
DMOZ is considered as a Web snippet. Moresque
is composed by 114 queries selected from a list
of ambiguous Wikipedia entries. For each query, a
set of Web results have been collected from a com-
mercial search engine and manually classified into
the disambiguation Wikipedia pages which form
the reference clusters.
In Table 2, we report the results obtained so
far in the literature by text-based and knowledge-
based strategies for the standard F1 over ODP-239
and Moresque datasets.
</bodyText>
<table confidence="0.993839375">
F1
ODP239 Moresque
Text STC 0.324 0.455
LINGO 0.273 0.326
(Carpineto and Romano, 2010) 0.313 -
(Moreno et al., 2013) 0.390 0.665
Know. (Scaiella et al., 2012) 0.413 -
(Di Marco and Navigli, 2013) - 0.7204*
</table>
<tableCaption confidence="0.638356666666667">
Table 2: State-of-the-art Results for SRC. (*) The
result of (Di Marco and Navigli, 2013) is based
on a reduced version of AMBIENT + Moresque.
</tableCaption>
<sectionHeader confidence="0.99656" genericHeader="method">
3 Baseline SRC Algorithms
</sectionHeader>
<bodyText confidence="0.998319333333333">
Newly proposed algorithms are usually tuned to-
wards their maximal performance. However, the
results of baseline algorithms are usually run with
</bodyText>
<footnote confidence="0.999971">
2http://credo.fub.it/ambient/ [Last acc.: Jan., 2014]
3http://credo.fub.it/odp239/ [Last acc.: Jan., 2014]
4http://lcl.uniroma1.it/moresque/ [Last acc.: Jan., 2014]
5http://www.dmoz.org [Last acc.: Jan., 2014]
</footnote>
<bodyText confidence="0.999418944444445">
default parameters based on available implemen-
tations. As such, no conclusive remarks can be
drawn knowing that tuned versions might provide
improved results.
In particular, available implementations6 of
STC, LINGO and the Bisection K-means (BiKm)
include a fixed stopping criterion. However, it
is well-known that tuning the number of output
clusters may greatly impact the clustering perfor-
mance. In order to provide fair results for base-
line algorithms, we evaluated a k-dependent7 ver-
sion for all baselines. We ran all algorithms for
k = 2..20 and chose the best result as the “op-
timal” performance. Table 1 sums up results for
all the baselines in their different configurations
and shows that tuned versions outperform standard
(available) ones both for F1 and Fb3 over ODP-
239 and Moresque.
</bodyText>
<sectionHeader confidence="0.991484" genericHeader="method">
4 Cascade SRC Algorithms
</sectionHeader>
<bodyText confidence="0.999936619047619">
In the previous section, our aim was to claim that
tunable versions of existing baseline algorithms
might evidence improved results when faced to
the ones reported in the literature. And these
values should be taken as the “real” baseline re-
sults within the context of controllable environ-
ments. However, exploring all the parameter space
is not an applicable solution in a real-world situa-
tion where the reference is unknown. As such, a
stopping criterion must be defined to adapt to any
dataset distribution. This is the particular case for
the standard implementations of STC and LINGO.
Previous results (Carpineto and Romano, 2010)
showed that different SRC algorithms provide dif-
ferent results and hopefully complementary ones.
For instance, STC demonstrates high recall and
low precision, while LINGO inversely evidences
high precision for low recall. Iteratively apply-
ing baseline SRC algorithms may thus lead to
improved results by exploiting each algorithm’s
strengths.
</bodyText>
<footnote confidence="0.986598666666667">
6http://carrot2.org [Last acc.: Jan., 2014]
7Carrot2 parameters maxClusters, desiredClusterCount-
Base and clusterCount were used to set k value.
</footnote>
<page confidence="0.992174">
2
</page>
<bodyText confidence="0.999991032258065">
In a cascade strategy, we first cluster the ini-
tial set of Web page snippets with any SRC al-
gorithm. Then, the input of the second SRC al-
gorithm is the set of meta-documents built from
the documents belonging to the same cluster8. Fi-
nally, each clustered meta-document is mapped to
the original documents generating the final clus-
ters. This process can iteratively be applied, al-
though we only consider two-level cascade strate-
gies in this paper.
This strategy can be viewed as an easy, re-
producible and parameter free baseline SRC im-
plementation that should be compared to existing
state-of-the-art algorithms. Table 3 shows the re-
sults obtained with different combinations of SRC
baseline algorithms for the cascade strategy both
for F1 and Fb3 over ODP-239 and Moresque. The
“Stand.” column corresponds to the performance
of the cascade strategy and k to the automatically
obtained number of clusters. Results show that
the combination STC-STC achieves the best per-
formance overall for the F1 and STC-LINGO is
the best combination for the Fb3 in both datasets.
In order to provide a more complete evaluation,
we included in column “Equiv.” the performance
that could be obtained by the tunable version of
each single baseline algorithm based on the same
k. Interestingly, the cascade strategy outperforms
the tunable version for any k for F1 but fails to
compete (not by far) with Fb3. This issue will be
discussed in the next section.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999875529411765">
In Table 1, one can see that when using the tuned
version and evaluating with F1, the best perfor-
mance for each baseline algorithm is obtained for
the same number of output clusters independently
of the dataset (i.e. around 3 for STC and LINGO
and 2 for BiKm). As such, a fast conclusion would
be that the tuned versions of STC, LINGO and
BiKm are strong baselines as they show similar
behaviour over datasets. Then, in a realistic situa-
tion, k might be directly tuned to these values.
However, when comparing the output number
of clusters based on the best F1 value to the refer-
ence number of clusters, a huge difference is ev-
idenced. Indeed, in Moresque, the ground-truth
average number of clusters is 6.6 and exactly 10
in ODP-239. Interestingly, Fb3 shows more accu-
rate values for the number of output clusters for
</bodyText>
<subsectionHeader confidence="0.43258">
8Fused using concatenation of strings.
</subsectionHeader>
<bodyText confidence="0.999961777777778">
the best tuned baseline performances. In particu-
lar, the best Fb3 results are obtained for LINGO
with 5.8 clusters for Moresque and 8.7 clusters
for ODP-239 which most approximate the ground-
truths.
In order to better understand the behaviour of
each evaluation metric (i.e. Fβ and Fb3) over dif-
ferent k values, we experienced a uniform random
clustering over Moresque and ODP-239. In Fig-
ure 1(c), we illustrate these results. The important
issue is that Fβ is more sensitive to the number
of output clusters than Fb3. On the one hand, all
Fβ measures provide best results for k = 2 and
a random algorithm could reach F1=0.5043 for
Moresque and F1=0.2980 for ODP-239 (see Ta-
ble 1), thus outperforming almost all standard im-
plementations of STC, LINGO and BiKm for both
datasets. On the other hand, Fb3 shows that most
standard baseline implementations outperform the
random algorithm.
Moreover, in Figures 1(a) and 1(b), we illus-
trate the different behaviours between F1 and Fb3
for k = 2..20 for both standard and tuned ver-
sions of STC, LINGO and BiKm. One may clearly
see that Fb3 is capable to discard the algorithm
(BiKm) which performs worst in the standard ver-
sion while this is not the case for F1. And, for
LINGO, the optimal performances over Moresque
and ODP-239 are near the ground-truth number of
clusters while this is not the case for F1 which ev-
idences a decreasing tendency when k increases.
In section 4, we showed that competitive results
could be achieved with a cascade strategy based on
baseline algorithms. Although results outperform
standard and tunable baseline implementations for
F1, it is wise to use Fb3 to better evaluate the SRC
task, based on our previous discussion. In this
case, the best values are obtained by STC-LINGO
with Fb3=0.4980 for Moresque and Fb3=0.4249
for ODP-239, which highly approximate the val-
ues reported in (Moreno et al., 2013): Fb3=0.490
(Moresque) and Fb3=0.452 (ODP-239). Addition-
ally, when STC is performed first and LINGO later
the cascade algorithm scale better due to LINGO
and STC scaling properties9.
</bodyText>
<sectionHeader confidence="0.998875" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9987985">
This work presents a discussion about the use of
baseline algorithms in SRC and evaluation met-
</bodyText>
<footnote confidence="0.995818">
9http://carrotsearch.com/lingo3g-comparison [Last acc.:
Jan., 2014]
</footnote>
<page confidence="0.995554">
3
</page>
<table confidence="0.999654">
Moresque ODP-239
F1 Fb3 F1 Fb3
Level 1 Level 2 Stand. Equiv. k Stand. Equiv. k Stand. Equiv. k Stand. Equiv. k
STC STC 0.6145 0.5594 3.1 0.4550 0.4913 3.1 0.3629 0.3304 3.2 0.3982 0.4023 3.2
LINGO 0.5611 0.4932 7.3 0.4980 0.4716 7.3 0.3624 0.3258 6.9 0.4249 0.4010 6.9
BiKm 0.5413 0.5160 4.5 0.4395 0.4776 4.5 0.3319 0.3276 4.3 0.3845 0.4020 4.3
LINGO STC 0.5696 0.5176 6.7 0.4602 0.4854 6.7 0.3457 0.3029 7.2 0.4229 0.4429 7.2
LINGO 0.4629 0.4371 13.7 0.4447 0.4566 13.7 0.2789 0.2690 13.6 0.3931 0.4237 13.6
BiKm 0.4038 0.4966 8.6 0.3801 0.4750 8.6 0.2608 0.2953 8.5 0.3510 0.4423 8.5
BiKm STC 0.5873 0.5891 2.7 0.4144 0.4069 2.7 0.3425 0.3381 2.7 0.3787 0.3677 2.7
LINGO 0.4773 0.5186 5.4 0.3832 0.3869 5.4 0.2819 0.3191 6.3 0.3546 0.3644 6.3
BiKm 0.4684 0.5764 3.5 0.3615 0.4114 3.5 0.2767 0.3322 4.3 0.3328 0.3693 4.3
</table>
<tableCaption confidence="0.999767">
Table 3: Cascade Results for Moresque and ODP-239 datasets.
</tableCaption>
<figure confidence="0.999851779816513">
(a) F1 for Moresque (Left) and ODP-239 (Right).
0.65
0.6
0.55
0.5
F1
0.45
0.4
0.35
0.3
0.36
0.34
0.32
0.3
0.28
F1
0.26
0.24
0.22
0.2
0.18
BiKm(Tuned)
STC(Tuned)
LINGO(Tuned)
BiKm(Stand.)
STC(Stand.)
LINGO(Stand.)
2 4 6 8 10 12 14 16 18 20
k
BiKm(Tuned)
STC(Tuned)
LINGO(Tuned)
BiKm(Stand.)
STC(Stand.)
LINGO(Stand.)
2 4 6 8 10 12 14 16 18 20
k
(b) Fb3 for Moresque (Left) and ODP-239 (Right).
BiKm(Tuned)
STC(Tuned)
LINGO(Tuned)
BiKm(Stand.)
STC(Stand.)
LINGO(Stand.)
2 4 6 8 10 12 14 16 18 20
k
BiKm(Tuned)
STC(Tuned)
LINGO(Tuned)
BiKm(Stand.)
STC(Stand.)
LINGO(Stand.)
2 4 6 8 10 12 14 16 18 20
k
Fbcubed 0.52
0.5
0.48
0.46
0.44
0.42
0.4
0.38
0.36
0.34
0.32
0.3
Fbcubed 0.46
0.44
0.42
0.4
0.38
0.36
0.34
0.32
0.3
(c) Evaluation Metrics for Random Clustering for Moresque (Left) and ODP-239 (Right).
2 4 6 8 10 12 14 16 18 20
k
2 4 6 8 10 12 14 16 18 20
k
F1
F2
F5
Fbcubed
Performance 0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
Performance 0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
F1
F2
F5
Fbcubed
</figure>
<figureCaption confidence="0.999993">
Figure 1: F1 and Fb3 for Moresque and ODP-239 for Standard, Tuned and Random Clustering.
</figureCaption>
<bodyText confidence="0.994798714285714">
rics. Our experiments show that Fb3 seems more
adapted to evaluate SRC systems than the com-
monly used F1 over the standard datasets avail-
able so far. New baseline values which approxi-
mate state-of-the-art algorithms in terms of clus-
tering performance can also be obtained by an
easy, reproducible and parameter free implemen-
tation (the cascade strategy) and could be consid-
ered as the “new” baseline results for future works.
H.J. Zeng, Q.C. He, Z. Chen, W.Y. Ma, and J. Ma.
2004. Learning to cluster web search results. In
27th Annual International Conference on Research
and Development in Information Retrieval (SIGIR),
pages 210–217.
</bodyText>
<sectionHeader confidence="0.872094" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.94695025">
E. Amig´o, J. Gonzalo, J. Artiles, and F. Verdejo. 2009.
A comparison of extrinsic clustering evaluation met-
rics based on formal constraints. Information Re-
trieval, 12(4):461–486.
</bodyText>
<reference confidence="0.999606456521739">
C. Carpineto and G. Romano. 2009. Mobile infor-
mation retrieval with search results clustering : Pro-
totypes and evaluations. Journal of the American
Society for Information Science, 60:877–895.
C. Carpineto and G. Romano. 2010. Optimal meta
search results clustering. In 33rd International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 170–177.
C. Carpineto, M. D’Amico, and A. Bernardini. 2011.
Full discrimination of subtopics in search results
with keyphrase-based clustering. Web Intelligence
and Agent Systems, 9(4):337–349.
A. Di Marco and R. Navigli. 2013. Clustering and
diversifying web search results with graph-based
word sense induction. Computational Linguistics,
39(3):709–754.
P. Ferragina and A. Gulli. 2008. A personalized search
engine based on web-snippet hierarchical clustering.
Software: Practice and Experience, 38(2):189–225.
M.A. Hearst and J.O. Pedersen. 1996. Re-examining
the cluster hypothesis: Scatter/gather on retrieval re-
sults. In 19th Annual International Conference on
Research and Development in Information Retrieval
(SIGIR), pages 76–84.
J.G. Moreno, G. Dias, and G. Cleuziou. 2013. Post-
retrieval clustering using third-order similarity mea-
sures. In 51st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 153–158.
R. Navigli and G. Crisafulli. 2010. Inducing word
senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 116–126.
S. Osinski, J. Stefanowski, and D. Weiss. 2004. Lingo:
Search results clustering algorithm based on singu-
lar value decomposition. In Intelligent Information
Systems Conference (IIPWM), pages 369–378.
U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita.
2012. Topical clustering of search results. In 5th
ACM International Conference on Web Search and
Data Mining (WSDM), pages 223–232.
O. Zamir and O. Etzioni. 1998. Web document clus-
tering: A feasibility demonstration. In 21st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 46–54.
</reference>
<page confidence="0.99433">
5
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.544012">
<title confidence="0.997937">Easy Web Search Results Clustering: When Baselines Can Reach State-of-the-Art Algorithms</title>
<author confidence="0.992671">G Jose</author>
<affiliation confidence="0.7698605">Normandie UNICAEN, GREYC</affiliation>
<address confidence="0.981873">F-14032 Caen,</address>
<email confidence="0.998982">jose.moreno@unicaen.fr</email>
<abstract confidence="0.998672076923077">This work discusses the evaluation of baseline algorithms for Web search results clustering. An analysis is performed over frequently used baseline algorithms and standard datasets. Our work shows that competitive results can be obtained by either fine tuning or performing cascade clustering over well-known algorithms. In particular, the latter strategy can lead to a scalable and real-world solution, which evidences comparative results to recent text-based state-of-the-art algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Carpineto</author>
<author>G Romano</author>
</authors>
<title>Mobile information retrieval with search results clustering : Prototypes and evaluations.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>60--877</pages>
<contexts>
<context position="4915" citStr="Carpineto and Romano, 2009" startWordPosition="769" endWordPosition="772">0 0.3461 27.7 0.4459 8.7 BiKm 0.3165 9.7 0.5891 2.1 0.3145 9.7 0.4240 2.1 0.1995 12.1 0.3381 2.2 0.3074 12.1 0.3751 2.2 Random - - 0.5043 2 - - 0.3548 2 - - 0.2980 2 - - 0.3212 2 Table 1: Standard, Tuned and Random Results for Moresque and ODP-239 datasets. 2010) evidenced more complete results with the general definition of the Fβ-measure for Q = {1, 2, 5}, (Navigli and Crisafulli, 2010) introduced the Rand Index metric and (Moreno et al., 2013) used Fb3 introduced by (Amig´o et al., 2009) as a more adequate metric for clustering. Different standard datasets have been built such as AMBIENT2 (Carpineto and Romano, 2009), ODP-2393 (Carpineto and Romano, 2010) and Moresque4 (Navigli and Crisafulli, 2010). ODP239, an improved version of AMBIENT, is based on DMOZ5 where each query, over 239 ones, is a selected category in DMOZ and its associated subcategories are considered as the respective cluster results. The small text description included in DMOZ is considered as a Web snippet. Moresque is composed by 114 queries selected from a list of ambiguous Wikipedia entries. For each query, a set of Web results have been collected from a commercial search engine and manually classified into the disambiguation Wikiped</context>
</contexts>
<marker>Carpineto, Romano, 2009</marker>
<rawString>C. Carpineto and G. Romano. 2009. Mobile information retrieval with search results clustering : Prototypes and evaluations. Journal of the American Society for Information Science, 60:877–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Carpineto</author>
<author>G Romano</author>
</authors>
<title>Optimal meta search results clustering.</title>
<date>2010</date>
<booktitle>In 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>170--177</pages>
<contexts>
<context position="1312" citStr="Carpineto and Romano, 2010" startWordPosition="185" endWordPosition="188">s. 1 Introduction Visualizing Web search results remains an open problem in Information Retrieval (IR). For example, in order to deal with ambiguous or multifaceted queries, many works present Web page results using groups of correlated contents instead of long flat lists of relevant documents. Among existing techniques, Web Search Results Clustering (SRC) is a commonly studied area, which consists in clustering “on-the-fly” Web page results based on their Web snippets. Therefore, many works have been recently presented including task adapted clustering (Moreno et al., 2013), meta clustering (Carpineto and Romano, 2010) and knowledge-based clustering (Scaiella et al., 2012). Evaluation is also a hot topic both in Natural Language Processing (NLP) and IR. Within the specific case of SRC, different metrics have been used such as F1-measure (F1), k55L1 and Fbsmeasure (Fbs) over different standard datasets: ODP-239 (Carpineto and Romano, 2010) and Moresque (Navigli and Crisafulli, 2010). Unfortunately, comparative results are usually biased as 1This metric is based on subjective label evaluation and as such is out of the scope of this paper. Ga¨el Dias Normandie University UNICAEN, GREYC CNRS F-14032 Caen, Franc</context>
<context position="2914" citStr="Carpineto and Romano, 2010" startWordPosition="436" endWordPosition="439">e results of baseline algorithms with their best parameter settings. Second, we show that a simple cascade strategy of baseline algorithms can lead to a scalable and realworld solution, which evidences comparative results to recent text-based algorithms. Finally, we draw some conclusions about evaluation metrics and their bias to the number of output clusters. 2 Related Work Search results clustering is an active research area. Two main streams have been proposed so far: text-based strategies such as (Hearst and Pedersen, 1996; Zamir and Etzioni, 1998; Zeng et al., 2004; Osinski et al., 2004; Carpineto and Romano, 2010; Carpineto et al., 2011; Moreno et al., 2013) and knowledge-based ones (Ferragina and Gulli, 2008; Scaiella et al., 2012; Di Marco and Navigli, 2013). Successful results have been obtained by recent works compared to STC (Zamir and Etzioni, 1998) and LINGO (Osinski et al., 2004) which provide publicly available implementations, and as a consequence, are often used as stateof-the-art baselines. On the one hand, STC proposes a monothetic methodology which merges base clusters with high string overlap relying on suffix trees. On the other hand, LINGO is a polythetic solution which reduces a term</context>
<context position="4954" citStr="Carpineto and Romano, 2010" startWordPosition="774" endWordPosition="777">7 0.5891 2.1 0.3145 9.7 0.4240 2.1 0.1995 12.1 0.3381 2.2 0.3074 12.1 0.3751 2.2 Random - - 0.5043 2 - - 0.3548 2 - - 0.2980 2 - - 0.3212 2 Table 1: Standard, Tuned and Random Results for Moresque and ODP-239 datasets. 2010) evidenced more complete results with the general definition of the Fβ-measure for Q = {1, 2, 5}, (Navigli and Crisafulli, 2010) introduced the Rand Index metric and (Moreno et al., 2013) used Fb3 introduced by (Amig´o et al., 2009) as a more adequate metric for clustering. Different standard datasets have been built such as AMBIENT2 (Carpineto and Romano, 2009), ODP-2393 (Carpineto and Romano, 2010) and Moresque4 (Navigli and Crisafulli, 2010). ODP239, an improved version of AMBIENT, is based on DMOZ5 where each query, over 239 ones, is a selected category in DMOZ and its associated subcategories are considered as the respective cluster results. The small text description included in DMOZ is considered as a Web snippet. Moresque is composed by 114 queries selected from a list of ambiguous Wikipedia entries. For each query, a set of Web results have been collected from a commercial search engine and manually classified into the disambiguation Wikipedia pages which form the reference clust</context>
<context position="7913" citStr="Carpineto and Romano, 2010" startWordPosition="1240" endWordPosition="1243">revious section, our aim was to claim that tunable versions of existing baseline algorithms might evidence improved results when faced to the ones reported in the literature. And these values should be taken as the “real” baseline results within the context of controllable environments. However, exploring all the parameter space is not an applicable solution in a real-world situation where the reference is unknown. As such, a stopping criterion must be defined to adapt to any dataset distribution. This is the particular case for the standard implementations of STC and LINGO. Previous results (Carpineto and Romano, 2010) showed that different SRC algorithms provide different results and hopefully complementary ones. For instance, STC demonstrates high recall and low precision, while LINGO inversely evidences high precision for low recall. Iteratively applying baseline SRC algorithms may thus lead to improved results by exploiting each algorithm’s strengths. 6http://carrot2.org [Last acc.: Jan., 2014] 7Carrot2 parameters maxClusters, desiredClusterCountBase and clusterCount were used to set k value. 2 In a cascade strategy, we first cluster the initial set of Web page snippets with any SRC algorithm. Then, the</context>
</contexts>
<marker>Carpineto, Romano, 2010</marker>
<rawString>C. Carpineto and G. Romano. 2010. Optimal meta search results clustering. In 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 170–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Carpineto</author>
<author>M D’Amico</author>
<author>A Bernardini</author>
</authors>
<title>Full discrimination of subtopics in search results with keyphrase-based clustering.</title>
<date>2011</date>
<journal>Web Intelligence and Agent Systems,</journal>
<volume>9</volume>
<issue>4</issue>
<marker>Carpineto, D’Amico, Bernardini, 2011</marker>
<rawString>C. Carpineto, M. D’Amico, and A. Bernardini. 2011. Full discrimination of subtopics in search results with keyphrase-based clustering. Web Intelligence and Agent Systems, 9(4):337–349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Di Marco</author>
<author>R Navigli</author>
</authors>
<title>Clustering and diversifying web search results with graph-based word sense induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>A. Di Marco and R. Navigli. 2013. Clustering and diversifying web search results with graph-based word sense induction. Computational Linguistics, 39(3):709–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ferragina</author>
<author>A Gulli</author>
</authors>
<title>A personalized search engine based on web-snippet hierarchical clustering.</title>
<date>2008</date>
<journal>Software: Practice and Experience,</journal>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="3012" citStr="Ferragina and Gulli, 2008" startWordPosition="451" endWordPosition="454">cascade strategy of baseline algorithms can lead to a scalable and realworld solution, which evidences comparative results to recent text-based algorithms. Finally, we draw some conclusions about evaluation metrics and their bias to the number of output clusters. 2 Related Work Search results clustering is an active research area. Two main streams have been proposed so far: text-based strategies such as (Hearst and Pedersen, 1996; Zamir and Etzioni, 1998; Zeng et al., 2004; Osinski et al., 2004; Carpineto and Romano, 2010; Carpineto et al., 2011; Moreno et al., 2013) and knowledge-based ones (Ferragina and Gulli, 2008; Scaiella et al., 2012; Di Marco and Navigli, 2013). Successful results have been obtained by recent works compared to STC (Zamir and Etzioni, 1998) and LINGO (Osinski et al., 2004) which provide publicly available implementations, and as a consequence, are often used as stateof-the-art baselines. On the one hand, STC proposes a monothetic methodology which merges base clusters with high string overlap relying on suffix trees. On the other hand, LINGO is a polythetic solution which reduces a term-document matrix using single value decomposition and assigns documents to each discovered latent </context>
</contexts>
<marker>Ferragina, Gulli, 2008</marker>
<rawString>P. Ferragina and A. Gulli. 2008. A personalized search engine based on web-snippet hierarchical clustering. Software: Practice and Experience, 38(2):189–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
<author>J O Pedersen</author>
</authors>
<title>Re-examining the cluster hypothesis: Scatter/gather on retrieval results.</title>
<date>1996</date>
<booktitle>In 19th Annual International Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>76--84</pages>
<contexts>
<context position="2820" citStr="Hearst and Pedersen, 1996" startWordPosition="419" endWordPosition="423"> on deep understanding of the evaluation task within the context of SRC. First, we provide the results of baseline algorithms with their best parameter settings. Second, we show that a simple cascade strategy of baseline algorithms can lead to a scalable and realworld solution, which evidences comparative results to recent text-based algorithms. Finally, we draw some conclusions about evaluation metrics and their bias to the number of output clusters. 2 Related Work Search results clustering is an active research area. Two main streams have been proposed so far: text-based strategies such as (Hearst and Pedersen, 1996; Zamir and Etzioni, 1998; Zeng et al., 2004; Osinski et al., 2004; Carpineto and Romano, 2010; Carpineto et al., 2011; Moreno et al., 2013) and knowledge-based ones (Ferragina and Gulli, 2008; Scaiella et al., 2012; Di Marco and Navigli, 2013). Successful results have been obtained by recent works compared to STC (Zamir and Etzioni, 1998) and LINGO (Osinski et al., 2004) which provide publicly available implementations, and as a consequence, are often used as stateof-the-art baselines. On the one hand, STC proposes a monothetic methodology which merges base clusters with high string overlap r</context>
</contexts>
<marker>Hearst, Pedersen, 1996</marker>
<rawString>M.A. Hearst and J.O. Pedersen. 1996. Re-examining the cluster hypothesis: Scatter/gather on retrieval results. In 19th Annual International Conference on Research and Development in Information Retrieval (SIGIR), pages 76–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Moreno</author>
<author>G Dias</author>
<author>G Cleuziou</author>
</authors>
<title>Postretrieval clustering using third-order similarity measures.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>153--158</pages>
<contexts>
<context position="1266" citStr="Moreno et al., 2013" startWordPosition="179" endWordPosition="182">t text-based state-of-the-art algorithms. 1 Introduction Visualizing Web search results remains an open problem in Information Retrieval (IR). For example, in order to deal with ambiguous or multifaceted queries, many works present Web page results using groups of correlated contents instead of long flat lists of relevant documents. Among existing techniques, Web Search Results Clustering (SRC) is a commonly studied area, which consists in clustering “on-the-fly” Web page results based on their Web snippets. Therefore, many works have been recently presented including task adapted clustering (Moreno et al., 2013), meta clustering (Carpineto and Romano, 2010) and knowledge-based clustering (Scaiella et al., 2012). Evaluation is also a hot topic both in Natural Language Processing (NLP) and IR. Within the specific case of SRC, different metrics have been used such as F1-measure (F1), k55L1 and Fbsmeasure (Fbs) over different standard datasets: ODP-239 (Carpineto and Romano, 2010) and Moresque (Navigli and Crisafulli, 2010). Unfortunately, comparative results are usually biased as 1This metric is based on subjective label evaluation and as such is out of the scope of this paper. Ga¨el Dias Normandie Univ</context>
<context position="2960" citStr="Moreno et al., 2013" startWordPosition="444" endWordPosition="447">ameter settings. Second, we show that a simple cascade strategy of baseline algorithms can lead to a scalable and realworld solution, which evidences comparative results to recent text-based algorithms. Finally, we draw some conclusions about evaluation metrics and their bias to the number of output clusters. 2 Related Work Search results clustering is an active research area. Two main streams have been proposed so far: text-based strategies such as (Hearst and Pedersen, 1996; Zamir and Etzioni, 1998; Zeng et al., 2004; Osinski et al., 2004; Carpineto and Romano, 2010; Carpineto et al., 2011; Moreno et al., 2013) and knowledge-based ones (Ferragina and Gulli, 2008; Scaiella et al., 2012; Di Marco and Navigli, 2013). Successful results have been obtained by recent works compared to STC (Zamir and Etzioni, 1998) and LINGO (Osinski et al., 2004) which provide publicly available implementations, and as a consequence, are often used as stateof-the-art baselines. On the one hand, STC proposes a monothetic methodology which merges base clusters with high string overlap relying on suffix trees. On the other hand, LINGO is a polythetic solution which reduces a term-document matrix using single value decomposit</context>
<context position="4738" citStr="Moreno et al., 2013" startWordPosition="741" endWordPosition="744">STC 0.4550 12.7 0.6000 2.9 0.4602 12.7 0.4987 2.9 0.3238 12.4 0.3350 3.0 0.4027 12.4 0.4046 14.5 LINGO 0.3258 26.7 0.6034 3.0 0.3989 26.7 0.5004 5.8 0.2029 27.7 0.3320 3.0 0.3461 27.7 0.4459 8.7 BiKm 0.3165 9.7 0.5891 2.1 0.3145 9.7 0.4240 2.1 0.1995 12.1 0.3381 2.2 0.3074 12.1 0.3751 2.2 Random - - 0.5043 2 - - 0.3548 2 - - 0.2980 2 - - 0.3212 2 Table 1: Standard, Tuned and Random Results for Moresque and ODP-239 datasets. 2010) evidenced more complete results with the general definition of the Fβ-measure for Q = {1, 2, 5}, (Navigli and Crisafulli, 2010) introduced the Rand Index metric and (Moreno et al., 2013) used Fb3 introduced by (Amig´o et al., 2009) as a more adequate metric for clustering. Different standard datasets have been built such as AMBIENT2 (Carpineto and Romano, 2009), ODP-2393 (Carpineto and Romano, 2010) and Moresque4 (Navigli and Crisafulli, 2010). ODP239, an improved version of AMBIENT, is based on DMOZ5 where each query, over 239 ones, is a selected category in DMOZ and its associated subcategories are considered as the respective cluster results. The small text description included in DMOZ is considered as a Web snippet. Moresque is composed by 114 queries selected from a list</context>
<context position="12594" citStr="Moreno et al., 2013" startWordPosition="2018" endWordPosition="2021">nd ODP-239 are near the ground-truth number of clusters while this is not the case for F1 which evidences a decreasing tendency when k increases. In section 4, we showed that competitive results could be achieved with a cascade strategy based on baseline algorithms. Although results outperform standard and tunable baseline implementations for F1, it is wise to use Fb3 to better evaluate the SRC task, based on our previous discussion. In this case, the best values are obtained by STC-LINGO with Fb3=0.4980 for Moresque and Fb3=0.4249 for ODP-239, which highly approximate the values reported in (Moreno et al., 2013): Fb3=0.490 (Moresque) and Fb3=0.452 (ODP-239). Additionally, when STC is performed first and LINGO later the cascade algorithm scale better due to LINGO and STC scaling properties9. 6 Conclusion This work presents a discussion about the use of baseline algorithms in SRC and evaluation met9http://carrotsearch.com/lingo3g-comparison [Last acc.: Jan., 2014] 3 Moresque ODP-239 F1 Fb3 F1 Fb3 Level 1 Level 2 Stand. Equiv. k Stand. Equiv. k Stand. Equiv. k Stand. Equiv. k STC STC 0.6145 0.5594 3.1 0.4550 0.4913 3.1 0.3629 0.3304 3.2 0.3982 0.4023 3.2 LINGO 0.5611 0.4932 7.3 0.4980 0.4716 7.3 0.3624 </context>
</contexts>
<marker>Moreno, Dias, Cleuziou, 2013</marker>
<rawString>J.G. Moreno, G. Dias, and G. Cleuziou. 2013. Postretrieval clustering using third-order similarity measures. In 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 153–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>G Crisafulli</author>
</authors>
<title>Inducing word senses to improve web search result clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>116--126</pages>
<contexts>
<context position="1682" citStr="Navigli and Crisafulli, 2010" startWordPosition="241" endWordPosition="244">only studied area, which consists in clustering “on-the-fly” Web page results based on their Web snippets. Therefore, many works have been recently presented including task adapted clustering (Moreno et al., 2013), meta clustering (Carpineto and Romano, 2010) and knowledge-based clustering (Scaiella et al., 2012). Evaluation is also a hot topic both in Natural Language Processing (NLP) and IR. Within the specific case of SRC, different metrics have been used such as F1-measure (F1), k55L1 and Fbsmeasure (Fbs) over different standard datasets: ODP-239 (Carpineto and Romano, 2010) and Moresque (Navigli and Crisafulli, 2010). Unfortunately, comparative results are usually biased as 1This metric is based on subjective label evaluation and as such is out of the scope of this paper. Ga¨el Dias Normandie University UNICAEN, GREYC CNRS F-14032 Caen, France gael.dias@unicaen.fr baseline algorithms are run with default parameters whereas proposed methodologies are usually tuned to increase performance over the studied datasets. Moreover, evaluation metrics tend to correlate with the number of produced clusters. In this paper, we focus on deep understanding of the evaluation task within the context of SRC. First, we prov</context>
<context position="4679" citStr="Navigli and Crisafulli, 2010" startWordPosition="731" endWordPosition="734">Stand. k Tuned k Stand. k Tuned k Stand. k Tuned k Stand. k Tuned k STC 0.4550 12.7 0.6000 2.9 0.4602 12.7 0.4987 2.9 0.3238 12.4 0.3350 3.0 0.4027 12.4 0.4046 14.5 LINGO 0.3258 26.7 0.6034 3.0 0.3989 26.7 0.5004 5.8 0.2029 27.7 0.3320 3.0 0.3461 27.7 0.4459 8.7 BiKm 0.3165 9.7 0.5891 2.1 0.3145 9.7 0.4240 2.1 0.1995 12.1 0.3381 2.2 0.3074 12.1 0.3751 2.2 Random - - 0.5043 2 - - 0.3548 2 - - 0.2980 2 - - 0.3212 2 Table 1: Standard, Tuned and Random Results for Moresque and ODP-239 datasets. 2010) evidenced more complete results with the general definition of the Fβ-measure for Q = {1, 2, 5}, (Navigli and Crisafulli, 2010) introduced the Rand Index metric and (Moreno et al., 2013) used Fb3 introduced by (Amig´o et al., 2009) as a more adequate metric for clustering. Different standard datasets have been built such as AMBIENT2 (Carpineto and Romano, 2009), ODP-2393 (Carpineto and Romano, 2010) and Moresque4 (Navigli and Crisafulli, 2010). ODP239, an improved version of AMBIENT, is based on DMOZ5 where each query, over 239 ones, is a selected category in DMOZ and its associated subcategories are considered as the respective cluster results. The small text description included in DMOZ is considered as a Web snippe</context>
</contexts>
<marker>Navigli, Crisafulli, 2010</marker>
<rawString>R. Navigli and G. Crisafulli. 2010. Inducing word senses to improve web search result clustering. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 116–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Osinski</author>
<author>J Stefanowski</author>
<author>D Weiss</author>
</authors>
<title>Lingo: Search results clustering algorithm based on singular value decomposition.</title>
<date>2004</date>
<booktitle>In Intelligent Information Systems Conference (IIPWM),</booktitle>
<pages>369--378</pages>
<contexts>
<context position="2886" citStr="Osinski et al., 2004" startWordPosition="432" endWordPosition="435">. First, we provide the results of baseline algorithms with their best parameter settings. Second, we show that a simple cascade strategy of baseline algorithms can lead to a scalable and realworld solution, which evidences comparative results to recent text-based algorithms. Finally, we draw some conclusions about evaluation metrics and their bias to the number of output clusters. 2 Related Work Search results clustering is an active research area. Two main streams have been proposed so far: text-based strategies such as (Hearst and Pedersen, 1996; Zamir and Etzioni, 1998; Zeng et al., 2004; Osinski et al., 2004; Carpineto and Romano, 2010; Carpineto et al., 2011; Moreno et al., 2013) and knowledge-based ones (Ferragina and Gulli, 2008; Scaiella et al., 2012; Di Marco and Navigli, 2013). Successful results have been obtained by recent works compared to STC (Zamir and Etzioni, 1998) and LINGO (Osinski et al., 2004) which provide publicly available implementations, and as a consequence, are often used as stateof-the-art baselines. On the one hand, STC proposes a monothetic methodology which merges base clusters with high string overlap relying on suffix trees. On the other hand, LINGO is a polythetic s</context>
</contexts>
<marker>Osinski, Stefanowski, Weiss, 2004</marker>
<rawString>S. Osinski, J. Stefanowski, and D. Weiss. 2004. Lingo: Search results clustering algorithm based on singular value decomposition. In Intelligent Information Systems Conference (IIPWM), pages 369–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Scaiella</author>
<author>P Ferragina</author>
<author>A Marino</author>
<author>M Ciaramita</author>
</authors>
<title>Topical clustering of search results.</title>
<date>2012</date>
<booktitle>In 5th ACM International Conference on Web Search and Data Mining (WSDM),</booktitle>
<pages>223--232</pages>
<contexts>
<context position="1367" citStr="Scaiella et al., 2012" startWordPosition="192" endWordPosition="195">open problem in Information Retrieval (IR). For example, in order to deal with ambiguous or multifaceted queries, many works present Web page results using groups of correlated contents instead of long flat lists of relevant documents. Among existing techniques, Web Search Results Clustering (SRC) is a commonly studied area, which consists in clustering “on-the-fly” Web page results based on their Web snippets. Therefore, many works have been recently presented including task adapted clustering (Moreno et al., 2013), meta clustering (Carpineto and Romano, 2010) and knowledge-based clustering (Scaiella et al., 2012). Evaluation is also a hot topic both in Natural Language Processing (NLP) and IR. Within the specific case of SRC, different metrics have been used such as F1-measure (F1), k55L1 and Fbsmeasure (Fbs) over different standard datasets: ODP-239 (Carpineto and Romano, 2010) and Moresque (Navigli and Crisafulli, 2010). Unfortunately, comparative results are usually biased as 1This metric is based on subjective label evaluation and as such is out of the scope of this paper. Ga¨el Dias Normandie University UNICAEN, GREYC CNRS F-14032 Caen, France gael.dias@unicaen.fr baseline algorithms are run with</context>
<context position="3035" citStr="Scaiella et al., 2012" startWordPosition="455" endWordPosition="458">e algorithms can lead to a scalable and realworld solution, which evidences comparative results to recent text-based algorithms. Finally, we draw some conclusions about evaluation metrics and their bias to the number of output clusters. 2 Related Work Search results clustering is an active research area. Two main streams have been proposed so far: text-based strategies such as (Hearst and Pedersen, 1996; Zamir and Etzioni, 1998; Zeng et al., 2004; Osinski et al., 2004; Carpineto and Romano, 2010; Carpineto et al., 2011; Moreno et al., 2013) and knowledge-based ones (Ferragina and Gulli, 2008; Scaiella et al., 2012; Di Marco and Navigli, 2013). Successful results have been obtained by recent works compared to STC (Zamir and Etzioni, 1998) and LINGO (Osinski et al., 2004) which provide publicly available implementations, and as a consequence, are often used as stateof-the-art baselines. On the one hand, STC proposes a monothetic methodology which merges base clusters with high string overlap relying on suffix trees. On the other hand, LINGO is a polythetic solution which reduces a term-document matrix using single value decomposition and assigns documents to each discovered latent topic. All solutions ha</context>
<context position="5885" citStr="Scaiella et al., 2012" startWordPosition="928" endWordPosition="931"> as a Web snippet. Moresque is composed by 114 queries selected from a list of ambiguous Wikipedia entries. For each query, a set of Web results have been collected from a commercial search engine and manually classified into the disambiguation Wikipedia pages which form the reference clusters. In Table 2, we report the results obtained so far in the literature by text-based and knowledgebased strategies for the standard F1 over ODP-239 and Moresque datasets. F1 ODP239 Moresque Text STC 0.324 0.455 LINGO 0.273 0.326 (Carpineto and Romano, 2010) 0.313 - (Moreno et al., 2013) 0.390 0.665 Know. (Scaiella et al., 2012) 0.413 - (Di Marco and Navigli, 2013) - 0.7204* Table 2: State-of-the-art Results for SRC. (*) The result of (Di Marco and Navigli, 2013) is based on a reduced version of AMBIENT + Moresque. 3 Baseline SRC Algorithms Newly proposed algorithms are usually tuned towards their maximal performance. However, the results of baseline algorithms are usually run with 2http://credo.fub.it/ambient/ [Last acc.: Jan., 2014] 3http://credo.fub.it/odp239/ [Last acc.: Jan., 2014] 4http://lcl.uniroma1.it/moresque/ [Last acc.: Jan., 2014] 5http://www.dmoz.org [Last acc.: Jan., 2014] default parameters based on a</context>
</contexts>
<marker>Scaiella, Ferragina, Marino, Ciaramita, 2012</marker>
<rawString>U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita. 2012. Topical clustering of search results. In 5th ACM International Conference on Web Search and Data Mining (WSDM), pages 223–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Zamir</author>
<author>O Etzioni</author>
</authors>
<title>Web document clustering: A feasibility demonstration.</title>
<date>1998</date>
<booktitle>In 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>46--54</pages>
<contexts>
<context position="2845" citStr="Zamir and Etzioni, 1998" startWordPosition="424" endWordPosition="427">he evaluation task within the context of SRC. First, we provide the results of baseline algorithms with their best parameter settings. Second, we show that a simple cascade strategy of baseline algorithms can lead to a scalable and realworld solution, which evidences comparative results to recent text-based algorithms. Finally, we draw some conclusions about evaluation metrics and their bias to the number of output clusters. 2 Related Work Search results clustering is an active research area. Two main streams have been proposed so far: text-based strategies such as (Hearst and Pedersen, 1996; Zamir and Etzioni, 1998; Zeng et al., 2004; Osinski et al., 2004; Carpineto and Romano, 2010; Carpineto et al., 2011; Moreno et al., 2013) and knowledge-based ones (Ferragina and Gulli, 2008; Scaiella et al., 2012; Di Marco and Navigli, 2013). Successful results have been obtained by recent works compared to STC (Zamir and Etzioni, 1998) and LINGO (Osinski et al., 2004) which provide publicly available implementations, and as a consequence, are often used as stateof-the-art baselines. On the one hand, STC proposes a monothetic methodology which merges base clusters with high string overlap relying on suffix trees. O</context>
</contexts>
<marker>Zamir, Etzioni, 1998</marker>
<rawString>O. Zamir and O. Etzioni. 1998. Web document clustering: A feasibility demonstration. In 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 46–54.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>