<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.958504">
ISP: Learning Inferential Selectional Preferences
</title>
<author confidence="0.990641">
Patrick Pantel†, Rahul Bhagat†, Bonaventura Coppola‡,
Timothy Chklovski†, Eduard Hovy†
</author>
<affiliation confidence="0.997976">
†Information Sciences Institute ‡ITC-Irst and University of Trento
University of Southern California Via Sommarive, 18 – Povo 38050
</affiliation>
<address confidence="0.649003">
Marina del Rey, CA Trento, Italy
</address>
<email confidence="0.998931">
{pantel,rahul,timc,hovy}@isi.edu coppolab@itc.it
</email>
<sectionHeader confidence="0.994791" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999944666666666">
Semantic inference is a key component
for advanced natural language under-
standing. However, existing collections of
automatically acquired inference rules
have shown disappointing results when
used in applications such as textual en-
tailment and question answering. This pa-
per presents ISP, a collection of methods
for automatically learning admissible ar-
gument values to which an inference rule
can be applied, which we call inferential
selectional preferences, and methods for
filtering out incorrect inferences. We
evaluate ISP and present empirical evi-
dence of its effectiveness.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987793848484848">
Semantic inference is a key component for ad-
vanced natural language understanding. Several
important applications are already relying heavily
on inference, including question answering
(Moldovan et al. 2003; Harabagiu and Hickl 2006),
information extraction (Romano et al. 2006), and
textual entailment (Szpektor et al. 2004).
In response, several researchers have created re-
sources for enabling semantic inference. Among
manual resources used for this task are WordNet
(Fellbaum 1998) and Cyc (Lenat 1995). Although
important and useful, these resources primarily
contain prescriptive inference rules such as “X di-
vorces Y ⇒ X married Y”. In practical NLP appli-
cations, however, plausible inference rules such as
“X married Y” ⇒ “X dated Y” are very useful. This,
along with the difficulty and labor-intensiveness of
generating exhaustive lists of rules, has led re-
searchers to focus on automatic methods for build-
ing inference resources such as inference rule
collections (Lin and Pantel 2001; Szpektor et al.
2004) and paraphrase collections (Barzilay and
McKeown 2001).
Using these resources in applications has been
hindered by the large amount of incorrect infer-
ences they generate, either because of altogether
incorrect rules or because of blind application of
plausible rules without considering the context of
the relations or the senses of the words. For exam-
ple, consider the following sentence:
Terry Nichols was charged by federal prosecutors for murder
and conspiracy in the Oklahoma City bombing.
and an inference rule such as:
</bodyText>
<equation confidence="0.903413">
X is charged by Y ⇒ Y announced the arrest of X (1)
</equation>
<bodyText confidence="0.971882695652174">
Using this rule, we can infer that “federal prosecu-
tors announced the arrest of Terry Nichols”. How-
ever, given the sentence:
Fraud was suspected when accounts were charged by CCM
telemarketers without obtaining consumer authorization.
the plausible inference rule (1) would incorrectly
infer that “CCM telemarketers announced the ar-
rest of accounts”.
This example depicts a major obstacle to the ef-
fective use of automatically learned inference
rules. What is missing is knowledge about the ad-
missible argument values for which an inference
rule holds, which we call Inferential Selectional
Preferences. For example, inference rule (1)
should only be applied if X is a Person and Y is a
Law Enforcement Agent or a Law Enforcement
Agency. This knowledge does not guarantee that
the inference rule will hold, but, as we show in this
paper, goes a long way toward filtering out errone-
ous applications of rules.
In this paper, we propose ISP, a collection of
methods for learning inferential selectional prefer-
ences and filtering out incorrect inferences. The
</bodyText>
<page confidence="0.974011">
564
</page>
<note confidence="0.798463">
Proceedings of NAACL HLT 2007, pages 564–571,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.997284583333333">
presented algorithms apply to any collection of
inference rules between binary semantic relations,
such as example (1). ISP derives inferential selec-
tional preferences by aggregating statistics of in-
ference rule instantiations over a large corpus of
text. Within ISP, we explore different probabilistic
models of selectional preference to accept or reject
specific inferences. We present empirical evidence
to support the following main contribution:
Claim: Inferential selectional preferences can be
automatically learned and used for effectively fil-
tering out incorrect inferences.
</bodyText>
<sectionHeader confidence="0.996283" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999945757575758">
Selectional preference (SP) as a foundation for
computational semantics is one of the earliest top-
ics in AI and NLP, and has its roots in (Katz and
Fodor 1963). Overviews of NLP research on this
theme are (Wilks and Fass 1992), which includes
the influential theory of Preference Semantics by
Wilks, and more recently (Light and Greiff 2002).
Rather than venture into learning inferential
SPs, much previous work has focused on learning
SPs for simpler structures. Resnik (1996), the
seminal paper on this topic, introduced a statistical
model for learning SPs for predicates using an un-
supervised method.
Learning SPs often relies on an underlying set of
semantic classes, as in both Resnik’s and our ap-
proach. Semantic classes can be specified manu-
ally or derived automatically. Manual collections
of semantic classes include the hierarchies of
WordNet (Fellbaum 1998), Levin verb classes
(Levin 1993), and FrameNet (Baker et al. 1998).
Automatic derivation of semantic classes can take
a variety of approaches, but often uses corpus
methods and the Distributional Hypothesis (Harris
1964) to automatically cluster similar entities into
classes, e.g. CBC (Pantel and Lin 2002). In this
paper, we experiment with two sets of semantic
classes, one from WordNet and one from CBC.
Another thread related to our work includes ex-
tracting from text corpora paraphrases (Barzilay
and McKeown 2001) and inference rules, e.g.
TEASE1 (Szpektor et al. 2004) and DIRT (Lin and
Pantel 2001). While these systems differ in their
approaches, neither provides for the extracted in-
</bodyText>
<footnote confidence="0.8131085">
1 Some systems refer to inferences they extract as entail-
ments; the two terms are sometimes used interchangeably.
</footnote>
<bodyText confidence="0.999876909090909">
ference rules to hold or fail based on SPs. Zanzotto
et al. (2006) recently explored a different interplay
between SPs and inferences. Rather than examine
the role of SPs in inferences, they use SPs of a par-
ticular type to derive inferences. For instance the
preference of win for the subject player, a nomi-
nalization of play, is used to derive that “win =&gt;
play”. Our work can be viewed as complementary
to the work on extracting semantic inferences and
paraphrases, since we seek to refine when a given
inference applies, filtering out incorrect inferences.
</bodyText>
<sectionHeader confidence="0.979778" genericHeader="method">
3 Selectional Preference Models
</sectionHeader>
<bodyText confidence="0.961937047619048">
The aim of this paper is to learn inferential selec-
tional preferences for filtering inference rules.
Let pi =&gt; pj be an inference rule where p is a bi-
nary semantic relation between two entities x and
y. Let (x, p, y) be an instance of relation p.
Formal task definition: Given an inference rule
pi =&gt; pj and the instance (x, pi, y), our task is to
determine if (x, pj, y) is valid.
Consider the example in Section 1 where we
have the inference rule “X is charged by Y” =&gt; “Y
announced the arrest of X”. Our task is to auto-
matically determine that “federal prosecutors an-
nounced the arrest of Terry Nichols” (i.e.,
(Terry Nichols, pj, federal prosecutors)) is valid
but that “CCM telemarketers announced the arrest
of accounts” is invalid.
Because the semantic relations p are binary, the
selectional preferences on their two arguments may
be either considered jointly or independently. For
example, the relation p = “X is charged by Y”
could have joint SPs:
</bodyText>
<equation confidence="0.971652428571429">
(Person, Law Enforcement Agent)
(Person, Law Enforcement Agency) (2)
(Bank Account, Organization)
or independent SPs:
(Person, *)
(*, Organization) (3)
(*, Law Enforcement Agent)
</equation>
<bodyText confidence="0.99988625">
This distinction between joint and independent
selectional preferences constitutes the difference
between the two models we present in this section.
The remainder of this section describes the ISP
approach. In Section 3.1, we describe methods for
automatically determining the semantic contexts of
each single relation’s selectional preferences. Sec-
tion 3.2 uses these for developing our inferential
</bodyText>
<page confidence="0.945329">
565
</page>
<equation confidence="0.994805625">
P e e
( ) (3.1)
1 2
,
pmi e e =
( ; ) log
1 2
P(e1 )P(e2 )
</equation>
<bodyText confidence="0.933037666666667">
selectional preference models. Finally, we propose
inference filtering algorithms in Section 3.3.
cx
</bodyText>
<subsectionHeader confidence="0.999414">
3.1 Relational Selectional Preferences
</subsectionHeader>
<bodyText confidence="0.982456914893617">
Resnik (1996) defined the selectional preferences
of a predicate as the semantic classes of the words
that appear as its arguments. Similarly, we define
the relational selectional preferences of a binary
semantic relation pi as the semantic classes C(x) of
the words that can be instantiated for x and as the
semantic classes C(y) of the words that can be in-
stantiated for y.
The semantic classes C(x) and C(y) can be ob-
tained from a conceptual taxonomy as proposed in
(Resnik 1996), such as WordNet, or from the
classes extracted from a word clustering algorithm
such as CBC (Pantel and Lin 2002). For example,
given the relation “X is charged by Y”, its rela-
tional selection preferences from WordNet could
be {social group, organism, state...} for X and
{authority, state, section...} for Y.
Below we propose joint and independent mod-
els, based on a corpus analysis, for automatically
determining relational selectional preferences.
Model 1: Joint Relational Model (JRM)
Our joint model uses a corpus analysis to learn SPs
for binary semantic relations by considering their
arguments jointly, as in example (2).
Given a large corpus of English text, we first
find the occurrences of each semantic relation p.
For each instance 〈x, p, y〉, we retrieve the sets C(x)
and C(y) of the semantic classes that x and y be-
long to and accumulate the frequencies of the tri-
ples 〈c(x), p, c(y)〉, where c(x) ∈ C(x) and
c(y) ∈ C(y)2.
Each triple 〈c(x), p, c(y)〉 is a candidate selec-
tional preference for p. Candidates can be incorrect
when: a) they were generated from the incorrect
sense of a polysemous word; or b) p does not hold
for the other words in the semantic class.
Intuitively, we have more confidence in a par-
ticular candidate if its semantic classes are closely
associated given the relation p. Pointwise mutual
information (Cover and Thomas 1991) is a com-
monly used metric for measuring this association
strength between two events e1 and e2:
2 In this paper, the semantic classes C(x) and C(y) are ex-
tracted from WordNet and CBC (described in Section 4.2).
We define our ranking function as the strength
of association between two semantic classes, cx and
cy3, given the relation p:
</bodyText>
<equation confidence="0.998926555555555">
( ) ( )
P c c p
,
x y (3.2)
pmi c p c p
; = log
x y
P ( c p) P ( c p)
x y
</equation>
<bodyText confidence="0.99925075">
Let |cx, p, cy |denote the frequency of observing
the instance 〈c(x), p, c(y)〉. We estimate the prob-
abilities of Equation 3.2 using maximum likeli-
hood estimates over our corpus:
</bodyText>
<equation confidence="0.995541142857143">
c p
, ,∗ ∗,,
p c c p c
, , (3.3)
y y
P c p x
( ) ∗ ∗
x x y
= , , Pc p
( ) ∗ ∗
y = , , P c c p x
( ) ∗ ∗
, = , ,
p p p
</equation>
<bodyText confidence="0.865496">
Similarly to (Resnik 1996), we estimate the
above frequencies using:
</bodyText>
<equation confidence="0.977263318181818">
w p
, , ∗ ∗ pw
,, w p w
1 , , 2
, ,
p ∗ = ∑ ∗ , ,
p c = ∑ c p c
, , =
y x y ∈ ∑∈
C w
( ) C w C w C w
w cx
∈ ( ) ( ) ( )
×
w cy
∈ w c x w c y
1 , 2 1 2
the arrest
we may find occurrences from our corpus of the
particular class
for X and
for Y, however we may never see both of
</equation>
<bodyText confidence="0.923479">
these classes co-occurring even though they would
form a valid relational selectional preference.
To alleviate this problem, we propose a second
model that is less strict by considering the argu-
ments of the binary semantic relations independ-
ently, as in example (3).
Similarly to JRM, we extract each instance
p,
of each semantic relation p and retrieve the
set of semantic classes C(x) and C(y) that x and y
belong to, accumulating the frequencies of the tri-
ples
p,
and
p,
where
</bodyText>
<figure confidence="0.773584259259259">
c(x)
C(x) and c(y)
C(y).
All tuples
p,
and
p,
are candi-
date selectional preferences for p. We rank candi-
dates by the probability of the seman
Y announced
of
“MoneyHandler”
“Law-
yer”
〈x,
y〉
〈c(x),
*〉
〈*,
c(y)〉,
∈
∈
〈c(x),
*〉
〈*,
c(y)〉
</figure>
<bodyText confidence="0.7812834">
tic class given
the relation p, according to Equations 3.3.
where
p,
denotes the frequency of observing
</bodyText>
<figure confidence="0.875348277777778">
the instance
p,
and
denotes the number
of classes to which word w belongs.
distri
|x,
y|
〈x,
y〉
|C(w)|
|C(w)|
b-
utes w’s mass equally to all of its senses cw.
Model 2: Independent Relational Model (IRM)
Because of sparse data, our joint model can miss
some correct selectional preference pairs. For ex-
ample, given the relation
</figure>
<footnote confidence="0.490637">
cx and
are shorthand for c(x) an
</footnote>
<page confidence="0.931098">
3
</page>
<bodyText confidence="0.2751955">
cy
d c(y) in our equations.
</bodyText>
<page confidence="0.957602">
566
</page>
<bodyText confidence="0.662564166666667">
The intersection of the two sets of SPs forms the
candidate inferential SPs for the inference pi =&gt; pj:
(Law Enforcement Agent, *)
(*, Person)
We use the same minimum, maximum, and av-
erage ranking strategies as in JIM.
</bodyText>
<subsectionHeader confidence="0.984192">
3.2 Inferential Selectional Preferences
</subsectionHeader>
<bodyText confidence="0.964873454545454">
Whereas in Section 3.1 we learned selectional
preferences for the arguments of a relation p, in
this section we learn selectional preferences for the
arguments of an inference rule pi =&gt; pj.
Model 1: Joint Inferential Model (JIM)
Given an inference rule pi =&gt; pj, our joint model
defines the set of inferential SPs as the intersection
of the relational SPs for pi and pj, as defined in the
Joint Relational Model (JRM). For example, sup-
pose relation pi = “X is charged by Y” gives the
following SP scores under the JRM:
</bodyText>
<construct confidence="0.628163333333333">
(Person, pi, Law Enforcement Agent) = 1.45
(Person, pi, Law Enforcement Agency) = 1.21
(Bank Account, pi, Organization) = 0.97
</construct>
<bodyText confidence="0.947448">
and that pj = “Y announced the arrest of X” gives
the following SP scores under the JRM:
</bodyText>
<equation confidence="0.588827666666667">
(Law Enforcement Agent, pj, Person) = 2.01
(Reporter, pj, Person) = 1.98
(Law Enforcement Agency, pj, Person) = 1.61
</equation>
<bodyText confidence="0.890828">
The intersection of the two sets of SPs forms the
candidate inferential SPs for the inference pi =&gt; pj:
</bodyText>
<subsubsectionHeader confidence="0.482317">
(Law Enforcement Agent, Person)
(Law Enforcement Agency, Person)
</subsubsectionHeader>
<bodyText confidence="0.958864375">
We rank the candidate inferential SPs according
to three ways to combine their relational SP scores,
using the minimum, maximum, and average of the
SPs. For example, for (Law Enforcement Agent,
Person), the respective scores would be 1.45, 2.01,
and 1.73. These different ranking strategies pro-
duced nearly identical results in our experiments,
as discussed in Section 5.
Model 2: Independent Inferential Model (IIM)
Our independent model is the same as the joint
model above except that it computes candidate in-
ferential SPs using the Independent Relational
Model (IRM) instead of the JRM. Consider the
same example relations pi and pj from the joint
model and suppose that the IRM gives the follow-
ing relational SP scores for pi:
</bodyText>
<equation confidence="0.628127333333333">
(Law Enforcement Agent, pi, *) = 3.43
(*, pi, Person) = 2.17
(*, pi, Organization) = 1.24
</equation>
<bodyText confidence="0.963595">
and the following relational SP scores for pj:
</bodyText>
<equation confidence="0.927233666666667">
(*, pj, Person) = 2.87
(Law Enforcement Agent, pj, *) = 1.92
(Reporter, pj, *) = 0.89
</equation>
<subsectionHeader confidence="0.99567">
3.3 Filtering Inferences
</subsectionHeader>
<bodyText confidence="0.9974365">
Given an inference rule pi =&gt; pj and the instance
(x, pi, y), the system’s task is to determine whether
(x, pj, y) is valid. Let C(w) be the set of semantic
classes c(w) to which word w belongs. Below we
present three filtering algorithms which range from
the least to the most permissive:
</bodyText>
<listItem confidence="0.9723195">
• ISP.JIM, accepts the inference (x, pj, y) if the
inferential SP (c(x), pj, c(y)) was admitted by the
Joint Inferential Model for some c(x) E C(x) and
c(y) E C(y).
• ISP.IIM.∧, accepts the inference (x, pj, y) if the
inferential SPs (c(x), pj, *) AND (*, pj, c(y)) were
admitted by the Independent Inferential Model
for some c(x) E C(x) and c(y) E C(y) .
• ISP.IIM.v, accepts the inference (x, pj, y) if the
inferential SP (c(x), pj, *) OR (*, pj, c(y)) was
admitted by the Independent Inferential Model
for some c(x) E C(x) and c(y) E C(y) .
</listItem>
<bodyText confidence="0.998625142857143">
Since both JIM and IIM use a ranking score in
their inferential SPs, each filtering algorithm can
be tuned to be more or less strict by setting an ac-
ceptance threshold on the ranking scores or by se-
lecting only the top i percent highest ranking SPs.
In our experiments, reported in Section 5, we
tested each model using various values of i.
</bodyText>
<sectionHeader confidence="0.998188" genericHeader="method">
4 Experimental Methodology
</sectionHeader>
<bodyText confidence="0.9999815">
This section describes the methodology for testing
our claim that inferential selectional preferences
can be learned to filter incorrect inferences.
Given a collection of inference rules of the form
pi =&gt; pj, our task is to determine whether a particu-
lar instance (x, pj, y) holds given that (x, pi, y)
holds4. In the next sections, we describe our collec-
tion of inference rules, the semantic classes used
for forming selectional preferences, and evaluation
criteria for measuring the filtering quality.
</bodyText>
<footnote confidence="0.995497666666667">
4 Recall that the inference rules we consider in this paper are
not necessary strict logical inference rules, but plausible in-
ference rules; see Section 3.
</footnote>
<page confidence="0.993217">
567
</page>
<subsectionHeader confidence="0.829282">
4.1 Inference Rules
</subsectionHeader>
<bodyText confidence="0.979819363636364">
Our models for learning inferential selectional
preferences can be applied to any collection of in-
ference rules between binary semantic relations. In
this paper, we focus on the inference rules con-
tained in the DIRT resource (Lin and Pantel 2001).
DIRT consists of over 12 million rules which were
extracted from a 1GB newspaper corpus (San Jose
Mercury, Wall Street Journal and AP Newswire
from the TREC-9 collection). For example, here
are DIRT’s top 3 inference rules for “X solves Y”:
“Y is solved by X”, “X resolves Y”, “X finds a solution to Y”
</bodyText>
<subsectionHeader confidence="0.988797">
4.2 Semantic Classes
</subsectionHeader>
<bodyText confidence="0.984290045454545">
The choice of semantic classes is of great impor-
tance for selectional preference. One important
aspect is the granularity of the classes. Too general
a class will provide no discriminatory power while
too fine-grained a class will offer little generaliza-
tion and apply in only extremely few cases.
The absence of an attested high-quality set of
semantic classes for this task makes discovering
preferences difficult. Since many of the criteria for
developing such a set are not even known, we de-
cided to experiment with two very different sets of
semantic classes, in the hope that in addition to
learning semantic preferences, we might also un-
cover some clues for the eventual decisions about
what makes good semantic classes in general.
Our first set of semantic classes was directly ex-
tracted from the output of the CBC clustering algo-
rithm (Pantel and Lin 2002). We applied CBC to
the TREC-9 and TREC-2002 (Aquaint) newswire
collections consisting of over 600 million words.
CBC generated 1628 noun concepts and these were
used as our semantic classes for SPs.
Secondly, we extracted semantic classes from
WordNet 2.1 (Fellbaum 1998). In the absence of
any externally motivated distinguishing features
(for example, the Basic Level categories from Pro-
totype Theory, developed by Eleanor Rosch
(1978)), we used the simple but effective method
of manually truncating the noun synset hierarchy5
and considering all synsets below each cut point as
part of the semantic class at that node. To select
the cut points, we inspected several different hier-
archy levels and found the synsets at a depth of 4
5 Only nouns are considered since DIRT semantic relations
connect only nouns.
to form the most natural semantic classes. Since
the noun hierarchy in WordNet has an average
depth of 12, our truncation created a set of con-
cepts considerably coarser-grained than WordNet
itself. The cut produced 1287 semantic classes, a
number similar to the classes in CBC. To properly
test WordNet as a source of semantic classes for
our selectional preferences, we would need to ex-
periment with different extraction algorithms.
</bodyText>
<subsectionHeader confidence="0.997763">
4.3 Evaluation Criteria
</subsectionHeader>
<bodyText confidence="0.999974866666667">
The goal of the filtering task is to minimize false
positives (incorrectly accepted inferences) and
false negatives (incorrectly rejected inferences). A
standard methodology for evaluating such tasks is
to compare system filtering results with a gold
standard using a confusion matrix. A confusion
matrix captures the filtering performance on both
correct and incorrect inferences:
where A represents the number of correct instances
correctly identified by the system, D represents the
number of incorrect instances correctly identified
by the system, B represents the number of false
positives and C represents the number of false
negatives. To compare systems, three key meas-
ures are used to summarize confusion matrices:
</bodyText>
<listItem confidence="0.852982666666667">
• Sensitivity, defined as A C
A + , captures a filter’s
probability of accepting correct inferences;
• Specificity, defined as B +DD , captures a filter’s
probability of rejecting incorrect inferences;
• Accuracy, defined as A B C D
</listItem>
<equation confidence="0.972034666666667">
A D
+ , captures the
+ + +
</equation>
<bodyText confidence="0.978765">
probability of a filter being correct.
</bodyText>
<sectionHeader confidence="0.991706" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.995134428571429">
In this section, we provide empirical evidence to
support the main claim of this paper.
Given a collection of DIRT inference rules of
the form pi =&gt; pj, our experiments, using the meth-
odology of Section 4, evaluate the capability of our
ISP models for determining if (x, pj, y) holds given
that (x, pi, y) holds.
</bodyText>
<figure confidence="0.998541714285714">
GOLD STANDARD
SYSTEM
0 C D
1
A
1 0
B
</figure>
<page confidence="0.992388">
568
</page>
<tableCaption confidence="0.93306">
Table 1. Filtering quality of best performing systems according to the evaluation criteria defined in Section 4.3 on
the TEST set – the reported systems were selected based on the Accuracy criterion on the DEV set.
</tableCaption>
<table confidence="0.955364">
SYSTEM PARAMETERS SELECTED FROM DEV SET SENSITIVITY SPECIFICITY ACCURACY
(95% CONF) (95% CONF) (95% CONF)
RANKING STRATEGY o
T (/o)
B0 - - 0.00±0.00 1.00±0.00 0.50±0.04
B1 - - 1.00±0.00 0.00±0.00 0.49±0.04
Random - - 0.50±0.06 0.47±0.07 0.50±0.04
CBC ISP.JIM maximum 100 0.17±0.04 0.88±0.04 0.53±0.04
ISP.IIM.∧ maximum 100 0.24±0.05 0.84±0.04 0.54±0.04
ISP.IIM.v maximum 90 0.73±0.05 0.45±0.06 0.59±0.04†
WordNet ISP.JIM minimum 40 0.20±0.06 0.75±0.06 0.47±0.04
ISP.IIM.∧ minimum 10 0.33±0.07 0.77±0.06 0.55±0.04
ISP.IIM.v minimum 20 0.87±0.04 0.17±0.05 0.51±0.05
Indicates statistically significant results (with 95% confidence) when compared with all baseline systems using pairwise t-test.
</table>
<subsectionHeader confidence="0.8585765">
5.1 Experimental Setup
Model Implementation
</subsectionHeader>
<bodyText confidence="0.9999745">
For each filtering algorithm in Section 3.3, ISP.JIM,
ISP.IIM.∧, and ISP.IIM.v, we trained their probabil-
istic models using corpus statistics extracted from
the 1999 AP newswire collection (part of the
TREC-2002 Aquaint collection) consisting of ap-
proximately 31 million words. We used the Mini-
par parser (Lin 1993) to match DIRT patterns in
the text. This permits exact matches since DIRT
inference rules are built from Minipar parse trees.
For each system, we experimented with the dif-
ferent ways of combining relational SP scores:
minimum, maximum, and average (see Section
3.2). Also, we experimented with various values
for the i parameter described in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.89367">
Gold Standard Construction
</subsectionHeader>
<bodyText confidence="0.999042541666667">
In order to compute the confusion matrices de-
scribed in Section 4.3, we must first construct a
representative set of inferences and manually anno-
tate them as correct or incorrect.
We randomly selected 100 inference rules of the
form pi =&gt; pj from DIRT. For each pattern pi, we
then extracted its instances from the Aquaint 1999
AP newswire collection (approximately 22 million
words), and randomly selected 10 distinct in-
stances, resulting in a total of 1000 instances. For
each instance of pi, applying DIRT’s inference rule
would assert the instance (x, pj, y). Our evaluation
tests how well our models can filter these so that
only correct inferences are made.
To form the gold standard, two human judges
were asked to tag each instance (x, pj, y) as correct
or incorrect. For example, given a randomly se-
lected inference rule “X is charged by Y =&gt; Y an-
nounced the arrest of X” and the instance “Terry
Nichols was charged by federal prosecutors”, the
judges must determine if the instance (federal
prosecutors, Y announced the arrest of X, Terry
Nichols) is correct. The judges were asked to con-
sider the following two criteria for their decision:
</bodyText>
<listItem confidence="0.9980915">
• (x, pj, y) is a semantically meaningful instance;
• The inference pi =&gt; pj holds for this instance.
</listItem>
<bodyText confidence="0.999422133333333">
Judges found that annotation decisions can range
from trivial to difficult. The differences often were
in the instances for which one of the judges fails to
see the right context under which the inference
could hold. To minimize disagreements, the judges
went through an extensive round of training.
To that end, the 1000 instances (x, pj, y) were
split into DEV and TEST sets, 500 in each. The
two judges trained themselves by annotating DEV
together. The TEST set was then annotated sepa-
rately to verify the inter-annotator agreement and
to verify whether the task is well-defined. The
kappa statistic (Siegel and Castellan Jr. 1988) was
x = 0.72. For the 70 disagreements between the
judges, a third judge acted as an adjudicator.
</bodyText>
<subsectionHeader confidence="0.880788">
Baselines
</subsectionHeader>
<bodyText confidence="0.9985665">
We compare our ISP algorithms to the following
baselines:
</bodyText>
<listItem confidence="0.999975">
• B0: Rejects all inferences;
• B1: Accepts all inferences;
• Rand: Randomly accepts or rejects inferences.
</listItem>
<bodyText confidence="0.775893">
One alternative to our approach is admit instances
on the Web using literal search queries. We inves-
tigated this technique but discarded it due to subtle
yet critical issues with pattern canonicalization that
resulted in rejecting nearly all inferences. How-
ever, we are investigating other ways of using Web
corpora for this task.
</bodyText>
<page confidence="0.991031">
569
</page>
<figure confidence="0.999166583333333">
a) GOLD STANDARD
SYSTEM
0 63 114
1
184
1 0
139
GOLD STANDARD
1 0
b)
42 28
205 225
</figure>
<figureCaption confidence="0.873404">
Figure 1. Confusion matrices for a) ISP.IIM.v – best
Accuracy; and b) ISP.JIM – best 90%-Specificity.
</figureCaption>
<subsectionHeader confidence="0.999437">
5.2 Filtering Quality
</subsectionHeader>
<bodyText confidence="0.999939375">
For each ISP algorithm and parameter combina-
tion, we constructed a confusion matrix on the de-
velopment set and computed the system sensitivity,
specificity and accuracy as described in Section
4.3. This resulted in 180 experiments on the devel-
opment set. For each ISP algorithm and semantic
class source, we selected the best parameter com-
binations according to the following criteria:
</bodyText>
<listItem confidence="0.999677666666667">
• Accuracy: This system has the best overall abil-
ity to correctly accept and reject inferences.
• 90%-Specificity: Several formal semantics and
</listItem>
<bodyText confidence="0.963134571428571">
textual entailment researchers have commented
that inference rule collections like DIRT are dif-
ficult to use due to low precision. Many have
asked for filtered versions that remove incorrect
inferences even at the cost of removing correct
inferences. In response, we show results for the
system achieving the best sensitivity while main-
taining at least 90% specificity on the DEV set.
We evaluated the selected systems on the TEST
set. Table 1 summarizes the quality of the systems
selected according to the Accuracy criterion. The
best performing system, ISP.IIM.v, performed sta-
tistically significantly better than all three base-
lines. The best system according to the 90%-
Specificity criteria was ISP.JIM, which coinciden-
tally has the highest accuracy for that model as
shown in Table 16. This result is very promising
for researchers that require highly accurate infer-
ence rules since they can use ISP.JIM and expect to
recall 17% of the correct inferences by only ac-
cepting false positives 12% of the time.
</bodyText>
<subsectionHeader confidence="0.504425">
Performance and Error Analysis
</subsectionHeader>
<bodyText confidence="0.9988566">
Figures 1a) and 1b) present the full confusion ma-
trices for the most accurate and highly specific sys-
tems, with both systems selected on the DEV set.
The most accurate system was ISP.IIM.v, which is
the most permissive of the algorithms. This sug-
</bodyText>
<footnote confidence="0.527909">
6 The reported sensitivity of ISP.Joint in Table 1 is below
90%, however it achieved 90.7% on the DEV set.
</footnote>
<figureCaption confidence="0.999679">
Figure 2. ROC curves for our systems on TEST.
</figureCaption>
<bodyText confidence="0.99601540625">
gests that a larger corpus for learning SPs may be
needed to support stronger performance on the
more restrictive methods. The system in Figure
1b), selected for maximizing sensitivity while
maintaining high specificity, was 70% correct in
predicting correct inferences.
Figure 2 illustrates the ROC curve for all our
systems and parameter combinations on the TEST
set. ROC curves plot the true positive rate against
the false positive rate. The near-diagonal line plots
the three baseline systems.
Several trends can be observed from this figure.
First, systems using the semantic classes from
WordNet tend to perform less well than systems
using CBC classes. As discussed in Section 4.2, we
used a very simplistic extraction of semantic
classes from WordNet. The results in Figure 2
serve as a lower bound on what could be achieved
with a better extraction from WordNet. Upon in-
spection of instances that WordNet got incorrect
but CBC got correct, it seemed that CBC had a
much higher lexical coverage than WordNet. For
example, several of the instances contained proper
names as either the X or Y argument (WordNet has
poor proper name coverage). When an argument is
not covered by any class, the inference is rejected.
Figure 2 also illustrates how our three different
ISP algorithms behave. The strictest filters, ISP.JIM
and ISP.IIM.n, have the poorest overall perform-
ance but, as expected, have a generally very low
rate of false positives. ISP.IIM.v, which is a much
more permissive filter because it does not require
</bodyText>
<figure confidence="0.998914722222222">
Sensitivity
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1-Specificity
Baselines WordNet CBC ISP.JIM ISP.IIM.AND ISP.IIM.OR
ROC on the TEST Set
SYSTEM 1
0
</figure>
<page confidence="0.614888">
570
</page>
<figureCaption confidence="0.994954">
Figure 3. ISP.IIM.v (Best System)’s performance
variation over different values for the i threshold.
</figureCaption>
<bodyText confidence="0.999507818181818">
both arguments of a relation to match, has gener-
ally many more false positives but has an overall
better performance.
We did not include in Figure 2 an analysis of the
minimum, maximum, and average ranking strate-
gies presented in Section 3.2 since they generally
produced nearly identical results.
For the most accurate system, ISP.IIM.v, we ex-
plored the impact of the cutoff threshold i on the
sensitivity, specificity, and accuracy, as shown in
Figure 3. Rather than step the values by 10% as we
did on the DEV set, here we stepped the threshold
value by 2% on the TEST set. The more permis-
sive values of i increase sensitivity at the expense
of specificity. Interestingly, the overall accuracy
remained fairly constant across the entire range of
i, staying within 0.05 of the maximum of 0.62
achieved at i=30%.
Finally, we manually inspected several incorrect
inferences that were missed by our filters. A com-
mon source of errors was due to the many incorrect
“antonymy” inference rules generated by DIRT,
such as “X is rejected in Y”=&gt;“X is accepted in Y”.
This recognized problem in DIRT occurs because
of the distributional hypothesis assumption used to
form the inference rules. Our ISP algorithms suffer
from a similar quandary since, typically, antony-
mous relations take the same sets of arguments for
X (and Y). For these cases, ISP algorithms learn
many selectional preferences that accept the same
types of entities as those that made DIRT learn the
inference rule in the first place, hence ISP will not
filter out many incorrect inferences.
</bodyText>
<sectionHeader confidence="0.999607" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999579">
We presented algorithms for learning what we call
inferential selectional preferences, and presented
evidence that learning selectional preferences can
be useful in filtering out incorrect inferences. Fu-
ture work in this direction includes further explora-
tion of the appropriate inventory of semantic
classes used as SP’s. This work constitutes a step
towards better understanding of the interaction of
selectional preferences and inferences, bridging
these two aspects of semantics.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999896489795919">
Barzilay, R.; and McKeown, K.R. 2001.Extracting Paraphrases from a
Parallel Corpus. In Proceedings of ACL 2001. pp. 50–57. Toulose,
France.
Baker, C.F.; Fillmore, C.J.; and Lowe, J.B. 1998. The Berkeley
FrameNet Project. In Proceedings of COLING/ACL 1998. pp. 86-
90. Montreal, Canada.
Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory.
John Wiley &amp; Sons.
Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT
Press.
Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual
Entailment in Open-Domain Question Answering. In Proceedings
of ACL 2006. pp. 905-912. Sydney, Australia.
Katz, J.; and Fodor, J.A. 1963. The Structure of a Semantic Theory.
Language, vol 39. pp.170–210.
Lenat, D. 1995. CYC: A large-scale investment in knowledge
infrastructure. Communications of the ACM, 38(11):33–38.
Levin, B. 1993. English Verb Classes and Alternations: A Preliminary
Investigation. University of Chicago Press, Chicago, IL.
Light, M. and Greiff, W.R. 2002. Statistical Models for the Induction
and Use of Selectional Preferences. Cognitive Science,26:269–281.
Lin, D. 1993. Parsing Without OverGeneration. In Proceedings of
ACL-93. pp. 112-120. Columbus, OH.
Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for
Question Answering. Natural Language Engineering 7(4):343-360.
Moldovan, D.I.; Clark, C.; Harabagiu, S.M.; Maiorano, S.J. 2003.
COGEX: A Logic Prover for Question Answering. In Proceedings
of HLT-NAACL-03. pp. 87-93. Edmonton, Canada.
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In
Proceedings of KDD-02. pp. 613-619. Edmonton, Canada.
Resnik, P. 1996. Selectional Constraints: An Information-Theoretic
Model and its Computational Realization. Cognition, 61:127–159.
Romano, L.; Kouylekov, M.; Szpektor, I.; Dagan, I.; Lavelli, A. 2006.
Investigating a Generic Paraphrase-Based Approach for Relation
Extraction. In EACL-2006. pp. 409-416. Trento, Italy.
Rosch, E. 1978. Human Categorization. In E. Rosch and B.B. Lloyd
(eds.) Cognition and Categorization. Hillsdale, NJ: Erlbaum.
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for
the Behavioral Sciences. McGraw-Hill.
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling
web-based acquisition of entailment relations. In Proceedings of
EMNLP 2004. pp. 41-48. Barcelona,Spain.
Wilks, Y.; and Fass, D. 1992. Preference Semantics: a family history.
Computing and Mathematics with Applications, 23(2). A shorter
version in the second edition of the Encyclopedia of Artificial
Intelligence, (ed.) S. Shapiro.
Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering
Asymmetric Entailment Relations between Verbs using Selectional
Preferences. In COLING/ACL-06. pp. 849-856. Sydney, Australia.
</reference>
<figure confidence="0.996397454545454">
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0 10 20 30 40 50 60 70 80 90 100
Tau-Thresholds
ISP.IIM.OR (Best System)&apos;s Performance vs. Tau-Thresholds
Sensitivity Specificity Accuracy
</figure>
<page confidence="0.954726">
571
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.325624">
<title confidence="0.997899">ISP: Learning Inferential Selectional Preferences</title>
<author confidence="0.997316">Rahul Bonaventura</author>
<affiliation confidence="0.8106685">Sciences Institute and University of Trento University of Southern California Via Sommarive, 18 – Povo 38050</affiliation>
<address confidence="0.4473">Marina del Rey, CA Trento, Italy</address>
<email confidence="0.991433">pantel@isi.educoppolab@itc.it</email>
<email confidence="0.991433">rahul@isi.educoppolab@itc.it</email>
<email confidence="0.991433">timc@isi.educoppolab@itc.it</email>
<email confidence="0.991433">hovy@isi.educoppolab@itc.it</email>
<abstract confidence="0.9987375">Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This papresents a collection of methods for automatically learning admissible argument values to which an inference rule be applied, which we call and methods for filtering out incorrect inferences. We present empirical evidence of its effectiveness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
</authors>
<title>2001.Extracting Paraphrases from a Parallel Corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>50--57</pages>
<location>Toulose, France.</location>
<contexts>
<context position="2030" citStr="Barzilay and McKeown 2001" startWordPosition="285" endWordPosition="288">r this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to focus on automatic methods for building inference resources such as inference rule collections (Lin and Pantel 2001; Szpektor et al. 2004) and paraphrase collections (Barzilay and McKeown 2001). Using these resources in applications has been hindered by the large amount of incorrect inferences they generate, either because of altogether incorrect rules or because of blind application of plausible rules without considering the context of the relations or the senses of the words. For example, consider the following sentence: Terry Nichols was charged by federal prosecutors for murder and conspiracy in the Oklahoma City bombing. and an inference rule such as: X is charged by Y ⇒ Y announced the arrest of X (1) Using this rule, we can infer that “federal prosecutors announced the arrest</context>
<context position="5727" citStr="Barzilay and McKeown 2001" startWordPosition="861" endWordPosition="864"> automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither provides for the extracted in1 Some systems refer to inferences they extract as entailments; the two terms are sometimes used interchangeably. ference rules to hold or fail based on SPs. Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences. Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences. For instance the preference of win for the subject player, a nominaliz</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Barzilay, R.; and McKeown, K.R. 2001.Extracting Paraphrases from a Parallel Corpus. In Proceedings of ACL 2001. pp. 50–57. Toulose, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<pages>86--90</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="5273" citStr="Baker et al. 1998" startWordPosition="791" endWordPosition="794">Light and Greiff 2002). Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures. Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or derived automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither p</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Baker, C.F.; Fillmore, C.J.; and Lowe, J.B. 1998. The Berkeley FrameNet Project. In Proceedings of COLING/ACL 1998. pp. 86-90. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="10194" citStr="Cover and Thomas 1991" startWordPosition="1611" endWordPosition="1614">nce 〈x, p, y〉, we retrieve the sets C(x) and C(y) of the semantic classes that x and y belong to and accumulate the frequencies of the triples 〈c(x), p, c(y)〉, where c(x) ∈ C(x) and c(y) ∈ C(y)2. Each triple 〈c(x), p, c(y)〉 is a candidate selectional preference for p. Candidates can be incorrect when: a) they were generated from the incorrect sense of a polysemous word; or b) p does not hold for the other words in the semantic class. Intuitively, we have more confidence in a particular candidate if its semantic classes are closely associated given the relation p. Pointwise mutual information (Cover and Thomas 1991) is a commonly used metric for measuring this association strength between two events e1 and e2: 2 In this paper, the semantic classes C(x) and C(y) are extracted from WordNet and CBC (described in Section 4.2). We define our ranking function as the strength of association between two semantic classes, cx and cy3, given the relation p: ( ) ( ) P c c p , x y (3.2) pmi c p c p ; = log x y P ( c p) P ( c p) x y Let |cx, p, cy |denote the frequency of observing the instance 〈c(x), p, c(y)〉. We estimate the probabilities of Equation 3.2 using maximum likelihood estimates over our corpus: c p , ,∗ ∗</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1443" citStr="Fellbaum 1998" startWordPosition="194" endWordPosition="195">hods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness. 1 Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to focus on automatic methods for building inference resources such as inference rule collections (Lin and Pantel 2001; Szpektor et al. 2004) and paraphrase collections (Barzilay and McKeown 2001). Using these</context>
<context position="5206" citStr="Fellbaum 1998" startWordPosition="782" endWordPosition="783">al theory of Preference Semantics by Wilks, and more recently (Light and Greiff 2002). Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures. Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or derived automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pant</context>
<context position="18313" citStr="Fellbaum 1998" startWordPosition="3115" endWordPosition="3116"> very different sets of semantic classes, in the hope that in addition to learning semantic preferences, we might also uncover some clues for the eventual decisions about what makes good semantic classes in general. Our first set of semantic classes was directly extracted from the output of the CBC clustering algorithm (Pantel and Lin 2002). We applied CBC to the TREC-9 and TREC-2002 (Aquaint) newswire collections consisting of over 600 million words. CBC generated 1628 noun concepts and these were used as our semantic classes for SPs. Secondly, we extracted semantic classes from WordNet 2.1 (Fellbaum 1998). In the absence of any externally motivated distinguishing features (for example, the Basic Level categories from Prototype Theory, developed by Eleanor Rosch (1978)), we used the simple but effective method of manually truncating the noun synset hierarchy5 and considering all synsets below each cut point as part of the semantic class at that node. To select the cut points, we inspected several different hierarchy levels and found the synsets at a depth of 4 5 Only nouns are considered since DIRT semantic relations connect only nouns. to form the most natural semantic classes. Since the noun </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
</authors>
<title>Methods for Using Textual Entailment in Open-Domain Question Answering.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>905--912</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1191" citStr="Harabagiu and Hickl 2006" startWordPosition="156" endWordPosition="159">cations such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness. 1 Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exha</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual Entailment in Open-Domain Question Answering. In Proceedings of ACL 2006. pp. 905-912. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Katz</author>
<author>J A Fodor</author>
</authors>
<title>The Structure of a Semantic Theory.</title>
<date>1963</date>
<journal>Language,</journal>
<volume>39</volume>
<pages>170--210</pages>
<contexts>
<context position="4495" citStr="Katz and Fodor 1963" startWordPosition="670" endWordPosition="673">tial selectional preferences by aggregating statistics of inference rule instantiations over a large corpus of text. Within ISP, we explore different probabilistic models of selectional preference to accept or reject specific inferences. We present empirical evidence to support the following main contribution: Claim: Inferential selectional preferences can be automatically learned and used for effectively filtering out incorrect inferences. 2 Previous Work Selectional preference (SP) as a foundation for computational semantics is one of the earliest topics in AI and NLP, and has its roots in (Katz and Fodor 1963). Overviews of NLP research on this theme are (Wilks and Fass 1992), which includes the influential theory of Preference Semantics by Wilks, and more recently (Light and Greiff 2002). Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures. Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or d</context>
</contexts>
<marker>Katz, Fodor, 1963</marker>
<rawString>Katz, J.; and Fodor, J.A. 1963. The Structure of a Semantic Theory. Language, vol 39. pp.170–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lenat</author>
</authors>
<title>CYC: A large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1464" citStr="Lenat 1995" startWordPosition="198" endWordPosition="199">ncorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness. 1 Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to focus on automatic methods for building inference resources such as inference rule collections (Lin and Pantel 2001; Szpektor et al. 2004) and paraphrase collections (Barzilay and McKeown 2001). Using these resources in applica</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Lenat, D. 1995. CYC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="5239" citStr="Levin 1993" startWordPosition="787" endWordPosition="788"> Wilks, and more recently (Light and Greiff 2002). Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures. Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or derived automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems dif</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Light</author>
<author>W R Greiff</author>
</authors>
<title>Statistical Models for the Induction and Use of Selectional Preferences. Cognitive Science,26:269–281.</title>
<date>2002</date>
<contexts>
<context position="4677" citStr="Light and Greiff 2002" startWordPosition="699" endWordPosition="702">ional preference to accept or reject specific inferences. We present empirical evidence to support the following main contribution: Claim: Inferential selectional preferences can be automatically learned and used for effectively filtering out incorrect inferences. 2 Previous Work Selectional preference (SP) as a foundation for computational semantics is one of the earliest topics in AI and NLP, and has its roots in (Katz and Fodor 1963). Overviews of NLP research on this theme are (Wilks and Fass 1992), which includes the influential theory of Preference Semantics by Wilks, and more recently (Light and Greiff 2002). Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures. Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or derived automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Au</context>
</contexts>
<marker>Light, Greiff, 2002</marker>
<rawString>Light, M. and Greiff, W.R. 2002. Statistical Models for the Induction and Use of Selectional Preferences. Cognitive Science,26:269–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Parsing Without OverGeneration.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL-93.</booktitle>
<pages>112--120</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="21980" citStr="Lin 1993" startWordPosition="3691" endWordPosition="3692">.75±0.06 0.47±0.04 ISP.IIM.∧ minimum 10 0.33±0.07 0.77±0.06 0.55±0.04 ISP.IIM.v minimum 20 0.87±0.04 0.17±0.05 0.51±0.05 Indicates statistically significant results (with 95% confidence) when compared with all baseline systems using pairwise t-test. 5.1 Experimental Setup Model Implementation For each filtering algorithm in Section 3.3, ISP.JIM, ISP.IIM.∧, and ISP.IIM.v, we trained their probabilistic models using corpus statistics extracted from the 1999 AP newswire collection (part of the TREC-2002 Aquaint collection) consisting of approximately 31 million words. We used the Minipar parser (Lin 1993) to match DIRT patterns in the text. This permits exact matches since DIRT inference rules are built from Minipar parse trees. For each system, we experimented with the different ways of combining relational SP scores: minimum, maximum, and average (see Section 3.2). Also, we experimented with various values for the i parameter described in Section 3.3. Gold Standard Construction In order to compute the confusion matrices described in Section 4.3, we must first construct a representative set of inferences and manually annotate them as correct or incorrect. We randomly selected 100 inference ru</context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>Lin, D. 1993. Parsing Without OverGeneration. In Proceedings of ACL-93. pp. 112-120. Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Discovery of Inference Rules for Question Answering.</title>
<date>2001</date>
<journal>Natural Language Engineering</journal>
<pages>7--4</pages>
<contexts>
<context position="1952" citStr="Lin and Pantel 2001" startWordPosition="274" endWordPosition="277">sources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to focus on automatic methods for building inference resources such as inference rule collections (Lin and Pantel 2001; Szpektor et al. 2004) and paraphrase collections (Barzilay and McKeown 2001). Using these resources in applications has been hindered by the large amount of incorrect inferences they generate, either because of altogether incorrect rules or because of blind application of plausible rules without considering the context of the relations or the senses of the words. For example, consider the following sentence: Terry Nichols was charged by federal prosecutors for murder and conspiracy in the Oklahoma City bombing. and an inference rule such as: X is charged by Y ⇒ Y announced the arrest of X (1</context>
<context position="5814" citStr="Lin and Pantel 2001" startWordPosition="876" endWordPosition="879">llbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither provides for the extracted in1 Some systems refer to inferences they extract as entailments; the two terms are sometimes used interchangeably. ference rules to hold or fail based on SPs. Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences. Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences. For instance the preference of win for the subject player, a nominalization of play, is used to derive that “win =&gt; play”. Our work can be viewed as compleme</context>
<context position="16850" citStr="Lin and Pantel 2001" startWordPosition="2869" endWordPosition="2872"> In the next sections, we describe our collection of inference rules, the semantic classes used for forming selectional preferences, and evaluation criteria for measuring the filtering quality. 4 Recall that the inference rules we consider in this paper are not necessary strict logical inference rules, but plausible inference rules; see Section 3. 567 4.1 Inference Rules Our models for learning inferential selectional preferences can be applied to any collection of inference rules between binary semantic relations. In this paper, we focus on the inference rules contained in the DIRT resource (Lin and Pantel 2001). DIRT consists of over 12 million rules which were extracted from a 1GB newspaper corpus (San Jose Mercury, Wall Street Journal and AP Newswire from the TREC-9 collection). For example, here are DIRT’s top 3 inference rules for “X solves Y”: “Y is solved by X”, “X resolves Y”, “X finds a solution to Y” 4.2 Semantic Classes The choice of semantic classes is of great importance for selectional preference. One important aspect is the granularity of the classes. Too general a class will provide no discriminatory power while too fine-grained a class will offer little generalization and apply in on</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for Question Answering. Natural Language Engineering 7(4):343-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D I Moldovan</author>
<author>C Clark</author>
<author>S M Harabagiu</author>
<author>S J Maiorano</author>
</authors>
<title>COGEX: A Logic Prover for Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL-03.</booktitle>
<pages>87--93</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1164" citStr="Moldovan et al. 2003" startWordPosition="152" endWordPosition="155">lts when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness. 1 Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-inten</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>Moldovan, D.I.; Clark, C.; Harabagiu, S.M.; Maiorano, S.J. 2003. COGEX: A Logic Prover for Question Answering. In Proceedings of HLT-NAACL-03. pp. 87-93. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of KDD-02.</booktitle>
<pages>613--619</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5514" citStr="Pantel and Lin 2002" startWordPosition="826" endWordPosition="829">Ps for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or derived automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither provides for the extracted in1 Some systems refer to inferences they extract as entailments; the two terms are sometimes used interchangeably. ference rules to hold or fail based on SPs. Zanzotto et al. (2006) recently explored a different in</context>
<context position="8944" citStr="Pantel and Lin 2002" startWordPosition="1400" endWordPosition="1403">electional Preferences Resnik (1996) defined the selectional preferences of a predicate as the semantic classes of the words that appear as its arguments. Similarly, we define the relational selectional preferences of a binary semantic relation pi as the semantic classes C(x) of the words that can be instantiated for x and as the semantic classes C(y) of the words that can be instantiated for y. The semantic classes C(x) and C(y) can be obtained from a conceptual taxonomy as proposed in (Resnik 1996), such as WordNet, or from the classes extracted from a word clustering algorithm such as CBC (Pantel and Lin 2002). For example, given the relation “X is charged by Y”, its relational selection preferences from WordNet could be {social group, organism, state...} for X and {authority, state, section...} for Y. Below we propose joint and independent models, based on a corpus analysis, for automatically determining relational selectional preferences. Model 1: Joint Relational Model (JRM) Our joint model uses a corpus analysis to learn SPs for binary semantic relations by considering their arguments jointly, as in example (2). Given a large corpus of English text, we first find the occurrences of each semanti</context>
<context position="18041" citStr="Pantel and Lin 2002" startWordPosition="3071" endWordPosition="3074">eneralization and apply in only extremely few cases. The absence of an attested high-quality set of semantic classes for this task makes discovering preferences difficult. Since many of the criteria for developing such a set are not even known, we decided to experiment with two very different sets of semantic classes, in the hope that in addition to learning semantic preferences, we might also uncover some clues for the eventual decisions about what makes good semantic classes in general. Our first set of semantic classes was directly extracted from the output of the CBC clustering algorithm (Pantel and Lin 2002). We applied CBC to the TREC-9 and TREC-2002 (Aquaint) newswire collections consisting of over 600 million words. CBC generated 1628 noun concepts and these were used as our semantic classes for SPs. Secondly, we extracted semantic classes from WordNet 2.1 (Fellbaum 1998). In the absence of any externally motivated distinguishing features (for example, the Basic Level categories from Prototype Theory, developed by Eleanor Rosch (1978)), we used the simple but effective method of manually truncating the noun synset hierarchy5 and considering all synsets below each cut point as part of the seman</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In Proceedings of KDD-02. pp. 613-619. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional Constraints: An Information-Theoretic Model and its Computational Realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="4814" citStr="Resnik (1996)" startWordPosition="721" endWordPosition="722">tial selectional preferences can be automatically learned and used for effectively filtering out incorrect inferences. 2 Previous Work Selectional preference (SP) as a foundation for computational semantics is one of the earliest topics in AI and NLP, and has its roots in (Katz and Fodor 1963). Overviews of NLP research on this theme are (Wilks and Fass 1992), which includes the influential theory of Preference Semantics by Wilks, and more recently (Light and Greiff 2002). Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures. Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or derived automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis </context>
<context position="8360" citStr="Resnik (1996)" startWordPosition="1301" endWordPosition="1302">s distinction between joint and independent selectional preferences constitutes the difference between the two models we present in this section. The remainder of this section describes the ISP approach. In Section 3.1, we describe methods for automatically determining the semantic contexts of each single relation’s selectional preferences. Section 3.2 uses these for developing our inferential 565 P e e ( ) (3.1) 1 2 , pmi e e = ( ; ) log 1 2 P(e1 )P(e2 ) selectional preference models. Finally, we propose inference filtering algorithms in Section 3.3. cx 3.1 Relational Selectional Preferences Resnik (1996) defined the selectional preferences of a predicate as the semantic classes of the words that appear as its arguments. Similarly, we define the relational selectional preferences of a binary semantic relation pi as the semantic classes C(x) of the words that can be instantiated for x and as the semantic classes C(y) of the words that can be instantiated for y. The semantic classes C(x) and C(y) can be obtained from a conceptual taxonomy as proposed in (Resnik 1996), such as WordNet, or from the classes extracted from a word clustering algorithm such as CBC (Pantel and Lin 2002). For example, g</context>
<context position="10928" citStr="Resnik 1996" startWordPosition="1793" endWordPosition="1794">ntic classes C(x) and C(y) are extracted from WordNet and CBC (described in Section 4.2). We define our ranking function as the strength of association between two semantic classes, cx and cy3, given the relation p: ( ) ( ) P c c p , x y (3.2) pmi c p c p ; = log x y P ( c p) P ( c p) x y Let |cx, p, cy |denote the frequency of observing the instance 〈c(x), p, c(y)〉. We estimate the probabilities of Equation 3.2 using maximum likelihood estimates over our corpus: c p , ,∗ ∗,, p c c p c , , (3.3) y y P c p x ( ) ∗ ∗ x x y = , , Pc p ( ) ∗ ∗ y = , , P c c p x ( ) ∗ ∗ , = , , p p p Similarly to (Resnik 1996), we estimate the above frequencies using: w p , , ∗ ∗ pw ,, w p w 1 , , 2 , , p ∗ = ∑ ∗ , , p c = ∑ c p c , , = y x y ∈ ∑∈ C w ( ) C w C w C w w cx ∈ ( ) ( ) ( ) × w cy ∈ w c x w c y 1 , 2 1 2 the arrest we may find occurrences from our corpus of the particular class for X and for Y, however we may never see both of these classes co-occurring even though they would form a valid relational selectional preference. To alleviate this problem, we propose a second model that is less strict by considering the arguments of the binary semantic relations independently, as in example (3). Similarly to J</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Resnik, P. 1996. Selectional Constraints: An Information-Theoretic Model and its Computational Realization. Cognition, 61:127–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Romano</author>
<author>M Kouylekov</author>
<author>I Szpektor</author>
<author>I Dagan</author>
<author>A Lavelli</author>
</authors>
<title>Investigating a Generic Paraphrase-Based Approach for Relation Extraction.</title>
<date>2006</date>
<booktitle>In EACL-2006.</booktitle>
<pages>409--416</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="1236" citStr="Romano et al. 2006" startWordPosition="162" endWordPosition="165">wering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness. 1 Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to</context>
</contexts>
<marker>Romano, Kouylekov, Szpektor, Dagan, Lavelli, 2006</marker>
<rawString>Romano, L.; Kouylekov, M.; Szpektor, I.; Dagan, I.; Lavelli, A. 2006. Investigating a Generic Paraphrase-Based Approach for Relation Extraction. In EACL-2006. pp. 409-416. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rosch</author>
</authors>
<title>Human Categorization.</title>
<date>1978</date>
<booktitle>Cognition and Categorization.</booktitle>
<editor>In E. Rosch and B.B. Lloyd (eds.)</editor>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="18479" citStr="Rosch (1978)" startWordPosition="3139" endWordPosition="3140">ut what makes good semantic classes in general. Our first set of semantic classes was directly extracted from the output of the CBC clustering algorithm (Pantel and Lin 2002). We applied CBC to the TREC-9 and TREC-2002 (Aquaint) newswire collections consisting of over 600 million words. CBC generated 1628 noun concepts and these were used as our semantic classes for SPs. Secondly, we extracted semantic classes from WordNet 2.1 (Fellbaum 1998). In the absence of any externally motivated distinguishing features (for example, the Basic Level categories from Prototype Theory, developed by Eleanor Rosch (1978)), we used the simple but effective method of manually truncating the noun synset hierarchy5 and considering all synsets below each cut point as part of the semantic class at that node. To select the cut points, we inspected several different hierarchy levels and found the synsets at a depth of 4 5 Only nouns are considered since DIRT semantic relations connect only nouns. to form the most natural semantic classes. Since the noun hierarchy in WordNet has an average depth of 12, our truncation created a set of concepts considerably coarser-grained than WordNet itself. The cut produced 1287 sema</context>
</contexts>
<marker>Rosch, 1978</marker>
<rawString>Rosch, E. 1978. Human Categorization. In E. Rosch and B.B. Lloyd (eds.) Cognition and Categorization. Hillsdale, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siegel</author>
<author>Castellan Jr</author>
<author>N J</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill.</publisher>
<marker>Siegel, Jr, J, 1988</marker>
<rawString>Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>H Tanev</author>
<author>I Dagan</author>
<author>B Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>41--48</pages>
<publisher>Barcelona,Spain.</publisher>
<contexts>
<context position="1283" citStr="Szpektor et al. 2004" startWordPosition="169" endWordPosition="172"> of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness. 1 Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to focus on automatic methods for building infere</context>
<context position="5783" citStr="Szpektor et al. 2004" startWordPosition="870" endWordPosition="873">e the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither provides for the extracted in1 Some systems refer to inferences they extract as entailments; the two terms are sometimes used interchangeably. ference rules to hold or fail based on SPs. Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences. Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences. For instance the preference of win for the subject player, a nominalization of play, is used to derive that “win =&gt; play”. Our</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP 2004. pp. 41-48. Barcelona,Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>D Fass</author>
</authors>
<title>Preference Semantics: a family history.</title>
<date>1992</date>
<booktitle>Computing and Mathematics with Applications, 23(2). A shorter version in the second edition of the Encyclopedia of Artificial Intelligence,</booktitle>
<editor>(ed.) S. Shapiro.</editor>
<contexts>
<context position="4562" citStr="Wilks and Fass 1992" startWordPosition="682" endWordPosition="685"> rule instantiations over a large corpus of text. Within ISP, we explore different probabilistic models of selectional preference to accept or reject specific inferences. We present empirical evidence to support the following main contribution: Claim: Inferential selectional preferences can be automatically learned and used for effectively filtering out incorrect inferences. 2 Previous Work Selectional preference (SP) as a foundation for computational semantics is one of the earliest topics in AI and NLP, and has its roots in (Katz and Fodor 1963). Overviews of NLP research on this theme are (Wilks and Fass 1992), which includes the influential theory of Preference Semantics by Wilks, and more recently (Light and Greiff 2002). Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures. Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or derived automatically. Manual collections of semantic classes includ</context>
</contexts>
<marker>Wilks, Fass, 1992</marker>
<rawString>Wilks, Y.; and Fass, D. 1992. Preference Semantics: a family history. Computing and Mathematics with Applications, 23(2). A shorter version in the second edition of the Encyclopedia of Artificial Intelligence, (ed.) S. Shapiro.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>M Pennacchiotti</author>
<author>M T Pazienza</author>
</authors>
<title>Discovering Asymmetric Entailment Relations between Verbs using Selectional Preferences.</title>
<date>2006</date>
<booktitle>In COLING/ACL-06.</booktitle>
<pages>849--856</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="6081" citStr="Zanzotto et al. (2006)" startWordPosition="920" endWordPosition="923">r entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither provides for the extracted in1 Some systems refer to inferences they extract as entailments; the two terms are sometimes used interchangeably. ference rules to hold or fail based on SPs. Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences. Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences. For instance the preference of win for the subject player, a nominalization of play, is used to derive that “win =&gt; play”. Our work can be viewed as complementary to the work on extracting semantic inferences and paraphrases, since we seek to refine when a given inference applies, filtering out incorrect inferences. 3 Selectional Preference Models The aim of this paper is to learn inferential selectional preferences for </context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering Asymmetric Entailment Relations between Verbs using Selectional Preferences. In COLING/ACL-06. pp. 849-856. Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>