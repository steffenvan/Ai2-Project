<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001281">
<title confidence="0.77992">
What to be? - Electronic Career Guidance Based on Semantic Relatedness
</title>
<author confidence="0.855346">
Iryna Gurevych, Christof M¨uller and Torsten Zesch
</author>
<affiliation confidence="0.917188">
Ubiquitous Knowledge Processing Group
Telecooperation, Darmstadt University of Technology
</affiliation>
<address confidence="0.834027">
Hochschulstr. 10, 64289 Darmstadt, Germany
</address>
<email confidence="0.8082605">
http://www.ukp.tu-darmstadt.de
{gurevych,mueller,zesch}@tk.informatik.tu-darmstadt.de
</email>
<sectionHeader confidence="0.998556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999844333333333">
We present a study aimed at investigating
the use of semantic information in a novel
NLP application, Electronic Career Guid-
ance (ECG), in German. ECG is formu-
lated as an information retrieval (IR) task,
whereby textual descriptions of professions
(documents) are ranked for their relevance
to natural language descriptions of a per-
son’s professional interests (the topic). We
compare the performance of two semantic
IR models: (IR-1) utilizing semantic relat-
edness (SR) measures based on either word-
net or Wikipedia and a set of heuristics,
and (IR-2) measuring the similarity between
the topic and documents based on Explicit
Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007). We evaluate the perfor-
mance of SR measures intrinsically on the
tasks of (T-1) computing SR, and (T-2) solv-
ing Reader’s Digest Word Power (RDWP)
questions.
</bodyText>
<sectionHeader confidence="0.988312" genericHeader="method">
1 Electronic Career Guidance
</sectionHeader>
<bodyText confidence="0.999100780487805">
Career guidance is important both for the person in-
volved and for the state. Not well informed deci-
sions may cause people to drop the training program
they are enrolled in, yielding loss of time and finan-
cial investments. However, there is a mismatch bet-
ween what people know about existing professions
and the variety of professions, which exist in real-
ity. Some studies report that school leavers typi-
cally choose the professions known to them, such
as policeman, nurse, etc. Many other professions,
which can possibly match the interests of the person
very well, are not chosen, as their titles are unknown
and people seeking career advice do not know about
their existence, e.g. electronics installer, or chem-
ical laboratory worker. However, people are very
good at describing their professional interests in nat-
ural language. That is why they are even asked to
write a short essay prior to an appointment with a
career guidance expert.
Electronic career guidance is, thus, a supplement
to career guidance by human experts, helping young
people to decide which profession to choose. The
goal is to automatically compute a ranked list of pro-
fessions according to the user’s interests. A current
system employed by the German Federal Labour
Office (GFLO) in their automatic career guidance
front-end1 is based on vocational trainings, manu-
ally annotated using a tagset of 41 keywords. The
user must select appropriate keywords according to
her interests. In reply, the system consults a knowl-
edge base with professions manually annotated with
the keywords by career guidance experts. There-
after, it outputs a list of the best matching profes-
sions to the user. This approach has two significant
disadvantages. Firstly, the knowledge base has to
be maintained and steadily updated, as the number
of professions and keywords associated with them
is continuously changing. Secondly, the user has to
describe her interests in a very restricted way.
At the same time, GFLO maintains an extensive
database with textual descriptions of professions,
</bodyText>
<footnote confidence="0.985359">
1http://www.interesse-beruf.de/
</footnote>
<page confidence="0.900907">
1032
</page>
<note confidence="0.9271085">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1032–1039,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.986054033333333">
called BERUFEnet.2 Therefore, we cast the prob-
lem of ECG as an IR task, trying to remove the
disadvantages of conventional ECG outlined above
by letting the user describe her interests in a short
natural language essay, called a professional profile.
Example essay translated to English
I would like to work with animals, to treat and look
after them, but I cannot stand the sight of blood and
take too much pity on them. On the other hand, I like
to work on the computer, can program in C, Python and
VB and so I could consider software development as an
appropriate profession. I cannot imagine working in a
kindergarden, as a social worker or as a teacher, as I
am not very good at asserting myself.
Textual descriptions of professions are ranked
given such an essay by using NLP and IR tech-
niques. As essays and descriptions of professions
display a mismatch between the vocabularies of top-
ics and documents and there is lack of contextual in-
formation, due to the documents being fairly short
as compared to standard IR scenarios, lexical se-
mantic information should be especially beneficial
to an IR system. For example, the profile can con-
tain words about some objects or activities related to
the profession, but not directly mentioned in the de-
scription, e.g. oven, cakes in the profile and pastries,
baker, or confectioner in the document. Therefore,
we propose to utilize semantic relatedness as a rank-
ing function instead of conventional IR techniques,
as will be substantiated below.
</bodyText>
<sectionHeader confidence="0.950205" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999164357142857">
Integrating lexical semantic knowledge in ECG re-
quires the existence of knowledge bases encoding
domain and lexical knowledge. In this paper, we in-
vestigate the utility of two knowledge bases: (i) a
German wordnet, GermaNet (Kunze, 2004), and (ii)
the German portion of Wikipedia.3 A large body of
research exists on using wordnets in NLP applica-
tions and in particular in IR (Moldovan and Mihal-
cea, 2000). The knowledge in wordnets has been
typically utilized by expanding queries with related
terms (Vorhees, 1994; Smeaton et al., 1994), con-
cept indexing (Gonzalo et al., 1998), or similarity
measures as ranking functions (Smeaton et al., 1994;
M¨uller and Gurevych, 2006). Recently, Wikipedia
</bodyText>
<footnote confidence="0.998895666666667">
2http://infobub.arbeitsagentur.de/
berufe/
3http://de.wikipedia.org/
</footnote>
<bodyText confidence="0.999864090909091">
has been discovered as a promising lexical seman-
tic resource and successfully used in such different
NLP tasks as question answering (Ahn et al., 2004),
named entity disambiguation (Bunescu and Pasca,
2006), and information retrieval (Katz et al., 2005).
Further research (Zesch et al., 2007b) indicates that
German wordnet and Wikipedia show different per-
formance depending on the task at hand.
Departing from this, we first compare two seman-
tic relatedness (SR) measures based on the informa-
tion either in the German wordnet (Lin, 1998) called
LIN, or in Wikipedia (Gabrilovich and Markovitch,
2007) called Explicit Semantic Analysis, or ESA.
We evaluate their performance intrinsically on the
tasks of (T-1) computing semantic relatedness, and
(T-2) solving Reader’s Digest Word Power (RDWP)
questions and make conclusions about the ability of
the measures to model certain aspects of semantic
relatedness and their coverage. Furthermore, we fol-
low the approach by M¨uller and Gurevych (2006),
who proposed to utilize the LIN measure and a set
of heuristics as an IR model (IR-1).
Additionally, we utilize the ESA measure in a
semantic information retrieval model, as this mea-
sure is significantly better at vocabulary cover-
age and at modelling cross part-of-speech relations
(Gabrilovich and Markovitch, 2007). We compare
the performance of ESA and LIN measures in a task-
based IR evaluation and analyze their strengths and
limitations. Finally, we apply ESA to directly com-
pute text similarities between topics and documents
(IR-2) and compare the performance of two seman-
tic IR models and a baseline Extended Boolean (EB)
model (Salton et al., 1983) with query expansion.4
To summarize, the contributions of this paper are
three-fold: (i) we present a novel system, utilizing
NLP and IR techniques to perform Electronic Career
Guidance, (ii) we study the properties and intrinsi-
cally evaluate two SR measures based on GermaNet
and Wikipedia for the tasks of computing seman-
tic relatedness and solving Reader’s Digest Word
Power Game questions, and (iii) we investigate the
performance of two semantic IR models in a task
based evaluation.
</bodyText>
<footnote confidence="0.99458875">
4We also ran experiments with Okapi BM25 model as im-
plemented in the Terrier framework, but the results were worse
than those with the EB model. Therefore, we limit our discus-
sion to the latter.
</footnote>
<page confidence="0.991414">
1033
</page>
<sectionHeader confidence="0.95276" genericHeader="method">
3 Computing Semantic Relatedness
</sectionHeader>
<subsectionHeader confidence="0.992861">
3.1 SR Measures
</subsectionHeader>
<bodyText confidence="0.999943478260869">
GermaNet based measures GermaNet is a Ger-
man wordnet, which adopted the major properties
and database technology from Princeton’s Word-
Net (Fellbaum, 1998). However, GermaNet dis-
plays some structural differences and content ori-
ented modifications. Its designers relied mainly on
linguistic evidence, such as corpus frequency, rather
than psycholinguistic motivations. Also, GermaNet
employs artificial, i.e. non-lexicalized concepts, and
adjectives are structured hierarchically as opposed
to WordNet. Currently, GermaNet includes about
40000 synsets with more than 60000 word senses
modelling nouns, verbs and adjectives.
We use the semantic relatedness measure by Lin
(1998) (referred to as LIN), as it consistently is
among the best performing wordnet based measures
(Gurevych and Niederlich, 2005; Budanitsky and
Hirst, 2006). Lin defined semantic similarity using a
formula derived from information theory. This mea-
sure is sometimes called a universal semantic sim-
ilarity measure as it is supposed to be application,
domain, and resource independent. Lin is computed
as:
</bodyText>
<equation confidence="0.983027">
2 x log p(LCS(c1, c2))
log p(c1) + log p(c2)
</equation>
<bodyText confidence="0.999503941176471">
where c1 and c2 are concepts (word senses) corre-
sponding to w1 and w2, log p(c) is the information
content, and LCS(c1, c2) is the lowest common sub-
sumer of the two concepts. The probability p is com-
puted as the relative frequency of words (represent-
ing that concept) in the taz5 corpus.
Wikipedia based measures Wikipedia is a free
online encyclopedia that is constructed in a col-
laborative effort of voluntary contributors and still
grows exponentially. During this process, Wikipedia
has probably become the largest collection of freely
available knowledge. Wikipedia shares many of
its properties with other well known lexical seman-
tic resources (like dictionaries, thesauri, semantic
wordnets or conventional encyclopedias) (Zesch et
al., 2007a). As Wikipedia also models relatedness
between concepts, it is better suited for computing
</bodyText>
<footnote confidence="0.740828">
5http://www.taz.de
</footnote>
<bodyText confidence="0.9998428">
semantic relatedness than GermaNet (Zesch et al.,
2007b).
In very recent work, Gabrilovich and Markovitch
(2007) introduce a SR measure called Explicit Se-
mantic Analysis (ESA). The ESA measure repre-
sents the meaning of a term as a high-dimensional
concept vector. The concept vector is derived from
Wikipedia articles, as each article focuses on a cer-
tain topic, and can thus be viewed as expressing a
concept. The dimension of the concept vector is the
number of Wikipedia articles. Each element of the
vector is associated with a certain Wikipedia article
(or concept). If the term can be found in this article,
the term’s tfidf score (Salton and McGill, 1983) in
this article is assigned to the vector element. Oth-
erwise, 0 is assigned. As a result, a term’s con-
cept vector represents the importance of the term for
each concept. Semantic relatedness of two terms can
then be easily computed as the cosine of their corre-
sponding concept vectors. If we want to measure
the semantic relatedness of texts instead of terms,
we can also use ESA concept vectors. A text is rep-
resented as the average concept vector of its terms’
concept vectors. Then, the relatedness of two texts
is computed as the cosine of their average concept
vectors.
As ESA uses all textual information in Wikipedia,
the measure shows excellent coverage. Therefore,
we select it as the second measure for integration
into our IR system.
</bodyText>
<subsectionHeader confidence="0.993961">
3.2 Datasets
</subsectionHeader>
<bodyText confidence="0.9999940625">
Semantic relatedness datasets for German em-
ployed in our study are presented in Table 1.
Gurevych (2005) conducted experiments with two
datasets: i) a German translation of the English
dataset by Rubenstein and Goodenough (1965)
(Gur65), and ii) a larger dataset containing 350
word pairs (Gur350). Zesch and Gurevych (2006)
created a third dataset from domain-specific corpora
using a semi-automatic process (ZG222). Gur65 is
rather small and contains only noun-noun pairs con-
nected by either synonymy or hypernymy. Gur350
contains nouns, verbs and adjectives that are con-
nected by classical and non-classical relations (Mor-
ris and Hirst, 2004). However, word pairs for
this dataset are biased towards strong classical rela-
tions, as they were manually selected from a corpus.
</bodyText>
<equation confidence="0.888842">
simC1,C2 =
</equation>
<page confidence="0.965593">
1034
</page>
<table confidence="0.924795111111111">
DATASET YEAR LANGUAGE # PAIRS POS
Gur65 2005 German 65 N
Gur350 2006 German 350 N, V, A
ZG222 2006 German 222 N, V, A
CORRELATION r
SCORES # SUBJECTS INTER INTRA
discrete {0,1,2,3,4} 24 .810 -
discrete {0,1,2,3,4} 8 .690 -
discrete {0,1,2,3,4} 21 .490 .647
</table>
<tableCaption confidence="0.764304">
Table 1: Comparison of datasets used for evaluating semantic relatedness in German.
ZG222 does not have this bias.
</tableCaption>
<bodyText confidence="0.999850166666667">
Following the work by Jarmasz and Szpakow-
icz (2003) and Turney (2006), we created a sec-
ond dataset containing multiple choice questions.
We collected 1072 multiple-choice word analogy
questions from the German Reader’s Digest Word
Power Game (RDWP) from January 2001 to De-
cember 2005 (Wallace and Wallace, 2005). We dis-
carded 44 questions that had more than one correct
answer, and 20 questions that used a phrase instead
of a single term as query. The resulting 1008 ques-
tions form our evaluation dataset. An example ques-
tion is given below:
</bodyText>
<figure confidence="0.985873">
Muffin (muffin)
a) Kleingeb¨ack (small cake)
b) Spenglerwerkzeug (plumbing tool)
c) Miesepeter (killjoy)
d) Wildschaf (moufflon)
</figure>
<bodyText confidence="0.99449925">
The task is to find the correct choice - ‘a)’ in this
case.
This dataset is significantly larger than any of the
previous datasets employed in this type of evalua-
tion. Also, it is not restricted to synonym questions,
as in the work by Jarmasz and Szpakowicz (2003),
but also includes hypernymy/hyponymy, and few
non-classical relations.
</bodyText>
<subsectionHeader confidence="0.999942">
3.3 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.999906583333333">
Table 2 gives the results of evaluation on the task
of correlating the results of an SR measure with hu-
man judgments using Pearson correlation. The Ger-
maNet based LIN measure outperforms ESA on the
Gur65 dataset. On the other datasets, ESA is better
than LIN. This is clearly due to the fact, that Gur65
contains only noun-noun word pairs connected by
classical semantic relations, while the other datasets
also contain cross part-of-speech pairs connected by
non-classical relations. The Wikipedia based ESA
measure can better capture such relations. Addition-
ally, Table 3 shows that ESA also covers almost all
</bodyText>
<table confidence="0.9989276">
GUR65 GUR350 ZG222
# covered word pairs 53 116 55
Upper bound 0.80 0.64 0.44
GermaNet Lin 0.73 0.50 0.08
Wikipedia ESA 0.56 0.52 0.32
</table>
<tableCaption confidence="0.960814666666667">
Table 2: Pearson correlation r of human judgments
with SR measures on word pairs covered by Ger-
maNet and Wikipedia.
</tableCaption>
<table confidence="0.9996094">
COVERED PAIRS
DATASET # PAIRS LIN ESA
Gur65 65 60 65
Gur350 350 208 333
ZG222 222 88 205
</table>
<tableCaption confidence="0.914709">
Table 3: Number of covered word pairs based on Lin
or ESA measure on different datasets.
</tableCaption>
<bodyText confidence="0.994024625">
word pairs in each dataset, while GermaNet is much
lower for Gur350 and ZG222. ESA performs even
better on the Reader’s Digest task (see Table 4). It
shows high coverage and near human performance
regarding the relative number of correctly solved
questions.6 Given the high performance and cover-
age of the Wikipedia based ESA measure, we expect
it to yield better IR results than LIN.
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="method">
4 Information Retrieval
</sectionHeader>
<subsectionHeader confidence="0.649823">
4.1 IR Models
</subsectionHeader>
<bodyText confidence="0.993451666666667">
Preprocessing For creating the search index for
IR models, we apply first tokenization and then re-
move stop words. We use a general German stop
</bodyText>
<footnote confidence="0.623282333333333">
6Values for human performance are for one subject. Thus,
they only indicate the approximate difficulty of the task. We
plan to use this dataset with a much larger group of subjects.
</footnote>
<table confidence="0.99365325">
#ANSWERED #CORRECT RATIO
Human 1008 874 0.87
GermaNet Lin 298 153 0.51
Wikipedia ESA 789 572 0.72
</table>
<tableCaption confidence="0.9940895">
Table 4: Evaluation results on multiple-choice word
analogy questions.
</tableCaption>
<page confidence="0.989339">
1035
</page>
<bodyText confidence="0.999766">
word list extended with highly frequent domain spe-
cific terms. Before adding the remaining words to
the index, they are lemmatized. We finally split
compounds into their constituents, and add both,
constituents and compounds, to the index.
EB model Lucene7 is an open source text search
library based on an EB model. After matching the
preprocessed queries against the index, the docu-
ment collection is divided into a set of relevant and
irrelevant documents. The set of relevant documents
is, then, ranked according to the formula given in the
following equation:
</bodyText>
<equation confidence="0.954118333333333">
ny
rEB(d, q) = tf(tq, d)·idf(tq)·lengthNorm(d)
i=1
</equation>
<bodyText confidence="0.997199115384615">
where nq is the number of terms in the query,
tf(tq, d) is the term frequency factor for term tq
in document d, idf(tq) is the inverse document fre-
quency of the term, and lengthNorm(d) is a nor-
malization value of document d, given the number
of terms within the document. We added a simple
query expansion algorithm using (i) synonyms, and
(ii) hyponyms, extracted from GermaNet.
IR based on SR For the (IR-1) model, we uti-
lize two SR measures and a set of heuristics: (i)
the Lin measure based on GermaNet (LIN), and (ii)
the ESA measure based on Wikipedia (ESA-Word).
This algorithm was applied to the German IR bench-
mark with positive results by M¨uller and Gurevych
(2006). The algorithm computes a SR score for each
query and document term pair. Scores above a pre-
defined threshold are summed up and weighted by
different factors, which boost or lower the scores for
documents, depending on how many query terms are
contained exactly or contribute a high enough SR
score. In order to integrate the strengths of tradi-
tional IR models, the inverse document frequency
idf is considered, which measures the general im-
portance of a term for predicting the content of a
document. The final formula of the model is as fol-
lows:
</bodyText>
<footnote confidence="0.924452">
7http://lucene.apache.org
</footnote>
<bodyText confidence="0.999959181818182">
where nd is the number of tokens in the document,
nq the number of tokens in the query, td,i the i-th
document token, tq,j the j-th query token, s(td,i, tq,j)
the SR score for the respective document and query
term, nnsm the number of query terms not exactly
contained in the document, nnr the number of query
tokens, which do not contribute a SR score above the
threshold.
For the (IR-2) model, we apply the ESA method
for directly comparing the query with documents, as
described in Section 3.1.
</bodyText>
<subsectionHeader confidence="0.966487">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.999932272727273">
The corpus employed in our experiments was built
based on a real-life IR scenario in the domain of
ECG, as described in Section 1. The document col-
lection is extracted from BERUFEnet,8 a database
created by the GFLO. It contains textual descrip-
tions of about 1,800 vocational trainings, and 4,000
descriptions of professions. We restrict the collec-
tion to a subset of BERUFEnet documents, consist-
ing of 529 descriptions of vocational trainings, due
to the process necessary to obtain relevance judg-
ments, as described below. The documents contain
not only details of professions, but also a lot of infor-
mation concerning the training and administrative
issues. We only use those portions of the descrip-
tions, which characterize the profession itself.
We collected real natural language topics by ask-
ing 30 human subjects to write an essay about their
professional interests. The topics contain 130 words,
on average. Making relevance judgments for ECG
requires domain expertise. Therefore, we applied an
automatic method, which uses the knowledge base
employed by the GFLO, described in Section 1. To
obtain relevance judgments, we first annotate each
essay with relevant keywords from the tagset of 41
and retrieve a ranked list of professions, which were
assigned one or more keywords by domain experts.
To map the ranked list to a set of relevant and ir-
relevant professions, we use a threshold of 3, as
suggested by career guidance experts. This setting
yields on average 93 relevant documents per topic.
The quality of the automatically created gold stan-
dard depends on the quality of the applied knowl-
edge base. As the knowledge base was created by
</bodyText>
<equation confidence="0.883391333333333">
8http://berufenet.arbeitsamt.de/
rSR(d, q) = (1 + nnsm) · (1 + nnr)
Eid1 Ej91 idf (tq,j) · s(td,i, tq,j)
</equation>
<page confidence="0.958953">
1036
</page>
<bodyText confidence="0.9998895">
domain experts and is at the core of the electronic ca-
reer guidance system of the GFLO, we assume that
the quality is adequate to ensure a reliable evalua-
tion.
</bodyText>
<subsectionHeader confidence="0.999694">
4.3 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.999838053571429">
In Table 5, we summarize the results of the ex-
periments applying different IR models on the
BERUFEnet data. We build queries from natural
language essays by (QT-1) extracting nouns, verbs,
and adjectives, (QT-2) using only nouns, and (QT-
3) manually assigning suitable keywords from the
tagset with 41 keywords to each topic. We report the
results with two different thresholds (.85 and .98) for
the Lin model, and with three different thresholds
(.11, .13 and .24) for the ESA-Word models. The
evaluation metrics used are mean average precision
(MAP), precision after ten documents (P10), the
number of relevant returned documents (#RRD). We
compute the absolute value of Spearman’s rank cor-
relation coefficient (SRCC) by comparing the rele-
vance ranking of our system with the relevance rank-
ing of the knowledge base employed by the GFLO.
Using query expansion for the EB model de-
creases the retrieval performance for most configu-
rations. The SR based models outperform the EB
model in all configurations and evaluation metrics,
except for P10 on the keyword based queries. The
Lin model is always outperformed by at least one of
the ESA models, except for (QT-3). (IR-2) performs
best on longer queries using nouns, verbs, adjectives
or just nouns.
Comparing the number of relevant retrieved doc-
uments, we observe that the IR models based on SR
are able to return more relevant documents than the
EB model. This supports the claim that semantic
knowledge is especially helpful for the vocabulary
mismatch problem, which cannot be addressed by
conventional IR models. E.g., only SR-based mod-
els can find the job information technician for a pro-
file which contains the sentence My interests and
skills are in the field of languages and IT. The job
could only be judged as relevant, as the semantic
relation between IT in the profile and information
technology in the professional description could be
found.
In our analysis of the BERUFEnet results ob-
tained on (QT-1), we noticed that many errors were
due to the topics expressed in free natural language
essays. Some subjects deviated from the given task
to describe their professional interests and described
facts that are rather irrelevant to the task of ECG,
e.g. It is important to speak different languages in
the growing European Union. If all content words
are extracted to build a query, a lot of noise is intro-
duced.
Therefore, we conducted further experiments
with (QT-2) and (QT-3): building the query using
only nouns, and using manually assigned keywords
based on the tagset of 41 keywords. For example,
the following query is built for the professional pro-
file given in Section 1.
</bodyText>
<subsubsectionHeader confidence="0.43076">
Keywords assigned:
</subsubsectionHeader>
<figureCaption confidence="0.625559">
care for/nurse/educate/teach; use/program computer;
office; outside: outside facilities/natural
environment; animals/plants
</figureCaption>
<bodyText confidence="0.999671153846154">
IR results obtained on (QT-2) and (QT-3) show
that the performance is better for nouns, and sig-
nificantly better for the queries built of keywords.
This suggests that in order to achieve high IR perfor-
mance for the task of Electronic Career Guidance,
it is necessary to preprocess the topics by perform-
ing information extraction to remove the noise from
free text essays. As a result of the preprocessing,
natural language essays should be mapped to a set
of keywords relevant for describing a person’s in-
terests. Our results suggest that the word-based se-
mantic relatedness IR model (IR-1) performs signif-
icantly better in this setting.
</bodyText>
<sectionHeader confidence="0.99649" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999910357142857">
We presented a system for Electronic Career Guid-
ance utilizing NLP and IR techniques. Given a nat-
ural language professional profile, relevant profes-
sions are computed based on the information about
semantic relatedness. We intrinsically evaluated and
analyzed the properties of two semantic relatedness
measures utilizing the lexical semantic information
in a German wordnet and Wikipedia on the tasks of
estimating semantic relatedness scores and answer-
ing multiple-choice questions. Furthermore, we ap-
plied these measures to an IR task, whereby they
were used either in combination with a set of heuris-
tics or the Wikipedia based measure was used to di-
rectly compute semantic relatedness of topics and
</bodyText>
<page confidence="0.974741">
1037
</page>
<table confidence="0.999671727272727">
MODEL (QT-1) NOUNS, VERBS, ADJ. MAP (QT-2) NOUNS SRCC MAP (QT-3) KEYWORDS
MAP P10 #RRD SRCC P10 #RRD P10 #RRD SRCC
EB .39 .58 2581 .306 .38 .58 2297 .335 .54 .76 2755 .497
EB+SYN .37 .56 2589 .288 .38 .57 2310 .331 .54 .73 2768 .530
EB+HYPO .34 .47 2702 .275 .38 .56 2328 .327 .47 .65 2782 .399
Lin .85 .41 .56 2787 .338 .40 .59 2770 .320 .59 .73 2787 .578
Lin .98 .41 .61 2753 .326 .42 .59 2677 .341 .58 .74 2783 .563
ESA-Word .11 .39 .56 2787 .309 .44 .63 2787 .355 .60 .77 2787 .535
ESA-Word .13 .38 .59 2787 .282 .43 .62 2787 .338 .62 .76 2787 .550
ESA-Word .24 .40 .60 2787 .259 .43 .60 2699 .306 .54 .73 2772 .482
ESA-Text .47 .62 2787 .368 .55 .71 2787 .462 .56 .74 2787 .489
</table>
<tableCaption confidence="0.999774">
Table 5: Information Retrieval performance on the BERUFEnet dataset.
</tableCaption>
<bodyText confidence="0.999961155172414">
documents. We experimented with three different
query types, which were built from the topics by:
(QT-1) extracting nouns, verbs, adjectives, (QT-2)
extracting only nouns, or (QT-3) manually assign-
ing several keywords to each topic from a tagset of
41 keywords.
In an intrinsic evaluation of LIN and ESA mea-
sures on the task of computing semantic relatedness,
we found that ESA captures the information about
semantic relatedness and non-classical semantic re-
lations considerably better than LIN, which operates
on an is-a hierarchy and, thus, better captures the in-
formation about semantic similarity. On the task of
solving RDWP questions, the ESA measure signif-
icantly outperformed the LIN measure in terms of
correctness. On both tasks, the coverage of ESA is
much better. Despite this, the performance of LIN
and ESA as part of an IR model is only slightly
different. ESA performs better for all lengths of
queries, but the differences are not as significant as
in the intrinsic evaluation. This indicates that the
information provided by both measures, based on
different knowledge bases, might be complementary
for the IR task.
When ESA is applied to directly compute seman-
tic relatedness between topics and documents, it out-
performs IR-1 and the baseline EB model by a large
margin for QT-1 and QT-2 queries. For QT-3, i.e.,
the shortest type of query, it performs worse than
IR-1 utilizing ESA and a set of heuristics. Also,
the performance of the baseline EB model is very
strong in this experimental setting. This result in-
dicates that IR-2 utilizing conventional information
retrieval techniques and semantic information from
Wikipedia is better suited for longer queries provid-
ing enough context. For shorter queries, soft match-
ing techniques utilizing semantic relatedness tend to
be beneficial.
It should be born in mind, that the construction
of QT-3 queries involved a manual step of assigning
the keywords to a given essay. In this experimen-
tal setting, all models show the best performance.
This indicates that professional profiles contain a lot
of noise, so that more sophisticated NLP analysis
of topics is required. This will be improved in our
future work, whereby the system will incorporate
an information extraction component for automat-
ically mapping the professional profile to a set of
keywords. We will also integrate a component for
analyzing the sentiment structure of the profiles. We
believe that the findings from our work on apply-
ing IR techniques to the task of Electronic Career
Guidance generalize to similar application domains,
where topics and documents display similar proper-
ties (with respect to their length, free-text structure
and mismatch of vocabularies) and domain and lex-
ical knowledge is required to achieve high levels of
performance.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999909333333334">
This work was supported by the German Research
Foundation under grant ”Semantic Information Re-
trieval from Texts in the Example Domain Elec-
tronic Career Guidance”, GU 798/1-2. We are grate-
ful to the Bundesagentur f¨ur Arbeit for providing
the BERUFEnet corpus. We would like to thank the
anonymous reviewers for valuable feedback on this
paper. We would also like to thank Piklu Gupta for
helpful comments.
</bodyText>
<page confidence="0.994157">
1038
</page>
<sectionHeader confidence="0.995944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999845237623763">
David Ahn, Valentin Jijkoun, Gilad Mishne, Karin
M¨uller, Maarten de Rijke, and Stefan Schlobach.
2004. Using Wikipedia at the TREC QA Track. In
Proceedings of TREC 2004.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based Measures of Semantic Distance.
Computational Linguistics, 32(1).
Razvan Bunescu and Marius Pasca. 2006. Using En-
cyclopedic Knowledge for Named Entity Disambigua-
tion. In Proceedings ofACL, pages 9–16, Trento, Italy.
Christiane Fellbaum. 1998. WordNet An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of The
20th International Joint Conference on Artificial In-
telligence (IJCAI), Hyderabad, India, January.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan
Cigarran. 1998. Indexing with WordNet synsets can
improve text retrieval. In Proceedings of the Coling-
ACL ’98 Workshop Usage of WordNet in Natural Lan-
guage Processing Systems, Montreal, Canada, August.
Iryna Gurevych and Hendrik Niederlich. 2005. Comput-
ing semantic relatedness in german with revised infor-
mation content metrics. In Proceedings of ”OntoLex
2005 - Ontologies and Lexical Resources” IJCNLP’05
Workshop, pages 28–33, October 11 – 13.
Iryna Gurevych. 2005. Using the Structure of a Concep-
tual Network in Computing Semantic Relatedness. In
Proceedings of the 2nd International Joint Conference
on Natural Language Processing, pages 767–778, Jeju
Island, Republic of Korea.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s the-
saurus and semantic similarity. In RANLP, pages 111–
120.
Boris Katz, Gregory Marton, Gary Borchardt, Alexis
Brownell, Sue Felshin, Daniel Loreto, Jesse Louis-
Rosenberg, Ben Lu, Federico Mora, Stephan Stiller,
Ozlem Uzuner, and Angela Wilcox. 2005. External
knowledge sources for question answering. In Pro-
ceedings of the 14th Annual Text REtrieval Conference
(TREC’2005), November.
Claudia Kunze, 2004. Lexikalisch-semantische Wort-
netze, chapter Computerlinguistik und Sprachtech-
nologie, pages 423–431. Spektrum Akademischer
Verlag.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Interna-
tional Conference on Machine Learning, pages 296–
304. Morgan Kaufmann, San Francisco, CA.
Dan Moldovan and Rada Mihalcea. 2000. Using Word-
Net and lexical operators to improve Internet searches.
IEEE Internet Computing, 4(1):34–43.
Jane Morris and Graeme Hirst. 2004. Non-Classical
Lexical Semantic Relations. In Workshop on Com-
putational Lexical Semantics, Human Language Tech-
nology Conference of the North American Chapter of
the ACL, Boston.
Christof M¨uller and Iryna Gurevych. 2006. Exploring
the Potential of Semantic Relatedness in Information
Retrieval. In Proceedings of LWA 2006 Lernen - Wis-
sensentdeckung - Adaptivit¨at: Information Retrieval,
pages 126–131, Hildesheim, Germany. GI-Fachgruppe
Information Retrieval.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627–633.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill, New
York.
Gerard Salton, Edward Fox, and Harry Wu. 1983. Ex-
tended boolean information retrieval. Communication
of the ACM, 26(11):1022–1036.
Alan F. Smeaton, Fergus Kelledy, and Ruari O’Donell.
1994. TREC-4 Experiments at Dublin City Univer-
sity: Thresholding posting lists, query expansion with
WordNet and POS tagging of Spanish. In Proceedings
of TREC-4, pages 373–390.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Ellen Vorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th An-
nual ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 61–69.
DeWitt Wallace and Lila Acheson Wallace. 2005.
Reader’s Digest, das Beste f¨ur Deutschland. Jan
2001–Dec 2005. Verlag Das Beste, Stuttgart.
Torsten Zesch and Iryna Gurevych. 2006. Automatically
Creating Datasets for Measures of Semantic Related-
ness. In Proceedings of the Workshop on Linguistic
Distances, pages 16–24, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser.
2007a. Analyzing and Accessing Wikipedia as a Lexi-
cal Semantic Resource. In Biannual Conference of the
Society for Computational Linguistics and Language
Technology, pages 213–221, Tuebingen, Germany.
Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser.
2007b. Comparing Wikipedia and German Word-
net by Evaluating Semantic Relatedness on Multiple
Datasets. In Proceedings of NAACL-HLT.
</reference>
<page confidence="0.995967">
1039
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996425">What to be? - Electronic Career Guidance Based on Semantic Relatedness</title>
<author confidence="0.942921">Iryna Gurevych</author>
<author confidence="0.942921">Christof M¨uller</author>
<author confidence="0.942921">Torsten Zesch</author>
<affiliation confidence="0.993176">Ubiquitous Knowledge Processing Group Telecooperation, Darmstadt University of Technology</affiliation>
<address confidence="0.999293">Hochschulstr. 10, 64289 Darmstadt, Germany</address>
<web confidence="0.621096">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.99584963095238">We present a study aimed at investigating the use of semantic information in a novel NLP application, Electronic Career Guidance (ECG), in German. ECG is formulated as an information retrieval (IR) task, whereby textual descriptions of professions are ranked for their relevance to natural language descriptions of a perprofessional interests We compare the performance of two semantic IR models: (IR-1) utilizing semantic relatedness (SR) measures based on either wordnet or Wikipedia and a set of heuristics, and (IR-2) measuring the similarity between the topic and documents based on Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). We evaluate the performance of SR measures intrinsically on the tasks of (T-1) computing SR, and (T-2) solving Reader’s Digest Word Power (RDWP) questions. 1 Electronic Career Guidance Career guidance is important both for the person involved and for the state. Not well informed decisions may cause people to drop the training program they are enrolled in, yielding loss of time and financial investments. However, there is a mismatch between what people know about existing professions and the variety of professions, which exist in reality. Some studies report that school leavers typically choose the professions known to them, such etc. Many other professions, which can possibly match the interests of the person very well, are not chosen, as their titles are unknown and people seeking career advice do not know about existence, e.g. or chemlaboratory However, people are very good at describing their professional interests in natural language. That is why they are even asked to write a short essay prior to an appointment with a career guidance expert. career guidance thus, a supplement to career guidance by human experts, helping young people to decide which profession to choose. The goal is to automatically compute a ranked list of professions according to the user’s interests. A current system employed by the German Federal Labour Office (GFLO) in their automatic career guidance is based on vocational trainings, manually annotated using a tagset of 41 keywords. The user must select appropriate keywords according to her interests. In reply, the system consults a knowledge base with professions manually annotated with the keywords by career guidance experts. Thereafter, it outputs a list of the best matching professions to the user. This approach has two significant disadvantages. Firstly, the knowledge base has to be maintained and steadily updated, as the number of professions and keywords associated with them is continuously changing. Secondly, the user has to describe her interests in a very restricted way. At the same time, GFLO maintains an extensive database with textual descriptions of professions, 1032 of the 45th Annual Meeting of the Association of Computational pages 1032–1039, Czech Republic, June 2007. Association for Computational Linguistics Therefore, we cast the problem of ECG as an IR task, trying to remove the disadvantages of conventional ECG outlined above by letting the user describe her interests in a short language essay, called professional Example essay translated to English I would like to work with animals, to treat and look after them, but I cannot stand the sight of blood and take too much pity on them. On the other hand, I like to work on the computer, can program in C, Python and VB and so I could consider software development as an appropriate profession. I cannot imagine working in a kindergarden, as a social worker or as a teacher, as I am not very good at asserting myself. Textual descriptions of professions are ranked given such an essay by using NLP and IR techniques. As essays and descriptions of professions display a mismatch between the vocabularies of topics and documents and there is lack of contextual information, due to the documents being fairly short as compared to standard IR scenarios, lexical semantic information should be especially beneficial to an IR system. For example, the profile can contain words about some objects or activities related to the profession, but not directly mentioned in the dee.g. the profile and or the document. Therefore, we propose to utilize semantic relatedness as a ranking function instead of conventional IR techniques, as will be substantiated below. 2 System Architecture Integrating lexical semantic knowledge in ECG requires the existence of knowledge bases encoding domain and lexical knowledge. In this paper, we investigate the utility of two knowledge bases: (i) a German wordnet, GermaNet (Kunze, 2004), and (ii) German portion of A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia berufe/ has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on the information either in the German wordnet (Lin, 1998) called or in Wikipedia (Gabrilovich and Markovitch, called Explicit Semantic Analysis, or We evaluate their performance intrinsically on the tasks of (T-1) computing semantic relatedness, and (T-2) solving Reader’s Digest Word Power (RDWP) questions and make conclusions about the ability of the measures to model certain aspects of semantic relatedness and their coverage. Furthermore, we follow the approach by M¨uller and Gurevych (2006), who proposed to utilize the LIN measure and a set of heuristics as an IR model (IR-1). Additionally, we utilize the ESA measure in a semantic information retrieval model, as this measure is significantly better at vocabulary coverage and at modelling cross part-of-speech relations (Gabrilovich and Markovitch, 2007). We compare the performance of ESA and LIN measures in a taskbased IR evaluation and analyze their strengths and limitations. Finally, we apply ESA to directly compute text similarities between topics and documents (IR-2) and compare the performance of two semantic IR models and a baseline Extended Boolean (EB) (Salton et al., 1983) with query summarize, the of this paper three-fold: (i) we present a novel system, utilizing NLP and IR techniques to perform Electronic Career Guidance, (ii) we study the properties and intrinsically evaluate two SR measures based on GermaNet and Wikipedia for the tasks of computing semantic relatedness and solving Reader’s Digest Word Power Game questions, and (iii) we investigate the performance of two semantic IR models in a task based evaluation. also ran experiments with Okapi BM25 model as implemented in the Terrier framework, but the results were worse than those with the EB model. Therefore, we limit our discussion to the latter. 1033 3 Computing Semantic Relatedness 3.1 SR Measures based measures is a German wordnet, which adopted the major properties and database technology from Princeton’s Word- Net (Fellbaum, 1998). However, GermaNet displays some structural differences and content oriented modifications. Its designers relied mainly on linguistic evidence, such as corpus frequency, rather than psycholinguistic motivations. Also, GermaNet employs artificial, i.e. non-lexicalized concepts, and adjectives are structured hierarchically as opposed to WordNet. Currently, GermaNet includes about 40000 synsets with more than 60000 word senses modelling nouns, verbs and adjectives. We use the semantic relatedness measure by Lin (1998) (referred to as LIN), as it consistently is among the best performing wordnet based measures (Gurevych and Niederlich, 2005; Budanitsky and Hirst, 2006). Lin defined semantic similarity using a formula derived from information theory. This measure is sometimes called a universal semantic similarity measure as it is supposed to be application, and resource independent. computed as: + log concepts (word senses) correto the information and the lowest common subof the two concepts. The probability computed as the relative frequency of words (representthat concept) in the corpus. based measures is a free online encyclopedia that is constructed in a collaborative effort of voluntary contributors and still grows exponentially. During this process, Wikipedia has probably become the largest collection of freely available knowledge. Wikipedia shares many of its properties with other well known lexical semantic resources (like dictionaries, thesauri, semantic wordnets or conventional encyclopedias) (Zesch et al., 2007a). As Wikipedia also models relatedness between concepts, it is better suited for computing semantic relatedness than GermaNet (Zesch et al., 2007b). In very recent work, Gabrilovich and Markovitch introduce a SR measure called Se- Analysis The ESA measure represents the meaning of a term as a high-dimensional concept vector. The concept vector is derived from Wikipedia articles, as each article focuses on a certain topic, and can thus be viewed as expressing a concept. The dimension of the concept vector is the number of Wikipedia articles. Each element of the vector is associated with a certain Wikipedia article (or concept). If the term can be found in this article, the term’s tfidf score (Salton and McGill, 1983) in this article is assigned to the vector element. Otherwise, 0 is assigned. As a result, a term’s concept vector represents the importance of the term for each concept. Semantic relatedness of two terms can then be easily computed as the cosine of their corresponding concept vectors. If we want to measure the semantic relatedness of texts instead of terms, we can also use ESA concept vectors. A text is represented as the average concept vector of its terms’ concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual information in Wikipedia, the measure shows excellent coverage. Therefore, we select it as the second measure for integration into our IR system. 3.2 Datasets relatedness datasets German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) and ii) a larger dataset containing 350 pairs Zesch and Gurevych (2006) created a third dataset from domain-specific corpora a semi-automatic process Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus.</abstract>
<date confidence="0.559863">1034</date>
<affiliation confidence="0.819468">POS</affiliation>
<address confidence="0.983707333333333">Gur65 2005 German 65 N Gur350 2006 German 350 N, V, A ZG222 2006 German 222 N, V, A</address>
<abstract confidence="0.963736775510204">810 - .690 - .490 .647 Table 1: Comparison of datasets used for evaluating semantic relatedness in German. ZG222 does not have this bias. Following the work by Jarmasz and Szpakowicz (2003) and Turney (2006), we created a second dataset containing multiple choice questions. We collected 1072 multiple-choice word analogy from the German Digest Word Game from January 2001 to December 2005 (Wallace and Wallace, 2005). We discarded 44 questions that had more than one correct answer, and 20 questions that used a phrase instead of a single term as query. The resulting 1008 questions form our evaluation dataset. An example question is given below: a) Kleingeb¨ack (small cake) b) Spenglerwerkzeug (plumbing tool) c) Miesepeter (killjoy) d) Wildschaf (moufflon) The task is to find the correct choice - ‘a)’ in this case. This dataset is significantly larger than any of the previous datasets employed in this type of evaluation. Also, it is not restricted to synonym questions, as in the work by Jarmasz and Szpakowicz (2003), but also includes hypernymy/hyponymy, and few non-classical relations. 3.3 Analysis of Results Table 2 gives the results of evaluation on the task of correlating the results of an SR measure with human judgments using Pearson correlation. The GermaNet based LIN measure outperforms ESA on the Gur65 dataset. On the other datasets, ESA is better than LIN. This is clearly due to the fact, that Gur65 contains only noun-noun word pairs connected by classical semantic relations, while the other datasets also contain cross part-of-speech pairs connected by non-classical relations. The Wikipedia based ESA measure can better capture such relations. Additionally, Table 3 shows that ESA also covers almost all ZG222 Upper bound 0.80 0.64 0.44 0.73 0.50 0.08 0.56 0.52 0.32 2: Pearson correlation human judgments with SR measures on word pairs covered by GermaNet and Wikipedia.</abstract>
<email confidence="0.567275">ESA</email>
<phone confidence="0.582783666666667">Gur65 65 60 65 Gur350 350 208 333 ZG222 222 88 205</phone>
<abstract confidence="0.979373447183099">Table 3: Number of covered word pairs based on Lin or ESA measure on different datasets. word pairs in each dataset, while GermaNet is much lower for Gur350 and ZG222. ESA performs even better on the Reader’s Digest task (see Table 4). It shows high coverage and near human performance regarding the relative number of correctly solved Given the high performance and coverage of the Wikipedia based ESA measure, we expect it to yield better IR results than LIN. 4 Information Retrieval 4.1 IR Models creating the search index for IR models, we apply first tokenization and then remove stop words. We use a general German stop for human performance are for one subject. Thus, they only indicate the approximate difficulty of the task. We plan to use this dataset with a much larger group of subjects. Human 1008 874 0.87 GermaNet Lin 298 153 0.51 Wikipedia ESA 789 572 0.72 Table 4: Evaluation results on multiple-choice word analogy questions. 1035 word list extended with highly frequent domain specific terms. Before adding the remaining words to the index, they are lemmatized. We finally split compounds into their constituents, and add both, constituents and compounds, to the index. model is an open source text search library based on an EB model. After matching the preprocessed queries against the index, the document collection is divided into a set of relevant and irrelevant documents. The set of relevant documents is, then, ranked according to the formula given in the following equation: = is the number of terms in the query, the term frequency factor for term document the inverse document freof the term, and a normalization value of document d, given the number of terms within the document. We added a simple query expansion algorithm using (i) synonyms, and (ii) hyponyms, extracted from GermaNet. based on SR the (IR-1) model, we utilize two SR measures and a set of heuristics: (i) the Lin measure based on GermaNet (LIN), and (ii) the ESA measure based on Wikipedia (ESA-Word). This algorithm was applied to the German IR benchmark with positive results by M¨uller and Gurevych (2006). The algorithm computes a SR score for each query and document term pair. Scores above a predefined threshold are summed up and weighted by different factors, which boost or lower the scores for documents, depending on how many query terms are contained exactly or contribute a high enough SR score. In order to integrate the strengths of traditional IR models, the inverse document frequency considered, which measures the general importance of a term for predicting the content of a document. The final formula of the model is as follows: the number of tokens in the document, the number of tokens in the query, i-th token, j-th query token, the SR score for the respective document and query number of query terms not exactly in the document, number of query tokens, which do not contribute a SR score above the threshold. For the (IR-2) model, we apply the ESA method for directly comparing the query with documents, as described in Section 3.1. 4.2 Data The corpus employed in our experiments was built based on a real-life IR scenario in the domain of as described in Section 1. The colextracted from a database created by the GFLO. It contains textual descriptions of about 1,800 vocational trainings, and 4,000 descriptions of professions. We restrict the collection to a subset of BERUFEnet documents, consisting of 529 descriptions of vocational trainings, due to the process necessary to obtain relevance judgments, as described below. The documents contain not only details of professions, but also a lot of information concerning the training and administrative issues. We only use those portions of the descriptions, which characterize the profession itself. collected real natural language asking 30 human subjects to write an essay about their professional interests. The topics contain 130 words, average. Making judgments ECG requires domain expertise. Therefore, we applied an automatic method, which uses the knowledge base employed by the GFLO, described in Section 1. To obtain relevance judgments, we first annotate each essay with relevant keywords from the tagset of 41 and retrieve a ranked list of professions, which were assigned one or more keywords by domain experts. To map the ranked list to a set of relevant and irrelevant professions, we use a threshold of 3, as suggested by career guidance experts. This setting yields on average 93 relevant documents per topic. The quality of the automatically created gold standard depends on the quality of the applied knowledge base. As the knowledge base was created by = (1 + + 1036 domain experts and is at the core of the electronic career guidance system of the GFLO, we assume that the quality is adequate to ensure a reliable evaluation. 4.3 Analysis of Results In Table 5, we summarize the results of the experiments applying different IR models on the BERUFEnet data. We build queries from natural language essays by (QT-1) extracting nouns, verbs, and adjectives, (QT-2) using only nouns, and (QT- 3) manually assigning suitable keywords from the tagset with 41 keywords to each topic. We report the results with two different thresholds (.85 and .98) for the Lin model, and with three different thresholds (.11, .13 and .24) for the ESA-Word models. The metrics used are average precision after ten documents of relevant returned documents We the absolute value of rank corcoefficient by comparing the relevance ranking of our system with the relevance ranking of the knowledge base employed by the GFLO. Using query expansion for the EB model decreases the retrieval performance for most configurations. The SR based models outperform the EB model in all configurations and evaluation metrics, except for P10 on the keyword based queries. The Lin model is always outperformed by at least one of the ESA models, except for (QT-3). (IR-2) performs best on longer queries using nouns, verbs, adjectives or just nouns. Comparing the number of relevant retrieved documents, we observe that the IR models based on SR are able to return more relevant documents than the EB model. This supports the claim that semantic knowledge is especially helpful for the vocabulary mismatch problem, which cannot be addressed by conventional IR models. E.g., only SR-based modcan find the job technician a prowhich contains the sentence interests and are in the field of languages and The job could only be judged as relevant, as the semantic between the profile and the professional description could be found. In our analysis of the BERUFEnet results obtained on (QT-1), we noticed that many errors were due to the topics expressed in free natural language essays. Some subjects deviated from the given task to describe their professional interests and described facts that are rather irrelevant to the task of ECG, is important to speak different languages in growing European If all content words are extracted to build a query, a lot of noise is introduced. Therefore, we conducted further experiments with (QT-2) and (QT-3): building the query using only nouns, and using manually assigned keywords based on the tagset of 41 keywords. For example, the following query is built for the professional profile given in Section 1. Keywords assigned: care for/nurse/educate/teach; use/program computer; office; outside: outside facilities/natural environment; animals/plants IR results obtained on (QT-2) and (QT-3) show that the performance is better for nouns, and significantly better for the queries built of keywords. This suggests that in order to achieve high IR performance for the task of Electronic Career Guidance, it is necessary to preprocess the topics by performing information extraction to remove the noise from free text essays. As a result of the preprocessing, natural language essays should be mapped to a set of keywords relevant for describing a person’s interests. Our results suggest that the word-based semantic relatedness IR model (IR-1) performs significantly better in this setting. 5 Conclusions We presented a system for Electronic Career Guidance utilizing NLP and IR techniques. Given a natural language professional profile, relevant professions are computed based on the information about semantic relatedness. We intrinsically evaluated and analyzed the properties of two semantic relatedness measures utilizing the lexical semantic information in a German wordnet and Wikipedia on the tasks of estimating semantic relatedness scores and answering multiple-choice questions. Furthermore, we applied these measures to an IR task, whereby they were used either in combination with a set of heuristics or the Wikipedia based measure was used to directly compute semantic relatedness of topics and 1037 MAP SRCC MAP MAP P10 #RRD SRCC P10 #RRD P10 #RRD SRCC EB .39 .58 2581 .306 .38 .58 2297 .335 .54 .76 2755 .497 EB+SYN .37 .56 2589 .288 .38 .57 2310 .331 .54 .73 2768 .530 EB+HYPO .34 .47 2702 .275 .38 .56 2328 .327 .47 .65 2782 .399 Lin .85 .41 .56 2787 .338 .40 .59 2770 .320 .59 .73 2787 .578 Lin .98 .41 .61 2753 .326 .42 .59 2677 .341 .58 .74 2783 .563 ESA-Word .11 .39 .56 2787 .309 .44 .63 2787 .355 .60 .77 2787 .535 ESA-Word .13 .38 .59 2787 .282 .43 .62 2787 .338 .62 .76 2787 .550 ESA-Word .24 .40 .60 2787 .259 .43 .60 2699 .306 .54 .73 2772 .482 ESA-Text .47 .62 2787 .368 .55 .71 2787 .462 .56 .74 2787 .489 Table 5: Information Retrieval performance on the BERUFEnet dataset. documents. We experimented with three different query types, which were built from the topics by: (QT-1) extracting nouns, verbs, adjectives, (QT-2) extracting only nouns, or (QT-3) manually assigning several keywords to each topic from a tagset of 41 keywords. In an intrinsic evaluation of LIN and ESA measures on the task of computing semantic relatedness, we found that ESA captures the information about semantic relatedness and non-classical semantic relations considerably better than LIN, which operates an and, thus, better captures the information about semantic similarity. On the task of solving RDWP questions, the ESA measure significantly outperformed the LIN measure in terms of correctness. On both tasks, the coverage of ESA is much better. Despite this, the performance of LIN and ESA as part of an IR model is only slightly different. ESA performs better for all lengths of queries, but the differences are not as significant as in the intrinsic evaluation. This indicates that the information provided by both measures, based on different knowledge bases, might be complementary for the IR task. When ESA is applied to directly compute semantic relatedness between topics and documents, it outperforms IR-1 and the baseline EB model by a large margin for QT-1 and QT-2 queries. For QT-3, i.e., the shortest type of query, it performs worse than IR-1 utilizing ESA and a set of heuristics. Also, the performance of the baseline EB model is very strong in this experimental setting. This result indicates that IR-2 utilizing conventional information retrieval techniques and semantic information from Wikipedia is better suited for longer queries providenough context. For shorter queries, matching techniques utilizing semantic relatedness tend to be beneficial. It should be born in mind, that the construction of QT-3 queries involved a manual step of assigning the keywords to a given essay. In this experimental setting, all models show the best performance. This indicates that professional profiles contain a lot of noise, so that more sophisticated NLP analysis of topics is required. This will be improved in our future work, whereby the system will incorporate an information extraction component for automatically mapping the professional profile to a set of keywords. We will also integrate a component for analyzing the sentiment structure of the profiles. We believe that the findings from our work on applying IR techniques to the task of Electronic Career Guidance generalize to similar application domains, where topics and documents display similar properties (with respect to their length, free-text structure and mismatch of vocabularies) and domain and lexical knowledge is required to achieve high levels of performance. Acknowledgments This work was supported by the German Research Foundation under grant ”Semantic Information Retrieval from Texts in the Example Domain Electronic Career Guidance”, GU 798/1-2. We are grateto the f¨ur Arbeit providing the BERUFEnet corpus. We would like to thank the anonymous reviewers for valuable feedback on this paper. We would also like to thank Piklu Gupta for helpful comments.</abstract>
<date confidence="0.510275">1038</date>
<title confidence="0.982208">References</title>
<author confidence="0.9078595">David Ahn</author>
<author confidence="0.9078595">Valentin Jijkoun</author>
<author confidence="0.9078595">Gilad Mishne</author>
<author confidence="0.9078595">Karin M¨uller</author>
<author confidence="0.9078595">Maarten de_Rijke</author>
<author confidence="0.9078595">Stefan Schlobach</author>
<note confidence="0.858012636363636">2004. Using Wikipedia at the TREC QA Track. In of TREC Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based Measures of Semantic Distance. 32(1). Razvan Bunescu and Marius Pasca. 2006. Using Encyclopedic Knowledge for Named Entity Disambigua- In pages 9–16, Trento, Italy. Fellbaum. 1998. An Electronic Lex- MIT Press, Cambridge, MA. Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness using Wikipedia-based Semantic Analysis. In of The 20th International Joint Conference on Artificial In- Hyderabad, India, January. Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan Cigarran. 1998. Indexing with WordNet synsets can text retrieval. In of the Coling- ACL ’98 Workshop Usage of WordNet in Natural Lan- Processing Systems, Canada, August. Iryna Gurevych and Hendrik Niederlich. 2005. Computing semantic relatedness in german with revised inforcontent metrics. In of ”OntoLex 2005 - Ontologies and Lexical Resources” IJCNLP’05 pages 28–33, October 11 – 13. Iryna Gurevych. 2005. Using the Structure of a Conceptual Network in Computing Semantic Relatedness. In Proceedings of the 2nd International Joint Conference Natural Language pages 767–778, Jeju Island, Republic of Korea. Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s theand semantic similarity. In pages 111– 120.</note>
<author confidence="0.697101666666667">Boris Katz</author>
<author confidence="0.697101666666667">Gregory Marton</author>
<author confidence="0.697101666666667">Gary Borchardt</author>
<author confidence="0.697101666666667">Alexis Brownell</author>
<author confidence="0.697101666666667">Sue Felshin</author>
<author confidence="0.697101666666667">Daniel Loreto</author>
<author confidence="0.697101666666667">Jesse Louis- Rosenberg</author>
<author confidence="0.697101666666667">Ben Lu</author>
<author confidence="0.697101666666667">Federico Mora</author>
<author confidence="0.697101666666667">Stephan Stiller</author>
<note confidence="0.938321121212121">Ozlem Uzuner, and Angela Wilcox. 2005. External sources for question answering. In Proceedings of the 14th Annual Text REtrieval Conference November. Kunze, 2004. Wortchapter Computerlinguistik und Sprachtechnologie, pages 423–431. Spektrum Akademischer Verlag. Dekang Lin. 1998. An information-theoretic definiof similarity. In of the 15th Interna- Conference on Machine pages 296– 304. Morgan Kaufmann, San Francisco, CA. Dan Moldovan and Rada Mihalcea. 2000. Using Word- Net and lexical operators to improve Internet searches. Internet 4(1):34–43. Jane Morris and Graeme Hirst. 2004. Non-Classical Semantic Relations. In on Computational Lexical Semantics, Human Language Technology Conference of the North American Chapter of Boston. Christof M¨uller and Iryna Gurevych. 2006. Exploring the Potential of Semantic Relatedness in Information In of LWA 2006 Lernen - Wis- - Adaptivit¨at: Information pages 126–131, Hildesheim, Germany. GI-Fachgruppe Information Retrieval. Herbert Rubenstein and John B. Goodenough. 1965. Correlates of Synonymy. the 8(10):627–633. Salton and Michael J. McGill. 1983. Modern Information McGraw-Hill, New York. Gerard Salton, Edward Fox, and Harry Wu. 1983. Ex-</note>
<abstract confidence="0.761042916666667">boolean information retrieval. the 26(11):1022–1036. Alan F. Smeaton, Fergus Kelledy, and Ruari O’Donell. 1994. TREC-4 Experiments at Dublin City University: Thresholding posting lists, query expansion with and POS tagging of Spanish. In pages 373–390. Peter D. Turney. 2006. Similarity of semantic relations. 32(3):379–416. Ellen Vorhees. 1994. Query expansion using lexicalrelations. In of the 17th Annual ACM SIGIR Conference on Research and Devel-</abstract>
<note confidence="0.972231833333333">in Information pages 61–69. DeWitt Wallace and Lila Acheson Wallace. 2005. Digest, das Beste f¨ur Jan 2001–Dec 2005. Verlag Das Beste, Stuttgart. Torsten Zesch and Iryna Gurevych. 2006. Automatically Creating Datasets for Measures of Semantic Related- In of the Workshop on Linguistic pages 16–24, Sydney, Australia, July. Association for Computational Linguistics. Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser. 2007a. Analyzing and Accessing Wikipedia as a Lexi- Semantic Resource. In Conference of the Society for Computational Linguistics and Language pages 213–221, Tuebingen, Germany. Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser. 2007b. Comparing Wikipedia and German Wordnet by Evaluating Semantic Relatedness on Multiple In of</note>
<date confidence="0.515031">1039</date>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Ahn</author>
<author>Valentin Jijkoun</author>
<author>Gilad Mishne</author>
<author>Karin M¨uller</author>
<author>Maarten de Rijke</author>
<author>Stefan Schlobach</author>
</authors>
<title>Using Wikipedia at the TREC QA Track.</title>
<date>2004</date>
<booktitle>In Proceedings of TREC</booktitle>
<marker>Ahn, Jijkoun, Mishne, M¨uller, de Rijke, Schlobach, 2004</marker>
<rawString>David Ahn, Valentin Jijkoun, Gilad Mishne, Karin M¨uller, Maarten de Rijke, and Stefan Schlobach. 2004. Using Wikipedia at the TREC QA Track. In Proceedings of TREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based Measures of Semantic Distance.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="8997" citStr="Budanitsky and Hirst, 2006" startWordPosition="1386" endWordPosition="1389">fferences and content oriented modifications. Its designers relied mainly on linguistic evidence, such as corpus frequency, rather than psycholinguistic motivations. Also, GermaNet employs artificial, i.e. non-lexicalized concepts, and adjectives are structured hierarchically as opposed to WordNet. Currently, GermaNet includes about 40000 synsets with more than 60000 word senses modelling nouns, verbs and adjectives. We use the semantic relatedness measure by Lin (1998) (referred to as LIN), as it consistently is among the best performing wordnet based measures (Gurevych and Niederlich, 2005; Budanitsky and Hirst, 2006). Lin defined semantic similarity using a formula derived from information theory. This measure is sometimes called a universal semantic similarity measure as it is supposed to be application, domain, and resource independent. Lin is computed as: 2 x log p(LCS(c1, c2)) log p(c1) + log p(c2) where c1 and c2 are concepts (word senses) corresponding to w1 and w2, log p(c) is the information content, and LCS(c1, c2) is the lowest common subsumer of the two concepts. The probability p is computed as the relative frequency of words (representing that concept) in the taz5 corpus. Wikipedia based meas</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based Measures of Semantic Distance. Computational Linguistics, 32(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using Encyclopedic Knowledge for Named Entity Disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>9--16</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="5982" citStr="Bunescu and Pasca, 2006" startWordPosition="924" endWordPosition="927">ons and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia 2http://infobub.arbeitsagentur.de/ berufe/ 3http://de.wikipedia.org/ has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on the information either in the German wordnet (Lin, 1998) called LIN, or in Wikipedia (Gabrilovich and Markovitch, 2007) called Explicit Semantic Analysis, or ESA. We evaluate their performance intrinsically on the tasks of (T-1) computing semantic relatedness, and (T-2) solving Reader’s Digest Word Power (RDWP) questions a</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pasca. 2006. Using Encyclopedic Knowledge for Named Entity Disambiguation. In Proceedings ofACL, pages 9–16, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8323" citStr="Fellbaum, 1998" startWordPosition="1296" endWordPosition="1297">and Wikipedia for the tasks of computing semantic relatedness and solving Reader’s Digest Word Power Game questions, and (iii) we investigate the performance of two semantic IR models in a task based evaluation. 4We also ran experiments with Okapi BM25 model as implemented in the Terrier framework, but the results were worse than those with the EB model. Therefore, we limit our discussion to the latter. 1033 3 Computing Semantic Relatedness 3.1 SR Measures GermaNet based measures GermaNet is a German wordnet, which adopted the major properties and database technology from Princeton’s WordNet (Fellbaum, 1998). However, GermaNet displays some structural differences and content oriented modifications. Its designers relied mainly on linguistic evidence, such as corpus frequency, rather than psycholinguistic motivations. Also, GermaNet employs artificial, i.e. non-lexicalized concepts, and adjectives are structured hierarchically as opposed to WordNet. Currently, GermaNet includes about 40000 synsets with more than 60000 word senses modelling nouns, verbs and adjectives. We use the semantic relatedness measure by Lin (1998) (referred to as LIN), as it consistently is among the best performing wordnet </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of The 20th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<location>Hyderabad, India,</location>
<contexts>
<context position="1036" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="136" endWordPosition="139">semantic information in a novel NLP application, Electronic Career Guidance (ECG), in German. ECG is formulated as an information retrieval (IR) task, whereby textual descriptions of professions (documents) are ranked for their relevance to natural language descriptions of a person’s professional interests (the topic). We compare the performance of two semantic IR models: (IR-1) utilizing semantic relatedness (SR) measures based on either wordnet or Wikipedia and a set of heuristics, and (IR-2) measuring the similarity between the topic and documents based on Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). We evaluate the performance of SR measures intrinsically on the tasks of (T-1) computing SR, and (T-2) solving Reader’s Digest Word Power (RDWP) questions. 1 Electronic Career Guidance Career guidance is important both for the person involved and for the state. Not well informed decisions may cause people to drop the training program they are enrolled in, yielding loss of time and financial investments. However, there is a mismatch between what people know about existing professions and the variety of professions, which exist in reality. Some studies report that school leavers typically choo</context>
<context position="6377" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="986" endWordPosition="989">rufe/ 3http://de.wikipedia.org/ has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on the information either in the German wordnet (Lin, 1998) called LIN, or in Wikipedia (Gabrilovich and Markovitch, 2007) called Explicit Semantic Analysis, or ESA. We evaluate their performance intrinsically on the tasks of (T-1) computing semantic relatedness, and (T-2) solving Reader’s Digest Word Power (RDWP) questions and make conclusions about the ability of the measures to model certain aspects of semantic relatedness and their coverage. Furthermore, we follow the approach by M¨uller and Gurevych (2006), who proposed to utilize the LIN measure and a set of heuristics as an IR model (IR-1). Additionally, we utilize the ESA measure in a semantic information retrieval model, as this measure is significantly </context>
<context position="10261" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="1581" endWordPosition="1584"> encyclopedia that is constructed in a collaborative effort of voluntary contributors and still grows exponentially. During this process, Wikipedia has probably become the largest collection of freely available knowledge. Wikipedia shares many of its properties with other well known lexical semantic resources (like dictionaries, thesauri, semantic wordnets or conventional encyclopedias) (Zesch et al., 2007a). As Wikipedia also models relatedness between concepts, it is better suited for computing 5http://www.taz.de semantic relatedness than GermaNet (Zesch et al., 2007b). In very recent work, Gabrilovich and Markovitch (2007) introduce a SR measure called Explicit Semantic Analysis (ESA). The ESA measure represents the meaning of a term as a high-dimensional concept vector. The concept vector is derived from Wikipedia articles, as each article focuses on a certain topic, and can thus be viewed as expressing a concept. The dimension of the concept vector is the number of Wikipedia articles. Each element of the vector is associated with a certain Wikipedia article (or concept). If the term can be found in this article, the term’s tfidf score (Salton and McGill, 1983) in this article is assigned to the vector element</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis. In Proceedings of The 20th International Joint Conference on Artificial Intelligence (IJCAI), Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
<author>Irina Chugur</author>
<author>Juan Cigarran</author>
</authors>
<title>Indexing with WordNet synsets can improve text retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the ColingACL ’98 Workshop Usage of WordNet in Natural Language Processing Systems,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="5589" citStr="Gonzalo et al., 1998" startWordPosition="873" endWordPosition="876">ubstantiated below. 2 System Architecture Integrating lexical semantic knowledge in ECG requires the existence of knowledge bases encoding domain and lexical knowledge. In this paper, we investigate the utility of two knowledge bases: (i) a German wordnet, GermaNet (Kunze, 2004), and (ii) the German portion of Wikipedia.3 A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia 2http://infobub.arbeitsagentur.de/ berufe/ 3http://de.wikipedia.org/ has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from th</context>
</contexts>
<marker>Gonzalo, Verdejo, Chugur, Cigarran, 1998</marker>
<rawString>Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan Cigarran. 1998. Indexing with WordNet synsets can improve text retrieval. In Proceedings of the ColingACL ’98 Workshop Usage of WordNet in Natural Language Processing Systems, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Hendrik Niederlich</author>
</authors>
<title>Computing semantic relatedness in german with revised information content metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of ”OntoLex 2005 - Ontologies and Lexical Resources” IJCNLP’05 Workshop,</booktitle>
<volume>11</volume>
<pages>28--33</pages>
<contexts>
<context position="8968" citStr="Gurevych and Niederlich, 2005" startWordPosition="1382" endWordPosition="1385">Net displays some structural differences and content oriented modifications. Its designers relied mainly on linguistic evidence, such as corpus frequency, rather than psycholinguistic motivations. Also, GermaNet employs artificial, i.e. non-lexicalized concepts, and adjectives are structured hierarchically as opposed to WordNet. Currently, GermaNet includes about 40000 synsets with more than 60000 word senses modelling nouns, verbs and adjectives. We use the semantic relatedness measure by Lin (1998) (referred to as LIN), as it consistently is among the best performing wordnet based measures (Gurevych and Niederlich, 2005; Budanitsky and Hirst, 2006). Lin defined semantic similarity using a formula derived from information theory. This measure is sometimes called a universal semantic similarity measure as it is supposed to be application, domain, and resource independent. Lin is computed as: 2 x log p(LCS(c1, c2)) log p(c1) + log p(c2) where c1 and c2 are concepts (word senses) corresponding to w1 and w2, log p(c) is the information content, and LCS(c1, c2) is the lowest common subsumer of the two concepts. The probability p is computed as the relative frequency of words (representing that concept) in the taz5</context>
</contexts>
<marker>Gurevych, Niederlich, 2005</marker>
<rawString>Iryna Gurevych and Hendrik Niederlich. 2005. Computing semantic relatedness in german with revised information content metrics. In Proceedings of ”OntoLex 2005 - Ontologies and Lexical Resources” IJCNLP’05 Workshop, pages 28–33, October 11 – 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
</authors>
<title>Using the Structure of a Conceptual Network in Computing Semantic Relatedness.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing,</booktitle>
<pages>767--778</pages>
<location>Jeju Island, Republic of</location>
<contexts>
<context position="11674" citStr="Gurevych (2005)" startWordPosition="1824" endWordPosition="1825">f their corresponding concept vectors. If we want to measure the semantic relatedness of texts instead of terms, we can also use ESA concept vectors. A text is represented as the average concept vector of its terms’ concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual information in Wikipedia, the measure shows excellent coverage. Therefore, we select it as the second measure for integration into our IR system. 3.2 Datasets Semantic relatedness datasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards s</context>
</contexts>
<marker>Gurevych, 2005</marker>
<rawString>Iryna Gurevych. 2005. Using the Structure of a Conceptual Network in Computing Semantic Relatedness. In Proceedings of the 2nd International Joint Conference on Natural Language Processing, pages 767–778, Jeju Island, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In RANLP,</booktitle>
<pages>111--120</pages>
<contexts>
<context position="12786" citStr="Jarmasz and Szpakowicz (2003)" startWordPosition="2000" endWordPosition="2004">sical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. simC1,C2 = 1034 DATASET YEAR LANGUAGE # PAIRS POS Gur65 2005 German 65 N Gur350 2006 German 350 N, V, A ZG222 2006 German 222 N, V, A CORRELATION r SCORES # SUBJECTS INTER INTRA discrete {0,1,2,3,4} 24 .810 - discrete {0,1,2,3,4} 8 .690 - discrete {0,1,2,3,4} 21 .490 .647 Table 1: Comparison of datasets used for evaluating semantic relatedness in German. ZG222 does not have this bias. Following the work by Jarmasz and Szpakowicz (2003) and Turney (2006), we created a second dataset containing multiple choice questions. We collected 1072 multiple-choice word analogy questions from the German Reader’s Digest Word Power Game (RDWP) from January 2001 to December 2005 (Wallace and Wallace, 2005). We discarded 44 questions that had more than one correct answer, and 20 questions that used a phrase instead of a single term as query. The resulting 1008 questions form our evaluation dataset. An example question is given below: Muffin (muffin) a) Kleingeb¨ack (small cake) b) Spenglerwerkzeug (plumbing tool) c) Miesepeter (killjoy) d) </context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s thesaurus and semantic similarity. In RANLP, pages 111– 120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
<author>Gregory Marton</author>
<author>Gary Borchardt</author>
<author>Alexis Brownell</author>
<author>Sue Felshin</author>
<author>Daniel Loreto</author>
<author>Jesse LouisRosenberg</author>
<author>Ben Lu</author>
<author>Federico Mora</author>
<author>Stephan Stiller</author>
<author>Ozlem Uzuner</author>
<author>Angela Wilcox</author>
</authors>
<title>External knowledge sources for question answering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th Annual Text REtrieval Conference (TREC’2005),</booktitle>
<contexts>
<context position="6029" citStr="Katz et al., 2005" startWordPosition="931" endWordPosition="934">000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia 2http://infobub.arbeitsagentur.de/ berufe/ 3http://de.wikipedia.org/ has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on the information either in the German wordnet (Lin, 1998) called LIN, or in Wikipedia (Gabrilovich and Markovitch, 2007) called Explicit Semantic Analysis, or ESA. We evaluate their performance intrinsically on the tasks of (T-1) computing semantic relatedness, and (T-2) solving Reader’s Digest Word Power (RDWP) questions and make conclusions about the ability of the me</context>
</contexts>
<marker>Katz, Marton, Borchardt, Brownell, Felshin, Loreto, LouisRosenberg, Lu, Mora, Stiller, Uzuner, Wilcox, 2005</marker>
<rawString>Boris Katz, Gregory Marton, Gary Borchardt, Alexis Brownell, Sue Felshin, Daniel Loreto, Jesse LouisRosenberg, Ben Lu, Federico Mora, Stephan Stiller, Ozlem Uzuner, and Angela Wilcox. 2005. External knowledge sources for question answering. In Proceedings of the 14th Annual Text REtrieval Conference (TREC’2005), November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Kunze</author>
</authors>
<title>Lexikalisch-semantische Wortnetze, chapter Computerlinguistik und Sprachtechnologie,</title>
<date>2004</date>
<pages>423--431</pages>
<publisher>Spektrum Akademischer Verlag.</publisher>
<contexts>
<context position="5247" citStr="Kunze, 2004" startWordPosition="818" endWordPosition="819">ontain words about some objects or activities related to the profession, but not directly mentioned in the description, e.g. oven, cakes in the profile and pastries, baker, or confectioner in the document. Therefore, we propose to utilize semantic relatedness as a ranking function instead of conventional IR techniques, as will be substantiated below. 2 System Architecture Integrating lexical semantic knowledge in ECG requires the existence of knowledge bases encoding domain and lexical knowledge. In this paper, we investigate the utility of two knowledge bases: (i) a German wordnet, GermaNet (Kunze, 2004), and (ii) the German portion of Wikipedia.3 A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia 2http://infobub.arbeitsagentur.de/ berufe/ 3http://de.wikipedia.org/ has been discovered as a promising lexical semantic resource and succes</context>
</contexts>
<marker>Kunze, 2004</marker>
<rawString>Claudia Kunze, 2004. Lexikalisch-semantische Wortnetze, chapter Computerlinguistik und Sprachtechnologie, pages 423–431. Spektrum Akademischer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="6314" citStr="Lin, 1998" startWordPosition="979" endWordPosition="980">ia 2http://infobub.arbeitsagentur.de/ berufe/ 3http://de.wikipedia.org/ has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on the information either in the German wordnet (Lin, 1998) called LIN, or in Wikipedia (Gabrilovich and Markovitch, 2007) called Explicit Semantic Analysis, or ESA. We evaluate their performance intrinsically on the tasks of (T-1) computing semantic relatedness, and (T-2) solving Reader’s Digest Word Power (RDWP) questions and make conclusions about the ability of the measures to model certain aspects of semantic relatedness and their coverage. Furthermore, we follow the approach by M¨uller and Gurevych (2006), who proposed to utilize the LIN measure and a set of heuristics as an IR model (IR-1). Additionally, we utilize the ESA measure in a semantic</context>
<context position="8844" citStr="Lin (1998)" startWordPosition="1365" endWordPosition="1366">ed the major properties and database technology from Princeton’s WordNet (Fellbaum, 1998). However, GermaNet displays some structural differences and content oriented modifications. Its designers relied mainly on linguistic evidence, such as corpus frequency, rather than psycholinguistic motivations. Also, GermaNet employs artificial, i.e. non-lexicalized concepts, and adjectives are structured hierarchically as opposed to WordNet. Currently, GermaNet includes about 40000 synsets with more than 60000 word senses modelling nouns, verbs and adjectives. We use the semantic relatedness measure by Lin (1998) (referred to as LIN), as it consistently is among the best performing wordnet based measures (Gurevych and Niederlich, 2005; Budanitsky and Hirst, 2006). Lin defined semantic similarity using a formula derived from information theory. This measure is sometimes called a universal semantic similarity measure as it is supposed to be application, domain, and resource independent. Lin is computed as: 2 x log p(LCS(c1, c2)) log p(c1) + log p(c2) where c1 and c2 are concepts (word senses) corresponding to w1 and w2, log p(c) is the information content, and LCS(c1, c2) is the lowest common subsumer o</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, pages 296– 304. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Rada Mihalcea</author>
</authors>
<title>Using WordNet and lexical operators to improve Internet searches.</title>
<date>2000</date>
<journal>IEEE Internet Computing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="5415" citStr="Moldovan and Mihalcea, 2000" startWordPosition="845" endWordPosition="849">and pastries, baker, or confectioner in the document. Therefore, we propose to utilize semantic relatedness as a ranking function instead of conventional IR techniques, as will be substantiated below. 2 System Architecture Integrating lexical semantic knowledge in ECG requires the existence of knowledge bases encoding domain and lexical knowledge. In this paper, we investigate the utility of two knowledge bases: (i) a German wordnet, GermaNet (Kunze, 2004), and (ii) the German portion of Wikipedia.3 A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia 2http://infobub.arbeitsagentur.de/ berufe/ 3http://de.wikipedia.org/ has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz</context>
</contexts>
<marker>Moldovan, Mihalcea, 2000</marker>
<rawString>Dan Moldovan and Rada Mihalcea. 2000. Using WordNet and lexical operators to improve Internet searches. IEEE Internet Computing, 4(1):34–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Non-Classical Lexical Semantic Relations.</title>
<date>2004</date>
<booktitle>In Workshop on Computational Lexical Semantics, Human Language Technology Conference of the North American Chapter of the ACL,</booktitle>
<location>Boston.</location>
<contexts>
<context position="12215" citStr="Morris and Hirst, 2004" startWordPosition="1902" endWordPosition="1906">atasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. simC1,C2 = 1034 DATASET YEAR LANGUAGE # PAIRS POS Gur65 2005 German 65 N Gur350 2006 German 350 N, V, A ZG222 2006 German 222 N, V, A CORRELATION r SCORES # SUBJECTS INTER INTRA discrete {0,1,2,3,4} 24 .810 - discrete {0,1,2,3,4} 8 .690 - discrete {0,1,2,3,4} 21 .490 .647 Table 1: Comparison of datasets used for evaluating semantic relatedness in German. ZG222 does not have this bias. Following the work by Jarmasz and Szpakowicz (2003) and Turney (2006), we create</context>
</contexts>
<marker>Morris, Hirst, 2004</marker>
<rawString>Jane Morris and Graeme Hirst. 2004. Non-Classical Lexical Semantic Relations. In Workshop on Computational Lexical Semantics, Human Language Technology Conference of the North American Chapter of the ACL, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Exploring the Potential of Semantic Relatedness in Information Retrieval.</title>
<date>2006</date>
<booktitle>In Proceedings of LWA 2006 Lernen - Wissensentdeckung - Adaptivit¨at: Information Retrieval,</booktitle>
<pages>126--131</pages>
<location>Hildesheim,</location>
<marker>M¨uller, Gurevych, 2006</marker>
<rawString>Christof M¨uller and Iryna Gurevych. 2006. Exploring the Potential of Semantic Relatedness in Information Retrieval. In Proceedings of LWA 2006 Lernen - Wissensentdeckung - Adaptivit¨at: Information Retrieval, pages 126–131, Hildesheim, Germany. GI-Fachgruppe Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual Correlates of Synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="11798" citStr="Rubenstein and Goodenough (1965)" startWordPosition="1840" endWordPosition="1843">erms, we can also use ESA concept vectors. A text is represented as the average concept vector of its terms’ concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual information in Wikipedia, the measure shows excellent coverage. Therefore, we select it as the second measure for integration into our IR system. 3.2 Datasets Semantic relatedness datasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. simC1,C2 = 1034 DATASET YEAR LANGUAGE # PAIRS POS G</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual Correlates of Synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="10811" citStr="Salton and McGill, 1983" startWordPosition="1675" endWordPosition="1678">sch et al., 2007b). In very recent work, Gabrilovich and Markovitch (2007) introduce a SR measure called Explicit Semantic Analysis (ESA). The ESA measure represents the meaning of a term as a high-dimensional concept vector. The concept vector is derived from Wikipedia articles, as each article focuses on a certain topic, and can thus be viewed as expressing a concept. The dimension of the concept vector is the number of Wikipedia articles. Each element of the vector is associated with a certain Wikipedia article (or concept). If the term can be found in this article, the term’s tfidf score (Salton and McGill, 1983) in this article is assigned to the vector element. Otherwise, 0 is assigned. As a result, a term’s concept vector represents the importance of the term for each concept. Semantic relatedness of two terms can then be easily computed as the cosine of their corresponding concept vectors. If we want to measure the semantic relatedness of texts instead of terms, we can also use ESA concept vectors. A text is represented as the average concept vector of its terms’ concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Edward Fox</author>
<author>Harry Wu</author>
</authors>
<title>Extended boolean information retrieval.</title>
<date>1983</date>
<journal>Communication of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="7430" citStr="Salton et al., 1983" startWordPosition="1151" endWordPosition="1154">re and a set of heuristics as an IR model (IR-1). Additionally, we utilize the ESA measure in a semantic information retrieval model, as this measure is significantly better at vocabulary coverage and at modelling cross part-of-speech relations (Gabrilovich and Markovitch, 2007). We compare the performance of ESA and LIN measures in a taskbased IR evaluation and analyze their strengths and limitations. Finally, we apply ESA to directly compute text similarities between topics and documents (IR-2) and compare the performance of two semantic IR models and a baseline Extended Boolean (EB) model (Salton et al., 1983) with query expansion.4 To summarize, the contributions of this paper are three-fold: (i) we present a novel system, utilizing NLP and IR techniques to perform Electronic Career Guidance, (ii) we study the properties and intrinsically evaluate two SR measures based on GermaNet and Wikipedia for the tasks of computing semantic relatedness and solving Reader’s Digest Word Power Game questions, and (iii) we investigate the performance of two semantic IR models in a task based evaluation. 4We also ran experiments with Okapi BM25 model as implemented in the Terrier framework, but the results were w</context>
</contexts>
<marker>Salton, Fox, Wu, 1983</marker>
<rawString>Gerard Salton, Edward Fox, and Harry Wu. 1983. Extended boolean information retrieval. Communication of the ACM, 26(11):1022–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan F Smeaton</author>
<author>Fergus Kelledy</author>
<author>Ruari O’Donell</author>
</authors>
<title>TREC-4 Experiments at Dublin City University: Thresholding posting lists, query expansion with WordNet and POS tagging of Spanish.</title>
<date>1994</date>
<booktitle>In Proceedings of TREC-4,</booktitle>
<pages>373--390</pages>
<marker>Smeaton, Kelledy, O’Donell, 1994</marker>
<rawString>Alan F. Smeaton, Fergus Kelledy, and Ruari O’Donell. 1994. TREC-4 Experiments at Dublin City University: Thresholding posting lists, query expansion with WordNet and POS tagging of Spanish. In Proceedings of TREC-4, pages 373–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="12804" citStr="Turney (2006)" startWordPosition="2006" endWordPosition="2007">(Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. simC1,C2 = 1034 DATASET YEAR LANGUAGE # PAIRS POS Gur65 2005 German 65 N Gur350 2006 German 350 N, V, A ZG222 2006 German 222 N, V, A CORRELATION r SCORES # SUBJECTS INTER INTRA discrete {0,1,2,3,4} 24 .810 - discrete {0,1,2,3,4} 8 .690 - discrete {0,1,2,3,4} 21 .490 .647 Table 1: Comparison of datasets used for evaluating semantic relatedness in German. ZG222 does not have this bias. Following the work by Jarmasz and Szpakowicz (2003) and Turney (2006), we created a second dataset containing multiple choice questions. We collected 1072 multiple-choice word analogy questions from the German Reader’s Digest Word Power Game (RDWP) from January 2001 to December 2005 (Wallace and Wallace, 2005). We discarded 44 questions that had more than one correct answer, and 20 questions that used a phrase instead of a single term as query. The resulting 1008 questions form our evaluation dataset. An example question is given below: Muffin (muffin) a) Kleingeb¨ack (small cake) b) Spenglerwerkzeug (plumbing tool) c) Miesepeter (killjoy) d) Wildschaf (moufflo</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Vorhees</author>
</authors>
<title>Query expansion using lexicalsemantic relations.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>61--69</pages>
<contexts>
<context position="5525" citStr="Vorhees, 1994" startWordPosition="864" endWordPosition="865">tion instead of conventional IR techniques, as will be substantiated below. 2 System Architecture Integrating lexical semantic knowledge in ECG requires the existence of knowledge bases encoding domain and lexical knowledge. In this paper, we investigate the utility of two knowledge bases: (i) a German wordnet, GermaNet (Kunze, 2004), and (ii) the German portion of Wikipedia.3 A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia 2http://infobub.arbeitsagentur.de/ berufe/ 3http://de.wikipedia.org/ has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show differ</context>
</contexts>
<marker>Vorhees, 1994</marker>
<rawString>Ellen Vorhees. 1994. Query expansion using lexicalsemantic relations. In Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, pages 61–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DeWitt Wallace</author>
<author>Lila Acheson Wallace</author>
</authors>
<date>2005</date>
<booktitle>Reader’s Digest, das Beste f¨ur Deutschland. Jan 2001–Dec 2005. Verlag Das Beste,</booktitle>
<location>Stuttgart.</location>
<contexts>
<context position="13046" citStr="Wallace and Wallace, 2005" startWordPosition="2041" endWordPosition="2044">Gur350 2006 German 350 N, V, A ZG222 2006 German 222 N, V, A CORRELATION r SCORES # SUBJECTS INTER INTRA discrete {0,1,2,3,4} 24 .810 - discrete {0,1,2,3,4} 8 .690 - discrete {0,1,2,3,4} 21 .490 .647 Table 1: Comparison of datasets used for evaluating semantic relatedness in German. ZG222 does not have this bias. Following the work by Jarmasz and Szpakowicz (2003) and Turney (2006), we created a second dataset containing multiple choice questions. We collected 1072 multiple-choice word analogy questions from the German Reader’s Digest Word Power Game (RDWP) from January 2001 to December 2005 (Wallace and Wallace, 2005). We discarded 44 questions that had more than one correct answer, and 20 questions that used a phrase instead of a single term as query. The resulting 1008 questions form our evaluation dataset. An example question is given below: Muffin (muffin) a) Kleingeb¨ack (small cake) b) Spenglerwerkzeug (plumbing tool) c) Miesepeter (killjoy) d) Wildschaf (moufflon) The task is to find the correct choice - ‘a)’ in this case. This dataset is significantly larger than any of the previous datasets employed in this type of evaluation. Also, it is not restricted to synonym questions, as in the work by Jarm</context>
</contexts>
<marker>Wallace, Wallace, 2005</marker>
<rawString>DeWitt Wallace and Lila Acheson Wallace. 2005. Reader’s Digest, das Beste f¨ur Deutschland. Jan 2001–Dec 2005. Verlag Das Beste, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Automatically Creating Datasets for Measures of Semantic Relatedness.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Linguistic Distances,</booktitle>
<pages>16--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="11894" citStr="Zesch and Gurevych (2006)" startWordPosition="1855" endWordPosition="1858">erms’ concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual information in Wikipedia, the measure shows excellent coverage. Therefore, we select it as the second measure for integration into our IR system. 3.2 Datasets Semantic relatedness datasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. simC1,C2 = 1034 DATASET YEAR LANGUAGE # PAIRS POS Gur65 2005 German 65 N Gur350 2006 German 350 N, V, A ZG222 2006 German 222 N, V, A CORRELATION r</context>
</contexts>
<marker>Zesch, Gurevych, 2006</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2006. Automatically Creating Datasets for Measures of Semantic Relatedness. In Proceedings of the Workshop on Linguistic Distances, pages 16–24, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
<author>Max M¨uhlh¨auser</author>
</authors>
<title>Analyzing and Accessing Wikipedia as a Lexical Semantic Resource.</title>
<date>2007</date>
<booktitle>In Biannual Conference of the Society for Computational Linguistics and Language Technology,</booktitle>
<pages>213--221</pages>
<location>Tuebingen, Germany.</location>
<marker>Zesch, Gurevych, M¨uhlh¨auser, 2007</marker>
<rawString>Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser. 2007a. Analyzing and Accessing Wikipedia as a Lexical Semantic Resource. In Biannual Conference of the Society for Computational Linguistics and Language Technology, pages 213–221, Tuebingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
<author>Max M¨uhlh¨auser</author>
</authors>
<title>Comparing Wikipedia and German Wordnet by Evaluating Semantic Relatedness on Multiple Datasets.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<marker>Zesch, Gurevych, M¨uhlh¨auser, 2007</marker>
<rawString>Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser. 2007b. Comparing Wikipedia and German Wordnet by Evaluating Semantic Relatedness on Multiple Datasets. In Proceedings of NAACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>