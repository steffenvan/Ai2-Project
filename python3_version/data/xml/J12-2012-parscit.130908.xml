<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.073598">
<title confidence="0.969693">
Linguistic Structure Prediction
</title>
<author confidence="0.979988">
Noah A. Smith
</author>
<affiliation confidence="0.950112">
Carnegie Mellon University
</affiliation>
<bodyText confidence="0.409425666666667">
Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited
by Graeme Hirst, volume 13), 2011, xx+248 pp; paperbound, ISBN 978-1-60845-405-1,
$60.00; ebook, ISBN 978-1-60845-406-8, $30.00 or by subscription
</bodyText>
<figure confidence="0.806973666666667">
Reviewed by
Chris Quirk
Microsoft Research
</figure>
<bodyText confidence="0.998865">
Noah Smith’s ambitious new monograph, Linguistic Structure Prediction, “aims to bridge
the gap between natural language processing and machine learning.” Given that cur-
rent natural language processing (NLP) research makes heavy demands on machine-
learning techniques, and a sizeable fraction of modern machine learning (ML) research
focuses on structure prediction, this is clearly a timely and important topic. To address
the gaps and overlaps between these two large and well-developed fields in five brief
chapters is a difficult feat. The text, though not without its flaws, does an admirable job
of building this bridge.
An introductory first chapter surveys current research areas in statistical NLP,
cataloging and defining many common linguistic structure prediction tasks. Machine
learning students new to the area are likely to find this helpful albeit a bit terse; NLP
students will likely consider this section primarily a review. The subsequent chapters
change character abruptly, delving into mathematical details and heavy formalism.
Chapter 2 introduces the concept of decoding, presenting five distinct viewpoints
on the search for the highest scoring structure. The reader is quickly ushered through
graphical models, polytopes, grammars, hypergraphs, and weighted deduction sys-
tems, with descriptions based on an example in sequence tagging. The broad coverage,
multi-viewpoint discussion encourages the reader to make connections between many
distinct approaches, and provides solid formalism for reasoning about decoding prob-
lems. It is a comprehensive introduction to the most common and effective decoding
approaches, with one significant exception: the recent advances in dual decomposition
and Lagrangian relaxation methods. Timing is likely the culprit. This book was devel-
oped mainly from 2006 to 2009, whereas dual decomposition did not attain notoriety in
our community until a few years later (Rush et al. 2010). Relaxation approaches, though
potentially a passing phase, have successfully broadened the reach of simpler decoding
techniques into more complicated domains such as structured event extraction. They
would have made a nice addition. Regardless, this second chapter equips the reader
with sufficient machinery to solve a number of structured prediction problems.
Chapter 3 applies the machinery described in the prior chapter to the problem
of supervised structure induction. Probabilistic generative and conditional models are
introduced in some detail, followed by a discussion of margin-based methods. Hidden
Markov models (HMMs) and probabilistic context-free grammars are introduced in
detail, followed by solid descriptions of maximum likelihood estimation and smooth-
ing. The section on conditional models is well written and crucial, because so many
commonly used tasks can be treated as sequence modeling using techniques such
as conditional random fields. Much of the subject matter introduced abstractly in
Chapter 2 is presented in this chapter using specific algorithms. For instance, sequence
</bodyText>
<table confidence="0.4958205">
© 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
</table>
<bodyText confidence="0.999854">
modeling is discussed broadly in Chapter 2; the specific algorithms for HMMs are fully
defined in Chapter 3. This coarse-to-fine introduction of material may challenge readers
who are accustomed to more practical descriptions of material. Were I to teach a course
based on this book, I would be tempted to present the third chapter before the second.
Chapter 4 focuses on semisupervised, unsupervised, and hidden variable learning.
With a good mix of theory and practical examples, Expectation-Maximization (EM) is
introduced and grounded in several problems, then generalized with log-linear models
and approximated with contrastive estimation. Hard EM is mentioned in the context
of several examples, though a more detailed description of this potentially important
technique (cf. Spitkovsky et al., 2010) would bridge the material of Chapters 2 and 4. The
chapter then describes Bayesian approaches to NLP, working from theory into specific
techniques and landing in models. Finally, a brief section is devoted to the related area
of hidden variable learning.
Chapter 5 begins by describing the partition function, as well as inference tech-
niques for the partition function and decoding methods. I found it strange that this
important section was postponed so late in the book; much of the material was forward-
referenced throughout Chapter 4. Regardless, the techniques are described in a unifying,
generic manner. The book concludes with a discussion of minimum Bayes risk decod-
ing, and a few other variants.
Four appendices are devoted to optimization, experimental techniques, maximum
entropy, and locally normalized conditional models. All of these sections provide some
useful background. The section on hypothesis testing in Appendix B would be espe-
cially useful to students new to the area. It can be difficult to pick the correct hypothesis
testing method in general, and this problem is exacerbated in structure prediction. This
material serves as a good guide for a researcher hoping to evaluate how effective these
methods are.
I have some concerns about the intended audience. Descriptions quickly descend
into heavy notation and require knowledge of a broad range of mathematical concepts,
from marginal polytopes to semirings. I suspect the average NLP graduate student
would find it difficult to approach much of the material without a series of courses
in probability, statistics, and machine learning. The book is also very theoretical: Few
concrete algorithms are provided. Instead, the concepts are introduced using only
mathematics and formalism. For readers already conversant in the mapping from
mathematical descriptions into concrete algorithms and implementations, this will not
be a significant barrier. From the other direction, the structures used in NLP (e.g.,
dependency trees) are relatively well motivated, though a machine-learning researcher
new to the area might benefit from a fuller introduction to NLP. However, the text serves
as an effective guide for introducing the machine-learning community into the NLP
community, but I feel it would be challenging to use in the other direction.
This may be a personal bias, but I was surprised by the avoidance of machine-
translation–related techniques, despite obvious influences. Why resort to the term “de-
coding” if not because of decipherment and translation? One of the most effective uses
of hidden variable learning is in word alignment; it seems like a personal example.
Of course, building an effective machine-translation system requires a huge amount
of engineering in addition to the underlying theory, but I felt some discussion of the
problem and effective techniques would be pertinent.
Despite my struggles with the book’s organization and a few important omissions,
I must admit that I want a copy for my bookshelf. The author covers a huge amount of
material in just under 200 pages, touching on some of the most important algorithmic
techniques and viewpoints in modern statistical NLP. At times the text reads like a
</bodyText>
<page confidence="0.996483">
456
</page>
<subsectionHeader confidence="0.837012">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.9997628">
survey, touching very briefly on a huge range of topics. Yet the survey is comprehensive
and enlightening, tying together a broad range of topics and viewpoints. Younger grad-
uate students may require serious effort to comprehend the full text, but the modern
NLP researcher looking to advance the state of the art in structured prediction must
truly understand the concepts presented here.
</bodyText>
<reference confidence="0.979652076923077">
References Spitkovsky, Valentin I., Hiyan Alshawi,
Rush, Alexander M., David Sontag, Daniel Jurafsky, and Christopher
Michael Collins, and Tommi Jaakkola. D. Manning. 2010. Viterbi training
2010. On dual decomposition and linear improves unsupervised dependency
programming relaxations for natural parsing. In Proceedings of the Fourteenth
language processing. In Proceedings of the Conference on Computational Natural
2010 Conference on Empirical Methods in Language Learning, pages 9–17.
Natural Language Processing, pages 1–11. Uppsala.
Cambridge, MA.
This book review was edited by Pierre Isabelle.
Chris Quirk is a Researcher in the Natural Language Processing group at Microsoft Research. His
research interests include machine translation and paraphrase, particularly at the confluence of
large data and linguistic analysis tools. Quirk’s e-mail address is chrisq@microsoft.com.
</reference>
<page confidence="0.998298">
457
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.114536">
<title confidence="0.99983">Linguistic Structure Prediction</title>
<author confidence="0.999986">Noah A Smith</author>
<affiliation confidence="0.99955">Carnegie Mellon University</affiliation>
<address confidence="0.379491">Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited</address>
<note confidence="0.626077666666667">by Graeme Hirst, volume 13), 2011, xx+248 pp; paperbound, ISBN 978-1-60845-405-1, $60.00; ebook, ISBN 978-1-60845-406-8, $30.00 or by subscription Reviewed by</note>
<author confidence="0.997822">Chris Quirk</author>
<affiliation confidence="0.996391">Microsoft Research</affiliation>
<abstract confidence="0.991969">Smith’s ambitious new monograph, Structure “aims to bridge the gap between natural language processing and machine learning.” Given that current natural language processing (NLP) research makes heavy demands on machinelearning techniques, and a sizeable fraction of modern machine learning (ML) research focuses on structure prediction, this is clearly a timely and important topic. To address the gaps and overlaps between these two large and well-developed fields in five brief chapters is a difficult feat. The text, though not without its flaws, does an admirable job of building this bridge. An introductory first chapter surveys current research areas in statistical NLP, cataloging and defining many common linguistic structure prediction tasks. Machine learning students new to the area are likely to find this helpful albeit a bit terse; NLP students will likely consider this section primarily a review. The subsequent chapters change character abruptly, delving into mathematical details and heavy formalism. Chapter 2 introduces the concept of decoding, presenting five distinct viewpoints on the search for the highest scoring structure. The reader is quickly ushered through graphical models, polytopes, grammars, hypergraphs, and weighted deduction systems, with descriptions based on an example in sequence tagging. The broad coverage, multi-viewpoint discussion encourages the reader to make connections between many distinct approaches, and provides solid formalism for reasoning about decoding prob-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>References Rush</author>
<author>M Alexander</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--11</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="2280" citStr="Rush et al. 2010" startWordPosition="324" endWordPosition="327">d on an example in sequence tagging. The broad coverage, multi-viewpoint discussion encourages the reader to make connections between many distinct approaches, and provides solid formalism for reasoning about decoding problems. It is a comprehensive introduction to the most common and effective decoding approaches, with one significant exception: the recent advances in dual decomposition and Lagrangian relaxation methods. Timing is likely the culprit. This book was developed mainly from 2006 to 2009, whereas dual decomposition did not attain notoriety in our community until a few years later (Rush et al. 2010). Relaxation approaches, though potentially a passing phase, have successfully broadened the reach of simpler decoding techniques into more complicated domains such as structured event extraction. They would have made a nice addition. Regardless, this second chapter equips the reader with sufficient machinery to solve a number of structured prediction problems. Chapter 3 applies the machinery described in the prior chapter to the problem of supervised structure induction. Probabilistic generative and conditional models are introduced in some detail, followed by a discussion of margin-based met</context>
</contexts>
<marker>Rush, Alexander, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>References Rush, Alexander M., David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1–11. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>9--17</pages>
<location>Uppsala.</location>
<contexts>
<context position="4289" citStr="Spitkovsky et al., 2010" startWordPosition="613" endWordPosition="616"> accustomed to more practical descriptions of material. Were I to teach a course based on this book, I would be tempted to present the third chapter before the second. Chapter 4 focuses on semisupervised, unsupervised, and hidden variable learning. With a good mix of theory and practical examples, Expectation-Maximization (EM) is introduced and grounded in several problems, then generalized with log-linear models and approximated with contrastive estimation. Hard EM is mentioned in the context of several examples, though a more detailed description of this potentially important technique (cf. Spitkovsky et al., 2010) would bridge the material of Chapters 2 and 4. The chapter then describes Bayesian approaches to NLP, working from theory into specific techniques and landing in models. Finally, a brief section is devoted to the related area of hidden variable learning. Chapter 5 begins by describing the partition function, as well as inference techniques for the partition function and decoding methods. I found it strange that this important section was postponed so late in the book; much of the material was forwardreferenced throughout Chapter 4. Regardless, the techniques are described in a unifying, gener</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>Spitkovsky, Valentin I., Hiyan Alshawi, Daniel Jurafsky, and Christopher D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 9–17. Uppsala.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>