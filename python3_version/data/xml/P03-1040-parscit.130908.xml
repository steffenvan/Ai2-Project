<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001420">
<title confidence="0.995743">
Feature-Rich Statistical Translation of Noun Phrases
</title>
<author confidence="0.994736">
Philipp Koehn and Kevin Knight
</author>
<affiliation confidence="0.997642">
Information Sciences Institute
Department of Computer Science
University of Southern California
</affiliation>
<email confidence="0.998887">
koehn@isi.edu, knight@isi.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9985242">
We define noun phrase translation as a
subtask of machine translation. This en-
ables us to build a dedicated noun phrase
translation subsystem that improves over
the currently best general statistical ma-
chine translation methods by incorporat-
ing special modeling and special features.
We achieved 65.5% translation accuracy
in a German-English translation task vs.
53.2% with IBM Model 4.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999217451612903">
Recent research in machine translation challenges
us with the exciting problem of combining statisti-
cal methods with prior linguistic knowledge. The
power of statistical methods lies in the quick acquisi-
tion of knowledge from vast amounts of data, while
linguistic analysis both provides a fitting framework
for these methods and contributes additional knowl-
edge sources useful for finding correct translations.
We present work that successfully defines a sub-
task of machine translation: the translation of noun
phrases. We demonstrate through analysis and ex-
periments that it is feasible and beneficial to treat
noun phrase translation as a subtask. This opens the
path to dedicated modeling of other types of syn-
tactic constructs, e.g., verb clauses, where issues of
subcategorization of the verb play a big role.
Focusing on a narrower problem allows not only
more dedicated modeling, but also the use of com-
putationally more expensive methods.
We go on to tackle the task of noun phrase trans-
lation in a maximum entropy reranking framework.
Treating translation as a reranking problem instead
of as a search problem enables us to use features
over the full translation pair. We integrate both em-
pirical and symbolic knowledge sources as features
into our system which outperforms the best known
methods in statistical machine translation.
Previous work on defining subtasks within sta-
tistical machine translation has been performed on,
e.g., noun-noun pair (Cao and Li, 2002) and named
entity translation (Al-Onaizan and Knight, 2002).
</bodyText>
<sectionHeader confidence="0.826865" genericHeader="method">
2 Noun Phrase Translation as a Subtask
</sectionHeader>
<bodyText confidence="0.996127636363636">
In this work, we consider both noun phrases and
prepositional phrases, which we will refer to as
NP/PPs. We include prepositional phrases for a
number of reasons. Both are attached at the clause
level. Also, the translation of the preposition of-
ten depends heavily on the noun phrase (in the
morning). Moreover, the distinction between noun
phrases and prepositional phrases is not always clear
(note the Japanese bunsetsu) or hard to separate
(German joining of preposition and determiner into
one lexical unit, e.g., ins in das in the).
</bodyText>
<subsectionHeader confidence="0.761197">
2.1 Definition
</subsectionHeader>
<bodyText confidence="0.9998935">
We define the NP/PPs in a sentence as follows:
Given a sentence and its syntactic parse
tree , the NP/PPs of the sentence are the
subtrees that contain at least one noun
and no verb, and are not part of a larger
subtree that contains no verb.
</bodyText>
<figureCaption confidence="0.999893">
Figure 1: The noun phrases and preposition phrases (NP/PPs) addressed in this work
</figureCaption>
<figure confidence="0.983627777777778">
S
NP/PP
NP
VP
DT NNP NN
VBZ VP
has VBN VP
decided TO VP
to VB
renounce
the Bush administration
NP/PP
NP
NP PP
DT NN
IN DT-A NN
any involvement
in a treaty
</figure>
<bodyText confidence="0.995574">
The NP/PPs are the maximal noun phrases of the
sentence, not just the base NPs. This definition ex-
cludes NP/PPs that consist of only a pronoun. It also
excludes noun phrases that contain relative clauses.
NP/PPs may have connectives such as and.
For an illustration, see Figure 1.
</bodyText>
<subsectionHeader confidence="0.999901">
2.2 Translation of NP/PPs
</subsectionHeader>
<bodyText confidence="0.999820048780488">
To understand the behavior of noun phrases in the
translation process, we carried out a study to exam-
ine how they are translated in a typical parallel cor-
pus. Clearly, we cannot simply expect that certain
syntactic types in one language translate to equiv-
alent types in another language. Equivalent types
might not even exist.
This study answers the questions:
Do human translators translate noun phrases in
foreign texts into noun phrases in English?
If all noun phrases in a foreign text are trans-
lated into noun phrases in English, is an accept-
able sentence translation possible?
What are the properties of noun phrases which
cannot be translated as noun phrases without
rendering the overall sentence translation unac-
ceptable?
Using the Europarl corpus&apos;, we consider a trans-
lation task from German to English. We marked the
NP/PPs in the German side of a small 100 sentence
parallel corpus manually. This yielded 168 NP/PPs
according to our definition.
We examined if these units are realized as noun
phrases in the English side of the parallel corpus.
This is the case for 75% of the NP/PPs.
Second, we tried to construct translations of these
NP/PPs that take the form of NP/PPs in English in
an overall acceptable translation of the sentence. We
could do this for 98% of the NP/PPs.
The four exceptions are:
in Anspruch genommen; Gloss: take in demand
Abschied nehmen; take good-bye
meine Zustimmung geben; give my agreement
in der Hauptsache; in the main-thing
The first three cases are noun phrases or preposi-
tional phrases that merge with the verb. This is simi-
lar to the English construction make an observation,
which translates best into some languages as a verb
equivalent to observe. The fourth example, literally
translated as in the main thing, is best translated as
mainly.
</bodyText>
<footnote confidence="0.558266">
&apos;Available at http://www.isi.edu/ koehn/
</footnote>
<bodyText confidence="0.999742117647059">
Why is there such a considerable discrepancy be-
tween the number of noun phrases that can be trans-
lated as noun phrases into English and noun phrases
that are translated as noun phrases?
The main reason is that translators generally try
to translate the meaning of a sentence, and do not
feel bound to preserve the same syntactic structure.
This leads them to sometimes arbitrarily restructure
the sentence. Also, occasionally the translations are
sloppy.
The conclusion of this study is: Most NP/PPs in
German are translated to English as NP/PPs. Nearly
all of them, 98%, can be translated as NP/PPs into
English. The exceptions to this rule should be
treated as special cases and handled separately.
We carried out studies for Chinese-English and
Portuguese-English NP/PPs with similar results.
</bodyText>
<subsectionHeader confidence="0.999061">
2.3 The Role of External Context
</subsectionHeader>
<bodyText confidence="0.99991545">
One interesting question is if external context is nec-
essary for the translation of noun phrases. While the
sentence and document context may be available to
the NP/PP subsystem, the English output is only as-
sembled later and therefore harder to integrate.
To address this issue, we carried out a manual ex-
periment to check if humans can translate NP/PPs
without any external context. Using the same corpus
of 168 NP/PPs as in the previous section, a human
translator translated 89% of the noun phrases cor-
rectly, 9% had the wrong leading preposition, and
only 2% were mistranslated with the wrong content
word meaning.
Picking the right phrase start (e.g., preposition or
determiner) can sometimes only be resolved when
the English verb is chosen and its subcategoriza-
tion is known. Otherwise, sentence context does
not play a big role: Word choice can almost always
be resolved within the internal context of the noun
phrase.
</bodyText>
<subsectionHeader confidence="0.977839">
2.4 Integration into an MT System
</subsectionHeader>
<bodyText confidence="0.999919428571428">
The findings of the previous section indicate that
NP/PP translation can be conceived as a separate
subsystem of a complete machine translation system
– with due attention to special cases. We will now
estimate the importance of such a system.
As a general observation, we note that NP/PPs
cover roughly half of the words in news or similar
</bodyText>
<table confidence="0.99779975">
System Correct BLEU
Basic MT system 7% 0.16
NP/PPs translated in isolation 8% 0.17
Perfect NP/PP translation 24% 0.35
</table>
<tableCaption confidence="0.952096">
Table 1: Integration of an NP/PP subsystem: Correct
sentence translations and BLEU score
</tableCaption>
<bodyText confidence="0.968499322033898">
texts. All nouns are covered by NP/PPs. Nouns are
the biggest group of open class words, in terms of
the number of distinct words. Constantly, new nouns
are added to the vocabulary of a language, be it by
borrowing foreign words such as Fahrvergn¨ugen or
Zeitgeist, or by creating new words from acronyms
such as AIDS, or by other means. In addition to
new words, new phrases with distinct meanings are
constantly formed: web server, home page, instant
messaging, etc. Learning new concepts from text
sources when they become available is an elegant
solution for this knowledge acquisition problem.
In a preliminary study, we assess the impact of an
NP/PP subsystem on the quality of an overall ma-
chine translation system. We try to answer the fol-
lowing questions:
What is the impact on a machine translation
system if noun phrases are translated in isola-
tion?
What is the performance gain for a machine
translation system if an NP/PP subsystem pro-
vides perfect translations of the noun phrases?
We built a subsystem for NP/PP translation that
uses the same modeling as the overall system (IBM
Model 4), but is trained on only NP/PPs. With this
system, we translate the NP/PPs in isolation, with-
out the assistance of sentence context. These trans-
lations are fixed and provided to the general machine
translation system, which does not change the fixed
NP/PP translation.
In a different experiment, we also provided cor-
rect translations (motivated by the reference transla-
tion) for the NP/PPs to the general machine trans-
lation system. We carried out these experiments on
the same 100 sentence corpus as in the previous sec-
tions. The 164 translatable NP/PPs are marked and
translated in isolation.
The results are summarized in Table 1. Treating
NP/PPs as isolated units, and translating them in iso-
translation
Figure 2: Design of the noun phrase translation sub-
system: The base model generates an n-best list that
is rescored using additional features
lation with the same methods as the overall system
has little impact on overall translation quality. In
fact, we achieved a slight improvement in results
due to the fact that NP/PPs are consistently trans-
lated as NP/PPs. A perfect NP/PP subsystem would
triple the number of correctly translated sentences.
Performance is also measured by the BLEU score
(Papineni et al., 2002), which measures similarity to
the reference translation taken from the English side
of the parallel corpus.
These findings indicate that solving the NP/PP
translation problem would be a significant step to-
ward improving overall translation quality, even if
the overall system is not changed in any way. The
findings also indicate that isolating the NP/PP trans-
lation task as a subtask does not harm performance.
</bodyText>
<sectionHeader confidence="0.997207" genericHeader="method">
3 Framework
</sectionHeader>
<bodyText confidence="0.999940076923077">
When translating a foreign input sentence, we detect
its NP/PPs and translate them with an NP/PP trans-
lation subsystem. The best translation (or multiple
best translations) is then passed on to the full sen-
tence translation system which in turn translates the
remaining parts of the sentence and integrates the
chosen NP/PP translations.
Our NP/PP translation subsystem is designed as
follows: We train a translation system on a NP/PP
parallel corpus. We use this system to generate an
n-best list of possible translations. We then rescore
this n-best list with the help of additional features.
This design is illustrated by Figure 2.
</bodyText>
<subsectionHeader confidence="0.984218">
3.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999981125">
To evaluate our methods, we automatically detected
all of the 1362 NP/PPs in 534 sentences from parts
of the Europarl corpus which are not already used
as training data. Our evaluation metric is human as-
sessment: Can the translation provided by the sys-
tem be part of an acceptable translation of the whole
sentence? In other words, the noun phrase has to be
translated correctly given the sentence context.
The NP/PPs are extracted in the same way that
NP/PPs are initially detected for the acquisition of
the NP/PP training corpus. This means that there
are some problems with parse errors, leading to sen-
tence fragments extracted as NP/PPs that cannot be
translated correctly. Also, the test corpus contains
all detected NP/PPs, even untranslatable ones, as
discussed in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.99994">
3.2 Acquisition of an NP/PP Training Corpus
</subsectionHeader>
<bodyText confidence="0.991090833333333">
To train a statistical machine translation model, we
need a training corpus of NP/PPs paired with their
translation. We create this corpus by extracting
NP/PPs from a parallel corpus.
First, we word-align the corpus with Giza++ (Och
and Ney, 2000). Then, we parse both sides with syn-
tactic parsers (Collins, 1997; Schmidt and Schulte
im Walde, 2000)2. Our definition easily translates
into an algorithm to detect NP/PPs in a sentence.
Recall that in such a corpus, only part of the
NP/PPs are translated as such into the foreign lan-
guage. In addition, the word-alignment and syntac-
tic parses may be faulty. As a consequence, initially
only 43.4% of all NP/PPs could be aligned. We raise
this number to 67.2% with a number of automatic
data cleaning steps:
NP/PPs that partially align are broken up
Systematic parse errors are fixed
Certain word types that are inconsistently
tagged as nouns in the two languages are har-
monized (e.g., the German wo and the English
today).
Because adverb + NP/PP constructions (e.g.,
specifically this issue are inconsistently parsed,
</bodyText>
<footnote confidence="0.94989775">
2English parser available at http://www.ai.mit.
edu/people/mcollins/code.html, German parser
available at http://www.ims.uni-stuttgart.de/
projekte/gramotron/SOFTWARE/LoPar-en.html
</footnote>
<figure confidence="0.979538">
features
features
n-best list
Reranker
features features
Model
</figure>
<bodyText confidence="0.9976175">
we always strip the adverb from these construc-
tions.
German verbal adjective constructions are bro-
ken up if they involve arguments or adjuncts
(e.g., der von mir gegessene Kuchen = the by
me eaten cake), because this poses problems
more related to verbal clauses.
Alignment points involving punctuation are
stripped from the word alignment. Punctuation
is also stripped from the edges of NP/PPs.
A total of 737,388 NP/PP pairs are collected
from the German-English Europarl corpus as train-
ing data.
Certain German NP/PPs consistently do not align
to NP/PPs in English (see the example in Sec-
tion 2.2). These are detected at this point. The
obtained data of unaligned NP/PPs can be used for
dealing with these special cases.
</bodyText>
<subsectionHeader confidence="0.993879">
3.3 Base Model
</subsectionHeader>
<bodyText confidence="0.999950772727273">
Given the NP/PP corpus, we can use any general sta-
tistical machine translation method to train a transla-
tion system for noun phrases. As a baseline, we use
an IBM Model 4 (Brown et al., 1993) systema with
a greedy decoder4 (Germann et al., 2001).
We found that phrase based models achieve better
translation quality than IBM Model 4. Such mod-
els segment the input sequence into a number of
(non-linguistic) phrases, translate each phrase using
a phrase translation table, and allow for reordering
of phrases in the output. No phrases may be dropped
or added.
We use a phrase translation model that extracts its
phrase translation table from word alignments gen-
erated by the Giza++ toolkit. Details of this model
are described by Koehn et al. (2003).
To obtain an n-best list of candidate translations,
we developed a beam search decoder. This decoder
employs hypothesis recombination and stores the
search states in a search graph – similar to work by
Ueffing et al. (2002) – which can be mined with stan-
dard finite state machine methods5 for n-best lists.
</bodyText>
<footnote confidence="0.992628333333333">
3Available at http://www-i6.informatik.rwth-
aachen.de/ och/software/GIZA++.html
4Available at http://www.isi.edu/licensed-sw
/rewrite-decoder/
5We use the Carmel toolkit available at http://www.
isi.edu/licensed-sw/carmel/
</footnote>
<figureCaption confidence="0.765514">
size of n-best list
Figure 3: Acceptable NP/PP translations in n-best
list for different sizes
</figureCaption>
<subsectionHeader confidence="0.996255">
3.4 Acceptable Translations in the n-Best List
</subsectionHeader>
<bodyText confidence="0.999974791666667">
One key question for our approach is how often an
acceptable translation can be found in an n-best list.
The answer to this is illustrated in Figure 3: While
an acceptable translation comes out on top for only
about 60% of the NP/PPs in our test corpus, one can
be found in the 100-best list for over 90% of the
NP/PPs6. This means that rescoring has the potential
to raise performance by 30%.
What are the problems with the remaining 10%
for which no translation can be found? To investi-
gate this, we carried out an error analysis of these
NP/PPs. Results are given in Table 2. The main
sources of error are unknown words (34%) or words
for which the correct translation does not occur in
the training data (14%), and errors during tagging
and parsing that lead to incorrectly detected NP/PPs
(28%).
There are also problems with NP/PPs that require
complex syntactic restructuring (7%), and NP/PPs
that are too long, so an acceptable translation could
not be found in the 100-best list, but only further
down the list (6%). There are also NP/PPs that can-
not be translated as NP/PPs into English (2%), as
discussed in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.942557">
3.5 Maximum Entropy Reranking
</subsectionHeader>
<bodyText confidence="0.999665166666667">
Given an n-best list of candidates and additional fea-
tures, we transform the translation task from a search
problem into a reranking problem, which we address
using a maximum entropy approach.
As training data for finding feature values, we col-
lected a development corpus of 683 NP/PPs. Each
</bodyText>
<footnote confidence="0.9490435">
6Note that these numbers are obtained after compound split-
ting, described in Section 4.1
</footnote>
<figure confidence="0.987464142857143">
100%
correct
1 2 4 8 16 32 64
90%
80%
70%
60%
</figure>
<table confidence="0.9524345">
Error Frequency
Unknown Word 34%
Tagging or parsing error 28%
Unknown translation 14%
Complex syntactic restructuring 7%
Too long 6%
Untranslatable 2%
Other 9%
</table>
<tableCaption confidence="0.99499">
Table 2: Error analysis for NP/PPs without accept-
able translation in 100-best list
</tableCaption>
<bodyText confidence="0.9986982">
NP/PP comes with an n-best list of candidate trans-
lations that are generated from our base model and
are annotated with accuracy judgments. The initial
features are the logarithm of the probability scores
that the model assigns to each candidate transla-
tion: the language model score, the phrase transla-
tion score and the reordering (distortion) score.
The task for the learning method is to find a prob-
ability distribution that indicates if the can-
didate translation is an accurate translation of the
input . The decision rule to pick the best translation
is best argmax .
The development corpus provides the empirical
probability distribution by distributing the proba-
bility mass over the acceptable translations :
. If none of the candidate trans-
lations for a given input is acceptable, we pick the
candidates that are closest to reference translations
measured by minimum edit distance.
We use a maximum entropy framework to
parametrize this probability distribution as
exp where the ’s
are the feature values and the ’s are the feature
weights.
Since we have only a sample of the possible trans-
lations for the given input , we normalize the
probability distribution, so that for
our sample of candidate translations.
Maximum entropy learning finds a set of fea-
ture values so that for
each feature . These expectations are com-
puted as sums over all candidate translations
for all inputs :
that it converges to a global optimum. There are a
number of methods and tools available to carry out
this training of feature values. We use the toolkit7
developed by Malouf (2002). Berger et al. (1996)
and Manning and Sch¨utze (1999) provide good in-
troductions to maximum entropy learning.
Note that any other machine learning, such as sup-
port vector machines, could be used as well. We
chose maximum entropy for its ability to deal with
both real-valued and binary features. This method
is also similar to work by Och and Ney (2002), who
use maximum entropy to tune model parameters.
</bodyText>
<sectionHeader confidence="0.925569" genericHeader="method">
4 Properties of NP/PP Translation
</sectionHeader>
<bodyText confidence="0.999806">
We will now discuss the properties of NP/PP trans-
lation that we exploit in order to improve our NP/PP
translation subsystem. The first of these (compound-
ing of words) is addressed by preprocessing, while
the others motivate features which are used in n-best
list reranking.
</bodyText>
<subsectionHeader confidence="0.999289">
4.1 Compound Splitting
</subsectionHeader>
<bodyText confidence="0.998149315789474">
Compounding of words, especially nouns, is com-
mon in a number of languages (German, Dutch,
Finnish, Greek), and poses a serious problem for
machine translation: The word Aktionsplan may not
be known to the system, but if the word were bro-
ken up into Aktion and Plan, the system could easily
translate it into action plan, or plan for action.
The issues for breaking up compounds are:
Knowing the morphological rules for joining words,
resolving ambiguities of breaking up a word (Haupt-
sturm Haupt-Turm or Haupt-Sturm), and finding
the right level of splitting granularity (Frei-Tag or
Freitag).
Here, we follow an approach introduced by
Koehn and Knight (2003): First, we collect fre-
quency statistics over words in our training cor-
pus. Compounds may be broken up only into known
words in the corpus. For each potential compound
we check if morphological splitting rules allow us to
break it up into such known words.
Finally, we pick a splitting option (perhaps not
breaking up the compound at all). This decision
is based on the frequency of the words involved.
. 7Available at http://www-rohan.sdsu.edu/ mal
A nice property of maximum entropy training is ouf/pubs.html
Specifically, we pick the splitting option with
highest geometric mean of word frequencies of its
parts : mgt argmaxS count
The German side of both the training and testing
corpus is broken up in this way. The base model
is trained on a compound-split corpus, and input is
broken up before being passed on to the system.
This method works especially well with our
phrase-based machine translation model, which can
recover more easily from too eager or too timid splits
than word-based models. After performing this type
of compound splitting, hardly any errors occur with
respect to compounded words.
</bodyText>
<subsectionHeader confidence="0.994019">
4.2 Web n-Grams
</subsectionHeader>
<bodyText confidence="0.999955451612903">
Generally speaking, the performance of statistical
machine translation systems can be improved by
better translation modeling (which ensures corre-
spondence between input and output) and language
modeling (which ensures fluent English output).
Language modeling can be improved by different
types of language models (e.g., syntactic language
models), or additional training data for the language
model.
Here, we investigate the use of the web as a lan-
guage model. In preliminary studies we found that
30% of all 7-grams in new text can be also found on
the web, as measured by consulting the search en-
gine Googleg, which currently indexes 3 billion web
pages. This is only the case for 15% of 7-grams gen-
erated by the base translation system.
There are various ways one may integrate this
vast resource into a machine translation system: By
building a traditional n-gram language model, by us-
ing the web frequencies of the n-grams in a candi-
date translation, or by checking if all n-grams in a
candidate translation occur on the web.
We settled on using the following binary features:
Does the candidate translation as a whole occur in
the web? Do all n-grams in the candidate translation
occur on the web? Do all n-grams in the candidate
translation occur at least 10 times on the web? We
use both positive and negative features for n-grams
of the size 2 to 7.
We were not successful in improving performance
by building a web n-gram language model or using
</bodyText>
<footnote confidence="0.837754">
8http://www.google.com/
</footnote>
<bodyText confidence="0.999661666666667">
the actual frequencies as features. The web may be
too noisy to be used in such a straight-forward way
without significant smoothing efforts.
</bodyText>
<subsectionHeader confidence="0.997189">
4.3 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999960606060606">
Unlike in decoding, for reranking we have the com-
plete candidate translation available. This means
that we can define features that address any prop-
erty of the full NP/PP translation pair. One such set
of features is syntactic features.
Syntactic features are computed over the syntac-
tic parse trees of both input and candidate transla-
tion. For the input NP/PPs, we keep the syntactic
parse tree we inherit from the NP/PP detection pro-
cess. For the candidate translation, we use a part-
of-speech tagger and syntactic parser to annotate the
candidate translation with its most likely syntactic
parse tree.
We use the following three syntactic features:
Preservation of the number of nouns: Plural
nouns generally translate as plural nouns, while
singular nouns generally translate as singular
Preservation of prepositions: base preposi-
tional phrases within NP/PPs generally trans-
late as prepositional phrases, unless there is
movement involved. BaseNPs generally trans-
late as baseNPs. German genitive baseNP are
treated as basePP.
Within a baseNP/PP the determiner generally
agree in number with the final noun (e.g., not:
this nice green flowers).
The features are realized as integers, i.e., how
many nouns did not preserve their number during
translation?
These features encode relevant general syntactic
knowledge about the translation of noun phrases.
They constitute soft constraints that may be over-
ruled by other components of the system.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999661625">
As described in Section 3.1, we evaluate the per-
formance of our NP/PP translation subsystem on a
blind test set of 1362 NP/PPs extracted from 534
sentences. The contributions of different compo-
nents of our system are displayed in Table 3.
Starting from the IBM Model 4 baseline, we
achieve gains using our phrase-based translation
model (+5.5%), applying compound splitting to
</bodyText>
<table confidence="0.999398428571428">
System NP/PP Correct BLEU
IBM Model 4 724 53.2% 0.172
Phrase Model 800 58.7% 0.188
Compound Splitting 838 61.5% 0.195
Re-Estimated Param. 858 63.0% 0.197
Web Count Features 881 64.7% 0.198
Syntactic Features 892 65.5% 0.199
</table>
<tableCaption confidence="0.999343">
Table 3: Improving noun phrase translation with
</tableCaption>
<bodyText confidence="0.94332545">
special modeling and additional features: Correct
NP/PPs and BLEU score for overall sentence trans-
lation
training and test data (+2.8%), re-estimating the
weights for the system components using the
maximum entropy reranking frame-work (+1.5%),
adding web count features (+1.7%) and syntactic
features (+0.8%). Overall we achieve an improve-
ment of 12.3% over the baseline. Improvements of
2.5% are statistically significant given the size of our
test corpus.
Table 3 also provides scores for overall sentence
translation quality. The chosen NP/PP translations
are integrated into a general IBM Model 4 sys-
tem that translates whole sentences. Performance is
measured by the BLEU score, which measures sim-
ilarity to a reference translation. As reference trans-
lation we used the English side of the parallel cor-
pus. The BLEU scores track the improvements of
our components, with an overall gain of 0.027.
</bodyText>
<sectionHeader confidence="0.999722" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999977172413793">
We have shown that noun phrase translation can be
separated out as a subtask. Our manual experiments
show that NP/PPs can almost always be translated as
NP/PPs across many languages, and that the transla-
tion of NP/PPs usually does not require additional
external context.
We also demonstrated that the reduced complex-
ity of noun phrase translation allows us to address
the problem in a maximum entropy reranking frame-
work, where we only consider the 100-best candi-
dates of a base translation system. This enables us
to introduce any features that can be computed over
a full translation pair, instead of being limited to
features that can be integrated into the search algo-
rithm of the decoder, which only has access to partial
translations.
We improved performance of noun phrase trans-
lation by 12.3% by using a phrase translation model,
a maximum entropy reranking method and address-
ing specific properties of noun phrase translation:
compound splitting, using the web as a language
model, and syntactic features. We showed not only
improvement on NP/PP translation over best known
methods, but also improved overall sentence trans-
lation quality.
Our long term goal is to address additional syntac-
tic constructs in a similarly dedicated fashion. The
next step would be verb clauses, where modeling of
the subcategorization of the verb is important.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999496102564102">
Al-Onaizan, Y. and Knight, K. (2002). Translating named enti-
ties using monolingual and bilingual resources. In Proceed-
ings ofACL.
Berger, A. L., Pietra, S. A. D., and Pietra, V. J. D. (1996). A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39–69.
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., and Mercer, R. L.
(1993). The mathematics of statistical machine translation.
Computational Linguistics, 19(2):263–313.
Cao, Y. and Li, H. (2002). Base noun phrase translation using
web data and the EM algorithm. In Proceedings of CoLing.
Collins, M. (1997). Three generative, lexicalized models for
statistical parsing. In Proceedings ofACL 35.
Germann, U., Jahr, M., Knight, K., Marcu, D., and Yamada,
K. (2001). Fast decoding and optimal decoding for machine
translation. In Proceedings ofACL 39.
Koehn, P. and Knight, K. (2003). Empirical methods for com-
pound splitting. In Proceedings of EACL.
Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase
based translation. In Proceedings ofHLT/NAACL.
Malouf, R. (2002). A comparison of algorithms for maximum
entropy parameter estimation. In Proceedings of CoNLL.
Manning, C. D. and Sch¨utze, H. (1999). Foundations of Statis-
tical Natural Language Processing. MIT Press.
Och, F. J. and Ney, H. (2000). Improved statistical alignment
models. In Proceedings ofACL, pages 440–447, Hongkong,
China.
Och, F. J. and Ney, H. (2002). Discriminative training and max-
imum entropy models for statistical machine translation. In
Proceedings ofACL.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
BLEU: a method for automatic evaluation of machine trans-
lation. In Proceedings ofACL.
Schmidt, H. and Schulte im Walde, S. (2000). Robust German
noun chunking with a probabilistic context-free grammar. In
Proceedings of COLING.
Ueffing, N., Och, F. J., and Ney, H. (2002). Generation of word
graphs in statistical machine translation. In Proceedings of
EMNLP.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497344">
<title confidence="0.999599">Feature-Rich Statistical Translation of Noun Phrases</title>
<author confidence="0.99937">Koehn Knight</author>
<affiliation confidence="0.999285">Information Sciences Institute Department of Computer Science University of Southern California</affiliation>
<abstract confidence="0.99962">We define noun phrase translation as a subtask of machine translation. This enables us to build a dedicated noun phrase translation subsystem that improves over the currently best general statistical machine translation methods by incorporating special modeling and special features. We achieved 65.5% translation accuracy in a German-English translation task vs.</abstract>
<note confidence="0.501299">53.2% with IBM Model 4.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Translating named entities using monolingual and bilingual resources.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2160" citStr="Al-Onaizan and Knight, 2002" startWordPosition="322" endWordPosition="325">mputationally more expensive methods. We go on to tackle the task of noun phrase translation in a maximum entropy reranking framework. Treating translation as a reranking problem instead of as a search problem enables us to use features over the full translation pair. We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on defining subtasks within statistical machine translation has been performed on, e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). 2 Noun Phrase Translation as a Subtask In this work, we consider both noun phrases and prepositional phrases, which we will refer to as NP/PPs. We include prepositional phrases for a number of reasons. Both are attached at the clause level. Also, the translation of the preposition often depends heavily on the noun phrase (in the morning). Moreover, the distinction between noun phrases and prepositional phrases is not always clear (note the Japanese bunsetsu) or hard to separate (German joining of preposition and determiner into one lexical unit, e.g., ins in das in the). 2.1 Definition We de</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan, Y. and Knight, K. (2002). Translating named entities using monolingual and bilingual resources. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="18846" citStr="Berger et al. (1996)" startWordPosition="3065" endWordPosition="3068">p where the ’s are the feature values and the ’s are the feature weights. Since we have only a sample of the possible translations for the given input , we normalize the probability distribution, so that for our sample of candidate translations. Maximum entropy learning finds a set of feature values so that for each feature . These expectations are computed as sums over all candidate translations for all inputs : that it converges to a global optimum. There are a number of methods and tools available to carry out this training of feature values. We use the toolkit7 developed by Malouf (2002). Berger et al. (1996) and Manning and Sch¨utze (1999) provide good introductions to maximum entropy learning. Note that any other machine learning, such as support vector machines, could be used as well. We chose maximum entropy for its ability to deal with both real-valued and binary features. This method is also similar to work by Och and Ney (2002), who use maximum entropy to tune model parameters. 4 Properties of NP/PP Translation We will now discuss the properties of NP/PP translation that we exploit in order to improve our NP/PP translation subsystem. The first of these (compounding of words) is addressed by</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A. L., Pietra, S. A. D., and Pietra, V. J. D. (1996). A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="14189" citStr="Brown et al., 1993" startWordPosition="2298" endWordPosition="2301">pped from the word alignment. Punctuation is also stripped from the edges of NP/PPs. A total of 737,388 NP/PP pairs are collected from the German-English Europarl corpus as training data. Certain German NP/PPs consistently do not align to NP/PPs in English (see the example in Section 2.2). These are detected at this point. The obtained data of unaligned NP/PPs can be used for dealing with these special cases. 3.3 Base Model Given the NP/PP corpus, we can use any general statistical machine translation method to train a translation system for noun phrases. As a baseline, we use an IBM Model 4 (Brown et al., 1993) systema with a greedy decoder4 (Germann et al., 2001). We found that phrase based models achieve better translation quality than IBM Model 4. Such models segment the input sequence into a number of (non-linguistic) phrases, translate each phrase using a phrase translation table, and allow for reordering of phrases in the output. No phrases may be dropped or added. We use a phrase translation model that extracts its phrase translation table from word alignments generated by the Giza++ toolkit. Details of this model are described by Koehn et al. (2003). To obtain an n-best list of candidate tra</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., and Mercer, R. L. (1993). The mathematics of statistical machine translation. Computational Linguistics, 19(2):263–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Cao</author>
<author>H Li</author>
</authors>
<title>Base noun phrase translation using web data and the EM algorithm.</title>
<date>2002</date>
<booktitle>In Proceedings of CoLing.</booktitle>
<contexts>
<context position="2101" citStr="Cao and Li, 2002" startWordPosition="314" endWordPosition="317"> more dedicated modeling, but also the use of computationally more expensive methods. We go on to tackle the task of noun phrase translation in a maximum entropy reranking framework. Treating translation as a reranking problem instead of as a search problem enables us to use features over the full translation pair. We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on defining subtasks within statistical machine translation has been performed on, e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). 2 Noun Phrase Translation as a Subtask In this work, we consider both noun phrases and prepositional phrases, which we will refer to as NP/PPs. We include prepositional phrases for a number of reasons. Both are attached at the clause level. Also, the translation of the preposition often depends heavily on the noun phrase (in the morning). Moreover, the distinction between noun phrases and prepositional phrases is not always clear (note the Japanese bunsetsu) or hard to separate (German joining of preposition and determiner into one l</context>
</contexts>
<marker>Cao, Li, 2002</marker>
<rawString>Cao, Y. and Li, H. (2002). Base noun phrase translation using web data and the EM algorithm. In Proceedings of CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL 35.</booktitle>
<contexts>
<context position="12260" citStr="Collins, 1997" startWordPosition="1998" endWordPosition="1999">us. This means that there are some problems with parse errors, leading to sentence fragments extracted as NP/PPs that cannot be translated correctly. Also, the test corpus contains all detected NP/PPs, even untranslatable ones, as discussed in Section 2.2. 3.2 Acquisition of an NP/PP Training Corpus To train a statistical machine translation model, we need a training corpus of NP/PPs paired with their translation. We create this corpus by extracting NP/PPs from a parallel corpus. First, we word-align the corpus with Giza++ (Och and Ney, 2000). Then, we parse both sides with syntactic parsers (Collins, 1997; Schmidt and Schulte im Walde, 2000)2. Our definition easily translates into an algorithm to detect NP/PPs in a sentence. Recall that in such a corpus, only part of the NP/PPs are translated as such into the foreign language. In addition, the word-alignment and syntactic parses may be faulty. As a consequence, initially only 43.4% of all NP/PPs could be aligned. We raise this number to 67.2% with a number of automatic data cleaning steps: NP/PPs that partially align are broken up Systematic parse errors are fixed Certain word types that are inconsistently tagged as nouns in the two languages </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, M. (1997). Three generative, lexicalized models for statistical parsing. In Proceedings ofACL 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Germann</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>K Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL 39.</booktitle>
<contexts>
<context position="14243" citStr="Germann et al., 2001" startWordPosition="2307" endWordPosition="2310">ripped from the edges of NP/PPs. A total of 737,388 NP/PP pairs are collected from the German-English Europarl corpus as training data. Certain German NP/PPs consistently do not align to NP/PPs in English (see the example in Section 2.2). These are detected at this point. The obtained data of unaligned NP/PPs can be used for dealing with these special cases. 3.3 Base Model Given the NP/PP corpus, we can use any general statistical machine translation method to train a translation system for noun phrases. As a baseline, we use an IBM Model 4 (Brown et al., 1993) systema with a greedy decoder4 (Germann et al., 2001). We found that phrase based models achieve better translation quality than IBM Model 4. Such models segment the input sequence into a number of (non-linguistic) phrases, translate each phrase using a phrase translation table, and allow for reordering of phrases in the output. No phrases may be dropped or added. We use a phrase translation model that extracts its phrase translation table from word alignments generated by the Giza++ toolkit. Details of this model are described by Koehn et al. (2003). To obtain an n-best list of candidate translations, we developed a beam search decoder. This de</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Germann, U., Jahr, M., Knight, K., Marcu, D., and Yamada, K. (2001). Fast decoding and optimal decoding for machine translation. In Proceedings ofACL 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="20221" citStr="Koehn and Knight (2003)" startWordPosition="3291" endWordPosition="3294">s, is common in a number of languages (German, Dutch, Finnish, Greek), and poses a serious problem for machine translation: The word Aktionsplan may not be known to the system, but if the word were broken up into Aktion and Plan, the system could easily translate it into action plan, or plan for action. The issues for breaking up compounds are: Knowing the morphological rules for joining words, resolving ambiguities of breaking up a word (Hauptsturm Haupt-Turm or Haupt-Sturm), and finding the right level of splitting granularity (Frei-Tag or Freitag). Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. Compounds may be broken up only into known words in the corpus. For each potential compound we check if morphological splitting rules allow us to break it up into such known words. Finally, we pick a splitting option (perhaps not breaking up the compound at all). This decision is based on the frequency of the words involved. . 7Available at http://www-rohan.sdsu.edu/ mal A nice property of maximum entropy training is ouf/pubs.html Specifically, we pick the splitting option with highest geometric mean of word frequencie</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Koehn, P. and Knight, K. (2003). Empirical methods for compound splitting. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="14746" citStr="Koehn et al. (2003)" startWordPosition="2390" endWordPosition="2393">ses. As a baseline, we use an IBM Model 4 (Brown et al., 1993) systema with a greedy decoder4 (Germann et al., 2001). We found that phrase based models achieve better translation quality than IBM Model 4. Such models segment the input sequence into a number of (non-linguistic) phrases, translate each phrase using a phrase translation table, and allow for reordering of phrases in the output. No phrases may be dropped or added. We use a phrase translation model that extracts its phrase translation table from word alignments generated by the Giza++ toolkit. Details of this model are described by Koehn et al. (2003). To obtain an n-best list of candidate translations, we developed a beam search decoder. This decoder employs hypothesis recombination and stores the search states in a search graph – similar to work by Ueffing et al. (2002) – which can be mined with standard finite state machine methods5 for n-best lists. 3Available at http://www-i6.informatik.rwthaachen.de/ och/software/GIZA++.html 4Available at http://www.isi.edu/licensed-sw /rewrite-decoder/ 5We use the Carmel toolkit available at http://www. isi.edu/licensed-sw/carmel/ size of n-best list Figure 3: Acceptable NP/PP translations in n-best</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase based translation. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="18824" citStr="Malouf (2002)" startWordPosition="3063" endWordPosition="3064">tribution as exp where the ’s are the feature values and the ’s are the feature weights. Since we have only a sample of the possible translations for the given input , we normalize the probability distribution, so that for our sample of candidate translations. Maximum entropy learning finds a set of feature values so that for each feature . These expectations are computed as sums over all candidate translations for all inputs : that it converges to a global optimum. There are a number of methods and tools available to carry out this training of feature values. We use the toolkit7 developed by Malouf (2002). Berger et al. (1996) and Manning and Sch¨utze (1999) provide good introductions to maximum entropy learning. Note that any other machine learning, such as support vector machines, could be used as well. We chose maximum entropy for its ability to deal with both real-valued and binary features. This method is also similar to work by Och and Ney (2002), who use maximum entropy to tune model parameters. 4 Properties of NP/PP Translation We will now discuss the properties of NP/PP translation that we exploit in order to improve our NP/PP translation subsystem. The first of these (compounding of </context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Malouf, R. (2002). A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, C. D. and Sch¨utze, H. (1999). Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China.</location>
<contexts>
<context position="12195" citStr="Och and Ney, 2000" startWordPosition="1985" endWordPosition="1988"> are initially detected for the acquisition of the NP/PP training corpus. This means that there are some problems with parse errors, leading to sentence fragments extracted as NP/PPs that cannot be translated correctly. Also, the test corpus contains all detected NP/PPs, even untranslatable ones, as discussed in Section 2.2. 3.2 Acquisition of an NP/PP Training Corpus To train a statistical machine translation model, we need a training corpus of NP/PPs paired with their translation. We create this corpus by extracting NP/PPs from a parallel corpus. First, we word-align the corpus with Giza++ (Och and Ney, 2000). Then, we parse both sides with syntactic parsers (Collins, 1997; Schmidt and Schulte im Walde, 2000)2. Our definition easily translates into an algorithm to detect NP/PPs in a sentence. Recall that in such a corpus, only part of the NP/PPs are translated as such into the foreign language. In addition, the word-alignment and syntactic parses may be faulty. As a consequence, initially only 43.4% of all NP/PPs could be aligned. We raise this number to 67.2% with a number of automatic data cleaning steps: NP/PPs that partially align are broken up Systematic parse errors are fixed Certain word ty</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och, F. J. and Ney, H. (2000). Improved statistical alignment models. In Proceedings ofACL, pages 440–447, Hongkong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="19178" citStr="Och and Ney (2002)" startWordPosition="3122" endWordPosition="3125">expectations are computed as sums over all candidate translations for all inputs : that it converges to a global optimum. There are a number of methods and tools available to carry out this training of feature values. We use the toolkit7 developed by Malouf (2002). Berger et al. (1996) and Manning and Sch¨utze (1999) provide good introductions to maximum entropy learning. Note that any other machine learning, such as support vector machines, could be used as well. We chose maximum entropy for its ability to deal with both real-valued and binary features. This method is also similar to work by Och and Ney (2002), who use maximum entropy to tune model parameters. 4 Properties of NP/PP Translation We will now discuss the properties of NP/PP translation that we exploit in order to improve our NP/PP translation subsystem. The first of these (compounding of words) is addressed by preprocessing, while the others motivate features which are used in n-best list reranking. 4.1 Compound Splitting Compounding of words, especially nouns, is common in a number of languages (German, Dutch, Finnish, Greek), and poses a serious problem for machine translation: The word Aktionsplan may not be known to the system, but</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och, F. J. and Ney, H. (2002). Discriminative training and maximum entropy models for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10043" citStr="Papineni et al., 2002" startWordPosition="1638" endWordPosition="1641">ts are summarized in Table 1. Treating NP/PPs as isolated units, and translating them in isotranslation Figure 2: Design of the noun phrase translation subsystem: The base model generates an n-best list that is rescored using additional features lation with the same methods as the overall system has little impact on overall translation quality. In fact, we achieved a slight improvement in results due to the fact that NP/PPs are consistently translated as NP/PPs. A perfect NP/PP subsystem would triple the number of correctly translated sentences. Performance is also measured by the BLEU score (Papineni et al., 2002), which measures similarity to the reference translation taken from the English side of the parallel corpus. These findings indicate that solving the NP/PP translation problem would be a significant step toward improving overall translation quality, even if the overall system is not changed in any way. The findings also indicate that isolating the NP/PP translation task as a subtask does not harm performance. 3 Framework When translating a foreign input sentence, we detect its NP/PPs and translate them with an NP/PP translation subsystem. The best translation (or multiple best translations) is</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). BLEU: a method for automatic evaluation of machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmidt</author>
<author>Schulte im Walde</author>
<author>S</author>
</authors>
<title>Robust German noun chunking with a probabilistic context-free grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING.</booktitle>
<marker>Schmidt, Walde, S, 2000</marker>
<rawString>Schmidt, H. and Schulte im Walde, S. (2000). Robust German noun chunking with a probabilistic context-free grammar. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="14971" citStr="Ueffing et al. (2002)" startWordPosition="2427" endWordPosition="2430">t the input sequence into a number of (non-linguistic) phrases, translate each phrase using a phrase translation table, and allow for reordering of phrases in the output. No phrases may be dropped or added. We use a phrase translation model that extracts its phrase translation table from word alignments generated by the Giza++ toolkit. Details of this model are described by Koehn et al. (2003). To obtain an n-best list of candidate translations, we developed a beam search decoder. This decoder employs hypothesis recombination and stores the search states in a search graph – similar to work by Ueffing et al. (2002) – which can be mined with standard finite state machine methods5 for n-best lists. 3Available at http://www-i6.informatik.rwthaachen.de/ och/software/GIZA++.html 4Available at http://www.isi.edu/licensed-sw /rewrite-decoder/ 5We use the Carmel toolkit available at http://www. isi.edu/licensed-sw/carmel/ size of n-best list Figure 3: Acceptable NP/PP translations in n-best list for different sizes 3.4 Acceptable Translations in the n-Best List One key question for our approach is how often an acceptable translation can be found in an n-best list. The answer to this is illustrated in Figure 3: </context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>Ueffing, N., Och, F. J., and Ney, H. (2002). Generation of word graphs in statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>