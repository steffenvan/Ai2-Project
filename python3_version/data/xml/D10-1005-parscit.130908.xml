<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.96006">
Holistic Sentiment Analysis Across Languages:
Multilingual Supervised Latent Dirichlet Allocation
</title>
<author confidence="0.91016">
Jordan Boyd-Graber
</author>
<affiliation confidence="0.872284">
UMD iSchool
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.888775">
College Park, MD
</address>
<email confidence="0.999447">
jbg@umiacs.umd.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999797388888889">
In this paper, we develop multilingual super-
vised latent Dirichlet allocation (MLSLDA),
a probabilistic generative model that allows
insights gleaned from one language’s data to
inform how the model captures properties of
other languages. MLSLDA accomplishes this
by jointly modeling two aspects of text: how
multilingual concepts are clustered into themat-
ically coherent topics and how topics associ-
ated with text connect to an observed regres-
sion variable (such as ratings on a sentiment
scale). Concepts are represented in a general
hierarchical framework that is flexible enough
to express semantic ontologies, dictionaries,
clustering constraints, and, as a special, degen-
erate case, conventional topic models. Both
the topics and the regression are discovered
via posterior inference from corpora. We show
MLSLDA can build topics that are consistent
across languages, discover sensible bilingual
lexical correspondences, and leverage multilin-
gual corpora to better predict sentiment.
Sentiment analysis (Pang and Lee, 2008) offers
the promise of automatically discerning how people
feel about a product, person, organization, or issue
based on what they write online, which is potentially
of great value to businesses and other organizations.
However, the vast majority of sentiment resources
and algorithms are limited to a single language, usu-
ally English (Wilson, 2008; Baccianella and Sebas-
tiani, 2010). Since no single language captures a
majority of the content online, adopting such a lim-
ited approach in an increasingly global community
risks missing important details and trends that might
only be available when text in multiple languages is
taken into account.
</bodyText>
<author confidence="0.754451">
Philip Resnik
</author>
<affiliation confidence="0.970791666666667">
Department of Linguistics
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.774753">
College Park, MD
</address>
<email confidence="0.995465">
resnik@umd.edu
</email>
<bodyText confidence="0.999917685714286">
Up to this point, multiple languages have been
addressed in sentiment analysis primarily by trans-
ferring knowledge from a resource-rich language to
a less rich language (Banea et al., 2008), or by ig-
noring differences in languages via translation into
English (Denecke, 2008). These approaches are lim-
ited to a view of sentiment that takes place through
an English-centric lens, and they ignore the poten-
tial to share information between languages. Ide-
ally, learning sentiment cues holistically, across lan-
guages, would result in a richer and more globally
consistent picture.
In this paper, we introduce Multilingual Super-
vised Latent Dirichlet Allocation (MLSLDA), a
model for sentiment analysis on a multilingual cor-
pus. MLSLDA discovers a consistent, unified picture
of sentiment across multiple languages by learning
“topics,” probabilistic partitions of the vocabulary
that are consistent in terms of both meaning and rel-
evance to observed sentiment. Our approach makes
few assumptions about available resources, requiring
neither parallel corpora nor machine translation.
The rest of the paper proceeds as follows. In Sec-
tion 1, we describe the probabilistic tools that we use
to create consistent topics bridging across languages
and the MLSLDA model. In Section 2, we present
the inference process. We discuss our set of seman-
tic bridges between languages in Section 3, and our
experiments in Section 4 demonstrate that this ap-
proach functions as an effective multilingual topic
model, discovers sentiment-biased topics, and uses
multilingual corpora to make better sentiment pre-
dictions across languages. Sections 5 and 6 discuss
related research and discusses future work, respec-
tively.
</bodyText>
<page confidence="0.992895">
45
</page>
<note confidence="0.855222">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 45–55,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.83662" genericHeader="keywords">
1 Predictions from Multilingual Topics
</sectionHeader>
<bodyText confidence="0.999851608695652">
As its name suggests, MLSLDA is an extension of
Latent Dirichlet allocation (LDA) (Blei et al., 2003),
a modeling approach that takes a corpus of unan-
notated documents as input and produces two out-
puts, a set of “topics” and assignments of documents
to topics. Both the topics and the assignments are
probabilistic: a topic is represented as a probability
distribution over words in the corpus, and each doc-
ument is assigned a probability distribution over all
the topics. Topic models built on the foundations of
LDA are appealing for sentiment analysis because
the learned topics can cluster together sentiment-
bearing words, and because topic distributions are a
parsimonious way to represent a document.1
LDA has been used to discover latent structure
in text (e.g. for discourse segmentation (Purver et
al., 2006) and authorship (Rosen-Zvi et al., 2004)).
MLSLDA extends the approach by ensuring that this
latent structure — the underlying topics — is consis-
tent across languages. We discuss multilingual topic
modeling in Section 1.1, and in Section 1.2 we show
how this enables supervised regression regardless of
a document’s language.
</bodyText>
<subsectionHeader confidence="0.998615">
1.1 Capturing Semantic Correlations
</subsectionHeader>
<bodyText confidence="0.998795526315789">
Topic models posit a straightforward generative pro-
cess that creates an observed corpus. For each docu-
ment d, some distribution Bd over unobserved topics
is chosen. Then, for each word position in the doc-
ument, a topic z is selected. Finally, the word for
that position is generated by selecting from the topic
indexed by z. (Recall that in LDA, a “topic” is a
distribution over words).
In monolingual topic models, the topic distribution
is usually drawn from a Dirichlet distribution. Us-
ing Dirichlet distributions makes it easy to specify
sparse priors, and it also simplifies posterior infer-
ence because Dirichlet distributions are conjugate
to multinomial distributions. However, drawing top-
ics from Dirichlet distributions will not suffice if
our vocabulary includes multiple languages. If we
are working with English, German, and Chinese at
the same time, a Dirichlet prior has no way to fa-
vor distributions z such thatp(goodlz), p(gutlz), and
</bodyText>
<footnote confidence="0.8486335">
1The latter property has also made LDA popular for infor-
mation retrieval (Wei and Croft, 2006)).
</footnote>
<bodyText confidence="0.9981935625">
p(hˇao|z) all tend to be high at the same time, or low
at the same time. More generally, the structure of our
model must encourage topics to be consistent across
languages, and Dirichlet distributions cannot encode
correlations between elements.
One possible solution to this problem is to use the
multivariate normal distribution, which can produce
correlated multinomials (Blei and Lafferty, 2005),
in place of the Dirichlet distribution. This has been
done successfully in multilingual settings (Cohen
and Smith, 2009). However, such models complicate
inference by not being conjugate.
Instead, we appeal to tree-based extensions of the
Dirichlet distribution, which has been used to induce
correlation in semantic ontologies (Boyd-Graber et
al., 2007) and to encode clustering constraints (An-
drzejewski et al., 2009). The key idea in this ap-
proach is to assume the vocabularies of all languages
are organized according to some shared semantic
structure that can be represented as a tree. For con-
creteness in this section, we will use WordNet (Miller,
1990) as the representation of this multilingual se-
mantic bridge, since it is well known, offers conve-
nient and intuitive terminology, and demonstrates the
full flexibility of our approach. However, the model
we describe generalizes to any tree-structured rep-
resentation of multilingual knowledge; we discuss
some alternatives in Section 3.
WordNet organizes a vocabulary into a rooted, di-
rected acyclic graph of nodes called synsets, short for
“synonym sets.” A synset is a child of another synset
if it satisfies a hyponomy relationship; each child “is
a” more specific instantiation of its parent concept
(thus, hyponomy is often called an “isa” relationship).
For example, a “dog” is a “canine” is an “animal” is
a “living thing,” etc. As an approximation, it is not
unreasonable to assume that WordNet’s structure of
meaning is language independent, i.e. the concept
encoded by a synset can be realized using terms in
different languages that share the same meaning. In
practice, this organization has been used to create
many alignments of international WordNets to the
original English WordNet (Ordan and Wintner, 2007;
Sagot and Fiˇser, 2008; Isahara et al., 2008).
Using the structure of WordNet, we can now de-
scribe a generative process that produces a distribu-
tion over a multilingual vocabulary, which encour-
ages correlations between words with similar mean-
</bodyText>
<page confidence="0.998819">
46
</page>
<bodyText confidence="0.987280333333333">
ings regardless of what language each word is in.
For each synset h, we create a multilingual word
distribution for that synset as follows:
</bodyText>
<listItem confidence="0.99587925">
1. Draw transition probabilities Qh — Dir (Th)
2. Draw stop probabilities wh — Dir (fah)
3. For each language l, draw emission probabilities for
that synset 0h,l — Dir (7rh,l).
</listItem>
<bodyText confidence="0.999618333333333">
For conciseness in the rest of the paper, we will refer
to this generative process as multilingual Dirichlet
hierarchy, or MULTDIRHIER(r, r., 7r).2 Each ob-
served token can be viewed as the end result of a
sequence of visited synsets A. At each node in the
tree, the path can end at node i with probability wi,1,
or it can continue to a child synset with probability
wi,0. If the path continues to another child synset, it
visits child j with probability Qi,j. If the path ends at
a synset, it generates word k with probability Oi,l,k.3
The probability of a word being emitted from a path
with visited synsets r and final synset h in language
</bodyText>
<equation confidence="0.94102275">
l is therefore
p(w, A = r, h|l,,Q, w, 0) =
(n Qi,jwi,0 �(1 — wh,1)0h,l,w. (1)
(i,j)Er
</equation>
<bodyText confidence="0.999981">
Note that the stop probability wh is independent of
language, but the emission 0h,l is dependent on the
language. This is done to prevent the following sce-
nario: while synset A is highly probable in a topic
and words in language 1 attached to that synset have
high probability, words in language 2 have low prob-
ability. If this could happen for many synsets in
a topic, an entire language would be effectively si-
lenced, which would lead to inconsistent topics (e.g.
</bodyText>
<footnote confidence="0.510528">
2Variables Th, irh,f, and r1h are hyperparameters. Their mean
is fixed, but their magnitude is sampled during inference (i.e.
Th,i is constant, but Th,i is not). For the bushier bridges,
-k Th,k
</footnote>
<bodyText confidence="0.9664768">
(e.g. dictionary and flat), their mean is uniform. For GermaNet,
we took frequencies from two balanced corpora of German and
English: the British National Corpus (University of Oxford,
2006) and the Kern Corpus of the Digitales W¨orterbuch der
Deutschen Sprache des 20. Jahrhunderts project (Geyken, 2007).
We took these frequencies and propagated them through the
multilingual hierarchy, following LDAWN’s (Boyd-Graber et
al., 2007) formulation of information content (Resnik, 1995) as
a Bayesian prior. The variance of the priors was initialized to be
1.0, but could be sampled during inference.
3Note that the language and word are taken as given, but the
path through the semantic hierarchy is a latent random variable.
Topic 1 is about baseball in English and about travel
in German). Separating path from emission helps
ensure that topics are consistent across languages.
Having defined topic distributions in a way that can
preserve cross-language correspondences, we now
use this distribution within a larger model that can
discover cross-language patterns of use that predict
sentiment.
</bodyText>
<subsectionHeader confidence="0.89271">
1.2 The MLSLDA Model
</subsectionHeader>
<bodyText confidence="0.9999486875">
We will view sentiment analysis as a regression prob-
lem: given an input document, we want to predict
a real-valued observation y that represents the senti-
ment of a document. Specifically, we build on super-
vised latent Dirichlet allocation (SLDA, (Blei and
McAuliffe, 2007)), which makes predictions based
on the topics expressed in a document; this can be
thought of projecting the words in a document to low
dimensional space of dimension equal to the number
of topics. Blei et al. showed that using this latent
topic structure can offer improved predictions over re-
gressions based on words alone, and the approach fits
well with our current goals, since word-level cues are
unlikely to be identical across languages. In addition
to text, SLDA has been successfully applied to other
domains such as social networks (Chang and Blei,
2009) and image classification (Wang et al., 2009).
The key innovation in this paper is to extend SLDA
by creating topics that are globally consistent across
languages, using the bridging approach above.
We express our model in the form of a probabilis-
tic generative latent-variable model that generates
documents in multiple languages and assigns a real-
valued score to each document. The score comes
from a normal distribution whose sum is the dot prod-
uct between a regression parameter q that encodes
the influence of each topic on the observation and
a variance a2. With this model in hand, we use sta-
tistical inference to determine the distribution over
latent variables that, given the model, best explains
observed data.
The generative model is as follows:
</bodyText>
<listItem confidence="0.9912516">
1. For each topic i = 1... K, draw a topic distribution
{,Qi, wi, 0i} from MULTDIRHIER(r, r., 7r).
2. For each document d = 1... M with language ld:
(a) Choose a distribution over topics Bd �
Dir (a).
</listItem>
<page confidence="0.990823">
47
</page>
<bodyText confidence="0.949569">
(b) For each word in the document n = 1... Nd,
choose a topic assignment zd,n u Mult (θd)
and a path λd,n ending at word wd,n according
to Equation 1 using {,Qzd�n,Wzd�n, Ozd�n}.
</bodyText>
<listItem confidence="0.695077">
3. Choose a response variable from y u
</listItem>
<equation confidence="0.438311">
Norm (ηT�z, σ2), 1 EN
</equation>
<bodyText confidence="0.99964175">
where zd — N n=1 zd,n.
Crucially, note that the topics are not indepen-
dent of the sentiment task; the regression encourages
terms with similar effects on the observation y to
be in the same topic. The consistency of topics de-
scribed above allows the same regression to be done
for the entire corpus regardless of the language of the
underlying document.
</bodyText>
<sectionHeader confidence="0.999028" genericHeader="introduction">
2 Inference
</sectionHeader>
<bodyText confidence="0.999980090909091">
Finding the model parameters most likely to explain
the data is a problem of statistical inference. We em-
ploy stochastic EM (Diebolt and Ip, 1996), using a
Gibbs sampler for the E-step to assign words to paths
and topics. After randomly initializing the topics,
we alternate between sampling the topic and path
of a word (zd,n, λd,n) and finding the regression pa-
rameters η that maximize the likelihood. We jointly
sample the topic and path conditioning on all of the
other path and document assignments in the corpus,
selecting a path and topic with probability
</bodyText>
<equation confidence="0.890471">
p(zn = k, λn = r|z−n, X−n, wn, η, σ, 6)) =
p(yd|z, η, σ)p(λn = r|zn = k, X−n, wn, r, K, 7)
p(zn = k|z−n, α). (2)
</equation>
<bodyText confidence="0.999986">
Each of these three terms reflects a different influence
on the topics from the vocabulary structure, the doc-
ument’s topics, and the response variable. In the next
paragraphs, we will expand each of them to derive
the full conditional topic distribution.
As discussed in Section 1.1, the structure of the
topic distribution encourages terms with the same
meaning to be in the same topic, even across lan-
guages. During inference, we marginalize over pos-
sible multinomial distributions β, ω, and φ, using
the observed transitions from i to j in topic k; Tk,i,j,
stop counts in synset i in topic k, Ok,i,0; continue
counts in synsets i in topic k, Ok,i,1; and emission
counts in synset i in language l in topic k, Fk,i,l. The
</bodyText>
<subsectionHeader confidence="0.749299">
Multilingual Topics Text Documents Sentiment Prediction
</subsectionHeader>
<bodyText confidence="0.64297425">
Figure 1: Graphical model representing MLSLDA.
Shaded nodes represent observations, plates denote repli-
cation, and lines show probabilistic dependencies.
probability of taking a path r is then
</bodyText>
<equation confidence="0.99567475">
p(λn = r|zn = k,X−n) =
!
Ok,i,1 + ωi
P
s∈0,1 Ok,i,s + ωi,s
 |{z }
Transition
Ok,rend,0 + ωrend Fk,rend,wn + πrend,l
Ps∈0,1 Ok,rend,s + ωrend,s
 |{z }
Emission
(3)
</equation>
<bodyText confidence="0.999910214285714">
Equation 3 reflects the multilingual aspect of this
model. The conditional topic distribution for
SLDA (Blei and McAuliffe, 2007) replaces this term
with the standard Multinomial-Dirichlet. However,
we believe this is the first published SLDA-style
model using MCMC inference, as prior work has
used variational inference (Blei and McAuliffe, 2007;
Chang and Blei, 2009; Wang et al., 2009).
Because the observed response variable depends
on the topic assignments of a document, the condi-
tional topic distribution is shifted toward topics that
explain the observed response. Topics that move the
predicted response yd toward the true yd will be fa-
vored. We drop terms that are constant across all
</bodyText>
<figure confidence="0.925636913043478">
7ch,l (Pi,h,l
�h
Th pi,h
(0i,h
L
K
H
wd,n
Xd,n
zd,n
0d
α
N
yd
M
6
71
Bk,i,j + τi,j
P
j/ Bk,i,j&apos; + τi,j
Y
(i,j)∈r
Pw/ Frend,w/ + πrend,w/ .
</figure>
<page confidence="0.991364">
48
</page>
<bodyText confidence="0.988365">
topics for the effect of the response variable,
The above equation represents the supervised aspect
of the model, which is inherited from SLDA.
Finally, there is the effect of the topics already
assigned to a document; the conditional distribution
favors topics already assigned in a document,
</bodyText>
<equation confidence="0.999703">
p(zn = k|z−n, α) = Td ,k + αk (5)
&amp;0 Td,k0 + αk0
</equation>
<bodyText confidence="0.984174814814815">
This term represents the document focus of this
model; it is present in all Gibbs sampling inference
schemes for LDA (Griffiths and Steyvers, 2004).
Multiplying together Equations 3, 4, and 5 allows
us to sample a topic using the conditional distribution
from Equation 2, based on the topic and path of the
other words in all languages. After sampling the
path and topic for each word in a document, we then
find new regression parameters η that maximize the
likelihood conditioned on the current state of the
sampler. This is simply a least squares regression
using the topic assignments zd to predict yd.
Prediction on documents for which we don’t have
an observed yd is equivalent to marginalizing over
yd and sampling topics for the document from Equa-
tions 3 and 5. The prediction for yd is then the dot
product of η and the empirical topic distribution zd.
We initially optimized all hyperparameters using
slice sampling. However, we found that the regres-
sion variance σ2 was not stable. Optimizing σ2 seems
to balance between modeling the language in the doc-
uments and the prediction, and thus is sensitive to
documents’ length. Given this sensitivity, we did
not optimize σ2 for our prediction experiments in
Section 4, but instead kept it fixed at 0.25. We leave
optimizing this variable, either through cross valida-
tion or adapting the model, to future work.
</bodyText>
<sectionHeader confidence="0.984088" genericHeader="method">
3 Bridges Across Languages
</sectionHeader>
<bodyText confidence="0.997080466666666">
In Section 1.1, we described connections across lan-
guages as offered by semantic networks in a general
way, using WordNet as an example. In this section,
we provide more specifics, as well as alternative ways
of building semantic connections across languages.
Flat First, we can consider a degenerate mapping
that is nearly equivalent to running SLDA indepen-
dently across multiple languages, relating topics only
based on the impact on the response variable. Con-
sider a degenerate tree with only one node, with all
words in all languages associated with that node. This
is consistent with our model, but there is really no
shared semantic space, as all emitted words must
come from this degenerate “synset” and the model
only represents the output distribution for this single
node.
Other words’ influence
WordNet We took the alignment of GermaNet to
WordNet 1.6 (Kunze and Lemnitzer, 2002) and re-
moved all synsets that were had no mapped German
words. Any German synsets that did not have English
translations had their words mapped to the lowest
extant English hypernym (e.g. “beinbruch,” a bro-
ken leg, was mapped to “fracture”). We stemmed
all words to account for inflected forms not being
present (Porter and Boulton, 1970). An example
of the paths for the German word “wunsch” (wish,
request) is shown in Figure 2(a).
Dictionaries A dictionary can be viewed as a many
to many mapping, where each entry ei maps one
or more words in one language si to one or more
words ti in another language. Entries were taken
from an English-German dictionary (Richter, 2008)
a Chinese-English dictionary (Denisowski, 1997),
and a Chinese-German dictionary (Hefti, 2005). As
with WordNet, the words in entries for English and
German were stemmed to improve coverage. An
example for German is shown in Figure 2(b).
Algorithmic Connections In addition to hand-
curated connections across languages, one could also
consider automatic means of mapping across lan-
guages, such as using edit distance or local con-
text (Haghighi et al., 2008; Rapp, 1995) or us-
ing a lexical translation table obtained from paral-
lel text (Melamed, 1998). While we experimented
</bodyText>
<figure confidence="0.928415881355932">
p(yd|z, η, σ) a
� exp
1 \ Ek0 Nd,k0ηk0 ηzk
2 yd −
σ Ek, Nd k0 / E
0Nd,k0
k
• .r •
� � �
2 . (4)
exp
−η
zk
2σ2 Ek0 N2d,k0
• .r •
This word’s influence
49
(a) GermaNet
(b) Dictionary
wish
wunsch
ask
request
anfrag
wunsch
altern
option
option.n.02
choic
cognition.n.01 event.n.01
preference.n.03
wish.n.04
event
option
abstraction.n.06
ereignis
deed
vorgang
entity.n.01
entiti
speech_act.n.01
request.n.02
act
objekt
act.n.02
handlung
root
dict.1
dict.2
room
gelass
room
raum
platz
room
zimm
raum
stub
dict.3
</figure>
<figureCaption confidence="0.9951834">
Figure 2: Two methods for constructing multilingual distributions over words. On the left, paths to the German word
“wunsch” in GermaNet are shown. On the right, paths to the English word “room” are shown. Both English and German
words are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines). Note
that different senses are denoted by different internal paths, and that internal paths are distinct from the per-language
expression.
</figureCaption>
<bodyText confidence="0.99994425">
with these techniques, constructing appropriate hier-
archies from these resources required many arbitrary
decisions about cutoffs and which words to include.
Thus, we do not consider them in this paper.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999764">
We evaluate MLSLDA on three criteria: how well
it can discover consistent topics across languages
for matching parallel documents, how well it can
discover sentiment-correlated word lists from non-
aligned text, and how well it can predict sentiment.
</bodyText>
<subsectionHeader confidence="0.999462">
4.1 Matching on Multilingual Topics
</subsectionHeader>
<bodyText confidence="0.999913333333333">
We took the 1996 documents from the Europarl cor-
pus (Koehn, 2005) using three bridges: GermaNet,
dictionary, and the uninformative flat matching.4 The
model is unaware that the translations of documents
in one language are present in the other language.
Note that this does not use the supervised framework
</bodyText>
<footnote confidence="0.629451">
4For English and German documents in all experiments,
we removed stop words (Loper and Bird, 2002), stemmed
words (Porter and Boulton, 1970), and created a vocabulary
</footnote>
<bodyText confidence="0.98830252">
of the most frequent 5000 words per language (this vocabulary
limit was mostly done to ensure that the dictionary-based bridge
was of manageable size). Documents shorter than fifty content
words were excluded.
(as there is no associated response variable for Eu-
roparl documents); this experiment is to demonstrate
the effectiveness of the multilingual aspect of the
model. To test whether the topics learned by the
model are consistent across languages, we represent
each document using the probability distribution Bd
over topic assignments. Each Bd is a vector of length
K and is a language-independent representation of
the document.
For each document in one language, we computed
the Hellinger distance between it and all of the docu-
ments in the other language and sorted the documents
by decreasing distance. The translation of the docu-
ment is somewhere in that set; the higher the normal-
ized rank (the percentage of documents with a rank
lower than the translation of the document), the better
the underlying topic model connects languages.
We compare three bridges against what is to our
knowledge the only other topic model for unaligned
text, Multilingual Topics for Unaligned Text (Boyd-
Graber and Blei, 2009).5
</bodyText>
<footnote confidence="0.4939248">
5The bipartite matching was initialized with the dictionary
weights as specified by the Multilingual Topics for Unaligned
Text algorithm. The matching size was limited to 250 and the
bipartite matching was only updated on the initial iteration then
held fixed. This yielded results comparable to when the matching
</footnote>
<page confidence="0.980195">
50
</page>
<figure confidence="0.999792131386861">
0.8
0.0
-0.8
god himmel diet essen book buch
us gedanken food diŠt books immer
religion glaube eat verlieren one leben
church unsere weight befinden life art
human kirche eating kšrper person lesen
wahrheit healthy rezepte people thema
fat autor
-1.2 -0.4 0.4
1.2
buch
roman
leser
seiten
geschichte
handlung
great
good
business
all
one
companies
right
gesellschaft
genau
Gberzeugt
ergebnis
mittel
verlangen
film
filme
episode
star
story
gibt
book
novel
reader
pages
books
tale
kind
kinder
eltern
baby
nacht
movie
film
episode
movies
scenes
separate
gives
children
baby
child
parents
sleep
(harry) harry
(volume) band
(sky) himmel
(universe) all
(vampire) vampir
(last) letzt
(part) teil
aM+J (harry)
M (belt)
3F (sky)
6 (both)
S (section)
I M31 (vampire)
A3.11 (strong)
)MF (last)
(god) gott
(lord) herr
(religion) religion
(universe) all
(world) welt
(science) wissenschaft
(medicine) medizin
(society) gesellschaft
fill (god)
$It (lord)
6 (both)
-11
(religion)
Aq- (science)
#!&amp; (community)
0.8
(book) buch
(itself) sich
(that) dass
(much) viel
(no) kein
(good) gut
(when) wenn
#3 (book)
aM ([I&apos;m afraid that...])
6 (both)
A (discard)
*:y (myself)
)Sft (mostly)
#3�}c (book)
89 ([really isn&apos;t])
(female) weiblich
(equal) gleich
(woman) frau
(fast) schnell
(point) punkt
(man) mann
(soon) bald
&apos;k (quick)
A (a little)
AA (woman)
NA (man)
A (female)
N (male)
Est): (female)
6 (both)
0 (good)
- (set)
. (treasure)
9p (handsome)
9 (both)
,&amp;* (story)
dk (small)
(good) gut
(sentence) satz
(two) zwei
(story) story
(treasure) schatz
(attractive) attraktiv
(elegant) elegant
(gem) juwel
(a) German / English (b) German /Chinese
</figure>
<figureCaption confidence="0.9983006">
Figure 4: Topics, along with associated regression coefficient 77 from a learned 25-topic model on German-English (left)
and German-Chinese (right) documents. Notice that theme-related topics have regression parameter near zero, topics
discussing the number of pages have negative regression parameters, topics with “good,” “great,” “hˇao” (good) and
“¨uberzeugt” (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of “gut”
(good) in one of the negative sentiment topics, showing the difficulty of learning collocations.
</figureCaption>
<table confidence="0.999468">
Train Test GermaNet Dictionary Flat
DE DE 73.8 24.8 92.2
EN DE 7.44 2.68 18.3
EN + DE DE 1.17 1.46 1.39
</table>
<tableCaption confidence="0.661388">
Table 1: Mean squared error on a film review corpus.
All results are on the same German test data, varying the
training data. Over-fitting prevents the model learning on
the German data alone; adding English data to the mix
allows the model to make better predictions.
</tableCaption>
<bodyText confidence="0.989077255813954">
5000 film reviews (Pang and Lee, 2005) to create a
multilingual film review corpus.6
The results for predicting sentiment in German
documents with 25 topics are presented in Table 1.
On a small monolingual corpus, prediction is very
poor. The model over-fits, especially when it has
the entire vocabulary to select from. The slightly
better performance using GermaNet and a dictionary
as topic priors can be viewed as basic feature selec-
tion, removing proper names from the vocabulary to
6We followed Pang and Lee’s method for creating a nu-
merical score between 0 and 1 from a star rating. We
then converted that to an integer by multiplying by 100;
this was done because initial data preprocessing assumed
integer values (although downstream processing did not as-
sume integer values). The German movie review corpus
is available at http://www.umiacs.umd.edu/˜jbg/
static/downloads_and_media.html
prevent over-fitting.
One would expect that prediction improves with a
larger training set. For this model, such an improve-
ment is seen even when the training set includes no
documents in the target language. Note that even the
degenerate flat bridge across languages provides use-
ful information. After introducing English data, the
model learns to prefer smaller regression parameters
(this can be seen as a form of regularization).
Performance is best when a reasonably large cor-
pus is available including some data in the target
language. For each bridge, performance improves
dramatically, showing that MLSLDA is successfully
able to incorporate information learned from both
languages to build a single, coherent picture of how
sentiment is expressed in both languages. With the
GermaNet bridge, performance is better than both
the degenerate and dictionary based bridges, showing
that the model is sharing information both through
the multilingual topics and the regression parameters.
Performance on English prediction is comparable
to previously published results on this dataset (Blei
and McAuliffe, 2007); with enough data, a monolin-
gual model is no longer helped by adding additional
multilingual data.
</bodyText>
<page confidence="0.998467">
52
</page>
<sectionHeader confidence="0.986012" genericHeader="method">
5 Relationship to Previous Research
</sectionHeader>
<bodyText confidence="0.999984333333334">
The advantages of MLSLDA reside largely in the
assumptions that it makes and does not make: docu-
ments need not be parallel, sentiment is a normally
distributed document-level property, words are ex-
changeable, and sentiment can be predicted as a re-
gression on a K-dimensional vector.
By not assuming parallel text, this approach can
be applied to a broad class of corpora. Other mul-
tilingual topic models require parallel text, either at
the document (Ni et al., 2009; Mimno et al., 2009)
or word-level (Kim and Khudanpur, 2004; Zhao and
Xing, 2006). Similarly, other multilingual sentiment
approaches also require parallel text, often supplied
via automatic translation; after the translated text
is available, either monolingual analysis (Denecke,
2008) or co-training is applied (Wan, 2009). In con-
trast, our approach requires fewer resources for a lan-
guage: a dictionary (or similar knowledge structure
relating words to nodes in a graph) and comparable
text, instead of parallel text or a machine translation
system.
Rather than viewing one language through the
lens of another language, MLSLDA views all lan-
guages through the lens of the topics present in a
document. This is a modeling decision with pros and
cons. It allows a language agnostic decision about
sentiment to be made, but it restricts the expressive-
ness of the model in terms of sentiment in two ways.
First, it throws away information important to sen-
timent analysis like syntactic constructions (Greene
and Resnik, 2009) and document structure (McDon-
ald et al., 2007) that may impact the sentiment rating.
Second, a single real number is not always sufficient
to capture the nuances of sentiment. Less critically,
assuming that sentiment is normally distributed is not
true of all real-world corpora; review corpora often
have a skew toward positive reviews. We standardize
responses by the mean and variance of the training
data to partially address this issue, but other response
distributions are possible, such as generalized linear
models (Blei and McAuliffe, 2007) and vector ma-
chines (Zhu et al., 2009), which would allow more
traditional classification predictions.
Other probabilistic models for sentiment classifi-
cation view sentiment as a word level feature. Some
models use sentiment word lists, either given or
learned from a corpus, as a prior to seed topics so
that they attract other sentiment bearing words (Mei
et al., 2007; Lin and He, 2009). Other approaches
view sentiment or perspective as a perturbation of
a log-linear topic model (Lin et al., 2008). Such
techniques could be combined with the multilingual
approach presented here by using distributions over
words that not only bridge different languages but
also encode additional information. For example, the
vocabulary hierarchies could be structured to encour-
age topics that encourage correlation among similar
sentiment-bearing words (e.g. clustering words asso-
ciated with price, size, etc.). Future work could also
more rigorously validate that the multilingual topics
discovered by MLSLDA are sentiment-bearing via
human judgments.
In contrast, MLSLDA draws on techniques that
view sentiment as a regression problem based on the
topics used in a document, as in supervised latent
Dirichlet allocation (SLDA) (Blei and McAuliffe,
2007) or in finer-grained parts of a document (Titov
and McDonald, 2008). Extending these models to
multilingual data would be more straightforward.
</bodyText>
<sectionHeader confidence="0.999603" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999960681818182">
MLSLDA is a “holistic” statistical model for multi-
lingual corpora that does not require parallel text
or expensive multilingual resources. It discovers
connections across languages that can recover la-
tent structure in parallel corpora, discover sentiment-
correlated word lists in multiple languages, and make
accurate predictions across languages that improve
with more multilingual data, as demonstrated in the
context of sentiment analysis.
More generally, MLSLDA provides a formalism
that can be used to incorporate the many insights of
topic modeling-driven sentiment analysis to multi-
lingual corpora by tying together word distributions
across languages. MLSLDA can also contribute to
the development of word list-based sentiment sys-
tems: the topics discovered by MLSLDA can serve
as a first-pass means of sentiment-based word lists
for languages that might lack annotated resources.
MLSLDA also can be viewed as a sentiment-
informed multilingual word sense disambiguation
(WSD) algorithm. When the multilingual bridge is an
explicit representation of sense such as WordNet, part
</bodyText>
<page confidence="0.995605">
53
</page>
<bodyText confidence="0.99997525">
of the generative process is an explicit assignment
of every word to sense (the path latent variable A);
this is discovered during inference. The dictionary-
based technique may be viewed as a disambiguation
via a transfer dictionary. How sentiment prediction
impacts the implicit WSD is left to future work.
Better capturing local syntax and meaningful col-
locations would also improve the model’s ability to
predict sentiment and model multilingual topics, as
would providing a better mechanism for represent-
ing words not included in our bridges. We intend to
develop such models as future work.
</bodyText>
<sectionHeader confidence="0.998914" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999216">
This research was funded in part by the Army Re-
search Laboratory through ARL Cooperative Agree-
ment W911NF-09-2-0072 and by the Office of the
Director of National Intelligence (ODNI), Intelli-
gence Advanced Research Projects Activity (IARPA),
through the Army Research Laboratory. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of ARL, IARPA, the ODNI, or the U.S. Govern-
ment. The authors thank the anonymous reviewers,
Jonathan Chang, Christiane Fellbaum, and Lawrence
Watts for helpful comments. The authors especially
thank Chris Potts for providing help in obtaining and
processing reviews.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856728571428">
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In ICML.
Andrea Esuli Stefano Baccianella and Fabrizio Sebastiani.
2010. Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In LREC.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer
Hassan. 2008. Multilingual subjectivity analysis using
machine translation. In EMNLP.
David M. Blei and John D. Lafferty. 2005. Correlated
topic models. In NIPS.
David M. Blei and Jon D. McAuliffe. 2007. Supervised
topic models. In NIPS. MIT Press.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. JMLR, 3:993–1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In UAI.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In EMNLP.
Jonathan Chang and David M. Blei. 2009. Relational
topic models for document networks. In AISTATS.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In NAACL.
Noah Constant, Christopher Davis, Christopher Potts, and
Florian Schwarz. 2009. The pragmatics of expressive
content: Evidence from large corpora. Sprache und
Datenverarbeitung, 33(1–2).
Kerstin Denecke. 2008. Using SentiWordNet for multilin-
gual sentiment analysis. In ICDEW 2008.
Paul Denisowski. 1997. CEDICT.
http://www.mdbg.net/chindict/.
Jean Diebolt and Eddie H.S. Ip, 1996. Markov Chain
Monte Carlo in Practice, chapter Stochastic EM:
method and application. Chapman and Hall, London.
Alexander Geyken. 2007. The DWDS corpus: A ref-
erence corpus for the German language of the 20th
century. In Idioms and Collocations: Corpus-based
Linguistic, Lexicographic Studies. Continuum Press.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
NAACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(Suppl 1):5228–5235.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and
Dan Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In ACL, Columbus, Ohio.
Jan Hefti. 2005. HanDeDict. http://chdw.de.
Hitoshi Isahara, Fransis Bond, Kiyotaka Uchimoto, Masao
Utiyama, and Kyoko Kanzaki. 2008. Development of
the Japanese WordNet. In LREC.
Mark Johnson. 2010. PCFGs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In ACL.
Woosung Kim and Sanjeev Khudanpur. 2004. Lexical
triggers and latent semantic analysis for cross-lingual
language model adaptation. TALIP, 3(2):94–112.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Summit.
http://www.statmt.org/europarl/.
Claudia Kunze and Lothar Lemnitzer. 2002. Standardiz-
ing WordNets in a web-compliant format: The case of
GermaNet. In Workshop on Wordnets Structures and
Standardisation.
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In CIKM.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In ECML PKDD.
</reference>
<page confidence="0.978303">
54
</page>
<reference confidence="0.9997525">
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching. ACL.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In ACL.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In WWW.
Ilya Dan Melamed. 1998. Empirical methods for exploit-
ing parallel texts. Ph.D. thesis, University of Pennsyl-
vania.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245–264.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In EMNLP.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
WWW.
Noam Ordan and Shuly Wintner. 2007. Hebrew Word-
Net: a test case of aligning lexical databases across lan-
guages. International Journal of Translation, 19(1):39–
58.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In ACL.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Now Publishers Inc.
Martin Porter and Richard Boulton. 1970. Snowball
stemmer. http://snowball.tartarus.org/credits.php.
Matthew Purver, Konrad K¨ording, Thomas L. Griffiths,
and Joshua Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In ACL.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In ACL, pages 320–322.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In IJCAI, pages
448–453.
Frank Richter. 2008. Dictionary nice grep. http://www-
user.tu-chemnitz.de/ fri/ding/.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern boot-
strapping. In NAACL.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model for
authors and documents. In UAI.
Benoit Sagot and Darja Fiˇser. 2008. Building a Free
French WordNet from Multilingual Resources. In On-
toLex.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization. In
ACL, pages 308–316.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Ju-
rafsky, and Christopher Manning. 2005. A conditional
random field word segmenter. In SIGHAN Workshop
on Chinese Language Processing.
University of Oxford. 2006. British Na-
tional Corpus. http://www.natcorp.ox.ac.uk/.
http://www.natcorp.ox.ac.uk/.
Tobias Vetter, Manfred Sauer, and Philipp Wallutat.
2000. Filmrezension.de: Online-magazin f¨ur filmkritik.
http://www.filmrezension.de.
Xiaojun Wan. 2009. Co-training for cross-lingual senti-
ment classification. In ACL.
Chong Wang, David Blei, and Li Fei-Fei. 2009. Simulta-
neous image classification and annotation. In CVPR.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrieval. In SIGIR.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In CIKM.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polarity,
and Attitudes of Private States. Ph.D. thesis, University
of Pittsburgh.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In ACL.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. Medlda:
maximum margin supervised topic models for regres-
sion and classification. In ICML.
</reference>
<page confidence="0.999062">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997842">Holistic Sentiment Analysis Across Multilingual Supervised Latent Dirichlet Allocation</title>
<author confidence="0.99836">Jordan</author>
<affiliation confidence="0.922058666666667">UMD and University of</affiliation>
<address confidence="0.716663">College Park,</address>
<email confidence="0.999863">jbg@umiacs.umd.edu</email>
<abstract confidence="0.999851918918919">In this paper, we develop multilingual superlatent Dirichlet allocation a probabilistic generative model that allows insights gleaned from one language’s data to inform how the model captures properties of languages. accomplishes this by jointly modeling two aspects of text: how multilingual concepts are clustered into thematically coherent topics and how topics associated with text connect to an observed regression variable (such as ratings on a sentiment scale). Concepts are represented in a general hierarchical framework that is flexible enough to express semantic ontologies, dictionaries, clustering constraints, and, as a special, degenerate case, conventional topic models. Both the topics and the regression are discovered via posterior inference from corpora. We show can build topics that are consistent across languages, discover sensible bilingual lexical correspondences, and leverage multilingual corpora to better predict sentiment. Sentiment analysis (Pang and Lee, 2008) offers the promise of automatically discerning how people feel about a product, person, organization, or issue based on what they write online, which is potentially of great value to businesses and other organizations. However, the vast majority of sentiment resources and algorithms are limited to a single language, usually English (Wilson, 2008; Baccianella and Sebastiani, 2010). Since no single language captures a majority of the content online, adopting such a limited approach in an increasingly global community risks missing important details and trends that might only be available when text in multiple languages is taken into account.</abstract>
<author confidence="0.923419">Philip</author>
<affiliation confidence="0.997764666666667">Department of and University of</affiliation>
<address confidence="0.640792">College Park,</address>
<email confidence="0.998875">resnik@umd.edu</email>
<abstract confidence="0.995641171428572">Up to this point, multiple languages have been addressed in sentiment analysis primarily by transferring knowledge from a resource-rich language to a less rich language (Banea et al., 2008), or by ignoring differences in languages via translation into English (Denecke, 2008). These approaches are limited to a view of sentiment that takes place through an English-centric lens, and they ignore the potential to share information between languages. Idelearning sentiment cues holistically, languages, would result in a richer and more globally consistent picture. In this paper, we introduce Multilingual Super- Latent Dirichlet Allocation a model for sentiment analysis on a multilingual cordiscovers a consistent, unified picture of sentiment across multiple languages by learning “topics,” probabilistic partitions of the vocabulary that are consistent in terms of both meaning and relevance to observed sentiment. Our approach makes few assumptions about available resources, requiring neither parallel corpora nor machine translation. The rest of the paper proceeds as follows. In Section 1, we describe the probabilistic tools that we use to create consistent topics bridging across languages the model. In Section 2, we present the inference process. We discuss our set of semantic bridges between languages in Section 3, and our experiments in Section 4 demonstrate that this approach functions as an effective multilingual topic model, discovers sentiment-biased topics, and uses multilingual corpora to make better sentiment predictions across languages. Sections 5 and 6 discuss related research and discusses future work, respectively.</abstract>
<note confidence="0.87623325">45 of the 2010 Conference on Empirical Methods in Natural Language pages 45–55, Massachusetts, USA, 9-11 October 2010. Association for Computational Linguistics 1 Predictions from Multilingual Topics</note>
<abstract confidence="0.996108808743171">its name suggests, is an extension of Latent Dirichlet allocation (LDA) (Blei et al., 2003), a modeling approach that takes a corpus of unannotated documents as input and produces two outputs, a set of “topics” and assignments of documents to topics. Both the topics and the assignments are probabilistic: a topic is represented as a probability distribution over words in the corpus, and each document is assigned a probability distribution over all the topics. Topic models built on the foundations of LDA are appealing for sentiment analysis because the learned topics can cluster together sentimentbearing words, and because topic distributions are a way to represent a LDA has been used to discover latent structure in text (e.g. for discourse segmentation (Purver et al., 2006) and authorship (Rosen-Zvi et al., 2004)). extends the approach by ensuring that this latent structure — the underlying topics — is consistent across languages. We discuss multilingual topic modeling in Section 1.1, and in Section 1.2 we show how this enables supervised regression regardless of a document’s language. 1.1 Capturing Semantic Correlations Topic models posit a straightforward generative process that creates an observed corpus. For each docusome distribution unobserved topics is chosen. Then, for each word position in the doca topic selected. Finally, the word for that position is generated by selecting from the topic by (Recall that in LDA, a “topic” is a distribution over words). In monolingual topic models, the topic distribution is usually drawn from a Dirichlet distribution. Using Dirichlet distributions makes it easy to specify sparse priors, and it also simplifies posterior inference because Dirichlet distributions are conjugate to multinomial distributions. However, drawing topics from Dirichlet distributions will not suffice if our vocabulary includes multiple languages. If we are working with English, German, and Chinese at the same time, a Dirichlet prior has no way to fadistributions and latter property has also made LDA popular for information retrieval (Wei and Croft, 2006)). tend to be high at the same time, or low at the same time. More generally, the structure of our model must encourage topics to be consistent across languages, and Dirichlet distributions cannot encode correlations between elements. One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution. This has been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For concreteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge, since it is well known, offers convenient and intuitive terminology, and demonstrates the full flexibility of our approach. However, the model we describe generalizes to any tree-structured representation of multilingual knowledge; we discuss some alternatives in Section 3. WordNet organizes a vocabulary into a rooted, directed acyclic graph of nodes called synsets, short for “synonym sets.” A synset is a child of another synset if it satisfies a hyponomy relationship; each child “is a” more specific instantiation of its parent concept (thus, hyponomy is often called an “isa” relationship). For example, a “dog” is a “canine” is an “animal” is a “living thing,” etc. As an approximation, it is not unreasonable to assume that WordNet’s structure of meaning is language independent, i.e. the concept encoded by a synset can be realized using terms in different languages that share the same meaning. In practice, this organization has been used to create many alignments of international WordNets to the original English WordNet (Ordan and Wintner, 2007; Sagot and Fiˇser, 2008; Isahara et al., 2008). Using the structure of WordNet, we can now describe a generative process that produces a distribution over a multilingual vocabulary, which encourcorrelations between words with similar mean- 46 ings regardless of what language each word is in. each synset we create a multilingual word distribution for that synset as follows: Draw transition probabilities Draw stop probabilities For each language draw emission probabilities for synset For conciseness in the rest of the paper, we will refer this generative process as Dirichlet or observed token can be viewed as the end result of a of visited synsets At each node in the the path can end at node probability or it can continue to a child synset with probability If the path continues to another child synset, it child probability If the path ends at synset, it generates word probability The probability of a word being emitted from a path visited synsets final synset language therefore A = that the stop probability independent of but the emission dependent on the language. This is done to prevent the following scewhile synset highly probable in a topic and words in language 1 attached to that synset have high probability, words in language 2 have low probability. If this could happen for many synsets in a topic, an entire language would be effectively silenced, which would lead to inconsistent topics (e.g. and hyperparameters. Their mean is fixed, but their magnitude is sampled during inference (i.e. constant, but not). For the bushier bridges, (e.g. dictionary and flat), their mean is uniform. For GermaNet, we took frequencies from two balanced corpora of German and English: the British National Corpus (University of Oxford, 2006) and the Kern Corpus of the Digitales W¨orterbuch der Deutschen Sprache des 20. Jahrhunderts project (Geyken, 2007). We took these frequencies and propagated them through the multilingual hierarchy, following LDAWN’s (Boyd-Graber et al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior. The variance of the priors was initialized to be 1.0, but could be sampled during inference. that the language and word are taken as given, but the path through the semantic hierarchy is a latent random variable. Topic 1 is about baseball in English and about travel in German). Separating path from emission helps ensure that topics are consistent across languages. Having defined topic distributions in a way that can preserve cross-language correspondences, we now use this distribution within a larger model that can discover cross-language patterns of use that predict sentiment. The Model We will view sentiment analysis as a regression problem: given an input document, we want to predict real-valued observation represents the sentiment of a document. Specifically, we build on supervised latent Dirichlet allocation (SLDA, (Blei and McAuliffe, 2007)), which makes predictions based on the topics expressed in a document; this can be thought of projecting the words in a document to low dimensional space of dimension equal to the number of topics. Blei et al. showed that using this latent topic structure can offer improved predictions over regressions based on words alone, and the approach fits well with our current goals, since word-level cues are unlikely to be identical across languages. In addition to text, SLDA has been successfully applied to other domains such as social networks (Chang and Blei, 2009) and image classification (Wang et al., 2009). The key innovation in this paper is to extend SLDA by creating topics that are globally consistent across languages, using the bridging approach above. We express our model in the form of a probabilistic generative latent-variable model that generates in multiple languages a realvalued score to each document. The score comes from a normal distribution whose sum is the dot prodbetween a regression parameter encodes the influence of each topic on the observation and variance With this model in hand, we use statistical inference to determine the distribution over latent variables that, given the model, best explains observed data. The generative model is as follows: For each topic draw a topic distribution For each document M language Choose a distribution over topics 47 For each word in the document a topic assignment a path at word Equation 1 using Choose a response variable from Crucially, note that the topics are not independent of the sentiment task; the regression encourages with similar effects on the observation be in the same topic. The consistency of topics described above allows the same regression to be done for the entire corpus regardless of the language of the underlying document. 2 Inference Finding the model parameters most likely to explain the data is a problem of statistical inference. We employ stochastic EM (Diebolt and Ip, 1996), using a Gibbs sampler for the E-step to assign words to paths and topics. After randomly initializing the topics, we alternate between sampling the topic and path a word finding the regression pamaximize the likelihood. We jointly sample the topic and path conditioning on all of the other path and document assignments in the corpus, selecting a path and topic with probability η, σ, = η, Each of these three terms reflects a different influence on the topics from the vocabulary structure, the document’s topics, and the response variable. In the next paragraphs, we will expand each of them to derive the full conditional topic distribution. As discussed in Section 1.1, the structure of the topic distribution encourages terms with the same meaning to be in the same topic, even across languages. During inference, we marginalize over posmultinomial distributions and using observed transitions from topic counts in synset topic continue in synsets topic and emission in synset language topic The Multilingual Topics Text Documents Sentiment Prediction 1: Graphical model representing Shaded nodes represent observations, plates denote replication, and lines show probabilistic dependencies. of taking a path then = ! P Transition Emission (3) 3 reflects the of this model. The conditional topic distribution for SLDA (Blei and McAuliffe, 2007) replaces this term with the standard Multinomial-Dirichlet. However, we believe this is the first published SLDA-style model using MCMC inference, as prior work has used variational inference (Blei and McAuliffe, 2007; Chang and Blei, 2009; Wang et al., 2009). Because the observed response variable depends on the topic assignments of a document, the conditional topic distribution is shifted toward topics that explain the observed response. Topics that move the response the true be favored. We drop terms that are constant across all L K H α N M 6 71 P Y 48 topics for the effect of the response variable, above equation represents the of the model, which is inherited from SLDA. Finally, there is the effect of the topics already assigned to a document; the conditional distribution favors topics already assigned in a document, = = ,k+ (5) + term represents the of this model; it is present in all Gibbs sampling inference schemes for LDA (Griffiths and Steyvers, 2004). Multiplying together Equations 3, 4, and 5 allows us to sample a topic using the conditional distribution from Equation 2, based on the topic and path of the other words in all languages. After sampling the path and topic for each word in a document, we then new regression parameters maximize the likelihood conditioned on the current state of the sampler. This is simply a least squares regression the topic assignments predict Prediction on documents for which we don’t have observed equivalent to marginalizing over sampling topics for the document from Equa- 3 and 5. The prediction for then the dot of the empirical topic distribution We initially optimized all hyperparameters using slice sampling. However, we found that the regresvariance was not stable. Optimizing seems to balance between modeling the language in the documents and the prediction, and thus is sensitive to documents’ length. Given this sensitivity, we did optimize for our prediction experiments in Section 4, but instead kept it fixed at 0.25. We leave optimizing this variable, either through cross validation or adapting the model, to future work. 3 Bridges Across Languages In Section 1.1, we described connections across languages as offered by semantic networks in a general way, using WordNet as an example. In this section, we provide more specifics, as well as alternative ways of building semantic connections across languages. we can consider a degenerate mapping that is nearly equivalent to running SLDA independently across multiple languages, relating topics only based on the impact on the response variable. Consider a degenerate tree with only one node, with all words in all languages associated with that node. This is consistent with our model, but there is really no shared semantic space, as all emitted words must come from this degenerate “synset” and the model only represents the output distribution for this single node. Other words’ influence took the alignment of GermaNet to WordNet 1.6 (Kunze and Lemnitzer, 2002) and removed all synsets that were had no mapped German words. Any German synsets that did not have English translations had their words mapped to the lowest extant English hypernym (e.g. “beinbruch,” a broken leg, was mapped to “fracture”). We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970). An example of the paths for the German word “wunsch” (wish, request) is shown in Figure 2(a). dictionary can be viewed as a many many mapping, where each entry one more words in one language one or more another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b). Connections addition to handcurated connections across languages, one could also consider automatic means of mapping across languages, such as using edit distance or local context (Haghighi et al., 2008; Rapp, 1995) or using a lexical translation table obtained from parallel text (Melamed, 1998). While we experimented η, 0Nd,k0 • .r • � � � 2 exp • This word’s influence 49 (a) GermaNet (b) Dictionary wish wunsch ask request anfrag wunsch altern option option.n.02 choic cognition.n.01 event.n.01 preference.n.03 wish.n.04 event option abstraction.n.06 ereignis deed vorgang entity.n.01 entiti speech_act.n.01 request.n.02 act objekt act.n.02 handlung root dict.1 dict.2 room gelass room raum platz room zimm raum stub dict.3 Figure 2: Two methods for constructing multilingual distributions over words. On the left, paths to the German word “wunsch” in GermaNet are shown. On the right, paths to the English word “room” are shown. Both English and German words are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines). Note that different senses are denoted by different internal paths, and that internal paths are distinct from the per-language expression. with these techniques, constructing appropriate hierarchies from these resources required many arbitrary decisions about cutoffs and which words to include. Thus, we do not consider them in this paper. 4 Experiments evaluate on three criteria: how well it can discover consistent topics across languages for matching parallel documents, how well it can discover sentiment-correlated word lists from nonaligned text, and how well it can predict sentiment. 4.1 Matching on Multilingual Topics We took the 1996 documents from the Europarl corpus (Koehn, 2005) using three bridges: GermaNet, and the uninformative flat The model is unaware that the translations of documents in one language are present in the other language. Note that this does not use the supervised framework English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content words were excluded. (as there is no associated response variable for Europarl documents); this experiment is to demonstrate the effectiveness of the multilingual aspect of the model. To test whether the topics learned by the model are consistent across languages, we represent document using the probability distribution topic assignments. Each a vector of length is a language-independent representation of the document. For each document in one language, we computed the Hellinger distance between it and all of the documents in the other language and sorted the documents by decreasing distance. The translation of the document is somewhere in that set; the higher the normalized rank (the percentage of documents with a rank lower than the translation of the document), the better the underlying topic model connects languages. We compare three bridges against what is to our knowledge the only other topic model for unaligned text, Multilingual Topics for Unaligned Text (Boydand Blei, bipartite matching was initialized with the dictionary weights as specified by the Multilingual Topics for Unaligned Text algorithm. The matching size was limited to 250 and the bipartite matching was only updated on the initial iteration then held fixed. This yielded results comparable to when the matching 50 0.8 0.0 -0.8 god himmel diet essen book buch us gedanken food diŠt books immer religion glaube eat verlieren one leben church unsere weight befinden life art human kirche eating kšrper person lesen wahrheithealthy rezepte people thema autor fat -1.2 -0.4 0.4 1.2 buch roman leser seiten geschichte handlung great good business all one companies right gesellschaft genau Gberzeugt ergebnis mittel verlangen film filme episode star story gibt book novel reader pages books tale kind kinder eltern baby nacht movie film episode movies scenes separate gives children baby child parents sleep (harry) harry (volume) band (sky) himmel (universe) all (vampire) vampir (last) letzt (part) teil M31 (god) gott (lord) herr (religion) religion (universe) all (world) welt (science) wissenschaft (medicine) medizin (society) gesellschaft (religion) 0.8 (book) buch (itself) sich (that) dass (much) viel (no) kein (good) gut (when) wenn afraid that...]) isn&apos;t]) (female) weiblich (equal) gleich (woman) frau (fast) schnell (point) punkt (man) mann (soon) bald little) (good) gut (sentence) satz (two) zwei (story) story (treasure) schatz (attractive) attraktiv (elegant) elegant (gem) juwel (a) German / English (b) German /Chinese 4: Topics, along with associated regression coefficient a learned 25-topic model on German-English (left) and German-Chinese (right) documents. Notice that theme-related topics have regression parameter near zero, topics discussing the number of pages have negative regression parameters, topics with “good,” “great,” “hˇao” (good) and “¨uberzeugt” (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of “gut” (good) in one of the negative sentiment topics, showing the difficulty of learning collocations. Train Test GermaNet Dictionary Flat DE DE 73.8 24.8 92.2 EN DE 7.44 2.68 18.3 EN + DE DE 1.17 1.46 1.39 Table 1: Mean squared error on a film review corpus. All results are on the same German test data, varying the training data. Over-fitting prevents the model learning on the German data alone; adding English data to the mix allows the model to make better predictions. 5000 film reviews (Pang and Lee, 2005) to create a film review The results for predicting sentiment in German documents with 25 topics are presented in Table 1. On a small monolingual corpus, prediction is very poor. The model over-fits, especially when it has the entire vocabulary to select from. The slightly better performance using GermaNet and a dictionary as topic priors can be viewed as basic feature selection, removing proper names from the vocabulary to followed Pang and Lee’s method for creating a numerical score between 0 and 1 from a star rating. We then converted that to an integer by multiplying by 100; this was done because initial data preprocessing assumed integer values (although downstream processing did not assume integer values). The German movie review corpus available at static/downloads_and_media.html prevent over-fitting. One would expect that prediction improves with a larger training set. For this model, such an improveis seen even when the training set includes documents in the target language. Note that even the degenerate flat bridge across languages provides useful information. After introducing English data, the model learns to prefer smaller regression parameters (this can be seen as a form of regularization). Performance is best when a reasonably large corpus is available including some data in the target language. For each bridge, performance improves showing that is successfully able to incorporate information learned from both languages to build a single, coherent picture of how sentiment is expressed in both languages. With the GermaNet bridge, performance is better than both the degenerate and dictionary based bridges, showing that the model is sharing information both through the multilingual topics and the regression parameters. Performance on English prediction is comparable to previously published results on this dataset (Blei and McAuliffe, 2007); with enough data, a monolingual model is no longer helped by adding additional multilingual data. 52 5 Relationship to Previous Research advantages of reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the of another language, views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws away information important to sentiment analysis like syntactic constructions (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating. Second, a single real number is not always sufficient to capture the nuances of sentiment. Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predictions. Other probabilistic models for sentiment classification view sentiment as a word level feature. Some models use sentiment word lists, either given or learned from a corpus, as a prior to seed topics so that they attract other sentiment bearing words (Mei et al., 2007; Lin and He, 2009). Other approaches view sentiment or perspective as a perturbation of a log-linear topic model (Lin et al., 2008). Such techniques could be combined with the multilingual approach presented here by using distributions over words that not only bridge different languages but also encode additional information. For example, the vocabulary hierarchies could be structured to encourage topics that encourage correlation among similar sentiment-bearing words (e.g. clustering words associated with price, size, etc.). Future work could also more rigorously validate that the multilingual topics by are sentiment-bearing via human judgments. contrast, draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008). Extending these models to multilingual data would be more straightforward. 6 Conclusions is a “holistic” statistical model for multilingual corpora that does not require parallel text or expensive multilingual resources. It discovers connections across languages that can recover latent structure in parallel corpora, discover sentimentcorrelated word lists in multiple languages, and make accurate predictions across languages that improve with more multilingual data, as demonstrated in the context of sentiment analysis. generally, provides a formalism that can be used to incorporate the many insights of topic modeling-driven sentiment analysis to multilingual corpora by tying together word distributions languages. can also contribute to the development of word list-based sentiment systhe topics discovered by can serve as a first-pass means of sentiment-based word lists for languages that might lack annotated resources. also can be viewed as a sentimentinformed multilingual word sense disambiguation (WSD) algorithm. When the multilingual bridge is an explicit representation of sense such as WordNet, part 53 of the generative process is an explicit assignment every word to sense (the path latent variable this is discovered during inference. The dictionarybased technique may be viewed as a disambiguation via a transfer dictionary. How sentiment prediction impacts the implicit WSD is left to future work. Better capturing local syntax and meaningful collocations would also improve the model’s ability to predict sentiment and model multilingual topics, as would providing a better mechanism for representing words not included in our bridges. We intend to develop such models as future work. 7 Acknowledgments This research was funded in part by the Army Research Laboratory through ARL Cooperative Agreement W911NF-09-2-0072 and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the Army Research Laboratory. All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of ARL, IARPA, the ODNI, or the U.S. Government. The authors thank the anonymous reviewers, Jonathan Chang, Christiane Fellbaum, and Lawrence Watts for helpful comments. The authors especially thank Chris Potts for providing help in obtaining and processing reviews.</abstract>
<note confidence="0.5285113125">References David Andrzejewski, Xiaojin Zhu, and Mark Craven. 2009. Incorporating domain knowledge into topic modvia Dirichlet forest priors. In Andrea Esuli Stefano Baccianella and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource sentiment analysis and opinion mining. In Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan. 2008. Multilingual subjectivity analysis using translation. In David M. Blei and John D. Lafferty. 2005. Correlated models. In David M. Blei and Jon D. McAuliffe. 2007. Supervised models. In MIT Press. David M. Blei, Andrew Ng, and Michael Jordan. 2003. Dirichlet allocation. 3:993–1022. Boyd-Graber and David M. Blei. 2009. Multilintopic models for unaligned text. In Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. Jonathan Chang and David M. Blei. 2009. Relational models for document networks. In Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in grammar induction. In Noah Constant, Christopher Davis, Christopher Potts, and Florian Schwarz. 2009. The pragmatics of expressive Evidence from large corpora. und 33(1–2). Kerstin Denecke. 2008. Using SentiWordNet for multilinsentiment analysis. In Paul Denisowski. 1997.</note>
<web confidence="0.835781">http://www.mdbg.net/chindict/.</web>
<note confidence="0.868574588235294">Diebolt and Eddie H.S. Ip, 1996. Chain Carlo in chapter Stochastic EM: method and application. Chapman and Hall, London. Alexander Geyken. 2007. The DWDS corpus: A reference corpus for the German language of the 20th In and Collocations: Corpus-based Lexicographic Continuum Press. Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Thomas L. Griffiths and Mark Steyvers. 2004. Finding topics. 101(Suppl 1):5228–5235. Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from corpora. In Columbus, Ohio. Jan Hefti. 2005. HanDeDict. http://chdw.de. Hitoshi Isahara, Fransis Bond, Kiyotaka Uchimoto, Masao Utiyama, and Kyoko Kanzaki. 2008. Development of</note>
<author confidence="0.55529">topic models PCFGs</author>
<author confidence="0.55529">adaptor</author>
<abstract confidence="0.9790116">grammars and learning topical collocations and the of proper names. In Woosung Kim and Sanjeev Khudanpur. 2004. Lexical triggers and latent semantic analysis for cross-lingual model adaptation. 3(2):94–112. Philipp Koehn. 2005. Europarl: A parallel corpus statistical machine translation. In http://www.statmt.org/europarl/. Claudia Kunze and Lothar Lemnitzer. 2002. Standardizing WordNets in a web-compliant format: The case of In on Wordnets Structures and Chenghua Lin and Yulan He. 2009. Joint sentiment/topic for sentiment analysis. In Wei-Hao Lin, Eric Xing, and Alexander Hauptmann. 2008. A joint topic and perspective model for ideodiscourse. In 54 Edward Loper and Steven Bird. 2002. NLTK: the natulanguage toolkit. In and methodologies for ACL.</abstract>
<author confidence="0.8238225">Structured models for</author>
<abstract confidence="0.8307337">sentiment analysis. In Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: facets and opinions in weblogs. In Dan Melamed. 1998. methods for exploitparallel Ph.D. thesis, University of Pennsylvania. George A. Miller. 1990. Nouns in WordNet: A lexical system. Journal of Lexicog- 3(4):245–264.</abstract>
<note confidence="0.651539777777778">David Mimno, Hanna Wallach, Jason Naradowsky, David Smith, and Andrew McCallum. 2009. Polylingual models. In Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2009. Mining multilingual topics from Wikipedia. In Noam Ordan and Shuly Wintner. 2007. Hebrew Word- Net: a test case of aligning lexical databases across lan- Journal of 19(1):39– 58. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with to rating scales. In Pang and Lillian Lee. 2008. Mining and Now Publishers Inc. Martin Porter and Richard Boulton. 1970. Snowball stemmer. http://snowball.tartarus.org/credits.php. Matthew Purver, Konrad K¨ording, Thomas L. Griffiths, and Joshua Tenenbaum. 2006. Unsupervised topic for multi-party spoken discourse. In Reinhard Rapp. 1995. Identifying word translations in texts. In pages 320–322. Philip Resnik. 1995. Using information content to evalusemantic similarity in a taxonomy. In pages 448–453. Frank Richter. 2008. Dictionary nice grep. http://wwwuser.tu-chemnitz.de/ fri/ding/. Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.</note>
<title confidence="0.6805015">Learning subjective nouns using extraction pattern boot- In</title>
<author confidence="0.972143333333333">The author-topic model for</author>
<author confidence="0.972143333333333">documents In</author>
<note confidence="0.72558975">Benoit Sagot and Darja Fiˇser. 2008. Building a Free WordNet from Multilingual Resources. In On- Ivan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In pages 308–316. Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional field word segmenter. In Workshop Chinese Language of Oxford. 2006. British National Corpus. http://www.natcorp.ox.ac.uk/. Tobias Vetter, Manfred Sauer, and Philipp Wallutat. 2000. Filmrezension.de: Online-magazin f¨ur filmkritik. Xiaojun Wan. 2009. Co-training for cross-lingual senticlassification. In Wang, David Blei, and Li Fei-Fei. 2009. Simultaimage classification and annotation. In Xing Wei and Bruce Croft. 2006. LDA-based document for ad-hoc retrieval. In Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal groups for sentiment analysis. Ann Wilson. 2008. Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, Attitudes of Private Ph.D. thesis, University of Pittsburgh. Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual admixture models for word alignment. In Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. Medlda: maximum margin supervised topic models for regresand classification. In 55</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
<author>Mark Craven</author>
</authors>
<title>Incorporating domain knowledge into topic modeling via Dirichlet forest priors.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="6973" citStr="Andrzejewski et al., 2009" startWordPosition="1045" endWordPosition="1049">utions cannot encode correlations between elements. One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution. This has been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For concreteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge, since it is well known, offers convenient and intuitive terminology, and demonstrates the full flexibility of our approach. However, the model we describe generalizes to any tree-structured representation of multilingual knowledge; we discuss some alternatives in Section 3. WordNet organizes a voca</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, 2009</marker>
<rawString>David Andrzejewski, Xiaojin Zhu, and Mark Craven. 2009. Incorporating domain knowledge into topic modeling via Dirichlet forest priors. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli Stefano Baccianella</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="1620" citStr="Baccianella and Sebastiani, 2010" startWordPosition="225" endWordPosition="229"> inference from corpora. We show MLSLDA can build topics that are consistent across languages, discover sensible bilingual lexical correspondences, and leverage multilingual corpora to better predict sentiment. Sentiment analysis (Pang and Lee, 2008) offers the promise of automatically discerning how people feel about a product, person, organization, or issue based on what they write online, which is potentially of great value to businesses and other organizations. However, the vast majority of sentiment resources and algorithms are limited to a single language, usually English (Wilson, 2008; Baccianella and Sebastiani, 2010). Since no single language captures a majority of the content online, adopting such a limited approach in an increasingly global community risks missing important details and trends that might only be available when text in multiple languages is taken into account. Philip Resnik Department of Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu Up to this point, multiple languages have been addressed in sentiment analysis primarily by transferring knowledge from a resource-rich language to a less rich language (Banea et al., 2008), or by ignoring differences in languag</context>
</contexts>
<marker>Baccianella, Sebastiani, 2010</marker>
<rawString>Andrea Esuli Stefano Baccianella and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
<author>Samer Hassan</author>
</authors>
<title>Multilingual subjectivity analysis using machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2181" citStr="Banea et al., 2008" startWordPosition="312" endWordPosition="315">y English (Wilson, 2008; Baccianella and Sebastiani, 2010). Since no single language captures a majority of the content online, adopting such a limited approach in an increasingly global community risks missing important details and trends that might only be available when text in multiple languages is taken into account. Philip Resnik Department of Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu Up to this point, multiple languages have been addressed in sentiment analysis primarily by transferring knowledge from a resource-rich language to a less rich language (Banea et al., 2008), or by ignoring differences in languages via translation into English (Denecke, 2008). These approaches are limited to a view of sentiment that takes place through an English-centric lens, and they ignore the potential to share information between languages. Ideally, learning sentiment cues holistically, across languages, would result in a richer and more globally consistent picture. In this paper, we introduce Multilingual Supervised Latent Dirichlet Allocation (MLSLDA), a model for sentiment analysis on a multilingual corpus. MLSLDA discovers a consistent, unified picture of sentiment acros</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, Hassan, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan. 2008. Multilingual subjectivity analysis using machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2005</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6552" citStr="Blei and Lafferty, 2005" startWordPosition="985" endWordPosition="988">, German, and Chinese at the same time, a Dirichlet prior has no way to favor distributions z such thatp(goodlz), p(gutlz), and 1The latter property has also made LDA popular for information retrieval (Wei and Croft, 2006)). p(hˇao|z) all tend to be high at the same time, or low at the same time. More generally, the structure of our model must encourage topics to be consistent across languages, and Dirichlet distributions cannot encode correlations between elements. One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution. This has been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For con</context>
</contexts>
<marker>Blei, Lafferty, 2005</marker>
<rawString>David M. Blei and John D. Lafferty. 2005. Correlated topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Jon D McAuliffe</author>
</authors>
<title>Supervised topic models. In NIPS.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11675" citStr="Blei and McAuliffe, 2007" startWordPosition="1822" endWordPosition="1825"> about travel in German). Separating path from emission helps ensure that topics are consistent across languages. Having defined topic distributions in a way that can preserve cross-language correspondences, we now use this distribution within a larger model that can discover cross-language patterns of use that predict sentiment. 1.2 The MLSLDA Model We will view sentiment analysis as a regression problem: given an input document, we want to predict a real-valued observation y that represents the sentiment of a document. Specifically, we build on supervised latent Dirichlet allocation (SLDA, (Blei and McAuliffe, 2007)), which makes predictions based on the topics expressed in a document; this can be thought of projecting the words in a document to low dimensional space of dimension equal to the number of topics. Blei et al. showed that using this latent topic structure can offer improved predictions over regressions based on words alone, and the approach fits well with our current goals, since word-level cues are unlikely to be identical across languages. In addition to text, SLDA has been successfully applied to other domains such as social networks (Chang and Blei, 2009) and image classification (Wang et</context>
<context position="15747" citStr="Blei and McAuliffe, 2007" startWordPosition="2525" endWordPosition="2528">opic k, Ok,i,1; and emission counts in synset i in language l in topic k, Fk,i,l. The Multilingual Topics Text Documents Sentiment Prediction Figure 1: Graphical model representing MLSLDA. Shaded nodes represent observations, plates denote replication, and lines show probabilistic dependencies. probability of taking a path r is then p(λn = r|zn = k,X−n) = ! Ok,i,1 + ωi P s∈0,1 Ok,i,s + ωi,s |{z } Transition Ok,rend,0 + ωrend Fk,rend,wn + πrend,l Ps∈0,1 Ok,rend,s + ωrend,s |{z } Emission (3) Equation 3 reflects the multilingual aspect of this model. The conditional topic distribution for SLDA (Blei and McAuliffe, 2007) replaces this term with the standard Multinomial-Dirichlet. However, we believe this is the first published SLDA-style model using MCMC inference, as prior work has used variational inference (Blei and McAuliffe, 2007; Chang and Blei, 2009; Wang et al., 2009). Because the observed response variable depends on the topic assignments of a document, the conditional topic distribution is shifted toward topics that explain the observed response. Topics that move the predicted response yd toward the true yd will be favored. We drop terms that are constant across all 7ch,l (Pi,h,l �h Th pi,h (0i,h L </context>
<context position="28405" citStr="Blei and McAuliffe, 2007" startWordPosition="4555" endWordPosition="4558">vailable including some data in the target language. For each bridge, performance improves dramatically, showing that MLSLDA is successfully able to incorporate information learned from both languages to build a single, coherent picture of how sentiment is expressed in both languages. With the GermaNet bridge, performance is better than both the degenerate and dictionary based bridges, showing that the model is sharing information both through the multilingual topics and the regression parameters. Performance on English prediction is comparable to previously published results on this dataset (Blei and McAuliffe, 2007); with enough data, a monolingual model is no longer helped by adding additional multilingual data. 52 5 Relationship to Previous Research The advantages of MLSLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al.,</context>
<context position="30586" citStr="Blei and McAuliffe, 2007" startWordPosition="4901" endWordPosition="4904">o sentiment analysis like syntactic constructions (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating. Second, a single real number is not always sufficient to capture the nuances of sentiment. Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predictions. Other probabilistic models for sentiment classification view sentiment as a word level feature. Some models use sentiment word lists, either given or learned from a corpus, as a prior to seed topics so that they attract other sentiment bearing words (Mei et al., 2007; Lin and He, 2009). Other approaches view sentiment or perspective as a perturbation of a log-linear topic model (Lin et al., 2008). Such techniques could be combined with the multilingual approach presented here by using distri</context>
<context position="31833" citStr="Blei and McAuliffe, 2007" startWordPosition="5089" endWordPosition="5092">t not only bridge different languages but also encode additional information. For example, the vocabulary hierarchies could be structured to encourage topics that encourage correlation among similar sentiment-bearing words (e.g. clustering words associated with price, size, etc.). Future work could also more rigorously validate that the multilingual topics discovered by MLSLDA are sentiment-bearing via human judgments. In contrast, MLSLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008). Extending these models to multilingual data would be more straightforward. 6 Conclusions MLSLDA is a “holistic” statistical model for multilingual corpora that does not require parallel text or expensive multilingual resources. It discovers connections across languages that can recover latent structure in parallel corpora, discover sentimentcorrelated word lists in multiple languages, and make accurate predictions across languages that improve with more multilingual data, as demonstrated in the context of sentiment analysis. </context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>David M. Blei and Jon D. McAuliffe. 2007. Supervised topic models. In NIPS. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent Dirichlet allocation. JMLR,</booktitle>
<pages>3--993</pages>
<contexts>
<context position="4027" citStr="Blei et al., 2003" startWordPosition="588" endWordPosition="591">strate that this approach functions as an effective multilingual topic model, discovers sentiment-biased topics, and uses multilingual corpora to make better sentiment predictions across languages. Sections 5 and 6 discuss related research and discusses future work, respectively. 45 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 45–55, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics 1 Predictions from Multilingual Topics As its name suggests, MLSLDA is an extension of Latent Dirichlet allocation (LDA) (Blei et al., 2003), a modeling approach that takes a corpus of unannotated documents as input and produces two outputs, a set of “topics” and assignments of documents to topics. Both the topics and the assignments are probabilistic: a topic is represented as a probability distribution over words in the corpus, and each document is assigned a probability distribution over all the topics. Topic models built on the foundations of LDA are appealing for sentiment analysis because the learned topics can cluster together sentimentbearing words, and because topic distributions are a parsimonious way to represent a docu</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. JMLR, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Multilingual topic models for unaligned text.</title>
<date>2009</date>
<booktitle>In UAI.</booktitle>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>Jordan Boyd-Graber and David M. Blei. 2009. Multilingual topic models for unaligned text. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6908" citStr="Boyd-Graber et al., 2007" startWordPosition="1036" endWordPosition="1039"> topics to be consistent across languages, and Dirichlet distributions cannot encode correlations between elements. One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution. This has been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For concreteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge, since it is well known, offers convenient and intuitive terminology, and demonstrates the full flexibility of our approach. However, the model we describe generalizes to any tree-structured representation of multilingual knowledge; we</context>
<context position="10719" citStr="Boyd-Graber et al., 2007" startWordPosition="1671" endWordPosition="1674">riables Th, irh,f, and r1h are hyperparameters. Their mean is fixed, but their magnitude is sampled during inference (i.e. Th,i is constant, but Th,i is not). For the bushier bridges, -k Th,k (e.g. dictionary and flat), their mean is uniform. For GermaNet, we took frequencies from two balanced corpora of German and English: the British National Corpus (University of Oxford, 2006) and the Kern Corpus of the Digitales W¨orterbuch der Deutschen Sprache des 20. Jahrhunderts project (Geyken, 2007). We took these frequencies and propagated them through the multilingual hierarchy, following LDAWN’s (Boyd-Graber et al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior. The variance of the priors was initialized to be 1.0, but could be sampled during inference. 3Note that the language and word are taken as given, but the path through the semantic hierarchy is a latent random variable. Topic 1 is about baseball in English and about travel in German). Separating path from emission helps ensure that topics are consistent across languages. Having defined topic distributions in a way that can preserve cross-language correspondences, we now use this distribution within a larger model that can d</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>David M Blei</author>
</authors>
<title>Relational topic models for document networks.</title>
<date>2009</date>
<booktitle>In AISTATS.</booktitle>
<contexts>
<context position="12241" citStr="Chang and Blei, 2009" startWordPosition="1916" endWordPosition="1919"> Dirichlet allocation (SLDA, (Blei and McAuliffe, 2007)), which makes predictions based on the topics expressed in a document; this can be thought of projecting the words in a document to low dimensional space of dimension equal to the number of topics. Blei et al. showed that using this latent topic structure can offer improved predictions over regressions based on words alone, and the approach fits well with our current goals, since word-level cues are unlikely to be identical across languages. In addition to text, SLDA has been successfully applied to other domains such as social networks (Chang and Blei, 2009) and image classification (Wang et al., 2009). The key innovation in this paper is to extend SLDA by creating topics that are globally consistent across languages, using the bridging approach above. We express our model in the form of a probabilistic generative latent-variable model that generates documents in multiple languages and assigns a realvalued score to each document. The score comes from a normal distribution whose sum is the dot product between a regression parameter q that encodes the influence of each topic on the observation and a variance a2. With this model in hand, we use stat</context>
<context position="15987" citStr="Chang and Blei, 2009" startWordPosition="2560" endWordPosition="2563">cation, and lines show probabilistic dependencies. probability of taking a path r is then p(λn = r|zn = k,X−n) = ! Ok,i,1 + ωi P s∈0,1 Ok,i,s + ωi,s |{z } Transition Ok,rend,0 + ωrend Fk,rend,wn + πrend,l Ps∈0,1 Ok,rend,s + ωrend,s |{z } Emission (3) Equation 3 reflects the multilingual aspect of this model. The conditional topic distribution for SLDA (Blei and McAuliffe, 2007) replaces this term with the standard Multinomial-Dirichlet. However, we believe this is the first published SLDA-style model using MCMC inference, as prior work has used variational inference (Blei and McAuliffe, 2007; Chang and Blei, 2009; Wang et al., 2009). Because the observed response variable depends on the topic assignments of a document, the conditional topic distribution is shifted toward topics that explain the observed response. Topics that move the predicted response yd toward the true yd will be favored. We drop terms that are constant across all 7ch,l (Pi,h,l �h Th pi,h (0i,h L K H wd,n Xd,n zd,n 0d α N yd M 6 71 Bk,i,j + τi,j P j/ Bk,i,j&apos; + τi,j Y (i,j)∈r Pw/ Frend,w/ + πrend,w/ . 48 topics for the effect of the response variable, The above equation represents the supervised aspect of the model, which is inherite</context>
</contexts>
<marker>Chang, Blei, 2009</marker>
<rawString>Jonathan Chang and David M. Blei. 2009. Relational topic models for document networks. In AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="6674" citStr="Cohen and Smith, 2009" startWordPosition="1003" endWordPosition="1006">and 1The latter property has also made LDA popular for information retrieval (Wei and Croft, 2006)). p(hˇao|z) all tend to be high at the same time, or low at the same time. More generally, the structure of our model must encourage topics to be consistent across languages, and Dirichlet distributions cannot encode correlations between elements. One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution. This has been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For concreteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge, </context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Constant</author>
<author>Christopher Davis</author>
<author>Christopher Potts</author>
<author>Florian Schwarz</author>
</authors>
<title>The pragmatics of expressive content: Evidence from large corpora. Sprache und Datenverarbeitung,</title>
<date>2009</date>
<pages>33--1</pages>
<marker>Constant, Davis, Potts, Schwarz, 2009</marker>
<rawString>Noah Constant, Christopher Davis, Christopher Potts, and Florian Schwarz. 2009. The pragmatics of expressive content: Evidence from large corpora. Sprache und Datenverarbeitung, 33(1–2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Denecke</author>
</authors>
<title>Using SentiWordNet for multilingual sentiment analysis.</title>
<date>2008</date>
<booktitle>In ICDEW</booktitle>
<contexts>
<context position="2267" citStr="Denecke, 2008" startWordPosition="327" endWordPosition="328">es a majority of the content online, adopting such a limited approach in an increasingly global community risks missing important details and trends that might only be available when text in multiple languages is taken into account. Philip Resnik Department of Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu Up to this point, multiple languages have been addressed in sentiment analysis primarily by transferring knowledge from a resource-rich language to a less rich language (Banea et al., 2008), or by ignoring differences in languages via translation into English (Denecke, 2008). These approaches are limited to a view of sentiment that takes place through an English-centric lens, and they ignore the potential to share information between languages. Ideally, learning sentiment cues holistically, across languages, would result in a richer and more globally consistent picture. In this paper, we introduce Multilingual Supervised Latent Dirichlet Allocation (MLSLDA), a model for sentiment analysis on a multilingual corpus. MLSLDA discovers a consistent, unified picture of sentiment across multiple languages by learning “topics,” probabilistic partitions of the vocabulary </context>
<context position="29298" citStr="Denecke, 2008" startWordPosition="4696" endWordPosition="4697">ormally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, MLSLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of senti</context>
</contexts>
<marker>Denecke, 2008</marker>
<rawString>Kerstin Denecke. 2008. Using SentiWordNet for multilingual sentiment analysis. In ICDEW 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Denisowski</author>
</authors>
<date>1997</date>
<note>CEDICT. http://www.mdbg.net/chindict/.</note>
<contexts>
<context position="19808" citStr="Denisowski, 1997" startWordPosition="3212" endWordPosition="3213"> translations had their words mapped to the lowest extant English hypernym (e.g. “beinbruch,” a broken leg, was mapped to “fracture”). We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970). An example of the paths for the German word “wunsch” (wish, request) is shown in Figure 2(a). Dictionaries A dictionary can be viewed as a many to many mapping, where each entry ei maps one or more words in one language si to one or more words ti in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b). Algorithmic Connections In addition to handcurated connections across languages, one could also consider automatic means of mapping across languages, such as using edit distance or local context (Haghighi et al., 2008; Rapp, 1995) or using a lexical translation table obtained from parallel text (Melamed, 1998). While we experimented p(yd|z, η, σ) a � exp 1 \ Ek0 Nd,k0ηk0 ηzk 2 yd − σ Ek, Nd k0 / E 0Nd,k0 k</context>
</contexts>
<marker>Denisowski, 1997</marker>
<rawString>Paul Denisowski. 1997. CEDICT. http://www.mdbg.net/chindict/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Diebolt</author>
<author>Eddie H S Ip</author>
</authors>
<title>Markov Chain Monte Carlo in Practice, chapter Stochastic EM: method and application.</title>
<date>1996</date>
<publisher>Chapman and Hall,</publisher>
<location>London.</location>
<contexts>
<context position="13955" citStr="Diebolt and Ip, 1996" startWordPosition="2217" endWordPosition="2220">ding to Equation 1 using {,Qzd�n,Wzd�n, Ozd�n}. 3. Choose a response variable from y u Norm (ηT�z, σ2), 1 EN where zd — N n=1 zd,n. Crucially, note that the topics are not independent of the sentiment task; the regression encourages terms with similar effects on the observation y to be in the same topic. The consistency of topics described above allows the same regression to be done for the entire corpus regardless of the language of the underlying document. 2 Inference Finding the model parameters most likely to explain the data is a problem of statistical inference. We employ stochastic EM (Diebolt and Ip, 1996), using a Gibbs sampler for the E-step to assign words to paths and topics. After randomly initializing the topics, we alternate between sampling the topic and path of a word (zd,n, λd,n) and finding the regression parameters η that maximize the likelihood. We jointly sample the topic and path conditioning on all of the other path and document assignments in the corpus, selecting a path and topic with probability p(zn = k, λn = r|z−n, X−n, wn, η, σ, 6)) = p(yd|z, η, σ)p(λn = r|zn = k, X−n, wn, r, K, 7) p(zn = k|z−n, α). (2) Each of these three terms reflects a different influence on the topics</context>
</contexts>
<marker>Diebolt, Ip, 1996</marker>
<rawString>Jean Diebolt and Eddie H.S. Ip, 1996. Markov Chain Monte Carlo in Practice, chapter Stochastic EM: method and application. Chapman and Hall, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Geyken</author>
</authors>
<title>The DWDS corpus: A reference corpus for the German language of the 20th century. In Idioms and Collocations: Corpus-based Linguistic, Lexicographic Studies.</title>
<date>2007</date>
<publisher>Continuum Press.</publisher>
<contexts>
<context position="10591" citStr="Geyken, 2007" startWordPosition="1656" endWordPosition="1657">sets in a topic, an entire language would be effectively silenced, which would lead to inconsistent topics (e.g. 2Variables Th, irh,f, and r1h are hyperparameters. Their mean is fixed, but their magnitude is sampled during inference (i.e. Th,i is constant, but Th,i is not). For the bushier bridges, -k Th,k (e.g. dictionary and flat), their mean is uniform. For GermaNet, we took frequencies from two balanced corpora of German and English: the British National Corpus (University of Oxford, 2006) and the Kern Corpus of the Digitales W¨orterbuch der Deutschen Sprache des 20. Jahrhunderts project (Geyken, 2007). We took these frequencies and propagated them through the multilingual hierarchy, following LDAWN’s (Boyd-Graber et al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior. The variance of the priors was initialized to be 1.0, but could be sampled during inference. 3Note that the language and word are taken as given, but the path through the semantic hierarchy is a latent random variable. Topic 1 is about baseball in English and about travel in German). Separating path from emission helps ensure that topics are consistent across languages. Having defined topic distri</context>
</contexts>
<marker>Geyken, 2007</marker>
<rawString>Alexander Geyken. 2007. The DWDS corpus: A reference corpus for the German language of the 20th century. In Idioms and Collocations: Corpus-based Linguistic, Lexicographic Studies. Continuum Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="30036" citStr="Greene and Resnik, 2009" startWordPosition="4816" endWordPosition="4819">onary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, MLSLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws away information important to sentiment analysis like syntactic constructions (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating. Second, a single real number is not always sufficient to capture the nuances of sentiment. Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which wou</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<journal>PNAS,</journal>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="16946" citStr="Griffiths and Steyvers, 2004" startWordPosition="2733" endWordPosition="2736">,l (Pi,h,l �h Th pi,h (0i,h L K H wd,n Xd,n zd,n 0d α N yd M 6 71 Bk,i,j + τi,j P j/ Bk,i,j&apos; + τi,j Y (i,j)∈r Pw/ Frend,w/ + πrend,w/ . 48 topics for the effect of the response variable, The above equation represents the supervised aspect of the model, which is inherited from SLDA. Finally, there is the effect of the topics already assigned to a document; the conditional distribution favors topics already assigned in a document, p(zn = k|z−n, α) = Td ,k + αk (5) &amp;0 Td,k0 + αk0 This term represents the document focus of this model; it is present in all Gibbs sampling inference schemes for LDA (Griffiths and Steyvers, 2004). Multiplying together Equations 3, 4, and 5 allows us to sample a topic using the conditional distribution from Equation 2, based on the topic and path of the other words in all languages. After sampling the path and topic for each word in a document, we then find new regression parameters η that maximize the likelihood conditioned on the current state of the sampler. This is simply a least squares regression using the topic assignments zd to predict yd. Prediction on documents for which we don’t have an observed yd is equivalent to marginalizing over yd and sampling topics for the document f</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. PNAS, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="20216" citStr="Haghighi et al., 2008" startWordPosition="3275" endWordPosition="3278"> entry ei maps one or more words in one language si to one or more words ti in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b). Algorithmic Connections In addition to handcurated connections across languages, one could also consider automatic means of mapping across languages, such as using edit distance or local context (Haghighi et al., 2008; Rapp, 1995) or using a lexical translation table obtained from parallel text (Melamed, 1998). While we experimented p(yd|z, η, σ) a � exp 1 \ Ek0 Nd,k0ηk0 ηzk 2 yd − σ Ek, Nd k0 / E 0Nd,k0 k • .r • � � � 2 . (4) exp −η zk 2σ2 Ek0 N2d,k0 • .r • This word’s influence 49 (a) GermaNet (b) Dictionary wish wunsch ask request anfrag wunsch altern option option.n.02 choic cognition.n.01 event.n.01 preference.n.03 wish.n.04 event option abstraction.n.06 ereignis deed vorgang entity.n.01 entiti speech_act.n.01 request.n.02 act objekt act.n.02 handlung root dict.1 dict.2 room gelass room raum platz roo</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In ACL, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hefti</author>
</authors>
<title>HanDeDict. http://chdw.de. Hitoshi Isahara, Fransis Bond, Kiyotaka Uchimoto, Masao Utiyama, and Kyoko Kanzaki.</title>
<date>2005</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="19855" citStr="Hefti, 2005" startWordPosition="3218" endWordPosition="3219">xtant English hypernym (e.g. “beinbruch,” a broken leg, was mapped to “fracture”). We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970). An example of the paths for the German word “wunsch” (wish, request) is shown in Figure 2(a). Dictionaries A dictionary can be viewed as a many to many mapping, where each entry ei maps one or more words in one language si to one or more words ti in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b). Algorithmic Connections In addition to handcurated connections across languages, one could also consider automatic means of mapping across languages, such as using edit distance or local context (Haghighi et al., 2008; Rapp, 1995) or using a lexical translation table obtained from parallel text (Melamed, 1998). While we experimented p(yd|z, η, σ) a � exp 1 \ Ek0 Nd,k0ηk0 ηzk 2 yd − σ Ek, Nd k0 / E 0Nd,k0 k • .r • � � � 2 . (4) exp −η zk 2σ2 Ek0 N2d,k0 </context>
</contexts>
<marker>Hefti, 2005</marker>
<rawString>Jan Hefti. 2005. HanDeDict. http://chdw.de. Hitoshi Isahara, Fransis Bond, Kiyotaka Uchimoto, Masao Utiyama, and Kyoko Kanzaki. 2008. Development of the Japanese WordNet. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<marker>Johnson, 2010</marker>
<rawString>Mark Johnson. 2010. PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Woosung Kim</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Lexical triggers and latent semantic analysis for cross-lingual language model adaptation.</title>
<date>2004</date>
<journal>TALIP,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="29070" citStr="Kim and Khudanpur, 2004" startWordPosition="4665" endWordPosition="4668"> no longer helped by adding additional multilingual data. 52 5 Relationship to Previous Research The advantages of MLSLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, MLSLDA views all languages through</context>
</contexts>
<marker>Kim, Khudanpur, 2004</marker>
<rawString>Woosung Kim and Sanjeev Khudanpur. 2004. Lexical triggers and latent semantic analysis for cross-lingual language model adaptation. TALIP, 3(2):94–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit. http://www.statmt.org/europarl/.</booktitle>
<contexts>
<context position="21883" citStr="Koehn, 2005" startWordPosition="3547" endWordPosition="3548">re distinct from the per-language expression. with these techniques, constructing appropriate hierarchies from these resources required many arbitrary decisions about cutoffs and which words to include. Thus, we do not consider them in this paper. 4 Experiments We evaluate MLSLDA on three criteria: how well it can discover consistent topics across languages for matching parallel documents, how well it can discover sentiment-correlated word lists from nonaligned text, and how well it can predict sentiment. 4.1 Matching on Multilingual Topics We took the 1996 documents from the Europarl corpus (Koehn, 2005) using three bridges: GermaNet, dictionary, and the uninformative flat matching.4 The model is unaware that the translations of documents in one language are present in the other language. Note that this does not use the supervised framework 4For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content wo</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit. http://www.statmt.org/europarl/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Kunze</author>
<author>Lothar Lemnitzer</author>
</authors>
<title>Standardizing WordNets in a web-compliant format: The case of GermaNet.</title>
<date>2002</date>
<booktitle>In Workshop on Wordnets Structures and Standardisation.</booktitle>
<contexts>
<context position="19084" citStr="Kunze and Lemnitzer, 2002" startWordPosition="3088" endWordPosition="3091"> First, we can consider a degenerate mapping that is nearly equivalent to running SLDA independently across multiple languages, relating topics only based on the impact on the response variable. Consider a degenerate tree with only one node, with all words in all languages associated with that node. This is consistent with our model, but there is really no shared semantic space, as all emitted words must come from this degenerate “synset” and the model only represents the output distribution for this single node. Other words’ influence WordNet We took the alignment of GermaNet to WordNet 1.6 (Kunze and Lemnitzer, 2002) and removed all synsets that were had no mapped German words. Any German synsets that did not have English translations had their words mapped to the lowest extant English hypernym (e.g. “beinbruch,” a broken leg, was mapped to “fracture”). We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970). An example of the paths for the German word “wunsch” (wish, request) is shown in Figure 2(a). Dictionaries A dictionary can be viewed as a many to many mapping, where each entry ei maps one or more words in one language si to one or more words ti in another la</context>
</contexts>
<marker>Kunze, Lemnitzer, 2002</marker>
<rawString>Claudia Kunze and Lothar Lemnitzer. 2002. Standardizing WordNets in a web-compliant format: The case of GermaNet. In Workshop on Wordnets Structures and Standardisation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="30976" citStr="Lin and He, 2009" startWordPosition="4965" endWordPosition="4968">sitive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predictions. Other probabilistic models for sentiment classification view sentiment as a word level feature. Some models use sentiment word lists, either given or learned from a corpus, as a prior to seed topics so that they attract other sentiment bearing words (Mei et al., 2007; Lin and He, 2009). Other approaches view sentiment or perspective as a perturbation of a log-linear topic model (Lin et al., 2008). Such techniques could be combined with the multilingual approach presented here by using distributions over words that not only bridge different languages but also encode additional information. For example, the vocabulary hierarchies could be structured to encourage topics that encourage correlation among similar sentiment-bearing words (e.g. clustering words associated with price, size, etc.). Future work could also more rigorously validate that the multilingual topics discovere</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Eric Xing</author>
<author>Alexander Hauptmann</author>
</authors>
<title>A joint topic and perspective model for ideological discourse.</title>
<date>2008</date>
<booktitle>In ECML PKDD.</booktitle>
<contexts>
<context position="31089" citStr="Lin et al., 2008" startWordPosition="4983" endWordPosition="4986">issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predictions. Other probabilistic models for sentiment classification view sentiment as a word level feature. Some models use sentiment word lists, either given or learned from a corpus, as a prior to seed topics so that they attract other sentiment bearing words (Mei et al., 2007; Lin and He, 2009). Other approaches view sentiment or perspective as a perturbation of a log-linear topic model (Lin et al., 2008). Such techniques could be combined with the multilingual approach presented here by using distributions over words that not only bridge different languages but also encode additional information. For example, the vocabulary hierarchies could be structured to encourage topics that encourage correlation among similar sentiment-bearing words (e.g. clustering words associated with price, size, etc.). Future work could also more rigorously validate that the multilingual topics discovered by MLSLDA are sentiment-bearing via human judgments. In contrast, MLSLDA draws on techniques that view sentimen</context>
</contexts>
<marker>Lin, Xing, Hauptmann, 2008</marker>
<rawString>Wei-Hao Lin, Eric Xing, and Alexander Hauptmann. 2008. A joint topic and perspective model for ideological discourse. In ECML PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: the natural language toolkit. In Tools and methodologies for teaching.</title>
<date>2002</date>
<publisher>ACL.</publisher>
<contexts>
<context position="22223" citStr="Loper and Bird, 2002" startWordPosition="3598" endWordPosition="3601">tent topics across languages for matching parallel documents, how well it can discover sentiment-correlated word lists from nonaligned text, and how well it can predict sentiment. 4.1 Matching on Multilingual Topics We took the 1996 documents from the Europarl corpus (Koehn, 2005) using three bridges: GermaNet, dictionary, and the uninformative flat matching.4 The model is unaware that the translations of documents in one language are present in the other language. Note that this does not use the supervised framework 4For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content words were excluded. (as there is no associated response variable for Europarl documents); this experiment is to demonstrate the effectiveness of the multilingual aspect of the model. To test whether the topics learned by the model are consistent across languages, we represent each document using the probability distribution Bd over topic a</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: the natural language toolkit. In Tools and methodologies for teaching. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured models for fine-to-coarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30083" citStr="McDonald et al., 2007" startWordPosition="4823" endWordPosition="4827">rds to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, MLSLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws away information important to sentiment analysis like syntactic constructions (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating. Second, a single real number is not always sufficient to capture the nuances of sentiment. Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predic</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured models for fine-to-coarse sentiment analysis. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="30957" citStr="Mei et al., 2007" startWordPosition="4961" endWordPosition="4964">e a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predictions. Other probabilistic models for sentiment classification view sentiment as a word level feature. Some models use sentiment word lists, either given or learned from a corpus, as a prior to seed topics so that they attract other sentiment bearing words (Mei et al., 2007; Lin and He, 2009). Other approaches view sentiment or perspective as a perturbation of a log-linear topic model (Lin et al., 2008). Such techniques could be combined with the multilingual approach presented here by using distributions over words that not only bridge different languages but also encode additional information. For example, the vocabulary hierarchies could be structured to encourage topics that encourage correlation among similar sentiment-bearing words (e.g. clustering words associated with price, size, etc.). Future work could also more rigorously validate that the multilingu</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Dan Melamed</author>
</authors>
<title>Empirical methods for exploiting parallel texts.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="20310" citStr="Melamed, 1998" startWordPosition="3293" endWordPosition="3294">ies were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b). Algorithmic Connections In addition to handcurated connections across languages, one could also consider automatic means of mapping across languages, such as using edit distance or local context (Haghighi et al., 2008; Rapp, 1995) or using a lexical translation table obtained from parallel text (Melamed, 1998). While we experimented p(yd|z, η, σ) a � exp 1 \ Ek0 Nd,k0ηk0 ηzk 2 yd − σ Ek, Nd k0 / E 0Nd,k0 k • .r • � � � 2 . (4) exp −η zk 2σ2 Ek0 N2d,k0 • .r • This word’s influence 49 (a) GermaNet (b) Dictionary wish wunsch ask request anfrag wunsch altern option option.n.02 choic cognition.n.01 event.n.01 preference.n.03 wish.n.04 event option abstraction.n.06 ereignis deed vorgang entity.n.01 entiti speech_act.n.01 request.n.02 act objekt act.n.02 handlung root dict.1 dict.2 room gelass room raum platz room zimm raum stub dict.3 Figure 2: Two methods for constructing multilingual distributions over</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>Ilya Dan Melamed. 1998. Empirical methods for exploiting parallel texts. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Nouns in WordNet: A lexical inheritance system.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="7213" citStr="Miller, 1990" startWordPosition="1090" endWordPosition="1091">as been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For concreteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge, since it is well known, offers convenient and intuitive terminology, and demonstrates the full flexibility of our approach. However, the model we describe generalizes to any tree-structured representation of multilingual knowledge; we discuss some alternatives in Section 3. WordNet organizes a vocabulary into a rooted, directed acyclic graph of nodes called synsets, short for “synonym sets.” A synset is a child of another synset if it satisfies a hyponomy relationship; each child “is a” more specific instantiation of its parent conce</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. Nouns in WordNet: A lexical inheritance system. International Journal of Lexicography, 3(4):245–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Jason Naradowsky</author>
<author>David Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="29031" citStr="Mimno et al., 2009" startWordPosition="4659" endWordPosition="4662">enough data, a monolingual model is no longer helped by adding additional multilingual data. 52 5 Relationship to Previous Research The advantages of MLSLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another langu</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna Wallach, Jason Naradowsky, David Smith, and Andrew McCallum. 2009. Polylingual topic models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Jian Hu</author>
<author>Zheng Chen</author>
</authors>
<title>Mining multilingual topics from Wikipedia.</title>
<date>2009</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="29010" citStr="Ni et al., 2009" startWordPosition="4655" endWordPosition="4658">ffe, 2007); with enough data, a monolingual model is no longer helped by adding additional multilingual data. 52 5 Relationship to Previous Research The advantages of MLSLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the </context>
</contexts>
<marker>Ni, Sun, Hu, Chen, 2009</marker>
<rawString>Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2009. Mining multilingual topics from Wikipedia. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Ordan</author>
<author>Shuly Wintner</author>
</authors>
<title>Hebrew WordNet: a test case of aligning lexical databases across languages.</title>
<date>2007</date>
<journal>International Journal of Translation,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>58</pages>
<contexts>
<context position="8333" citStr="Ordan and Wintner, 2007" startWordPosition="1264" endWordPosition="1267">it satisfies a hyponomy relationship; each child “is a” more specific instantiation of its parent concept (thus, hyponomy is often called an “isa” relationship). For example, a “dog” is a “canine” is an “animal” is a “living thing,” etc. As an approximation, it is not unreasonable to assume that WordNet’s structure of meaning is language independent, i.e. the concept encoded by a synset can be realized using terms in different languages that share the same meaning. In practice, this organization has been used to create many alignments of international WordNets to the original English WordNet (Ordan and Wintner, 2007; Sagot and Fiˇser, 2008; Isahara et al., 2008). Using the structure of WordNet, we can now describe a generative process that produces a distribution over a multilingual vocabulary, which encourages correlations between words with similar mean46 ings regardless of what language each word is in. For each synset h, we create a multilingual word distribution for that synset as follows: 1. Draw transition probabilities Qh — Dir (Th) 2. Draw stop probabilities wh — Dir (fah) 3. For each language l, draw emission probabilities for that synset 0h,l — Dir (7rh,l). For conciseness in the rest of the p</context>
</contexts>
<marker>Ordan, Wintner, 2007</marker>
<rawString>Noam Ordan and Shuly Wintner. 2007. Hebrew WordNet: a test case of aligning lexical databases across languages. International Journal of Translation, 19(1):39– 58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="26432" citStr="Pang and Lee, 2005" startWordPosition="4256" endWordPosition="4259">gt” (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of “gut” (good) in one of the negative sentiment topics, showing the difficulty of learning collocations. Train Test GermaNet Dictionary Flat DE DE 73.8 24.8 92.2 EN DE 7.44 2.68 18.3 EN + DE DE 1.17 1.46 1.39 Table 1: Mean squared error on a film review corpus. All results are on the same German test data, varying the training data. Over-fitting prevents the model learning on the German data alone; adding English data to the mix allows the model to make better predictions. 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus.6 The results for predicting sentiment in German documents with 25 topics are presented in Table 1. On a small monolingual corpus, prediction is very poor. The model over-fits, especially when it has the entire vocabulary to select from. The slightly better performance using GermaNet and a dictionary as topic priors can be viewed as basic feature selection, removing proper names from the vocabulary to 6We followed Pang and Lee’s method for creating a numerical score between 0 and 1 from a star rating. We then converted that to an integer by multiply</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis.</title>
<date>2008</date>
<publisher>Now Publishers Inc.</publisher>
<contexts>
<context position="1237" citStr="Pang and Lee, 2008" startWordPosition="168" endWordPosition="171">text connect to an observed regression variable (such as ratings on a sentiment scale). Concepts are represented in a general hierarchical framework that is flexible enough to express semantic ontologies, dictionaries, clustering constraints, and, as a special, degenerate case, conventional topic models. Both the topics and the regression are discovered via posterior inference from corpora. We show MLSLDA can build topics that are consistent across languages, discover sensible bilingual lexical correspondences, and leverage multilingual corpora to better predict sentiment. Sentiment analysis (Pang and Lee, 2008) offers the promise of automatically discerning how people feel about a product, person, organization, or issue based on what they write online, which is potentially of great value to businesses and other organizations. However, the vast majority of sentiment resources and algorithms are limited to a single language, usually English (Wilson, 2008; Baccianella and Sebastiani, 2010). Since no single language captures a majority of the content online, adopting such a limited approach in an increasingly global community risks missing important details and trends that might only be available when t</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Now Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
<author>Richard Boulton</author>
</authors>
<date>1970</date>
<note>Snowball stemmer. http://snowball.tartarus.org/credits.php.</note>
<contexts>
<context position="19422" citStr="Porter and Boulton, 1970" startWordPosition="3145" endWordPosition="3148"> there is really no shared semantic space, as all emitted words must come from this degenerate “synset” and the model only represents the output distribution for this single node. Other words’ influence WordNet We took the alignment of GermaNet to WordNet 1.6 (Kunze and Lemnitzer, 2002) and removed all synsets that were had no mapped German words. Any German synsets that did not have English translations had their words mapped to the lowest extant English hypernym (e.g. “beinbruch,” a broken leg, was mapped to “fracture”). We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970). An example of the paths for the German word “wunsch” (wish, request) is shown in Figure 2(a). Dictionaries A dictionary can be viewed as a many to many mapping, where each entry ei maps one or more words in one language si to one or more words ti in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b). Algorithmic Connections</context>
<context position="22265" citStr="Porter and Boulton, 1970" startWordPosition="3604" endWordPosition="3607">ing parallel documents, how well it can discover sentiment-correlated word lists from nonaligned text, and how well it can predict sentiment. 4.1 Matching on Multilingual Topics We took the 1996 documents from the Europarl corpus (Koehn, 2005) using three bridges: GermaNet, dictionary, and the uninformative flat matching.4 The model is unaware that the translations of documents in one language are present in the other language. Note that this does not use the supervised framework 4For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content words were excluded. (as there is no associated response variable for Europarl documents); this experiment is to demonstrate the effectiveness of the multilingual aspect of the model. To test whether the topics learned by the model are consistent across languages, we represent each document using the probability distribution Bd over topic assignments. Each Bd is a vector of length </context>
</contexts>
<marker>Porter, Boulton, 1970</marker>
<rawString>Martin Porter and Richard Boulton. 1970. Snowball stemmer. http://snowball.tartarus.org/credits.php.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Konrad K¨ording</author>
<author>Thomas L Griffiths</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>Unsupervised topic modelling for multi-party spoken discourse.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<marker>Purver, K¨ording, Griffiths, Tenenbaum, 2006</marker>
<rawString>Matthew Purver, Konrad K¨ording, Thomas L. Griffiths, and Joshua Tenenbaum. 2006. Unsupervised topic modelling for multi-party spoken discourse. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In ACL,</booktitle>
<pages>320--322</pages>
<contexts>
<context position="20229" citStr="Rapp, 1995" startWordPosition="3279" endWordPosition="3280">ore words in one language si to one or more words ti in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b). Algorithmic Connections In addition to handcurated connections across languages, one could also consider automatic means of mapping across languages, such as using edit distance or local context (Haghighi et al., 2008; Rapp, 1995) or using a lexical translation table obtained from parallel text (Melamed, 1998). While we experimented p(yd|z, η, σ) a � exp 1 \ Ek0 Nd,k0ηk0 ηzk 2 yd − σ Ek, Nd k0 / E 0Nd,k0 k • .r • � � � 2 . (4) exp −η zk 2σ2 Ek0 N2d,k0 • .r • This word’s influence 49 (a) GermaNet (b) Dictionary wish wunsch ask request anfrag wunsch altern option option.n.02 choic cognition.n.01 event.n.01 preference.n.03 wish.n.04 event option abstraction.n.06 ereignis deed vorgang entity.n.01 entiti speech_act.n.01 request.n.02 act objekt act.n.02 handlung root dict.1 dict.2 room gelass room raum platz room zimm raum s</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In ACL, pages 320–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In IJCAI,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="10769" citStr="Resnik, 1995" startWordPosition="1679" endWordPosition="1680">fixed, but their magnitude is sampled during inference (i.e. Th,i is constant, but Th,i is not). For the bushier bridges, -k Th,k (e.g. dictionary and flat), their mean is uniform. For GermaNet, we took frequencies from two balanced corpora of German and English: the British National Corpus (University of Oxford, 2006) and the Kern Corpus of the Digitales W¨orterbuch der Deutschen Sprache des 20. Jahrhunderts project (Geyken, 2007). We took these frequencies and propagated them through the multilingual hierarchy, following LDAWN’s (Boyd-Graber et al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior. The variance of the priors was initialized to be 1.0, but could be sampled during inference. 3Note that the language and word are taken as given, but the path through the semantic hierarchy is a latent random variable. Topic 1 is about baseball in English and about travel in German). Separating path from emission helps ensure that topics are consistent across languages. Having defined topic distributions in a way that can preserve cross-language correspondences, we now use this distribution within a larger model that can discover cross-language patterns of use that predic</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In IJCAI, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Richter</author>
</authors>
<title>Dictionary nice grep.</title>
<date>2008</date>
<note>http://wwwuser.tu-chemnitz.de/ fri/ding/.</note>
<contexts>
<context position="19760" citStr="Richter, 2008" startWordPosition="3207" endWordPosition="3208"> Any German synsets that did not have English translations had their words mapped to the lowest extant English hypernym (e.g. “beinbruch,” a broken leg, was mapped to “fracture”). We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970). An example of the paths for the German word “wunsch” (wish, request) is shown in Figure 2(a). Dictionaries A dictionary can be viewed as a many to many mapping, where each entry ei maps one or more words in one language si to one or more words ti in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b). Algorithmic Connections In addition to handcurated connections across languages, one could also consider automatic means of mapping across languages, such as using edit distance or local context (Haghighi et al., 2008; Rapp, 1995) or using a lexical translation table obtained from parallel text (Melamed, 1998). While we experimented p(yd|z, η, σ) a � exp 1 \ </context>
</contexts>
<marker>Richter, 2008</marker>
<rawString>Frank Richter. 2008. Dictionary nice grep. http://wwwuser.tu-chemnitz.de/ fri/ding/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Padhraic Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="4783" citStr="Rosen-Zvi et al., 2004" startWordPosition="709" endWordPosition="712">ts of documents to topics. Both the topics and the assignments are probabilistic: a topic is represented as a probability distribution over words in the corpus, and each document is assigned a probability distribution over all the topics. Topic models built on the foundations of LDA are appealing for sentiment analysis because the learned topics can cluster together sentimentbearing words, and because topic distributions are a parsimonious way to represent a document.1 LDA has been used to discover latent structure in text (e.g. for discourse segmentation (Purver et al., 2006) and authorship (Rosen-Zvi et al., 2004)). MLSLDA extends the approach by ensuring that this latent structure — the underlying topics — is consistent across languages. We discuss multilingual topic modeling in Section 1.1, and in Section 1.2 we show how this enables supervised regression regardless of a document’s language. 1.1 Capturing Semantic Correlations Topic models posit a straightforward generative process that creates an observed corpus. For each document d, some distribution Bd over unobserved topics is chosen. Then, for each word position in the document, a topic z is selected. Finally, the word for that position is gener</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers, and Padhraic Smyth. 2004. The author-topic model for authors and documents. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
<author>Darja Fiˇser</author>
</authors>
<title>Building a Free French WordNet from Multilingual Resources. In OntoLex.</title>
<date>2008</date>
<marker>Sagot, Fiˇser, 2008</marker>
<rawString>Benoit Sagot and Darja Fiˇser. 2008. Building a Free French WordNet from Multilingual Resources. In OntoLex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization. In</title>
<date>2008</date>
<booktitle>ACL,</booktitle>
<pages>308--316</pages>
<contexts>
<context position="31900" citStr="Titov and McDonald, 2008" startWordPosition="5100" endWordPosition="5103">formation. For example, the vocabulary hierarchies could be structured to encourage topics that encourage correlation among similar sentiment-bearing words (e.g. clustering words associated with price, size, etc.). Future work could also more rigorously validate that the multilingual topics discovered by MLSLDA are sentiment-bearing via human judgments. In contrast, MLSLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008). Extending these models to multilingual data would be more straightforward. 6 Conclusions MLSLDA is a “holistic” statistical model for multilingual corpora that does not require parallel text or expensive multilingual resources. It discovers connections across languages that can recover latent structure in parallel corpora, discover sentimentcorrelated word lists in multiple languages, and make accurate predictions across languages that improve with more multilingual data, as demonstrated in the context of sentiment analysis. More generally, MLSLDA provides a formalism that can be used to inc</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In ACL, pages 308–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In SIGHAN Workshop on Chinese Language Processing.</booktitle>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<journal>British National Corpus.</journal>
<institution>University of Oxford.</institution>
<note>http://www.natcorp.ox.ac.uk/. http://www.natcorp.ox.ac.uk/.</note>
<marker>2006</marker>
<rawString>University of Oxford. 2006. British National Corpus. http://www.natcorp.ox.ac.uk/. http://www.natcorp.ox.ac.uk/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Vetter</author>
<author>Manfred Sauer</author>
<author>Philipp Wallutat</author>
</authors>
<date>2000</date>
<note>Filmrezension.de: Online-magazin f¨ur filmkritik. http://www.filmrezension.de.</note>
<marker>Vetter, Sauer, Wallutat, 2000</marker>
<rawString>Tobias Vetter, Manfred Sauer, and Philipp Wallutat. 2000. Filmrezension.de: Online-magazin f¨ur filmkritik. http://www.filmrezension.de.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="29336" citStr="Wan, 2009" startWordPosition="4702" endWordPosition="4703">y, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, MLSLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws awa</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>David Blei</author>
<author>Li Fei-Fei</author>
</authors>
<title>Simultaneous image classification and annotation.</title>
<date>2009</date>
<booktitle>In CVPR. Xing Wei and</booktitle>
<contexts>
<context position="12286" citStr="Wang et al., 2009" startWordPosition="1923" endWordPosition="1926">, 2007)), which makes predictions based on the topics expressed in a document; this can be thought of projecting the words in a document to low dimensional space of dimension equal to the number of topics. Blei et al. showed that using this latent topic structure can offer improved predictions over regressions based on words alone, and the approach fits well with our current goals, since word-level cues are unlikely to be identical across languages. In addition to text, SLDA has been successfully applied to other domains such as social networks (Chang and Blei, 2009) and image classification (Wang et al., 2009). The key innovation in this paper is to extend SLDA by creating topics that are globally consistent across languages, using the bridging approach above. We express our model in the form of a probabilistic generative latent-variable model that generates documents in multiple languages and assigns a realvalued score to each document. The score comes from a normal distribution whose sum is the dot product between a regression parameter q that encodes the influence of each topic on the observation and a variance a2. With this model in hand, we use statistical inference to determine the distributi</context>
<context position="16007" citStr="Wang et al., 2009" startWordPosition="2564" endWordPosition="2567"> probabilistic dependencies. probability of taking a path r is then p(λn = r|zn = k,X−n) = ! Ok,i,1 + ωi P s∈0,1 Ok,i,s + ωi,s |{z } Transition Ok,rend,0 + ωrend Fk,rend,wn + πrend,l Ps∈0,1 Ok,rend,s + ωrend,s |{z } Emission (3) Equation 3 reflects the multilingual aspect of this model. The conditional topic distribution for SLDA (Blei and McAuliffe, 2007) replaces this term with the standard Multinomial-Dirichlet. However, we believe this is the first published SLDA-style model using MCMC inference, as prior work has used variational inference (Blei and McAuliffe, 2007; Chang and Blei, 2009; Wang et al., 2009). Because the observed response variable depends on the topic assignments of a document, the conditional topic distribution is shifted toward topics that explain the observed response. Topics that move the predicted response yd toward the true yd will be favored. We drop terms that are constant across all 7ch,l (Pi,h,l �h Th pi,h (0i,h L K H wd,n Xd,n zd,n 0d α N yd M 6 71 Bk,i,j + τi,j P j/ Bk,i,j&apos; + τi,j Y (i,j)∈r Pw/ Frend,w/ + πrend,w/ . 48 topics for the effect of the response variable, The above equation represents the supervised aspect of the model, which is inherited from SLDA. Finally</context>
</contexts>
<marker>Wang, Blei, Fei-Fei, 2009</marker>
<rawString>Chong Wang, David Blei, and Li Fei-Fei. 2009. Simultaneous image classification and annotation. In CVPR. Xing Wei and Bruce Croft. 2006. LDA-based document models for ad-hoc retrieval. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In CIKM.</booktitle>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal groups for sentiment analysis. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Ann Wilson</author>
</authors>
<title>Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of Private States.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="1585" citStr="Wilson, 2008" startWordPosition="223" endWordPosition="224"> via posterior inference from corpora. We show MLSLDA can build topics that are consistent across languages, discover sensible bilingual lexical correspondences, and leverage multilingual corpora to better predict sentiment. Sentiment analysis (Pang and Lee, 2008) offers the promise of automatically discerning how people feel about a product, person, organization, or issue based on what they write online, which is potentially of great value to businesses and other organizations. However, the vast majority of sentiment resources and algorithms are limited to a single language, usually English (Wilson, 2008; Baccianella and Sebastiani, 2010). Since no single language captures a majority of the content online, adopting such a limited approach in an increasingly global community risks missing important details and trends that might only be available when text in multiple languages is taken into account. Philip Resnik Department of Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu Up to this point, multiple languages have been addressed in sentiment analysis primarily by transferring knowledge from a resource-rich language to a less rich language (Banea et al., 2008), or</context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>Theresa Ann Wilson. 2008. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of Private States. Ph.D. thesis, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>BiTAM: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="29092" citStr="Zhao and Xing, 2006" startWordPosition="4669" endWordPosition="4672">ng additional multilingual data. 52 5 Relationship to Previous Research The advantages of MLSLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, MLSLDA views all languages through the lens of the topic</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual topic admixture models for word alignment. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Medlda: maximum margin supervised topic models for regression and classification.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="30625" citStr="Zhu et al., 2009" startWordPosition="4909" endWordPosition="4912">ns (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating. Second, a single real number is not always sufficient to capture the nuances of sentiment. Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predictions. Other probabilistic models for sentiment classification view sentiment as a word level feature. Some models use sentiment word lists, either given or learned from a corpus, as a prior to seed topics so that they attract other sentiment bearing words (Mei et al., 2007; Lin and He, 2009). Other approaches view sentiment or perspective as a perturbation of a log-linear topic model (Lin et al., 2008). Such techniques could be combined with the multilingual approach presented here by using distributions over words that not only bridge</context>
</contexts>
<marker>Zhu, Ahmed, Xing, 2009</marker>
<rawString>Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. Medlda: maximum margin supervised topic models for regression and classification. In ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>