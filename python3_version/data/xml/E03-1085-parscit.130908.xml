<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010136">
<title confidence="0.7635675">
PEAS, the first instantiation of a comparative framework
for evaluating parsers of French
</title>
<author confidence="0.570151">
V. Gendner, G. Illouz, M. Jardino, L. Monceaux, P. Paroubek, I. Robba, A. Vilnat
</author>
<affiliation confidence="0.196998">
L1MSI — CNRS, BP 133, 91403 Orsay — France
</affiliation>
<email confidence="0.990018">
{gendner,gabrieli,jardino,monceaux,pap,isabelle,anne}@limsi.fr
</email>
<sectionHeader confidence="0.993508" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999786">
This paper presents PEAS, the first
comparative evaluation framework for
parsers of French whose annotation
formalism allows the annotation of both
constituents and functional relations. A
test corpus containing an assortment of
different text types has been built and
part of it has been manually annotated.
Precision/Recall and crossing brackets
metrics will be adapted to our formalism
and applied to the parses produced by
one parser from academia and another
one from industry in order to validate the
framework.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998318">
In natural language understanding, many
complex applications use a syntactic parser as a
basic functionality. Today, in particular for the
French language, the developers face the great
diversity of the offer in the domain. Therefore,
the need for a complete comparative evaluation
framework — including a pivot annotation
formalism, a reference treebank, evaluation
metrics and the associated software — is
increasing.
It is worth noting that most of the recently
developed parsers use a robust approach.
Consequently, they do not always produce a
complete parse of the sentence, but they are able
to produce a result, whatever the size, the
particularities and the grammaticality of the
input. For this reason, it is essential to be able to
compare in a fair way the parses they produce
against those produced by other parsers whatever
their characteristics. One possible solution is to
offer a common reference annotation formalism
along with a fully parsed reference corpus and a
set of robust metrics, allowing for both complete
and selective evaluation over an assortment of
different text types and syntactic phenomena.
The aim of our research is to build such
evaluation framework, which to date is missing
for French. Figure 1 presents the different
modules of our evaluation protocol as it stands
today.
</bodyText>
<figureCaption confidence="0.999823">
Figure 1: Evaluation protocol modules
</figureCaption>
<figure confidence="0.994544884615385">
Parsed Corpus Parsed Corpus
Transcription
2 Participants
Xerox / XRCE Grenoble
GREYC / Caen University
Constitution of Format: XML
a heterogeneous Tools:
corpus 2 specific converters :
Parser 1 -&gt; XML
Parser 2 -&gt; XML
Formated Corpus
normal i Jan on
tokenization
sentence segmentation
20 K words annotated
reference hidden
in a 1 million word corpus
Scoring
Annotated Corpus Metres
Precision / Recall
Crossing Brackets
Formats : XML / HTML
Tools. HTML editor
2 specific converters.
XML -&gt; HTML
HTML -&gt; XML
</figure>
<page confidence="0.994139">
95
</page>
<sectionHeader confidence="0.943502" genericHeader="method">
2 Annotation formalism
</sectionHeader>
<bodyText confidence="0.929773975">
The definition of the annotation formalism is the
core element of the evaluation process. Indeed,
the formalism must have a coverage of
syntactical phenomena as broad as possible in
order to allow any parser to participate, whatever
the grammatical formalism it uses.
We have decided immediately upon a two-
steps annotation: first the chunks annotation is
carried out, second functional information is
annotated through relations between words,
words and chunks or between chunks. The
constituents or chunks are continuous and non-
embedded. They are as small as possible to allow
any segmentation chosen by a parser to be
converted into our formalism. For the same
reason, the information that is not expressed in
the constituents is expressed through a large
number of functional relations: twelve in all.
Such formalism is closer to a dependency-based
formalism than to a constituent based formalism
(Sleator and Temperley, 1991). It neither
prevents the &amp;quot;deep&amp;quot; parsers to be evaluated, nor
disadvantages them, but the transcription of their
parses could be more complex. The six types of
chunks and twelve functional relations are given
in table 1. They were mainly inspired by Abeille
et al. (2000), and have been adapted while
annotating corpus excerpts.
Functional relations
subject-verb
auxiliary-verb
argument-verb
modifier-verb
modifier-noun
modifier-adjective
modifier-adverb
attribute-subject/object
Coordination
Apposition
Complementer
</bodyText>
<tableCaption confidence="0.997484">
Table 1: Annotated chunks and relations
</tableCaption>
<bodyText confidence="0.999525153846154">
No clausal or sentential segmentation is
identified, because as in a dependency-based
formalism, the complex structure of the sentence
is obtained through the whole chain of relations.
The following sentencel that contains three noun
phrases (NP) gives an example: &lt;NP1&gt; la porte
de la chambre fermee a clef a l&apos;interieur
&lt;/NPI&gt;&lt;NP2&gt; les volets de l&apos;unique fenetre
fermes, eux aussi, a rinterieur &lt;/NP2&gt; et
&lt;NP3&gt; par-dessus les volets, les barreaux
intacts &lt;/NP3&gt;, 1...]. In our formalism, the noun
phrases are described through the following
chunks and relations:
</bodyText>
<construct confidence="0.887128214285714">
&lt;GN1&gt; la porte &lt;/GN1&gt;
&lt;GN2&gt; les volets &lt;/GN2&gt;
&lt;GN3&gt; les barreawc &lt;/GN3&gt;
coordination (&amp;quot;,&amp;quot; , GN1, GN2)
coordination (et, GN2, GN3)
And the noun phrase NP1 is expressed through:
&lt;GN1&gt; la porte &lt;/GM&gt;
&lt;GP1&gt; de la chambre &lt;/GP1&gt;
&lt;GAl&gt; fermee &lt;/GAl&gt;
&lt;GP2&gt; a clef &lt;/GP2&gt;
&lt;GP3&gt; a Vinterieur &lt;/GP3&gt;
modifier-noun (GP1, porte)
modifier-noun (GA1, porte)
modifier-adjective (GP2, fermee)
</construct>
<bodyText confidence="0.988853857142857">
Moreover, since our chunks are not embedded,
all the modifiers placed before a noun are
included in the same nominal group as the noun
itself. And here again, the relations are used to
express the links between the particular terms, as
in the annotated example of mon tres riche et
tres proche ami2:
</bodyText>
<construct confidence="0.9281446">
&lt;GN&gt; mon tres riche et tres proche ami &lt;/GN&gt;
modifier-adjective (tres, riche)
modifier-adjective (tres, proche)
coordination (et, riche, proche)
modifier-noun (et, ami)
</construct>
<bodyText confidence="0.999711333333333">
The formalism gives the possibility to
annotate ambiguities at dependency level (by
duplicating the relation tables). Note that we are
</bodyText>
<footnote confidence="0.9417832">
1 This original sentence is extracted from (Leroux, 1907),
and may be translated as: the shutters of the single window
also closed from inside, and over the shutters, the bars
intact.
2 Translation: my very rich and very close friend.
</footnote>
<table confidence="0.999458090909091">
Chunks
NV — verbal
GN — nominal
GR — adverbial
GA — adjectival
GP — prepositional
introducing a
nominal phrase
PV — prepositional
introducing a verbal
phrase
</table>
<page confidence="0.990977">
96
</page>
<bodyText confidence="0.9993125">
still studying how our evaluation will handle this
phenomenon.
</bodyText>
<sectionHeader confidence="0.764714" genericHeader="method">
3 Corpus and tools for annotation
</sectionHeader>
<bodyText confidence="0.999981208333333">
The corpus retained for annotation is a set of
texts whose nature is as diverse as possible.
Indeed the corpus contains excerpts from:
newspapers, novels, Web pages, automatic audio
transcriptions, and a set of questions translated
from the question-answering track of TREC. The
whole corpus contains 1 million words; each text
has been segmented in sentences and tokenized
in words. Each participant to the evaluation
protocol has received the texts both in pre-
segmented and original format.
The part of the corpus that has been annotated
contains about 20,000 words. The annotation
tools, that we have developed, use an HTML
editor. For chunk marking, the annotator selects
chunks and colors them (each type of chunk
corresponding to a particular color). For the
twelve functional relations, the annotator has a
set of twelve tables to fill in for each sentence;
giving for each relation the address of its
parameters. Of course, all of them are not to be
filled in. All the information thus annotated is
then translated into an XML format. Annotation
of the example of §2 is translated in:
</bodyText>
<figure confidence="0.968020333333333">
&lt;E id=&amp;quot;0&amp;quot;&gt;
&lt;constituants&gt;
&lt;Groupe type=&amp;quot;GN&amp;quot; id=&amp;quot;GO&amp;quot;&gt;
&lt;F id=&amp;quot;FO&amp;quot;&gt; la &lt;F&gt;
&lt;F id=&amp;quot;Fl&amp;quot;&gt; porte &lt;F&gt;
&lt;/Groupe&gt;
&lt;Groupe type=&amp;quot;GP&amp;quot; id=&amp;quot;Gl&amp;quot;&gt;
&lt;F id=&amp;quot;F2&amp;quot;&gt; de &lt;F&gt;
&lt;F id=&amp;quot;F3&amp;quot;&gt; la &lt;F&gt;
id=&amp;quot;F4&amp;quot;&gt; chambre &lt;F&gt;
&lt;/Groupe&gt;
&lt;Groupe type=&amp;quot;GA&amp;quot; id=&amp;quot;G2&amp;quot;&gt;
&lt;F id=&amp;quot;F5&amp;quot;&gt; fermee &lt;F&gt;
&lt;/Groupe&gt;
&lt;Groupe type=&amp;quot;GP&amp;quot; id=&amp;quot;G3&amp;quot;&gt;
&lt;F id=&amp;quot;F6&amp;quot;&gt; a &lt;F&gt;
&lt;F id=&amp;quot;F7&amp;quot;&gt; &lt;F&gt;
&lt;F id=&amp;quot;F8&amp;quot;&gt; interieur &lt;F&gt;
&lt;/Groupe&gt;
&lt;F id=&amp;quot;F9&amp;quot;&gt; , &lt;F&gt;
&lt;Groupe type=&amp;quot;GN&amp;quot; id=&amp;quot;G4&amp;quot;&gt;
&lt;F id=&amp;quot;F10&amp;quot;&gt; les &lt;F&gt;
&lt;F id=&amp;quot;F11&amp;quot;&gt; volets &lt;F&gt;
&lt;/Groupe&gt;
&lt;Groupe type=&amp;quot;GP&amp;quot; id=&amp;quot;G5&amp;quot;&gt;
&lt;F id=&amp;quot;F12&amp;quot;&gt; de &lt;F&gt;
&lt;F id=&amp;quot;F13&amp;quot;&gt; &lt;F&gt;
&lt;F id=&amp;quot;F14&amp;quot;&gt; unique &lt;F&gt;
id=&amp;quot;F15&amp;quot;&gt; fenetre &lt;F&gt;
&lt;/Groupe&gt; ...
&lt;constituants&gt;
&lt;relations&gt;
&lt;rel xmlns:xlink=&amp;quot;extended&amp;quot; type=&amp;quot;MOD-N&amp;quot; id=&amp;quot;RO&amp;quot;&gt;
&lt;modifieur xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;Gl&amp;quot;&gt;
&lt;nom xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;Fl&amp;quot;&gt;
&lt;/re&gt;
&lt;rel xmlns:xlink=&amp;quot;extended&amp;quot; type=&amp;quot;MOD-N&amp;quot; id=&amp;quot;R1&amp;quot;&gt;
&lt;modifieur xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;G2&amp;quot;&gt;
&lt;nom xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;Fl&amp;quot;&gt;
&lt;/re&gt;
&lt;rel xmlns:xlink=&amp;quot;extended&amp;quot; type=&amp;quot;MOD-A&amp;quot; id=&amp;quot;R2&amp;quot;&gt;
&lt;modifieur xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;G3&amp;quot;&gt;
&lt;adjectif xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;F5&amp;quot;&gt;
&lt;/re&gt; ...
&lt;rel xmlns:xlink=&amp;quot;extended&amp;quot; type=&amp;quot;COORD&amp;quot; id=&amp;quot;R9&amp;quot;&gt;
&lt;coordonnant xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;F9&amp;quot;&gt;
&lt;coord-g xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;Fl&amp;quot;&gt;
&lt;coord-d xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;F 11&amp;quot;&gt;
&lt;/rel&gt;
&lt;relations&gt;
&lt;E&gt;
</figure>
<bodyText confidence="0.9993043">
For the French language, Abeille et al. (2000)
is the only other attempt at building a treebank.
In this case, the corpus is homogeneous in text
genre, since it contains only newspaper articles
extracted from Le Monde although it covers
various domains from politics to sports. The
approach is however ambitious and interesting:
the corpus contains 1M words, 17 000 different
lemma; it is annotated both with morpho-syntax
and grammatical functions.
</bodyText>
<sectionHeader confidence="0.994836" genericHeader="method">
4 Evaluation metrics
</sectionHeader>
<bodyText confidence="0.999954210526316">
The first proposals for parser evaluation were
made in Parseval (Black et al., 1991). Carroll et
al. (1998) gave a survey and proposed a new
evaluation scheme. Since, two orientations have
emerged. The first, inspired by Parseval, is based
on phrase boundaries and uses recall plus
crossing-bracket measures. Although it has been
criticized (Gaizauskas 1998, Lin 1998), it is still
in use nowadays. The second one is based on
dependency relations, (on which recall and
precision can also be computed) and seems to be
more and more in favor (see the workshop
Beyond Parseval 2002).
Since our annotation formalism has both
constituents and functional relations, there is no
reason to dismiss either approaches.
Nevertheless, we have to outline that the
transcription of the parses will be more
systematic for the relations than for the
</bodyText>
<page confidence="0.999034">
97
</page>
<bodyText confidence="0.9997369">
constituents. Indeed, in our formalism, relations
can associate words, chunks or words and
chunks, but it is always possible to match any
relation argument with the reference parse,
because we always know to which chunk a word
belongs. On the other hand, for the segmentation,
the chunk boundaries may vary a lot from one
parse to another. So we have to foresee either an
important set of matching rules, or flexible
evaluation methods.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="evaluation">
5 Prospective
</sectionHeader>
<bodyText confidence="0.999986392857143">
Based on this preliminary research, a larger
project for syntactic parser evaluation, named
EASY/EVALDA has been accepted by
TECHNOLANGUE, a joint program of the three
French Ministries of Industry, Culture and
Research. A rather large francophone community
has declared its interest for the project, fourteen
participants (belonging to universities or to
private institutions) are ready to evaluate their
parser, while five corpus providers are interested
in annotating large size corpora both in syntax
and in functional relations. This community will
contribute to enrich every aspect of our proposal:
annotation formalism, tools and metrics.
Moreover, the participation of a sufficient
number of parsers will allow the production of a
good quality validated linguistic resource.
Indeed, we will produce the automatic fusion of
all annotated data of the parsers, and then
manually correct the divergent parses.&apos;
Last of all, the XML format into which we
translate the parses is an open exchange format.
It is an important asset for portability and reuse
of parsing technolgy. E.g. for question answering
application, where a parser is often needed to
parse both the questions and the huge set of
candidate answers, the use of XML makes easier
the selection of the paser for the task at hand.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.983109083333333">
At the time of writing, we have developed all
the different phases of our evaluation process
except for the evaluation metrics tools. The two
candidate parsers have parsed the corpus, and we
are now translating their outputs within our
3 Monceaux (2002) proposes a rover algorithm, which
merges in one parse the outputs of several parsers.
formalism. Here, the difficulty is neither to loose
information nor to miss incorrect parses. The
application of our metrics and the results
examination will constitute a first validation of
our framework.
</bodyText>
<sectionHeader confidence="0.996427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999501357142857">
A. Abeille, L. Clement and A. Kinyon. 2000. Building
a treebank for French. In Proceedings of the 2nd
Intemational Conference on Language Resources
and Evaluation (LREC), (1):87-94, Athens, Greece,
May. ELRA
Beyond Pars eval — Towards Improved Evaluation
Measures for Parsing Systems. Workshop of the 31-d
International Conference LREC. Las Palmas,
Spain. John Carroll editor.
E. Black et al., A procedure for quantitatively
comparing the syntactic coverage of English
grammars. In DARPA, editor Proceedings of the
Fourth Darpa Speech and Natural Language
Workshop, pages 306-311, Pacific Grove,
California, February, Morgan Kaufmann.
R. Gaizauskas, M. Hepple and H. Huyck. 1998. A
scheme for comparative evaluation of diverse
parsing systems. In Proceedings of the 1st
International Conference LREC, (1):143-149,
Granada, Spain, May. ELRA
V. Gendner, G. Illouz, M. Jardino, L. Monceaux, P.
Paroubek, I. Robba, A. Vilnat. 2002. A Protocol
for Evaluating Analyzers of Syntax (PEAS). In
Proceedings of the 3rd LREC, May 2002, Las
Palmas, Spain.
G. Leroux. 1907. Le mystere de la chambre jaune.
L&apos;Illustration, Paris.
D. Lin. 1998. Dependency based method for
evaluating broad-coverage parsers. Natural
Language Engineering 4 (2 ):97-114.
L. Monceaux. 2002. Adaptation du niveau d&apos;ana-lyse
des interventions dans un dialogue. Application 6
un systeme de question-reponse. PhD thesis Paris
11, December 2002.
D. Sleator and D. Temperley. 1991. Parsing English
with a Link Grammar. Research report CMU-CS-
91-196, Carnegie Mellon U., School of Computer
Science, 91 p.
J. Carroll, T. Briscoe and A. Sanfilippo. 1998.
Parser Evaluation: a Survey and a New Proposal.
In Proceedings of the 1st International Conference
LREC (1):447-454, Granada, Spain, May. ELRA.
</reference>
<page confidence="0.996162">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341986">
<title confidence="0.9952175">PEAS, the first instantiation of a comparative framework for evaluating parsers of French</title>
<author confidence="0.992092">V Gendner</author>
<author confidence="0.992092">G Illouz</author>
<author confidence="0.992092">M Jardino</author>
<author confidence="0.992092">L Monceaux</author>
<author confidence="0.992092">P Paroubek</author>
<author confidence="0.992092">I Robba</author>
<author confidence="0.992092">A Vilnat</author>
<address confidence="0.383299">L1MSI — CNRS, BP 133, 91403 Orsay — France</address>
<email confidence="0.992536">gendner@limsi.fr</email>
<email confidence="0.992536">gabrieli@limsi.fr</email>
<email confidence="0.992536">jardino@limsi.fr</email>
<email confidence="0.992536">monceaux@limsi.fr</email>
<email confidence="0.992536">pap@limsi.fr</email>
<email confidence="0.992536">isabelle@limsi.fr</email>
<email confidence="0.992536">anne@limsi.fr</email>
<abstract confidence="0.993860333333333">This paper presents PEAS, the first comparative evaluation framework for parsers of French whose annotation formalism allows the annotation of both constituents and functional relations. A test corpus containing an assortment of different text types has been built and part of it has been manually annotated. Precision/Recall and crossing brackets metrics will be adapted to our formalism and applied to the parses produced by one parser from academia and another one from industry in order to validate the framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeille</author>
<author>L Clement</author>
<author>A Kinyon</author>
</authors>
<title>Building a treebank for French.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd Intemational Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1--87</pages>
<publisher>ELRA</publisher>
<location>Athens, Greece,</location>
<contexts>
<context position="3902" citStr="Abeille et al. (2000)" startWordPosition="604" endWordPosition="607">ny segmentation chosen by a parser to be converted into our formalism. For the same reason, the information that is not expressed in the constituents is expressed through a large number of functional relations: twelve in all. Such formalism is closer to a dependency-based formalism than to a constituent based formalism (Sleator and Temperley, 1991). It neither prevents the &amp;quot;deep&amp;quot; parsers to be evaluated, nor disadvantages them, but the transcription of their parses could be more complex. The six types of chunks and twelve functional relations are given in table 1. They were mainly inspired by Abeille et al. (2000), and have been adapted while annotating corpus excerpts. Functional relations subject-verb auxiliary-verb argument-verb modifier-verb modifier-noun modifier-adjective modifier-adverb attribute-subject/object Coordination Apposition Complementer Table 1: Annotated chunks and relations No clausal or sentential segmentation is identified, because as in a dependency-based formalism, the complex structure of the sentence is obtained through the whole chain of relations. The following sentencel that contains three noun phrases (NP) gives an example: &lt;NP1&gt; la porte de la chambre fermee a clef a l&apos;in</context>
<context position="8602" citStr="Abeille et al. (2000)" startWordPosition="1282" endWordPosition="1285">difieur xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;Gl&amp;quot;&gt; &lt;nom xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;Fl&amp;quot;&gt; &lt;/re&gt; &lt;rel xmlns:xlink=&amp;quot;extended&amp;quot; type=&amp;quot;MOD-N&amp;quot; id=&amp;quot;R1&amp;quot;&gt; &lt;modifieur xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;G2&amp;quot;&gt; &lt;nom xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;Fl&amp;quot;&gt; &lt;/re&gt; &lt;rel xmlns:xlink=&amp;quot;extended&amp;quot; type=&amp;quot;MOD-A&amp;quot; id=&amp;quot;R2&amp;quot;&gt; &lt;modifieur xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;G3&amp;quot;&gt; &lt;adjectif xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;F5&amp;quot;&gt; &lt;/re&gt; ... &lt;rel xmlns:xlink=&amp;quot;extended&amp;quot; type=&amp;quot;COORD&amp;quot; id=&amp;quot;R9&amp;quot;&gt; &lt;coordonnant xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;F9&amp;quot;&gt; &lt;coord-g xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;Fl&amp;quot;&gt; &lt;coord-d xmlns:xlink=&amp;quot;locator&amp;quot; href=&amp;quot;F 11&amp;quot;&gt; &lt;/rel&gt; &lt;relations&gt; &lt;E&gt; For the French language, Abeille et al. (2000) is the only other attempt at building a treebank. In this case, the corpus is homogeneous in text genre, since it contains only newspaper articles extracted from Le Monde although it covers various domains from politics to sports. The approach is however ambitious and interesting: the corpus contains 1M words, 17 000 different lemma; it is annotated both with morpho-syntax and grammatical functions. 4 Evaluation metrics The first proposals for parser evaluation were made in Parseval (Black et al., 1991). Carroll et al. (1998) gave a survey and proposed a new evaluation scheme. Since, two orie</context>
</contexts>
<marker>Abeille, Clement, Kinyon, 2000</marker>
<rawString>A. Abeille, L. Clement and A. Kinyon. 2000. Building a treebank for French. In Proceedings of the 2nd Intemational Conference on Language Resources and Evaluation (LREC), (1):87-94, Athens, Greece, May. ELRA</rawString>
</citation>
<citation valid="false">
<title>Beyond Pars eval — Towards Improved Evaluation Measures for Parsing Systems.</title>
<booktitle>Workshop of the 31-d International Conference LREC. Las Palmas,</booktitle>
<editor>Carroll editor.</editor>
<publisher>Spain. John</publisher>
<marker></marker>
<rawString>Beyond Pars eval — Towards Improved Evaluation Measures for Parsing Systems. Workshop of the 31-d International Conference LREC. Las Palmas, Spain. John Carroll editor.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Black</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<booktitle>In DARPA, editor Proceedings of the Fourth Darpa Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<publisher>February, Morgan Kaufmann.</publisher>
<location>Pacific Grove, California,</location>
<marker>Black, </marker>
<rawString>E. Black et al., A procedure for quantitatively comparing the syntactic coverage of English grammars. In DARPA, editor Proceedings of the Fourth Darpa Speech and Natural Language Workshop, pages 306-311, Pacific Grove, California, February, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gaizauskas</author>
<author>M Hepple</author>
<author>H Huyck</author>
</authors>
<title>A scheme for comparative evaluation of diverse parsing systems.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference LREC, (1):143-149,</booktitle>
<publisher>ELRA</publisher>
<location>Granada, Spain,</location>
<marker>Gaizauskas, Hepple, Huyck, 1998</marker>
<rawString>R. Gaizauskas, M. Hepple and H. Huyck. 1998. A scheme for comparative evaluation of diverse parsing systems. In Proceedings of the 1st International Conference LREC, (1):143-149, Granada, Spain, May. ELRA</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Gendner</author>
<author>G Illouz</author>
<author>M Jardino</author>
<author>L Monceaux</author>
<author>P Paroubek</author>
<author>I Robba</author>
<author>A Vilnat</author>
</authors>
<title>A Protocol for Evaluating Analyzers of Syntax (PEAS).</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd LREC,</booktitle>
<location>Las Palmas,</location>
<marker>Gendner, Illouz, Jardino, Monceaux, Paroubek, Robba, Vilnat, 2002</marker>
<rawString>V. Gendner, G. Illouz, M. Jardino, L. Monceaux, P. Paroubek, I. Robba, A. Vilnat. 2002. A Protocol for Evaluating Analyzers of Syntax (PEAS). In Proceedings of the 3rd LREC, May 2002, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leroux</author>
</authors>
<title>Le mystere de la chambre jaune. L&apos;Illustration,</title>
<date>1907</date>
<location>Paris.</location>
<contexts>
<context position="5791" citStr="Leroux, 1907" startWordPosition="881" endWordPosition="882"> the modifiers placed before a noun are included in the same nominal group as the noun itself. And here again, the relations are used to express the links between the particular terms, as in the annotated example of mon tres riche et tres proche ami2: &lt;GN&gt; mon tres riche et tres proche ami &lt;/GN&gt; modifier-adjective (tres, riche) modifier-adjective (tres, proche) coordination (et, riche, proche) modifier-noun (et, ami) The formalism gives the possibility to annotate ambiguities at dependency level (by duplicating the relation tables). Note that we are 1 This original sentence is extracted from (Leroux, 1907), and may be translated as: the shutters of the single window also closed from inside, and over the shutters, the bars intact. 2 Translation: my very rich and very close friend. Chunks NV — verbal GN — nominal GR — adverbial GA — adjectival GP — prepositional introducing a nominal phrase PV — prepositional introducing a verbal phrase 96 still studying how our evaluation will handle this phenomenon. 3 Corpus and tools for annotation The corpus retained for annotation is a set of texts whose nature is as diverse as possible. Indeed the corpus contains excerpts from: newspapers, novels, Web pages</context>
</contexts>
<marker>Leroux, 1907</marker>
<rawString>G. Leroux. 1907. Le mystere de la chambre jaune. L&apos;Illustration, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency based method for evaluating broad-coverage parsers.</title>
<date>1998</date>
<journal>Natural Language Engineering</journal>
<volume>4</volume>
<issue>2</issue>
<pages>97--114</pages>
<contexts>
<context position="9395" citStr="Lin 1998" startWordPosition="1407" endWordPosition="1408">covers various domains from politics to sports. The approach is however ambitious and interesting: the corpus contains 1M words, 17 000 different lemma; it is annotated both with morpho-syntax and grammatical functions. 4 Evaluation metrics The first proposals for parser evaluation were made in Parseval (Black et al., 1991). Carroll et al. (1998) gave a survey and proposed a new evaluation scheme. Since, two orientations have emerged. The first, inspired by Parseval, is based on phrase boundaries and uses recall plus crossing-bracket measures. Although it has been criticized (Gaizauskas 1998, Lin 1998), it is still in use nowadays. The second one is based on dependency relations, (on which recall and precision can also be computed) and seems to be more and more in favor (see the workshop Beyond Parseval 2002). Since our annotation formalism has both constituents and functional relations, there is no reason to dismiss either approaches. Nevertheless, we have to outline that the transcription of the parses will be more systematic for the relations than for the 97 constituents. Indeed, in our formalism, relations can associate words, chunks or words and chunks, but it is always possible to mat</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Dependency based method for evaluating broad-coverage parsers. Natural Language Engineering 4 (2 ):97-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Monceaux</author>
</authors>
<title>Adaptation du niveau d&apos;ana-lyse des interventions dans un dialogue. Application 6 un systeme de question-reponse.</title>
<date>2002</date>
<tech>PhD thesis Paris 11,</tech>
<marker>Monceaux, 2002</marker>
<rawString>L. Monceaux. 2002. Adaptation du niveau d&apos;ana-lyse des interventions dans un dialogue. Application 6 un systeme de question-reponse. PhD thesis Paris 11, December 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sleator</author>
<author>D Temperley</author>
</authors>
<title>Parsing English with a Link Grammar. Research report CMU-CS91-196, Carnegie Mellon U.,</title>
<date>1991</date>
<journal>School of Computer Science,</journal>
<volume>91</volume>
<pages>p.</pages>
<contexts>
<context position="3631" citStr="Sleator and Temperley, 1991" startWordPosition="559" endWordPosition="562">a twosteps annotation: first the chunks annotation is carried out, second functional information is annotated through relations between words, words and chunks or between chunks. The constituents or chunks are continuous and nonembedded. They are as small as possible to allow any segmentation chosen by a parser to be converted into our formalism. For the same reason, the information that is not expressed in the constituents is expressed through a large number of functional relations: twelve in all. Such formalism is closer to a dependency-based formalism than to a constituent based formalism (Sleator and Temperley, 1991). It neither prevents the &amp;quot;deep&amp;quot; parsers to be evaluated, nor disadvantages them, but the transcription of their parses could be more complex. The six types of chunks and twelve functional relations are given in table 1. They were mainly inspired by Abeille et al. (2000), and have been adapted while annotating corpus excerpts. Functional relations subject-verb auxiliary-verb argument-verb modifier-verb modifier-noun modifier-adjective modifier-adverb attribute-subject/object Coordination Apposition Complementer Table 1: Annotated chunks and relations No clausal or sentential segmentation is id</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>D. Sleator and D. Temperley. 1991. Parsing English with a Link Grammar. Research report CMU-CS91-196, Carnegie Mellon U., School of Computer Science, 91 p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>T Briscoe</author>
<author>A Sanfilippo</author>
</authors>
<title>Parser Evaluation: a Survey and a New Proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference LREC (1):447-454,</booktitle>
<publisher>ELRA.</publisher>
<location>Granada, Spain,</location>
<contexts>
<context position="9134" citStr="Carroll et al. (1998)" startWordPosition="1366" endWordPosition="1369">cator&amp;quot; href=&amp;quot;F 11&amp;quot;&gt; &lt;/rel&gt; &lt;relations&gt; &lt;E&gt; For the French language, Abeille et al. (2000) is the only other attempt at building a treebank. In this case, the corpus is homogeneous in text genre, since it contains only newspaper articles extracted from Le Monde although it covers various domains from politics to sports. The approach is however ambitious and interesting: the corpus contains 1M words, 17 000 different lemma; it is annotated both with morpho-syntax and grammatical functions. 4 Evaluation metrics The first proposals for parser evaluation were made in Parseval (Black et al., 1991). Carroll et al. (1998) gave a survey and proposed a new evaluation scheme. Since, two orientations have emerged. The first, inspired by Parseval, is based on phrase boundaries and uses recall plus crossing-bracket measures. Although it has been criticized (Gaizauskas 1998, Lin 1998), it is still in use nowadays. The second one is based on dependency relations, (on which recall and precision can also be computed) and seems to be more and more in favor (see the workshop Beyond Parseval 2002). Since our annotation formalism has both constituents and functional relations, there is no reason to dismiss either approaches</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>J. Carroll, T. Briscoe and A. Sanfilippo. 1998. Parser Evaluation: a Survey and a New Proposal. In Proceedings of the 1st International Conference LREC (1):447-454, Granada, Spain, May. ELRA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>