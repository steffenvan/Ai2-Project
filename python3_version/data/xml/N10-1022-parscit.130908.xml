<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994558">
Unsupervised Model Adaptation using Information-Theoretic Criterion
</title>
<author confidence="0.998854">
Ariya Rastrow&apos;, Frederick Jelinek&apos;, Abhinav Sethy2 and Bhuvana Ramabhadran2
</author>
<affiliation confidence="0.8917085">
&apos;Human Language Technology Center of Excellence, and
Center for Language and Speech Processing, Johns Hopkins University
</affiliation>
<email confidence="0.968579">
{ariya, jelinek}@jhu.edu
</email>
<address confidence="0.487551">
2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
</address>
<email confidence="0.989528">
{asethy, bhuvana}@us.ibm.com
</email>
<sectionHeader confidence="0.998646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995294833333333">
In this paper we propose a novel general
framework for unsupervised model adapta-
tion. Our method is based on entropy which
has been used previously as a regularizer in
semi-supervised learning. This technique in-
cludes another term which measures the sta-
bility of posteriors w.r.t model parameters, in
addition to conditional entropy. The idea is to
use parameters which result in both low con-
ditional entropy and also stable decision rules.
As an application, we demonstrate how this
framework can be used for adjusting language
model interpolation weight for speech recog-
nition task to adapt from Broadcast news data
to MIT lecture data. We show how the new
technique can obtain comparable performance
to completely supervised estimation of inter-
polation parameters.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99889">
All statistical and machine learning techniques for
classification, in principle, work under the assump-
tion that
</bodyText>
<listItem confidence="0.9765775">
1. A reasonable amount of training data is avail-
able.
2. Training data and test data are drawn from the
same underlying distribution.
</listItem>
<bodyText confidence="0.995088230769231">
In fact, the success of statistical models is cru-
cially dependent on training data. Unfortunately,
the latter assumption is not fulfilled in many appli-
cations. Therefore, model adaptation is necessary
when training data is not matched (not drawn from
same distribution) with test data. It is often the case
where we have plenty of labeled data for one specific
domain/genre (source domain) and little amount of
labeled data (or no labeled data at all) for the de-
sired domain/genre (target domain). Model adapta-
tion techniques are commonly used to address this
problem. Model adaptation starts with trained mod-
els (trained on source domain with rich amount of la-
beled data) and then modify them using the available
labeled data from target domain (or instead unla-
beled data). A survey on different methods of model
adaptation can be found in (Jiang, 2008).
Information regularization framework has been
previously proposed in literature to control the la-
bel conditional probabilities via input distribution
(Szummer and Jaakkola, 2003). The idea is that la-
bels should not change too much in dense regions
of the input distribution. The authors use the mu-
tual information between input features and labels as
a measure of label complexity. Another framework
previously suggested is to use label entropy (condi-
tional entropy) on unlabeled data as a regularizer to
Maximum Likelihood (ML) training on labeled data
(Grandvalet and Bengio, 2004).
Availability of resources for the target domain cat-
egorizes these techniques into either supervised or
unsupervised. In this paper we propose a general
framework for unsupervised adaptation using Shan-
non entropy and stability of entropy. The assump-
tion is that in-domain and out-of-domain distribu-
tions are not too different such that one can improve
the performance of initial models on in-domain data
by little adjustment of initial decision boundaries
(learned on out-of-domain data).
</bodyText>
<page confidence="0.969628">
190
</page>
<note confidence="0.785115">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 190–197,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.946301" genericHeader="introduction">
2 Conditional Entropy based Adaptation
</sectionHeader>
<bodyText confidence="0.99989125">
In this section, conditional entropy and its relation
to classifier performance are first described. Next,
we introduce our proposed objective function for do-
main adaptation.
</bodyText>
<subsectionHeader confidence="0.992641">
2.1 Conditional Entropy
</subsectionHeader>
<bodyText confidence="0.99998875">
Considering the classification problem where X and
Y are the input features and the corresponding class
labels respectively, the conditional entropy is a mea-
sure of the class overlap and is calculated as follows
</bodyText>
<equation confidence="0.999455">
H(Y|X) = EX[H(Y|X = x)] =
− f p(x) (1:y �p(y|x)log p(y|x) dx (1)
</equation>
<bodyText confidence="0.961507">
Through Fano’s Inequality theorem, one can see
how conditional entropy is related to classification
performance.
</bodyText>
<equation confidence="0.749888">
Theorem 1 (Fano’s Inequality) Suppose
Pe = P{ Y� =6 Y} where Y� = g(X) are the
assigned labels for the data points, based on the
classification rule. Then
H(Y|X) − 1
Pe ≥
log(|Y |− 1)
where Y is the number of possible classes and
H(Y |X) is the conditional entropy with respect to
true distibution.
</equation>
<bodyText confidence="0.968761315789474">
The proof to this theorem can be found in (Cover and
Thomas, 2006). This inequality indicates that Y can
be estimated with low probability of error only if the
conditional entropy H(Y|X) is small.
Although the above theorem is useful in a sense
that it connects the classification problem to Shan-
non entropy, the true distributions are almost never
known to us1. In most classification methods, a spe-
cific model structure for the distributions is assumed
and the task is to estimate the model parameters
within the assumed model space. Given the model
1In fact, Theorem 1 shows how relevant the input features
are for the classification task by putting a lower bound on the
best possible classifier performance. As the overlap between
features from different classes increases, conditional entropy in-
creases as well, thus lowering the performance of the best pos-
sible classifier.
structure and parameters, one can modify Fano’s In-
equality as follows,
</bodyText>
<equation confidence="0.828717">
Corollary 1
Y�=6Y |θ} ≥ Hθ(Y|X) − 1
log(|Y |− 1)
(2)
where Pe(θ) is the classifier probability of error
given model parameters, θ and
Hθ(Y|X) =
− f p(x) (1:y �pθ(y|x) log pθ(y|x) dx
Here, Hθ(Y|X) is the conditional entropy imposed
by model parameters.
</equation>
<bodyText confidence="0.99889075">
Eqn. 2 indicates the fact that models with low
conditional entropy are preferable. However, a low
entropy model does not necessarily have good per-
formance (this will be reviewed later on) 2
</bodyText>
<subsectionHeader confidence="0.998957">
2.2 Objective Function
</subsectionHeader>
<bodyText confidence="0.99689952173913">
Minimization of conditional entropy as a framework
in the classification task is not a new concept and
has been tried by researchers. In fact, (Grandvalet
and Bengio, 2004) use this along with the maxi-
mum likelihood criterion in a semi-supervised set
up such that parameters with both maximum like-
lihood on labeled data and minimum conditional en-
tropy on unlabeled data are chosen. By minimiz-
ing the entropy, the method assumes a prior which
prefers minimal class overlap. Entropy minimiza-
tion is used in (Li et al., 2004) as an unsupervised
non-parametric clustering method and is shown to
result in significant improvement over k-mean, hier-
archical clustering and etc.
These methods are all based on the fact that mod-
els with low conditional entropy have their decision
boundaries passing through low-density regions of
the input distribution, P(X). This is consistent with
the assumption that classes are well separated so that
one can expect to take advantage of unlabeled exam-
ples (Grandvalet and Bengio, 2004).
In many cases shifting from one domain to an-
other domain, initial trained decision boundaries (on
</bodyText>
<footnote confidence="0.911744">
2Imagine a model which classifies any input as class 1.
Clearly for this model HB(Y|X) = 0.
</footnote>
<equation confidence="0.977747">
Pe(θ) = P{
</equation>
<page confidence="0.974566">
191
</page>
<bodyText confidence="0.999399111111111">
out-of-domain data) result in high conditional en-
tropy for the new domain, due to mismatch be-
tween distributions. Therefore, there is a need to
adjust model parameters such that decision bound-
aries goes through low-density regions of the distri-
bution. This motivates the idea of using minimum
conditional entropy criterion for adapting to a new
domain. At the same time, two domains are often
close enough that one would expect that the optimal
parameters for the new domain should not deviate
too much from initial parameters. In order to formu-
late the technique mentioned in the above paragraph,
let us define Oinit to be the initial model parame-
ters estimated on out-of-domain data (using labeled
data). Assuming the availability of enough amount
of unlabeled data for in-domain task, we try to min-
imize the following objective function w.r.t the pa-
rameters,
</bodyText>
<equation confidence="0.997710333333333">
Bnew = argmin Hθ(Y|X) + A ||B − Binit||p
θ
(3)
</equation>
<bodyText confidence="0.999825888888889">
where ||B − Binit||p is an Lp regularizer and tries to
prevent parameters from deviating too much from
their initial values3.
Once again the idea here is to adjust the param-
eters (using unlabeled data) such that low-density
separation between the classes is achieved. In the
following section we will discuss the drawback of
this objective function for adaptation in realistic sce-
narios.
</bodyText>
<sectionHeader confidence="0.93405" genericHeader="method">
3 Issues with Minimum Entropy Criterion
</sectionHeader>
<bodyText confidence="0.972647023255814">
It is discussed in Section 2.2 that the model param-
eters are adapted such that a minimum conditional
entropy is achieved. It was also discussed how this is
related to finding decision boundaries through low-
density regions of input distribution. However, the
obvious assumption here is that the classes are well
separated and there in fact exists low-density regions
between classes which can be treated as boundaries.
Although this is a suitable/ideal assumption for clas-
sification, in most practical problems this assump-
tion is not satisfied and often classes overlap. There-
fore, we can not expect the conditional entropy to be
3The other reason for using a regularizer is to prevent trivial
solutions of minimum entropy criterion
convex in this situation and to achieve minimization
w.r.t parameters (other than the trivial solutions).
Let us clarify this through an example. Consider
X to be generated by mixture of two 2-D Gaus-
sians (each with a particular mean and covariance
matrix) where each Gaussian corresponds to a par-
ticular class ( binary class situation) . Also in order
to have linear decision boundaries, let the Gaussians
have same covariance matrix and let the parameter
being estimated be the prior for class 1, P(Y = 1).
Fig. 1 shows two different situations with over-
lapping classes and non-overlapping classes. The
left panel shows a distribution in which classes are
well separated whereas the right panel corresponds
to the situation where there is considerable overlap
between classes. Clearly, in the later case there is
no low-density region separating the classes. There-
fore, as we change the parameter (here, the prior on
the class Y = 1), there will not be any well defined
point with minimum entropy. This can be seen from
Fig. 2 where model conditional entropy is plotted
vs. class prior parameter for both cases. In the case
of no-overlap between classes, entropy is a convex
function w.r.t the parameter (excluding trivial solu-
tions which happens at P(Y = 1) = 0,1) and is
minimum at P(Y = 1) = 0.7 which is the true prior
with which the data was generated.
We summarize issues with minimum entropy cri-
terion and our proposed solutions as follows:
</bodyText>
<listItem confidence="0.959382153846154">
• Trivial solution: this happens when we put de-
cision boundaries such that both classes are
considered as one class (this can be avoided us-
ing the regularizer in Eqn. 3 and the assump-
tion that initial models have a reasonable solu-
tion, e.g. close to the optimal solution for new
domain)
• Overlapped Classes: As it was discussed in
this section, if the overlap is considerable then
the entropy will not be convex w.r.t to model
parameters. We will address this issue in
the next section by introducing the entropy-
stability concept.
</listItem>
<sectionHeader confidence="0.961792" genericHeader="method">
4 Entropy-Stability
</sectionHeader>
<bodyText confidence="0.9988005">
It was discussed in the previous section that a mini-
mum entropy criterion can not be used (by itself) in
</bodyText>
<page confidence="0.995704">
192
</page>
<figureCaption confidence="0.993992">
Figure 1: Mixture of two Gaussians and the corresponding Bayes decision boundary: (left) with no class overlap
(right) with class overlap
</figureCaption>
<figure confidence="0.994486869565217">
10
7
6
8
5
6
4
3
4
No
2
2
1
0
0
ï1
-2
ï2
-4
-3 -2 -1 0 1 2 3 4 5 6 7
X1
ï3
ï3 ï2 ï1 01 2 3 4 5 6 7
X1
0.035
0.03
0.025
0.02
0.015
Conditional Entropy
0.01
0.005
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
P(Y=1)
without overlap
with overlap
0.3
0.25
0.2
0.15
0.1
0.05
0
Recall: since θ is a vector of parameters, ∂He(�|X)
∂θ
</figure>
<bodyText confidence="0.952343272727273">
will be a vector and by using Lp norm Entropy-
stability will be a scalar.
The introduced concept basically measures the
stability of label entropies w.r.t the model parame-
ters. The idea is that we prefer models which not
only have low-conditional entropy but also have sta-
ble decision rules imposed by the model. Next, we
show through the following theorem how Entropy-
Stability measures the stability over posterior prob-
abilities (decision rules) of the model.
Theorem 2
</bodyText>
<figureCaption confidence="0.980163">
Figure 2: Condtional entropy vs. prior parameter, P(Y = �� ∂Hθ(Y|X) �� =
</figureCaption>
<equation confidence="0.970566111111111">
1) �� ∂θ ��
�� ��
� � � ~p
~~Z��p(x)
�
∂pθ (y |x) log pθ(y|x)!dx
∂θ
�����
y p
</equation>
<bodyText confidence="0.9999548">
situations where there is a considerable amount of
overlap among classes. Assuming that class bound-
aries happen in the regions close to the tail of class
distributions, we introduce the concept of Entropy-
Stability and show how it can be used to detect
boundary regions. Define Entropy-Stability to be the
reciprocal of the following
where the term inside the parenthesis is the weighted
sum (by log-likelihood) over the gradient of poste-
rior probabilities of labels for a given sample x
</bodyText>
<equation confidence="0.7619375">
Proof The proof is straight forward and uses the fact
that P ∂pe(y|x)_ ∂(Epe(y|x))
∂θ — = 0 .
∂θ
</equation>
<bodyText confidence="0.931297142857143">
Using Theorem 2 and Eqn. 4, it should be clear
how Entropy-Stability measures the expected sta-
bility over the posterior probabilities of the model.
A high value of ∂He e IX) implies models with
p
less stable decision rules. In order to explain how
this is used for detecting boundaries (overlapped
</bodyText>
<figure confidence="0.9931859375">
∂Hθ(Y|X)
∂θ
~~Pp(x)∂ y pθ(y |x) log pθ(y |x)) dx
� ∂θ
��
��
��
� �
������
~~~~~~p
(4)
=
��
��
��
� ~p
</figure>
<page confidence="0.994915">
193
</page>
<bodyText confidence="0.999858285714286">
regions) we once again refer back to our mixture
of Gaussians’ example. As the decision boundary
moves from class specific regions to overlapped re-
gions (by changing the parameter which is here class
prior probability) we expect the entropy to continu-
ously decrease (due to the assumption that the over-
laps occur at the tail of class distributions). How-
</bodyText>
<figure confidence="0.549726">
(a) I
</figure>
<bodyText confidence="0.999883444444444">
ever, as we get close to the overlapping regions the
added data points from other class(es) will resist
changes in the entropy. resulting in stability over the
entropy until we enter the regions specific to other
class(es).
In the following subsection we use this idea to
propose a new objective function which can be used
as an unsupervised adaptation method even for the
case of input distribution with overlapping classes.
</bodyText>
<subsectionHeader confidence="0.991984">
4.1 Better Objective Function
</subsectionHeader>
<bodyText confidence="0.945916764705882">
The idea here is to use the Entropy-Stability con-
cept to accept only regions which are close to the
overlapped parts of the distribution (based on our
assumption, these are valid regions for decision
boundaries) and then using the minimum entropy
criterion we find optimum solutions for our parame-
ters inside these regions. Therefore, we modify Eqn.
o
3 such that it also includes the Entropy-Stability
term
to particular instants of time, and arcs (edges con-
necting nodes) represent possible word hypotheses.
Associated with each arc is an acoustic likelihood
and language model likelihood scores. Fig. 3 shows
an example of recognition lattice 4 (for the purpose
ang ogni
of demonstration
</bodyText>
<equation confidence="0.538211">
�� ��� in ing likelihood
</equation>
<bodyText confidence="0.707432">
on n inscores are not ionshown).
</bodyText>
<figureCaption confidence="0.998398">
Figure 3: Lattice Example
</figureCaption>
<sectionHeader confidence="0.945673" genericHeader="method">
HAVE IT VEAL
</sectionHeader>
<bodyText confidence="0.9992315">
Since lattices contain all the likely hypotheses
(unlikely hypotheses are pruned during recognition
</bodyText>
<sectionHeader confidence="0.558655" genericHeader="method">
FAST
</sectionHeader>
<bodyText confidence="0.9808035">
and will not be included in the lattice), conditional
entropy for any given input speech signal, x, can be
</bodyText>
<figure confidence="0.904694809523809">
am l r gni ion la i an orr on ing m l i l ali gnm n r r n
approximated by the conditional entropy of the lat-
or
tice. That is,
es ris to th standad sring dit dist
Hθ(Y|X = xi) = Hθ(Y|Li)
I
HAVE
SIL
OFTEN
FINE
VEAL
SIL
SIL
SIL
VERY
MOVE
HAVE IT
HAVE
VERY
I
OFTEN
I
MOVE
VERY
HAVE
IT
VERY
FINE
SIL
SIL
VERY
FAST
θnew = argmin Hθ(Y|X) + γ �� ∂Hθ(Y|X) h �p ��
θ �� ∂θ the
�� �y ��� �p�
� �
�
+ λ ||θ − θinit||p
pot
(5)
g p p
</figure>
<bodyText confidence="0.976238928571428">
where Li is the corresponding decoded lattice (given
it inl ligt and d e bt to hth
mptd din t that liet Th ltil li
speech recognizer parameters) of utterance xi.
For the calculation of entropy we need to
stg d distane whi w will all MWE(� Whil
ment may i some cases oeretimatethe wrd error betw
know the distribution of X because Hθ(Y|X) =
s as we wil how i Section 5 it gives very similar results in prac
EX[Hθ(Y|X = x)] and since this distribution is not
known to us, we use Law of Large Numbers to ap-
The parameter γ and λ can be tuned using small
the hy
amount of labeled data (Dev set).
</bodyText>
<sectionHeader confidence="0.989102" genericHeader="method">
5 Speech Recognition Task
</sectionHeader>
<bodyText confidence="0.968225764705882">
o
In this section we will discuss how the proposed d
framework can be used in a speech recognition task.
whih
In the speech recognition task, Y is the sequence
easy t
of words and X is the input speech signal. For a
by pic
given speech signal, almost every word sequence is
We ca
a possible output and therefore there is a need for
a compact representation of output labels (words).
For this, word graphs (Lattices) are generated dur-n
(
ing the recognition process. In fact, each lattice is
an acyclic directed graph whose nodes correspond
proximate it by the empirical average
</bodyText>
<equation confidence="0.976873333333333">
p N
HB(Y|X)lPz�m−N L pθ(yJLi) logpe(y|Li)o(6)
i=1kyELi
</equation>
<bodyText confidence="0.958554235294118">
utation of word posterir probabilities. The posterior probability
Here N indicates the number of unlabeled utter-
othesis s the sum o the posterior pobabilities of all lattice path
ances for which we calculate the empirical value of
conditional entropy. Similarly, expectation w.r.t in-
od is a par Ge a aligmet a posteror paliies
put distribution in entropy-stability term is also ap-
h te hypots i t st expetd d o
th wod wh th hg poteor h pti h
proximated by the empirical average of samples.
i h h ti
e en ypo
Since the number of paths (hypotheses) in the lat-
tice is very large, it would be computationally infea-
sible to compute the conditional entropy by enumer-
i a l om r ning o la i o r mo lo ro ili or o
ating all possible paths in the lattice and calculating
</bodyText>
<footnote confidence="0.9033095">
4)
4The figure is adopted from (Mangu et al., 1999)
</footnote>
<page confidence="0.989985">
194
</page>
<table confidence="0.999623714285714">
Element hp, ri
��,„„ hp1p2, p1r2 + p2r1i
(N ®�(p„„2,r2i
�l,r1i
(N1,r1i ® (P2, r2i hp1 + p2, r1 + r2i
0 h0,0i
1 h1,0i
</table>
<tableCaption confidence="0.986051666666667">
Table 1: First-Order (Expectation) semiring: Defining
multiplication and sum operations for first-order semir-
ings.
</tableCaption>
<bodyText confidence="0.999951818181818">
their corresponding posterior probabilities. Instead
we use Finite-State Transducers (FST) to represent
the hypothesis space (lattice). To calculate entropy
and the gradient of entropy, the weights for the FST
are defined to be First- and Second-Order semirings
(Li and Eisner, 2009). The idea is to use semirings
and their corresponding operations along with the
forward-backward algorithm to calculate first- and
second-order statistics to compute entropy and the
gradient of entropy respectively. Assume we are in-
terested in calculating the entropy of the lattice,
</bodyText>
<equation confidence="0.997663125">
p(d)
Z log(p(d) Z)
1 �
= log Z −
Z
dELi
r� (7)
Z
</equation>
<bodyText confidence="0.9999808">
where Z is the total probability of all the paths in
the lattice (normalization factor). In order to do so,
we need to compute hZ, ri on the lattice. It can
be proved that if we define the first-order semir-
ing hpe, pe log pei (pe is the non-normalized score of
each arc in the lattice) as our FST weights and define
semiring operations as in Table. 1, then applying the
forward algorithm will result in the calculation of
hZ, ri as the weight (semiring weight) for the final
node.
The details for using Second-Order semirings for
calculating the gradient of entropy can be found
in (Li and Eisner, 2009). The same paper de-
scribes how to use the forward-backward algorithm
to speed-up the this procedure.
</bodyText>
<sectionHeader confidence="0.997834" genericHeader="method">
6 Language Model Adaptation
</sectionHeader>
<bodyText confidence="0.999116933333333">
Language Model Adaptation is crucial when the
training data does not match the test data being de-
coded. This is a frequent scenario for all Automatic
Speech Recognition (ASR) systems. The applica-
tion domain very often contains named entities and
N-gram sequences that are unique to the domain of
interest. For example, conversational speech has
a very different structure than class-room lectures.
Linear Interpolation based methods are most com-
monly used to adapt LMs to a new domain. As
explained in (Bacchiani et al., 2003), linear inter-
polation is a special case of Maximum A Posterior
(MAP) estimation, where an N-gram LM is built on
the adaptation data from the new domain and the two
LMs are combined using:
</bodyText>
<equation confidence="0.9974885">
p(wi|h) = λpB(wi|h) + (1 − λ)pA(wi|h)
0 &lt; λ &lt; 1
</equation>
<bodyText confidence="0.9751645">
where pB refers to out-of-domain (background)
models and pA is the adaptation (in-domain) mod-
els. Here λ is the interpolation weight.
Conventionally, λ is calculated by optimizing per-
plexity (PPL) or Word Error Rate (WER) on some
held-out data from target domain. Instead using
our proposed framework, we estimate λ on enough
amount of unlabeled data from target domain. The
idea is that resources on the new domain have al-
ready been used to build domain specific models
and it does not make sense to again use in-domain
resources for estimating the interpolation weight.
Since we are trying to just estimate one parameter
and the performance of the interpolated model is
bound by in-domain/out-of-domain models, there is
no need to include a regularization term in Eqn. 5.
��
Also �
have one parameter. Therefore, interpolation weight
will be chosen by the following criterion
</bodyText>
<equation confidence="0.9995855">
λ� = argmin Hλ(Y|X) + γ|∂Hλ(Y|X)
0&lt;λ&lt;1 ∂λ  |(8)
</equation>
<bodyText confidence="0.999816">
For the purpose of estimating one parameter λ, we
use γ = 1 in the above equation
</bodyText>
<sectionHeader confidence="0.999223" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9987108">
The large vocabulary continuous speech recognition
(LVCSR) system used throughout this paper is based
on the 2007 IBM Speech transcription system for
GALE Distillation Go/No-go Evaluation (Chen et
al., 2006). The acoustic models used in this system
</bodyText>
<equation confidence="0.978476625">
�
H(p) = −
dELi
p(d) log p(d)
= log Z −
i ∂Hg(Y|X) �� = |∂H�(Y|X)
∂θ �� ∂λ  |because we only
� �p
</equation>
<page confidence="0.996081">
195
</page>
<bodyText confidence="0.999982366666667">
are state-of-the-art discriminatively trained models
and are the same ones used for all experiments pre-
sented in this paper.
For LM adaptation experiments, the out-of-
domain LM (pB, Broadcast News LM) training
text consists of 335M words from the follow-
ing broadcast news (BN) data sources (Chen et
al., 2006): 1996 CSR Hub4 Language Model
data, EARS BN03 closed captions, GALE Phase
2 Distillation GNG Evaluation Supplemental Mul-
tilingual data, Hub4 acoustic model training tran-
scripts, TDT4 closed captions, TDT4 newswire, and
GALE Broadcast Conversations and GALE Broad-
cast News. This language model is of order 4-gram
with Kneser-Ney smoothing and contains 4.6M n-
grams based on a lexicon size of 84K.
The second source of data is the MIT lectures data
set (J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D.
Huynh, and R. Barzilay, 2007) . This serves as the
target domain (in-domain) set for language model
adaptation experiments. This set is split into 8 hours
for in-domain LM building, another 8 hours served
as unlabeled data for interpolation weight estimation
using criterion in Eqn. 8 (we refer to this as unsuper-
vised training data) and finally 2.5 hours Dev set for
estimating the interpolation weight w.r.t WER (su-
pervised tuning) . The lattice entropy and gradient
of entropy w.r.t A are calculated on the unsupervised
training data set. The results are discussed in the
next section.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.9999896">
In order to optimize the interpolation weight A based
on criterion in Eqn. 8, we devide [0, 1] to 20 differ-
ent points and evaluate the objective function (Eqn.
8) on those points. For this, we need to calculate
entropy and gradient of the entropy on the decoded
lattices of the ASR system on 8 hours of MIT lecture
set which is used as an unlabeled training data. Fig.
4 shows the value of the objective function against
different values of model parameters (interpolation
weight A). As it can be seen from this figure just
considering the conditional entropy will result in a
non-convex objective function whereas adding the
entropy-stability term will make the objective func-
tion convex. For the purpose of the evaluation, we
show the results for estimating A directly on the tran-
</bodyText>
<figure confidence="0.965331666666667">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
BN-LM MIT-LM
λ
</figure>
<figureCaption confidence="0.990689666666667">
Figure 4: Objective function with and without including
Entropy-Stability term vs. interpolation weight A on 8
hours MIT lecture unlabeled data
</figureCaption>
<bodyText confidence="0.999697066666667">
scription of the 8 hour MIT lecture data and compare
it to estimated value using our framework. The re-
sults are shown in Fig. 5. Using A = 0 and A = 1
the WERs are 24.7% and 21.1% respectively. Us-
ing the new proposed objective function, the optimal
A is estimated to be 0.6 with WER of 20.1% (Red
circle on the figure). Estimating A w.r.t 8 hour train-
ing data transcription (supervised adaptation) will
result in A = 0.7 (green circle) and WER of 20.0%.
Instead A = 0.8 will be chosen by tuning the inter-
polation weight on 2.5 hour Dev set with compara-
ble WER of 20.1%. Also it is clear from the figure
that the new objective function can be used to pre-
dict the WER trend w.r.t the interpolation weight
parameter.
</bodyText>
<figure confidence="0.5993635">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
λ
</figure>
<figureCaption confidence="0.9785475">
Figure 5: Estimating A based on WER vs. the
information-theoretic criterion
</figureCaption>
<bodyText confidence="0.788394">
Therefore, it can be seen that the new unsuper-
</bodyText>
<figure confidence="0.993834625">
Model Entropy
Model Entropy+Entropy-Stability
24.7%
Model Entropy + Entropy Stability
WER
supervised tuning
20.0%
21.1%
</figure>
<page confidence="0.995195">
196
</page>
<bodyText confidence="0.986865">
vised method results in the same performance as su-
pervised adaptation in speech recognition task.
</bodyText>
<sectionHeader confidence="0.984775" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999897107142857">
In this paper we introduced the notion of entropy
stability and presented a new criterion for unsu-
pervised adaptation which combines conditional en-
tropy minimization with entropy stability. The en-
tropy stability criterion helps in selecting parameter
settings which correspond to stable decision bound-
aries. Entropy minimization on the other hand tends
to push decision boundaries into sparse regions of
the input distributions. We show that combining
the two criterion helps to improve unsupervised pa-
rameter adaptation in real world scenario where
class conditional distributions show significant over-
lap. Although conditional entropy has been previ-
ously proposed as a regularizer, to our knowledge,
the gradient of entropy (entropy-stability) has not
been used previously in the literature. We presented
experimental results where the proposed criterion
clearly outperforms entropy minimization. For the
speech recognition task presented in this paper, the
proposed unsupervised scheme results in the same
performance as the supervised technique.
As a future work, we plan to use the proposed
criterion for adapting log-linear models used in
Machine Translation, Conditional Random Fields
(CRF) and other applications. We also plan to ex-
pand linear interpolation Language Model scheme
to include history specific (context dependent)
weights.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999568">
The Authors want to thank Markus Dreyer and
Zhifei Li for their insightful discussions and sugges-
tions.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999793685714286">
M. Bacchiani, B. Roark, and M. Saraclar. 2003. Un-
supervised language model adaptation. In Proc.
ICASSP, pages 224–227.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596–1608.
Thomas M. Cover and Joy A. Thomas. 2006. Elements
of information theory. Wiley-Interscience, 3rd edition.
Yves Grandvalet and Yoshua Bengio. 2004. Semi-
supervised learning by entropy minimization. In
Advances in neural information processing systems
(NIPS), volume 17, pages 529–536.
J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh,
and R. Barzilay. 2007. Recent progress in MIT spo-
ken lecture processing project. In Proc. Interspeech.
Jing Jiang. 2008. A literature survey on domain adapta-
tion of statistical classifiers, March.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP.
Haifeng Li, Keshu Zhang, and Tao Jiang. 2004. Min-
imum entropy clustering and applications to gene ex-
pression analysis. In Proceedings of IEEE Computa-
tional Systems Bioinformatics Conference, pages 142–
151.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 1999.
Finding consensus among words: Lattice-based word
error minimization. In Sixth European Conference on
Speech Communication and Technology.
M. Szummer and T. Jaakkola. 2003. Information regu-
larization with partially labeled data. In Advances in
Neural Information Processing Systems, pages 1049–
1056.
</reference>
<page confidence="0.998189">
197
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925648">
<title confidence="0.999013">Unsupervised Model Adaptation using Information-Theoretic Criterion</title>
<author confidence="0.988097">Frederick Abhinav</author>
<author confidence="0.988097">Bhuvana</author>
<affiliation confidence="0.9985125">Language Technology Center of Excellence, Center for Language and Speech Processing, Johns Hopkins University</affiliation>
<address confidence="0.995456">T.J. Watson Research Center, Yorktown Heights, NY, USA</address>
<abstract confidence="0.996994368421052">In this paper we propose a novel general framework for unsupervised model adaptation. Our method is based on entropy which has been used previously as a regularizer in semi-supervised learning. This technique includes another term which measures the stability of posteriors w.r.t model parameters, in addition to conditional entropy. The idea is to use parameters which result in both low conditional entropy and also stable decision rules. As an application, we demonstrate how this framework can be used for adjusting language model interpolation weight for speech recognition task to adapt from Broadcast news data to MIT lecture data. We show how the new technique can obtain comparable performance to completely supervised estimation of interpolation parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bacchiani</author>
<author>B Roark</author>
<author>M Saraclar</author>
</authors>
<title>Unsupervised language model adaptation. In</title>
<date>2003</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>224--227</pages>
<contexts>
<context position="19773" citStr="Bacchiani et al., 2003" startWordPosition="3364" endWordPosition="3367">o use the forward-backward algorithm to speed-up the this procedure. 6 Language Model Adaptation Language Model Adaptation is crucial when the training data does not match the test data being decoded. This is a frequent scenario for all Automatic Speech Recognition (ASR) systems. The application domain very often contains named entities and N-gram sequences that are unique to the domain of interest. For example, conversational speech has a very different structure than class-room lectures. Linear Interpolation based methods are most commonly used to adapt LMs to a new domain. As explained in (Bacchiani et al., 2003), linear interpolation is a special case of Maximum A Posterior (MAP) estimation, where an N-gram LM is built on the adaptation data from the new domain and the two LMs are combined using: p(wi|h) = λpB(wi|h) + (1 − λ)pA(wi|h) 0 &lt; λ &lt; 1 where pB refers to out-of-domain (background) models and pA is the adaptation (in-domain) models. Here λ is the interpolation weight. Conventionally, λ is calculated by optimizing perplexity (PPL) or Word Error Rate (WER) on some held-out data from target domain. Instead using our proposed framework, we estimate λ on enough amount of unlabeled data from target </context>
</contexts>
<marker>Bacchiani, Roark, Saraclar, 2003</marker>
<rawString>M. Bacchiani, B. Roark, and M. Saraclar. 2003. Unsupervised language model adaptation. In Proc. ICASSP, pages 224–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>B Kingsbury</author>
<author>L Mangu</author>
<author>D Povey</author>
<author>G Saon</author>
<author>H Soltau</author>
<author>G Zweig</author>
</authors>
<date>2006</date>
<booktitle>Advances in speech transcription at IBM under the DARPA EARS program. IEEE Transactions on Audio, Speech and Language Processing,</booktitle>
<pages>1596--1608</pages>
<contexts>
<context position="21245" citStr="Chen et al., 2006" startWordPosition="3612" endWordPosition="3615">meter and the performance of the interpolated model is bound by in-domain/out-of-domain models, there is no need to include a regularization term in Eqn. 5. �� Also � have one parameter. Therefore, interpolation weight will be chosen by the following criterion λ� = argmin Hλ(Y|X) + γ|∂Hλ(Y|X) 0&lt;λ&lt;1 ∂λ |(8) For the purpose of estimating one parameter λ, we use γ = 1 in the above equation 7 Experimental Setup The large vocabulary continuous speech recognition (LVCSR) system used throughout this paper is based on the 2007 IBM Speech transcription system for GALE Distillation Go/No-go Evaluation (Chen et al., 2006). The acoustic models used in this system � H(p) = − dELi p(d) log p(d) = log Z − i ∂Hg(Y|X) �� = |∂H�(Y|X) ∂θ �� ∂λ |because we only � �p 195 are state-of-the-art discriminatively trained models and are the same ones used for all experiments presented in this paper. For LM adaptation experiments, the out-ofdomain LM (pB, Broadcast News LM) training text consists of 335M words from the following broadcast news (BN) data sources (Chen et al., 2006): 1996 CSR Hub4 Language Model data, EARS BN03 closed captions, GALE Phase 2 Distillation GNG Evaluation Supplemental Multilingual data, Hub4 acousti</context>
</contexts>
<marker>Chen, Kingsbury, Mangu, Povey, Saon, Soltau, Zweig, 2006</marker>
<rawString>S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon, H. Soltau, and G. Zweig. 2006. Advances in speech transcription at IBM under the DARPA EARS program. IEEE Transactions on Audio, Speech and Language Processing, pages 1596–1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of information theory. Wiley-Interscience, 3rd edition.</title>
<date>2006</date>
<contexts>
<context position="4542" citStr="Cover and Thomas, 2006" startWordPosition="700" endWordPosition="703">ditional entropy is a measure of the class overlap and is calculated as follows H(Y|X) = EX[H(Y|X = x)] = − f p(x) (1:y �p(y|x)log p(y|x) dx (1) Through Fano’s Inequality theorem, one can see how conditional entropy is related to classification performance. Theorem 1 (Fano’s Inequality) Suppose Pe = P{ Y� =6 Y} where Y� = g(X) are the assigned labels for the data points, based on the classification rule. Then H(Y|X) − 1 Pe ≥ log(|Y |− 1) where Y is the number of possible classes and H(Y |X) is the conditional entropy with respect to true distibution. The proof to this theorem can be found in (Cover and Thomas, 2006). This inequality indicates that Y can be estimated with low probability of error only if the conditional entropy H(Y|X) is small. Although the above theorem is useful in a sense that it connects the classification problem to Shannon entropy, the true distributions are almost never known to us1. In most classification methods, a specific model structure for the distributions is assumed and the task is to estimate the model parameters within the assumed model space. Given the model 1In fact, Theorem 1 shows how relevant the input features are for the classification task by putting a lower bound</context>
</contexts>
<marker>Cover, Thomas, 2006</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 2006. Elements of information theory. Wiley-Interscience, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Grandvalet</author>
<author>Yoshua Bengio</author>
</authors>
<title>Semisupervised learning by entropy minimization.</title>
<date>2004</date>
<booktitle>In Advances in neural information processing systems (NIPS),</booktitle>
<volume>17</volume>
<pages>529--536</pages>
<contexts>
<context position="2857" citStr="Grandvalet and Bengio, 2004" startWordPosition="433" endWordPosition="436">el adaptation can be found in (Jiang, 2008). Information regularization framework has been previously proposed in literature to control the label conditional probabilities via input distribution (Szummer and Jaakkola, 2003). The idea is that labels should not change too much in dense regions of the input distribution. The authors use the mutual information between input features and labels as a measure of label complexity. Another framework previously suggested is to use label entropy (conditional entropy) on unlabeled data as a regularizer to Maximum Likelihood (ML) training on labeled data (Grandvalet and Bengio, 2004). Availability of resources for the target domain categorizes these techniques into either supervised or unsupervised. In this paper we propose a general framework for unsupervised adaptation using Shannon entropy and stability of entropy. The assumption is that in-domain and out-of-domain distributions are not too different such that one can improve the performance of initial models on in-domain data by little adjustment of initial decision boundaries (learned on out-of-domain data). 190 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19</context>
<context position="6064" citStr="Grandvalet and Bengio, 2004" startWordPosition="950" endWordPosition="953"> 1 Y�=6Y |θ} ≥ Hθ(Y|X) − 1 log(|Y |− 1) (2) where Pe(θ) is the classifier probability of error given model parameters, θ and Hθ(Y|X) = − f p(x) (1:y �pθ(y|x) log pθ(y|x) dx Here, Hθ(Y|X) is the conditional entropy imposed by model parameters. Eqn. 2 indicates the fact that models with low conditional entropy are preferable. However, a low entropy model does not necessarily have good performance (this will be reviewed later on) 2 2.2 Objective Function Minimization of conditional entropy as a framework in the classification task is not a new concept and has been tried by researchers. In fact, (Grandvalet and Bengio, 2004) use this along with the maximum likelihood criterion in a semi-supervised set up such that parameters with both maximum likelihood on labeled data and minimum conditional entropy on unlabeled data are chosen. By minimizing the entropy, the method assumes a prior which prefers minimal class overlap. Entropy minimization is used in (Li et al., 2004) as an unsupervised non-parametric clustering method and is shown to result in significant improvement over k-mean, hierarchical clustering and etc. These methods are all based on the fact that models with low conditional entropy have their decision </context>
</contexts>
<marker>Grandvalet, Bengio, 2004</marker>
<rawString>Yves Grandvalet and Yoshua Bengio. 2004. Semisupervised learning by entropy minimization. In Advances in neural information processing systems (NIPS), volume 17, pages 529–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
<author>T Hazen</author>
<author>S Cyphers</author>
<author>I Malioutov</author>
<author>D Huynh</author>
<author>R Barzilay</author>
</authors>
<title>Recent progress in MIT spoken lecture processing project.</title>
<date>2007</date>
<booktitle>In Proc. Interspeech.</booktitle>
<marker>Glass, Hazen, Cyphers, Malioutov, Huynh, Barzilay, 2007</marker>
<rawString>J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh, and R. Barzilay. 2007. Recent progress in MIT spoken lecture processing project. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
</authors>
<title>A literature survey on domain adaptation of statistical classifiers,</title>
<date>2008</date>
<contexts>
<context position="2272" citStr="Jiang, 2008" startWordPosition="346" endWordPosition="347">awn from same distribution) with test data. It is often the case where we have plenty of labeled data for one specific domain/genre (source domain) and little amount of labeled data (or no labeled data at all) for the desired domain/genre (target domain). Model adaptation techniques are commonly used to address this problem. Model adaptation starts with trained models (trained on source domain with rich amount of labeled data) and then modify them using the available labeled data from target domain (or instead unlabeled data). A survey on different methods of model adaptation can be found in (Jiang, 2008). Information regularization framework has been previously proposed in literature to control the label conditional probabilities via input distribution (Szummer and Jaakkola, 2003). The idea is that labels should not change too much in dense regions of the input distribution. The authors use the mutual information between input features and labels as a measure of label complexity. Another framework previously suggested is to use label entropy (conditional entropy) on unlabeled data as a regularizer to Maximum Likelihood (ML) training on labeled data (Grandvalet and Bengio, 2004). Availability </context>
</contexts>
<marker>Jiang, 2008</marker>
<rawString>Jing Jiang. 2008. A literature survey on domain adaptation of statistical classifiers, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="18181" citStr="Li and Eisner, 2009" startWordPosition="3092" endWordPosition="3095">ll possible paths in the lattice and calculating 4) 4The figure is adopted from (Mangu et al., 1999) 194 Element hp, ri ��,„„ hp1p2, p1r2 + p2r1i (N ®�(p„„2,r2i �l,r1i (N1,r1i ® (P2, r2i hp1 + p2, r1 + r2i 0 h0,0i 1 h1,0i Table 1: First-Order (Expectation) semiring: Defining multiplication and sum operations for first-order semirings. their corresponding posterior probabilities. Instead we use Finite-State Transducers (FST) to represent the hypothesis space (lattice). To calculate entropy and the gradient of entropy, the weights for the FST are defined to be First- and Second-Order semirings (Li and Eisner, 2009). The idea is to use semirings and their corresponding operations along with the forward-backward algorithm to calculate first- and second-order statistics to compute entropy and the gradient of entropy respectively. Assume we are interested in calculating the entropy of the lattice, p(d) Z log(p(d) Z) 1 � = log Z − Z dELi r� (7) Z where Z is the total probability of all the paths in the lattice (normalization factor). In order to do so, we need to compute hZ, ri on the lattice. It can be proved that if we define the first-order semiring hpe, pe log pei (pe is the non-normalized score of each </context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haifeng Li</author>
<author>Keshu Zhang</author>
<author>Tao Jiang</author>
</authors>
<title>Minimum entropy clustering and applications to gene expression analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of IEEE Computational Systems Bioinformatics Conference,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="6414" citStr="Li et al., 2004" startWordPosition="1010" endWordPosition="1013">t necessarily have good performance (this will be reviewed later on) 2 2.2 Objective Function Minimization of conditional entropy as a framework in the classification task is not a new concept and has been tried by researchers. In fact, (Grandvalet and Bengio, 2004) use this along with the maximum likelihood criterion in a semi-supervised set up such that parameters with both maximum likelihood on labeled data and minimum conditional entropy on unlabeled data are chosen. By minimizing the entropy, the method assumes a prior which prefers minimal class overlap. Entropy minimization is used in (Li et al., 2004) as an unsupervised non-parametric clustering method and is shown to result in significant improvement over k-mean, hierarchical clustering and etc. These methods are all based on the fact that models with low conditional entropy have their decision boundaries passing through low-density regions of the input distribution, P(X). This is consistent with the assumption that classes are well separated so that one can expect to take advantage of unlabeled examples (Grandvalet and Bengio, 2004). In many cases shifting from one domain to another domain, initial trained decision boundaries (on 2Imagin</context>
</contexts>
<marker>Li, Zhang, Jiang, 2004</marker>
<rawString>Haifeng Li, Keshu Zhang, and Tao Jiang. 2004. Minimum entropy clustering and applications to gene expression analysis. In Proceedings of IEEE Computational Systems Bioinformatics Conference, pages 142– 151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Brill</author>
<author>Andreas Stolcke</author>
</authors>
<title>Finding consensus among words: Lattice-based word error minimization.</title>
<date>1999</date>
<booktitle>In Sixth European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="17661" citStr="Mangu et al., 1999" startWordPosition="3011" endWordPosition="3014">ch we calculate the empirical value of conditional entropy. Similarly, expectation w.r.t inod is a par Ge a aligmet a posteror paliies put distribution in entropy-stability term is also aph te hypots i t st expetd d o th wod wh th hg poteor h pti h proximated by the empirical average of samples. i h h ti e en ypo Since the number of paths (hypotheses) in the lattice is very large, it would be computationally infeasible to compute the conditional entropy by enumeri a l om r ning o la i o r mo lo ro ili or o ating all possible paths in the lattice and calculating 4) 4The figure is adopted from (Mangu et al., 1999) 194 Element hp, ri ��,„„ hp1p2, p1r2 + p2r1i (N ®�(p„„2,r2i �l,r1i (N1,r1i ® (P2, r2i hp1 + p2, r1 + r2i 0 h0,0i 1 h1,0i Table 1: First-Order (Expectation) semiring: Defining multiplication and sum operations for first-order semirings. their corresponding posterior probabilities. Instead we use Finite-State Transducers (FST) to represent the hypothesis space (lattice). To calculate entropy and the gradient of entropy, the weights for the FST are defined to be First- and Second-Order semirings (Li and Eisner, 2009). The idea is to use semirings and their corresponding operations along with the</context>
</contexts>
<marker>Mangu, Brill, Stolcke, 1999</marker>
<rawString>Lidia Mangu, Eric Brill, and Andreas Stolcke. 1999. Finding consensus among words: Lattice-based word error minimization. In Sixth European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Szummer</author>
<author>T Jaakkola</author>
</authors>
<title>Information regularization with partially labeled data.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1049--1056</pages>
<contexts>
<context position="2452" citStr="Szummer and Jaakkola, 2003" startWordPosition="367" endWordPosition="370"> of labeled data (or no labeled data at all) for the desired domain/genre (target domain). Model adaptation techniques are commonly used to address this problem. Model adaptation starts with trained models (trained on source domain with rich amount of labeled data) and then modify them using the available labeled data from target domain (or instead unlabeled data). A survey on different methods of model adaptation can be found in (Jiang, 2008). Information regularization framework has been previously proposed in literature to control the label conditional probabilities via input distribution (Szummer and Jaakkola, 2003). The idea is that labels should not change too much in dense regions of the input distribution. The authors use the mutual information between input features and labels as a measure of label complexity. Another framework previously suggested is to use label entropy (conditional entropy) on unlabeled data as a regularizer to Maximum Likelihood (ML) training on labeled data (Grandvalet and Bengio, 2004). Availability of resources for the target domain categorizes these techniques into either supervised or unsupervised. In this paper we propose a general framework for unsupervised adaptation usi</context>
</contexts>
<marker>Szummer, Jaakkola, 2003</marker>
<rawString>M. Szummer and T. Jaakkola. 2003. Information regularization with partially labeled data. In Advances in Neural Information Processing Systems, pages 1049– 1056.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>