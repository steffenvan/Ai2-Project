<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018771">
<title confidence="0.949238">
Finding Common Ground: Towards a Surface Realisation Shared Task
</title>
<author confidence="0.985671">
Anja Belz
</author>
<affiliation confidence="0.996429">
Natural Language Technology Group
Computing, Mathematical and Information Sciences
University of Brighton, Brighton BN2 4GJ, UK
</affiliation>
<email confidence="0.983611">
a.s.belz@brighton.ac.uk
</email>
<author confidence="0.992212">
Mike White
</author>
<affiliation confidence="0.994672">
Department of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.827654">
Columbus, OH, USA
</address>
<email confidence="0.971667">
mwhite@ling.osu.edu
</email>
<author confidence="0.491739">
Amanda Stent
</author>
<affiliation confidence="0.501541">
AT&amp;T Labs Research, Inc.,
</affiliation>
<address confidence="0.972933">
180 Park Avenue
Florham Park, NJ 07932, USA
</address>
<email confidence="0.971635">
stent@research.att.com
</email>
<author confidence="0.757932">
Josef van Genabith and Deirdre Hogan
</author>
<affiliation confidence="0.985072333333333">
National Centre for Language Technology
School of Computing
Dublin City University
</affiliation>
<address confidence="0.692402">
Dublin 9, Ireland
</address>
<email confidence="0.996559">
{dhogan,josef}@computing.dcu.ie
</email>
<sectionHeader confidence="0.998583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999928">
In many areas of NLP reuse of utility tools
such as parsers and POS taggers is now
common, but this is still rare in NLG. The
subfield of surface realisation has perhaps
come closest, but at present we still lack
a basis on which different surface realis-
ers could be compared, chiefly because of
the wide variety of different input repre-
sentations used by different realisers. This
paper outlines an idea for a shared task in
surface realisation, where inputs are pro-
vided in a common-ground representation
formalism which participants map to the
types of input required by their system.
These inputs are derived from existing an-
notated corpora developed for language
analysis (parsing etc.). Outputs (realisa-
tions) are evaluated by automatic compari-
son against the human-authored text in the
corpora as well as by human assessors.
</bodyText>
<sectionHeader confidence="0.995686" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999896767676768">
When reading a paper reporting a new NLP sys-
tem, it is common these days to find that the
authors have taken an NLP utility tool off the
shelf and reused it. Researchers frequently reuse
parsers, POS-taggers, named entity recognisers,
coreference resolvers, and many other tools. Not
only is there a real choice between a range of dif-
ferent systems performing the same task, there are
also evaluation methodologies to help determine
what the state of the art is.
Natural Language Generation (NLG) has not
so far developed generic tools and methods for
comparing them to the same extent as Natural
Language Analysis (NLA) has. The subfield of
NLG that has perhaps come closest to developing
generic tools is surface realisation. Wide-coverage
surface realisers such as PENMAN/NIGEL (Mann
and Mathiesen, 1983), FUF/SURGE (Elhadad and
Robin, 1996) and REALPRO (Lavoie and Ram-
bow, 1997) were intended to be more or less off-
the-shelf plug-and-play modules. But they tended
to require a significant amount of work to adapt
and integrate, and required highly specific inputs
incorporating up to several hundred features that
needed to be set.
With the advent of statistical techniques in NLG
surface realisers appeared for which it was far sim-
pler to supply inputs, as information not provided
in the inputs could be added on the basis of like-
lihood. An early example, the Japan-Gloss sys-
tem (Knight et al., 1995) replaced PENMAN’s de-
fault settings with statistical decisions. The Halo-
gen/Nitrogen developers (Langkilde and Knight,
1998a) allowed inputs to be arbitrarily underspec-
ified, and any decision not made before the realiser
was decided simply by highest likelihood accord-
ing to a language model, automatically trainable
from raw corpora.
The Halogen/Nitrogen work sparked an interest
in statistical NLG which led to a range of surface
realisation methods that used corpus frequencies
in one way or another (Varges and Mellish, 2001;
White, 2004; Velldal et al., 2004; Paiva and Evans,
2005). Some surface realisation work looked at
directly applying statistical models during a lin-
guistically informed generation process to prune
the search space (White, 2004; Carroll and Oepen,
2005).
While statistical techniques have led to realisers
that are more (re)usable, we currently still have
no way of determining what the state of the art
is. A significant subset of statistical realisation
work (Langkilde, 2002; Callaway, 2003; Nakan-
ishi et al., 2005; Zhong and Stent, 2005; Cahill and
van Genabith, 2006; White and Rajkumar, 2009)
has recently produced results for regenerating the
Penn Treebank. The basic approach in all this
work is to remove information from the Penn Tree-
bank parses (the word strings themselves as well
as some of the parse information), and then con-
vert and use these underspecified representations
as inputs to the surface realiser whose task it is to
reproduce the original treebank sentence. Results
are typically evaluated using BLEU, and, roughly
speaking, BLEU scores go down as more informa-
tion is removed.
While publications of work along these lines do
refer to each other and (tentatively) compare BLEU
scores, the results are not in fact directly compara-
ble, because of the differences in the input repre-
sentations automatically derived from Penn Tree-
bank annotations. In particular, the extent to which
they are underspecified varies from one system to
the next.
The idea we would like to put forward with
this short paper is to develop a shared task in sur-
face realisation based on common inputs and an-
notated corpora of paired inputs and outputs de-
rived from various resources from NLA that build
on the Penn Treebank. Inputs are provided in a
common-ground representation formalism which
participants map to the types of input required by
their system. These inputs are automatically de-
rived from the Penn Treebank and the various lay-
ers of annotation (syntactic, semantic, discourse)
that have been developed for the documents in it.
Outputs (realisations) are evaluated by automatic
comparison against the human-authored text in the
corpora as well as by by human assessors.
In the short term, such a shared task would
make existing and new approaches directly com-
parable by evaluation on the benchmark data asso-
ciated with the shared task. In the long term, the
common-ground input representation may lead to
a standardised level of representation that can act
as a link between surface realisers and preceding
modules, and can make it possible to use alterna-
tive surface realisers as drop-in replacements for
each other.
</bodyText>
<sectionHeader confidence="0.950807" genericHeader="introduction">
2 Towards Common Inputs
</sectionHeader>
<bodyText confidence="0.999970630434783">
One hugely challenging aspect in developing a
Surface Realisation task is developing a common
input representation that all, or at least a major-
ity of, surface realisation researchers are happy to
work with. While many different formalisms have
been used for input representations to surface re-
alisers, one cannot simply use e.g. van Genabith
et al.’s automatically generated LFG f-structures,
White et al’s CCG logical forms, Nivre’s depen-
dencies, Miyao et al.’s HPSG predicate-argument
structures or Copestake’s MRSs etc., as each of
them would introduce a bias in favour of one type
of system.
One possible solution is to develop a meta-
representation which contains, perhaps on multi-
ple layers of representation, all the information
needed to map to any of a given set of realiser in-
put representations, a common-ground representa-
tion that acts as a kind of interlingua for translating
between different input representations.
An important issue in deriving input repre-
sentations from semantically, syntactically and
discourse-annotated corpora is deciding what in-
formation not to include. A concern is that mak-
ing such decisions by committee may be difficult.
One way to make it easier might be to define sev-
eral versions of the task, where each version uses
inputs of different levels of specificity.
Basing a common input representation on what
can feasibly be obtained from non-NLG resources
would put everyone on reasonably common foot-
ing. If, moreover, the common input representa-
tions can be automatically derived from annota-
tions in existing resources, then data can be pro-
duced in sufficient quantities to make it feasible
for participants to automatically learn mappings
from the system-neutral input to their own input.
The above could be achieved by doing some-
thing along the lines of the CoNLL’08 shared task
on Joint Parsing of Syntactic and Semantic De-
pendencies, for which the organisers combined the
Penn Treebank, Propbank, Nombank and the BBN
Named Entity corpus into a dependency represen-
tation. Brief descriptions of these resources and
more details on this idea are provided in Section 4
below.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.99999275862069">
As many NLG researchers have argued, there is
usually not a single right answer in NLG, but var-
ious answers, some better than others, and NLG
tasks should take this into account. If a surface
realisation task is focused on single-best realiza-
tions, then it will not encourage research on pro-
ducing all possible good realizations, or multiple
acceptable realizations in a ranked list, etc. It
may not be the best approach to encourage sys-
tems that try to make a single, safe choice; in-
stead, perhaps one should encourage approaches
that can tell when multiple choices would be ok,
and if some would be better than others.
In the long term we need to develop task defi-
nitions, data resources and evaluation methodolo-
gies that properly take into account the one-to-
many nature of NLG, but in the short term it may be
more realistic to reuse existing non-NLG resources
(which do not provide alternative realisations) and
to adapt existing evaluation methodologies includ-
ing intrinsic assessment of Fluency, Clarity and
Appropriateness by trained evaluators, and auto-
matic intrinsic methods such as BLEU and NIST.
One simple way of adapting the latter, for exam-
ple, could be to calculate scores for the n best re-
alisations produced by a realiser and then to com-
pute a weighted average where scores for reali-
sations are weighted in inverse proportion to the
ranks given to the realisations by the realiser.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.988116625">
There is a wide variety of different annotated re-
sources that could be of use in a shared task in sur-
face realisation. Many of these include documents
originally included in the Penn Treebank, and thus
make it possible in principle to combine the var-
ious levels of annotation into a single common-
ground representation. The following is a (non-
exhaustive) list of such resources:
</bodyText>
<listItem confidence="0.9875974">
1. Penn Treebank-3 (Marcus et al., 1999): one
million words of hand-parsed 1989 Wall
Street Journal material annotated in Treebank
II style. The Treebank bracketing style al-
lows extraction of simple predicate/argument
structure. In addition to Treebank-1 mate-
rial, Treebank-3 contains documents from the
Switchboard and Brown corpora.
2. Propbank (Palmer et al., 2005): This is a se-
mantic annotation of the Wall Street Journal
</listItem>
<bodyText confidence="0.8676106">
section of Penn Treebank-2. More specifi-
cally, each verb occurring in the Treebank has
been treated as a semantic predicate and the
surrounding text has been annotated for ar-
guments and adjuncts of the predicate. The
verbs have also been tagged with coarse
grained senses and with inflectional informa-
tion.
3. NomBank 1.0 (Meyers et al., 2004): Nom-
Bank is an annotation project at New York
University that provides argument structure
for common nouns in the Penn Treebank.
NomBank marks the sets of arguments that
occur with nouns in PropBank I, just as the
latter records such information for verbs.
</bodyText>
<listItem confidence="0.90879075">
4. BBN Pronoun Coreference and Entity Type
Corpus (Weischedel and Brunstein, 2005):
supplements the Wall Street Journal corpus,
adding annotation of pronoun coreference,
and a variety of entity and numeric types.
5. FrameNet (Johnson et al., 2002): 150,000
sentences annotated for semantic roles and
possible syntactic realisations. The annotated
sentences come from a variety of sources, in-
cluding some PropBank texts.
6. OntoNotes 2.0 (Weischedel et al., 2008):
OntoNotes 1.0 contains 674k words of Chi-
nese and 500k words of English newswire
and broadcast news data. OntoNotes follows
the Penn Treebank for syntax and PropBank
for predicate-argument structure. Its seman-
</listItem>
<bodyText confidence="0.975014">
tic representation will include word sense
disambiguation for nouns and verbs, with
each word sense connected to an ontology,
and coreference. The current goal is to anno-
tate over a million words each of English and
Chinese, and half a million words of Arabic
over five years.
There are other resources which may be use-
ful. Zettelmoyer and Collins (2009) have man-
ually converted the original SQL meaning an-
notations of the ATIS corpus (et al., 1994)—
some 4,637 sentences—into lambda-calculus ex-
pressions which were used for training and testing
their semantic parser. This resource might make a
good out-of-domain test set for generation systems
trained on WSJ data.
FrameNet, used for semantic parsing, see for
example Gildea and Jurafsky (2002), identifies a
sentence’s frame elements and assigns semantic
roles to the frame elements. FrameNet data (Baker
and Sato, 2003) was used for training and test sets
in one of the SensEval-3 shared tasks in 2004 (Au-
tomatic Labeling of Semantic Roles). There has
been some work combining FrameNet with other
lexical resources. For example, Shi and Mihal-
cea (2005) integrated FrameNet with VerbNet and
WordNet for the purpose of enabling more robust
semantic parsing.
The Semlink project (http://verbs.colorado.
edu/semlink/) aims to integrate Propbank,
FrameNet, WordNet and VerbNet.
Other relevant work includes Moldovan and
Rus (Moldovan and Rus, 2001; Rus, 2002) who
developed a technique for parsing into logical
forms and used this to transform WordNet concept
definitions into logical forms. The same method
(with additional manual correction) was used to
produce the test set for another SensEval-3 shared
task (Identification of Logic Forms in English).
</bodyText>
<subsectionHeader confidence="0.994186">
4.1 CoNLL 2008 Shared Task Data
</subsectionHeader>
<bodyText confidence="0.999950166666667">
Perhaps the most immediately promising resource
is is the CoNLL shared task data from 2008 (Sur-
deanu et al., 2008) which has syntactic depen-
dency annotations, named-entity boundaries and
the semantic dependencies model roles of both
verbal and nominal predicates. The data consist
of excerpts from Penn Treebank-3, BBN Pronoun
Coreference and Entity Type Corpus, PropBank I
and NomBank 1.0. In CoNLL ’08, the data was
used to train and test systems for the task of pro-
ducing a joint semantic and syntactic dependency
analysis of English sentences (the 2009 CoNLL
Shared Task extended this to multi-lingual data).
It seems feasible that we could reuse the CoNLL
data for a prototype Surface Realisation task,
adapting it and inversing the direction of the task,
i.e. mapping from syntactic-semantic dependency
representations to word strings.
</bodyText>
<sectionHeader confidence="0.960375" genericHeader="method">
5 Developing the Task
</sectionHeader>
<bodyText confidence="0.99997215">
The first step in developing a Surface Realisa-
tion task could be to get together a working
group of surface realisation researchers to develop
a common-ground input representation automati-
cally derivable from a set of existing resources.
As part of this task a prototype corpus exempli-
fying inputs/outputs and annotations could be de-
veloped. At the end of this stage it would be use-
ful to write a white paper and circulate it and the
prototype corpus among the NLG (and wider NLP)
community for feedback and input.
After a further stage of development, it may be
feasible to run a prototype surface realisation task
at Generation Challenges 2011, combined with a
session for discussion and roadmapping. Depend-
ing on the outcome of all of this, a full-blown task
might be feasible by 2012. Some of this work will
need funding to be feasible, and the authors of this
paper are in the process of applying for financial
support for these plans.
</bodyText>
<sectionHeader confidence="0.961308" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.99916385">
In this paper we have provided an overview of ex-
isting resources that could potentially be used for
a surface realisation task, and have outlined ideas
for how such a task might work. The core idea
is to develop a common-ground input representa-
tion which participants map to the types of input
required by their system. These inputs are derived
from existing annotated corpora developed for lan-
guage analysis. Outputs (realisations) are evalu-
ated by automatic comparison against the human-
authored text in the corpora as well as by by hu-
man assessors. Evaluation methods are adapted to
take account of the one-to-many nature of the re-
alisation mapping.
The ideas outlined in this paper began as a pro-
longed email exchange, interspersed with discus-
sions at conferences, among the authors. This pa-
per summarises our ideas as they have evolved so
far, to enable feedback and input from other re-
searchers interested in this type of task.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993350294117647">
Colin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proceedings of ACL’03.
A. Cahill and J. van Genabith. 2006. Robust PCFG-
based generation using automatically acquired LFG
approximations. In Proc. ACL’06, pages 1033–44.
Charles Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proceedings of the
18th International Joint Conference on Artificial In-
telligence (IJCAI2003), pages 811–817.
J. Carroll and S. Oepen. 2005. High efficiency
realization for a wide-coverage unification gram-
mar. In Proceedings of the 2nd International Joint
Conference on Natural Language Processing (IJC-
NLP’05), volume 3651, pages 165–176. Springer
Lecture Notes in Artificial Intelligence.
M. Elhadad and J. Robin. 1996. An overview of
SURGE: A reusable comprehensive syntactic real-
ization component. Technical Report 96-03, Dept
of Mathematics and Computer Science, Ben Gurion
University, Beer Sheva, Israel.
Deborah Dahl et al. 1994. Expanding the scope of the
ATIS task: the ATIS-3 corpus. In Proceedings of the
ARPA HLT Workshop.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
C. Johnson, C. Fillmore, M. Petruck, C. Baker,
M. Ellsworth, J. Ruppenhoper, and E.Wood. 2002.
Framenet theory and practice. Technical report.
K. Knight, I. Chander, M. Haines, V. Hatzivassiloglou,
E. Hovy, M. Iida, S. Luk, R. Whitney, and K. Ya-
mada. 1995. Filling knowledge gaps in a broad-
coverage MT system. In Proceedings of the Four-
teenth International Joint Conference on Artificial
Intelligence (IJCAI ’95), pages 1390–1397.
I. Langkilde and K. Knight. 1998a. Generation
that exploits corpus-based statistical knowledge. In
Proc. COLING-ACL. http://www.isi.edu/licensed-
sw/halogen/nitro98.ps.
I. Langkilde. 2002. An empirical verification of cover-
age and correctness for a general-purpose sentence
generator. In Proc. 2nd International Natural Lan-
guage Generation Conference (INLG ’02).
B. Lavoie and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the 5th Conference on Applied Natural Language
Processing (ANLP’97), pages 265–268.
W. Mann and C. Mathiesen. 1983. NIGEL: A sys-
temic grammar for text generation. Technical Re-
port ISI/RR-85-105, Information Sciences Institute.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Technical report, Linguistic Data Consortium,
Philadelphia.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2004. Np-external arguments a study of argument
sharing in english. In MWE ’04: Proceedings of
the Workshop on Multiword Expressions, pages 96–
103, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Dan I. Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL’01.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic models for disambiguation of an
hpsg-based chart generator. In Proceedings of the
9th International Workshop on Parsing Technology
(Parsing’05), pages 93–102. Association for Com-
putational Linguistics.
D. S. Paiva and R. Evans. 2005. Empirically-based
control of natural language generation. In Proceed-
ings ACL’05.
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
Vasile Rus. 2002. Logic Form For WordNet Glosses
and Application to Question Answering. Ph.D. the-
sis.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In Proceedings of CI-
CLing’05.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL ’08:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159–177.
S. Varges and C. Mellish. 2001. Instance-based natu-
ral language generation. In Proceedings of the 2nd
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL ’01),
pages 1–8.
E. Velldal, S. Oepen, and D. Flickinger. 2004. Para-
phrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Tree-
banks and Linguistic Theories (TLT ’04), Tuebin-
gen, Germany.
Ralph Weischedel and Ada Brunstein. 2005. Bbn pro-
noun coreference and entity type corpus. Technical
report, Linguistic Data Consortium.
Ralph Weischedel et al. 2008. Ontonotes release 2.0.
Technical report, Linguistic Data Consortium.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realisation. In Pro-
ceedings of the 2009 Conference on Empririal Meth-
ods in Natural Language Processing (EMNLP’09),
pages 410–419.
M. White. 2004. Reining in CCG chart realization. In
A. Belz, R. Evans, and P. Piwek, editors, Proceed-
ings INLG’04, volume 3123 of LNAI, pages 182–
191. Springer.
Luke Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical forms. In Proceedings of ACL-IJCNLP’09.
H. Zhong and A. Stent. 2005. Building surface
realizers automatically from corpora. In A. Belz
and S. Varges, editors, Proceedings of UCNLG’05,
pages 49–54.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.999565">Finding Common Ground: Towards a Surface Realisation Shared Task</title>
<author confidence="0.608661">Anja</author>
<affiliation confidence="0.913364666666667">Natural Language Technology Computing, Mathematical and Information University of Brighton, Brighton BN2 4GJ,</affiliation>
<email confidence="0.993323">a.s.belz@brighton.ac.uk</email>
<author confidence="0.985049">Mike</author>
<affiliation confidence="0.979705">Department of The Ohio State</affiliation>
<address confidence="0.620385">Columbus, OH,</address>
<email confidence="0.998892">mwhite@ling.osu.edu</email>
<author confidence="0.918356">Amanda</author>
<affiliation confidence="0.986431">AT&amp;T Labs Research,</affiliation>
<address confidence="0.9976315">180 Park Florham Park, NJ 07932,</address>
<email confidence="0.999584">stent@research.att.com</email>
<author confidence="0.998702">van_Genabith</author>
<affiliation confidence="0.987368333333333">National Centre for Language School of Dublin City</affiliation>
<address confidence="0.6548">Dublin 9,</address>
<abstract confidence="0.991842572237961">many areas of of utility tools as parsers and is now but this is still rare in The subfield of surface realisation has perhaps come closest, but at present we still lack a basis on which different surface realisers could be compared, chiefly because of the wide variety of different input representations used by different realisers. This paper outlines an idea for a shared task in surface realisation, where inputs are provided in a common-ground representation formalism which participants map to the types of input required by their system. These inputs are derived from existing annotated corpora developed for language analysis (parsing etc.). Outputs (realisations) are evaluated by automatic comparison against the human-authored text in the corpora as well as by human assessors. 1 Background reading a paper reporting a new system, it is common these days to find that the have taken an tool off the shelf and reused it. Researchers frequently reuse named entity recognisers, coreference resolvers, and many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Language Generation has not so far developed generic tools and methods for comparing them to the same extent as Natural Analysis has. The subfield of has perhaps come closest to developing generic tools is surface realisation. Wide-coverage realisers such as Mathiesen, 1983), and 1996) and and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. the advent of statistical techniques in surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss sys- (Knight et al., 1995) replaced default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest statistical led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results typically evaluated using and, roughly go down as more information is removed. While publications of work along these lines do to each other and (tentatively) compare scores, the results are not in fact directly comparable, because of the differences in the input representations automatically derived from Penn Treebank annotations. In particular, the extent to which they are underspecified varies from one system to the next. The idea we would like to put forward with this short paper is to develop a shared task in surface realisation based on common inputs and annotated corpora of paired inputs and outputs defrom various resources from build on the Penn Treebank. Inputs are provided in a common-ground representation formalism which participants map to the types of input required by their system. These inputs are automatically derived from the Penn Treebank and the various layers of annotation (syntactic, semantic, discourse) that have been developed for the documents in it. Outputs (realisations) are evaluated by automatic comparison against the human-authored text in the corpora as well as by by human assessors. In the short term, such a shared task would make existing and new approaches directly comparable by evaluation on the benchmark data associated with the shared task. In the long term, the common-ground input representation may lead to a standardised level of representation that can act as a link between surface realisers and preceding modules, and can make it possible to use alternative surface realisers as drop-in replacements for each other. 2 Towards Common Inputs One hugely challenging aspect in developing a Surface Realisation task is developing a common input representation that all, or at least a majority of, surface realisation researchers are happy to work with. While many different formalisms have been used for input representations to surface realisers, one cannot simply use e.g. van Genabith al.’s automatically generated et al’s forms, Nivre’s depen- Miyao et al.’s or Copestake’s etc., as each of them would introduce a bias in favour of one type of system. One possible solution is to develop a metarepresentation which contains, perhaps on multiple layers of representation, all the information needed to map to any of a given set of realiser input representations, a common-ground representation that acts as a kind of interlingua for translating between different input representations. An important issue in deriving input representations from semantically, syntactically and discourse-annotated corpora is deciding what ininclude. A concern is that making such decisions by committee may be difficult. One way to make it easier might be to define several versions of the task, where each version uses inputs of different levels of specificity. Basing a common input representation on what feasibly be obtained from would put everyone on reasonably common footing. If, moreover, the common input representations can be automatically derived from annotations in existing resources, then data can be produced in sufficient quantities to make it feasible for participants to automatically learn mappings from the system-neutral input to their own input. The above could be achieved by doing somealong the lines of the shared task on Joint Parsing of Syntactic and Semantic Dependencies, for which the organisers combined the Treebank, Propbank, Nombank and the Named Entity corpus into a dependency representation. Brief descriptions of these resources and more details on this idea are provided in Section 4 below. 3 Evaluation many have argued, there is not a single right answer in but varanswers, some better than others, and tasks should take this into account. If a surface realisation task is focused on single-best realizations, then it will not encourage research on producing all possible good realizations, or multiple acceptable realizations in a ranked list, etc. It may not be the best approach to encourage systems that try to make a single, safe choice; instead, perhaps one should encourage approaches that can tell when multiple choices would be ok, and if some would be better than others. In the long term we need to develop task definitions, data resources and evaluation methodologies that properly take into account the one-tonature of but in the short term it may be realistic to reuse existing (which do not provide alternative realisations) and to adapt existing evaluation methodologies including intrinsic assessment of Fluency, Clarity and Appropriateness by trained evaluators, and autointrinsic methods such as One simple way of adapting the latter, for examcould be to calculate scores for the realisations produced by a realiser and then to compute a weighted average where scores for realisations are weighted in inverse proportion to the ranks given to the realisations by the realiser. 4 Data There is a wide variety of different annotated resources that could be of use in a shared task in surface realisation. Many of these include documents originally included in the Penn Treebank, and thus make it possible in principle to combine the various levels of annotation into a single commonground representation. The following is a (nonexhaustive) list of such resources: 1. Penn Treebank-3 (Marcus et al., 1999): one million words of hand-parsed 1989 Wall Street Journal material annotated in Treebank II style. The Treebank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): Nom- Bank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBank I, just as the latter records such information for verbs. BBN Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005): supplements the Wall Street Journal corpus, adding annotation of pronoun coreference, and a variety of entity and numeric types. 5. FrameNet (Johnson et al., 2002): 150,000 sentences annotated for semantic roles and possible syntactic realisations. The annotated sentences come from a variety of sources, including some PropBank texts. 6. OntoNotes 2.0 (Weischedel et al., 2008): OntoNotes 1.0 contains 674k words of Chinese and 500k words of English newswire and broadcast news data. OntoNotes follows the Penn Treebank for syntax and PropBank for predicate-argument structure. Its semantic representation will include word sense disambiguation for nouns and verbs, with each word sense connected to an ontology, and coreference. The current goal is to annotate over a million words each of English and Chinese, and half a million words of Arabic over five years. There are other resources which may be useful. Zettelmoyer and Collins (2009) have manconverted the original anof the (et al., 1994)— some 4,637 sentences—into lambda-calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems on FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and used this to transform WordNet concept definitions into logical forms. The same method (with additional manual correction) was used to produce the test set for another SensEval-3 shared task (Identification of Logic Forms in English). 4.1 CoNLL 2008 Shared Task Data Perhaps the most immediately promising resource is the task data from 2008 (Surdeanu et al., 2008) which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predicates. The data consist excerpts from Penn Treebank-3, Coreference and Entity Type Corpus, PropBank I NomBank 1.0. In the data was used to train and test systems for the task of producing a joint semantic and syntactic dependency of English sentences (the 2009 Shared Task extended this to multi-lingual data). seems feasible that we could reuse the data for a prototype Surface Realisation task, adapting it and inversing the direction of the task, i.e. mapping from syntactic-semantic dependency representations to word strings. 5 Developing the Task The first step in developing a Surface Realisation task could be to get together a working group of surface realisation researchers to develop a common-ground input representation automatically derivable from a set of existing resources. As part of this task a prototype corpus exemplifying inputs/outputs and annotations could be de- At the end of this stage it would be useful to write a white paper and circulate it and the corpus among the wider community for feedback and input. After a further stage of development, it may be feasible to run a prototype surface realisation task at Generation Challenges 2011, combined with a session for discussion and roadmapping. Depending on the outcome of all of this, a full-blown task might be feasible by 2012. Some of this work will need funding to be feasible, and the authors of this paper are in the process of applying for financial support for these plans. 6 Concluding Remarks In this paper we have provided an overview of existing resources that could potentially be used for a surface realisation task, and have outlined ideas for how such a task might work. The core idea is to develop a common-ground input representation which participants map to the types of input required by their system. These inputs are derived from existing annotated corpora developed for language analysis. Outputs (realisations) are evaluated by automatic comparison against the humanauthored text in the corpora as well as by by human assessors. Evaluation methods are adapted to take account of the one-to-many nature of the realisation mapping. The ideas outlined in this paper began as a prolonged email exchange, interspersed with discussions at conferences, among the authors. This paper summarises our ideas as they have evolved so far, to enable feedback and input from other researchers interested in this type of task. References Colin F. Baker and Hiroaki Sato. 2003. The framenet and software. In of A. Cahill and J. van Genabith. 2006. Robust PCFGbased generation using automatically acquired LFG</abstract>
<note confidence="0.77584625">In pages 1033–44. Charles Callaway. 2003. Evaluating coverage for large NLG grammars. In of the 18th International Joint Conference on Artificial Inpages 811–817. J. Carroll and S. Oepen. 2005. High efficiency realization for a wide-coverage unification gram- In of the 2nd International Joint Conference on Natural Language Processing (IJCvolume 3651, pages 165–176. Springer Lecture Notes in Artificial Intelligence. M. Elhadad and J. Robin. 1996. An overview of SURGE: A reusable comprehensive syntactic realization component. Technical Report 96-03, Dept of Mathematics and Computer Science, Ben Gurion University, Beer Sheva, Israel. Deborah Dahl et al. 1994. Expanding the scope of the task: the ATIS-3 corpus. In of the HLT Daniel Gildea and Daniel Jurafsky. 2002. Automatic of semantic roles. Linguis- 28(3):245–288. C. Johnson, C. Fillmore, M. Petruck, C. Baker, M. Ellsworth, J. Ruppenhoper, and E.Wood. 2002.</note>
<title confidence="0.431265">Framenet theory and practice. Technical report.</title>
<author confidence="0.870281">K Knight</author>
<author confidence="0.870281">I Chander</author>
<author confidence="0.870281">M Haines</author>
<author confidence="0.870281">V Hatzivassiloglou</author>
<author confidence="0.870281">E Hovy</author>
<author confidence="0.870281">M Iida</author>
<author confidence="0.870281">S Luk</author>
<author confidence="0.870281">R Whitney</author>
<author confidence="0.870281">K Ya-</author>
<abstract confidence="0.727766055555556">mada. 1995. Filling knowledge gaps in a broad- MT system. In of the Fourteenth International Joint Conference on Artificial (IJCAI pages 1390–1397. I. Langkilde and K. Knight. 1998a. Generation that exploits corpus-based statistical knowledge. In http://www.isi.edu/licensedsw/halogen/nitro98.ps. I. Langkilde. 2002. An empirical verification of coverage and correctness for a general-purpose sentence In 2nd International Natural Lan- Generation Conference (INLG B. Lavoie and O. Rambow. 1997. A fast and portable for text generation systems. In of the 5th Conference on Applied Natural Language pages 265–268. W. Mann and C. Mathiesen. 1983. NIGEL: A systemic grammar for text generation. Technical Re-</abstract>
<note confidence="0.945485454545455">port ISI/RR-85-105, Information Sciences Institute. Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank- 3. Technical report, Linguistic Data Consortium, Philadelphia. Adam Meyers, Ruth Reeves, and Catherine Macleod. 2004. Np-external arguments a study of argument in english. In ’04: Proceedings of Workshop on Multiword pages 96– 103, Morristown, NJ, USA. Association for Computational Linguistics.</note>
<author confidence="0.466172">Logic form</author>
<abstract confidence="0.837160631578947">transformation of wordnet and its applicability to answering. In of Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic models for disambiguation of an chart generator. In of the 9th International Workshop on Parsing Technology pages 93–102. Association for Computational Linguistics. D. S. Paiva and R. Evans. 2005. Empirically-based of natural language generation. In Proceed- M. Palmer, P. Kingsbury, and D. Gildea. 2005. The proposition bank: An annotated corpus of semantic 31(1):71–106. Rus. 2002. Form For WordNet Glosses Application to Question Ph.D. thesis. Lei Shi and Rada Mihalcea. 2005. Putting pieces together: Combining framenet, verbnet and wordnet robust semantic parsing. In of CI-</abstract>
<note confidence="0.895183714285714">Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of synand semantic dependencies. In ’08: Proceedings of the Twelfth Conference on Computa- Natural Language pages 159–177. S. Varges and C. Mellish. 2001. Instance-based natu-</note>
<abstract confidence="0.531771766666667">language generation. In of the 2nd Meeting of the North American Chapter of the Assofor Computational Linguistics (NAACL pages 1–8. E. Velldal, S. Oepen, and D. Flickinger. 2004. Paraphrasing treebanks for stochastic realization rank- In of the 3rd Workshop on Treeand Linguistic Theories (TLT Tuebingen, Germany. Ralph Weischedel and Ada Brunstein. 2005. Bbn pronoun coreference and entity type corpus. Technical report, Linguistic Data Consortium. Ralph Weischedel et al. 2008. Ontonotes release 2.0. Technical report, Linguistic Data Consortium. Michael White and Rajakrishnan Rajkumar. 2009. reranking for ccg realisation. In Proceedings of the 2009 Conference on Empririal Methin Natural Language Processing pages 410–419. M. White. 2004. Reining in CCG chart realization. In Belz, R. Evans, and P. Piwek, editors, Proceedvolume 3123 of pages 182– 191. Springer. Luke Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to forms. In of H. Zhong and A. Stent. 2005. Building surface realizers automatically from corpora. In A. Belz S. Varges, editors, of pages 49–54.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin F Baker</author>
<author>Hiroaki Sato</author>
</authors>
<title>The framenet data and software.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03.</booktitle>
<contexts>
<context position="12540" citStr="Baker and Sato, 2003" startWordPosition="2002" endWordPosition="2005">Arabic over five years. There are other resources which may be useful. Zettelmoyer and Collins (2009) have manually converted the original SQL meaning annotations of the ATIS corpus (et al., 1994)— some 4,637 sentences—into lambda-calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems trained on WSJ data. FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and use</context>
</contexts>
<marker>Baker, Sato, 2003</marker>
<rawString>Colin F. Baker and Hiroaki Sato. 2003. The framenet data and software. In Proceedings of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>J van Genabith</author>
</authors>
<title>Robust PCFGbased generation using automatically acquired LFG approximations.</title>
<date>2006</date>
<booktitle>In Proc. ACL’06,</booktitle>
<pages>1033--44</pages>
<marker>Cahill, van Genabith, 2006</marker>
<rawString>A. Cahill and J. van Genabith. 2006. Robust PCFGbased generation using automatically acquired LFG approximations. In Proc. ACL’06, pages 1033–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Callaway</author>
</authors>
<title>Evaluating coverage for large symbolic NLG grammars.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI2003),</booktitle>
<pages>811--817</pages>
<contexts>
<context position="3886" citStr="Callaway, 2003" startWordPosition="601" endWordPosition="602">d to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as more information </context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>Charles Callaway. 2003. Evaluating coverage for large symbolic NLG grammars. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI2003), pages 811–817.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>S Oepen</author>
</authors>
<title>High efficiency realization for a wide-coverage unification grammar.</title>
<date>2005</date>
<journal>Lecture Notes in Artificial Intelligence.</journal>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP’05),</booktitle>
<volume>3651</volume>
<pages>165--176</pages>
<publisher>Springer</publisher>
<contexts>
<context position="3646" citStr="Carroll and Oepen, 2005" startWordPosition="562" endWordPosition="565"> underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use </context>
</contexts>
<marker>Carroll, Oepen, 2005</marker>
<rawString>J. Carroll and S. Oepen. 2005. High efficiency realization for a wide-coverage unification grammar. In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP’05), volume 3651, pages 165–176. Springer Lecture Notes in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>J Robin</author>
</authors>
<title>An overview of SURGE: A reusable comprehensive syntactic realization component.</title>
<date>1996</date>
<tech>Technical Report 96-03,</tech>
<institution>Dept of Mathematics and Computer Science, Ben Gurion University, Beer Sheva, Israel.</institution>
<contexts>
<context position="2302" citStr="Elhadad and Robin, 1996" startWordPosition="349" endWordPosition="352">y recognisers, coreference resolvers, and many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Natural Language Generation (NLG) has not so far developed generic tools and methods for comparing them to the same extent as Natural Language Analysis (NLA) has. The subfield of NLG that has perhaps come closest to developing generic tools is surface realisation. Wide-coverage surface realisers such as PENMAN/NIGEL (Mann and Mathiesen, 1983), FUF/SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings wi</context>
</contexts>
<marker>Elhadad, Robin, 1996</marker>
<rawString>M. Elhadad and J. Robin. 1996. An overview of SURGE: A reusable comprehensive syntactic realization component. Technical Report 96-03, Dept of Mathematics and Computer Science, Ben Gurion University, Beer Sheva, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Dahl</author>
</authors>
<title>Expanding the scope of the ATIS task: the ATIS-3 corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA HLT Workshop.</booktitle>
<marker>Dahl, 1994</marker>
<rawString>Deborah Dahl et al. 1994. Expanding the scope of the ATIS task: the ATIS-3 corpus. In Proceedings of the ARPA HLT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="12413" citStr="Gildea and Jurafsky (2002)" startWordPosition="1983" endWordPosition="1986">ogy, and coreference. The current goal is to annotate over a million words each of English and Chinese, and half a million words of Arabic over five years. There are other resources which may be useful. Zettelmoyer and Collins (2009) have manually converted the original SQL meaning annotations of the ATIS corpus (et al., 1994)— some 4,637 sentences—into lambda-calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems trained on WSJ data. FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Johnson</author>
<author>C Fillmore</author>
<author>M Petruck</author>
<author>C Baker</author>
<author>M Ellsworth</author>
<author>J Ruppenhoper</author>
<author>E Wood</author>
</authors>
<title>Framenet theory and practice.</title>
<date>2002</date>
<tech>Technical report.</tech>
<contexts>
<context position="11241" citStr="Johnson et al., 2002" startWordPosition="1799" endWordPosition="1802"> also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBank I, just as the latter records such information for verbs. 4. BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005): supplements the Wall Street Journal corpus, adding annotation of pronoun coreference, and a variety of entity and numeric types. 5. FrameNet (Johnson et al., 2002): 150,000 sentences annotated for semantic roles and possible syntactic realisations. The annotated sentences come from a variety of sources, including some PropBank texts. 6. OntoNotes 2.0 (Weischedel et al., 2008): OntoNotes 1.0 contains 674k words of Chinese and 500k words of English newswire and broadcast news data. OntoNotes follows the Penn Treebank for syntax and PropBank for predicate-argument structure. Its semantic representation will include word sense disambiguation for nouns and verbs, with each word sense connected to an ontology, and coreference. The current goal is to annotate </context>
</contexts>
<marker>Johnson, Fillmore, Petruck, Baker, Ellsworth, Ruppenhoper, Wood, 2002</marker>
<rawString>C. Johnson, C. Fillmore, M. Petruck, C. Baker, M. Ellsworth, J. Ruppenhoper, and E.Wood. 2002. Framenet theory and practice. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
<author>M Haines</author>
<author>V Hatzivassiloglou</author>
<author>E Hovy</author>
<author>M Iida</author>
<author>S Luk</author>
<author>R Whitney</author>
<author>K Yamada</author>
</authors>
<title>Filling knowledge gaps in a broadcoverage MT system.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI ’95),</booktitle>
<pages>1390--1397</pages>
<contexts>
<context position="2864" citStr="Knight et al., 1995" startWordPosition="445" endWordPosition="448"> and Mathiesen, 1983), FUF/SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surfac</context>
</contexts>
<marker>Knight, Chander, Haines, Hatzivassiloglou, Hovy, Iida, Luk, Whitney, Yamada, 1995</marker>
<rawString>K. Knight, I. Chander, M. Haines, V. Hatzivassiloglou, E. Hovy, M. Iida, S. Luk, R. Whitney, and K. Yamada. 1995. Filling knowledge gaps in a broadcoverage MT system. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI ’95), pages 1390–1397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL. http://www.isi.edu/licensedsw/halogen/nitro98.ps.</booktitle>
<contexts>
<context position="2987" citStr="Langkilde and Knight, 1998" startWordPosition="461" endWordPosition="464">more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to pr</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998a. Generation that exploits corpus-based statistical knowledge. In Proc. COLING-ACL. http://www.isi.edu/licensedsw/halogen/nitro98.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<booktitle>In Proc. 2nd International Natural Language Generation Conference (INLG ’02).</booktitle>
<contexts>
<context position="3870" citStr="Langkilde, 2002" startWordPosition="599" endWordPosition="600">ical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as m</context>
</contexts>
<marker>Langkilde, 2002</marker>
<rawString>I. Langkilde. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Proc. 2nd International Natural Language Generation Conference (INLG ’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lavoie</author>
<author>O Rambow</author>
</authors>
<title>A fast and portable realizer for text generation systems.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP’97),</booktitle>
<pages>265--268</pages>
<contexts>
<context position="2340" citStr="Lavoie and Rambow, 1997" startWordPosition="355" endWordPosition="359">and many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Natural Language Generation (NLG) has not so far developed generic tools and methods for comparing them to the same extent as Natural Language Analysis (NLA) has. The subfield of NLG that has perhaps come closest to developing generic tools is surface realisation. Wide-coverage surface realisers such as PENMAN/NIGEL (Mann and Mathiesen, 1983), FUF/SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/</context>
</contexts>
<marker>Lavoie, Rambow, 1997</marker>
<rawString>B. Lavoie and O. Rambow. 1997. A fast and portable realizer for text generation systems. In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP’97), pages 265–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>C Mathiesen</author>
</authors>
<title>NIGEL: A systemic grammar for text generation.</title>
<date>1983</date>
<tech>Technical Report ISI/RR-85-105,</tech>
<institution>Information Sciences Institute.</institution>
<contexts>
<context position="2265" citStr="Mann and Mathiesen, 1983" startWordPosition="344" endWordPosition="347">euse parsers, POS-taggers, named entity recognisers, coreference resolvers, and many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Natural Language Generation (NLG) has not so far developed generic tools and methods for comparing them to the same extent as Natural Language Analysis (NLA) has. The subfield of NLG that has perhaps come closest to developing generic tools is surface realisation. Wide-coverage surface realisers such as PENMAN/NIGEL (Mann and Mathiesen, 1983), FUF/SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) </context>
</contexts>
<marker>Mann, Mathiesen, 1983</marker>
<rawString>W. Mann and C. Mathiesen. 1983. NIGEL: A systemic grammar for text generation. Technical Report ISI/RR-85-105, Information Sciences Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<tech>Treebank3. Technical report,</tech>
<institution>Linguistic Data Consortium,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="10006" citStr="Marcus et al., 1999" startWordPosition="1604" endWordPosition="1607">ealisations produced by a realiser and then to compute a weighted average where scores for realisations are weighted in inverse proportion to the ranks given to the realisations by the realiser. 4 Data There is a wide variety of different annotated resources that could be of use in a shared task in surface realisation. Many of these include documents originally included in the Penn Treebank, and thus make it possible in principle to combine the various levels of annotation into a single commonground representation. The following is a (nonexhaustive) list of such resources: 1. Penn Treebank-3 (Marcus et al., 1999): one million words of hand-parsed 1989 Wall Street Journal material annotated in Treebank II style. The Treebank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank3. Technical report, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
</authors>
<title>Np-external arguments a study of argument sharing in english.</title>
<date>2004</date>
<booktitle>In MWE ’04: Proceedings of the Workshop on Multiword Expressions,</booktitle>
<pages>96--103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10736" citStr="Meyers et al., 2004" startWordPosition="1719" endWordPosition="1722">ank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBank I, just as the latter records such information for verbs. 4. BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005): supplements the Wall Street Journal corpus, adding annotation of pronoun coreference, and a variety of entity and numeric types. 5. FrameNet (Johnson et al., 2002): 150,000 sentences annotated for semantic roles and possible syntactic realisations. The annot</context>
</contexts>
<marker>Meyers, Reeves, Macleod, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, and Catherine Macleod. 2004. Np-external arguments a study of argument sharing in english. In MWE ’04: Proceedings of the Workshop on Multiword Expressions, pages 96– 103, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan I Moldovan</author>
<author>Vasile Rus</author>
</authors>
<title>Logic form transformation of wordnet and its applicability to question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL’01.</booktitle>
<contexts>
<context position="13063" citStr="Moldovan and Rus, 2001" startWordPosition="2081" endWordPosition="2084">ame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and used this to transform WordNet concept definitions into logical forms. The same method (with additional manual correction) was used to produce the test set for another SensEval-3 shared task (Identification of Logic Forms in English). 4.1 CoNLL 2008 Shared Task Data Perhaps the most immediately promising resource is is the CoNLL shared task data from 2008 (Surdeanu et al., 2008) which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predica</context>
</contexts>
<marker>Moldovan, Rus, 2001</marker>
<rawString>Dan I. Moldovan and Vasile Rus. 2001. Logic form transformation of wordnet and its applicability to question answering. In Proceedings of ACL’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an hpsg-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technology (Parsing’05),</booktitle>
<pages>93--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3910" citStr="Nakanishi et al., 2005" startWordPosition="603" endWordPosition="607">surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as more information is removed. While public</context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic models for disambiguation of an hpsg-based chart generator. In Proceedings of the 9th International Workshop on Parsing Technology (Parsing’05), pages 93–102. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Paiva</author>
<author>R Evans</author>
</authors>
<title>Empirically-based control of natural language generation.</title>
<date>2005</date>
<booktitle>In Proceedings ACL’05.</booktitle>
<contexts>
<context position="3451" citStr="Paiva and Evans, 2005" startWordPosition="534" endWordPosition="537">-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Tre</context>
</contexts>
<marker>Paiva, Evans, 2005</marker>
<rawString>D. S. Paiva and R. Evans. 2005. Empirically-based control of natural language generation. In Proceedings ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>P Kingsbury</author>
<author>D Gildea</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="10334" citStr="Palmer et al., 2005" startWordPosition="1651" endWordPosition="1654"> these include documents originally included in the Penn Treebank, and thus make it possible in principle to combine the various levels of annotation into a single commonground representation. The following is a (nonexhaustive) list of such resources: 1. Penn Treebank-3 (Marcus et al., 1999): one million words of hand-parsed 1989 Wall Street Journal material annotated in Treebank II style. The Treebank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBan</context>
</contexts>
<marker>Palmer, Kingsbury, Gildea, 2005</marker>
<rawString>M. Palmer, P. Kingsbury, and D. Gildea. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
</authors>
<title>Logic Form For WordNet Glosses and Application to Question Answering.</title>
<date>2002</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="13075" citStr="Rus, 2002" startWordPosition="2085" endWordPosition="2086"> semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and used this to transform WordNet concept definitions into logical forms. The same method (with additional manual correction) was used to produce the test set for another SensEval-3 shared task (Identification of Logic Forms in English). 4.1 CoNLL 2008 Shared Task Data Perhaps the most immediately promising resource is is the CoNLL shared task data from 2008 (Surdeanu et al., 2008) which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predicates. The dat</context>
</contexts>
<marker>Rus, 2002</marker>
<rawString>Vasile Rus. 2002. Logic Form For WordNet Glosses and Application to Question Answering. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Putting pieces together: Combining framenet, verbnet and wordnet for robust semantic parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing’05.</booktitle>
<contexts>
<context position="12773" citStr="Shi and Mihalcea (2005)" startWordPosition="2041" endWordPosition="2045">calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems trained on WSJ data. FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and used this to transform WordNet concept definitions into logical forms. The same method (with additional manual correction) was used to produce the test set for another SensEval-3 shared task (Identification of Logic Forms in English). 4</context>
</contexts>
<marker>Shi, Mihalcea, 2005</marker>
<rawString>Lei Shi and Rada Mihalcea. 2005. Putting pieces together: Combining framenet, verbnet and wordnet for robust semantic parsing. In Proceedings of CICLing’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In CoNLL ’08: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>159--177</pages>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In CoNLL ’08: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 159–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Varges</author>
<author>C Mellish</author>
</authors>
<title>Instance-based natural language generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL ’01),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3392" citStr="Varges and Mellish, 2001" startWordPosition="524" endWordPosition="527">added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) </context>
</contexts>
<marker>Varges, Mellish, 2001</marker>
<rawString>S. Varges and C. Mellish. 2001. Instance-based natural language generation. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL ’01), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Velldal</author>
<author>S Oepen</author>
<author>D Flickinger</author>
</authors>
<title>Paraphrasing treebanks for stochastic realization ranking.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd Workshop on Treebanks and Linguistic Theories (TLT ’04),</booktitle>
<location>Tuebingen, Germany.</location>
<contexts>
<context position="3427" citStr="Velldal et al., 2004" startWordPosition="530" endWordPosition="533">rly example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for r</context>
</contexts>
<marker>Velldal, Oepen, Flickinger, 2004</marker>
<rawString>E. Velldal, S. Oepen, and D. Flickinger. 2004. Paraphrasing treebanks for stochastic realization ranking. In Proceedings of the 3rd Workshop on Treebanks and Linguistic Theories (TLT ’04), Tuebingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<title>Bbn pronoun coreference and entity type corpus.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Linguistic Data Consortium.</institution>
<contexts>
<context position="11076" citStr="Weischedel and Brunstein, 2005" startWordPosition="1774" endWordPosition="1777">verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBank I, just as the latter records such information for verbs. 4. BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005): supplements the Wall Street Journal corpus, adding annotation of pronoun coreference, and a variety of entity and numeric types. 5. FrameNet (Johnson et al., 2002): 150,000 sentences annotated for semantic roles and possible syntactic realisations. The annotated sentences come from a variety of sources, including some PropBank texts. 6. OntoNotes 2.0 (Weischedel et al., 2008): OntoNotes 1.0 contains 674k words of Chinese and 500k words of English newswire and broadcast news data. OntoNotes follows the Penn Treebank for syntax and PropBank for predicate-argument structure. Its semantic repres</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. Bbn pronoun coreference and entity type corpus. Technical report, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes release 2.0.</title>
<date>2008</date>
<tech>Technical report, Linguistic Data Consortium.</tech>
<marker>Weischedel, 2008</marker>
<rawString>Ralph Weischedel et al. 2008. Ontonotes release 2.0. Technical report, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>Perceptron reranking for ccg realisation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empririal Methods in Natural Language Processing (EMNLP’09),</booktitle>
<pages>410--419</pages>
<contexts>
<context position="3991" citStr="White and Rajkumar, 2009" startWordPosition="617" endWordPosition="620"> (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as more information is removed. While publications of work along these lines do refer to each other and (tentatively) compare</context>
</contexts>
<marker>White, Rajkumar, 2009</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2009. Perceptron reranking for ccg realisation. In Proceedings of the 2009 Conference on Empririal Methods in Natural Language Processing (EMNLP’09), pages 410–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
</authors>
<title>Reining in CCG chart realization.</title>
<date>2004</date>
<booktitle>Proceedings INLG’04, volume 3123 of LNAI,</booktitle>
<pages>182--191</pages>
<editor>In A. Belz, R. Evans, and P. Piwek, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3405" citStr="White, 2004" startWordPosition="528" endWordPosition="529">lihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently </context>
</contexts>
<marker>White, 2004</marker>
<rawString>M. White. 2004. Reining in CCG chart realization. In A. Belz, R. Evans, and P. Piwek, editors, Proceedings INLG’04, volume 3123 of LNAI, pages 182– 191. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical forms.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP’09.</booktitle>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical forms. In Proceedings of ACL-IJCNLP’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhong</author>
<author>A Stent</author>
</authors>
<title>Building surface realizers automatically from corpora.</title>
<date>2005</date>
<booktitle>Proceedings of UCNLG’05,</booktitle>
<pages>49--54</pages>
<editor>In A. Belz and S. Varges, editors,</editor>
<contexts>
<context position="3933" citStr="Zhong and Stent, 2005" startWordPosition="608" endWordPosition="611">ods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as more information is removed. While publications of work along th</context>
</contexts>
<marker>Zhong, Stent, 2005</marker>
<rawString>H. Zhong and A. Stent. 2005. Building surface realizers automatically from corpora. In A. Belz and S. Varges, editors, Proceedings of UCNLG’05, pages 49–54.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>