<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.946556">
Context-theoretic Semantics for Natural Language: an Overview
</title>
<author confidence="0.970016">
Daoud Clarke
</author>
<affiliation confidence="0.978853">
University of Sussex
</affiliation>
<address confidence="0.628597">
Falmer, Brighton, UK
</address>
<email confidence="0.974658">
daoud.clarke@gmail.com
</email>
<sectionHeader confidence="0.994358" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898">
We present the context-theoretic frame-
work, which provides a set of rules for the
nature of composition of meaning based
on the philosophy of meaning as context.
Principally, in the framework the composi-
tion of the meaning of words can be repre-
sented as multiplication of their represen-
tative vectors, where multiplication is dis-
tributive with respect to the vector space.
We discuss the applicability of the frame-
work to a range of techniques in natu-
ral language processing, including subse-
quence matching, the lexical entailment
model of Dagan et al. (2005), vector-based
representations of taxonomies, statistical
parsing and the representation of uncer-
tainty in logical semantics.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999826211538462">
Techniques such as latent semantic analysis (Deer-
wester et al., 1990) and its variants have been
very successful in representing the meanings of
words as vectors, yet there is currently no theory
of natural language semantics that explains how
we should compose these representations: what
should the representation of a phrase be, given the
representation of the words in the phrase? In this
paper we present such a theory, which is based
on the philosophy of meaning as context, as epit-
omised by the famous sayings of Wittgenstein
(1953), “Meaning just is use” and Firth (1957),
“You shall know a word by the company it keeps”.
For the sake of brevity we shall present only a
summary of our research, which is described in
full in (Clarke, 2007), and we give a simplified
version of the framework, which nevertheless suf-
fices for the examples which follow.
We believe that the development of theories that
can take vector representations of meaning beyond
the word level, to the phrasal and sentence lev-
els and beyond are essential for vector based se-
mantics to truly compete with logical semantics,
both in their academic standing and in application
to real problems in natural language processing.
Moreover the time is ripe for such a theory: never
has there been such an abundance of immediately
available textual data (in the form of the world-
wide web) or cheap computing power to enable
vector-based representations of meaning to be ob-
tained. The need to organise and understand the
new abundance of data makes these techniques all
the more attractive since meanings are determined
automatically and are thus more robust in compar-
ison to hand-built representations of meaning. A
guiding theory of vector based semantics would
undoubtedly be invaluable in the application of
these representations to problems in natural lan-
guage processing.
The context-theoretic framework does not pro-
vide a formula for how to compose meaning;
rather it provides mathematical guidelines for the-
ories of meaning. It describes the nature of the
vector space in which meanings live, gives some
restrictions on how meanings compose, and pro-
vides us with a measure of the degree of entail-
ment between strings for any implementation of
the framework.
The remainder of the paper is structured as fol-
lows: in Section 2 we present the framework; in
Section 3 we present applications of the frame-
work:
</bodyText>
<listItem confidence="0.85143525">
• We describe subsequence matching (Section
3.1) and the lexical entailment model of (Da-
gan et al., 2005) (Section 3.2), both of which
have been applied to the task of recognising
textual entailment.
• We show how a vector based representation
of a taxonomy incorporating probabilistic in-
formation about word meanings can be con-
</listItem>
<note confidence="0.938337">
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 112–119,
Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.996668">
112
</page>
<bodyText confidence="0.999676">
xˆ E L1(S)+, where L1(S) means the set of all
functions from S to the real numbers R which are
finite under the L1 norm,
</bodyText>
<equation confidence="0.9662375">
llull1 = 1: |u(s)|
d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6
</equation>
<note confidence="0.482804">
orange fruit orange ∧ fruit
</note>
<figureCaption confidence="0.813957">
Figure 1: Vector representations of two terms in
a space L1(S) where S = {d1, d2, d3, d4, d5, d6}
and their vector lattice meet (the darker shaded
area).
</figureCaption>
<bodyText confidence="0.884994">
structed in Section 3.3.
</bodyText>
<listItem confidence="0.9959768">
• We show how syntax can be represented
within the framework in Section 3.4.
• We summarise our approach to representing
uncertainty in logical semantics in Section
3.5.
</listItem>
<sectionHeader confidence="0.983849" genericHeader="method">
2 Context-theoretic Framework
</sectionHeader>
<bodyText confidence="0.991273186046512">
The context-theoretic framework is based on the
idea that the vector representation of the meaning
of a word is derived from the contexts in which it
occurs. However it extends this idea to strings of
any length: we assume there is some set S con-
taining all the possible contexts associated with
any string. A context theory is an implementa-
tion of the context-theoretic framework; a key re-
quirement for a context theory is a mapping from
strings to vectors formed from the set of contexts.
In vector based techniques, the set of contexts
may be the set of possible dependency relations
between words, or the set of documents in which
strings may occur; in context-theoretic semantics
however, the set of “contexts” can be any set.
We continue to refer to it as a set of contexts
since the intuition and philosophy which forms the
basis for the framework derives from this idea;
in practice the set may even consist of logical
sentences describing the meanings of strings in
model-theoretic terms.
An important aspect of vector-based techniques
is measuring the frequency of occurrence of
strings in each context. We model this in a gen-
eral way as follows: let A be a set consisting of
the words of the language under consideration.
The first requirement of a context theory is a map-
ping x H xˆ from a string x E A∗ to a vector
S∈S
and L1(S)+ restricts this to functions to the non-
negative real numbers, R+; these functions are
called the positive elements of the vector space
L1(S). The requirement that the L1 norm is finite,
and that the map is only to positive elements re-
flects the fact that the vectors are intended to repre-
sent an estimate of relative frequency distributions
of the strings over the contexts, since a frequency
distribution will always satisfy these requirements.
Note also that the l1 norm of the context vector of
a string is simply the sum of all its components
and is thus proportional to its probability.
The set of functions L1(S) is a vector space un-
der the point-wise operations:
</bodyText>
<equation confidence="0.877921">
(αu)(s) = αu(s)
(u + v)(s) = u(s) + v(s)
for u, v E L1(S) and α E R, but it is also a lattice
under the operations
(u ∧ v)(s) = min(u(s), v(s))
(u V v)(s) = max(u(s), v(s)).
</equation>
<bodyText confidence="0.999042333333333">
In fact it is a vector lattice or Riesz space (Alipran-
tis and Burkinshaw, 1985) since it satisfies the fol-
lowing relationships
</bodyText>
<construct confidence="0.87144">
if u &lt; v then αu &lt; αv
if u &lt; v then u + w &lt; v + w,
</construct>
<bodyText confidence="0.998692727272727">
where α E R+ and &lt; is the partial ordering asso-
ciated with the lattice operations, defined by u &lt; v
if u ∧ v = u.
Together with the l1 norm, the vector lattice
defines an Abstract Lebesgue space (Abramovich
and Aliprantis, 2002) a vector space incorporating
all the properties of a measure space, and thus can
also be thought of as defining a probability space,
where V and ∧ correspond to the union and inter-
section of events in the σ algebra, and the norm
corresponds to the (un-normalised) probability.
</bodyText>
<subsectionHeader confidence="0.993325">
2.1 Distributional Generality
</subsectionHeader>
<bodyText confidence="0.99719625">
The vector lattice nature of the space under consid-
eration is important in the context-theoretic frame-
work since it is used to define a degree of entail-
ment between strings. Our notion of entailment is
</bodyText>
<page confidence="0.998681">
113
</page>
<bodyText confidence="0.997670058823529">
based on the concept of distributional generality
(Weeds et al., 2004), a generalisation of the distri-
butional hypothesis of Harris (1985), in which it
is assumed that terms with a more general mean-
ing will occur in a wider array of contexts, an
idea later developed by Geffet and Dagan (2005).
Weeds et al. (2004) also found that frequency
played a large role in determining the direction
of entailment, with the more general term often
occurring more frequently. The partial ordering
of the vector lattice encapsulates these properties
since xˆ &lt; yˆ if and only if y occurs more frequently
in all the contexts in which x occurs.
This partial ordering is a strict relationship,
however, that is unlikely to exist between any two
given vectors. Because of this, we define a degree
of entailment
</bodyText>
<equation confidence="0.81958">
Ent(u, v) =
</equation>
<bodyText confidence="0.9994924">
This value has the properties of a conditional prob-
ability; in the case of u = xˆ and v = yˆ it is a
measure of the degree to which the contexts string
x occurs in are shared by the contexts string y oc-
curs in.
</bodyText>
<subsectionHeader confidence="0.997749">
2.2 Multiplication
</subsectionHeader>
<bodyText confidence="0.99895125">
The map from strings to vectors already tells us ev-
erything we need to know about the composition
of words: given two words x and y, we have their
individual context vectors xˆ and ˆy, and the mean-
ing of the string xy is represented by the vector
Ry. The question we address is what relationship
should be imposed between the representation of
the meanings of individual words xˆ and yˆ and the
meaning of their composition Ry. As it stands, we
have little guidance on what maps from strings to
context vectors are appropriate.
The first restriction we propose is that vector
representations of meanings should be compos-
able in their own right, without consideration of
what words they originated from. In fact we place
a strong requirement on the nature of multiplica-
tion on elements: we require that the multiplica-
tion · on the vector space defines a lattice-ordered
algebra. This means that multiplication is asso-
ciative, distributive with respect to addition, and
satisfies u · v &gt; 0 if u &gt; 0 and v &gt; 0, i.e. the
product of positive elements is also positive.
We argue that composition of context vectors
needs to be compatible with concatenation of
words, i.e.
i.e. the map from strings to context vectors defines
a semigroup homomorphism. Then the require-
ment that multiplication is associative can be seen
to be a natural one since the homomorphism en-
forces this requirement for context vectors. Sim-
ilarly since all context vectors are positive their
product in the algebra must also be positive, thus it
is natural to extend this to all elements of the alge-
bra. The requirement for distributivity is justified
by our own model of meaning as context in text
corpora, described in full elsewhere.
</bodyText>
<subsectionHeader confidence="0.999197">
2.3 Context Theory
</subsectionHeader>
<bodyText confidence="0.963821">
The above requirements give us all we need to de-
fine a context theory.
Definition 1 (Context theory). (A, S, ˆ,· ) defines
a context theory if L1(S) is a lattice-ordered al-
gebra under the multiplication defined by · and ˆ
defines a semigroup homomorphism x H xˆ from
A* to L1(S)+.
</bodyText>
<sectionHeader confidence="0.892545" genericHeader="method">
3 Context Theories for Natural
Language
</sectionHeader>
<bodyText confidence="0.9997645">
In this section we describe applications of the
context-theoretic framework to applications in
computational linguistics and natural language
processing. We shall commonly use a construc-
tion in which there is a binary operation o on S
that makes it a semigroup. In this case L1(S) is a
lattice-ordered algebra with convolution as multi-
plication:
</bodyText>
<equation confidence="0.8930155">
E(u · v)(r) = u(s)v(t)
sot=r
</equation>
<bodyText confidence="0.811425">
for r, s, t E S and u, v E L1(S). We denote the
unit basis element associated with an element x E
S by ex, that is ex(y) = 1 if and only if y = x,
otherwise ex(y) = 0.
</bodyText>
<subsectionHeader confidence="0.999367">
3.1 Subsequence Matching
</subsectionHeader>
<bodyText confidence="0.999988125">
A string x E A* is called a “subsequence” of
y E A* if each element of x occurs in y in the
same order, but with the possibility of other ele-
ments occurring in between, so for example abba
is a subsequence of acabcba in {a, b, c}*. We de-
note the set of subsequences of x (including the
empty string) by Sub(x). Subsequence match-
ing compares the subsequences of two strings: the
</bodyText>
<figure confidence="0.5919562">
xˆ ·
yˆ=
xy,
11u n v111
11u111 .
</figure>
<page confidence="0.990267">
114
</page>
<bodyText confidence="0.974175">
more subsequences they have in common the more
similar they are assumed to be. This idea has
been used successfully in text classification (Lodhi
et al., 2002) and recognising textual entailment
(Clarke, 2006).
We can describe such models using a context
theory (A, A*, ˆ,� ), where • is convolution in
</bodyText>
<equation confidence="0.850253333333333">
L1(A*) and
xˆ = (1/2|x|) � ey,
yESub(x)
</equation>
<bodyText confidence="0.998550416666667">
i.e. the context vector of a string is a weighted sum
of its subsequences. Under this context theory xˆ &lt;
ˆy, i.e. x completely entails y if x is a subsequence
of y.
Many variations on this context theory are pos-
sible, for example using more complex mappings
to L1(A*). The context theory can also be adapted
to incorporate a measure of lexical overlap be-
tween strings, an approach that, although simple,
performs comparably to more complex techniques
in tasks such as recognising textual entailment
(Dagan et al., 2005)
</bodyText>
<subsectionHeader confidence="0.993909">
3.2 Lexical Entailment Model
</subsectionHeader>
<bodyText confidence="0.928627714285714">
Glickman and Dagan (2005) define their own
model of entailment and apply it to the task of
recognising textual entailment. They estimate
entailment between words based on occurrences
in documents: they estimate a lexical entailment
probability LEP(x, y) between two terms x and y
to be
</bodyText>
<equation confidence="0.915938333333333">
nx &apos;y
LEP (x, y) ^
ny
</equation>
<bodyText confidence="0.990947">
where ny and nx,y denote the number of docu-
ments that the word y occurs in and the words x
and y both occur in respectively.
We can describe this using a context theory
(A, D, ˆ, • ), where D is the set of documents, and
</bodyText>
<table confidence="0.925405">
Newman (Dublin) 0.565 0.6
115 Model Accuracy CWS
Dirichlet (107) 0.576 0.642
Bayer (MITRE) 0.586 0.617
Jijkoun (Amsterdam) 0.552 0.559
</table>
<bodyText confidence="0.904614333333333">
Glickman and
(2005) do not use this
measure, possibly because the problem of data
sparseness makes it useless for long strings. How-
ever the measure they use can be viewed as an ap-
proximation to this context theory.
We have also used this idea to determine en-
tailment, using latent Dirichlet allocation to get
around the problem of data sparseness. A model
was built using a subset of around 380,000 docu-
ments from the Gigaword corpus, and the model
was evaluated on the dataset from the first Recog-
nising Textual Entailment Challenge; the results
are shown in Table 1. In order to use the model, a
document length had to be chosen; it was found
that very long documents yielded better perfor-
Dagan
ce at this task.
</bodyText>
<table confidence="0.6065964">
Dirichlet (106) 0.584 0.630
Glickman (Bar Ilan) 0.586 0.572
confidence weighted score —see
(Dagan et al.,
2005) for the definition.
</table>
<equation confidence="0.9625">
� 1 if x occurs in document d
ˆx(d) =
.
0 otherwise.
</equation>
<bodyText confidence="0.9981215">
In this case the estimate of LEP(x, y) coincides
with our own degree of entailment Ent(x, y).
There are many ways in which the multiplica-
tion • can be defined on L1(D). The simplest one
defines ed • ef = ed if d = f and edef = 0 oth-
erwise. The effect of multiplication of the context
vectors of two strings is then set intersection:
context-theoretic fr
amework. This opens up the
way to the possibility of new techniques that
combine the vector-based representations of word
meanings with the ontological ones, for example:
</bodyText>
<tableCaption confidence="0.679963">
Table 1: Results obtained with our Latent Dirichlet
projection model on the data from the first Recog-
nising Textual Entailment Challenge for two doc-
ument lengths N = 106 and N = 107 using a cut-
off for the degree of entailment of 0.5 at which
entailment was regarded as holding. CWS is the
</tableCaption>
<bodyText confidence="0.710047">
man
</bodyText>
<subsectionHeader confidence="0.99921">
3.3 Representing Taxonomies
</subsectionHeader>
<bodyText confidence="0.9940646">
In this section we describe how the relationships
described by a taxonomy, the collection of is-
a relationships described by ontologies such as
WordNet (Fellbaum, 1989), can be embedded in
the vector lattice structure that is crucial to the
</bodyText>
<listItem confidence="0.988768">
• Semantic smoothing could be applied to
vector based representations of an ontology,
for example using distributional similarity
measures to move words that are distri
bution-
ally similar closer to each other in the vector
space. This type of technique may allow the
</listItem>
<equation confidence="0.957657333333333">
� 1 if x and y occur in document d
(ˆx•ˆy)(d) =
0 otherwise.
</equation>
<bodyText confidence="0.7280475">
benefits of vector based techniques and on-
tologies to be combined.
</bodyText>
<listItem confidence="0.8315540625">
• Automatic classification: representing the
taxonomy in a vector space may make it
easier to look for relationships between the
meanings in the taxonomy and meanings de-
rived from vector based techniques such as
latent semantic analysis, potentially aiding in
classifying word meanings in a taxonomy.
• The new vector representation could lead to
new measures of semantic distance, for ex-
ample, the Lp norms can all be used to
measure distance between the vector rep-
resentations of meanings in a taxonomy.
Moreover, the vector-based representation al-
lows ambiguity to be represented by adding
the weighted representations of individual
senses.
</listItem>
<bodyText confidence="0.996275272727273">
We assume that the is-a relation is a partial or-
dering; this is true for many ontologies. We wish
to incorporate the partial ordering of the taxonomy
into the partial ordering of the vector lattice. We
will make use of the following result relating to
partial orders:
Definition 2 (Ideals). A lower set in a partially
ordered set S is a set T such that for all x, y E S,
if x E T and y &lt; x then y E T.
The principal ideal generated by an element x in
a partially ordered set S is defined to be the lower
</bodyText>
<equation confidence="0.528993">

set y(x) = {y E S : y &lt; x}.
</equation>
<bodyText confidence="0.987470545454546">
Proposition 3 (Ideal Completion). If S is a par-
tially ordered set, then 10 can be considered as
a function from S to the powerset 2S. Under the
partial ordering defined by set inclusion, the set of

lower sets form a complete lattice, and y(�) is a
completion of S, the ideal completion.
We are also concerned with the probability of
concepts. This is an idea that has come about
through the introduction of “distance measures”
on taxonomies (Resnik, 1995). Since terms can
be ascribed probabilities based on their frequen-
cies of occurrence in corpora, the concepts they re-
fer to can similarly be assigned probabilities. The
probability of a concept is the probability of en-
countering an instance of that concept in the cor-
pus, that is, the probability that a term selected
at random from the corpus has a meaning that is
subsumed by that particular concept. This ensures
that more general concepts are given higher proba-
bilities, for example if there is a most general con-
cept (a top-most node in the taxonomy, which may
correspond for example to “entity”) its probability
will be one, since every term can be considered an
instance of that concept.
We give a general definition based on this idea
which does not require probabilities to be assigned
based on corpus counts:
Definition 4 (Real Valued Taxonomy). A real val-
ued taxonomy is a finite set S of concepts with a
partial ordering &lt; and a positive real function p
over S. The measure of a concept is then defined
in terms ofp as
</bodyText>
<equation confidence="0.9995125">
Xˆp(x) = p(y).
y∈1(x)
</equation>
<bodyText confidence="0.964770576923077">
The taxonomy is called probabilistic if
Px∈S p(s) = 1. In this case pˆ refers to the
probability of a concept.
Thus in a probabilistic taxonomy, the function
p corresponds to the probability that a term is ob-
served whose meaning corresponds (in that con-
text) to that concept. The function pˆ denotes the
probability that a term is observed whose meaning
in that context is subsumed by the concept.
Note that if S has a top element I then in the
probabilistic case, clearly ˆp(I) = 1. In studies of
distance measures on ontologies, the concepts in
S often correspond to senses of terms, in this case
the function p represents the (normalised) proba-
bility that a given term will occur with the sense
indicated by the concept. The top-most concept
often exists, and may be something with the mean-
ing “entity”—intended to include the meaning of
all concepts below it.
The most simple completion we consider is into
the vector lattice L1(S), with basis elements {ex :
x E S}.
Proposition 5 (Ideal Vector Completion). Let S
be a probabilistic taxonomy with probability dis-
tribution function p that is non-zero everywhere on
S. The function ψ from S to L1(S) defined by
</bodyText>
<equation confidence="0.9975345">
ψ(x) = X p(y)ey
y∈1(x)
</equation>
<footnote confidence="0.598471">
is a completion of the partial ordering of S un-
der the vector lattice order of L1(S), satisfying
Ilψ(x)Il1 = ˆp(x).
</footnote>
<page confidence="0.995393">
116
</page>
<figure confidence="0.999053066666667">
entity
organism
plant
d
a
d
o
s
m
j
they mashed their way through the thick mud
grass tree
oat rice barley
beech chestnut oak
cereal
</figure>
<figureCaption confidence="0.9975668">
Figure 3: A link grammar parse. Link types:
s: subject, o: object, m: modifying phrases,
a: adjective, j: preposition, d: determiner.
Figure 2: A small example taxonomy extracted
from WordNet (Fellbaum, 1989).
</figureCaption>
<bodyText confidence="0.720627">
Proof. The function ψ is clearly order-preserving:
</bodyText>
<equation confidence="0.9851915">
� �
if x ≤ y in S then since �(x) ⊆ �(y), neces-
sarily ψ(x) ≤ ψ(y). Conversely, the only way
� �
</equation>
<bodyText confidence="0.985203909090909">
that ψ(x) ≤ ψ(y) can be true is if �(x) ⊆ �(y)
since p is non-zero everywhere. If this is the case,
then x ≤ y by the nature of the ideal completion.
Thus ψ is an order-embedding, and since L1(S) is
a complete lattice, it is also a completion. Finally,
note that kψ(x)k1 = Ey∈↓(x) p(y) = ˆp(x).
This completion allows us to represent concepts
as elements within a vector lattice so that not only
the partial ordering of the taxonomy is preserved,
but the probability of concepts is also preserved as
the size of the vector under the L1 norm.
</bodyText>
<subsectionHeader confidence="0.998102">
3.4 Representing Syntax
</subsectionHeader>
<bodyText confidence="0.965930257142857">
In this section we give a description link grammar
(Sleator and Temperley, 1991) in terms of a con-
text theory. Link grammar is a lexicalised syntac-
tic formalism which describes properties of words
in terms of links formed between them, and which
is context-free in terms of its generative power; for
the sake of brevity we omit the details, although a
sample link grammar parse is show in Figure 3.
Our formulation of link grammar as a context
theory makes use of a construction called a free
inverse semigroup. Informally, the free inverse
semigroup on a set S is formed from elements
of S and their inverses, S−1 = {s−1 : s ∈ S},
satisfying no other condition than those of an in-
verse semigroup. Formally, the free inverse semi-
group is defined in terms of a congruence rela-
tion on (S ∪S−1)∗ specifying the inverse property
and commutativity of idempotents — see (Munn,
1974) for details. We denote the free inverse semi-
group on S by FIS(S).
Free inverse semigroups were shown by Munn
(1974) to be equivalent to birooted word trees. A
birooted word-tree on a set A is a directed acyclic
graph whose edges are labelled by elements of A
which does not contain any subgraphs of the form
−→ •, together with
a
two distinguished nodes, called the start node, ✷
and finish node, ◦.
An element in the free semigroup FIS(S) is de-
noted as a sequence xd1x22 ... xdnn where xi ∈ S
and di ∈ {1, −1}.
We construct the birooted word tree by starting
with a single node as the start node, and for each i
from 1 to n:
</bodyText>
<listItem confidence="0.9046565">
• Determine if there is an edge labelled xi leav-
ing the current node if di = 1, or arriving at
the current node if di = −1.
• If so, follow this edge and make the resulting
node the current node.
• If not, create a new node and join it with an
edge labelled xi in the appropriate direction,
and make this node the current node.
</listItem>
<bodyText confidence="0.938785461538462">
The finish node is the current node after the n iter-
ations.
The product of two elements x and y in the free
inverse semigroup can be computed by finding the
birooted word-tree of x and that of y, joining the
graphs by equating the start node of y with the fin-
ish node of x (and making it a normal node), and
merging any other nodes and edges necessary to
remove any subgraphs of the form • a−→ • ←−a •
−→ •. The inverse of an element
a
has the same graph with start and finish nodes ex-
changed.
</bodyText>
<figure confidence="0.99376">
←− •
a
• −→ •
a
←− • or •
a
or •
←− •
a
</figure>
<page confidence="0.990915">
117
</page>
<bodyText confidence="0.98729108">
We can represent parses of sentences in link
grammar by translating words to syntactic cate-
gories in the free inverse semigroup. The parse
shown earlier for “they mashed their way through
the thick mud” can be represented in the inverse
semigroup on S = Is, m, o, d, j, a} as
ss−1modd−1o−1m−1jdaa−1d−1j−1
which has the following birooted word-tree (the
words which the links derive from are shown in
brackets):
Let A be the set of words in the natural lan-
guage under consideration, S be the set of link
types. Then we can form a context theory
(A, FIS(S), ˆ, · ) where · is multiplication defined
by convolution on FIS(S), and a word a E A is
mapped to a probabilistic sum aˆ of its link possible
grammar representations (called disjuncts). Thus
we have a context theory which maps a string x
to elements of L1(FIS(S)); if there is a parse for
this string then there will be some component of
xˆ which corresponds to an idempotent element of
FIS(S). Moreover we can interpret the magnitude
of the component as the probability of that par-
ticular parse, thus the context theory describes a
probabilistic variation of link grammar.
</bodyText>
<subsectionHeader confidence="0.935146">
3.5 Uncertainty in Logical Semantics
</subsectionHeader>
<bodyText confidence="0.999993882352941">
For the sake of brevity, we summarise our ap-
proach to representing uncertainty in logical se-
mantics, which is described in full elsewhere. Our
aim is to be able to reason with probabilistic infor-
mation about uncertainty in logical semantics. For
example, in order to represent a natural language
sentence as a logical statement, it is necessary
to parse it, which may well be with a statistical
parser. We may have hundreds of possible parses
and logical representations of a sentence, and as-
sociated probabilities. Alternatively, we may wish
to describe our uncertainty about word-sense dis-
ambiguation in the representation. Incorporating
such probabilistic information into the representa-
tion of meaning may lead to more robust systems
which are able to cope when one component fails.
The basic principle we propose is to first repre-
sent unambiguous logical statements as a context
theory. Our uncertainty about the meaning of a
sentence can then be represented as a probability
distribution over logical statements, whether the
uncertainty arises from parsing, word-sense dis-
ambiguation or any other source. Incorporating
this information is then straightforward: the rep-
resentation of the sentence is the weighted sum
of the representation of each possible meaning,
where the weights are given by the probability dis-
tribution.
Computing the degree of entailment using this
approach is computationally challenging, however
we have shown that it is possible to estimate the
degree of entailment by computing a lower bound
on this value by calculating pairwise degrees of
entailment for each possible logical statement.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999871423076923">
Mitchell and Lapata (2008) proposed a framework
for composing meaning that is extremely gen-
eral in nature: there is no requirement for linear-
ity in the composition function, although in prac-
tice the authors do adopt this assumption. Indeed
their “multiplicative models” require composition
of two vectors to be a linear function of their ten-
sor product; this is equivalent to our requirement
of distributivity with respect to vector space addi-
tion.
Various ways of composing vector based repre-
sentations of meaning were investigated by Wid-
dows (2008), including the tensor product and di-
rect sum. Both of these are compatible with the
context theoretic framework since they are dis-
tributive with respect to the vector space addition.
Clark et al. (2008) proposed a method of com-
posing meaning that generalises Montague seman-
tics; further work is required to determine how
their method of composition relates to the context-
theoretic framework.
Erk and Pado (2008) describe a method of com-
position that allows the incorporation of selec-
tional preferences; again further work is required
to determine the relation between this work and
the context-theoretic framework.
</bodyText>
<figure confidence="0.94031">
s(they, mashed)
m(mashed, through)
o(mashed, way)
d(their, way)
j(through, mud)
d(the, mud)
a(thick, mud)
</figure>
<page confidence="0.986266">
118
</page>
<sectionHeader confidence="0.996759" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999995125">
We have given an introduction to the context-
theoretic framework, which provides mathemat-
ical guidelines on how vector-based representa-
tions of meaning should be composed, how en-
tailment should be determined between these rep-
resentations, and how probabilistic information
should be incorporated.
We have shown how the framework can be ap-
plied to a wide range of problems in computational
linguistics, including subsequence matching, vec-
tor based representations of taxonomies and statis-
tical parsing. The ideas we have presented here are
only a fraction of those described in full in (Clarke,
2007), and we believe that even that is only the tip
of the iceberg with regards to what it is possible to
achieve with the framework.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99977">
I am very grateful to my supervisor David Weir
for all his help in the development of these ideas,
and to Rudi Lutz and the anonymous reviewers for
many useful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998743" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918797468354">
Y. A. Abramovich and Charalambos D. Aliprantis.
2002. An Invitation to Operator Theory. American
Mathematical Society.
Charalambos D. Aliprantis and Owen Burkinshaw.
1985. Positive Operators. Academic Press.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of
the Second Symposium on Quantum Interaction,
Oxford, UK, pages 133–140.
Daoud Clarke. 2006. Meaning as context and subse-
quence analysis for textual entailment. In Proceed-
ings of the Second PASCAL Recognising Textual En-
tailment Challenge.
Daoud Clarke. 2007. Context-theoretic Semantics
for Natural Language: an Algebraic Framework.
Ph.D. thesis, Department of Informatics, University
of Sussex.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entail-
ment.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal
of the American Society for Information Science,
41(6):391–407.
Katrin Erk and Sebastian Pado. 2008. A structured
vector space model for word meaning in context. In
Proceedings ofEMNLP.
Christaine Fellbaum, editor. 1989. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, Massachusetts.
John R. Firth. 1957. Modes of meaning. In Papers
in Linguistics 1934–1951. Oxford University Press,
London.
Maayan Geffet and Ido Dagan. 2005. The dis-
tributional inclusion hypotheses and lexical entail-
ment. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL’05), University of Michigan.
Oren Glickman and Ido Dagan. 2005. A probabilis-
tic setting and lexical cooccurrence model for tex-
tual entailment. In ACL-05 Workshop on Empirical
Modeling of Semantic Equivalence and Entailment.
Zellig Harris. 1985. Distributional structure. In Jer-
rold J. Katz, editor, The Philosophy of Linguistics,
pages 26–47. Oxford University Press.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, 2:419–444.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
ofACL-08: HLT, pages 236–244, Columbus, Ohio,
June. Association for Computational Linguistics.
W. D. Munn. 1974. Free inverse semigroup. Proceed-
ings of the London Mathematical Society, 29:385–
404.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In IJ-
CAI, pages 448–453.
Daniel D. Sleator and Davy Temperley. 1991. Pars-
ing english with a link grammar. Technical Report
CMU-CS-91-196, Department of Computer Sci-
ence, Carnegie Mellon University.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th International
Conference of Computational Linguistics, COLING-
2004, Geneva, Switzerland.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
Second Symposium on Quantum Interaction, Ox-
ford, UK.
Ludwig Wittgenstein. 1953. PhilosophicalInvestiga-
tions. Macmillan, New York. G. Anscombe, trans-
lator.
</reference>
<page confidence="0.999091">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867555">
<title confidence="0.997637">Context-theoretic Semantics for Natural Language: an Overview</title>
<author confidence="0.976259">Daoud</author>
<affiliation confidence="0.999873">University of</affiliation>
<address confidence="0.936832">Falmer, Brighton,</address>
<email confidence="0.999947">daoud.clarke@gmail.com</email>
<abstract confidence="0.997187277777778">We present the context-theoretic framework, which provides a set of rules for the nature of composition of meaning based the philosophy of as Principally, in the framework the composition of the meaning of words can be represented as multiplication of their representative vectors, where multiplication is distributive with respect to the vector space. We discuss the applicability of the framework to a range of techniques in natural language processing, including subsequence matching, the lexical entailment model of Dagan et al. (2005), vector-based representations of taxonomies, statistical parsing and the representation of uncertainty in logical semantics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y A Abramovich</author>
<author>Charalambos D Aliprantis</author>
</authors>
<title>An Invitation to Operator Theory.</title>
<date>2002</date>
<publisher>American Mathematical Society.</publisher>
<contexts>
<context position="6947" citStr="Abramovich and Aliprantis, 2002" startWordPosition="1193" endWordPosition="1196"> is a vector space under the point-wise operations: (αu)(s) = αu(s) (u + v)(s) = u(s) + v(s) for u, v E L1(S) and α E R, but it is also a lattice under the operations (u ∧ v)(s) = min(u(s), v(s)) (u V v)(s) = max(u(s), v(s)). In fact it is a vector lattice or Riesz space (Aliprantis and Burkinshaw, 1985) since it satisfies the following relationships if u &lt; v then αu &lt; αv if u &lt; v then u + w &lt; v + w, where α E R+ and &lt; is the partial ordering associated with the lattice operations, defined by u &lt; v if u ∧ v = u. Together with the l1 norm, the vector lattice defines an Abstract Lebesgue space (Abramovich and Aliprantis, 2002) a vector space incorporating all the properties of a measure space, and thus can also be thought of as defining a probability space, where V and ∧ correspond to the union and intersection of events in the σ algebra, and the norm corresponds to the (un-normalised) probability. 2.1 Distributional Generality The vector lattice nature of the space under consideration is important in the context-theoretic framework since it is used to define a degree of entailment between strings. Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al., 2004), a generalisati</context>
</contexts>
<marker>Abramovich, Aliprantis, 2002</marker>
<rawString>Y. A. Abramovich and Charalambos D. Aliprantis. 2002. An Invitation to Operator Theory. American Mathematical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charalambos D Aliprantis</author>
<author>Owen Burkinshaw</author>
</authors>
<title>Positive Operators.</title>
<date>1985</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="6620" citStr="Aliprantis and Burkinshaw, 1985" startWordPosition="1121" endWordPosition="1125">t an estimate of relative frequency distributions of the strings over the contexts, since a frequency distribution will always satisfy these requirements. Note also that the l1 norm of the context vector of a string is simply the sum of all its components and is thus proportional to its probability. The set of functions L1(S) is a vector space under the point-wise operations: (αu)(s) = αu(s) (u + v)(s) = u(s) + v(s) for u, v E L1(S) and α E R, but it is also a lattice under the operations (u ∧ v)(s) = min(u(s), v(s)) (u V v)(s) = max(u(s), v(s)). In fact it is a vector lattice or Riesz space (Aliprantis and Burkinshaw, 1985) since it satisfies the following relationships if u &lt; v then αu &lt; αv if u &lt; v then u + w &lt; v + w, where α E R+ and &lt; is the partial ordering associated with the lattice operations, defined by u &lt; v if u ∧ v = u. Together with the l1 norm, the vector lattice defines an Abstract Lebesgue space (Abramovich and Aliprantis, 2002) a vector space incorporating all the properties of a measure space, and thus can also be thought of as defining a probability space, where V and ∧ correspond to the union and intersection of events in the σ algebra, and the norm corresponds to the (un-normalised) probabil</context>
</contexts>
<marker>Aliprantis, Burkinshaw, 1985</marker>
<rawString>Charalambos D. Aliprantis and Owen Burkinshaw. 1985. Positive Operators. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Symposium on Quantum Interaction,</booktitle>
<pages>133--140</pages>
<location>Oxford, UK,</location>
<contexts>
<context position="26435" citStr="Clark et al. (2008)" startWordPosition="4653" endWordPosition="4656"> linearity in the composition function, although in practice the authors do adopt this assumption. Indeed their “multiplicative models” require composition of two vectors to be a linear function of their tensor product; this is equivalent to our requirement of distributivity with respect to vector space addition. Various ways of composing vector based representations of meaning were investigated by Widdows (2008), including the tensor product and direct sum. Both of these are compatible with the context theoretic framework since they are distributive with respect to the vector space addition. Clark et al. (2008) proposed a method of composing meaning that generalises Montague semantics; further work is required to determine how their method of composition relates to the contexttheoretic framework. Erk and Pado (2008) describe a method of composition that allows the incorporation of selectional preferences; again further work is required to determine the relation between this work and the context-theoretic framework. s(they, mashed) m(mashed, through) o(mashed, way) d(their, way) j(through, mud) d(the, mud) a(thick, mud) 118 5 Conclusion We have given an introduction to the contexttheoretic framework,</context>
</contexts>
<marker>Clark, Coecke, Sadrzadeh, 2008</marker>
<rawString>Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of meaning. In Proceedings of the Second Symposium on Quantum Interaction, Oxford, UK, pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Meaning as context and subsequence analysis for textual entailment.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Recognising Textual Entailment Challenge.</booktitle>
<contexts>
<context position="11733" citStr="Clarke, 2006" startWordPosition="2045" endWordPosition="2046">E A* is called a “subsequence” of y E A* if each element of x occurs in y in the same order, but with the possibility of other elements occurring in between, so for example abba is a subsequence of acabcba in {a, b, c}*. We denote the set of subsequences of x (including the empty string) by Sub(x). Subsequence matching compares the subsequences of two strings: the xˆ · yˆ= xy, 11u n v111 11u111 . 114 more subsequences they have in common the more similar they are assumed to be. This idea has been used successfully in text classification (Lodhi et al., 2002) and recognising textual entailment (Clarke, 2006). We can describe such models using a context theory (A, A*, ˆ,� ), where • is convolution in L1(A*) and xˆ = (1/2|x|) � ey, yESub(x) i.e. the context vector of a string is a weighted sum of its subsequences. Under this context theory xˆ &lt; ˆy, i.e. x completely entails y if x is a subsequence of y. Many variations on this context theory are possible, for example using more complex mappings to L1(A*). The context theory can also be adapted to incorporate a measure of lexical overlap between strings, an approach that, although simple, performs comparably to more complex techniques in tasks such </context>
</contexts>
<marker>Clarke, 2006</marker>
<rawString>Daoud Clarke. 2006. Meaning as context and subsequence analysis for textual entailment. In Proceedings of the Second PASCAL Recognising Textual Entailment Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic Semantics for Natural Language: an Algebraic Framework.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Informatics, University of Sussex.</institution>
<contexts>
<context position="1596" citStr="Clarke, 2007" startWordPosition="252" endWordPosition="253">eanings of words as vectors, yet there is currently no theory of natural language semantics that explains how we should compose these representations: what should the representation of a phrase be, given the representation of the words in the phrase? In this paper we present such a theory, which is based on the philosophy of meaning as context, as epitomised by the famous sayings of Wittgenstein (1953), “Meaning just is use” and Firth (1957), “You shall know a word by the company it keeps”. For the sake of brevity we shall present only a summary of our research, which is described in full in (Clarke, 2007), and we give a simplified version of the framework, which nevertheless suffices for the examples which follow. We believe that the development of theories that can take vector representations of meaning beyond the word level, to the phrasal and sentence levels and beyond are essential for vector based semantics to truly compete with logical semantics, both in their academic standing and in application to real problems in natural language processing. Moreover the time is ripe for such a theory: never has there been such an abundance of immediately available textual data (in the form of the wor</context>
</contexts>
<marker>Clarke, 2007</marker>
<rawString>Daoud Clarke. 2007. Context-theoretic Semantics for Natural Language: an Algebraic Framework. Ph.D. thesis, Department of Informatics, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="708" citStr="Dagan et al. (2005)" startWordPosition="105" endWordPosition="108">Sussex Falmer, Brighton, UK daoud.clarke@gmail.com Abstract We present the context-theoretic framework, which provides a set of rules for the nature of composition of meaning based on the philosophy of meaning as context. Principally, in the framework the composition of the meaning of words can be represented as multiplication of their representative vectors, where multiplication is distributive with respect to the vector space. We discuss the applicability of the framework to a range of techniques in natural language processing, including subsequence matching, the lexical entailment model of Dagan et al. (2005), vector-based representations of taxonomies, statistical parsing and the representation of uncertainty in logical semantics. 1 Introduction Techniques such as latent semantic analysis (Deerwester et al., 1990) and its variants have been very successful in representing the meanings of words as vectors, yet there is currently no theory of natural language semantics that explains how we should compose these representations: what should the representation of a phrase be, given the representation of the words in the phrase? In this paper we present such a theory, which is based on the philosophy o</context>
<context position="3336" citStr="Dagan et al., 2005" startWordPosition="534" endWordPosition="538"> framework does not provide a formula for how to compose meaning; rather it provides mathematical guidelines for theories of meaning. It describes the nature of the vector space in which meanings live, gives some restrictions on how meanings compose, and provides us with a measure of the degree of entailment between strings for any implementation of the framework. The remainder of the paper is structured as follows: in Section 2 we present the framework; in Section 3 we present applications of the framework: • We describe subsequence matching (Section 3.1) and the lexical entailment model of (Dagan et al., 2005) (Section 3.2), both of which have been applied to the task of recognising textual entailment. • We show how a vector based representation of a taxonomy incorporating probabilistic information about word meanings can be conProceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 112–119, Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics 112 xˆ E L1(S)+, where L1(S) means the set of all functions from S to the real numbers R which are finite under the L1 norm, llull1 = 1: |u(s)| d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6 d1 d2 </context>
<context position="12387" citStr="Dagan et al., 2005" startWordPosition="2158" endWordPosition="2161"> a context theory (A, A*, ˆ,� ), where • is convolution in L1(A*) and xˆ = (1/2|x|) � ey, yESub(x) i.e. the context vector of a string is a weighted sum of its subsequences. Under this context theory xˆ &lt; ˆy, i.e. x completely entails y if x is a subsequence of y. Many variations on this context theory are possible, for example using more complex mappings to L1(A*). The context theory can also be adapted to incorporate a measure of lexical overlap between strings, an approach that, although simple, performs comparably to more complex techniques in tasks such as recognising textual entailment (Dagan et al., 2005) 3.2 Lexical Entailment Model Glickman and Dagan (2005) define their own model of entailment and apply it to the task of recognising textual entailment. They estimate entailment between words based on occurrences in documents: they estimate a lexical entailment probability LEP(x, y) between two terms x and y to be nx &apos;y LEP (x, y) ^ ny where ny and nx,y denote the number of documents that the word y occurs in and the words x and y both occur in respectively. We can describe this using a context theory (A, D, ˆ, • ), where D is the set of documents, and Newman (Dublin) 0.565 0.6 115 Model Accur</context>
<context position="13906" citStr="Dagan et al., 2005" startWordPosition="2426" endWordPosition="2429"> theory. We have also used this idea to determine entailment, using latent Dirichlet allocation to get around the problem of data sparseness. A model was built using a subset of around 380,000 documents from the Gigaword corpus, and the model was evaluated on the dataset from the first Recognising Textual Entailment Challenge; the results are shown in Table 1. In order to use the model, a document length had to be chosen; it was found that very long documents yielded better perforDagan ce at this task. Dirichlet (106) 0.584 0.630 Glickman (Bar Ilan) 0.586 0.572 confidence weighted score —see (Dagan et al., 2005) for the definition. � 1 if x occurs in document d ˆx(d) = . 0 otherwise. In this case the estimate of LEP(x, y) coincides with our own degree of entailment Ent(x, y). There are many ways in which the multiplication • can be defined on L1(D). The simplest one defines ed • ef = ed if d = f and edef = 0 otherwise. The effect of multiplication of the context vectors of two strings is then set intersection: context-theoretic fr amework. This opens up the way to the possibility of new techniques that combine the vector-based representations of word meanings with the ontological ones, for example: T</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>George Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="918" citStr="Deerwester et al., 1990" startWordPosition="132" endWordPosition="136">ning as context. Principally, in the framework the composition of the meaning of words can be represented as multiplication of their representative vectors, where multiplication is distributive with respect to the vector space. We discuss the applicability of the framework to a range of techniques in natural language processing, including subsequence matching, the lexical entailment model of Dagan et al. (2005), vector-based representations of taxonomies, statistical parsing and the representation of uncertainty in logical semantics. 1 Introduction Techniques such as latent semantic analysis (Deerwester et al., 1990) and its variants have been very successful in representing the meanings of words as vectors, yet there is currently no theory of natural language semantics that explains how we should compose these representations: what should the representation of a phrase be, given the representation of the words in the phrase? In this paper we present such a theory, which is based on the philosophy of meaning as context, as epitomised by the famous sayings of Wittgenstein (1953), “Meaning just is use” and Firth (1957), “You shall know a word by the company it keeps”. For the sake of brevity we shall presen</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="26644" citStr="Erk and Pado (2008)" startWordPosition="4686" endWordPosition="4689">r product; this is equivalent to our requirement of distributivity with respect to vector space addition. Various ways of composing vector based representations of meaning were investigated by Widdows (2008), including the tensor product and direct sum. Both of these are compatible with the context theoretic framework since they are distributive with respect to the vector space addition. Clark et al. (2008) proposed a method of composing meaning that generalises Montague semantics; further work is required to determine how their method of composition relates to the contexttheoretic framework. Erk and Pado (2008) describe a method of composition that allows the incorporation of selectional preferences; again further work is required to determine the relation between this work and the context-theoretic framework. s(they, mashed) m(mashed, through) o(mashed, way) d(their, way) j(through, mud) d(the, mud) a(thick, mud) 118 5 Conclusion We have given an introduction to the contexttheoretic framework, which provides mathematical guidelines on how vector-based representations of meaning should be composed, how entailment should be determined between these representations, and how probabilistic information s</context>
</contexts>
<marker>Erk, Pado, 2008</marker>
<rawString>Katrin Erk and Sebastian Pado. 2008. A structured vector space model for word meaning in context. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1989</date>
<editor>Christaine Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>1989</marker>
<rawString>Christaine Fellbaum, editor. 1989. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>Modes of meaning.</title>
<date>1957</date>
<booktitle>In Papers in Linguistics 1934–1951. Oxford</booktitle>
<publisher>University Press,</publisher>
<location>London.</location>
<contexts>
<context position="1428" citStr="Firth (1957)" startWordPosition="220" endWordPosition="221">ogical semantics. 1 Introduction Techniques such as latent semantic analysis (Deerwester et al., 1990) and its variants have been very successful in representing the meanings of words as vectors, yet there is currently no theory of natural language semantics that explains how we should compose these representations: what should the representation of a phrase be, given the representation of the words in the phrase? In this paper we present such a theory, which is based on the philosophy of meaning as context, as epitomised by the famous sayings of Wittgenstein (1953), “Meaning just is use” and Firth (1957), “You shall know a word by the company it keeps”. For the sake of brevity we shall present only a summary of our research, which is described in full in (Clarke, 2007), and we give a simplified version of the framework, which nevertheless suffices for the examples which follow. We believe that the development of theories that can take vector representations of meaning beyond the word level, to the phrasal and sentence levels and beyond are essential for vector based semantics to truly compete with logical semantics, both in their academic standing and in application to real problems in natura</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. Modes of meaning. In Papers in Linguistics 1934–1951. Oxford University Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<institution>University of Michigan.</institution>
<contexts>
<context position="7754" citStr="Geffet and Dagan (2005)" startWordPosition="1331" endWordPosition="1334">tion of events in the σ algebra, and the norm corresponds to the (un-normalised) probability. 2.1 Distributional Generality The vector lattice nature of the space under consideration is important in the context-theoretic framework since it is used to define a degree of entailment between strings. Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al., 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). Weeds et al. (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. The partial ordering of the vector lattice encapsulates these properties since xˆ &lt; yˆ if and only if y occurs more frequently in all the contexts in which x occurs. This partial ordering is a strict relationship, however, that is unlikely to exist between any two given vectors. Because of this, we define a degree of entailment Ent(u, v) = This value has the properties of a conditional probability; in the case of u = xˆ and </context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Maayan Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
</authors>
<title>A probabilistic setting and lexical cooccurrence model for textual entailment.</title>
<date>2005</date>
<booktitle>In ACL-05 Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</booktitle>
<contexts>
<context position="12442" citStr="Glickman and Dagan (2005)" startWordPosition="2166" endWordPosition="2169">lution in L1(A*) and xˆ = (1/2|x|) � ey, yESub(x) i.e. the context vector of a string is a weighted sum of its subsequences. Under this context theory xˆ &lt; ˆy, i.e. x completely entails y if x is a subsequence of y. Many variations on this context theory are possible, for example using more complex mappings to L1(A*). The context theory can also be adapted to incorporate a measure of lexical overlap between strings, an approach that, although simple, performs comparably to more complex techniques in tasks such as recognising textual entailment (Dagan et al., 2005) 3.2 Lexical Entailment Model Glickman and Dagan (2005) define their own model of entailment and apply it to the task of recognising textual entailment. They estimate entailment between words based on occurrences in documents: they estimate a lexical entailment probability LEP(x, y) between two terms x and y to be nx &apos;y LEP (x, y) ^ ny where ny and nx,y denote the number of documents that the word y occurs in and the words x and y both occur in respectively. We can describe this using a context theory (A, D, ˆ, • ), where D is the set of documents, and Newman (Dublin) 0.565 0.6 115 Model Accuracy CWS Dirichlet (107) 0.576 0.642 Bayer (MITRE) 0.586</context>
</contexts>
<marker>Glickman, Dagan, 2005</marker>
<rawString>Oren Glickman and Ido Dagan. 2005. A probabilistic setting and lexical cooccurrence model for textual entailment. In ACL-05 Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1985</date>
<booktitle>The Philosophy of Linguistics,</booktitle>
<pages>26--47</pages>
<editor>In Jerrold J. Katz, editor,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="7599" citStr="Harris (1985)" startWordPosition="1303" endWordPosition="1304">roperties of a measure space, and thus can also be thought of as defining a probability space, where V and ∧ correspond to the union and intersection of events in the σ algebra, and the norm corresponds to the (un-normalised) probability. 2.1 Distributional Generality The vector lattice nature of the space under consideration is important in the context-theoretic framework since it is used to define a degree of entailment between strings. Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al., 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). Weeds et al. (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. The partial ordering of the vector lattice encapsulates these properties since xˆ &lt; yˆ if and only if y occurs more frequently in all the contexts in which x occurs. This partial ordering is a strict relationship, however, that is unlikely to exist between any two given ve</context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Zellig Harris. 1985. Distributional structure. In Jerrold J. Katz, editor, The Philosophy of Linguistics, pages 26–47. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="11683" citStr="Lodhi et al., 2002" startWordPosition="2037" endWordPosition="2040">therwise ex(y) = 0. 3.1 Subsequence Matching A string x E A* is called a “subsequence” of y E A* if each element of x occurs in y in the same order, but with the possibility of other elements occurring in between, so for example abba is a subsequence of acabcba in {a, b, c}*. We denote the set of subsequences of x (including the empty string) by Sub(x). Subsequence matching compares the subsequences of two strings: the xˆ · yˆ= xy, 11u n v111 11u111 . 114 more subsequences they have in common the more similar they are assumed to be. This idea has been used successfully in text classification (Lodhi et al., 2002) and recognising textual entailment (Clarke, 2006). We can describe such models using a context theory (A, A*, ˆ,� ), where • is convolution in L1(A*) and xˆ = (1/2|x|) � ey, yESub(x) i.e. the context vector of a string is a weighted sum of its subsequences. Under this context theory xˆ &lt; ˆy, i.e. x completely entails y if x is a subsequence of y. Many variations on this context theory are possible, for example using more complex mappings to L1(A*). The context theory can also be adapted to incorporate a measure of lexical overlap between strings, an approach that, although simple, performs co</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="25708" citStr="Mitchell and Lapata (2008)" startWordPosition="4535" endWordPosition="4538">tainty arises from parsing, word-sense disambiguation or any other source. Incorporating this information is then straightforward: the representation of the sentence is the weighted sum of the representation of each possible meaning, where the weights are given by the probability distribution. Computing the degree of entailment using this approach is computationally challenging, however we have shown that it is possible to estimate the degree of entailment by computing a lower bound on this value by calculating pairwise degrees of entailment for each possible logical statement. 4 Related Work Mitchell and Lapata (2008) proposed a framework for composing meaning that is extremely general in nature: there is no requirement for linearity in the composition function, although in practice the authors do adopt this assumption. Indeed their “multiplicative models” require composition of two vectors to be a linear function of their tensor product; this is equivalent to our requirement of distributivity with respect to vector space addition. Various ways of composing vector based representations of meaning were investigated by Widdows (2008), including the tensor product and direct sum. Both of these are compatible </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings ofACL-08: HLT, pages 236–244, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W D Munn</author>
</authors>
<title>Free inverse semigroup.</title>
<date>1974</date>
<booktitle>Proceedings of the London Mathematical Society,</booktitle>
<pages>29--385</pages>
<contexts>
<context position="21387" citStr="Munn, 1974" startWordPosition="3765" endWordPosition="3766">terms of its generative power; for the sake of brevity we omit the details, although a sample link grammar parse is show in Figure 3. Our formulation of link grammar as a context theory makes use of a construction called a free inverse semigroup. Informally, the free inverse semigroup on a set S is formed from elements of S and their inverses, S−1 = {s−1 : s ∈ S}, satisfying no other condition than those of an inverse semigroup. Formally, the free inverse semigroup is defined in terms of a congruence relation on (S ∪S−1)∗ specifying the inverse property and commutativity of idempotents — see (Munn, 1974) for details. We denote the free inverse semigroup on S by FIS(S). Free inverse semigroups were shown by Munn (1974) to be equivalent to birooted word trees. A birooted word-tree on a set A is a directed acyclic graph whose edges are labelled by elements of A which does not contain any subgraphs of the form −→ •, together with a two distinguished nodes, called the start node, ✷ and finish node, ◦. An element in the free semigroup FIS(S) is denoted as a sequence xd1x22 ... xdnn where xi ∈ S and di ∈ {1, −1}. We construct the birooted word tree by starting with a single node as the start node, a</context>
</contexts>
<marker>Munn, 1974</marker>
<rawString>W. D. Munn. 1974. Free inverse semigroup. Proceedings of the London Mathematical Society, 29:385– 404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In IJCAI,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="17096" citStr="Resnik, 1995" startWordPosition="2998" endWordPosition="2999">T and y &lt; x then y E T. The principal ideal generated by an element x in a partially ordered set S is defined to be the lower  set y(x) = {y E S : y &lt; x}. Proposition 3 (Ideal Completion). If S is a partially ordered set, then 10 can be considered as a function from S to the powerset 2S. Under the partial ordering defined by set inclusion, the set of  lower sets form a complete lattice, and y(�) is a completion of S, the ideal completion. We are also concerned with the probability of concepts. This is an idea that has come about through the introduction of “distance measures” on taxonomies (Resnik, 1995). Since terms can be ascribed probabilities based on their frequencies of occurrence in corpora, the concepts they refer to can similarly be assigned probabilities. The probability of a concept is the probability of encountering an instance of that concept in the corpus, that is, the probability that a term selected at random from the corpus has a meaning that is subsumed by that particular concept. This ensures that more general concepts are given higher probabilities, for example if there is a most general concept (a top-most node in the taxonomy, which may correspond for example to “entity”</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In IJCAI, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing english with a link grammar.</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-196,</tech>
<institution>Department of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="20591" citStr="Sleator and Temperley, 1991" startWordPosition="3621" endWordPosition="3624">an be true is if �(x) ⊆ �(y) since p is non-zero everywhere. If this is the case, then x ≤ y by the nature of the ideal completion. Thus ψ is an order-embedding, and since L1(S) is a complete lattice, it is also a completion. Finally, note that kψ(x)k1 = Ey∈↓(x) p(y) = ˆp(x). This completion allows us to represent concepts as elements within a vector lattice so that not only the partial ordering of the taxonomy is preserved, but the probability of concepts is also preserved as the size of the vector under the L1 norm. 3.4 Representing Syntax In this section we give a description link grammar (Sleator and Temperley, 1991) in terms of a context theory. Link grammar is a lexicalised syntactic formalism which describes properties of words in terms of links formed between them, and which is context-free in terms of its generative power; for the sake of brevity we omit the details, although a sample link grammar parse is show in Figure 3. Our formulation of link grammar as a context theory makes use of a construction called a free inverse semigroup. Informally, the free inverse semigroup on a set S is formed from elements of S and their inverses, S−1 = {s−1 : s ∈ S}, satisfying no other condition than those of an i</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Daniel D. Sleator and Davy Temperley. 1991. Parsing english with a link grammar. Technical Report CMU-CS-91-196, Department of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference of Computational Linguistics, COLING2004,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="7531" citStr="Weeds et al., 2004" startWordPosition="1291" endWordPosition="1294">e (Abramovich and Aliprantis, 2002) a vector space incorporating all the properties of a measure space, and thus can also be thought of as defining a probability space, where V and ∧ correspond to the union and intersection of events in the σ algebra, and the norm corresponds to the (un-normalised) probability. 2.1 Distributional Generality The vector lattice nature of the space under consideration is important in the context-theoretic framework since it is used to define a degree of entailment between strings. Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al., 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). Weeds et al. (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. The partial ordering of the vector lattice encapsulates these properties since xˆ &lt; yˆ if and only if y occurs more frequently in all the contexts in which x occurs. This partial ordering is a strict relat</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of the 20th International Conference of Computational Linguistics, COLING2004, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Symposium on Quantum Interaction,</booktitle>
<location>Oxford, UK.</location>
<contexts>
<context position="26232" citStr="Widdows (2008)" startWordPosition="4620" endWordPosition="4622"> entailment for each possible logical statement. 4 Related Work Mitchell and Lapata (2008) proposed a framework for composing meaning that is extremely general in nature: there is no requirement for linearity in the composition function, although in practice the authors do adopt this assumption. Indeed their “multiplicative models” require composition of two vectors to be a linear function of their tensor product; this is equivalent to our requirement of distributivity with respect to vector space addition. Various ways of composing vector based representations of meaning were investigated by Widdows (2008), including the tensor product and direct sum. Both of these are compatible with the context theoretic framework since they are distributive with respect to the vector space addition. Clark et al. (2008) proposed a method of composing meaning that generalises Montague semantics; further work is required to determine how their method of composition relates to the contexttheoretic framework. Erk and Pado (2008) describe a method of composition that allows the incorporation of selectional preferences; again further work is required to determine the relation between this work and the context-theor</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second Symposium on Quantum Interaction, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludwig Wittgenstein</author>
</authors>
<date>1953</date>
<publisher>PhilosophicalInvestigations. Macmillan,</publisher>
<location>New York. G. Anscombe, translator.</location>
<contexts>
<context position="1388" citStr="Wittgenstein (1953)" startWordPosition="213" endWordPosition="214">sing and the representation of uncertainty in logical semantics. 1 Introduction Techniques such as latent semantic analysis (Deerwester et al., 1990) and its variants have been very successful in representing the meanings of words as vectors, yet there is currently no theory of natural language semantics that explains how we should compose these representations: what should the representation of a phrase be, given the representation of the words in the phrase? In this paper we present such a theory, which is based on the philosophy of meaning as context, as epitomised by the famous sayings of Wittgenstein (1953), “Meaning just is use” and Firth (1957), “You shall know a word by the company it keeps”. For the sake of brevity we shall present only a summary of our research, which is described in full in (Clarke, 2007), and we give a simplified version of the framework, which nevertheless suffices for the examples which follow. We believe that the development of theories that can take vector representations of meaning beyond the word level, to the phrasal and sentence levels and beyond are essential for vector based semantics to truly compete with logical semantics, both in their academic standing and i</context>
</contexts>
<marker>Wittgenstein, 1953</marker>
<rawString>Ludwig Wittgenstein. 1953. PhilosophicalInvestigations. Macmillan, New York. G. Anscombe, translator.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>