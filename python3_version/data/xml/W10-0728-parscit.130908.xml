<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023799">
<title confidence="0.9995175">
Preliminary Experience with Amazon’s Mechanical Turk
for Annotating Medical Named Entities
</title>
<author confidence="0.925015">
Meliha Yetisgen-Yildiz, Imre Solti Fei Xia, Scott Russell Halgrim
</author>
<affiliation confidence="0.833887">
Biomedical &amp; Health Informatics Department of Linguistics
University of Washington University of Washington
Seattle, WA 98195, USA Seattle, WA 98195, USA
</affiliation>
<email confidence="0.999056">
{melihay,solti}@uw.edu {fxia,captnpi}@uw.edu
</email>
<sectionHeader confidence="0.997343" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997570384615385">
Amazon’s Mechanical Turk (MTurk) service
is becoming increasingly popular in Natural
Language Processing (NLP) research. In this
paper, we report our findings in using MTurk
to annotate medical text extracted from clini-
cal trial descriptions with three entity types:
medical condition, medication, and laboratory
test. We compared MTurk annotations with a
gold standard manually created by a domain
expert. Based on the good performance re-
sults, we conclude that MTurk is a very prom-
ising tool for annotating large-scale corpora
for biomedical NLP tasks.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996475">
The manual construction of annotated corpora is ex-
tremely expensive both in terms of time and money.
Snow et al. (2008) demonstrated the potential power of
Amazon’s Mechanical Turk (MTurk) service in annotat-
ing large corpora for natural language tasks cheaply and
quickly. We are working on a Natural Language Proc-
essing (NLP) project to automate the clinical trial eligi-
bility screening of patients. This project involves
building statistical models for medical named entity
recognition which requires a large-scale annotated cor-
pus for training. As part of corpus development, we
tested the feasibility of using MTurk for the annotation
of medical named entities in biomedical text and we
report our findings in this paper.
In the following sections we describe how we used
MTurk to annotate the biomedical corpus created from
publicly available clinical trial announcements. The
main goal of our study was to understand how well non-
experts perform compared to medical expert in annotat-
ing the biomedical text.
</bodyText>
<sectionHeader confidence="0.998907" genericHeader="related work">
2 Related Work
</sectionHeader>
<footnote confidence="0.71037425">
MTurk1 is an online micro-task market that allows re-
questers to distribute work to a large number of workers
from all over the world. The inspiration of the system
1 https://www.MTurk.com/MTurk/welcome
</footnote>
<bodyText confidence="0.999791454545454">
was to have human workers complete simple tasks that
would otherwise be extremely difficult for computers to
perform (Kittur et al., 2008). A complex task is broken
down into simple, one-time tasks called Human Intelli-
gence Tasks (HITs). Requesters post their HITs on the
MTurk marketplace by specifying the amount paid for
the completion of each task, and the workers select from
the available HITs the ones that they would like to work
on. In 2007, Amazon claimed that the user base of
MTurk consisted of over 100,000 users from 100 coun-
tries2.
MTurk has been adopted for a variety of uses both in
industry and academia, ranging from user studies (Kittur
et al., 2008) to image labeling (Sorokin and Forsyth,
2008). Snow et al. (2008) examined the quality of labels
created by MTurk workers for various NLP tasks in-
cluding word sense disambiguation, word similarity,
text entailment, and temporal ordering. Since the publi-
cation of Snow et al.’s paper, MTurk has become in-
creasingly popular as an annotation tool for NLP
research. Nakov (2008) used MTurk to create a manu-
ally annotated resource for noun-noun compound inter-
pretation based on paraphrasing verbs. In a different
NLP task, Callison-Burch (2009) used MTurk to evalu-
ate machine translation quality. With a budget of only
$10, Callison-Burch demonstrated the feasibility of per-
forming manual evaluations of machine translation
quality by recreating judgments from a WMT08 transla-
tion task.
In our pilot study we used MTurk to annotate entities in
the biomedical text. To our knowledge, this is the first
study that investigates the feasibility of MTurk for bio-
medical named entity annotation.
</bodyText>
<sectionHeader confidence="0.98924" genericHeader="method">
3 Annotation Task Description
</sectionHeader>
<bodyText confidence="0.998552">
In this section we will describe the types of entities in
our annotation task and the details of our corpus crea-
tion process.
</bodyText>
<footnote confidence="0.99492575">
2 Source: New York Times article “Artificial Intelligence,
With Help from the Humans” , Available at:
http://www.nytimes.com/2007/03/25/business/yourmoney/25S
tream.html
</footnote>
<page confidence="0.898584">
180
</page>
<note confidence="0.8367525">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 180–183,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.998327">
3.1 Entity Types
</subsectionHeader>
<bodyText confidence="0.999853">
We used MTurk to annotate the biomedical text for the
following three entity types:
</bodyText>
<listItem confidence="0.921360727272727">
• Medical Conditions
Example: First-degree relative who developed
&lt;Medical_Condition&gt;breast cancer
&lt;/Medical_Condition&gt; at ≤ 50 years of age.
• Medications
Example: Previous treatment with an &lt;Medica-
tion&gt;anthracycline&lt;/Medication&gt; in the metas-
tatic breast cancer setting.
• Laboratory Test
Example: &lt;Laboratory_Test&gt;Platelet count
&gt;=100,000 cells/mL&lt;/Laboratory_Test&gt;.
</listItem>
<subsectionHeader confidence="0.998554">
3.2 Corpus
</subsectionHeader>
<bodyText confidence="0.999989424242424">
Our corpus came from the publicly available clinical
trial announcements available at the ClinicalTrials.gov
website. This website is a registry of federally and pri-
vately supported clinical trials conducted in the United
States and around the world. The objectives and proce-
dures of each clinical trial are explained in detail along
with participant selection criteria and logistical informa-
tion such as locations and contact information.
For this task we selected 50,109 announcements from
the roughly 85,000 announcements posted on the Clini-
calTrials.gov site. For selection criteria we relied on the
following keywords: &amp;quot;heart  |cancer  |tumor  |influenza |
alzheimer  |parkinson  |malignant  |stroke  |respiratory |
diabetes  |pneumonia  |nephritis  |nephrotic  |nephrosis |
septicemia  |liver  |cirrhosis  |hypertension  |renal  |neo-
plasm&amp;quot;. We chose these keywords because they were
part of the phrases of diagnoses for the top 12 leading
causes of death excluding suicide, homicide and acci-
dents (Heron et al., 2009). We limited the selection to
trials for &amp;quot;Adult&amp;quot; or &amp;quot;Senior&amp;quot; patients.
After downloading the corpus of XML files we con-
verted them to ANSI text using ABC Amber XML
Converter3. 49,794 files successfully converted to ANSI
text format. Using a simple regular expression search
we selected documents that had both the &amp;quot;Inclusion Cri-
teria&amp;quot; and &amp;quot;Exclusion Criteria&amp;quot; phrases. The final selec-
tion process resulted in 35,385 files. From this latest set
we randomly selected 100 files to build the corpus for
our pilot study. One of the authors, who has medical
training, then manually annotated the three entity types
in those selected files. We used this annotated set as the
gold standard to measure the quality of the MTurk
workers’ annotations.
</bodyText>
<sectionHeader confidence="0.995518" genericHeader="method">
4 HIT Design
</sectionHeader>
<page confidence="0.3362615">
3 ABC Amber XML Converter. Available at:
http://www.processtext.com/abcxml.html.
</page>
<bodyText confidence="0.9998765">
Biomedical text is full of jargon, and finding the three
entity types in such text can be difficult for non-expert
annotators. To make the annotation task more conven-
ient for the MTurk workers, we used a customized user
interface and provided detailed annotation guidelines.
We also tested the bonus system available in the MTurk
environment and evaluated the performance of the
workers.
</bodyText>
<subsectionHeader confidence="0.991637">
4.1 User Interface
</subsectionHeader>
<bodyText confidence="0.9996725">
In order to adapt the task of entity annotation to the
MTurk format, we used an in-house web-based graphi-
cal user interface that allows the worker to select a span
of text with the mouse cursor. The interface also uses
simple tokenization heuristics to divide the text into
highlightable spans and resolve partial token highlights
or double-clicks into the next largest span. For instance,
highlighting the word “cancer” from the second “c” to
“e” will result in the entire span “cancer” being high-
lighted.
</bodyText>
<subsectionHeader confidence="0.99586">
4.2 Annotation Guidelines
</subsectionHeader>
<bodyText confidence="0.999833">
We created three separate annotation tasks, one for each
entity type. For each task, we wrote annotation guide-
lines that explained the task and showed examples of
entities that should be tagged and the ones that should
not.
</bodyText>
<subsectionHeader confidence="0.999799">
4.3 Bonus System
</subsectionHeader>
<bodyText confidence="0.9999149375">
MTurk provides two methods for paying workers –
fixed rates on each document and bonuses to workers
for especially good work. In this study, we experi-
mented with the bonus system to see its effect on per-
formance and annotation time. Annotating a document
would receive a base rate of $0.01-$0.05, but each
tagged entity span could elicit a bonus of $0.01. The
base rate would cover the case where the document
truly contained no entities, but the bonus amount could
potentially be much larger than the base rate if the
document was entity-rich. Bonuses for each tagged en-
tity span were awarded based on an agreement threshold
with peer workers. In this study, each document was
annotated by four workers and we granted bonuses for
entity spans that were agreed upon by at least three
workers.
</bodyText>
<subsectionHeader confidence="0.993802">
4.4 Performance Monitoring
</subsectionHeader>
<bodyText confidence="0.999995">
We monitored a worker’s performance by comparing
the worker’s annotations with his/her peer workers’
annotations. After we posted the HITs, we continuously
monitored the workers’ performance and rejected the
annotations from the ones who tried to cheat the system
by either not doing any annotations (e.g., immediately
submitting the document after accepting it) or con-
</bodyText>
<page confidence="0.999358">
181
</page>
<tableCaption confidence="0.946745">
Table 1. Cost analysis of annotation experiments (“File” in this table means the annotation of a document.
There are 100 documents, and each document is annotated by four workers.)
</tableCaption>
<table confidence="0.999740545454545">
Experiment Label File Count Total MONETARY COST TIME COST
Worker
Count
Pay Rate ($) Total Cost ($) Completion Time
Total Completed File Bonus File Bonus Per file Total
(seconds) (hours)
MedicalCondition-I 400 272 45 0.01 0 2.72 0 156.09 71.16
MedicalCondition-II 400 400 30 0.05 0.01 20 22.61 162.66 7.28
Medication-I 400 400 45 0.01 0.01 4 4.43 87.96 31.65
Medication-II 400 400 17 0.05 0.01 20 6.11 89.06 4.36
Laboratory Test 400 400 26 0.05 0.01 20 1.49 75.61 24.41
</table>
<bodyText confidence="0.998295142857143">
stantly doing wrong annotations (e.g., always annotating
the first word of the text). Those rejected documents
were automatically re-posted on the MTurk so other
workers could work on them. In this pilot study, per-
formance monitoring was done mainly manually. As
future work, we plan to automate the process in order to
scale it for larger annotation tasks.
</bodyText>
<subsectionHeader confidence="0.999342">
4.5 Communication with Workers
</subsectionHeader>
<bodyText confidence="0.999974">
The workers could send us their questions and com-
ments about the individual documents or the general
annotation task through a text box in the interface. Dur-
ing this study we received more than 100 messages
from the workers. The majority of the messages were
positive messages (“thank you”, “easy hit!”). However,
some of the comments included questions such as: “Is
pregnancy a medical condition?” or “Text doesn’t men-
tion the type of insulin but I highlighted it because insu-
lin is a medication!”. We responded to the questions in
a timely manner to increase the quality of annotations.
</bodyText>
<sectionHeader confidence="0.987387" genericHeader="method">
5 Annotation Experiments
</sectionHeader>
<bodyText confidence="0.9999854">
In our annotation experiments, each of 100 documents
in our corpus was annotated by four workers, resulting
in 100×4=400 files per experiment. We experimented
with different pay scales to understand how they affect
the quality and speed of the annotations.
</bodyText>
<subsectionHeader confidence="0.999744">
5.1 Cost of Annotations
</subsectionHeader>
<bodyText confidence="0.999966446808511">
We investigated the cost of annotations both in terms of
money and time. The summary of the results is in Table
1. We ran five different MTurk annotation experiments
for our corpus of 100 documents. A total of 139 workers
were involved in our experiments, and we identified
eight of those workers as cheaters and rejected their
annotation. The remaining workers spent 138.86 hours
to complete 1872 files. The slowest experiment was
MedicalCondition-I, in which we paid a base document
rate of $0.01 without any bonuses. With this pay scale,
it took 71.16 hours for workers to annotate 272 out of
400 files. We suspected we could not attract enough
workers to finish the annotation task on time so we
stopped the experiment before all 400 files were com-
pleted. When we compiled the results, we noticed that
there was a general tendency for the workers to tag the
first one or two entities and then ignore the rest of the
document. Based on this observation, we decided to add
bonuses to motivate the workers to read through the
whole document. We ran the same annotation task,
MedicalCondition-II, with a higher base document rate
of $0.05 and a bonus rate of $0.01. With this new pay-
ment scale the annotation task was fully completed in
7.28 hours.
We also compared the effect of base rates when the bo-
nus amounts were kept the same. For medication anno-
tations, increasing the base document rate from $0.01 to
$0.05 decreased the total amount of annotation time
from 31.65 hours to 4.36 hours and also decreased the
number of workers from 45 to 17. We ordered the
workers based on the number files they annotated. The
top ranked 5 workers in Medication-I annotated 187
files (46%) and the top ranked 5 workers in Medication-
II annotated 313 files (78%). The difference between
those two values was interesting since it indicated that
by increasing the base rate, we managed to attract work-
ers who worked on more documents.
The average amount of time workers spent per docu-
ment varied based on entity type. They spent the longest
amount of time for medical condition and shortest
amount of time for laboratory test. This can be ex-
plained by the richness of documents in terms of enti-
ties. In the manually created gold standard there were
1159 mentions of medical condition, 518 mentions of
medication, and 249 mentions of laboratory tests. An-
other observation was that the change in pay scales did
not affect the average annotation time per document.
</bodyText>
<subsectionHeader confidence="0.999965">
5.2 Quality of Annotations
</subsectionHeader>
<bodyText confidence="0.9996035">
We measured the quality of the MTurk annotations at
different inter-annotator agreement levels by comparing
the agreed entity spans with the spans in the gold stan-
dard.
</bodyText>
<page confidence="0.998069">
182
</page>
<tableCaption confidence="0.99799">
Table 2. Quality measurement of MTurk annotations (k: Agreement level, P: Precision, R: Recall, F: F-measure;
</tableCaption>
<table confidence="0.990218625">
the highest value for each column is in boldface)
k Medical Condition-II Medication-II Laboratory Test
P Exact F Overlap P Exact F Overlap P Exact F Overlap
R P R F R P R F R P R F
1 0.51 0.66 0.58 0.70 0.99 0.79 0.43 0.73 0.54 0.50 0.84 0.62 0.30 0.52 0.38 0.42 0.73 0.53
2 0.64 0.66 0.65 0.84 0.87 0.86 0.71 0.66 0.68 0.79 0.73 0.76 0.47 0.43 0.45 0.72 0.65 0.68
3 0.63 0.52 0.57 0.89 0.73 0.80 0.78 0.38 0.51 0.93 0.45 0.61 0.29 0.13 0.18 0.86 0.40 0.54
4 0.60 0.31 0.41 0.93 0.48 0.63 0.76 0.10 0.18 0.89 0.12 0.21 0.05 0.00 0.01 1.00 0.08 0.14
</table>
<bodyText confidence="0.9999538">
Given a document annotated by multiple workers and an
agreement level k, there are different ways of creating a
new span file that includes only the spans that are
agreed by at least k workers. One method is to go over
each span in each annotation and output only the spans
that are marked by at least k workers. This method does
not work well when the spans are long and the workers
could disagree on the boundary. We used an alternative
method which first goes over each word position in the
document and marks the positions that are part of spans
in at least k annotations, and then outputs the spans that
cover those marked positions. We call the new span file
agreement-k file.
Once we have created agreement-k file, we compare it
with the gold standard to calculate precision, recall, and
F-measure. A span in agreement-k file and a span in the
gold standard are called an exact match if they are iden-
tical and are called an overlap match if they overlap
(exact match is a special case of overlap match). Table 2
shows the performance for the MedicalCondition-II,
Medication-II, and LaboratoryTest experiments at dif-
ferent agreement levels (k). As can be seen from the
table, as the value of k increased, the precision values
increased and the recall values decreased. For all of the
experiments, the best F-Score was achieved at agree-
ment-level 2.
Of the three entity types, laboratory test was the hardest
partly because laboratory test entities tend to be longer
(the average length for entities in gold standard was
5.25 words, compared to 1.84 words for medication and
3.18 words for medical condition), making the exact
boundary harder to define. The results for MedicalCon-
dition-II and Medication-II were higher than Laborato-
ryTest. In addition, accuracy for Medication-I (not
shown here due to space limit) and Medication-II were
similar, indicating that pay rate did not affect accuracy
much in our experiments. In the future, we plan to in-
crease the number of annotations for each document,
which we believe could further improve the perform-
ance.
</bodyText>
<sectionHeader confidence="0.99968" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999946">
Human annotation is crucial for many NLP tasks. In this
paper, we demonstrated the potential of using MTurk
for annotating medical text. By continuously monitoring
the workers’ performance and using the bonus system,
we acquired high quality annotations from non-expert
MTurk workers with limited time and budget.
As future work, we plan to analyze the MTurk annota-
tions in detail in order to understand the problematic
areas. Based on our observations, we will redesign our
annotation tasks and continue our experiments with
MTurk to create large-scale annotated corpora to be
used in biomedical NLP projects.
</bodyText>
<sectionHeader confidence="0.989644" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9580855">
This project was supported in part by NIH Grants
1K99LM010227-0110 and 5 U54 LM008748.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999962888888889">
[1] Chris Callison-Burch. 2009. Fast, Cheap, and Crea-
tive: Evaluating Translation Quality Using Amazon’s
Mechanical Turk. In Proceedings of EMNLP’09.
[2] Melonie Heron, Donna L. Hoyert, Sherry L. Mur-
phy, Jiaquan Xu, Kenneth D. Kochanek, and Bet-
zaida Tejada-Vera. 2009. Deaths: Final data for
2006. National Vital Statistics Reports, 57:14.
[3] Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing User Studies with Mechanical Turk.
In Proceedings of CHI’08.
[4] Preslav Nakov. 2008. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In Pro-
ceedings of the 13th international conference on Arti-
ficial Intelligence: Methodology, Systems and
Applications (AIMSA 2008), 103–117.
[5] Philip V. Ogren. 2006. Knowtator: a protégé plug-in
for annotated corpus construction. In Proceedings
NAACL HLT’06, 273-275.
[6] Rion Snow, Brendan O&apos;Connor, Daniel Jurafsky and
Andrew Y. Ng. 2008. Cheap and Fast - But is it
Good? Evaluating Non-Expert Annotations for Natu-
ral Language Tasks. In Proceedings of EMNLP’08,
254-263.
[7] Alexander Sorokin and David Forsyth. Utility data
annotation with Amazon Mechanical Turk. In Pro-
ceedings of Computer Vision and Pattern Recogni-
tion Workshop at CVPR’08.
</reference>
<page confidence="0.999198">
183
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958917">
<title confidence="0.999428">Preliminary Experience with Amazon’s Mechanical for Annotating Medical Named Entities</title>
<author confidence="0.997463">Meliha Yetisgen-Yildiz</author>
<author confidence="0.997463">Imre Solti Fei Xia</author>
<author confidence="0.997463">Scott Russell Halgrim</author>
<affiliation confidence="0.998857">Biomedical &amp; Health Informatics Department of Linguistics University of Washington University of Washington</affiliation>
<address confidence="0.999939">Seattle, WA 98195, USA Seattle, WA 98195, USA</address>
<email confidence="0.99968">melihay@uw.edu</email>
<email confidence="0.99968">solti}@uw.edu{fxia@uw.edu</email>
<email confidence="0.99968">captnpi@uw.edu</email>
<abstract confidence="0.997426928571429">Amazon’s Mechanical Turk (MTurk) service is becoming increasingly popular in Natural Language Processing (NLP) research. In this paper, we report our findings in using MTurk to annotate medical text extracted from clinical trial descriptions with three entity types: medical condition, medication, and laboratory test. We compared MTurk annotations with a gold standard manually created by a domain expert. Based on the good performance results, we conclude that MTurk is a very promising tool for annotating large-scale corpora for biomedical NLP tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP’09.</booktitle>
<marker>[1]</marker>
<rawString>Chris Callison-Burch. 2009. Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk. In Proceedings of EMNLP’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melonie Heron</author>
<author>Donna L Hoyert</author>
<author>Sherry L Murphy</author>
<author>Jiaquan Xu</author>
<author>Kenneth D Kochanek</author>
<author>Betzaida Tejada-Vera</author>
</authors>
<title>Deaths: Final data for 2006. National Vital Statistics Reports,</title>
<date>2009</date>
<pages>57--14</pages>
<marker>[2]</marker>
<rawString>Melonie Heron, Donna L. Hoyert, Sherry L. Murphy, Jiaquan Xu, Kenneth D. Kochanek, and Betzaida Tejada-Vera. 2009. Deaths: Final data for 2006. National Vital Statistics Reports, 57:14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Kittur</author>
<author>Ed H Chi</author>
<author>Bongwon Suh</author>
</authors>
<title>Crowdsourcing User Studies with Mechanical Turk.</title>
<date>2008</date>
<booktitle>In Proceedings of CHI’08.</booktitle>
<marker>[3]</marker>
<rawString>Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008. Crowdsourcing User Studies with Mechanical Turk. In Proceedings of CHI’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Noun compound interpretation using paraphrasing verbs: Feasibility study.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th international conference on Artificial Intelligence: Methodology, Systems and Applications (AIMSA 2008),</booktitle>
<pages>103--117</pages>
<marker>[4]</marker>
<rawString>Preslav Nakov. 2008. Noun compound interpretation using paraphrasing verbs: Feasibility study. In Proceedings of the 13th international conference on Artificial Intelligence: Methodology, Systems and Applications (AIMSA 2008), 103–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip V Ogren</author>
</authors>
<title>Knowtator: a protégé plug-in for annotated corpus construction.</title>
<date>2006</date>
<booktitle>In Proceedings NAACL HLT’06,</booktitle>
<pages>273--275</pages>
<marker>[5]</marker>
<rawString>Philip V. Ogren. 2006. Knowtator: a protégé plug-in for annotated corpus construction. In Proceedings NAACL HLT’06, 273-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O&apos;Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP’08,</booktitle>
<pages>254--263</pages>
<marker>[6]</marker>
<rawString>Rion Snow, Brendan O&apos;Connor, Daniel Jurafsky and Andrew Y. Ng. 2008. Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks. In Proceedings of EMNLP’08, 254-263.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexander Sorokin</author>
<author>David Forsyth</author>
</authors>
<title>Utility data annotation with Amazon Mechanical Turk.</title>
<booktitle>In Proceedings of Computer Vision and Pattern Recognition Workshop at CVPR’08.</booktitle>
<marker>[7]</marker>
<rawString>Alexander Sorokin and David Forsyth. Utility data annotation with Amazon Mechanical Turk. In Proceedings of Computer Vision and Pattern Recognition Workshop at CVPR’08.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>