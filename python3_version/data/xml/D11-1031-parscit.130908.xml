<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000818">
<title confidence="0.88555">
Training a Log-Linear Parser with Loss Functions via Softmax-Margin
</title>
<author confidence="0.991928">
Michael Auli Adam Lopez
</author>
<affiliation confidence="0.997119">
School of Informatics HLTCOE
University of Edinburgh Johns Hopkins University
</affiliation>
<email confidence="0.998469">
m.auli@sms.ed.ac.uk alopez@cs.jhu.edu
</email>
<sectionHeader confidence="0.996655" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999643681818182">
Log-linear parsing models are often trained
by optimizing likelihood, but we would prefer
to optimise for a task-specific metric like F-
measure. Softmax-margin is a convex objec-
tive for such models that minimises a bound
on expected risk for a given loss function, but
its naive application requires the loss to de-
compose over the predicted structure, which
is not true of F-measure. We use softmax-
margin to optimise a log-linear CCG parser for
a variety of loss functions, and demonstrate
a novel dynamic programming algorithm that
enables us to use it with F-measure, lead-
ing to substantial gains in accuracy on CCG-
Bank. When we embed our loss-trained parser
into a larger model that includes supertagging
features incorporated via belief propagation,
we obtain further improvements and achieve
a labelled/unlabelled dependency F-measure
of 89.3%/94.0% on gold part-of-speech tags,
and 87.2%/92.8% on automatic part-of-speech
tags, the best reported results for this task.
</bodyText>
<sectionHeader confidence="0.998855" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989182051282">
Parsing models based on Conditional Random
Fields (CRFs; Lafferty et al., 2001) have been very
successful (Clark and Curran, 2007; Finkel et al.,
2008). In practice, they are usually trained by max-
imising the conditional log-likelihood (CLL) of the
training data. However, it is widely appreciated that
optimizing for task-specific metrics often leads to
better performance on those tasks (Goodman, 1996;
Och, 2003).
An especially attractive means of accomplishing
this for CRFs is the softmax-margin (SMM) ob-
jective (Sha and Saul, 2006; Povey and Woodland,
2008; Gimpel and Smith, 2010a) (§2). In addition to
retaining a probabilistic interpretation and optimiz-
ing towards a loss function, it is also convex, mak-
ing it straightforward to optimise. Gimpel and Smith
(2010a) show that it can be easily implemented with
a simple change to standard likelihood-based train-
ing, provided that the loss function decomposes over
the predicted structure.
Unfortunately, the widely-used F-measure met-
ric does not decompose over parses. To solve this,
we introduce a novel dynamic programming algo-
rithm that enables us to compute the exact quanti-
ties needed under the softmax-margin objective us-
ing F-measure as a loss (§3). We experiment with
this and several other metrics, including precision,
recall, and decomposable approximations thereof.
Our ability to optimise towards exact metrics en-
ables us to verify the effectiveness of more effi-
cient approximations. We test the training proce-
dures on the state-of-the-art Combinatory Categorial
Grammar (CCG; Steedman 2000) parser of Clark
and Curran (2007), obtaining substantial improve-
ments under a variety of conditions. We then embed
this model into a more accurate model that incor-
porates additional supertagging features via loopy
belief propagation. The improvements are additive,
obtaining the best reported results on this task (§4).
</bodyText>
<sectionHeader confidence="0.991796" genericHeader="method">
2 Softmax-Margin Training
</sectionHeader>
<bodyText confidence="0.993966">
The softmax-margin objective modifies the standard
likelihood objective for CRF training by reweighting
</bodyText>
<page confidence="0.989876">
333
</page>
<note confidence="0.9578885">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 333–343,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999926933333333">
each possible outcome of a training input according
to its risk, which is simply the loss incurred on a par-
ticular example. This is done by incorporating the
loss function directly into the linear scoring function
of an individual example.
Formally, we are given m training pairs
(x(1), y(1))...(x(m), y(m)), where each x(z) E X is
drawn from the set of possible inputs, and each
y(z) E Y(x(z)) is drawn from a set of possible
instance-specific outputs. We want to learn the K
parameters θ of a log-linear model, where each λk E
θ is the weight of an associated feature hk(x, y).
Function f(x, y) maps input/output pairs to the vec-
tor h1(x, y)...hK(x, y), and our log-linear model as-
signs probabilities in the usual way.
</bodyText>
<equation confidence="0.994864666666667">
exp{θTf(x, y)}
p(y|x) = (1)
Ey,EY(x) exp{θTf (x, y&apos;)}
</equation>
<bodyText confidence="0.999651666666667">
The conditional log-likelihood objective function is
given by Eq. 2 (Figure 1). Now consider a function
`(y, y&apos;) that returns the loss incurred by choosing to
output y&apos; when the correct output is y. The softmax-
margin objective simply modifies the unnormalised,
unexponentiated score θTf(x, y&apos;) by adding `(y, y&apos;)
to it. This yields the objective function (Eq. 3) and
gradient computation (Eq. 4) shown in Figure 1.
This straightforward extension has several desir-
able properties. In addition to having a probabilis-
tic interpretation, it is related to maximum margin
and minimum-risk frameworks, it can be shown to
minimise a bound on expected risk, and it is convex
(Gimpel and Smith, 2010b).
We can also see from Eq. 4 that the only differ-
ence from standard CLL training is that we must
compute feature expectations with respect to the
cost-augmented scoring function. As Gimpel and
Smith (2010a) discuss, if the loss function decom-
poses over the predicted structure, we can treat its
decomposed elements as unweighted features that
fire on the corresponding structures, and compute
expectations in the normal way. In the case of
our parser, where we compute expectations using
the inside-outside algorithm, a loss function decom-
poses if it decomposes over spans or productions of
a CKY chart.
</bodyText>
<sectionHeader confidence="0.916746" genericHeader="method">
3 Loss Functions for Parsing
</sectionHeader>
<bodyText confidence="0.999671625">
Ideally, we would like to optimise our parser towards
a task-based evaluation. Our CCG parser is evalu-
ated on labeled, directed dependency recovery us-
ing F-measure (Clark and Hockenmaier, 2002). Un-
der this evaluation we will represent output y&apos; and
ground truth y as variable-sized sets of dependen-
cies. We can then compute precision P(y, y&apos;), recall
R(y, y&apos;), and F-measure F1(y, y&apos;).
</bodyText>
<equation confidence="0.999772166666667">
P(y, y&apos;) = |y n y&apos; |(5)
|y&apos;|
R(y, y&apos;) = |y n y&apos; |(6)
|y|
2PR 2|y n y&apos;|
F1(y, y&apos;) =
</equation>
<bodyText confidence="0.99832125">
These metrics are positively correlated with perfor-
mance – they are gain functions. To incorporate
them in the softmax-margin framework we reformu-
late them as loss functions by subtracting from one.
</bodyText>
<subsectionHeader confidence="0.999205">
3.1 Computing F-Measure-Augmented
Expectations at the Sentence Level
</subsectionHeader>
<bodyText confidence="0.99997755">
Unfortunately, none of these metrics decompose
over parses. However, the individual statistics that
are used to compute them do decompose, a fact we
will exploit to devise an algorithm that computes the
necessary expectations. Note that since y is fixed,
F1 is a function of two integers: |y n y&apos;|, represent-
ing the number of correct dependencies in y&apos;; and
|y&apos;|, representing the total number of dependencies
in y&apos;, which we will denote as n and d, respectively.1
Each pair (n, d) leads to a different value of F1. Im-
portantly, both n and d decompose over parses.
The key idea will be to treat F1 as a non-local fea-
ture of the parse, dependent on values n and d.2 To
compute expectations we split each span in an oth-
erwise usual inside-outside computation by all pairs
(n, d) incident at that span.
Formally, our goal will be to compute expecta-
tions over the sentence a1...aL. In order to abstract
away from the particulars of CCG we present the al-
gorithm in relatively familiar terms as a variant of
</bodyText>
<footnote confidence="0.9851035">
1For numerator and denominator.
2This is essentially the same trick used in the oracle F-measure
algorithm of Huang (2008), and indeed our algorithm is a sum-
product variant of that max-product algorithm.
</footnote>
<equation confidence="0.982158357142857">
P + R =|y |+ |y&apos; |(7)
334
Xm
i=1
min
θ
⎡ ⎤
X
⎣−θTf(x(i), y(i)) + log exp{θTf(x(i), y)} ⎦(2)
yEY(x(i))
⎡ ⎤
X
⎣−θTf(x(i), y(i)) + log exp{θTf(x(i), y) + `(y(i), y)} ⎦(3)
yEY(x(i))
Xm
i=1
min
θ
∂
Xm
i=1
=
∂λk
⎡
X−hk (x (i), y(i)) +
exp{θTf (x(i), y) + `(y(i), y)} (i)
yEY(x(i)) Py/EY(x(i)) exp{θTf (x(i), y&apos;) + `
(y(i), y&apos;)} hk(x , y) (4)
</equation>
<figureCaption confidence="0.998168">
Figure 1: Conditional log-likelihood (Eq. 2), Softmax-margin objective (Eq. 3) and gradient (Eq. 4).
</figureCaption>
<bodyText confidence="0.999150333333333">
the classic inside-outside algorithm (Baker, 1979).
We use the notation a : A for lexical entries and
BC ⇒ A to indicate that categories B and C com-
bine to form category A via forward or backward
composition or application.3 The weight of a rule
is denoted with w. The classic algorithm associates
inside score I(Ai,j) and outside score O(Ai,j) with
category A spanning sentence positions i through j,
computed via the following recursions.
</bodyText>
<equation confidence="0.9994963">
I(Ai,i+1) =w(ai+1 : A)
I(Ai,j) = X I(Bi,k)I(Ck,j)w(BC ⇒ A)
k,B,C
I(GOAL) =I(S0,L)
O(GOAL) =1
X
O(Ai,j) =
k,B,C
X O(Ck,j)I(Bk,i)w(BA ⇒ C)
k,B,C
</equation>
<bodyText confidence="0.9976525">
The expectation of A spanning positions i through j
is then I(Ai,j)O(Ai,j)/I(GOAL).
Our algorithm extends these computations to
state-split items Ai,j,n,d.4 Using functions n+(·) and
d+(·) to respectively represent the number of cor-
rect and total dependencies introduced by a parsing
action, we present our algorithm in Fig. 3. The fi-
nal inside equation and initial outside equation in-
corporate the loss function for all derivations hav-
ing a particular F-score, enabling us to obtain the
</bodyText>
<footnote confidence="0.99392675">
3These correspond respectively to unary rules A → a and bi-
nary rules A → BC in a Chomsky normal form grammar.
4Here we use state-splitting to refer to splitting an item Ai,j into
many items Ai,j,n,d, one for each (n, d) pair.
</footnote>
<bodyText confidence="0.99817175">
desired expectations. A simple modification of the
goal equations enables us to optimise precision, re-
call or a weighted F-measure.
To analyze the complexity of this algorithm, we
must ask: how many pairs (n, d) can be incident at
each span? A CCG parser does not necessarily re-
turn one dependency per word (see Figure 2 for an
example), so d is not necessarily equal to the sen-
tence length L as it might be in many dependency
parsers, though it is still bounded by O(L). How-
ever, this behavior is sufficiently uncommon that we
expect all parses of a sentence, good or bad, to have
close to L dependencies, and hence we expect the
range of d to be constant on average. Furthermore,
n will be bounded from below by zero and from
above by min(|y|, |y&apos;|). Hence the set of all possi-
ble F-measures for all possible parses is bounded by
O(L2), but on average it should be closer to O(L).
Following McAllester (1999), we can see from in-
spection of the free variables in Fig. 3 that the algo-
rithm requires worst-case O(L7) and average-case
O(L5) time complexity, and worse-case O(L4) and
average-case O(L3) space complexity.
Note finally that while this algorithm computes
exact sentence-level expectations, it is approximate
at the corpus level, since F-measure does not decom-
pose over sentences. We give the extension to exact
corpus-level expectations in Appendix A.
</bodyText>
<subsectionHeader confidence="0.998456">
3.2 Approximate Loss Functions
</subsectionHeader>
<bodyText confidence="0.999940333333333">
We will also consider approximate but more effi-
cient alternatives to our exact algorithms. The idea
is to use cost functions which only utilise statistics
</bodyText>
<equation confidence="0.978824666666667">
O(Ci,k)I(Bj,k)w(AB ⇒ C)+
335
I(Ai,i+1,n,d) = w(ai+1 : A) iffn = n+(ai+1 : A), d = d+(ai+1 : A)
XI(Ai,j,n,d) = X I(Bi,k,n0,d0)I(Ck,j,n00,d00)w(BC ==�, A)
k,B,C {n0,n00:n0+n00+n+(BCCA)=n},
{d0,d00:d0+d00+d+(BCCA)=d}
X � �
I(GOAL) = 1 − 2n
n,d I(S0,L,n,d)
d + jyj
X
{n0,n00:n0−n00−n+(ABCC)=n},
~ 2n \
O(S0 N,n d) = 1 −
XO(Ai,j,n,d) =
k,B,C
O(Ci,k,n0,d0)I(Bj,k,n00,d00)w(AB ==�, C)+
{d0,d00:d0−d00−d+(ABCC)=d}
X X O(Ck,j,n0,d0)I(Bk,i,n00,d00)w(BA ==�, C)
k,B,C {n0,n00:n0−n00−n+(BACC)=n},
{d0,d00:d0−d00−d+(BACC)=d}
</equation>
<figureCaption confidence="0.944634666666667">
Figure 3: State-split inside and outside recursions for computing softmax-margin with F-measure.
Figure 2: Example of flexible dependency realisation in
CCG: Our parser (Clark and Curran, 2007) creates de-
</figureCaption>
<bodyText confidence="0.958213434782609">
pendencies arising from coordination once all conjuncts
are found and treats “and” as the syntactic head of coor-
dinations. The coordination rule (Φ) does not yet estab-
lish the dependency “and - pears” (dotted line); it is the
backward application (&lt;) in the larger span, “apples and
pears”, that establishes it, together with “and - pears”.
CCG also deals with unbounded dependencies which po-
tentially lead to more dependencies than words (Steed-
man, 2000); in this example a unification mechanism cre-
ates the dependencies “likes - apples” and “likes - pears”
in the forward application (&gt;). For further examples and
a more detailed explanation of the mechanism as used in
the C&amp;C parser refer to Clark et al. (2002).
available within the current local structure, similar to
those used by Taskar et al. (2004) for tracking con-
stituent errors in a context-free parser. We design
three simple losses to approximate precision, recall
and F-measure on CCG dependency structures.
Let T(y) be the set of parsing actions required
to build parse y. Our decomposable approximation
to precision simply counts the number of incorrect
dependencies using the local dependency counts,
n+(·) and d+(·).
</bodyText>
<equation confidence="0.998218">
DecP(y) = X d+(t) − n+(t) (8)
tET(y)
</equation>
<bodyText confidence="0.99994725">
To compute our approximation to recall we require
the number of gold dependencies, c+(·), which
should have been introduced by a particular parsing
action. A gold dependency is due to be recovered
by a parsing action if its head lies within one child
span and its dependent within the other. This yields a
decomposed approximation to recall that counts the
number of missed dependencies.
</bodyText>
<equation confidence="0.9944275">
DecR(y) = X c+(t) − n+(t) (9)
tET(y)
</equation>
<page confidence="0.978566">
336
</page>
<bodyText confidence="0.999274615384615">
Unfortunately, the flexible handling of dependencies
in CCG complicates our formulation of c+, render-
ing it slightly more approximate. The unification
mechanism of CCG sometimes causes dependencies
to be realised later in the derivation, at a point when
both the head and the dependent are in the same
span, violating the assumption used to compute c+
(see again Figure 2). Exceptions like this can cause
mismatches between n+ and c+. We set c+ = n+
whenever c+ &lt; n+ to account for these occasional
discrepancies.
Finally, we obtain a decomposable approximation
to F-measure.
</bodyText>
<equation confidence="0.997001">
DecF1(y) = DecP(y) + DecR(y) (10)
</equation>
<sectionHeader confidence="0.997377" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999967927272728">
Parsing Strategy. CCG parsers use a pipeline strat-
egy: we first multitag each word of the sentence with
a small subset of its possible lexical categories us-
ing a supertagger, a sequence model over these cat-
egories (Bangalore and Joshi, 1999; Clark, 2002).
Then we parse the sentence under the requirement
that the lexical categories are fixed to those preferred
by the supertagger. In our experiments we used two
variants on this strategy.
First is the adaptive supertagging (AST) approach
of Clark and Curran (2004). It is based on a step
function over supertagger beam widths, relaxing the
pruning threshold for lexical categories only if the
parser fails to find an analysis. The process either
succeeds and returns a parse after some iteration or
gives up after a predefined number of iterations. As
Clark and Curran (2004) show, most sentences can
be parsed with very tight beams.
Reverse adaptive supertagging is a much less ag-
gressive method that seeks only to make sentences
parsable when they otherwise would not be due to an
impractically large search space. Reverse AST starts
with a wide beam, narrowing it at each iteration only
if a maximum chart size is exceeded. Table 1 shows
beam settings for both strategies.
Adaptive supertagging aims for speed via pruning
while the reverse strategy aims for accuracy by ex-
posing the parser to a larger search space. Although
Clark and Curran (2007) found no actual improve-
ments from the latter strategy, we will show that
with our softmax-margin-trained models it can have
a substantial effect.
Parser. We use the C&amp;C parser (Clark and Cur-
ran, 2007) and its supertagger (Clark, 2002). Our
baseline is the hybrid model of Clark and Curran
(2007), which contains features over both normal-
form derivations and CCG dependencies. The parser
relies solely on the supertagger for pruning, using
exact CKY for search over the pruned space. Train-
ing requires calculation of feature expectations over
packed charts of derivations. For training, we lim-
ited the number of items in this chart to 0.3 million,
and for testing, 1 million. We also used a more per-
missive training supertagger beam (Table 2) than in
previous work (Clark and Curran, 2007). Models
were trained with the parser’s L-BFGS trainer.
Evaluation. We evaluated on CCGbank (Hocken-
maier and Steedman, 2007), a right-most normal-
form CCG version of the Penn Treebank. We use
sections 02-21 (39603 sentences) for training, sec-
tion 00 (1913 sentences) for development and sec-
tion 23 (2407 sentences) for testing. We supply
gold-standard part-of-speech tags to the parsers. We
evaluate on labelled and unlabelled predicate argu-
ment structure recovery and supertag accuracy.
</bodyText>
<subsectionHeader confidence="0.999495">
4.1 Training with Maximum F-measure Parses
</subsectionHeader>
<bodyText confidence="0.999976142857143">
So far we discussed how to optimise towards task-
specific metrics via changing the training objective.
In our first experiment we change the data on which
we optimise CLL. This is a kind of simple base-
line to our later experiments, attempting to achieve
the same effect by simpler means. Specifically, we
use the algorithm of Huang (2008) to generate or-
acle F-measure parses for each sentence. Updating
towards these oracle parses corrects the reachabil-
ity problem in standard CLL training. Since the su-
pertagger is used to prune the training forests, the
correct parse is sometimes pruned away – reducing
data utilisation to 91%. Clark and Curran (2007)
correct for this by adding the gold tags to the parser
input. While this increases data utilisation, it bi-
ases the model by training in an idealised setting not
available at test time. Using oracle parses corrects
this bias while permitting 99% data utilisation. The
labelled F-score of the oracle parses lies at 98.1%.
Though we expected that this might result in some
improvement, results (Table 3) show that this has no
</bodyText>
<page confidence="0.983658">
337
</page>
<table confidence="0.9999498">
Condition Parameter Iteration 1 2 3 4 5
AST 0 (beam width) 0.075 0.03 0.01 0.005 0.001
k (dictionary cutoff) 20 20 20 20 150
0 0.001 0.005 0.01 0.03 0.075
Reverse k 150 20 20 20 20
</table>
<tableCaption confidence="0.863136">
Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.
Parameter 0 is a beam threshold while k bounds the number of lexical categories considered for each word.
</tableCaption>
<table confidence="0.9999182">
Condition Parameter Iteration 1 2 3 4 5 6 7
0 0.001 0.001 0.0045 0.0055 0.01 0.05 0.1
Training k 150 20 20 20 20 20 20
C&amp;C ’07 0 0.0045 0.0055 0.01 0.05 0.1
k 20 20 20 20 20
</table>
<tableCaption confidence="0.9932765">
Table 2: Beam step functions used for training: The first row shows the large scale settings used for most experiments
and the standard C&amp;C settings. (cf. Table 1)
</tableCaption>
<table confidence="0.99953">
LF LP LR UF UP UR Data Util (%)
Baseline 87.40 87.85 86.95 93.11 93.59 92.63 91%
Max-F Parses 87.46 87.95 86.98 93.09 93.61 92.57 99%
CCGbank+Max-F 87.45 87.96 86.94 93.09 93.63 92.55 99%
</table>
<tableCaption confidence="0.985845">
Table 3: Performance on section 00 of CCGbank when comparing models trained with treebank-parses (Baseline)
and maximum F-score parses (Max-F) using adaptive supertagging as well as a combination of CCGbank and Max-F
parses. Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
</tableCaption>
<figure confidence="0.480267333333333">
1000 12%
Average number of splits
effect. However, it does serve as a useful baseline.
</figure>
<subsectionHeader confidence="0.939459">
4.2 Training with the Exact Algorithm
</subsectionHeader>
<bodyText confidence="0.952484470588235">
We first tested our assumptions about the feasibil-
ity of training with our exact algorithm by measur-
ing the amount of state-splitting. Figure 4 plots the
average number of splits per span against the rela-
tive span-frequency; this is based on a typical set of
training forests containing over 600 million states.
The number of splits increases exponentially with
span size but equally so decreases the number of
spans with many splits. Hence the small number of
states with a high number of splits is balanced by a
large number of spans with only a few splits: The
highest number of splits per span observed with our
settings was 4888 but we find that the average num-
ber of splits lies at 44. Encouragingly, this enables
experimentation in all but very large scale settings.
Figure 5 shows the distribution of n and d pairs
across all split-states in the training corpus; since
</bodyText>
<figure confidence="0.9956475">
10%
100
6%
4%
2%
0%
1 11 21 31 41 51 61 71
span length
</figure>
<figureCaption confidence="0.99796">
Figure 4: Average number of state-splits per span length
</figureCaption>
<bodyText confidence="0.965165125">
as introduced by a sentence-level F-measure loss func-
tion. The statistics are averaged over the training forests
generated using the settings described in §4.
n, the number of correct dependencies, over d, the
number of all recovered dependencies, is precision,
the graph shows that only a minority of states have
either very high or very low precision. The range
of values suggests that the softmax-margin criterion
</bodyText>
<figure confidence="0.98092">
10
1
Average number of splits
Percentage of total spans
8%
% of total spans
</figure>
<page confidence="0.989894">
338
</page>
<bodyText confidence="0.9919295">
will have an opportunity to substantially modify the
expectations, hopefully to good effect.
</bodyText>
<figure confidence="0.978310777777778">
50
40
30 number of correct depedencles (n)
20
10
0
0 10 20 30 40 50 60 70
number of all dependencies (d)
0-5000000 5000000-10000000 10000000-15000000 15000000-20000000
</figure>
<figureCaption confidence="0.997197">
Figure 5: Distribution of states with d dependencies of
which n are correct in the training forests.
</figureCaption>
<bodyText confidence="0.999988909090909">
We next turn to the question of optimization with
these algorithms. Due to the significant computa-
tional requirements, we used the computationally
less intensive normal-form model of Clark and Cur-
ran (2007) as well as their more restrictive training
beam settings (Table 2). We train on all sentences of
the training set as above and test with AST.
In order to provide greater control over the influ-
ence of the loss function, we introduce a multiplier
T, which simply amends the second term of the ob-
jective function (3) to:
</bodyText>
<equation confidence="0.9805705">
log � expjθ&apos; f(xz, y) + T x E(yz, y)}
Y∈Y (x&apos;)
</equation>
<bodyText confidence="0.996088">
Figure 6 plots performance of the exact loss func-
tions across different settings of T on various evalu-
ation criteria, for models restricted to at most 3000
items per chart at training time to allow rapid ex-
perimentation with a wide parameter set. Even in
this constrained setting, it is encouraging to see that
each loss function performs best on the criteria it op-
timises. The precision-trained parser also does very
well on F-measure; this is because the parser has a
tendency to perform better in terms of precision than
recall.
</bodyText>
<subsectionHeader confidence="0.98476">
4.3 Exact vs. Approximate Loss Functions
</subsectionHeader>
<bodyText confidence="0.999984208333333">
With these results in mind, we conducted a compar-
ison of parsers trained using our exact and approxi-
mate loss functions. Table 4 compares their perfor-
mance head to head when restricting training chart
sizes to 100,000 items per sentence, the largest set-
ting our computing resources allowed us to experi-
ment with. The results confirm that the loss-trained
models improve over a likelihood-trained baseline,
and furthermore that the exact loss functions seem
to have the best performance. However, the approx-
imations are extremely competitive with their exact
counterparts. Because they are also efficient, this
makes them attractive for larger-scale experiments.
Training time increases by an order of magnitude
with exact loss functions despite increased theoreti-
cal complexity (§3.1); there is no significant change
with approximate loss functions.
Table 5 shows performance of the approximate
losses with the large scale settings initially outlined
(§4). One striking result is that the softmax-margin
trained models coax more accurate parses from the
larger search space, in contrast to the likelihood-
trained models. Our best loss model improves the
labelled F-measure by over 0.8%.
</bodyText>
<subsectionHeader confidence="0.997373">
4.4 Combination with Integrated Parsing and
Supertagging
</subsectionHeader>
<bodyText confidence="0.999987947368421">
As a final experiment, we embed our loss-trained
model into an integrated model that incorporates
Markov features over supertags into the parsing
model (Auli and Lopez, 2011). These features have
serious implications on search: even allowing for the
observation of Fowler and Penn (2010) that our CCG
is weakly context-free, the search problem is equiva-
lent to finding the optimal derivation in the weighted
intersection of a regular and context-free language
(Bar-Hillel et al., 1964), making search very expen-
sive. Therefore parsing with this model requires ap-
proximations.
To experiment with this combined model we use
loopy belief propagation (LBP; Pearl et al., 1985),
previously applied to dependency parsing by Smith
and Eisner (2008). A more detailed account of its
application to our combined model can be found in
(2011), but we sketch the idea here. We construct a
graphical model with two factors: one is a distribu-
</bodyText>
<page confidence="0.998546">
339
</page>
<figureCaption confidence="0.987015">
Figure 6: Performance of exact cost functions optimizing F-measure, precision and recall in terms of (a) labelled
F-measure, (b) precision, (c) recall and (d) supertag accuracy across various settings of τ on the development set.
</figureCaption>
<table confidence="0.999910888888889">
section 00 (dev) section 23 (test)
LF LP LR UF UP UR LF LP LR UF UP UR
CLL 86.76 87.16 86.36 92.73 93.16 92.30 87.46 87.80 87.12 92.85 93.22 92.49
DecP 87.18 87.93 86.44 92.93 93.73 92.14 87.75 88.34 87.17 93.04 93.66 92.43
DecR 87.31 87.55 87.07 93.00 93.26 92.75 87.57 87.71 87.42 92.92 93.07 92.76
DecF1 87.27 87.78 86.77 93.04 93.58 92.50 87.69 88.10 87.28 93.04 93.48 92.61
P 87.25 87.85 86.66 92.99 93.63 92.36 87.76 88.23 87.30 93.06 93.55 92.57
R 87.34 87.51 87.16 92.98 93.17 92.80 87.57 87.62 87.51 92.92 92.98 92.86
F1 87.34 87.74 86.94 93.05 93.47 92.62 87.71 88.01 87.41 93.02 93.34 92.70
</table>
<tableCaption confidence="0.978834333333333">
Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable
precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).
Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
</tableCaption>
<figure confidence="0.996073914893617">
86*5
Labelled F-measure
86*4
86*3
86*2
86*1
86
86*3
Labelled Recall
86*1
85*9
85*7
85*5
Baseline F1 loss
Precision loss Recall loss
2 3 4 5 6 10
Tau
2 3 4 5 6 10
Tau
(a) (b)
86*5
Baseline F1 loss
Precision loss Recall loss
2 3 4 5 6 10
Tau
Baseline F1 loss
Precision loss Recall loss
2 3 4 5 6 10
Tau
(c) (d)
87*1
86*9
Labelled Prec1s1on
86*7
86*5
86*3
86*1
85*9
Baseline F1 loss
Precision loss Recall loss
93*95
Supertagg1ng Accuracy
93*9
93*85
93*8
93*75
93*7
</figure>
<page confidence="0.987966">
340
</page>
<table confidence="0.999625571428571">
section 00 (dev) section 23 (test)
LF AST ST LF Reverse ST LF AST ST LF Reverse ST
UF UF UF UF
CLL 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01
DecP 87.35 92.99 94.25 87.75 93.25 94.22 88.10 93.26 94.51 88.51 93.50 94.39
DecR 87.48 93.00 94.34 87.70 93.16 94.30 87.66 92.83 94.38 87.77 92.91 94.22
DecF1 87.67 93.23 94.39 88.12 93.52 94.46 88.09 93.28 94.50 88.58 93.57 94.53
</table>
<tableCaption confidence="0.9897555">
Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and
unlabelled F-measure (LF/UF) and supertag accuracy (ST).
</tableCaption>
<bodyText confidence="0.998016742857143">
tion over supertag variables defined by a supertag-
ging model, and the other is a distribution over these
variables and a set of span variables defined by our
parsing model.5 The factors communicate by pass-
ing messages across the shared supertag variables
that correspond to their marginal distributions over
those variables. Hence, to compute approximate ex-
pectations across the entire model, we run forward-
backward to obtain posterior supertag assignments.
These marginals are passed as inside values to the
inside-outside algorithm, which returns a new set
of posteriors. The new posteriors are incorporated
into a new iteration of forward-backward, and the
algorithm iterates until convergence, or until a fixed
number of iterations is reached – we found that a
single iteration is sufficient, corresponding to a trun-
cated version of the algorithm in which posteriors
are simply passed from the supertagger to the parser.
To decode, we use the posteriors in a minimum-risk
parsing algorithm (Goodman, 1996).
Our baseline models are trained separately as be-
fore and combined at test time. For softmax-margin,
we combine a parsing model trained with F1 and
a supertagger trained with Hamming loss. Table 6
shows the results: we observe a gain of up to 1.5%
in labelled F1 and 0.9% in unlabelled F1 on the test
set. The loss functions prove their robustness by im-
proving the more accurate combined models up to
0.4% in labelled F1. Table 7 shows results with au-
tomatic part-of-speech tags and a direct comparison
with the Petrov parser trained on CCGbank (Fowler
and Penn, 2010) which we outpeform on all metrics.
5These complex factors resemble those of Smith and Eisner
(2008) and Dreyer and Eisner (2009); they can be thought of
as case-factor diagrams (McAllester et al., 2008)
</bodyText>
<sectionHeader confidence="0.993686" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999984037037037">
The softmax-margin criterion is a simple and effec-
tive approach to training log-linear parsers. We have
shown that it is possible to compute exact sentence-
level losses under standard parsing metrics, not only
approximations (Taskar et al., 2004). This enables
us to show the effectiveness of these approxima-
tions, and it turns out that they are excellent sub-
stitutes for exact loss functions. Indeed, the approxi-
mate losses are as easy to use as standard conditional
log-likelihood.
Empirically, softmax-margin training improves
parsing performance across the board, beating the
state-of-the-art CCG parsing model of Clark and
Curran (2007) by up to 0.8% labelled F-measure.
It also proves robust, improving a stronger base-
line based on a combined parsing and supertagging
model. Our final result of 89.3%/94.0% labelled
and unlabelled F-measure is the best result reported
for CCG parsing accuracy, beating the original C&amp;C
baseline by up to 1.5%.
In future work we plan to scale our exact loss
functions to larger settings and to explore training
with loss functions within loopy belief propagation.
Although we have focused on CCG parsing in this
work, we expect our methods to be equally appli-
cable to parsing with other grammar formalisms in-
cluding context-free grammar or LTAG.
</bodyText>
<sectionHeader confidence="0.99618" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998454833333333">
We would like to thank Stephen Clark, Chris-
tos Christodoulopoulos, Mark Granroth-Wilding,
Gholamreza Haffari, Alexandre Klementiev, Tom
Kwiatkowski, Kira Mourao, Matt Post, and Mark
Steedman for helpful discussion related to this
work and comments on previous drafts, and the
</bodyText>
<page confidence="0.993635">
341
</page>
<table confidence="0.999648428571429">
section 00 (dev) section 23 (test)
LF AST ST LF Reverse ST LF AST ST LF Reverse ST
UF UF UF UF
CLL 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01
BP 87.67 93.26 94.43 88.35 93.72 94.73 88.25 93.33 94.60 88.86 93.75 94.84
+DecF1 87.90 93.40 94.52 88.58 93.88 94.79 88.32 93.32 94.66 89.15 93.89 94.98
+SA 87.73 93.28 94.49 88.40 93.71 94.75 88.47 93.48 94.71 89.25 93.98 95.01
</table>
<tableCaption confidence="0.9954335">
Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as
parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
</tableCaption>
<table confidence="0.999664142857143">
LF LP section 00 (dev) UP UR LF LP section 23 (test) UP UR
LR UF LR UF
CLL 85.53 85.73 85.33 91.99 92.20 91.77 85.74 85.90 85.58 91.92 92.09 91.75
Petrov I-5 85.79 86.09 85.50 92.44 92.76 92.13 86.01 86.29 85.73 92.34 92.64 92.04
BP 86.45 86.75 86.17 92.60 92.92 92.29 86.84 87.08 86.61 92.57 92.82 92.32
+DecF1 86.73 87.07 86.39 92.79 93.16 92.43 87.08 87.37 86.78 92.68 93.00 92.37
+SA 86.51 86.86 86.16 92.60 92.98 92.23 87.20 87.50 86.90 92.76 93.08 92.44
</table>
<tableCaption confidence="0.991229">
Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
(2010); evaluation is based on sentences for which all parsers returned an analysis.
</tableCaption>
<bodyText confidence="0.99903775">
anonymous reviewers for helpful comments. We
also acknowledge funding from EPSRC grant
EP/P504171/1 (Auli); and the resources provided by
the Edinburgh Compute and Data Facility.
</bodyText>
<subsectionHeader confidence="0.755188">
A Computing F-Measure-Augmented
Expectations at the Corpus Level
</subsectionHeader>
<bodyText confidence="0.999948857142857">
To compute exact corpus-level expectations for softmax-
margin using F-measure, we add an additional transition
before reaching the GOAL item in our original program.
To reach it, we must parse every sentence in the corpus,
associating statistics of aggregate hn, d) pairs for the en-
tire training set in intermediate symbols Γ(1)...Γ(m) with
the following inside recursions.
</bodyText>
<equation confidence="0.998499571428571">
I(Γ(1)
n,d) = I(S(1)
0,jx(1)j,n,d)
E I(Γn0,d0))I(S(`)
0,N,n00,d00)
n0,n00:n0+n00=n
I (Γ� d) C1 − d + jyj /
</equation>
<bodyText confidence="0.99515675">
Outside recursions follow straightforwardly. Implemen-
tation of this algorithm would require substantial dis-
tributed computation or external data structures, so we
did not attempt it.
</bodyText>
<sectionHeader confidence="0.998534" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999752037037037">
M. Auli and A. Lopez. 2011. A Comparison of Loopy
Belief Propagation and Dual Decomposition for Inte-
grated CCG Supertagging and Parsing. In Proc. of
ACL, June.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. Journal of the Acoustical Society of America,
65.
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
Approach to Almost Parsing. Computational Linguis-
tics, 25(2):238–265, June.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116–150.
S. Clark and J. R. Curran. 2004. The importance of su-
pertagging for wide-coverage CCG parsing. In COL-
ING, Morristown, NJ, USA.
S. Clark and J. R. Curran. 2007. Wide-Coverage Ef-
ficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493–552.
S. Clark and J. Hockenmaier. 2002. Evaluating a Wide-
Coverage CCG Parser. In Proceedings of the LREC
2002 Beyond Parseval Workshop, pages 60–66, Las
Palmas, Spain.
S. Clark, J. Hockenmaier, and M. Steedman. 2002.
Building deep dependency structures with a wide-
coverage CCG parser. In Proc. ofACL.
</reference>
<figure confidence="0.57161025">
I(Γ(`)n,d) =
E
I(GOAL) =
n,d
</figure>
<page confidence="0.987267">
342
</page>
<reference confidence="0.999936087719298">
S. Clark. 2002. Supertagging for Combinatory Catego-
rial Grammar. In TAG+6.
M. Dreyer and J. Eisner. 2009. Graphical models over
multiple strings. In Proc. ofEMNLP.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Feature-based, conditional random field parsing. In
Proceedings ofACL-HLT.
T. A. D. Fowler and G. Penn. 2010. Accurate context-
free parsing with combinatory categorial grammar. In
Proc. ofACL.
K. Gimpel and N. A. Smith. 2010a. Softmax-margin
CRFs: training log-linear models with cost functions.
In HLT ’10: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
K. Gimpel and N. A. Smith. 2010b. Softmax-margin
training for structured log-linear models. Technical
Report CMU-LTI-10-008, Carnegie Mellon Univer-
sity.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. ofACL, pages 177–183, Jun.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355–396.
L. Huang. 2008. Forest Reranking: Discriminative pars-
ing with Non-Local Features. In Proceedings ofACL-
08: HLT.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ofICML,
pages 282–289.
D. McAllester, M. Collins, and F. Pereira. 2008. Case-
factor diagrams for structured probabilistic modeling.
Journal of Computer and System Sciences, 74(1):84–
96.
D. McAllester. 1999. On the complexity analysis of
static analyses. In Proc. of Static Analysis Symposium,
volume 1694/1999 ofLNCS. Springer Verlag.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ofACL, Jul.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
D. Povey and P. Woodland. 2008. Minimum phone er-
ror and I-smoothing for improved discrimative train-
ing. In Proc. ofICASSP.
F. Sha and L. K. Saul. 2006. Large margin hidden
Markov models for automatic speech recognition. In
Proc. ofNIPS.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
M. Steedman. 2000. The syntactic process. MIT Press,
Cambridge, MA.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. ofEMNLP,
pages 1–8, Jul.
</reference>
<page confidence="0.999371">
343
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976906">
<title confidence="0.999836">Training a Log-Linear Parser with Loss Functions via Softmax-Margin</title>
<author confidence="0.999972">Michael Auli Adam Lopez</author>
<affiliation confidence="0.999971">School of Informatics HLTCOE University of Edinburgh Johns Hopkins University</affiliation>
<email confidence="0.995507">m.auli@sms.ed.ac.ukalopez@cs.jhu.edu</email>
<abstract confidence="0.998835391304348">Log-linear parsing models are often trained by optimizing likelihood, but we would prefer to optimise for a task-specific metric like Fmeasure. Softmax-margin is a convex objective for such models that minimises a bound on expected risk for a given loss function, but its naive application requires the loss to decompose over the predicted structure, which is not true of F-measure. We use softmaxmargin to optimise a log-linear CCG parser for a variety of loss functions, and demonstrate a novel dynamic programming algorithm that enables us to use it with F-measure, leading to substantial gains in accuracy on CCG- Bank. When we embed our loss-trained parser into a larger model that includes supertagging features incorporated via belief propagation, we obtain further improvements and achieve a labelled/unlabelled dependency F-measure of 89.3%/94.0% on gold part-of-speech tags, and 87.2%/92.8% on automatic part-of-speech tags, the best reported results for this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Auli</author>
<author>A Lopez</author>
</authors>
<title>A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing.</title>
<date>2011</date>
<booktitle>In Proc. of ACL,</booktitle>
<contexts>
<context position="23330" citStr="Auli and Lopez, 2011" startWordPosition="3825" endWordPosition="3828">o significant change with approximate loss functions. Table 5 shows performance of the approximate losses with the large scale settings initially outlined (§4). One striking result is that the softmax-margin trained models coax more accurate parses from the larger search space, in contrast to the likelihoodtrained models. Our best loss model improves the labelled F-measure by over 0.8%. 4.4 Combination with Integrated Parsing and Supertagging As a final experiment, we embed our loss-trained model into an integrated model that incorporates Markov features over supertags into the parsing model (Auli and Lopez, 2011). These features have serious implications on search: even allowing for the observation of Fowler and Penn (2010) that our CCG is weakly context-free, the search problem is equivalent to finding the optimal derivation in the weighted intersection of a regular and context-free language (Bar-Hillel et al., 1964), making search very expensive. Therefore parsing with this model requires approximations. To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al., 1985), previously applied to dependency parsing by Smith and Eisner (2008). A more detailed account of its </context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>M. Auli and A. Lopez. 2011. A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing. In Proc. of ACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<journal>Journal of the Acoustical Society of America,</journal>
<volume>65</volume>
<contexts>
<context position="7946" citStr="Baker, 1979" startWordPosition="1283" endWordPosition="1284"> oracle F-measure algorithm of Huang (2008), and indeed our algorithm is a sumproduct variant of that max-product algorithm. P + R =|y |+ |y&apos; |(7) 334 Xm i=1 min θ ⎡ ⎤ X ⎣−θTf(x(i), y(i)) + log exp{θTf(x(i), y)} ⎦(2) yEY(x(i)) ⎡ ⎤ X ⎣−θTf(x(i), y(i)) + log exp{θTf(x(i), y) + `(y(i), y)} ⎦(3) yEY(x(i)) Xm i=1 min θ ∂ Xm i=1 = ∂λk ⎡ X−hk (x (i), y(i)) + exp{θTf (x(i), y) + `(y(i), y)} (i) yEY(x(i)) Py/EY(x(i)) exp{θTf (x(i), y&apos;) + ` (y(i), y&apos;)} hk(x , y) (4) Figure 1: Conditional log-likelihood (Eq. 2), Softmax-margin objective (Eq. 3) and gradient (Eq. 4). the classic inside-outside algorithm (Baker, 1979). We use the notation a : A for lexical entries and BC ⇒ A to indicate that categories B and C combine to form category A via forward or backward composition or application.3 The weight of a rule is denoted with w. The classic algorithm associates inside score I(Ai,j) and outside score O(Ai,j) with category A spanning sentence positions i through j, computed via the following recursions. I(Ai,i+1) =w(ai+1 : A) I(Ai,j) = X I(Bi,k)I(Ck,j)w(BC ⇒ A) k,B,C I(GOAL) =I(S0,L) O(GOAL) =1 X O(Ai,j) = k,B,C X O(Ck,j)I(Bk,i)w(BA ⇒ C) k,B,C The expectation of A spanning positions i through j is then I(Ai,j</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable grammars for speech recognition. Journal of the Acoustical Society of America, 65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A K Joshi</author>
</authors>
<title>Supertagging: An Approach to Almost Parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="13976" citStr="Bangalore and Joshi, 1999" startWordPosition="2266" endWordPosition="2269">t a point when both the head and the dependent are in the same span, violating the assumption used to compute c+ (see again Figure 2). Exceptions like this can cause mismatches between n+ and c+. We set c+ = n+ whenever c+ &lt; n+ to account for these occasional discrepancies. Finally, we obtain a decomposable approximation to F-measure. DecF1(y) = DecP(y) + DecR(y) (10) 4 Experiments Parsing Strategy. CCG parsers use a pipeline strategy: we first multitag each word of the sentence with a small subset of its possible lexical categories using a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Then we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger. In our experiments we used two variants on this strategy. First is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories only if the parser fails to find an analysis. The process either succeeds and returns a parse after some iteration or gives up after a predefined number of iterations. As Clark and Curran (2004) show, most s</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>S. Bangalore and A. K. Joshi. 1999. Supertagging: An Approach to Almost Parsing. Computational Linguistics, 25(2):238–265, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars.</title>
<date>1964</date>
<booktitle>In Language and Information: Selected Essays on their Theory and Application,</booktitle>
<pages>116--150</pages>
<contexts>
<context position="23641" citStr="Bar-Hillel et al., 1964" startWordPosition="3873" endWordPosition="3876">trained models. Our best loss model improves the labelled F-measure by over 0.8%. 4.4 Combination with Integrated Parsing and Supertagging As a final experiment, we embed our loss-trained model into an integrated model that incorporates Markov features over supertags into the parsing model (Auli and Lopez, 2011). These features have serious implications on search: even allowing for the observation of Fowler and Penn (2010) that our CCG is weakly context-free, the search problem is equivalent to finding the optimal derivation in the weighted intersection of a regular and context-free language (Bar-Hillel et al., 1964), making search very expensive. Therefore parsing with this model requires approximations. To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al., 1985), previously applied to dependency parsing by Smith and Eisner (2008). A more detailed account of its application to our combined model can be found in (2011), but we sketch the idea here. We construct a graphical model with two factors: one is a distribu339 Figure 6: Performance of exact cost functions optimizing F-measure, precision and recall in terms of (a) labelled F-measure, (b) precision, (c) recall and</context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Language and Information: Selected Essays on their Theory and Application, pages 116–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In COLING,</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14252" citStr="Clark and Curran (2004)" startWordPosition="2310" endWordPosition="2313">ally, we obtain a decomposable approximation to F-measure. DecF1(y) = DecP(y) + DecR(y) (10) 4 Experiments Parsing Strategy. CCG parsers use a pipeline strategy: we first multitag each word of the sentence with a small subset of its possible lexical categories using a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Then we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger. In our experiments we used two variants on this strategy. First is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories only if the parser fails to find an analysis. The process either succeeds and returns a parse after some iteration or gives up after a predefined number of iterations. As Clark and Curran (2004) show, most sentences can be parsed with very tight beams. Reverse adaptive supertagging is a much less aggressive method that seeks only to make sentences parsable when they otherwise would not be due to an impractically large search space. Reverse AST starts with a wide beam, narrowing </context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>S. Clark and J. R. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In COLING, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1335" citStr="Clark and Curran, 2007" startWordPosition="195" endWordPosition="198"> dynamic programming algorithm that enables us to use it with F-measure, leading to substantial gains in accuracy on CCGBank. When we embed our loss-trained parser into a larger model that includes supertagging features incorporated via belief propagation, we obtain further improvements and achieve a labelled/unlabelled dependency F-measure of 89.3%/94.0% on gold part-of-speech tags, and 87.2%/92.8% on automatic part-of-speech tags, the best reported results for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightf</context>
<context position="2801" citStr="Clark and Curran (2007)" startWordPosition="418" endWordPosition="421">used F-measure metric does not decompose over parses. To solve this, we introduce a novel dynamic programming algorithm that enables us to compute the exact quantities needed under the softmax-margin objective using F-measure as a loss (§3). We experiment with this and several other metrics, including precision, recall, and decomposable approximations thereof. Our ability to optimise towards exact metrics enables us to verify the effectiveness of more efficient approximations. We test the training procedures on the state-of-the-art Combinatory Categorial Grammar (CCG; Steedman 2000) parser of Clark and Curran (2007), obtaining substantial improvements under a variety of conditions. We then embed this model into a more accurate model that incorporates additional supertagging features via loopy belief propagation. The improvements are additive, obtaining the best reported results on this task (§4). 2 Softmax-Margin Training The softmax-margin objective modifies the standard likelihood objective for CRF training by reweighting 333 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 333–343, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computatio</context>
<context position="11448" citStr="Clark and Curran, 2007" startWordPosition="1850" endWordPosition="1853">d+(ai+1 : A) XI(Ai,j,n,d) = X I(Bi,k,n0,d0)I(Ck,j,n00,d00)w(BC ==�, A) k,B,C {n0,n00:n0+n00+n+(BCCA)=n}, {d0,d00:d0+d00+d+(BCCA)=d} X � � I(GOAL) = 1 − 2n n,d I(S0,L,n,d) d + jyj X {n0,n00:n0−n00−n+(ABCC)=n}, ~ 2n \ O(S0 N,n d) = 1 − XO(Ai,j,n,d) = k,B,C O(Ci,k,n0,d0)I(Bj,k,n00,d00)w(AB ==�, C)+ {d0,d00:d0−d00−d+(ABCC)=d} X X O(Ck,j,n0,d0)I(Bk,i,n00,d00)w(BA ==�, C) k,B,C {n0,n00:n0−n00−n+(BACC)=n}, {d0,d00:d0−d00−d+(BACC)=d} Figure 3: State-split inside and outside recursions for computing softmax-margin with F-measure. Figure 2: Example of flexible dependency realisation in CCG: Our parser (Clark and Curran, 2007) creates dependencies arising from coordination once all conjuncts are found and treats “and” as the syntactic head of coordinations. The coordination rule (Φ) does not yet establish the dependency “and - pears” (dotted line); it is the backward application (&lt;) in the larger span, “apples and pears”, that establishes it, together with “and - pears”. CCG also deals with unbounded dependencies which potentially lead to more dependencies than words (Steedman, 2000); in this example a unification mechanism creates the dependencies “likes - apples” and “likes - pears” in the forward application (&gt;)</context>
<context position="15139" citStr="Clark and Curran (2007)" startWordPosition="2459" endWordPosition="2462">umber of iterations. As Clark and Curran (2004) show, most sentences can be parsed with very tight beams. Reverse adaptive supertagging is a much less aggressive method that seeks only to make sentences parsable when they otherwise would not be due to an impractically large search space. Reverse AST starts with a wide beam, narrowing it at each iteration only if a maximum chart size is exceeded. Table 1 shows beam settings for both strategies. Adaptive supertagging aims for speed via pruning while the reverse strategy aims for accuracy by exposing the parser to a larger search space. Although Clark and Curran (2007) found no actual improvements from the latter strategy, we will show that with our softmax-margin-trained models it can have a substantial effect. Parser. We use the C&amp;C parser (Clark and Curran, 2007) and its supertagger (Clark, 2002). Our baseline is the hybrid model of Clark and Curran (2007), which contains features over both normalform derivations and CCG dependencies. The parser relies solely on the supertagger for pruning, using exact CKY for search over the pruned space. Training requires calculation of feature expectations over packed charts of derivations. For training, we limited th</context>
<context position="17111" citStr="Clark and Curran (2007)" startWordPosition="2778" endWordPosition="2781">imise towards taskspecific metrics via changing the training objective. In our first experiment we change the data on which we optimise CLL. This is a kind of simple baseline to our later experiments, attempting to achieve the same effect by simpler means. Specifically, we use the algorithm of Huang (2008) to generate oracle F-measure parses for each sentence. Updating towards these oracle parses corrects the reachability problem in standard CLL training. Since the supertagger is used to prune the training forests, the correct parse is sometimes pruned away – reducing data utilisation to 91%. Clark and Curran (2007) correct for this by adding the gold tags to the parser input. While this increases data utilisation, it biases the model by training in an idealised setting not available at test time. Using oracle parses corrects this bias while permitting 99% data utilisation. The labelled F-score of the oracle parses lies at 98.1%. Though we expected that this might result in some improvement, results (Table 3) show that this has no 337 Condition Parameter Iteration 1 2 3 4 5 AST 0 (beam width) 0.075 0.03 0.01 0.005 0.001 k (dictionary cutoff) 20 20 20 20 150 0 0.001 0.005 0.01 0.03 0.075 Reverse k 150 20 </context>
<context position="20974" citStr="Clark and Curran (2007)" startWordPosition="3443" endWordPosition="3447">lits Percentage of total spans 8% % of total spans 338 will have an opportunity to substantially modify the expectations, hopefully to good effect. 50 40 30 number of correct depedencles (n) 20 10 0 0 10 20 30 40 50 60 70 number of all dependencies (d) 0-5000000 5000000-10000000 10000000-15000000 15000000-20000000 Figure 5: Distribution of states with d dependencies of which n are correct in the training forests. We next turn to the question of optimization with these algorithms. Due to the significant computational requirements, we used the computationally less intensive normal-form model of Clark and Curran (2007) as well as their more restrictive training beam settings (Table 2). We train on all sentences of the training set as above and test with AST. In order to provide greater control over the influence of the loss function, we introduce a multiplier T, which simply amends the second term of the objective function (3) to: log � expjθ&apos; f(xz, y) + T x E(yz, y)} Y∈Y (x&apos;) Figure 6 plots performance of the exact loss functions across different settings of T on various evaluation criteria, for models restricted to at most 3000 items per chart at training time to allow rapid experimentation with a wide pa</context>
<context position="28748" citStr="Clark and Curran (2007)" startWordPosition="4719" endWordPosition="4722">n criterion is a simple and effective approach to training log-linear parsers. We have shown that it is possible to compute exact sentencelevel losses under standard parsing metrics, not only approximations (Taskar et al., 2004). This enables us to show the effectiveness of these approximations, and it turns out that they are excellent substitutes for exact loss functions. Indeed, the approximate losses are as easy to use as standard conditional log-likelihood. Empirically, softmax-margin training improves parsing performance across the board, beating the state-of-the-art CCG parsing model of Clark and Curran (2007) by up to 0.8% labelled F-measure. It also proves robust, improving a stronger baseline based on a combined parsing and supertagging model. Our final result of 89.3%/94.0% labelled and unlabelled F-measure is the best result reported for CCG parsing accuracy, beating the original C&amp;C baseline by up to 1.5%. In future work we plan to scale our exact loss functions to larger settings and to explore training with loss functions within loopy belief propagation. Although we have focused on CCG parsing in this work, we expect our methods to be equally applicable to parsing with other grammar formali</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>S. Clark and J. R. Curran. 2007. Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Hockenmaier</author>
</authors>
<title>Evaluating a WideCoverage CCG Parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC 2002 Beyond Parseval Workshop,</booktitle>
<pages>60--66</pages>
<location>Las Palmas,</location>
<contexts>
<context position="5709" citStr="Clark and Hockenmaier, 2002" startWordPosition="883" endWordPosition="886"> (2010a) discuss, if the loss function decomposes over the predicted structure, we can treat its decomposed elements as unweighted features that fire on the corresponding structures, and compute expectations in the normal way. In the case of our parser, where we compute expectations using the inside-outside algorithm, a loss function decomposes if it decomposes over spans or productions of a CKY chart. 3 Loss Functions for Parsing Ideally, we would like to optimise our parser towards a task-based evaluation. Our CCG parser is evaluated on labeled, directed dependency recovery using F-measure (Clark and Hockenmaier, 2002). Under this evaluation we will represent output y&apos; and ground truth y as variable-sized sets of dependencies. We can then compute precision P(y, y&apos;), recall R(y, y&apos;), and F-measure F1(y, y&apos;). P(y, y&apos;) = |y n y&apos; |(5) |y&apos;| R(y, y&apos;) = |y n y&apos; |(6) |y| 2PR 2|y n y&apos;| F1(y, y&apos;) = These metrics are positively correlated with performance – they are gain functions. To incorporate them in the softmax-margin framework we reformulate them as loss functions by subtracting from one. 3.1 Computing F-Measure-Augmented Expectations at the Sentence Level Unfortunately, none of these metrics decompose over pars</context>
</contexts>
<marker>Clark, Hockenmaier, 2002</marker>
<rawString>S. Clark and J. Hockenmaier. 2002. Evaluating a WideCoverage CCG Parser. In Proceedings of the LREC 2002 Beyond Parseval Workshop, pages 60–66, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>Building deep dependency structures with a widecoverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="12174" citStr="Clark et al. (2002)" startWordPosition="1971" endWordPosition="1974">tic head of coordinations. The coordination rule (Φ) does not yet establish the dependency “and - pears” (dotted line); it is the backward application (&lt;) in the larger span, “apples and pears”, that establishes it, together with “and - pears”. CCG also deals with unbounded dependencies which potentially lead to more dependencies than words (Steedman, 2000); in this example a unification mechanism creates the dependencies “likes - apples” and “likes - pears” in the forward application (&gt;). For further examples and a more detailed explanation of the mechanism as used in the C&amp;C parser refer to Clark et al. (2002). available within the current local structure, similar to those used by Taskar et al. (2004) for tracking constituent errors in a context-free parser. We design three simple losses to approximate precision, recall and F-measure on CCG dependency structures. Let T(y) be the set of parsing actions required to build parse y. Our decomposable approximation to precision simply counts the number of incorrect dependencies using the local dependency counts, n+(·) and d+(·). DecP(y) = X d+(t) − n+(t) (8) tET(y) To compute our approximation to recall we require the number of gold dependencies, c+(·), w</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>S. Clark, J. Hockenmaier, and M. Steedman. 2002. Building deep dependency structures with a widecoverage CCG parser. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
</authors>
<title>Supertagging for Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In TAG+6.</booktitle>
<contexts>
<context position="13990" citStr="Clark, 2002" startWordPosition="2270" endWordPosition="2271">d and the dependent are in the same span, violating the assumption used to compute c+ (see again Figure 2). Exceptions like this can cause mismatches between n+ and c+. We set c+ = n+ whenever c+ &lt; n+ to account for these occasional discrepancies. Finally, we obtain a decomposable approximation to F-measure. DecF1(y) = DecP(y) + DecR(y) (10) 4 Experiments Parsing Strategy. CCG parsers use a pipeline strategy: we first multitag each word of the sentence with a small subset of its possible lexical categories using a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Then we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger. In our experiments we used two variants on this strategy. First is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories only if the parser fails to find an analysis. The process either succeeds and returns a parse after some iteration or gives up after a predefined number of iterations. As Clark and Curran (2004) show, most sentences can b</context>
<context position="15374" citStr="Clark, 2002" startWordPosition="2500" endWordPosition="2501">ue to an impractically large search space. Reverse AST starts with a wide beam, narrowing it at each iteration only if a maximum chart size is exceeded. Table 1 shows beam settings for both strategies. Adaptive supertagging aims for speed via pruning while the reverse strategy aims for accuracy by exposing the parser to a larger search space. Although Clark and Curran (2007) found no actual improvements from the latter strategy, we will show that with our softmax-margin-trained models it can have a substantial effect. Parser. We use the C&amp;C parser (Clark and Curran, 2007) and its supertagger (Clark, 2002). Our baseline is the hybrid model of Clark and Curran (2007), which contains features over both normalform derivations and CCG dependencies. The parser relies solely on the supertagger for pruning, using exact CKY for search over the pruned space. Training requires calculation of feature expectations over packed charts of derivations. For training, we limited the number of items in this chart to 0.3 million, and for testing, 1 million. We also used a more permissive training supertagger beam (Table 2) than in previous work (Clark and Curran, 2007). Models were trained with the parser’s L-BFGS</context>
</contexts>
<marker>Clark, 2002</marker>
<rawString>S. Clark. 2002. Supertagging for Combinatory Categorial Grammar. In TAG+6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dreyer</author>
<author>J Eisner</author>
</authors>
<title>Graphical models over multiple strings. In</title>
<date>2009</date>
<booktitle>Proc. ofEMNLP.</booktitle>
<contexts>
<context position="28004" citStr="Dreyer and Eisner (2009)" startWordPosition="4605" endWordPosition="4608">t time. For softmax-margin, we combine a parsing model trained with F1 and a supertagger trained with Hamming loss. Table 6 shows the results: we observe a gain of up to 1.5% in labelled F1 and 0.9% in unlabelled F1 on the test set. The loss functions prove their robustness by improving the more accurate combined models up to 0.4% in labelled F1. Table 7 shows results with automatic part-of-speech tags and a direct comparison with the Petrov parser trained on CCGbank (Fowler and Penn, 2010) which we outpeform on all metrics. 5These complex factors resemble those of Smith and Eisner (2008) and Dreyer and Eisner (2009); they can be thought of as case-factor diagrams (McAllester et al., 2008) 5 Conclusion and Future Work The softmax-margin criterion is a simple and effective approach to training log-linear parsers. We have shown that it is possible to compute exact sentencelevel losses under standard parsing metrics, not only approximations (Taskar et al., 2004). This enables us to show the effectiveness of these approximations, and it turns out that they are excellent substitutes for exact loss functions. Indeed, the approximate losses are as easy to use as standard conditional log-likelihood. Empirically, </context>
</contexts>
<marker>Dreyer, Eisner, 2009</marker>
<rawString>M. Dreyer and J. Eisner. 2009. Graphical models over multiple strings. In Proc. ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT.</booktitle>
<contexts>
<context position="1357" citStr="Finkel et al., 2008" startWordPosition="199" endWordPosition="202">orithm that enables us to use it with F-measure, leading to substantial gains in accuracy on CCGBank. When we embed our loss-trained parser into a larger model that includes supertagging features incorporated via belief propagation, we obtain further improvements and achieve a labelled/unlabelled dependency F-measure of 89.3%/94.0% on gold part-of-speech tags, and 87.2%/92.8% on automatic part-of-speech tags, the best reported results for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightforward to optimise. Gi</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Feature-based, conditional random field parsing. In Proceedings ofACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A D Fowler</author>
<author>G Penn</author>
</authors>
<title>Accurate contextfree parsing with combinatory categorial grammar.</title>
<date>2010</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="23443" citStr="Fowler and Penn (2010)" startWordPosition="3842" endWordPosition="3845">he large scale settings initially outlined (§4). One striking result is that the softmax-margin trained models coax more accurate parses from the larger search space, in contrast to the likelihoodtrained models. Our best loss model improves the labelled F-measure by over 0.8%. 4.4 Combination with Integrated Parsing and Supertagging As a final experiment, we embed our loss-trained model into an integrated model that incorporates Markov features over supertags into the parsing model (Auli and Lopez, 2011). These features have serious implications on search: even allowing for the observation of Fowler and Penn (2010) that our CCG is weakly context-free, the search problem is equivalent to finding the optimal derivation in the weighted intersection of a regular and context-free language (Bar-Hillel et al., 1964), making search very expensive. Therefore parsing with this model requires approximations. To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al., 1985), previously applied to dependency parsing by Smith and Eisner (2008). A more detailed account of its application to our combined model can be found in (2011), but we sketch the idea here. We construct a graphical m</context>
<context position="27875" citStr="Fowler and Penn, 2010" startWordPosition="4584" endWordPosition="4587">s in a minimum-risk parsing algorithm (Goodman, 1996). Our baseline models are trained separately as before and combined at test time. For softmax-margin, we combine a parsing model trained with F1 and a supertagger trained with Hamming loss. Table 6 shows the results: we observe a gain of up to 1.5% in labelled F1 and 0.9% in unlabelled F1 on the test set. The loss functions prove their robustness by improving the more accurate combined models up to 0.4% in labelled F1. Table 7 shows results with automatic part-of-speech tags and a direct comparison with the Petrov parser trained on CCGbank (Fowler and Penn, 2010) which we outpeform on all metrics. 5These complex factors resemble those of Smith and Eisner (2008) and Dreyer and Eisner (2009); they can be thought of as case-factor diagrams (McAllester et al., 2008) 5 Conclusion and Future Work The softmax-margin criterion is a simple and effective approach to training log-linear parsers. We have shown that it is possible to compute exact sentencelevel losses under standard parsing metrics, not only approximations (Taskar et al., 2004). This enables us to show the effectiveness of these approximations, and it turns out that they are excellent substitutes </context>
<context position="30856" citStr="Fowler and Penn (2010)" startWordPosition="5070" endWordPosition="5073">-accuracy (SA) as loss in the supertagger. LF LP section 00 (dev) UP UR LF LP section 23 (test) UP UR LR UF LR UF CLL 85.53 85.73 85.33 91.99 92.20 91.77 85.74 85.90 85.58 91.92 92.09 91.75 Petrov I-5 85.79 86.09 85.50 92.44 92.76 92.13 86.01 86.29 85.73 92.34 92.64 92.04 BP 86.45 86.75 86.17 92.60 92.92 92.29 86.84 87.08 86.61 92.57 92.82 92.32 +DecF1 86.73 87.07 86.39 92.79 93.16 92.43 87.08 87.37 86.78 92.68 93.00 92.37 +SA 86.51 86.86 86.16 92.60 92.98 92.23 87.20 87.50 86.90 92.76 93.08 92.44 Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn (2010); evaluation is based on sentences for which all parsers returned an analysis. anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); and the resources provided by the Edinburgh Compute and Data Facility. A Computing F-Measure-Augmented Expectations at the Corpus Level To compute exact corpus-level expectations for softmaxmargin using F-measure, we add an additional transition before reaching the GOAL item in our original program. To reach it, we must parse every sentence in the corpus, associating statistics of aggregate hn, d) pairs for th</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>T. A. D. Fowler and G. Penn. 2010. Accurate contextfree parsing with combinatory categorial grammar. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Softmax-margin CRFs: training log-linear models with cost functions.</title>
<date>2010</date>
<booktitle>In HLT ’10: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1792" citStr="Gimpel and Smith, 2010" startWordPosition="265" endWordPosition="268">sults for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightforward to optimise. Gimpel and Smith (2010a) show that it can be easily implemented with a simple change to standard likelihood-based training, provided that the loss function decomposes over the predicted structure. Unfortunately, the widely-used F-measure metric does not decompose over parses. To solve this, we introduce a novel dynamic programming algorithm that enables us to compute the exact quantities needed under the softmax-margin objective usin</context>
<context position="4881" citStr="Gimpel and Smith, 2010" startWordPosition="750" endWordPosition="753"> (Figure 1). Now consider a function `(y, y&apos;) that returns the loss incurred by choosing to output y&apos; when the correct output is y. The softmaxmargin objective simply modifies the unnormalised, unexponentiated score θTf(x, y&apos;) by adding `(y, y&apos;) to it. This yields the objective function (Eq. 3) and gradient computation (Eq. 4) shown in Figure 1. This straightforward extension has several desirable properties. In addition to having a probabilistic interpretation, it is related to maximum margin and minimum-risk frameworks, it can be shown to minimise a bound on expected risk, and it is convex (Gimpel and Smith, 2010b). We can also see from Eq. 4 that the only difference from standard CLL training is that we must compute feature expectations with respect to the cost-augmented scoring function. As Gimpel and Smith (2010a) discuss, if the loss function decomposes over the predicted structure, we can treat its decomposed elements as unweighted features that fire on the corresponding structures, and compute expectations in the normal way. In the case of our parser, where we compute expectations using the inside-outside algorithm, a loss function decomposes if it decomposes over spans or productions of a CKY c</context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>K. Gimpel and N. A. Smith. 2010a. Softmax-margin CRFs: training log-linear models with cost functions. In HLT ’10: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Softmax-margin training for structured log-linear models.</title>
<date>2010</date>
<tech>Technical Report CMU-LTI-10-008,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="1792" citStr="Gimpel and Smith, 2010" startWordPosition="265" endWordPosition="268">sults for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightforward to optimise. Gimpel and Smith (2010a) show that it can be easily implemented with a simple change to standard likelihood-based training, provided that the loss function decomposes over the predicted structure. Unfortunately, the widely-used F-measure metric does not decompose over parses. To solve this, we introduce a novel dynamic programming algorithm that enables us to compute the exact quantities needed under the softmax-margin objective usin</context>
<context position="4881" citStr="Gimpel and Smith, 2010" startWordPosition="750" endWordPosition="753"> (Figure 1). Now consider a function `(y, y&apos;) that returns the loss incurred by choosing to output y&apos; when the correct output is y. The softmaxmargin objective simply modifies the unnormalised, unexponentiated score θTf(x, y&apos;) by adding `(y, y&apos;) to it. This yields the objective function (Eq. 3) and gradient computation (Eq. 4) shown in Figure 1. This straightforward extension has several desirable properties. In addition to having a probabilistic interpretation, it is related to maximum margin and minimum-risk frameworks, it can be shown to minimise a bound on expected risk, and it is convex (Gimpel and Smith, 2010b). We can also see from Eq. 4 that the only difference from standard CLL training is that we must compute feature expectations with respect to the cost-augmented scoring function. As Gimpel and Smith (2010a) discuss, if the loss function decomposes over the predicted structure, we can treat its decomposed elements as unweighted features that fire on the corresponding structures, and compute expectations in the normal way. In the case of our parser, where we compute expectations using the inside-outside algorithm, a loss function decomposes if it decomposes over spans or productions of a CKY c</context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>K. Gimpel and N. A. Smith. 2010b. Softmax-margin training for structured log-linear models. Technical Report CMU-LTI-10-008, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>177--183</pages>
<contexts>
<context position="1609" citStr="Goodman, 1996" startWordPosition="238" endWordPosition="239">ents and achieve a labelled/unlabelled dependency F-measure of 89.3%/94.0% on gold part-of-speech tags, and 87.2%/92.8% on automatic part-of-speech tags, the best reported results for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightforward to optimise. Gimpel and Smith (2010a) show that it can be easily implemented with a simple change to standard likelihood-based training, provided that the loss function decomposes over the predicted structure. Unfortunately, the widely-used F-measure metric does not </context>
<context position="27306" citStr="Goodman, 1996" startWordPosition="4486" endWordPosition="4487"> run forwardbackward to obtain posterior supertag assignments. These marginals are passed as inside values to the inside-outside algorithm, which returns a new set of posteriors. The new posteriors are incorporated into a new iteration of forward-backward, and the algorithm iterates until convergence, or until a fixed number of iterations is reached – we found that a single iteration is sufficient, corresponding to a truncated version of the algorithm in which posteriors are simply passed from the supertagger to the parser. To decode, we use the posteriors in a minimum-risk parsing algorithm (Goodman, 1996). Our baseline models are trained separately as before and combined at test time. For softmax-margin, we combine a parsing model trained with F1 and a supertagger trained with Hamming loss. Table 6 shows the results: we observe a gain of up to 1.5% in labelled F1 and 0.9% in unlabelled F1 on the test set. The loss functions prove their robustness by improving the more accurate combined models up to 0.4% in labelled F1. Table 7 shows results with automatic part-of-speech tags and a direct comparison with the Petrov parser trained on CCGbank (Fowler and Penn, 2010) which we outpeform on all metr</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing algorithms and metrics. In Proc. ofACL, pages 177–183, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="16052" citStr="Hockenmaier and Steedman, 2007" startWordPosition="2607" endWordPosition="2611"> Curran (2007), which contains features over both normalform derivations and CCG dependencies. The parser relies solely on the supertagger for pruning, using exact CKY for search over the pruned space. Training requires calculation of feature expectations over packed charts of derivations. For training, we limited the number of items in this chart to 0.3 million, and for testing, 1 million. We also used a more permissive training supertagger beam (Table 2) than in previous work (Clark and Curran, 2007). Models were trained with the parser’s L-BFGS trainer. Evaluation. We evaluated on CCGbank (Hockenmaier and Steedman, 2007), a right-most normalform CCG version of the Penn Treebank. We use sections 02-21 (39603 sentences) for training, section 00 (1913 sentences) for development and section 23 (2407 sentences) for testing. We supply gold-standard part-of-speech tags to the parsers. We evaluate on labelled and unlabelled predicate argument structure recovery and supertag accuracy. 4.1 Training with Maximum F-measure Parses So far we discussed how to optimise towards taskspecific metrics via changing the training objective. In our first experiment we change the data on which we optimise CLL. This is a kind of simpl</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>J. Hockenmaier and M. Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest Reranking: Discriminative parsing with Non-Local Features.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL08:</booktitle>
<publisher>HLT.</publisher>
<contexts>
<context position="7377" citStr="Huang (2008)" startWordPosition="1178" endWordPosition="1179">F1. Importantly, both n and d decompose over parses. The key idea will be to treat F1 as a non-local feature of the parse, dependent on values n and d.2 To compute expectations we split each span in an otherwise usual inside-outside computation by all pairs (n, d) incident at that span. Formally, our goal will be to compute expectations over the sentence a1...aL. In order to abstract away from the particulars of CCG we present the algorithm in relatively familiar terms as a variant of 1For numerator and denominator. 2This is essentially the same trick used in the oracle F-measure algorithm of Huang (2008), and indeed our algorithm is a sumproduct variant of that max-product algorithm. P + R =|y |+ |y&apos; |(7) 334 Xm i=1 min θ ⎡ ⎤ X ⎣−θTf(x(i), y(i)) + log exp{θTf(x(i), y)} ⎦(2) yEY(x(i)) ⎡ ⎤ X ⎣−θTf(x(i), y(i)) + log exp{θTf(x(i), y) + `(y(i), y)} ⎦(3) yEY(x(i)) Xm i=1 min θ ∂ Xm i=1 = ∂λk ⎡ X−hk (x (i), y(i)) + exp{θTf (x(i), y) + `(y(i), y)} (i) yEY(x(i)) Py/EY(x(i)) exp{θTf (x(i), y&apos;) + ` (y(i), y&apos;)} hk(x , y) (4) Figure 1: Conditional log-likelihood (Eq. 2), Softmax-margin objective (Eq. 3) and gradient (Eq. 4). the classic inside-outside algorithm (Baker, 1979). We use the notation a : A for</context>
<context position="16795" citStr="Huang (2008)" startWordPosition="2729" endWordPosition="2730">sentences) for development and section 23 (2407 sentences) for testing. We supply gold-standard part-of-speech tags to the parsers. We evaluate on labelled and unlabelled predicate argument structure recovery and supertag accuracy. 4.1 Training with Maximum F-measure Parses So far we discussed how to optimise towards taskspecific metrics via changing the training objective. In our first experiment we change the data on which we optimise CLL. This is a kind of simple baseline to our later experiments, attempting to achieve the same effect by simpler means. Specifically, we use the algorithm of Huang (2008) to generate oracle F-measure parses for each sentence. Updating towards these oracle parses corrects the reachability problem in standard CLL training. Since the supertagger is used to prune the training forests, the correct parse is sometimes pruned away – reducing data utilisation to 91%. Clark and Curran (2007) correct for this by adding the gold tags to the parser input. While this increases data utilisation, it biases the model by training in an idealised setting not available at test time. Using oracle parses corrects this bias while permitting 99% data utilisation. The labelled F-score</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>L. Huang. 2008. Forest Reranking: Discriminative parsing with Non-Local Features. In Proceedings ofACL08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ofICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1285" citStr="Lafferty et al., 2001" startWordPosition="187" endWordPosition="190">variety of loss functions, and demonstrate a novel dynamic programming algorithm that enables us to use it with F-measure, leading to substantial gains in accuracy on CCGBank. When we embed our loss-trained parser into a larger model that includes supertagging features incorporated via belief propagation, we obtain further improvements and achieve a labelled/unlabelled dependency F-measure of 89.3%/94.0% on gold part-of-speech tags, and 87.2%/92.8% on automatic part-of-speech tags, the best reported results for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a los</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ofICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
<author>M Collins</author>
<author>F Pereira</author>
</authors>
<title>Casefactor diagrams for structured probabilistic modeling.</title>
<date>2008</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>74</volume>
<issue>1</issue>
<pages>96</pages>
<contexts>
<context position="28078" citStr="McAllester et al., 2008" startWordPosition="4617" endWordPosition="4620"> a supertagger trained with Hamming loss. Table 6 shows the results: we observe a gain of up to 1.5% in labelled F1 and 0.9% in unlabelled F1 on the test set. The loss functions prove their robustness by improving the more accurate combined models up to 0.4% in labelled F1. Table 7 shows results with automatic part-of-speech tags and a direct comparison with the Petrov parser trained on CCGbank (Fowler and Penn, 2010) which we outpeform on all metrics. 5These complex factors resemble those of Smith and Eisner (2008) and Dreyer and Eisner (2009); they can be thought of as case-factor diagrams (McAllester et al., 2008) 5 Conclusion and Future Work The softmax-margin criterion is a simple and effective approach to training log-linear parsers. We have shown that it is possible to compute exact sentencelevel losses under standard parsing metrics, not only approximations (Taskar et al., 2004). This enables us to show the effectiveness of these approximations, and it turns out that they are excellent substitutes for exact loss functions. Indeed, the approximate losses are as easy to use as standard conditional log-likelihood. Empirically, softmax-margin training improves parsing performance across the board, bea</context>
</contexts>
<marker>McAllester, Collins, Pereira, 2008</marker>
<rawString>D. McAllester, M. Collins, and F. Pereira. 2008. Casefactor diagrams for structured probabilistic modeling. Journal of Computer and System Sciences, 74(1):84– 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
</authors>
<title>On the complexity analysis of static analyses.</title>
<date>1999</date>
<booktitle>In Proc. of Static Analysis Symposium,</booktitle>
<volume>1694</volume>
<pages>ofLNCS.</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="10104" citStr="McAllester (1999)" startWordPosition="1660" endWordPosition="1661">ency per word (see Figure 2 for an example), so d is not necessarily equal to the sentence length L as it might be in many dependency parsers, though it is still bounded by O(L). However, this behavior is sufficiently uncommon that we expect all parses of a sentence, good or bad, to have close to L dependencies, and hence we expect the range of d to be constant on average. Furthermore, n will be bounded from below by zero and from above by min(|y|, |y&apos;|). Hence the set of all possible F-measures for all possible parses is bounded by O(L2), but on average it should be closer to O(L). Following McAllester (1999), we can see from inspection of the free variables in Fig. 3 that the algorithm requires worst-case O(L7) and average-case O(L5) time complexity, and worse-case O(L4) and average-case O(L3) space complexity. Note finally that while this algorithm computes exact sentence-level expectations, it is approximate at the corpus level, since F-measure does not decompose over sentences. We give the extension to exact corpus-level expectations in Appendix A. 3.2 Approximate Loss Functions We will also consider approximate but more efficient alternatives to our exact algorithms. The idea is to use cost f</context>
</contexts>
<marker>McAllester, 1999</marker>
<rawString>D. McAllester. 1999. On the complexity analysis of static analyses. In Proc. of Static Analysis Symposium, volume 1694/1999 ofLNCS. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL,</booktitle>
<contexts>
<context position="1621" citStr="Och, 2003" startWordPosition="240" endWordPosition="241">e a labelled/unlabelled dependency F-measure of 89.3%/94.0% on gold part-of-speech tags, and 87.2%/92.8% on automatic part-of-speech tags, the best reported results for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightforward to optimise. Gimpel and Smith (2010a) show that it can be easily implemented with a simple change to standard likelihood-based training, provided that the loss function decomposes over the predicted structure. Unfortunately, the widely-used F-measure metric does not decompose ov</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ofACL, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann.</publisher>
<marker>Pearl, 1988</marker>
<rawString>J. Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Povey</author>
<author>P Woodland</author>
</authors>
<title>Minimum phone error and I-smoothing for improved discrimative training.</title>
<date>2008</date>
<booktitle>In Proc. ofICASSP.</booktitle>
<contexts>
<context position="1768" citStr="Povey and Woodland, 2008" startWordPosition="261" endWordPosition="264">tags, the best reported results for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightforward to optimise. Gimpel and Smith (2010a) show that it can be easily implemented with a simple change to standard likelihood-based training, provided that the loss function decomposes over the predicted structure. Unfortunately, the widely-used F-measure metric does not decompose over parses. To solve this, we introduce a novel dynamic programming algorithm that enables us to compute the exact quantities needed under the softm</context>
</contexts>
<marker>Povey, Woodland, 2008</marker>
<rawString>D. Povey and P. Woodland. 2008. Minimum phone error and I-smoothing for improved discrimative training. In Proc. ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>L K Saul</author>
</authors>
<title>Large margin hidden Markov models for automatic speech recognition.</title>
<date>2006</date>
<booktitle>In Proc. ofNIPS.</booktitle>
<contexts>
<context position="1742" citStr="Sha and Saul, 2006" startWordPosition="257" endWordPosition="260">atic part-of-speech tags, the best reported results for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightforward to optimise. Gimpel and Smith (2010a) show that it can be easily implemented with a simple change to standard likelihood-based training, provided that the loss function decomposes over the predicted structure. Unfortunately, the widely-used F-measure metric does not decompose over parses. To solve this, we introduce a novel dynamic programming algorithm that enables us to compute the exact quantit</context>
</contexts>
<marker>Sha, Saul, 2006</marker>
<rawString>F. Sha and L. K. Saul. 2006. Large margin hidden Markov models for automatic speech recognition. In Proc. ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proc. of</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="23897" citStr="Smith and Eisner (2008)" startWordPosition="3912" endWordPosition="3915"> supertags into the parsing model (Auli and Lopez, 2011). These features have serious implications on search: even allowing for the observation of Fowler and Penn (2010) that our CCG is weakly context-free, the search problem is equivalent to finding the optimal derivation in the weighted intersection of a regular and context-free language (Bar-Hillel et al., 1964), making search very expensive. Therefore parsing with this model requires approximations. To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al., 1985), previously applied to dependency parsing by Smith and Eisner (2008). A more detailed account of its application to our combined model can be found in (2011), but we sketch the idea here. We construct a graphical model with two factors: one is a distribu339 Figure 6: Performance of exact cost functions optimizing F-measure, precision and recall in terms of (a) labelled F-measure, (b) precision, (c) recall and (d) supertag accuracy across various settings of τ on the development set. section 00 (dev) section 23 (test) LF LP LR UF UP UR LF LP LR UF UP UR CLL 86.76 87.16 86.36 92.73 93.16 92.30 87.46 87.80 87.12 92.85 93.22 92.49 DecP 87.18 87.93 86.44 92.93 93.7</context>
<context position="27975" citStr="Smith and Eisner (2008)" startWordPosition="4600" endWordPosition="4603">s before and combined at test time. For softmax-margin, we combine a parsing model trained with F1 and a supertagger trained with Hamming loss. Table 6 shows the results: we observe a gain of up to 1.5% in labelled F1 and 0.9% in unlabelled F1 on the test set. The loss functions prove their robustness by improving the more accurate combined models up to 0.4% in labelled F1. Table 7 shows results with automatic part-of-speech tags and a direct comparison with the Petrov parser trained on CCGbank (Fowler and Penn, 2010) which we outpeform on all metrics. 5These complex factors resemble those of Smith and Eisner (2008) and Dreyer and Eisner (2009); they can be thought of as case-factor diagrams (McAllester et al., 2008) 5 Conclusion and Future Work The softmax-margin criterion is a simple and effective approach to training log-linear parsers. We have shown that it is possible to compute exact sentencelevel losses under standard parsing metrics, not only approximations (Taskar et al., 2004). This enables us to show the effectiveness of these approximations, and it turns out that they are excellent substitutes for exact loss functions. Indeed, the approximate losses are as easy to use as standard conditional </context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D. A. Smith and J. Eisner. 2008. Dependency parsing by belief propagation. In Proc. of EMNLP. M. Steedman. 2000. The syntactic process. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proc. ofEMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="12267" citStr="Taskar et al. (2004)" startWordPosition="1986" endWordPosition="1989">and - pears” (dotted line); it is the backward application (&lt;) in the larger span, “apples and pears”, that establishes it, together with “and - pears”. CCG also deals with unbounded dependencies which potentially lead to more dependencies than words (Steedman, 2000); in this example a unification mechanism creates the dependencies “likes - apples” and “likes - pears” in the forward application (&gt;). For further examples and a more detailed explanation of the mechanism as used in the C&amp;C parser refer to Clark et al. (2002). available within the current local structure, similar to those used by Taskar et al. (2004) for tracking constituent errors in a context-free parser. We design three simple losses to approximate precision, recall and F-measure on CCG dependency structures. Let T(y) be the set of parsing actions required to build parse y. Our decomposable approximation to precision simply counts the number of incorrect dependencies using the local dependency counts, n+(·) and d+(·). DecP(y) = X d+(t) − n+(t) (8) tET(y) To compute our approximation to recall we require the number of gold dependencies, c+(·), which should have been introduced by a particular parsing action. A gold dependency is due to </context>
<context position="28353" citStr="Taskar et al., 2004" startWordPosition="4660" endWordPosition="4663">e 7 shows results with automatic part-of-speech tags and a direct comparison with the Petrov parser trained on CCGbank (Fowler and Penn, 2010) which we outpeform on all metrics. 5These complex factors resemble those of Smith and Eisner (2008) and Dreyer and Eisner (2009); they can be thought of as case-factor diagrams (McAllester et al., 2008) 5 Conclusion and Future Work The softmax-margin criterion is a simple and effective approach to training log-linear parsers. We have shown that it is possible to compute exact sentencelevel losses under standard parsing metrics, not only approximations (Taskar et al., 2004). This enables us to show the effectiveness of these approximations, and it turns out that they are excellent substitutes for exact loss functions. Indeed, the approximate losses are as easy to use as standard conditional log-likelihood. Empirically, softmax-margin training improves parsing performance across the board, beating the state-of-the-art CCG parsing model of Clark and Curran (2007) by up to 0.8% labelled F-measure. It also proves robust, improving a stronger baseline based on a combined parsing and supertagging model. Our final result of 89.3%/94.0% labelled and unlabelled F-measure</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In Proc. ofEMNLP, pages 1–8, Jul.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>