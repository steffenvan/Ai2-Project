<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028296">
<title confidence="0.893405">
Classifying French Verbs Using French and English Lexical Resources
</title>
<author confidence="0.976344">
Ingrid Falk Claire Gardent Jean-Charles Lamirel
</author>
<affiliation confidence="0.891915">
Universit´e de Lorraine/LORIA, CNRS/LORIA, Universit´e de Strasbourg/LORIA,
Nancy, France Nancy, France Nancy, France
</affiliation>
<email confidence="0.995597">
ingrid.falk@loria.fr claire.gardent@loria.fr jean-charles.lamirel@loria.fr
</email>
<sectionHeader confidence="0.998585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833636363636">
We present a novel approach to the automatic
acquisition of a Verbnet like classification of
French verbs which involves the use (i) of
a neural clustering method which associates
clusters with features, (ii) of several super-
vised and unsupervised evaluation metrics and
(iii) of various existing syntactic and semantic
lexical resources. We evaluate our approach
on an established test set and show that it
outperforms previous related work with an F-
measure of 0.70.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935433962265">
Verb classifications have been shown to be useful
both from a theoretical and from a practical perspec-
tive. From the theoretical viewpoint, they permit
capturing syntactic and/or semantic generalisations
about verbs (Levin, 1993; Kipper Schuler, 2006).
From a practical perspective, they support factorisa-
tion and have been shown to be effective in various
NLP (Natural language Processing) tasks such as se-
mantic role labelling (Swier and Stevenson, 2005) or
word sense disambiguation (Dang, 2004).
While there has been much work on automatically
acquiring verb classes for English (Sun et al., 2010)
and to a lesser extent for German (Brew and Schulte
im Walde, 2002; Schulte im Walde, 2003; Schulte
im Walde, 2006), Japanese (Oishi and Matsumoto,
1997) and Italian (Merlo et al., 2002), few studies
have been conducted on the automatic classification
of French verbs. Recently however, two proposals
have been put forward.
On the one hand, (Sun et al., 2010) applied
a clustering approach developed for English to
French. They exploit features extracted from a large
scale subcategorisation lexicon (LexSchem (Mes-
siant, 2008)) acquired fully automatically from Le
Monde newspaper corpus and show that, as for En-
glish, syntactic frames and verb selectional prefer-
ences perform better than lexical cooccurence fea-
tures. Their approach achieves a F-measure of
55.1 on 116 verbs occurring at least 150 times in
Lexschem. The best performance is achieved when
restricting the approach to verbs occurring at least
4000 times (43 verbs) with an F-measure of 65.4.
On the other hand, Falk and Gardent (2011)
present a classification approach for French verbs
based on the use of Formal Concept Analysis (FCA).
FCA (Barbut and Monjardet, 1970) is a sym-
bolic classification technique which permits creating
classes associating sets of objects (eg. French verbs)
with sets of features (eg. syntactic frames). Falk
and Gardent (2011) provide no evaluation for their
results however, only a qualitative analysis.
In this paper, we describe a novel approach to the
clustering of French verbs which (i) gives good re-
sults on the established benchmark used in (Sun et
al., 2010) and (ii) associates verbs with a feature
profile describing their syntactic and semantic prop-
erties. The approach exploits a clustering method
called IGNGF (Incremental Growing Neural Gas
with Feature Maximisation, (Lamirel et al., 2011b))
which uses the features characterising each cluster
both to guide the clustering process and to label the
output clusters. We apply this method to the data
contained in various verb lexicons and we evalu-
</bodyText>
<page confidence="0.984753">
854
</page>
<note confidence="0.985757">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 854–863,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936166666667">
ate the resulting classification on a slightly modified
version of the gold standard provided by (Sun et al.,
2010). We show that the approach yields promising
results (F-measure of 70%) and that the clustering
produced systematically associates verbs with syn-
tactic frames and thematic grids thereby providing
an interesting basis for the creation and evaluation
of a Verbnet-like classification.
Section 2 describes the lexical resources used for
feature extraction and Section 3 the experimental
setup. Sections 4 and 5 present the data used for
and the results obtained. Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.995483" genericHeader="method">
2 Lexical Resources Used
</sectionHeader>
<bodyText confidence="0.999834444444444">
Our aim is to accquire a classification which covers
the core verbs of French, could be used to support
semantic role labelling and is similar in spirit to the
English Verbnet. In this first experiment, we there-
fore favoured extracting the features used for clus-
tering, not from a large corpus parsed automatically,
but from manually validated resources1. These lexi-
cal resources are (i) a syntactic lexicon produced by
merging three existing lexicons for French and (ii)
the English Verbnet.
Among the many syntactic lexicons available for
French (Nicolas et al., 2008; Messiant, 2008; Kup´s´c
and Abeill´e, 2008; van den Eynde and Mertens,
2003; Gross, 1975), we selected and merged three
lexicons built or validated manually namely, Dico-
valence, TreeLex and the LADL tables. The result-
ing lexicon contains 5918 verbs, 20433 lexical en-
tries (i.e., verb/frame pairs) and 345 subcategorisa-
tion frames. It also contains more detailed syntac-
tic and semantic features such as lexical preferences
(e.g., locative argument, concrete object) or thematic
role information (e.g., symmetric arguments, asset
role) which we make use of for clustering.
We use the English Verbnet as a resource for asso-
ciating French verbs with thematic grids as follows.
We translate the verbs in the English Verbnet classes
to French using English-French dictionaries2. To
</bodyText>
<footnote confidence="0.986007857142857">
1Of course, the same approach could be applied to corpus
based data (as done e.g., in (Sun et al., 2010)) thus making the
approach fully unsupervised and directly applicable to any lan-
guage for which a parser is available.
2For the translation we use the following resources: Sci-
Fran-Euradic, a French-English bilingual dictionary, built and
improved by linguists (http://catalog.elra.info/
</footnote>
<bodyText confidence="0.999129423076923">
deal with polysemy, we train a supervised classifier
as follows. We first map French verbs with English
Verbnet classes: A French verb is associated with
an English Verbnet class if, according to our dictio-
naries, it is a translation of an English verb in this
class. The task of the classifier is then to produce
a probability estimate for the correctness of this as-
sociation, given the training data. The training set
is built by stating for 1740 French verb, English
Verbnet class) pairs whether the verb has the the-
matic grid given by the pair’s Verbnet class3. This
set is used to train an SVM (support vector machine)
classifier4. The features we use are similar to those
used in (Mouton, 2010): they are numeric and are
derived for example from the number of translations
an English or French verb had, the size of the Verb-
net classes, the number of classes a verb is a member
of etc. The resulting classifier gives for each French
verb, English VN class) pair the estimated probabil-
ity of the pair’s verb being a member of the pair’s
class5. We select 6000 pairs with highest proba-
bility estimates and obtain the translated classes by
assigning each verb in a selected pair to the pair’s
class. This way French verbs are effectively asso-
ciated with one or more English Verbnet thematic
grids.
</bodyText>
<sectionHeader confidence="0.994197" genericHeader="method">
3 Clustering Methods, Evaluation Metrics
and Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.997984">
3.1 Clustering Methods
</subsectionHeader>
<bodyText confidence="0.999875555555556">
The IGNGF clustering method is an incremental
neural “winner-take-most” clustering method be-
longing to the family of the free topology neu-
ral clustering methods. Like other neural free
topology methods such as Neural Gas (NG) (Mar-
tinetz and Schulten, 1991), Growing Neural Gas
(GNG) (Fritzke, 1995), or Incremental Growing
Neural Gas (IGNG) (Prudent and Ennaji, 2005),
the IGNGF method makes use of Hebbian learning
</bodyText>
<footnote confidence="0.996116333333333">
product_info.php?products_id=666), Google dic-
tionary (http://www.google.com/dictionary) and
Dicovalence (van den Eynde and Mertens, 2003).
3The training data consists of the verbs and Verbnet classes
used in the gold standard presented in (Sun et al., 2010).
4We used the libsvm (Chang and Lin, 2011) implementation
of the classifier for this step.
5The accuracy of the classifier on the held out random test
set of 100 pairs was of 90%.
</footnote>
<page confidence="0.998774">
855
</page>
<bodyText confidence="0.999901846153846">
(Hebb, 1949) for dynamically structuring the learn-
ing space. However, contrary to these methods, the
use of a standard distance measure for determining a
winner is replaced in IGNGF by feature maximisa-
tion. Feature maximisation is a cluster quality metric
which associates each cluster with maximal features
i.e., features whose Feature F-measure is maximal.
Feature F-measure is the harmonic mean of Feature
Recall and Feature Precision which in turn are de-
fined as:
where W1 represents the weight of the feature f for
element x and Fc designates the set of features as-
sociated with the verbs occuring in the cluster c. A
feature is then said to be maximal for a given clus-
ter iff its Feature F-measure is higher for that cluster
than for any other cluster.
The IGNGF method was shown to outperform
other usual neural and non neural methods for clus-
tering tasks on relatively clean data (Lamirel et al.,
2011b). Since we use features extracted from man-
ually validated sources, this clustering technique
seems a good fit for our application. In addition,
the feature maximisation and cluster labeling per-
formed by the IGNGF method has proved promising
both for visualising clustering results (Lamirel et al.,
2008) and for validating or optimising a clustering
method (Attik et al., 2006). We make use of these
processes in all our experiments and systematically
compute cluster labelling and feature maximisation
on the output clusterings. As we shall see, this per-
mits distinguishing between clusterings with simi-
lar F-measure but lower “linguistic plausibility” (cf.
Section 5). This facilitates clustering interpretation
in that cluster labeling clearly indicates the associa-
tion between clusters (verbs) and their prevalent fea-
tures. And this supports the creation of a Verbnet
style classification in that cluster labeling directly
provides classes grouping together verbs, thematic
grids and subcategorisation frames.
</bodyText>
<subsectionHeader confidence="0.998014">
3.2 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999877571428571">
We use several evaluation metrics which bear on dif-
ferent properties of the clustering.
Modified Purity and Accuracy. Following (Sun
et al., 2010), we use modified purity (mPUR);
weighted class accuracy (ACC) and F-measure to
evaluate the clusterings produced. These are com-
puted as follows. Each induced cluster is assigned
the gold class (its prevalent class, prev(C)) to which
most of its member verbs belong. A verb is then said
to be correct if the gold associates it with the preva-
lent class of the cluster it is in. Given this, purity is
the ratio between the number of correct gold verbs
in the clustering and the total number of gold verbs
in the clustering6:
</bodyText>
<equation confidence="0.995175666666667">
E
CEClustering,|prev(C)|l1 |prev(C) ∩ C|
mP UR =
</equation>
<bodyText confidence="0.966832321428571">
where VerbsGoldnClustering is the total number of gold
verbs in the clustering.
Accuracy represents the proportion of gold verbs
in those clusters which are associated with a gold
class, compared to all the gold verbs in the clus-
tering. To compute accuracy we associate to each
gold class CGold a dominant cluster, ie. the cluster
dom(CGold) which has most verbs in common with
the gold class. Then accuracy is given by the follow-
ing formula:
ACC = ECEGold |dom(C) ∩ C|
VerbsGoldnClustering
Finally, F-measure is the harmonic mean of mPUR
and ACC.
Coverage. To assess the extent to which a cluster-
ing matches the gold classification, we additionally
compute the coverage of each clustering that is, the
proportion of gold classes that are prevalent classes
in the clustering.
Cumulative Micro Precision (CMP). As pointed
out in (Lamirel et al., 2008; Attik et al., 2006), un-
supervised evaluation metrics based on cluster la-
belling and feature maximisation can prove very
useful for identifying the best clustering strategy.
Following (Lamirel et al., 2011a), we use CMP to
identify the best clustering. Computed on the clus-
tering results, this metrics evaluates the quality of a
clustering w.r.t. the cluster features rather than w.r.t.
</bodyText>
<footnote confidence="0.8102995">
6Clusters for which the prevalent class has only one element
are ignored
</footnote>
<figure confidence="0.997973117647059">
E
vEc
E E
l
clECvEc
E
vEc
E
flEF�,vEc
Wvf
Wvf
FRc(f) =
, F Pc(f) =
W v f
Wfl
v
VerbsGoldnClustering ,
</figure>
<page confidence="0.994709">
856
</page>
<bodyText confidence="0.999838">
to a gold standard. It was shown in (Ghribi et al.,
2010) to be effective in detecting degenerated clus-
tering results including a small number of large het-
erogeneous, “garbage” clusters and a big number of
small size “chunk” clusters.
First, the local Recall (Rfc ) and the local Preci-
sion (Pcf) of a feature f in a cluster c are defined as
follows:
</bodyText>
<equation confidence="0.998542">
c |
Rf c = |vf
|V f |P f
c  |c = |vf |Vc|
</equation>
<bodyText confidence="0.9996014">
where vfc is the set of verbs having feature f in c, Vc
the set of verbs in c and V f, the set of verbs with
feature f.
Cumulative Micro-Precision (CMP) is then de-
fined as follows:
</bodyText>
<equation confidence="0.988597333333333">
�i=|Ci-.rl,Ic.-V||c+12 ECECi+,f∈Fc Pc
CMP = 1
Ei=|Cinf|,|Csup |Ci+
</equation>
<bodyText confidence="0.99782575">
where Ci+ represents the subset of clusters of C
for which the number of associated verbs is greater
than i, and: Cinf = argminci∈C|ci|, Csup =
argmaxci∈C|ci|
</bodyText>
<subsectionHeader confidence="0.826112">
3.3 Cluster display, feature f-Measure and
confidence score
</subsectionHeader>
<bodyText confidence="0.9999765">
To facilitate interpretation, clusters are displayed as
illustrated in Table 1. Features are displayed in
decreasing order of Feature F-measure (cf. Sec-
tion 3.1) and features whose Feature F-measure is
under the average Feature F-measure of the over-
all clustering are clearly delineated from others. In
addition, for each verb in a cluster, a confidence
score is displayed which is the ratio between the sum
of the F-measures of its cluster maximised features
over the sum of the F-measures of the overall cluster
maximised features. Verbs whose confidence score
is 0 are considered as orphan data.
</bodyText>
<subsectionHeader confidence="0.959598">
3.4 Experimental setup
</subsectionHeader>
<bodyText confidence="0.99938">
We applied an IDF-Norm weighting scheme
(Robertson and Jones, 1976) to decrease the influ-
ence of the most frequent features (IDF component)
and to compensate for discrepancies in feature num-
ber (normalisation).
</bodyText>
<table confidence="0.953846">
C6- 14(14) [197(197)]
———-
Prevalent Label — = AgExp-Cause
0.341100 G-AgExp-Cause
0.274864 C-SUJ:Ssub,OBJ:NP
0.061313 C-SUJ:Ssub
0.042544 C-SUJ:NP,DEOBJ:Ssub
**********
**********
0.017787 C-SUJ:NP,DEOBJ:VPinf
0.008108 C-SUJ:VPinf,AOBJ:PP
. . .
[**d´eprimer 0.934345 4(0)] [affliger 0.879122 3(0)]
[´eblouir 0.879122 3(0)] [choquer 0.879122 3(0)]
[d´ecevoir 0.879122 3(0)] [d´econtenancer 0.879122
3(0)] [d´econtracter 0.879122 3(0)] [d´esillusionner
0.879122 3(0)] [**ennuyer 0.879122 3(0)] [fasciner
0.879122 3(0)] [**heurter 0.879122 3(0)] ...
</table>
<tableCaption confidence="0.986101666666667">
Table 1: Sample output for a cluster produced with
the grid-scf-sem feature set and the IGNGF clustering
method.
</tableCaption>
<bodyText confidence="0.999946857142857">
We use K-Means as a baseline. For each cluster-
ing method (K-Means and IGNGF), we let the num-
ber of clusters vary between 1 and 30 to obtain a
partition that reaches an optimum F-measure and a
number of clusters that is in the same order of mag-
nitude as the initial number of Gold classes (i.e. 11
classes).
</bodyText>
<sectionHeader confidence="0.99871" genericHeader="method">
4 Features and Data
</sectionHeader>
<bodyText confidence="0.999830722222222">
Features In the simplest case the features are
the subcategorisation frames (scf) associated to the
verbs by our lexicon. We also experiment with dif-
ferent combinations of additional, syntactic (synt)
and semantic features (sem) extracted from the lex-
icon and with the thematic grids (grid) extracted
from the English Verbnet.
The thematic grid information is derived from the
English Verbnet as explained in Section 2. The syn-
tactic features extracted from the lexicon are listed
in Table 1(a). They indicate whether a verb accepts
symmetric arguments (e.g., John met Mary/John and
Mary met); has four or more arguments; combines
with a predicative phrase (e.g., John named Mary
president); takes a sentential complement or an op-
tional object; or accepts the passive in se (similar to
the English middle voice Les habits se vendent bien /
The clothes sell well). As shown in Table 1(a), these
</bodyText>
<page confidence="0.986103">
857
</page>
<table confidence="0.9837754">
(a) Additional syntactic features.
Feature related VN class
Symmetric arguments amalgamate-22.2, correspond-36.1
4 or more arguments get-13.5.1, send-11.1
Predicate characterize-29.2
Sentential argument correspond-36.1, characterize-29.2
Optional object implicit theme (Randall, 2010), p. 95
Passive built with se theme role (Randall, 2010), p. 120
(b) Additional semantic features.
Feature related VN class
Location role put-9.1, remove-10.1, ...
Concrete object hit-18.1 (eg. INSTRUMENT)
(non human role) other cos-45.4 ...
Asset role get-13.5.1
Plural role amalgamate-22.2, correspond-36.1
</table>
<tableCaption confidence="0.9813365">
Table 2: Additional syntactic (a) and semantic (b) fea-
tures extracted from the LADL and Dicovalence re-
sources and the alternations/roles they are possibly re-
lated to.
</tableCaption>
<bodyText confidence="0.999674346153846">
features are meant to help identify specific Verbnet
classes and thematic roles. Finally, we extract four
semantic features from the lexicon. These indicate
whether a verb takes a locative or an asset argument
and whether it requires a concrete object (non hu-
man role) or a plural role. The potential correlation
between these features and Verbnet classes is given
in Table 1(b).
French Gold Standard To evaluate our approach,
we use the gold standard proposed by Sun et al.
(2010). This resource consists of 16 fine grained
Levin classes with 12 verbs each whose predomi-
nant sense in English belong to that class. Since
our goal is to build a Verbnet like classification
for French, we mapped the 16 Levin classes of the
Sun et al. (2010)’s Gold Standard to 11 Verbnet
classes thereby associating each class with a the-
matic grid. In addition we group Verbnet semantic
roles as shown in Table 4. Table 3 shows the refer-
ence we use for evaluation.
Verbs For our clustering experiments we use the
2183 French verbs occurring in the translations of
the 11 classes in the gold standard (cf. Section 4).
Since we ignore verbs with only one feature the
number of verbs and verb, feature) pairs considered
may vary slightly across experiments.
</bodyText>
<table confidence="0.999766642857143">
AgExp Agent, Experiencer
AgentSym Actor, Actor1, Actor2
Theme Theme, Topic, Stimulus, Proposition
PredAtt Predicate, Attribute
ThemeSym Theme, Theme1, Theme2
Patient Patient
PatientSym Patient, Patient1, Patient2
Start Material (transformation), Source (motion,
End transfer)
Location Product (transformation), Destination (mo-
tion), Recipient (transfer)
Instrument
Cause
Beneficiary
</table>
<tableCaption confidence="0.998123">
Table 4: Verbnet role groups.
</tableCaption>
<sectionHeader confidence="0.999952" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.998293">
5.1 Quantitative Analysis
</subsectionHeader>
<bodyText confidence="0.999841066666667">
Table 4(a) includes the evaluation results for all the
feature sets when using IGNGF clustering.
In terms of F-measure, the results range from 0.61
to 0.70. This generally outperforms (Sun et al.,
2010) whose best F-measures vary between 0.55 for
verbs occurring at least 150 times in the training data
and 0.65 for verbs occurring at least 4000 times in
this training data. The results are not directly com-
parable however since the gold data is slightly dif-
ferent due to the grouping of Verbnet classes through
their thematic grids.
In terms of features, the best results are ob-
tained using the grid-scf-sem feature set with an F-
measure of 0.70. Moreover, for this data set, the un-
supervised evaluation metrics (cf. Section 3) high-
light strong cluster cohesion with a number of clus-
ters close to the number of gold classes (13 clusters
for 11 gold classes); a low number of orphan verbs
(i.e., verbs whose confidence score is zero); and a
high Cumulated Micro Precision (CMP = 0.3) indi-
cating homogeneous clusters in terms of maximis-
ing features. The coverage of 0.72 indicates that ap-
proximately 8 out of the 11 gold classes could be
matched to a prevalent label. That is, 8 clusters were
labelled with a prevalent label corresponding to 8
distinct gold classes.
In contrast, the classification obtained using the
scf-synt-sem feature set has a higher CMP for the
clustering with optimal mPUR (0.57); but a lower
F-measure (0.61), a larger number of classes (16)
</bodyText>
<page confidence="0.996514">
858
</page>
<table confidence="0.999774482758621">
AgExp, PatientSym
amalgamate-22.2: incorporer, associer, r´eunir, m´elanger, mˆeler, unir, assembler, combiner, lier, fusionner
Cause, AgExp
amuse-31.1: abattre, accabler, briser, d´eprimer, consterner, an´eantir, ´epuiser, ext´enuer, ´ecraser, ennuyer, ´ereinter, inonder
AgExp, PredAtt, Theme
characterize-29.2: appr´ehender, concevoir, consid´erer, d´ecrire, d´efinir, d´epeindre, d´esigner, envisager, identifier, montrer, percevoir, repr´esenter, ressen-
tir
AgentSym, Theme
correspond-36.1: coop´erer, participer, collaborer, concourir, contribuer, associer
AgExp, Beneficiary, Extent, Start, Theme
get-13.5.1: acheter, prendre, saisir, r´eserver, conserver, garder, pr´eserver, maintenir, retenir, louer, affr´eter
AgExp, Instrument, Patient
hit-18.1: cogner, heurter, battre, frapper, fouetter, taper, rosser, brutaliser, ´ereinter, maltraiter, corriger
other cos-45.4: m´elanger, fusionner, consolider, renforcer, fortifier, adoucir, polir, att´enuer, temp´erer, p´etrir, fac¸onner, former
AgExp, Location, Theme
light emission-43.1 briller, ´etinceler, flamboyer, luire, resplendir, p´etiller, rutiler, rayonner, scintiller
modes of being with motion-47.3: trembler, fr´emir, osciller, vaciller, vibrer, tressaillir, frissonner, palpiter, gr´esiller, trembloter, palpiter
run-51.3.2: voyager, aller, errer, circuler, courir, bouger, naviguer, passer, promener, d´eplacer
AgExp, End, Theme
manner speaking-37.3: rˆaler, gronder, crier, ronchonner, grogner, bougonner, maugr´eer, rousp´eter, grommeler, larmoyer, g´emir, geindre, hurler,
gueuler, brailler, chuchoter
put-9.1: accrocher, d´eposer, mettre, placer, r´epartir, r´eint´egrer, empiler, emporter, enfermer, ins´erer, installer
say-37.7: dire, r´ev´eler, d´eclarer, signaler, indiquer, montrer, annoncer, r´epondre, affirmer, certifier, r´epliquer
AgExp, Theme
peer-30.3: regarder, ´ecouter, examiner, consid´erer, voir, scruter, d´evisager
AgExp, Start, Theme
remove-10.1: ˆoter, enlever, retirer, supprimer, retrancher, d´ebarasser, soustraire, d´ecompter, ´eliminer
AgExp, End, Start, Theme
send-11.1: envoyer, lancer, transmettre, adresser, porter, exp´edier, transporter, jeter, renvoyer, livrer
</table>
<tableCaption confidence="0.999948">
Table 3: French gold classes and their member verbs presented in (Sun et al., 2010).
</tableCaption>
<bodyText confidence="0.999961869565217">
and a higher number of orphans (156). That is, this
clustering has many clusters with strong feature co-
hesion but a class structure that markedly differs
from the gold. Since there might be differences in
structure between the English Verbnet and the the-
matic classification for French we are building, this
is not necessarily incorrect however. Further inves-
tigation on a larger data set would be required to as-
sess which clustering is in fact better given the data
used and the classification searched for.
In general, data sets whose description includes
semantic features (sem or grid) tend to produce bet-
ter results than those that do not (scf or synt). This
is in line with results from (Sun et al., 2010) which
shows that semantic features help verb classifica-
tion. It differs from it however in that the seman-
tic features used by Sun et al. (2010) are selectional
preferences while ours are thematic grids and a re-
stricted set of manually encoded selectional prefer-
ences.
Noticeably, the synt feature degrades perfor-
mance throughout: grid,scf,synt has lower F-
measure than grid,scf; scf,synt,sem than scf,sem;
and scf,synt than scf. We have no clear explanation
for this.
The best results are obtained with IGNGF method
on most of the data sets. Table 4(b) illustrates
the differences between the results obtained with
IGNGF and those obtained with K-means on the
grid-scf-sem data set (best data set). Although K-
means and IGNGF optimal model reach similar F-
measure and display a similar number of clusters,
the very low CMP (0.10) of the K-means model
shows that, despite a good Gold class coverage
(0.81), K-means tend to produce more heteroge-
neous clusters in terms of features.
Table 4(b) also shows the impact of IDF feature
weighting and feature vector normalisation on clus-
tering. The benefit of preprocessing the data appears
clearly. When neither IDF weighting nor vector nor-
malisation are used, F-measure decreases from 0.70
to 0.68 and cumulative micro-precision from 0.30
to 0.21. When either normalisation or IDF weight-
ing is left out, the cumulative micro-precision drops
by up to 15 points (from 0.30 to 0.15 and 0.18) and
the number of orphans increases from 67 up to 180.
</bodyText>
<page confidence="0.997618">
859
</page>
<table confidence="0.999056461538461">
Nbr. feat. Nbr. verbs (a) The impact of the feature set. Cov. Nbr. orphans CMP at opt (13cl.)
mPUR ACC F (Gold) Nbr. classes
220 2085 0.93 0.48 0.64 17 0.55 129 0.28 (0.27)
231 2085 0.94 0.54 0.68 14 0.64 183 0.12 (0.12)
237 2183 0.86 0.59 0.70 13 0.72 67 0.30 (0.30)
236 2150 0.87 0.50 0.63 14 0.72 66 0.13 (0.14)
242 2201 0.99 0.52 0.69 16 0.82 100 0.50 (0.22)
226 2183 0.83 0.55 0.66 23 0.64 146 0.40 (0.26)
225 2150 0.91 0.45 0.61 15 0.45 83 0.17 (0.22)
231 2101 0.89 0.47 0.61 16 0.64 156 0.57 (0.11)
Feat. set
scf
grid, scf
grid, scf, sem
grid, scf, synt
grid, scf, synt, sem
scf, sem
scf, synt
scf, synt, sem
(b) Metrics for best performing clustering method (IGNGF) compared to K-means. Feature set is grid, scf, sem.
Method mPUR ACC F (Gold) Nbr. classes Cov. Nbr. orphans CMP at opt (13cl.)
IGNGF with IDF and norm. 0.86 0.59 0.70 13 0.72 67 0.30 (0.30)
K-means with IDF and norm. 0.88 0.57 0.70 13 0.81 67 0.10 (0.10)
IGNGF, no IDF 0.86 0.59 0.70 17 0.81 126 0.18 (0.14)
IGNGF, no norm. 0.78 0.62 0.70 18 0.72 180 0.15 (0.11)
IGNGF, no IDF, no norm. 0.87 0.55 0.68 14 0.81 103 0.21 (0.21)
</table>
<tableCaption confidence="0.994343">
Table 5: Results. Cumulative micro precision (CMP) is given for the clustering at the mPUR optimum and in paran-
theses for 13 classes clustering.
</tableCaption>
<bodyText confidence="0.9579315">
That is, clusters are less coherent in terms of fea-
tures.
</bodyText>
<subsectionHeader confidence="0.999727">
5.2 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999921518518519">
We carried out a manual analysis of the clusters ex-
amining both the semantic coherence of each cluster
(do the verbs in that cluster share a semantic com-
ponent?) and the association between the thematic
grids, the verbs and the syntactic frames provided
by clustering.
Semantic homogeneity: To assess semantic ho-
mogeneity, we examined each cluster and sought
to identify one or more Verbnet labels character-
ising the verbs contained in that cluster. From
the 13 clusters produced by clustering, 11 clus-
ters could be labelled. Table 6 shows these eleven
clusters, the associated labels (abbreviated Verbnet
class names), some example verbs, a sample sub-
categorisation frame drawn from the cluster max-
imising features and an illustrating sentence. As
can be seen, some clusters group together several
subclasses and conversely, some Verbnet classes are
spread over several clusters. This is not necessar-
ily incorrect though. To start with, recall that we
are aiming for a classification which groups together
verbs with the same thematic grid. Given this, clus-
ter C2 correctly groups together two Verbnet classes
(other cos-45.4 and hit-18.1) which share the same
thematic grid (cf. Table 3). In addition, the features
associated with this cluster indicate that verbs in
these two classes are transitive, select a concrete ob-
ject, and can be pronominalised which again is cor-
rect for most verbs in that cluster. Similarly, cluster
C11 groups together verbs from two Verbnet classes
with identical theta grid (light emission-43.1 and
modes of being with motion-47.3) while its associ-
ated features correctly indicate that verbs from both
classes accept both the intransitive form without ob-
ject (la jeune fille rayonne /the young girl glows, un
cheval galope / a horse gallops) and with a prepo-
sitional object (la jeune fille rayonne de bonheur /
the young girl glows with happiness, un cheval ga-
lope vers l’infini / a horse gallops to infinity). The
third cluster grouping together verbs from two Verb-
net classes is C7 which contains mainly judgement
verbs (to applaud, bless, compliment, punish) but
also some verbs from the (very large) other cos-45.4
class. In this case, a prevalent shared feature is
that both types of verbs accept a de-object that is,
a prepositional object introduced by ”de” (Jean ap-
plaudit Marie d’avoir dans´e /Jean applaudit Marie
for having danced; Jean d´egage le sable de la route/
Jean clears the sand of the road). The semantic fea-
tures necessary to provide a finer grained analysis of
their differences are lacking.
Interestingly, clustering also highlights classes
which are semantically homogeneous but syntac-
tically distinct. While clusters C6 and C10 both
</bodyText>
<page confidence="0.989237">
860
</page>
<bodyText confidence="0.999858633333334">
contain mostly verbs from the amuse-31.1 class
(amuser,agacer,´enerver,d´eprimer), their features in-
dicate that verbs in C10 accept the pronominal form
(e.g., Jean s’amuse) while verbs in C6 do not (e.g.,
*Jean se d´eprime). In this case, clustering highlights
a syntactic distinction which is present in French but
not in English. In contrast, the dispersion of verbs
from the other cos-45.4 class over clusters C2 and
C7 has no obvious explanation. One reason might
be that this class is rather large (361 verbs) and thus
might contain French verbs that do not necessarily
share properties with the original Verbnet class.
Syntax and Semantics. We examined whether the
prevalent syntactic features labelling each cluster
were compatible with the verbs and with the seman-
tic class(es) manually assigned to the clusters. Ta-
ble 6 sketches the relation between cluster, syntac-
tic frames and Verbnet like classes. It shows for in-
stance that the prevalent frame of the C0 class (man-
ner speaking-37.3) correctly indicates that verbs in
that cluster subcategorise for a sentential argument
and an AOBJ (prepositional object in “`a”) (e.g., Jean
bafouille a� Marie qu’il est amoureux / Jean stam-
mers to Mary that he is in love); and that verbs
in the C9 class (characterize-29.2) subcategorise for
an object NP and an attribute (Jean nomme Marie
pr´esidente /Jean appoints Marie president). In gen-
eral, we found that the prevalent frames associated
with each cluster adequately characterise the syntax
of that verb class.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999961533333333">
We presented an approach to the automatic classi-
fication of french verbs which showed good results
on an established testset and associates verb clusters
with syntactic and semantic features.
Whether the features associated by the IGNGF
clustering with the verb clusters appropriately car-
acterise these clusters remains an open question. We
carried out a first evaluation using these features
to label the syntactic arguments of verbs in a cor-
pus with thematic roles and found that precision is
high but recall low mainly because of polysemy: the
frames and grids made available by the classification
for a given verb are correct for that verb but not for
the verb sense occurring in the corpus. This sug-
gests that overlapping clustering techniques need to
</bodyText>
<figure confidence="0.966559411764706">
speaking: babiller, bafouiller, balbutier
SUJ:NP,OBJ:Ssub,AOBJ:PP
Jean bafouille a� Marie qu’il l’aime /Jean stammers to Mary that he is
in love
put: entasser, r´epandre, essaimer
SUJ:NP,POBJ:PP,DUMMY:REFL
Loc, Plural
Les d´echets s’entassent dans la cour/Waste piles in the yard
hit: broyer, d´emolir, fouetter
SUJ:NP,OBJ:NP
T-Nhum
Ces pierres broient les graines /These stones grind the seeds.
other cos: agrandir, all´eger, amincir
SUJ:NP,DUMMY:REFL
les a´eroports s’agrandissent sans arr@t /airports grow constantly
dedicate: s’engager `a, s’obliger `a,
SUJ:NP,AOBJ:VPinf,DUMMY:REFL
Cette promesse t’engage a� nous suivre / This promise commits you to
following us
conjecture: penser, attester, agr´eer
SUJ:NP,OBJ:Ssub
Le m´edecin atteste que l’employ´e n’est pas en ´etat de travailler / The
physician certi�es that the employee is notable to work
amuse: d´eprimer, d´econtenancer, d´ecevoir
SUJ:Ssub,OBJ:NP
SUJ:NP,DEOBJ:Ssub
Travailler d´eprime Marie /Working depresses Marie
Marie d´eprime de ce que Jean parte/Marie depresses because ofJean’s
leaving
other cos: d´egager, vider, drainer, sevrer
judgement
SUJ:NP,OBJ:NP,DEOBJ:PP
vider le r´ecipient de son contenu / empty the container of its contents
applaudir, b´enir, blˆamer,
SUJ:NP,OBJ:NP,DEOBJ:Ssub
Jean blame Marie d’avoir couru /Jean blames Mary for runnig
characterise: promouvoir, adouber, nommer
SUJ:NP,OBJ:NP,ATB:XP
Jean nomme Marie pr´esidente /Jean appoints Marie president
amuse: agacer, amuser, enorgueillir
SUJ:NP,DEOBJ:XP,DUMMY:REFL
Jean s’enorgueillit d’etre roi/ Jean is proud to be king
light: rayonner,clignoter,cliqueter
SUJ:NP,POBJ:PP
Jean clignote des yeux /Jean twinkles his eyes
motion: aller, passer, fuir, glisser
SUJ:NP,POBJ:PP
glisser sur le trottoir verglac´e / slip on the icy sidewalk
transfer msg: enseigner, permettre, interdire
SUJ:NP,OBJ:NP,AOBJ:PP
Jean enseigne l’anglais a� Marie /Jean teaches Marie English.
</figure>
<tableCaption confidence="0.9083995">
Table 6: Relations between clusters, syntactic frames and
Verbnet like classes.
</tableCaption>
<bodyText confidence="0.9932322">
be applied.
We are also investigating how the approach scales
up to the full set of verbs present in the lexicon. Both
Dicovalence and the LADL tables contain rich de-
tailed information about the syntactic and semantic
properties of French verbs. We intend to tap on that
potential and explore how well the various semantic
features that can be extracted from these resources
support automatic verb classification for the full set
of verbs present in our lexicon.
</bodyText>
<figure confidence="0.996429818181818">
C0
C1
C2
C4
C5
C6
C7
C9
C10
C11
C12
</figure>
<page confidence="0.990185">
861
</page>
<sectionHeader confidence="0.99543" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999695742857143">
M. Attik, S. Al Shehabi, and J.-C. Lamirel. 2006. Clus-
tering Quality Measures for Data Samples with Mul-
tiple Labels. In Databases and Applications, pages
58–65.
M. Barbut and B. Monjardet. 1970. Ordre et Classifica-
tion. Hachette Universit´e.
C. Brew and S. Schulte im Walde. 2002. Spectral Clus-
tering for German Verbs. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 117–124, Philadelphia, PA.
C. Chang and C. Lin. 2011. LIBSVM: A library for
support vector machines. ACM Transactions on Intel-
ligent Systems and Technology, 2:27:1–27:27. Soft-
ware available at http://www.csie.ntu.edu.
tw/˜cjlin/libsvm.
H. T. Dang. 2004. Investigations into the role of lexical
semantics in word sense disambiguation. Ph.D. thesis,
U. Pennsylvannia, US.
I. Falk and C. Gardent. 2011. Combining Formal Con-
cept Analysis and Translation to Assign Frames and
Thematic Role Sets to French Verbs. In Amedeo
Napoli and Vilem Vychodil, editors, Concept Lattices
and Their Applications, Nancy, France, October.
B. Fritzke. 1995. A growing neural gas network learns
topologies. Advances in Neural Information Process-
ing Systems 7, 7:625–632.
M. Ghribi, P. Cuxac, J.-C. Lamirel, and A. Lelu. 2010.
Mesures de qualit´e de clustering de documents : prise
en compte de la distribution des mots cl´es. In Nicolas
B´echet, editor, ´Evaluation des m´ethodes d’Extraction
de Connaissances dans les Donn´ees- EvalECD’2010,
pages 15–28, Hammamet, Tunisie, January. Fatiha
Sais.
M. Gross. 1975. M´ethodes en syntaxe. Hermann, Paris.
D. O. Hebb. 1949. The organization of behavior: a
neuropsychological theory. John Wiley &amp; Sons, New
York.
K. Kipper Schuler. 2006. VerbNet: A Broad-Coverage,
Comprehensive Verb Lexicon. Ph.D. thesis, University
of Pennsylvania.
A. Kup´s´c and A. Abeill´e. 2008. Growing treelex. In
Alexander Gelbkuh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 4919 of
Lecture Notes in Computer Science, pages 28–39.
Springer Berlin / Heidelberg.
J.-C. Lamirel, A. Phuong Ta, and M. Attik. 2008. Novel
Labeling Strategies for Hierarchical Representation of
Multidimensional Data Analysis Results. In AIA -
IASTED, Innbruck, Autriche.
J. C. Lamirel, P. Cuxac, and R. Mall. 2011a. A new
efficient and unbiased approach for clustering quality
evaluation. In QIMIE’11, PaKDD, Shenzen, China.
J.-C. Lamirel, R. Mall, P. Cuxac, and G. Safi. 2011b.
Variations to incremental growing neural gas algo-
rithm based on label maximization. In Neural Net-
works (IJCNN), The 2011 International Joint Confer-
ence on, pages 956 –965.
B. Levin. 1993. English Verb Classes and Alternations:
a preliminary investigation. University of Chicago
Press, Chicago and London.
T. Martinetz and K. Schulten. 1991. A ”Neural-Gas”
Network Learns Topologies. Artificial Neural Net-
works, I:397–402.
P. Merlo, S. Stevenson, V. Tsang, and G. Allaria. 2002.
A multilingual paradigm for automatic verb classifica-
tion. In ACL, pages 207–214.
C. Messiant. 2008. A subcategorization acquisition sys-
tem for French verbs. In Proceedings of the ACL-
08: HLT Student Research Workshop, pages 55–60,
Columbus, Ohio, June. Association for Computational
Linguistics.
C. Mouton. 2010. Ressources et m´ethodes semi-
supervis´ees pour l’analyse s´emantique de textes en
fran cais. Ph.D. thesis, Universit´e Paris 11 - Paris Sud
UFR d’informatique.
L. Nicolas, B. Sagot, ´E. de La Clergerie, and J. Farr´e.
2008. Computer aided correction and extension of a
syntactic wide-coverage lexicon. In Proc. of CoLing
2008, Manchester, UK, August.
A. Oishi and Y. Matsumoto. 1997. Detecting the orga-
nization of semantic subclasses of Japanese verbs. In-
ternational Journal of Corpus Linguistics, 2(1):65–89,
october.
Y. Prudent and A. Ennaji. 2005. An incremental grow-
ing neural gas learns topologies. In Neural Networks,
2005. IJCNN ’05. Proceedings. 2005 IEEE Interna-
tional Joint Conference on, volume 2, pages 1211–
1216.
J. H. Randall. 2010. Linking. Studies in Natural Lan-
guage and Linguistic Theory. Springer, Dordrecht.
S. E. Robertson and K. S. Jones. 1976. Relevance
weighting of search terms. Journal of the American
Society for Information Science, 27(3):129–146.
S. Schulte im Walde. 2003. Experiments on the Auto-
matic Induction of German Semantic Verb Classes.
Ph.D. thesis, Institut f¨ur Maschinelle Sprachverar-
beitung, Universit¨at Stuttgart. Published as AIMS Re-
port 9(2).
S. Schulte im Walde. 2006. Experiments on the au-
tomatic induction of german semantic verb classes.
Computational Linguistics, 32(2):159–194.
L. Sun, A. Korhonen, T. Poibeau, and C. Messiant. 2010.
Investigating the cross-linguistic potential of verbnet:
style classification. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
</reference>
<page confidence="0.977202">
862
</page>
<reference confidence="0.9988004">
COLING ’10, pages 1056–1064, Stroudsburg, PA,
USA. Association for Computational Linguistics.
R. S. Swier and S. Stevenson. 2005. Exploiting
a verb lexicon in automatic semantic role labelling.
In HLT/EMNLP. The Association for Computational
Linguistics.
K. van den Eynde and P. Mertens. 2003. La valence :
l’approche pronominale et son application au lexique
verbal. Journal of French Language Studies, 13:63–
104.
</reference>
<page confidence="0.999169">
863
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819147">
<title confidence="0.999493">Classifying French Verbs Using French and English Lexical Resources</title>
<author confidence="0.98955">Ingrid Falk Claire Gardent Jean-Charles Lamirel</author>
<affiliation confidence="0.999635">Universit´e de Lorraine/LORIA, CNRS/LORIA, Universit´e de Strasbourg/LORIA,</affiliation>
<address confidence="0.990615">Nancy, France Nancy, France Nancy, France</address>
<email confidence="0.973817">ingrid.falk@loria.frclaire.gardent@loria.frjean-charles.lamirel@loria.fr</email>
<abstract confidence="0.988080833333333">We present a novel approach to the automatic acquisition of a Verbnet like classification of French verbs which involves the use (i) of a neural clustering method which associates clusters with features, (ii) of several supervised and unsupervised evaluation metrics and (iii) of various existing syntactic and semantic lexical resources. We evaluate our approach on an established test set and show that it outperforms previous related work with an Fmeasure of 0.70.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Attik</author>
<author>S Al Shehabi</author>
<author>J-C Lamirel</author>
</authors>
<title>Clustering Quality Measures for Data Samples with Multiple Labels. In Databases and Applications,</title>
<date>2006</date>
<pages>58--65</pages>
<contexts>
<context position="9501" citStr="Attik et al., 2006" startWordPosition="1489" endWordPosition="1492"> cluster iff its Feature F-measure is higher for that cluster than for any other cluster. The IGNGF method was shown to outperform other usual neural and non neural methods for clustering tasks on relatively clean data (Lamirel et al., 2011b). Since we use features extracted from manually validated sources, this clustering technique seems a good fit for our application. In addition, the feature maximisation and cluster labeling performed by the IGNGF method has proved promising both for visualising clustering results (Lamirel et al., 2008) and for validating or optimising a clustering method (Attik et al., 2006). We make use of these processes in all our experiments and systematically compute cluster labelling and feature maximisation on the output clusterings. As we shall see, this permits distinguishing between clusterings with similar F-measure but lower “linguistic plausibility” (cf. Section 5). This facilitates clustering interpretation in that cluster labeling clearly indicates the association between clusters (verbs) and their prevalent features. And this supports the creation of a Verbnet style classification in that cluster labeling directly provides classes grouping together verbs, thematic</context>
<context position="11749" citStr="Attik et al., 2006" startWordPosition="1848" endWordPosition="1851">uracy we associate to each gold class CGold a dominant cluster, ie. the cluster dom(CGold) which has most verbs in common with the gold class. Then accuracy is given by the following formula: ACC = ECEGold |dom(C) ∩ C| VerbsGoldnClustering Finally, F-measure is the harmonic mean of mPUR and ACC. Coverage. To assess the extent to which a clustering matches the gold classification, we additionally compute the coverage of each clustering that is, the proportion of gold classes that are prevalent classes in the clustering. Cumulative Micro Precision (CMP). As pointed out in (Lamirel et al., 2008; Attik et al., 2006), unsupervised evaluation metrics based on cluster labelling and feature maximisation can prove very useful for identifying the best clustering strategy. Following (Lamirel et al., 2011a), we use CMP to identify the best clustering. Computed on the clustering results, this metrics evaluates the quality of a clustering w.r.t. the cluster features rather than w.r.t. 6Clusters for which the prevalent class has only one element are ignored E vEc E E l clECvEc E vEc E flEF�,vEc Wvf Wvf FRc(f) = , F Pc(f) = W v f Wfl v VerbsGoldnClustering , 856 to a gold standard. It was shown in (Ghribi et al., 20</context>
</contexts>
<marker>Attik, Shehabi, Lamirel, 2006</marker>
<rawString>M. Attik, S. Al Shehabi, and J.-C. Lamirel. 2006. Clustering Quality Measures for Data Samples with Multiple Labels. In Databases and Applications, pages 58–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Barbut</author>
<author>B Monjardet</author>
</authors>
<title>Ordre et Classification. Hachette Universit´e.</title>
<date>1970</date>
<contexts>
<context position="2538" citStr="Barbut and Monjardet, 1970" startWordPosition="376" endWordPosition="379">m (Messiant, 2008)) acquired fully automatically from Le Monde newspaper corpus and show that, as for English, syntactic frames and verb selectional preferences perform better than lexical cooccurence features. Their approach achieves a F-measure of 55.1 on 116 verbs occurring at least 150 times in Lexschem. The best performance is achieved when restricting the approach to verbs occurring at least 4000 times (43 verbs) with an F-measure of 65.4. On the other hand, Falk and Gardent (2011) present a classification approach for French verbs based on the use of Formal Concept Analysis (FCA). FCA (Barbut and Monjardet, 1970) is a symbolic classification technique which permits creating classes associating sets of objects (eg. French verbs) with sets of features (eg. syntactic frames). Falk and Gardent (2011) provide no evaluation for their results however, only a qualitative analysis. In this paper, we describe a novel approach to the clustering of French verbs which (i) gives good results on the established benchmark used in (Sun et al., 2010) and (ii) associates verbs with a feature profile describing their syntactic and semantic properties. The approach exploits a clustering method called IGNGF (Incremental Gr</context>
</contexts>
<marker>Barbut, Monjardet, 1970</marker>
<rawString>M. Barbut and B. Monjardet. 1970. Ordre et Classification. Hachette Universit´e.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brew</author>
<author>S Schulte im Walde</author>
</authors>
<title>Spectral Clustering for German Verbs.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<location>Philadelphia, PA.</location>
<marker>Brew, Walde, 2002</marker>
<rawString>C. Brew and S. Schulte im Walde. 2002. Spectral Clustering for German Verbs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 117–124, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chang</author>
<author>C Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http://www.csie.ntu.edu. tw/˜cjlin/libsvm.</note>
<contexts>
<context position="8071" citStr="Chang and Lin, 2011" startWordPosition="1250" endWordPosition="1253">ing to the family of the free topology neural clustering methods. Like other neural free topology methods such as Neural Gas (NG) (Martinetz and Schulten, 1991), Growing Neural Gas (GNG) (Fritzke, 1995), or Incremental Growing Neural Gas (IGNG) (Prudent and Ennaji, 2005), the IGNGF method makes use of Hebbian learning product_info.php?products_id=666), Google dictionary (http://www.google.com/dictionary) and Dicovalence (van den Eynde and Mertens, 2003). 3The training data consists of the verbs and Verbnet classes used in the gold standard presented in (Sun et al., 2010). 4We used the libsvm (Chang and Lin, 2011) implementation of the classifier for this step. 5The accuracy of the classifier on the held out random test set of 100 pairs was of 90%. 855 (Hebb, 1949) for dynamically structuring the learning space. However, contrary to these methods, the use of a standard distance measure for determining a winner is replaced in IGNGF by feature maximisation. Feature maximisation is a cluster quality metric which associates each cluster with maximal features i.e., features whose Feature F-measure is maximal. Feature F-measure is the harmonic mean of Feature Recall and Feature Precision which in turn are de</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>C. Chang and C. Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http://www.csie.ntu.edu. tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
</authors>
<title>Investigations into the role of lexical semantics in word sense disambiguation.</title>
<date>2004</date>
<tech>Ph.D. thesis, U. Pennsylvannia, US.</tech>
<contexts>
<context position="1299" citStr="Dang, 2004" startWordPosition="180" endWordPosition="181">established test set and show that it outperforms previous related work with an Fmeasure of 0.70. 1 Introduction Verb classifications have been shown to be useful both from a theoretical and from a practical perspective. From the theoretical viewpoint, they permit capturing syntactic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the automatic classification of French verbs. Recently however, two proposals have been put forward. On the one hand, (Sun et al., 2010) applied a clustering approach developed for English to French. They exploit features extracted from a large scale subcategorisation lexi</context>
</contexts>
<marker>Dang, 2004</marker>
<rawString>H. T. Dang. 2004. Investigations into the role of lexical semantics in word sense disambiguation. Ph.D. thesis, U. Pennsylvannia, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Falk</author>
<author>C Gardent</author>
</authors>
<title>Combining Formal Concept Analysis and Translation to Assign Frames and Thematic Role Sets to French Verbs.</title>
<date>2011</date>
<booktitle>In Amedeo Napoli and Vilem Vychodil, editors, Concept Lattices and Their Applications,</booktitle>
<location>Nancy, France,</location>
<contexts>
<context position="2403" citStr="Falk and Gardent (2011)" startWordPosition="355" endWordPosition="358">ing approach developed for English to French. They exploit features extracted from a large scale subcategorisation lexicon (LexSchem (Messiant, 2008)) acquired fully automatically from Le Monde newspaper corpus and show that, as for English, syntactic frames and verb selectional preferences perform better than lexical cooccurence features. Their approach achieves a F-measure of 55.1 on 116 verbs occurring at least 150 times in Lexschem. The best performance is achieved when restricting the approach to verbs occurring at least 4000 times (43 verbs) with an F-measure of 65.4. On the other hand, Falk and Gardent (2011) present a classification approach for French verbs based on the use of Formal Concept Analysis (FCA). FCA (Barbut and Monjardet, 1970) is a symbolic classification technique which permits creating classes associating sets of objects (eg. French verbs) with sets of features (eg. syntactic frames). Falk and Gardent (2011) provide no evaluation for their results however, only a qualitative analysis. In this paper, we describe a novel approach to the clustering of French verbs which (i) gives good results on the established benchmark used in (Sun et al., 2010) and (ii) associates verbs with a fea</context>
</contexts>
<marker>Falk, Gardent, 2011</marker>
<rawString>I. Falk and C. Gardent. 2011. Combining Formal Concept Analysis and Translation to Assign Frames and Thematic Role Sets to French Verbs. In Amedeo Napoli and Vilem Vychodil, editors, Concept Lattices and Their Applications, Nancy, France, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fritzke</author>
</authors>
<title>A growing neural gas network learns topologies.</title>
<date>1995</date>
<booktitle>Advances in Neural Information Processing Systems 7,</booktitle>
<pages>7--625</pages>
<contexts>
<context position="7653" citStr="Fritzke, 1995" startWordPosition="1192" endWordPosition="1193">ith highest probability estimates and obtain the translated classes by assigning each verb in a selected pair to the pair’s class. This way French verbs are effectively associated with one or more English Verbnet thematic grids. 3 Clustering Methods, Evaluation Metrics and Experimental Setup 3.1 Clustering Methods The IGNGF clustering method is an incremental neural “winner-take-most” clustering method belonging to the family of the free topology neural clustering methods. Like other neural free topology methods such as Neural Gas (NG) (Martinetz and Schulten, 1991), Growing Neural Gas (GNG) (Fritzke, 1995), or Incremental Growing Neural Gas (IGNG) (Prudent and Ennaji, 2005), the IGNGF method makes use of Hebbian learning product_info.php?products_id=666), Google dictionary (http://www.google.com/dictionary) and Dicovalence (van den Eynde and Mertens, 2003). 3The training data consists of the verbs and Verbnet classes used in the gold standard presented in (Sun et al., 2010). 4We used the libsvm (Chang and Lin, 2011) implementation of the classifier for this step. 5The accuracy of the classifier on the held out random test set of 100 pairs was of 90%. 855 (Hebb, 1949) for dynamically structuring</context>
</contexts>
<marker>Fritzke, 1995</marker>
<rawString>B. Fritzke. 1995. A growing neural gas network learns topologies. Advances in Neural Information Processing Systems 7, 7:625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ghribi</author>
<author>P Cuxac</author>
<author>J-C Lamirel</author>
<author>A Lelu</author>
</authors>
<title>Mesures de qualit´e de clustering de documents : prise en compte de la distribution des mots cl´es.</title>
<date>2010</date>
<booktitle>Evaluation des m´ethodes d’Extraction de Connaissances dans les Donn´ees- EvalECD’2010,</booktitle>
<pages>15--28</pages>
<editor>In Nicolas B´echet, editor,</editor>
<contexts>
<context position="12352" citStr="Ghribi et al., 2010" startWordPosition="1954" endWordPosition="1957">tik et al., 2006), unsupervised evaluation metrics based on cluster labelling and feature maximisation can prove very useful for identifying the best clustering strategy. Following (Lamirel et al., 2011a), we use CMP to identify the best clustering. Computed on the clustering results, this metrics evaluates the quality of a clustering w.r.t. the cluster features rather than w.r.t. 6Clusters for which the prevalent class has only one element are ignored E vEc E E l clECvEc E vEc E flEF�,vEc Wvf Wvf FRc(f) = , F Pc(f) = W v f Wfl v VerbsGoldnClustering , 856 to a gold standard. It was shown in (Ghribi et al., 2010) to be effective in detecting degenerated clustering results including a small number of large heterogeneous, “garbage” clusters and a big number of small size “chunk” clusters. First, the local Recall (Rfc ) and the local Precision (Pcf) of a feature f in a cluster c are defined as follows: c | Rf c = |vf |V f |P f c |c = |vf |Vc| where vfc is the set of verbs having feature f in c, Vc the set of verbs in c and V f, the set of verbs with feature f. Cumulative Micro-Precision (CMP) is then defined as follows: �i=|Ci-.rl,Ic.-V||c+12 ECECi+,f∈Fc Pc CMP = 1 Ei=|Cinf|,|Csup |Ci+ where Ci+ represen</context>
</contexts>
<marker>Ghribi, Cuxac, Lamirel, Lelu, 2010</marker>
<rawString>M. Ghribi, P. Cuxac, J.-C. Lamirel, and A. Lelu. 2010. Mesures de qualit´e de clustering de documents : prise en compte de la distribution des mots cl´es. In Nicolas B´echet, editor, ´Evaluation des m´ethodes d’Extraction de Connaissances dans les Donn´ees- EvalECD’2010, pages 15–28, Hammamet, Tunisie, January. Fatiha Sais.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gross</author>
</authors>
<title>M´ethodes en syntaxe.</title>
<date>1975</date>
<location>Hermann, Paris.</location>
<contexts>
<context position="4889" citStr="Gross, 1975" startWordPosition="743" endWordPosition="744">vers the core verbs of French, could be used to support semantic role labelling and is similar in spirit to the English Verbnet. In this first experiment, we therefore favoured extracting the features used for clustering, not from a large corpus parsed automatically, but from manually validated resources1. These lexical resources are (i) a syntactic lexicon produced by merging three existing lexicons for French and (ii) the English Verbnet. Among the many syntactic lexicons available for French (Nicolas et al., 2008; Messiant, 2008; Kup´s´c and Abeill´e, 2008; van den Eynde and Mertens, 2003; Gross, 1975), we selected and merged three lexicons built or validated manually namely, Dicovalence, TreeLex and the LADL tables. The resulting lexicon contains 5918 verbs, 20433 lexical entries (i.e., verb/frame pairs) and 345 subcategorisation frames. It also contains more detailed syntactic and semantic features such as lexical preferences (e.g., locative argument, concrete object) or thematic role information (e.g., symmetric arguments, asset role) which we make use of for clustering. We use the English Verbnet as a resource for associating French verbs with thematic grids as follows. We translate the</context>
</contexts>
<marker>Gross, 1975</marker>
<rawString>M. Gross. 1975. M´ethodes en syntaxe. Hermann, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D O Hebb</author>
</authors>
<title>The organization of behavior: a neuropsychological theory.</title>
<date>1949</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="8225" citStr="Hebb, 1949" startWordPosition="1280" endWordPosition="1281">rowing Neural Gas (GNG) (Fritzke, 1995), or Incremental Growing Neural Gas (IGNG) (Prudent and Ennaji, 2005), the IGNGF method makes use of Hebbian learning product_info.php?products_id=666), Google dictionary (http://www.google.com/dictionary) and Dicovalence (van den Eynde and Mertens, 2003). 3The training data consists of the verbs and Verbnet classes used in the gold standard presented in (Sun et al., 2010). 4We used the libsvm (Chang and Lin, 2011) implementation of the classifier for this step. 5The accuracy of the classifier on the held out random test set of 100 pairs was of 90%. 855 (Hebb, 1949) for dynamically structuring the learning space. However, contrary to these methods, the use of a standard distance measure for determining a winner is replaced in IGNGF by feature maximisation. Feature maximisation is a cluster quality metric which associates each cluster with maximal features i.e., features whose Feature F-measure is maximal. Feature F-measure is the harmonic mean of Feature Recall and Feature Precision which in turn are defined as: where W1 represents the weight of the feature f for element x and Fc designates the set of features associated with the verbs occuring in the cl</context>
</contexts>
<marker>Hebb, 1949</marker>
<rawString>D. O. Hebb. 1949. The organization of behavior: a neuropsychological theory. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper Schuler</author>
</authors>
<title>VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1052" citStr="Schuler, 2006" startWordPosition="142" endWordPosition="143"> the use (i) of a neural clustering method which associates clusters with features, (ii) of several supervised and unsupervised evaluation metrics and (iii) of various existing syntactic and semantic lexical resources. We evaluate our approach on an established test set and show that it outperforms previous related work with an Fmeasure of 0.70. 1 Introduction Verb classifications have been shown to be useful both from a theoretical and from a practical perspective. From the theoretical viewpoint, they permit capturing syntactic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the automatic classificati</context>
</contexts>
<marker>Schuler, 2006</marker>
<rawString>K. Kipper Schuler. 2006. VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kup´s´c</author>
<author>A Abeill´e</author>
</authors>
<title>Growing treelex.</title>
<date>2008</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>4919</volume>
<pages>28--39</pages>
<editor>In Alexander Gelbkuh, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>Kup´s´c, Abeill´e, 2008</marker>
<rawString>A. Kup´s´c and A. Abeill´e. 2008. Growing treelex. In Alexander Gelbkuh, editor, Computational Linguistics and Intelligent Text Processing, volume 4919 of Lecture Notes in Computer Science, pages 28–39. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Lamirel</author>
<author>A Phuong Ta</author>
<author>M Attik</author>
</authors>
<title>Novel Labeling Strategies for Hierarchical Representation of Multidimensional Data Analysis Results. In</title>
<date>2008</date>
<journal>AIA -IASTED, Innbruck, Autriche.</journal>
<contexts>
<context position="9427" citStr="Lamirel et al., 2008" startWordPosition="1477" endWordPosition="1480"> occuring in the cluster c. A feature is then said to be maximal for a given cluster iff its Feature F-measure is higher for that cluster than for any other cluster. The IGNGF method was shown to outperform other usual neural and non neural methods for clustering tasks on relatively clean data (Lamirel et al., 2011b). Since we use features extracted from manually validated sources, this clustering technique seems a good fit for our application. In addition, the feature maximisation and cluster labeling performed by the IGNGF method has proved promising both for visualising clustering results (Lamirel et al., 2008) and for validating or optimising a clustering method (Attik et al., 2006). We make use of these processes in all our experiments and systematically compute cluster labelling and feature maximisation on the output clusterings. As we shall see, this permits distinguishing between clusterings with similar F-measure but lower “linguistic plausibility” (cf. Section 5). This facilitates clustering interpretation in that cluster labeling clearly indicates the association between clusters (verbs) and their prevalent features. And this supports the creation of a Verbnet style classification in that cl</context>
<context position="11728" citStr="Lamirel et al., 2008" startWordPosition="1844" endWordPosition="1847">tering. To compute accuracy we associate to each gold class CGold a dominant cluster, ie. the cluster dom(CGold) which has most verbs in common with the gold class. Then accuracy is given by the following formula: ACC = ECEGold |dom(C) ∩ C| VerbsGoldnClustering Finally, F-measure is the harmonic mean of mPUR and ACC. Coverage. To assess the extent to which a clustering matches the gold classification, we additionally compute the coverage of each clustering that is, the proportion of gold classes that are prevalent classes in the clustering. Cumulative Micro Precision (CMP). As pointed out in (Lamirel et al., 2008; Attik et al., 2006), unsupervised evaluation metrics based on cluster labelling and feature maximisation can prove very useful for identifying the best clustering strategy. Following (Lamirel et al., 2011a), we use CMP to identify the best clustering. Computed on the clustering results, this metrics evaluates the quality of a clustering w.r.t. the cluster features rather than w.r.t. 6Clusters for which the prevalent class has only one element are ignored E vEc E E l clECvEc E vEc E flEF�,vEc Wvf Wvf FRc(f) = , F Pc(f) = W v f Wfl v VerbsGoldnClustering , 856 to a gold standard. It was shown </context>
</contexts>
<marker>Lamirel, Ta, Attik, 2008</marker>
<rawString>J.-C. Lamirel, A. Phuong Ta, and M. Attik. 2008. Novel Labeling Strategies for Hierarchical Representation of Multidimensional Data Analysis Results. In AIA -IASTED, Innbruck, Autriche.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Lamirel</author>
<author>P Cuxac</author>
<author>R Mall</author>
</authors>
<title>A new efficient and unbiased approach for clustering quality evaluation.</title>
<date>2011</date>
<booktitle>In QIMIE’11,</booktitle>
<location>PaKDD, Shenzen, China.</location>
<contexts>
<context position="3203" citStr="Lamirel et al., 2011" startWordPosition="479" endWordPosition="482"> permits creating classes associating sets of objects (eg. French verbs) with sets of features (eg. syntactic frames). Falk and Gardent (2011) provide no evaluation for their results however, only a qualitative analysis. In this paper, we describe a novel approach to the clustering of French verbs which (i) gives good results on the established benchmark used in (Sun et al., 2010) and (ii) associates verbs with a feature profile describing their syntactic and semantic properties. The approach exploits a clustering method called IGNGF (Incremental Growing Neural Gas with Feature Maximisation, (Lamirel et al., 2011b)) which uses the features characterising each cluster both to guide the clustering process and to label the output clusters. We apply this method to the data contained in various verb lexicons and we evalu854 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 854–863, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics ate the resulting classification on a slightly modified version of the gold standard provided by (Sun et al., 2010). We show that the approach yields promising results (F-measure of 70%) and that</context>
<context position="9122" citStr="Lamirel et al., 2011" startWordPosition="1431" endWordPosition="1434">h maximal features i.e., features whose Feature F-measure is maximal. Feature F-measure is the harmonic mean of Feature Recall and Feature Precision which in turn are defined as: where W1 represents the weight of the feature f for element x and Fc designates the set of features associated with the verbs occuring in the cluster c. A feature is then said to be maximal for a given cluster iff its Feature F-measure is higher for that cluster than for any other cluster. The IGNGF method was shown to outperform other usual neural and non neural methods for clustering tasks on relatively clean data (Lamirel et al., 2011b). Since we use features extracted from manually validated sources, this clustering technique seems a good fit for our application. In addition, the feature maximisation and cluster labeling performed by the IGNGF method has proved promising both for visualising clustering results (Lamirel et al., 2008) and for validating or optimising a clustering method (Attik et al., 2006). We make use of these processes in all our experiments and systematically compute cluster labelling and feature maximisation on the output clusterings. As we shall see, this permits distinguishing between clusterings wit</context>
<context position="11934" citStr="Lamirel et al., 2011" startWordPosition="1875" endWordPosition="1878">ormula: ACC = ECEGold |dom(C) ∩ C| VerbsGoldnClustering Finally, F-measure is the harmonic mean of mPUR and ACC. Coverage. To assess the extent to which a clustering matches the gold classification, we additionally compute the coverage of each clustering that is, the proportion of gold classes that are prevalent classes in the clustering. Cumulative Micro Precision (CMP). As pointed out in (Lamirel et al., 2008; Attik et al., 2006), unsupervised evaluation metrics based on cluster labelling and feature maximisation can prove very useful for identifying the best clustering strategy. Following (Lamirel et al., 2011a), we use CMP to identify the best clustering. Computed on the clustering results, this metrics evaluates the quality of a clustering w.r.t. the cluster features rather than w.r.t. 6Clusters for which the prevalent class has only one element are ignored E vEc E E l clECvEc E vEc E flEF�,vEc Wvf Wvf FRc(f) = , F Pc(f) = W v f Wfl v VerbsGoldnClustering , 856 to a gold standard. It was shown in (Ghribi et al., 2010) to be effective in detecting degenerated clustering results including a small number of large heterogeneous, “garbage” clusters and a big number of small size “chunk” clusters. Firs</context>
</contexts>
<marker>Lamirel, Cuxac, Mall, 2011</marker>
<rawString>J. C. Lamirel, P. Cuxac, and R. Mall. 2011a. A new efficient and unbiased approach for clustering quality evaluation. In QIMIE’11, PaKDD, Shenzen, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Lamirel</author>
<author>R Mall</author>
<author>P Cuxac</author>
<author>G Safi</author>
</authors>
<title>Variations to incremental growing neural gas algorithm based on label maximization.</title>
<date>2011</date>
<booktitle>In Neural Networks (IJCNN), The 2011 International Joint Conference on,</booktitle>
<pages>956--965</pages>
<contexts>
<context position="3203" citStr="Lamirel et al., 2011" startWordPosition="479" endWordPosition="482"> permits creating classes associating sets of objects (eg. French verbs) with sets of features (eg. syntactic frames). Falk and Gardent (2011) provide no evaluation for their results however, only a qualitative analysis. In this paper, we describe a novel approach to the clustering of French verbs which (i) gives good results on the established benchmark used in (Sun et al., 2010) and (ii) associates verbs with a feature profile describing their syntactic and semantic properties. The approach exploits a clustering method called IGNGF (Incremental Growing Neural Gas with Feature Maximisation, (Lamirel et al., 2011b)) which uses the features characterising each cluster both to guide the clustering process and to label the output clusters. We apply this method to the data contained in various verb lexicons and we evalu854 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 854–863, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics ate the resulting classification on a slightly modified version of the gold standard provided by (Sun et al., 2010). We show that the approach yields promising results (F-measure of 70%) and that</context>
<context position="9122" citStr="Lamirel et al., 2011" startWordPosition="1431" endWordPosition="1434">h maximal features i.e., features whose Feature F-measure is maximal. Feature F-measure is the harmonic mean of Feature Recall and Feature Precision which in turn are defined as: where W1 represents the weight of the feature f for element x and Fc designates the set of features associated with the verbs occuring in the cluster c. A feature is then said to be maximal for a given cluster iff its Feature F-measure is higher for that cluster than for any other cluster. The IGNGF method was shown to outperform other usual neural and non neural methods for clustering tasks on relatively clean data (Lamirel et al., 2011b). Since we use features extracted from manually validated sources, this clustering technique seems a good fit for our application. In addition, the feature maximisation and cluster labeling performed by the IGNGF method has proved promising both for visualising clustering results (Lamirel et al., 2008) and for validating or optimising a clustering method (Attik et al., 2006). We make use of these processes in all our experiments and systematically compute cluster labelling and feature maximisation on the output clusterings. As we shall see, this permits distinguishing between clusterings wit</context>
<context position="11934" citStr="Lamirel et al., 2011" startWordPosition="1875" endWordPosition="1878">ormula: ACC = ECEGold |dom(C) ∩ C| VerbsGoldnClustering Finally, F-measure is the harmonic mean of mPUR and ACC. Coverage. To assess the extent to which a clustering matches the gold classification, we additionally compute the coverage of each clustering that is, the proportion of gold classes that are prevalent classes in the clustering. Cumulative Micro Precision (CMP). As pointed out in (Lamirel et al., 2008; Attik et al., 2006), unsupervised evaluation metrics based on cluster labelling and feature maximisation can prove very useful for identifying the best clustering strategy. Following (Lamirel et al., 2011a), we use CMP to identify the best clustering. Computed on the clustering results, this metrics evaluates the quality of a clustering w.r.t. the cluster features rather than w.r.t. 6Clusters for which the prevalent class has only one element are ignored E vEc E E l clECvEc E vEc E flEF�,vEc Wvf Wvf FRc(f) = , F Pc(f) = W v f Wfl v VerbsGoldnClustering , 856 to a gold standard. It was shown in (Ghribi et al., 2010) to be effective in detecting degenerated clustering results including a small number of large heterogeneous, “garbage” clusters and a big number of small size “chunk” clusters. Firs</context>
</contexts>
<marker>Lamirel, Mall, Cuxac, Safi, 2011</marker>
<rawString>J.-C. Lamirel, R. Mall, P. Cuxac, and G. Safi. 2011b. Variations to incremental growing neural gas algorithm based on label maximization. In Neural Networks (IJCNN), The 2011 International Joint Conference on, pages 956 –965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations: a preliminary investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago and London.</location>
<contexts>
<context position="1029" citStr="Levin, 1993" startWordPosition="139" endWordPosition="140">verbs which involves the use (i) of a neural clustering method which associates clusters with features, (ii) of several supervised and unsupervised evaluation metrics and (iii) of various existing syntactic and semantic lexical resources. We evaluate our approach on an established test set and show that it outperforms previous related work with an Fmeasure of 0.70. 1 Introduction Verb classifications have been shown to be useful both from a theoretical and from a practical perspective. From the theoretical viewpoint, they permit capturing syntactic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations: a preliminary investigation. University of Chicago Press, Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Martinetz</author>
<author>K Schulten</author>
</authors>
<date>1991</date>
<journal>A ”Neural-Gas” Network Learns Topologies. Artificial Neural Networks,</journal>
<pages>397--402</pages>
<contexts>
<context position="7611" citStr="Martinetz and Schulten, 1991" startWordPosition="1183" endWordPosition="1187">ing a member of the pair’s class5. We select 6000 pairs with highest probability estimates and obtain the translated classes by assigning each verb in a selected pair to the pair’s class. This way French verbs are effectively associated with one or more English Verbnet thematic grids. 3 Clustering Methods, Evaluation Metrics and Experimental Setup 3.1 Clustering Methods The IGNGF clustering method is an incremental neural “winner-take-most” clustering method belonging to the family of the free topology neural clustering methods. Like other neural free topology methods such as Neural Gas (NG) (Martinetz and Schulten, 1991), Growing Neural Gas (GNG) (Fritzke, 1995), or Incremental Growing Neural Gas (IGNG) (Prudent and Ennaji, 2005), the IGNGF method makes use of Hebbian learning product_info.php?products_id=666), Google dictionary (http://www.google.com/dictionary) and Dicovalence (van den Eynde and Mertens, 2003). 3The training data consists of the verbs and Verbnet classes used in the gold standard presented in (Sun et al., 2010). 4We used the libsvm (Chang and Lin, 2011) implementation of the classifier for this step. 5The accuracy of the classifier on the held out random test set of 100 pairs was of 90%. 85</context>
</contexts>
<marker>Martinetz, Schulten, 1991</marker>
<rawString>T. Martinetz and K. Schulten. 1991. A ”Neural-Gas” Network Learns Topologies. Artificial Neural Networks, I:397–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>S Stevenson</author>
<author>V Tsang</author>
<author>G Allaria</author>
</authors>
<title>A multilingual paradigm for automatic verb classification.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>207--214</pages>
<contexts>
<context position="1589" citStr="Merlo et al., 2002" startWordPosition="227" endWordPosition="230">ic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the automatic classification of French verbs. Recently however, two proposals have been put forward. On the one hand, (Sun et al., 2010) applied a clustering approach developed for English to French. They exploit features extracted from a large scale subcategorisation lexicon (LexSchem (Messiant, 2008)) acquired fully automatically from Le Monde newspaper corpus and show that, as for English, syntactic frames and verb selectional preferences perform better than lexical cooccurence features. Their approach achieves a F-measure of 55.1 on 116 verbs occurring </context>
</contexts>
<marker>Merlo, Stevenson, Tsang, Allaria, 2002</marker>
<rawString>P. Merlo, S. Stevenson, V. Tsang, and G. Allaria. 2002. A multilingual paradigm for automatic verb classification. In ACL, pages 207–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Messiant</author>
</authors>
<title>A subcategorization acquisition system for French verbs.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL08: HLT Student Research Workshop,</booktitle>
<pages>55--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1929" citStr="Messiant, 2008" startWordPosition="279" endWordPosition="281">as been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the automatic classification of French verbs. Recently however, two proposals have been put forward. On the one hand, (Sun et al., 2010) applied a clustering approach developed for English to French. They exploit features extracted from a large scale subcategorisation lexicon (LexSchem (Messiant, 2008)) acquired fully automatically from Le Monde newspaper corpus and show that, as for English, syntactic frames and verb selectional preferences perform better than lexical cooccurence features. Their approach achieves a F-measure of 55.1 on 116 verbs occurring at least 150 times in Lexschem. The best performance is achieved when restricting the approach to verbs occurring at least 4000 times (43 verbs) with an F-measure of 65.4. On the other hand, Falk and Gardent (2011) present a classification approach for French verbs based on the use of Formal Concept Analysis (FCA). FCA (Barbut and Monjard</context>
<context position="4814" citStr="Messiant, 2008" startWordPosition="731" endWordPosition="732">es. 2 Lexical Resources Used Our aim is to accquire a classification which covers the core verbs of French, could be used to support semantic role labelling and is similar in spirit to the English Verbnet. In this first experiment, we therefore favoured extracting the features used for clustering, not from a large corpus parsed automatically, but from manually validated resources1. These lexical resources are (i) a syntactic lexicon produced by merging three existing lexicons for French and (ii) the English Verbnet. Among the many syntactic lexicons available for French (Nicolas et al., 2008; Messiant, 2008; Kup´s´c and Abeill´e, 2008; van den Eynde and Mertens, 2003; Gross, 1975), we selected and merged three lexicons built or validated manually namely, Dicovalence, TreeLex and the LADL tables. The resulting lexicon contains 5918 verbs, 20433 lexical entries (i.e., verb/frame pairs) and 345 subcategorisation frames. It also contains more detailed syntactic and semantic features such as lexical preferences (e.g., locative argument, concrete object) or thematic role information (e.g., symmetric arguments, asset role) which we make use of for clustering. We use the English Verbnet as a resource fo</context>
</contexts>
<marker>Messiant, 2008</marker>
<rawString>C. Messiant. 2008. A subcategorization acquisition system for French verbs. In Proceedings of the ACL08: HLT Student Research Workshop, pages 55–60, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mouton</author>
</authors>
<title>Ressources et m´ethodes semisupervis´ees pour l’analyse s´emantique de textes en fran cais.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit´e Paris</institution>
<contexts>
<context position="6667" citStr="Mouton, 2010" startWordPosition="1029" endWordPosition="1030">rbs with English Verbnet classes: A French verb is associated with an English Verbnet class if, according to our dictionaries, it is a translation of an English verb in this class. The task of the classifier is then to produce a probability estimate for the correctness of this association, given the training data. The training set is built by stating for 1740 French verb, English Verbnet class) pairs whether the verb has the thematic grid given by the pair’s Verbnet class3. This set is used to train an SVM (support vector machine) classifier4. The features we use are similar to those used in (Mouton, 2010): they are numeric and are derived for example from the number of translations an English or French verb had, the size of the Verbnet classes, the number of classes a verb is a member of etc. The resulting classifier gives for each French verb, English VN class) pair the estimated probability of the pair’s verb being a member of the pair’s class5. We select 6000 pairs with highest probability estimates and obtain the translated classes by assigning each verb in a selected pair to the pair’s class. This way French verbs are effectively associated with one or more English Verbnet thematic grids.</context>
</contexts>
<marker>Mouton, 2010</marker>
<rawString>C. Mouton. 2010. Ressources et m´ethodes semisupervis´ees pour l’analyse s´emantique de textes en fran cais. Ph.D. thesis, Universit´e Paris 11 - Paris Sud UFR d’informatique.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Nicolas</author>
<author>B Sagot</author>
<author>´E de La Clergerie</author>
<author>J Farr´e</author>
</authors>
<title>Computer aided correction and extension of a syntactic wide-coverage lexicon.</title>
<date>2008</date>
<booktitle>In Proc. of CoLing</booktitle>
<location>Manchester, UK,</location>
<marker>Nicolas, Sagot, Clergerie, Farr´e, 2008</marker>
<rawString>L. Nicolas, B. Sagot, ´E. de La Clergerie, and J. Farr´e. 2008. Computer aided correction and extension of a syntactic wide-coverage lexicon. In Proc. of CoLing 2008, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Oishi</author>
<author>Y Matsumoto</author>
</authors>
<title>Detecting the organization of semantic subclasses of Japanese verbs.</title>
<date>1997</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1556" citStr="Oishi and Matsumoto, 1997" startWordPosition="221" endWordPosition="224">viewpoint, they permit capturing syntactic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the automatic classification of French verbs. Recently however, two proposals have been put forward. On the one hand, (Sun et al., 2010) applied a clustering approach developed for English to French. They exploit features extracted from a large scale subcategorisation lexicon (LexSchem (Messiant, 2008)) acquired fully automatically from Le Monde newspaper corpus and show that, as for English, syntactic frames and verb selectional preferences perform better than lexical cooccurence features. Their approach achieves a F-measur</context>
</contexts>
<marker>Oishi, Matsumoto, 1997</marker>
<rawString>A. Oishi and Y. Matsumoto. 1997. Detecting the organization of semantic subclasses of Japanese verbs. International Journal of Corpus Linguistics, 2(1):65–89, october.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Prudent</author>
<author>A Ennaji</author>
</authors>
<title>An incremental growing neural gas learns topologies.</title>
<date>2005</date>
<booktitle>In Neural Networks,</booktitle>
<volume>2</volume>
<pages>1211--1216</pages>
<contexts>
<context position="7722" citStr="Prudent and Ennaji, 2005" startWordPosition="1200" endWordPosition="1203">d classes by assigning each verb in a selected pair to the pair’s class. This way French verbs are effectively associated with one or more English Verbnet thematic grids. 3 Clustering Methods, Evaluation Metrics and Experimental Setup 3.1 Clustering Methods The IGNGF clustering method is an incremental neural “winner-take-most” clustering method belonging to the family of the free topology neural clustering methods. Like other neural free topology methods such as Neural Gas (NG) (Martinetz and Schulten, 1991), Growing Neural Gas (GNG) (Fritzke, 1995), or Incremental Growing Neural Gas (IGNG) (Prudent and Ennaji, 2005), the IGNGF method makes use of Hebbian learning product_info.php?products_id=666), Google dictionary (http://www.google.com/dictionary) and Dicovalence (van den Eynde and Mertens, 2003). 3The training data consists of the verbs and Verbnet classes used in the gold standard presented in (Sun et al., 2010). 4We used the libsvm (Chang and Lin, 2011) implementation of the classifier for this step. 5The accuracy of the classifier on the held out random test set of 100 pairs was of 90%. 855 (Hebb, 1949) for dynamically structuring the learning space. However, contrary to these methods, the use of a</context>
</contexts>
<marker>Prudent, Ennaji, 2005</marker>
<rawString>Y. Prudent and A. Ennaji. 2005. An incremental growing neural gas learns topologies. In Neural Networks, 2005. IJCNN ’05. Proceedings. 2005 IEEE International Joint Conference on, volume 2, pages 1211– 1216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Randall</author>
</authors>
<date>2010</date>
<booktitle>Linking. Studies in Natural Language and Linguistic Theory.</booktitle>
<publisher>Springer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="16155" citStr="Randall, 2010" startWordPosition="2552" endWordPosition="2553"> Mary/John and Mary met); has four or more arguments; combines with a predicative phrase (e.g., John named Mary president); takes a sentential complement or an optional object; or accepts the passive in se (similar to the English middle voice Les habits se vendent bien / The clothes sell well). As shown in Table 1(a), these 857 (a) Additional syntactic features. Feature related VN class Symmetric arguments amalgamate-22.2, correspond-36.1 4 or more arguments get-13.5.1, send-11.1 Predicate characterize-29.2 Sentential argument correspond-36.1, characterize-29.2 Optional object implicit theme (Randall, 2010), p. 95 Passive built with se theme role (Randall, 2010), p. 120 (b) Additional semantic features. Feature related VN class Location role put-9.1, remove-10.1, ... Concrete object hit-18.1 (eg. INSTRUMENT) (non human role) other cos-45.4 ... Asset role get-13.5.1 Plural role amalgamate-22.2, correspond-36.1 Table 2: Additional syntactic (a) and semantic (b) features extracted from the LADL and Dicovalence resources and the alternations/roles they are possibly related to. features are meant to help identify specific Verbnet classes and thematic roles. Finally, we extract four semantic features </context>
</contexts>
<marker>Randall, 2010</marker>
<rawString>J. H. Randall. 2010. Linking. Studies in Natural Language and Linguistic Theory. Springer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>K S Jones</author>
</authors>
<title>Relevance weighting of search terms.</title>
<date>1976</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="13842" citStr="Robertson and Jones, 1976" startWordPosition="2211" endWordPosition="2214">ated in Table 1. Features are displayed in decreasing order of Feature F-measure (cf. Section 3.1) and features whose Feature F-measure is under the average Feature F-measure of the overall clustering are clearly delineated from others. In addition, for each verb in a cluster, a confidence score is displayed which is the ratio between the sum of the F-measures of its cluster maximised features over the sum of the F-measures of the overall cluster maximised features. Verbs whose confidence score is 0 are considered as orphan data. 3.4 Experimental setup We applied an IDF-Norm weighting scheme (Robertson and Jones, 1976) to decrease the influence of the most frequent features (IDF component) and to compensate for discrepancies in feature number (normalisation). C6- 14(14) [197(197)] ———- Prevalent Label — = AgExp-Cause 0.341100 G-AgExp-Cause 0.274864 C-SUJ:Ssub,OBJ:NP 0.061313 C-SUJ:Ssub 0.042544 C-SUJ:NP,DEOBJ:Ssub ********** ********** 0.017787 C-SUJ:NP,DEOBJ:VPinf 0.008108 C-SUJ:VPinf,AOBJ:PP . . . [**d´eprimer 0.934345 4(0)] [affliger 0.879122 3(0)] [´eblouir 0.879122 3(0)] [choquer 0.879122 3(0)] [d´ecevoir 0.879122 3(0)] [d´econtenancer 0.879122 3(0)] [d´econtracter 0.879122 3(0)] [d´esillusionner 0.879</context>
</contexts>
<marker>Robertson, Jones, 1976</marker>
<rawString>S. E. Robertson and K. S. Jones. 1976. Relevance weighting of search terms. Journal of the American Society for Information Science, 27(3):129–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
</authors>
<title>Experiments on the Automatic Induction of German Semantic Verb Classes.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart.</institution>
<note>Published as AIMS Report 9(2).</note>
<contexts>
<context position="1493" citStr="Walde, 2003" startWordPosition="214" endWordPosition="215">m a practical perspective. From the theoretical viewpoint, they permit capturing syntactic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the automatic classification of French verbs. Recently however, two proposals have been put forward. On the one hand, (Sun et al., 2010) applied a clustering approach developed for English to French. They exploit features extracted from a large scale subcategorisation lexicon (LexSchem (Messiant, 2008)) acquired fully automatically from Le Monde newspaper corpus and show that, as for English, syntactic frames and verb selectional preferences perform better than l</context>
</contexts>
<marker>Walde, 2003</marker>
<rawString>S. Schulte im Walde. 2003. Experiments on the Automatic Induction of German Semantic Verb Classes. Ph.D. thesis, Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart. Published as AIMS Report 9(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
</authors>
<title>Experiments on the automatic induction of german semantic verb classes.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="1518" citStr="Walde, 2006" startWordPosition="218" endWordPosition="219">e. From the theoretical viewpoint, they permit capturing syntactic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the automatic classification of French verbs. Recently however, two proposals have been put forward. On the one hand, (Sun et al., 2010) applied a clustering approach developed for English to French. They exploit features extracted from a large scale subcategorisation lexicon (LexSchem (Messiant, 2008)) acquired fully automatically from Le Monde newspaper corpus and show that, as for English, syntactic frames and verb selectional preferences perform better than lexical cooccurence featur</context>
</contexts>
<marker>Walde, 2006</marker>
<rawString>S. Schulte im Walde. 2006. Experiments on the automatic induction of german semantic verb classes. Computational Linguistics, 32(2):159–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sun</author>
<author>A Korhonen</author>
<author>T Poibeau</author>
<author>C Messiant</author>
</authors>
<title>Investigating the cross-linguistic potential of verbnet: style classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="1402" citStr="Sun et al., 2010" startWordPosition="195" endWordPosition="198"> 1 Introduction Verb classifications have been shown to be useful both from a theoretical and from a practical perspective. From the theoretical viewpoint, they permit capturing syntactic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the automatic classification of French verbs. Recently however, two proposals have been put forward. On the one hand, (Sun et al., 2010) applied a clustering approach developed for English to French. They exploit features extracted from a large scale subcategorisation lexicon (LexSchem (Messiant, 2008)) acquired fully automatically from Le Monde newspaper corpus and show th</context>
<context position="2966" citStr="Sun et al., 2010" startWordPosition="445" endWordPosition="448">re of 65.4. On the other hand, Falk and Gardent (2011) present a classification approach for French verbs based on the use of Formal Concept Analysis (FCA). FCA (Barbut and Monjardet, 1970) is a symbolic classification technique which permits creating classes associating sets of objects (eg. French verbs) with sets of features (eg. syntactic frames). Falk and Gardent (2011) provide no evaluation for their results however, only a qualitative analysis. In this paper, we describe a novel approach to the clustering of French verbs which (i) gives good results on the established benchmark used in (Sun et al., 2010) and (ii) associates verbs with a feature profile describing their syntactic and semantic properties. The approach exploits a clustering method called IGNGF (Incremental Growing Neural Gas with Feature Maximisation, (Lamirel et al., 2011b)) which uses the features characterising each cluster both to guide the clustering process and to label the output clusters. We apply this method to the data contained in various verb lexicons and we evalu854 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 854–863, Jeju, Republic of Korea, 8-14 July 2012. c�2012 </context>
<context position="5680" citStr="Sun et al., 2010" startWordPosition="866" endWordPosition="869"> entries (i.e., verb/frame pairs) and 345 subcategorisation frames. It also contains more detailed syntactic and semantic features such as lexical preferences (e.g., locative argument, concrete object) or thematic role information (e.g., symmetric arguments, asset role) which we make use of for clustering. We use the English Verbnet as a resource for associating French verbs with thematic grids as follows. We translate the verbs in the English Verbnet classes to French using English-French dictionaries2. To 1Of course, the same approach could be applied to corpus based data (as done e.g., in (Sun et al., 2010)) thus making the approach fully unsupervised and directly applicable to any language for which a parser is available. 2For the translation we use the following resources: SciFran-Euradic, a French-English bilingual dictionary, built and improved by linguists (http://catalog.elra.info/ deal with polysemy, we train a supervised classifier as follows. We first map French verbs with English Verbnet classes: A French verb is associated with an English Verbnet class if, according to our dictionaries, it is a translation of an English verb in this class. The task of the classifier is then to produce</context>
<context position="8028" citStr="Sun et al., 2010" startWordPosition="1242" endWordPosition="1245">nner-take-most” clustering method belonging to the family of the free topology neural clustering methods. Like other neural free topology methods such as Neural Gas (NG) (Martinetz and Schulten, 1991), Growing Neural Gas (GNG) (Fritzke, 1995), or Incremental Growing Neural Gas (IGNG) (Prudent and Ennaji, 2005), the IGNGF method makes use of Hebbian learning product_info.php?products_id=666), Google dictionary (http://www.google.com/dictionary) and Dicovalence (van den Eynde and Mertens, 2003). 3The training data consists of the verbs and Verbnet classes used in the gold standard presented in (Sun et al., 2010). 4We used the libsvm (Chang and Lin, 2011) implementation of the classifier for this step. 5The accuracy of the classifier on the held out random test set of 100 pairs was of 90%. 855 (Hebb, 1949) for dynamically structuring the learning space. However, contrary to these methods, the use of a standard distance measure for determining a winner is replaced in IGNGF by feature maximisation. Feature maximisation is a cluster quality metric which associates each cluster with maximal features i.e., features whose Feature F-measure is maximal. Feature F-measure is the harmonic mean of Feature Recall</context>
<context position="10307" citStr="Sun et al., 2010" startWordPosition="1604" endWordPosition="1607">guishing between clusterings with similar F-measure but lower “linguistic plausibility” (cf. Section 5). This facilitates clustering interpretation in that cluster labeling clearly indicates the association between clusters (verbs) and their prevalent features. And this supports the creation of a Verbnet style classification in that cluster labeling directly provides classes grouping together verbs, thematic grids and subcategorisation frames. 3.2 Evaluation metrics We use several evaluation metrics which bear on different properties of the clustering. Modified Purity and Accuracy. Following (Sun et al., 2010), we use modified purity (mPUR); weighted class accuracy (ACC) and F-measure to evaluate the clusterings produced. These are computed as follows. Each induced cluster is assigned the gold class (its prevalent class, prev(C)) to which most of its member verbs belong. A verb is then said to be correct if the gold associates it with the prevalent class of the cluster it is in. Given this, purity is the ratio between the number of correct gold verbs in the clustering and the total number of gold verbs in the clustering6: E CEClustering,|prev(C)|l1 |prev(C) ∩ C| mP UR = where VerbsGoldnClustering i</context>
<context position="17112" citStr="Sun et al. (2010)" startWordPosition="2700" endWordPosition="2703">a) and semantic (b) features extracted from the LADL and Dicovalence resources and the alternations/roles they are possibly related to. features are meant to help identify specific Verbnet classes and thematic roles. Finally, we extract four semantic features from the lexicon. These indicate whether a verb takes a locative or an asset argument and whether it requires a concrete object (non human role) or a plural role. The potential correlation between these features and Verbnet classes is given in Table 1(b). French Gold Standard To evaluate our approach, we use the gold standard proposed by Sun et al. (2010). This resource consists of 16 fine grained Levin classes with 12 verbs each whose predominant sense in English belong to that class. Since our goal is to build a Verbnet like classification for French, we mapped the 16 Levin classes of the Sun et al. (2010)’s Gold Standard to 11 Verbnet classes thereby associating each class with a thematic grid. In addition we group Verbnet semantic roles as shown in Table 4. Table 3 shows the reference we use for evaluation. Verbs For our clustering experiments we use the 2183 French verbs occurring in the translations of the 11 classes in the gold standard</context>
<context position="18519" citStr="Sun et al., 2010" startWordPosition="2918" endWordPosition="2921">Actor, Actor1, Actor2 Theme Theme, Topic, Stimulus, Proposition PredAtt Predicate, Attribute ThemeSym Theme, Theme1, Theme2 Patient Patient PatientSym Patient, Patient1, Patient2 Start Material (transformation), Source (motion, End transfer) Location Product (transformation), Destination (motion), Recipient (transfer) Instrument Cause Beneficiary Table 4: Verbnet role groups. 5 Results 5.1 Quantitative Analysis Table 4(a) includes the evaluation results for all the feature sets when using IGNGF clustering. In terms of F-measure, the results range from 0.61 to 0.70. This generally outperforms (Sun et al., 2010) whose best F-measures vary between 0.55 for verbs occurring at least 150 times in the training data and 0.65 for verbs occurring at least 4000 times in this training data. The results are not directly comparable however since the gold data is slightly different due to the grouping of Verbnet classes through their thematic grids. In terms of features, the best results are obtained using the grid-scf-sem feature set with an Fmeasure of 0.70. Moreover, for this data set, the unsupervised evaluation metrics (cf. Section 3) highlight strong cluster cohesion with a number of clusters close to the n</context>
<context position="22035" citStr="Sun et al., 2010" startWordPosition="3376" endWordPosition="3379">eint´egrer, empiler, emporter, enfermer, ins´erer, installer say-37.7: dire, r´ev´eler, d´eclarer, signaler, indiquer, montrer, annoncer, r´epondre, affirmer, certifier, r´epliquer AgExp, Theme peer-30.3: regarder, ´ecouter, examiner, consid´erer, voir, scruter, d´evisager AgExp, Start, Theme remove-10.1: ˆoter, enlever, retirer, supprimer, retrancher, d´ebarasser, soustraire, d´ecompter, ´eliminer AgExp, End, Start, Theme send-11.1: envoyer, lancer, transmettre, adresser, porter, exp´edier, transporter, jeter, renvoyer, livrer Table 3: French gold classes and their member verbs presented in (Sun et al., 2010). and a higher number of orphans (156). That is, this clustering has many clusters with strong feature cohesion but a class structure that markedly differs from the gold. Since there might be differences in structure between the English Verbnet and the thematic classification for French we are building, this is not necessarily incorrect however. Further investigation on a larger data set would be required to assess which clustering is in fact better given the data used and the classification searched for. In general, data sets whose description includes semantic features (sem or grid) tend to </context>
</contexts>
<marker>Sun, Korhonen, Poibeau, Messiant, 2010</marker>
<rawString>L. Sun, A. Korhonen, T. Poibeau, and C. Messiant. 2010. Investigating the cross-linguistic potential of verbnet: style classification. In Proceedings of the 23rd International Conference on Computational Linguistics,</rawString>
</citation>
<citation valid="false">
<authors>
<author>COLING</author>
</authors>
<pages>1056--1064</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>COLING, </marker>
<rawString>COLING ’10, pages 1056–1064, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Swier</author>
<author>S Stevenson</author>
</authors>
<title>Exploiting a verb lexicon in automatic semantic role labelling.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP. The Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1257" citStr="Swier and Stevenson, 2005" startWordPosition="172" endWordPosition="175">mantic lexical resources. We evaluate our approach on an established test set and show that it outperforms previous related work with an Fmeasure of 0.70. 1 Introduction Verb classifications have been shown to be useful both from a theoretical and from a practical perspective. From the theoretical viewpoint, they permit capturing syntactic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al., 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al., 2002), few studies have been conducted on the automatic classification of French verbs. Recently however, two proposals have been put forward. On the one hand, (Sun et al., 2010) applied a clustering approach developed for English to French. They exploit features extracted</context>
</contexts>
<marker>Swier, Stevenson, 2005</marker>
<rawString>R. S. Swier and S. Stevenson. 2005. Exploiting a verb lexicon in automatic semantic role labelling. In HLT/EMNLP. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van den Eynde</author>
<author>P Mertens</author>
</authors>
<title>La valence : l’approche pronominale et son application au lexique verbal.</title>
<date>2003</date>
<journal>Journal of French Language Studies,</journal>
<volume>13</volume>
<pages>104</pages>
<marker>van den Eynde, Mertens, 2003</marker>
<rawString>K. van den Eynde and P. Mertens. 2003. La valence : l’approche pronominale et son application au lexique verbal. Journal of French Language Studies, 13:63– 104.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>