<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989813">
Worst-Case Synchronous Grammar Rules
</title>
<author confidence="0.998761">
Daniel Gildea and Daniel Stefankoviˇc
</author>
<affiliation confidence="0.9962265">
Computer Science Dept.
University of Rochester
</affiliation>
<address confidence="0.315917">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.96064" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979636363636">
We relate the problem of finding the best
application of a Synchronous Context-
Free Grammar (SCFG) rule during pars-
ing to a Markov Random Field. This
representation allows us to use the the-
ory of expander graphs to show that the
complexity of SCFG parsing of an input
sentence of length N is Q(Ncn), for a
grammar with maximum rule length n and
some constant c. This improves on the
previous best result of Q(Nc✓n).
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999887738095238">
Recent interest in syntax-based methods for statis-
tical machine translation has lead to work in pars-
ing algorithms for synchronous context-free gram-
mars (SCFGs). Generally, parsing complexity de-
pends on the length of the longest rule in the gram-
mar, but the exact nature of this relationship has only
recently begun to be explored. It has been known
since the early days of automata theory (Aho and
Ullman, 1972) that the languages of string pairs gen-
erated by a synchronous grammar can be arranged in
an infinite hierarchy, with each rule size ≥ 4 pro-
ducing languages not possible with grammars re-
stricted to smaller rules. For any grammar with
maximum rule size n, a fairly straightforward dy-
namic programming strategy yields an O(Nn+4) al-
gorithm for parsing sentences of length N. How-
ever, this is often not the best achievable complexity,
and the exact bounds of the best possible algorithms
are not known. Satta and Peserico (2005) showed
that a permutation can be defined for any length n
such that tabular parsing strategies must take at least
Q(Nc✓n), that is, the exponent of the algorithm is
proportional to the square root of the rule length.
In this paper, we improve this result, showing that
in the worst case the exponent grows linearly with
the rule length. Using a probabilistic argument, we
show that the number of easily parsable permuta-
tions grows slowly enough that most permutations
must be difficult, where by difficult we mean that the
exponent in the complexity is greater than a constant
factor times the rule length. Thus, not only do there
exist permutations that have complexity higher than
the square root case of Satta and Peserico (2005),
but in fact the probability that a randomly chosen
permutation will have higher complexity approaches
one as the rule length grows.
Our approach is to first relate the problem of
finding an efficient parsing algorithm to finding the
treewidth of a graph derived from the SCFG rule’s
permutation. We then show that this class of graphs
are expander graphs, which in turn means that the
treewidth grows linearly with the graph size.
</bodyText>
<sectionHeader confidence="0.964667" genericHeader="method">
2 Synchronous Parsing Strategies
</sectionHeader>
<bodyText confidence="0.999768">
We write SCFG rules as productions with one
lefthand side nonterminal and two righthand side
strings. Nonterminals in the two strings are linked
with superscript indices; symbols with the same in-
dex must be further rewritten synchronously. For ex-
ample,
</bodyText>
<equation confidence="0.532531">
X , A(�) B(2) C(3) D(4), A(�) B(2) C(3) D(4)
is a rule with four children and no reordering, while
X , A(�) B(2) C(3) D(4), B(2) D(4) A(�) C(3)
</equation>
<page confidence="0.965647">
147
</page>
<note confidence="0.797677">
Proceedings of NAACL HLT 2007, pages 147–154,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.917611666666667">
Algorithm 1 BottomUpParser(grammar G, input strings e, f)
for x0, xn such that 1 &lt; x0 &lt; xn &lt; |e |in increasing order of xn − x0 do
for y0, yn such that 1 &lt; y0 &lt; yn &lt; |f |in increasing order of yn − y0 do
</bodyText>
<equation confidence="0.891762">
for Rules R of form X → X(1)
1 ...X(n)
n , X(π(1))
π(1) ...X(π(n))
π(n) in G do
p = P(R) max δ(Xi, xi−1, xi, yπ(i)−1, yπ(i))
</equation>
<note confidence="0.699254666666667">
x1..xn−1
y1..yn−1
i
</note>
<tableCaption confidence="0.74886125">
δ(X, x0, xn, y0, yn) = max{δ(X, x0, xn, y0, yn), p}
end for
end for
end for
</tableCaption>
<bodyText confidence="0.998704375">
expresses a more complex reordering. In general,
we can take indices in the first grammar dimen-
sion to be consecutive, and associate a permutation
π with the second dimension. If we use Xi for
0 ≤ i ≤ n as a set of variables over nonterminal
symbols (for example, X1 and X2 may both stand
for nonterminal A), we can write rules in the gen-
eral form:
</bodyText>
<equation confidence="0.9993752">
X0 → X(1)
1 ...X(n)
n , X(π(1))
π(1) ...X(π(n))
π(n)
</equation>
<bodyText confidence="0.999698285714286">
Grammar rules also contain terminal symbols, but as
their position does not affect parsing complexity, we
focus on nonterminals and their associated permuta-
tion π in the remainder of the paper. In a probabilis-
tic grammar, each rule R has an associated proba-
bility P(R). The synchronous parsing problem con-
sists of finding the tree covering both strings having
the maximum product of rule probabilities.1
We assume synchronous parsing is done by stor-
ing a dynamic programming table of recognized
nonterminals, as outlined in Algorithm 1. We refer
to a dynamic programming item for a given nonter-
minal with specified boundaries in each language as
a cell. The algorithm computes cells by maximiz-
ing over boundary variables xi and yi, which range
over positions in the two input strings, and specify
beginning and end points for the SCFG rule’s child
nonterminals.
The maximization in the inner loop of Algo-
rithm 1 is the most expensive part of the proce-
dure, as it would take O(N2n−2) with exhaustive
</bodyText>
<footnote confidence="0.584259">
1We describe our methods in terms of the Viterbi algorithm
(using the max-product semiring), but they also apply to non-
probabilistic parsing (boolean semiring), language modeling
(sum-product semiring), and Expectation Maximization (with
inside and outside passes).
</footnote>
<bodyText confidence="0.999819375">
search; making this step more efficient is our fo-
cus in this paper. The maximization can be done
with further dynamic programming, storing partial
results which contain some subset of an SCFG rule’s
righthand side nonterminals that have been recog-
nized. A parsing strategy for a specific SCFG rule
consists of an order in which these subsets should
be combined, until all the rule’s children have been
recognized. The complexity of an individual parsing
step depends on the number of free boundary vari-
ables, each of which can take O(N) values. It is
often helpful to visualize parsing strategies on the
permutation matrix corresponding to a rule’s per-
mutation π. Figure 1 shows the permutation matrix
of rule (2) with a three-step parsing strategy. Each
panel shows one combination step along with the
projections of the partial results in each dimension;
the endpoints of these projections correspond to free
boundary variables. The second step has the high-
est number of distinct endpoints, five in the vertical
dimension and three horizontally, meaning parsing
can be done in time O(N8).
As an example of the impact that the choice of
parsing strategy can make, Figure 2 shows a per-
mutation for which a clever ordering of partial re-
sults enables parsing in time O(N10) in the length
of the input strings. Permutations having this pattern
of diagonal stripes can be parsed using this strat-
egy in time O(N10) regardless of the length n of
the SCFG rule, whereas a naive strategy proceeding
from left to right in either input string would take
time O(Nn+3).
</bodyText>
<subsectionHeader confidence="0.985554">
2.1 Markov Random Fields for Cells
</subsectionHeader>
<bodyText confidence="0.980061">
In this section, we connect the maximization of
probabilities for a cell to the Markov Random Field
</bodyText>
<page confidence="0.968591">
148
</page>
<figure confidence="0.761596472222222">
{D}
{A, B, C}
{C}
{A, B}
{A} {B}
{A, B, C, D}
X0 X1 X2 X3 X4
X0 X1 X2 X3 X4
X0 X1 X2 X3 X4
Y4
Y3
Y2
Y1
Y0
B
A
C
D
D
D
B
A
B
A
C
C
Y4
Y3
Y2
Y1
Y0
Y4
Y3
Y2
Y1
Y0
</figure>
<figureCaption confidence="0.879017538461539">
Figure 1: The tree on the left defines a three-step parsing strategy for rule (2). In each step, the two subsets
of nonterminals in the inner marked spans are combined into a new chart item with the outer spans. The
intersection of the outer spans, shaded, has now been processed. Tic marks indicate distinct endpoints of the
spans being combined, corresponding to the free boundary variables.
(MRF) representation, which will later allow us to
use algorithms and complexity results based on the
graphical structure of MRFs. A Markov Random
Field is defined as a probability distribution2 over a
set of variables x that can be written as a product of
factors fi that are functions of various subsets xi of
x. The probability of an SCFG rule instance com-
puted by Algorithm 1 can be written in this func-
tional form:
</figureCaption>
<equation confidence="0.973696">
sR(x) = P (R) � fi(xi) Figure 2: A parsing strategy maintaining two spans
</equation>
<bodyText confidence="0.92209825">
i in each dimension is O(N10) for any length permu-
where tation of this general form.
x={xi,yi} for 0 ≤ i ≤ n
xi = {xi−1, xi, y&amp;quot;(i)−1, ylr(i)}
and the MRF has one factor fi for each child nonter-
minal Xi in the grammar rule R. The factor’s value
is the probability of the child nonterminal, which can
be expressed as a function of its four boundaries:
</bodyText>
<equation confidence="0.974058">
fi(xi) = s(Xi, xi−1, xi, ylr(i)−1, ylr(i))
</equation>
<bodyText confidence="0.9998888">
For reasons that are explained in the following
section, we augment our Markov Random Fields
with a dummy factor for the completed parent non-
terminal’s chart item. Thus there is one dummy fac-
tor d for each grammar rule:
</bodyText>
<equation confidence="0.729197">
d(x0, xn, y0, yn) = 1
</equation>
<bodyText confidence="0.9713995">
expressed as a function of the four outer boundary
variables of the completed rule, but with a constant
</bodyText>
<footnote confidence="0.723868">
2In our case unnormalized.
</footnote>
<bodyText confidence="0.995493384615385">
value of 1 so as not to change the probabilities com-
puted.
Thus an SCFG rule with n child nonterminals al-
ways results in a Markov Random Field with 2n + 2
variables and n + 1 factors, with each factor a func-
tion of exactly four variables.
Markov Random Fields are often represented as
graphs. A factor graph representation has a node
for each variable and factor, with an edge connect-
ing each factor to the variables it depends on. An ex-
ample for rule (2) is shown in Figure 3, with round
nodes for variables, square nodes for factors, and a
diamond for the special dummy factor.
</bodyText>
<subsectionHeader confidence="0.998906">
2.2 Junction Trees
</subsectionHeader>
<bodyText confidence="0.99989125">
Efficient computation on Markov Random Fields
is performed by first transforming the MRF into
a junction tree (Jensen et al., 1990; Shafer and
Shenoy, 1990), and then applying the standard
</bodyText>
<page confidence="0.956217">
149
</page>
<equation confidence="0.9543366">
d
y0 y1 y2 y3 y4
x0 x1 x2 x3 x4
f1
f2 f3
f4
y0 y1 y2 y3 y4
x0 x1 x2 x3 x4
y0 y1 y2 y3 y4
x0 x1 x2 x3 x4
</equation>
<figureCaption confidence="0.996422">
Figure 3: Markov Random Field for rule (2).
</figureCaption>
<bodyText confidence="0.999918571428571">
message-passing algorithm for graphical models
over this tree structure. The complexity of the mes-
sage passing algorithm depends on the structure of
the junction tree, which in turn depends on the graph
structure of the original MRF.
A junction tree can be constructed from a Markov
Random Field by the following three steps:
</bodyText>
<listItem confidence="0.994120625">
• Connect all variable nodes that share a factor,
and remove factor nodes. This results in the
graphs shown in Figure 4.
• Choose a triangulation of the resulting graph,
by adding chords to any cycle of length greater
than three.
• Decompose the triangulated graph into a tree of
cliques.
</listItem>
<bodyText confidence="0.996708">
We call nodes in the resulting tree, corresponding
to cliques in the triangulated graph, clusters. Each
cluster has a potential function, which is a function
of the variables in the cluster. For each factor in the
original MRF, the junction tree will have at least one
cluster containing all of the variables on which the
factor is defined. Each factor is associated with one
such cluster, and the cluster’s potential function is
set to be the product of its factors, for all combina-
tions of variable values. Triangulation ensures that
the resulting tree satisfies the junction tree property,
which states that for any two clusters containing the
same variable x, all nodes on the path connecting the
clusters also contain x. A junction tree derived from
the MRF of Figure 3 is shown in Figure 5.
The message-passing algorithm for graphical
models can be applied to the junction tree. The algo-
</bodyText>
<figureCaption confidence="0.98936525">
Figure 4: The graphs resulting from connecting
all interacting variables for the identity permutation
(1, 2, 3, 4) (top) and the (2, 4, 1, 3) permutation of
rule (2) (bottom).
</figureCaption>
<bodyText confidence="0.999144071428572">
rithm works from the leaves of the tree inward, alter-
nately multiplying in potential functions and maxi-
mizing over variables that are no longer needed, ef-
fectively distributing the max and product operators
so as to minimize the interaction between variables.
The complexity of the message-passing is O(nNk),
where the junction tree contain O(n) clusters, k is
the maximum cluster size, and each variable in the
cluster can take N values.
However, the standard algorithm assumes that the
factor functions are predefined as part of the input.
In our case, however, the factor functions themselves
depend on message-passing calculations from other
grammar rules:
</bodyText>
<equation confidence="0.997869">
fi(Xi) = s(Xi, xi−1, xi, yw(i)−1, yw(i))
P(R′) max
X′:
x′0=xi−1,x′ n′=xi
y′0=yπ(i−1),y′n′=yπ(i)
</equation>
<bodyText confidence="0.999903833333333">
We must modify the standard algorithm in order
to interleave computation among the junction trees
corresponding to the various rules in the grammar,
using the bottom-up ordering of computation from
Algorithm 1. Where, in the standard algorithm, each
message contains a complete table for all assign-
ments to its variables, we break these into a sepa-
rate message for each individual assignment of vari-
ables. The overall complexity is unchanged, because
each assignment to all variables in each cluster is
still considered only once.
The dummy factor d ensures that every junction
</bodyText>
<equation confidence="0.967883">
= max
R′:Xi→�,C3
sR′(X′) (3)
</equation>
<page confidence="0.979385">
150
</page>
<figureCaption confidence="0.999718">
Figure 5: Junction tree for rule (2).
</figureCaption>
<bodyText confidence="0.999907647058824">
tree we derive from an SCFG rule has a cluster con-
taining all four outer boundary variables, allowing
efficient lookup of the inner maximization in (3).
Because the outer boundary variables need not ap-
pear throughout the junction tree, this technique al-
lows reuse of some partial results across different
outer boundaries. As an example, consider message
passing on the junction tree of shown in Figure 5,
which corresponds to the parsing strategy of Fig-
ure 1. Only the final step involves all four bound-
aries of the complete cell, but the most complex step
is the second, with a total of eight boundaries. This
efficient reuse would not be achieved by applying
the junction tree technique directly to the maximiza-
tion operator in Algorithm 1, because we would be
fixing the outer boundaries and computing the junc-
tion tree only over the inner boundaries.
</bodyText>
<sectionHeader confidence="0.986509" genericHeader="method">
3 Treewidth and Tabular Parsing
</sectionHeader>
<bodyText confidence="0.98243655">
The complexity of the message passing algorithm
over an MRF’s junction tree is determined by the
treewidth of the MRF. In this section we show that,
because parsing strategies are in direct correspon-
dence with valid junction trees, we can use treewidth
to analyze the complexity of a grammar rule.
We define a tabular parsing strategy as any dy-
namic programming algorithm that stores partial re-
sults corresponding to subsets of a rule’s child non-
terminals. Such a strategy can be represented as a
recursive partition of child nonterminals, as shown
in Figure 1(left). We show below that a recursive
partition of children having maximum complexity k
at any step can be converted into a junction tree hav-
ing k as the maximum cluster size. This implies that
finding the optimal junction tree will give a parsing
strategy at least as good as the strategy of the opti-
mal recursive partition.
A recursive partition of child nonterminals can be
converted into a junction tree as follows:
</bodyText>
<listItem confidence="0.965035222222222">
• For each leaf of the recursive partition, which
represents a single child nonterminal i, cre-
ate a leaf in the junction tree with the cluster
(xi−1, xi, yπ(i)−1, yπ(i)) and the potential func-
tion fi(xi−1, xi, yπ(i)−1, yπ(i)).
• For each internal node in the recursive parti-
tion, create a corresponding node in the junc-
tion tree.
• Add each variable xi to all nodes in the junction
</listItem>
<bodyText confidence="0.991844194444444">
tree on the path from the node for child nonter-
minal i − 1 to the node for child nonterminal i.
Similarly, add each variable yπ(i) to all nodes
in the junction tree on the path from the node
for child nonterminal 7r(i) − 1 to the node for
child nonterminal 7r(i).
Because each variable appears as an argument of
only two factors, the junction tree nodes in which it
is present form a linear path from one leaf of the tree
to another. Since each variable is associated only
with nodes on one path through the tree, the result-
ing tree will satisfy the junction tree property. The
tree structure of the original recursive partition im-
plies that the variable rises from two leaf nodes to
the lowest common ancestor of both leaves, and is
not contained in any higher nodes. Thus each node
in the junction tree contains variables correspond-
ing to the set of endpoints of the spans defined by
the two subsets corresponding to its two children.
The number of variables at each node in the junction
tree is identical to the number of free endpoints at
the corresponding combination in the recursive par-
tition.
Because each recursive partition corresponds to a
junction tree with the same complexity, finding the
best recursive partition reduces to finding the junc-
tion tree with the best complexity, i.e., the smallest
maximum cluster size.
Finding the junction tree with the smallest clus-
ter size is equivalent to finding the input graph’s
treewidth, the smallest k such that the graph can be
embedded in a k-tree. In general, this problem was
shown to be NP-complete by Arnborg et al. (1987).
However, because the treewidth of a given rule lower
bounds the complexity of its tabular parsing strate-
gies, parsing complexity for general rules can be
</bodyText>
<equation confidence="0.999889">
X0 X3 X4 Y0 Y2 Y3 Y4
X0 X2 X3 Y0 Y1 Y2 Y3 Y4
X0 X1 X2 Y1 Y2 Y3 Y4
</equation>
<page confidence="0.98679">
151
</page>
<bodyText confidence="0.999808333333334">
bounded with treewidth results for worst-case rules,
without explicitly identifying the worst-case permu-
tations.
</bodyText>
<sectionHeader confidence="0.981406" genericHeader="method">
4 Treewidth Grows Linearly
</sectionHeader>
<bodyText confidence="0.99986725">
In this section, we show that the treewidth of the
graphs corresponding to worst-case permutations
growths linearly with the permutation’s length. Our
strategy is as follows:
</bodyText>
<listItem confidence="0.99827125">
1. Define a 3-regular graph for an input permu-
tation consisting of a subset of edges from the
original graph.
2. Show that the edge-expansion of the 3-regular
graph grows linearly for randomly chosen per-
mutations.
3. Use edge-expansion to bound the spectral gap.
4. Use spectral gap to bound treewidth.
</listItem>
<bodyText confidence="0.999730666666667">
For the first step, we define H = (V, E) as a ran-
dom 3-regular graph on 2n vertices obtained as fol-
lows. Let G1 = (V1, E1) and G2 = (V2, E2) be
cycles, each on a separate set of n vertices. These
two cycles correspond to the edges (xi, xi+1) and
(yi, yi+1) in the graphs of the type shown in Fig-
ure 4. Let M be a random perfect matching be-
tween V1 and V2. The matching represents the edges
(xi, yπ(i)) produced from the input permutation π.
Let H be the union of G1, G2, and M. While H
contains only some of the edges in the graphs de-
fined in the previous section, removing edges cannot
increase the treewidth.
For the second step of the proof, we use a proba-
bilistic argument detailed in the next subsection.
For the third step, we will use the following con-
nection between the edge-expansion and the eigen-
value gap (Alon and Milman, 1985; Tanner, 1984).
Lemma 4.1 Let G be a k-regular graph. Let λ2 be
the second largest eigenvalue of G. Let h(G) be the
edge-expansion of G. Then
</bodyText>
<equation confidence="0.911042">
k − λ2 ≥ h(G)2
2k .
</equation>
<bodyText confidence="0.998415428571429">
Finally, for the fourth step, we use a relation be-
tween the eigenvalue gap and treewidth for regu-
lar graphs shown by Chandran and Subramanian
(2003).
Lemma 4.2 Let G be a k-regular graph. Let n be
the number of vertices of G. Let λ2 be the second
largest eigenvalue of G. Then
</bodyText>
<equation confidence="0.9732235">
[ n ]
tw(G) ≥ �k (k − λ2) − 1
</equation>
<bodyText confidence="0.999608666666667">
Note that in our setting k = 3. In order to use
Lemma 4.2 we will need to give a lower bound on
the eigenvalue gap k − λ2 of G.
</bodyText>
<subsectionHeader confidence="0.997639">
4.1 Edge Expansion
</subsectionHeader>
<bodyText confidence="0.93272755">
The edge-expansion of a set of vertices T is the ra-
tio of the number of edges connecting vertices in T
to the rest of the graph, divided by the number of
vertices in T,
|E(T,V − T)|
|T|
where we assume that |T |≤ |V |/2. The edge ex-
pansion of a graph is the minimum edge expansion
of any subset of vertices:
|E(T,V − T)|
min{|T|, |V − T|}.
Intuitively, if all subsets of vertices are highly con-
nected to the remainder of the graph, there is no way
to decompose the graph into minimally interacting
subgraphs, and thus no way to decompose the dy-
namic programming problem of parsing into smaller
pieces.
Let (n � be the standard binomial coefficient, and
k
for α ∈ R, let
</bodyText>
<equation confidence="0.975470333333333">
� n⌊α⌋ (n)
≤ α) = 1: k
k��
</equation>
<bodyText confidence="0.9884015">
We will use the following standard inequality valid
for 0 ≤ α ≤ n:
�n �(nel«
≤ α \a/ (4)
Lemma 4.3 With probability at least 0.98 the graph
H has edge-expansion at least 1/50.
</bodyText>
<subsectionHeader confidence="0.66725">
Proof :
</subsectionHeader>
<bodyText confidence="0.99599">
Let ε = 1/50. Assume that T ⊆ V is a set with a
small edge-expansion, i. e.,
</bodyText>
<equation confidence="0.970533666666667">
|E(T,V − T) |≤ ε|T|, (5)
h(G) = min
T⊆V
</equation>
<page confidence="0.988302">
152
</page>
<bodyText confidence="0.999928058823529">
and |T |≤ |V |/2 = n. Let Ti = T ∩ Vi and let
ti = |Ti|, for i = 1, 2. We will w.l.o.g. assume
t1 ≤ t2. We will denote as ℓi the number of spans of
consecutive vertices from Ei contained in T. Thus
2ℓi = |E(Ti, Vi − Ti)|, for i = 1, 2. The spans
counted by ℓ1 and ℓ2 correspond to continuous spans
counted in computing the complexity of a chart pars-
ing operation. However, unlike in the diagrams in
the earlier part of this paper, in our graph theoretic
argument there is no requirement that T select only
corresponding pairs of vertices from V1 and V2.
There are at least 2(ℓ1+ℓ2)+t2−t1 edges between
T and V − T. This is because there are 2ℓi edges
within Vi at the left and right boundaries of the ℓi
spans, and at least t2 − t1 edges connecting the extra
vertices from T2 that have no matching vertex in T1.
Thus from assumption (5) we have
</bodyText>
<equation confidence="0.9297408">
t2 − t1 ≤ ε(t1 + t2)
which in turn implies
Thus
k ≥ (1 − ε)t1 + t2
2 ≥ (1 − ε)t1. (9)
</equation>
<bodyText confidence="0.9336965">
The probability that there are ≥ (1 − ε)t1 edges be-
tween T1 and T2 is bounded by
</bodyText>
<equation confidence="0.958146">
� t1 � ~t2 �(1−ε)t1
≤ εt1 n
</equation>
<bodyText confidence="0.945157">
where the first term selects vertices in T1 connected
to T2, and the second term upper bounds the proba-
bility that the selected vertices are indeed connected
to T2. Using 6, we obtain a bound in terms of t1
alone:
Similarly, using (6), we have
</bodyText>
<equation confidence="0.908731">
ε ε
ℓ1 + ℓ2 ≤ 2 (t1 + t2) ≤ t1. (7)
1 − ε
</equation>
<bodyText confidence="0.999828636363636">
That is, for T to have small edge expansion,
the vertices in T1 and T2 must be collected into a
small number of spans ℓ1 and ℓ2. This limit on the
number of spans allows us to limit the number of
ways of choosing T1 and T2. Suppose that t1 is
given. Any pair T1, T2 is determined by the edges
in E(T1, V1 − T1), and E(T2, V2 − T2), and two
bits (corresponding to the possible “swaps” of Ti
with Vi − Ti). Note that we can choose at most
2ℓ1 + 2ℓ2 ≤ t1 · 2ε/(1 − ε) edges in total. Thus the
number of choices of T1 and T2 is bounded above by
</bodyText>
<equation confidence="0.99200525">
~ 2n �
. (8)
≤ 2ε
1−ε t1
</equation>
<bodyText confidence="0.9998505">
For a given choice of T1 and T2, for T to have
small edge expansion, there must also not be too
many edges that connect T1 to vertices in V2 − T2.
Let k be the number of edges between T1 and T2.
There are at least t1 + t2 − 2k edges between T and
V − T and from assumption (5) we have
</bodyText>
<equation confidence="0.9740148">
t1 + t2 − 2k ≤ ε(t1 + t2)
� t1 � ~1 + ε �(1−ε)t1
, (10)
1 − ε · t1
≤ εt1 n
</equation>
<bodyText confidence="0.999934857142857">
Combining the number of ways of choosing T1
and T2 (8) with the bound on the probability that the
edges M from the input permutation connect almost
all the vertices in T1 to vertices from T2 (10), and
using the union bound over values of t1, we obtain
that the probability p that there exists T ⊆ V with
edge-expansion less than ε is bounded by:
</bodyText>
<equation confidence="0.9927764">
� 2n �� t1 � ~1 + ε
4· 1 − ε · t1
≤ 2ε
1−ε t1 ≤ εt1 n
(11)
</equation>
<bodyText confidence="0.999946375">
where the factor of 2 is due to the assumption t1 ≤
t2.
The graph H is connected and hence T has at least
one out-going edge. Therefore if t1 + t2 ≤ 1/ε, the
edge-expansion of T is at least ε. Thus a set with
edge-expansion less than ε must have t1 +t2 ≥ 1/ε,
which, by (6), implies t1 ≥ (1 − ε)/(2ε). Thus the
sum in (11) can be taken for t from ⌈(1 − ε)/(2ε)⌉
</bodyText>
<figure confidence="0.906666731707317">
t1 ≤ t2 ≤
t1. (6)
1 − ε
1 + ε
4 ·
2 Ln/2J X
t1=0
�(1−ε)t1
153
to Ln/2]. Using (4) we obtain 5 Conclusion
Ln/2J X
t1=�1−ε
2ε 1
!
2ne
1−εt1
2ε
2ε�εt1
1−ε t1 �t1e
εt1
p &lt; 8


~1 + ε �(1−ε)t1#
=
1 − ε � t1n
8
Ln/2J X
t1=�1−ε
2ε 1
Ce(1 − ε)�
ε
y
1−ε(e
ε
bound on the eigenvalue gap (Lemma
which in
turn implies an S2(n) bound on tre
4.1),
ewidth (Lemma
4.2), yielding:
</figure>
<bodyText confidence="0.807411214285714">
S2(Ncn) in the input string length N for some con-
stant c.
p, where p is prime,
=
mod p for 0 &lt;
= 0, provide a known family of
expander graphs (see Hoory
π(i)
i−1
π(0)
et al. (2006)).
NSF grants IIS-0546554, IIS-0428020, an
d IIS-
0325646.
</bodyText>
<sectionHeader confidence="0.994716" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.981580137931034">
N. Alon and V.D. Milman. 1985.
isoperimetric
inequalities for graphs and superconcentrators. J. of
Combinatorial Theory, Ser. B,
Stefen Arnborg, Derek G. Corneil, and Andrzej
Proskurowski. 1987. Complexity of finding embed-
dings in a k-tree. SIAM Journal of Algebraic and Dis-
crete Methods,
April.
L.S. Chandran and C.R. Subramanian. 2003. A spectral
lower bound for the treewidth of a graph and its conse-
quences. Information Processing Letters,
Shlomo Hoory, Nathan Linial, and Avi Wigderson. 2006.
Expander graphs an
λ1,
38:73–88.
8:277–284,
87:195–200.
d their applications. Bull. Amer.
Math. Soc., 43:439–561.
Statistics Quarterly,
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of HLT/EMNLP, pages
Vancouver, Can
4:269–282.
803–810,
ada, October.
2ε
</reference>
<bodyText confidence="0.988372">
While this constant bound on p is sufficient for
our main complexity result, it can further be shown
that p approaches zero as n increases, from the fact
that the geometric sum in (12) converges, and each
term for fixed t1 goes to zero as n grows.
This completes the second step of the proof as
outlined at the beginning of this section. The con-
stant bound on the edge expansion implies a constant
Theorem 4.4 Tabular parsing strategies for Syn-
chronous Context-Free Grammars containing rules
with all permutations of length n require time
We have shown our result without explicitly con-
structing adifficult permutation, but we close with
one example. The zero-based permutations of length
i &lt; p, and
We have shown in the exponent in the complex-
ity of polynomial-time parsing algorithms for syn-
chronous context-free grammars grows linearly with
the length of the grammar rules. While it is very
expensive computationally to test whether a speci-
fied permutation has a parsing algorithm of a certain
complexity, it turns out that randomly chosen per-
mutations are difficult with high probability.
</bodyText>
<sectionHeader confidence="0.872208" genericHeader="method">
Acknowledgments This work was supported by
</sectionHeader>
<reference confidence="0.989636214285714">
Albert V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
Finn V. Jensen, Steffen L. Lauritzen, and Kristian G. Ole-
sen. 1990. Bayesian updating in causal probabilis-
tic networks by local computations. Computational
G. Shafer and P. Shenoy. 1990. Probability propaga-
tion. Annals of Mathematics and Artificial Intelli-
gence,
R.M. Tanner. 1984. Explicit construction of concentra-
tors fr
2:327–353.
om generalized n-gons. J. Algebraic Discrete
Methods, 5:287–294.
</reference>
<figure confidence="0.998849727272727">
~1 + ε �1−ε ~t1 �1−ε− 2ε !t1
1−ε
.
1 − ε n
(12)
We will use t1/n &lt; 1/2 and plug ε = 1/50 into
(12). We obtain
00
p &lt; 8 X 0.74t1 &lt; 0.02.
t1=25
■
</figure>
<page confidence="0.998758">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923151">
<title confidence="0.999796">Worst-Case Synchronous Grammar Rules</title>
<author confidence="0.992769">Gildea</author>
<affiliation confidence="0.999828">Computer Science University of</affiliation>
<address confidence="0.999065">Rochester, NY 14627</address>
<abstract confidence="0.99369">We relate the problem of finding the best application of a Synchronous Context- Free Grammar (SCFG) rule during parsing to a Markov Random Field. This representation allows us to use the theory of expander graphs to show that the complexity of SCFG parsing of an input of length for a with maximum rule length constant This improves on the best result of</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Alon</author>
<author>V D Milman</author>
</authors>
<title>isoperimetric inequalities for graphs and superconcentrators.</title>
<date>1985</date>
<journal>J. of Combinatorial</journal>
<contexts>
<context position="18390" citStr="Alon and Milman, 1985" startWordPosition="3217" endWordPosition="3220">to the edges (xi, xi+1) and (yi, yi+1) in the graphs of the type shown in Figure 4. Let M be a random perfect matching between V1 and V2. The matching represents the edges (xi, yπ(i)) produced from the input permutation π. Let H be the union of G1, G2, and M. While H contains only some of the edges in the graphs defined in the previous section, removing edges cannot increase the treewidth. For the second step of the proof, we use a probabilistic argument detailed in the next subsection. For the third step, we will use the following connection between the edge-expansion and the eigenvalue gap (Alon and Milman, 1985; Tanner, 1984). Lemma 4.1 Let G be a k-regular graph. Let λ2 be the second largest eigenvalue of G. Let h(G) be the edge-expansion of G. Then k − λ2 ≥ h(G)2 2k . Finally, for the fourth step, we use a relation between the eigenvalue gap and treewidth for regular graphs shown by Chandran and Subramanian (2003). Lemma 4.2 Let G be a k-regular graph. Let n be the number of vertices of G. Let λ2 be the second largest eigenvalue of G. Then [ n ] tw(G) ≥ �k (k − λ2) − 1 Note that in our setting k = 3. In order to use Lemma 4.2 we will need to give a lower bound on the eigenvalue gap k − λ2 of G. 4.</context>
</contexts>
<marker>Alon, Milman, 1985</marker>
<rawString>N. Alon and V.D. Milman. 1985. isoperimetric inequalities for graphs and superconcentrators. J. of Combinatorial Theory, Ser. B, Stefen Arnborg, Derek G. Corneil, and Andrzej Proskurowski. 1987. Complexity of finding embeddings in a k-tree. SIAM Journal of Algebraic and Discrete Methods, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Chandran</author>
<author>C R Subramanian</author>
</authors>
<title>A spectral lower bound for the treewidth of a graph and its consequences. Information Processing Letters, Shlomo Hoory, Nathan Linial, and Avi Wigderson.</title>
<date>2003</date>
<contexts>
<context position="18701" citStr="Chandran and Subramanian (2003)" startWordPosition="3277" endWordPosition="3280">n the graphs defined in the previous section, removing edges cannot increase the treewidth. For the second step of the proof, we use a probabilistic argument detailed in the next subsection. For the third step, we will use the following connection between the edge-expansion and the eigenvalue gap (Alon and Milman, 1985; Tanner, 1984). Lemma 4.1 Let G be a k-regular graph. Let λ2 be the second largest eigenvalue of G. Let h(G) be the edge-expansion of G. Then k − λ2 ≥ h(G)2 2k . Finally, for the fourth step, we use a relation between the eigenvalue gap and treewidth for regular graphs shown by Chandran and Subramanian (2003). Lemma 4.2 Let G be a k-regular graph. Let n be the number of vertices of G. Let λ2 be the second largest eigenvalue of G. Then [ n ] tw(G) ≥ �k (k − λ2) − 1 Note that in our setting k = 3. In order to use Lemma 4.2 we will need to give a lower bound on the eigenvalue gap k − λ2 of G. 4.1 Edge Expansion The edge-expansion of a set of vertices T is the ratio of the number of edges connecting vertices in T to the rest of the graph, divided by the number of vertices in T, |E(T,V − T)| |T| where we assume that |T |≤ |V |/2. The edge expansion of a graph is the minimum edge expansion of any subset</context>
</contexts>
<marker>Chandran, Subramanian, 2003</marker>
<rawString>L.S. Chandran and C.R. Subramanian. 2003. A spectral lower bound for the treewidth of a graph and its consequences. Information Processing Letters, Shlomo Hoory, Nathan Linial, and Avi Wigderson. 2006.</rawString>
</citation>
<citation valid="false">
<title>Expander graphs an λ1,</title>
<pages>38--73</pages>
<marker></marker>
<rawString>Expander graphs an λ1, 38:73–88.</rawString>
</citation>
<citation valid="false">
<title>d their applications.</title>
<journal>Bull. Amer. Math. Soc.,</journal>
<pages>43--439</pages>
<marker></marker>
<rawString>8:277–284, 87:195–200. d their applications. Bull. Amer. Math. Soc., 43:439–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Statistics Quarterly</author>
<author>Giorgio Satta</author>
<author>Enoch Peserico</author>
</authors>
<title>Some computational complexity results for synchronous contextfree grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<volume>4</volume>
<pages>pages</pages>
<location>Vancouver, Can</location>
<marker>Quarterly, Satta, Peserico, 2005</marker>
<rawString>Statistics Quarterly, Giorgio Satta and Enoch Peserico. 2005. Some computational complexity results for synchronous contextfree grammars. In Proceedings of HLT/EMNLP, pages Vancouver, Can 4:269–282. 803–810,</rawString>
</citation>
<citation valid="true">
<authors>
<author>ada</author>
</authors>
<date></date>
<marker>ada, </marker>
<rawString>ada, October. 2ε</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert V Aho</author>
<author>Jeffery D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling,</booktitle>
<volume>1</volume>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="993" citStr="Aho and Ullman, 1972" startWordPosition="161" endWordPosition="164">at the complexity of SCFG parsing of an input sentence of length N is Q(Ncn), for a grammar with maximum rule length n and some constant c. This improves on the previous best result of Q(Nc✓n). 1 Introduction Recent interest in syntax-based methods for statistical machine translation has lead to work in parsing algorithms for synchronous context-free grammars (SCFGs). Generally, parsing complexity depends on the length of the longest rule in the grammar, but the exact nature of this relationship has only recently begun to be explored. It has been known since the early days of automata theory (Aho and Ullman, 1972) that the languages of string pairs generated by a synchronous grammar can be arranged in an infinite hierarchy, with each rule size ≥ 4 producing languages not possible with grammars restricted to smaller rules. For any grammar with maximum rule size n, a fairly straightforward dynamic programming strategy yields an O(Nn+4) algorithm for parsing sentences of length N. However, this is often not the best achievable complexity, and the exact bounds of the best possible algorithms are not known. Satta and Peserico (2005) showed that a permutation can be defined for any length n such that tabular</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Albert V. Aho and Jeffery D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, volume 1. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn V Jensen</author>
<author>Steffen L Lauritzen</author>
<author>Kristian G Olesen</author>
</authors>
<title>Bayesian updating in causal probabilistic networks by local computations.</title>
<date>1990</date>
<journal>Computational</journal>
<contexts>
<context position="9571" citStr="Jensen et al., 1990" startWordPosition="1686" endWordPosition="1689">results in a Markov Random Field with 2n + 2 variables and n + 1 factors, with each factor a function of exactly four variables. Markov Random Fields are often represented as graphs. A factor graph representation has a node for each variable and factor, with an edge connecting each factor to the variables it depends on. An example for rule (2) is shown in Figure 3, with round nodes for variables, square nodes for factors, and a diamond for the special dummy factor. 2.2 Junction Trees Efficient computation on Markov Random Fields is performed by first transforming the MRF into a junction tree (Jensen et al., 1990; Shafer and Shenoy, 1990), and then applying the standard 149 d y0 y1 y2 y3 y4 x0 x1 x2 x3 x4 f1 f2 f3 f4 y0 y1 y2 y3 y4 x0 x1 x2 x3 x4 y0 y1 y2 y3 y4 x0 x1 x2 x3 x4 Figure 3: Markov Random Field for rule (2). message-passing algorithm for graphical models over this tree structure. The complexity of the message passing algorithm depends on the structure of the junction tree, which in turn depends on the graph structure of the original MRF. A junction tree can be constructed from a Markov Random Field by the following three steps: • Connect all variable nodes that share a factor, and remove fa</context>
</contexts>
<marker>Jensen, Lauritzen, Olesen, 1990</marker>
<rawString>Finn V. Jensen, Steffen L. Lauritzen, and Kristian G. Olesen. 1990. Bayesian updating in causal probabilistic networks by local computations. Computational</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Shafer</author>
<author>P Shenoy</author>
</authors>
<title>Probability propagation.</title>
<date>1990</date>
<journal>Annals of Mathematics and Artificial Intelligence,</journal>
<contexts>
<context position="9597" citStr="Shafer and Shenoy, 1990" startWordPosition="1690" endWordPosition="1693">andom Field with 2n + 2 variables and n + 1 factors, with each factor a function of exactly four variables. Markov Random Fields are often represented as graphs. A factor graph representation has a node for each variable and factor, with an edge connecting each factor to the variables it depends on. An example for rule (2) is shown in Figure 3, with round nodes for variables, square nodes for factors, and a diamond for the special dummy factor. 2.2 Junction Trees Efficient computation on Markov Random Fields is performed by first transforming the MRF into a junction tree (Jensen et al., 1990; Shafer and Shenoy, 1990), and then applying the standard 149 d y0 y1 y2 y3 y4 x0 x1 x2 x3 x4 f1 f2 f3 f4 y0 y1 y2 y3 y4 x0 x1 x2 x3 x4 y0 y1 y2 y3 y4 x0 x1 x2 x3 x4 Figure 3: Markov Random Field for rule (2). message-passing algorithm for graphical models over this tree structure. The complexity of the message passing algorithm depends on the structure of the junction tree, which in turn depends on the graph structure of the original MRF. A junction tree can be constructed from a Markov Random Field by the following three steps: • Connect all variable nodes that share a factor, and remove factor nodes. This results i</context>
</contexts>
<marker>Shafer, Shenoy, 1990</marker>
<rawString>G. Shafer and P. Shenoy. 1990. Probability propagation. Annals of Mathematics and Artificial Intelligence,</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Tanner</author>
</authors>
<title>Explicit construction of concentrators fr 2:327–353. om generalized n-gons.</title>
<date>1984</date>
<journal>J. Algebraic Discrete Methods,</journal>
<pages>5--287</pages>
<contexts>
<context position="18405" citStr="Tanner, 1984" startWordPosition="3221" endWordPosition="3222"> and (yi, yi+1) in the graphs of the type shown in Figure 4. Let M be a random perfect matching between V1 and V2. The matching represents the edges (xi, yπ(i)) produced from the input permutation π. Let H be the union of G1, G2, and M. While H contains only some of the edges in the graphs defined in the previous section, removing edges cannot increase the treewidth. For the second step of the proof, we use a probabilistic argument detailed in the next subsection. For the third step, we will use the following connection between the edge-expansion and the eigenvalue gap (Alon and Milman, 1985; Tanner, 1984). Lemma 4.1 Let G be a k-regular graph. Let λ2 be the second largest eigenvalue of G. Let h(G) be the edge-expansion of G. Then k − λ2 ≥ h(G)2 2k . Finally, for the fourth step, we use a relation between the eigenvalue gap and treewidth for regular graphs shown by Chandran and Subramanian (2003). Lemma 4.2 Let G be a k-regular graph. Let n be the number of vertices of G. Let λ2 be the second largest eigenvalue of G. Then [ n ] tw(G) ≥ �k (k − λ2) − 1 Note that in our setting k = 3. In order to use Lemma 4.2 we will need to give a lower bound on the eigenvalue gap k − λ2 of G. 4.1 Edge Expansio</context>
</contexts>
<marker>Tanner, 1984</marker>
<rawString>R.M. Tanner. 1984. Explicit construction of concentrators fr 2:327–353. om generalized n-gons. J. Algebraic Discrete Methods, 5:287–294.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>