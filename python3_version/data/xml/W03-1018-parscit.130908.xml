<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000267">
<title confidence="0.9981045">
Evaluation and Extension of Maximum Entropy Models
with Inequality Constraints
</title>
<author confidence="0.918614">
Jun’ichi Kazama†
</author>
<email confidence="0.82471">
kazama@is.s.u-tokyo.ac.jp
</email>
<affiliation confidence="0.9977545">
†Department of Computer Science
University of Tokyo
</affiliation>
<address confidence="0.87114">
Hongo 7-3-1, Bunkyo-ku,
Tokyo 113-0033, Japan
Jun’ichi Tsujii†‡
</address>
<email confidence="0.976065">
tsujii@is.s.u-tokyo.ac.jp
</email>
<author confidence="0.568393">
‡CREST, JST
</author>
<affiliation confidence="0.509627">
(Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi,
Saitama 332-0012, Japan
</affiliation>
<sectionHeader confidence="0.976677" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871">
A maximum entropy (ME) model is usu-
ally estimated so that it conforms to equal-
ity constraints on feature expectations.
However, the equality constraint is inap-
propriate for sparse and therefore unre-
liable features. This study explores an
ME model with box-type inequality con-
straints, where the equality can be vio-
lated to reflect this unreliability. We eval-
uate the inequality ME model using text
categorization datasets. We also propose
an extension of the inequality ME model,
which results in a natural integration with
the Gaussian MAP estimation. Experi-
mental results demonstrate the advantage
of the inequality models and the proposed
extension.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955666666667">
The maximum entropy model (Berger et al., 1996;
Pietra et al., 1997) has attained great popularity in
the NLP field due to its power, robustness, and suc-
cessful performance in various NLP tasks (Ratna-
parkhi, 1996; Nigam et al., 1999; Borthwick, 1999).
In the ME estimation, an event is decomposed
into features, which indicate the strength of certain
aspects in the event, and the most uniform model
among the models that satisfy:
</bodyText>
<equation confidence="0.99704">
E˜p[fi] = Ep[fi], (1)
</equation>
<bodyText confidence="0.999197718309859">
for each feature. E˜p[fi] represents the expectation
of feature fi in the training data (empirical expec-
tation), and Ep[fi] is the expectation with respect
to the model being estimated. A powerful and ro-
bust estimation is possible since the features can be
as specific or general as required and does not need
to be independent of each other, and since the most
uniform model avoids overfitting the training data.
In spite of these advantages, the ME model still
suffers from a lack of data as long as it imposes the
equality constraint (1), since the empirical expecta-
tion calculated from the training data of limited size
is inevitably unreliable. A careful treatment is re-
quired especially in NLP applications since the fea-
tures are usually very sparse. In this study, text cat-
egorization is used as an example of such tasks with
sparse features.
Previous work on NLP proposed several solutions
for this unreliability such as the cut-off, which sim-
ply omits rare features, the MAP estimation with
the Gaussian prior (Chen and Rosenfeld, 2000), the
fuzzy maximum entropy model (Lau, 1994), and fat
constraints (Khudanpur, 1995; Newman, 1977).
Currently, the Gaussian MAP estimation (com-
bined with the cut-off) seems to be the most promis-
ing method from the empirical results. It succeeded
in language modeling (Chen and Rosenfeld, 2000)
and text categorization (Nigam et al., 1999). As
described later, it relaxes constraints like E˜p[fi] −
Ep[fi] = λi σ2, where λi is the model’s parameter.
This study follows this line, but explores the fol-
lowing box-type inequality constraints:
Ai ≥ E˜p[fi] − Ep[fi] ≥ −Bi, Ai, Bi &gt; 0. (2)
Here, the equality can be violated by the widths Ai
and Bi. We refer to the ME model with the above
inequality constraints as the inequality ME model.
This inequality constraint falls into a type of fat con-
straints, ai G Ep[fi] G bi, as suggested by (Khudan-
pur, 1995). However, as noted in (Chen and Rosen-
feld, 2000), this type of constraint has not yet been
applied nor evaluated for NLPs.
The inequality ME model differs from the Gaus-
sian MAP estimation in that its solution becomes
sparse (i.e., many parameters become zero) as a re-
sult of optimization with inequality constraints. The
features with a zero parameter can be removed from
the model without changing its prediction behavior.
Therefore, we can consider that the inequality ME
model embeds feature selection in its estimation.
Recently, the sparseness of the solution has been rec-
ognized as an important concept in constructing ro-
bust classifiers such as SVMs (Vapnik, 1995). We
believe that the sparse solution improves the robust-
ness of the ME model as well.
We also extend the inequality ME model so that
the constraint widths can move using slack vari-
ables. If we penalize the slack variables by their 2-
norm, we obtain a natural integration of the inequal-
ity ME model and the Gaussian MAP estimation.
While it incorporates the quadratic stabilization of
the parameters as in the Gaussian MAP estimation,
the sparseness of the solution is preserved.
We evaluate the inequality ME models empiri-
cally, using two text categorization datasets. The
results show that the inequality ME models outper-
form the cut-off and the Gaussian MAP estimation.
Such high accuracies are achieved with a fairly small
number of active features, indicating that the sparse
solution can effectively enhance the performance. In
addition, the 2-norm extended model is shown to be
more robust in several situations.
</bodyText>
<sectionHeader confidence="0.976624" genericHeader="method">
2 The Maximum Entropy Model
</sectionHeader>
<bodyText confidence="0.999358666666667">
The ME estimation of a conditional model p(y|x)
from the training examples {(xi, yi)} is formulated
as the following optimization problem.1
</bodyText>
<footnote confidence="0.824653714285714">
maximize H(p) = ˜p(x) p(y|x) log p(y|x)
p x y
subject to E˜p[fi] − Ep[fi] = 0 1 G i G F. (3)
1To be precise, we have also the constraints EY p(y|x) −
1 = 0 x E X. Note that although we explain using a condi-
tional model throughout the paper, the discussion can be applied
easily to a joint model by considering the condition x is fixed.
</footnote>
<bodyText confidence="0.9970735">
The empirical expectations and model expectations
in the equality constraints are defined as follows./
</bodyText>
<equation confidence="0.999994333333333">
E˜p[fi] = Exp(x) Ey˜p(y |x)fi(x, y), (4)
Ep[fi] = Ex˜p(x) Ey p(y|x)fi(x, y), (5)
˜p(x) = c(x)/L, ˜p(y|x) = c(x,y)/c(x), (6)
</equation>
<bodyText confidence="0.9998378">
where c(·) indicates the number of times · occurred
in the training data, and L is the number of training
examples.
By the Lagrange method, p(y|x) is found to have
the following parametric form:
</bodyText>
<equation confidence="0.998995">
1
pλ(y|x) = Z(x) exp( Aifi(x, y)), (7)
</equation>
<bodyText confidence="0.5625215">
where Z(x) = Ey exp(Ei Aifi(x, y)). The dual
objective function( becomes:
</bodyText>
<equation confidence="0.8470288">
L(A) = Ex˜p(x) Eyp(y |x) Ei Aifi(x, y) (8)
− Ex ˜p(x) log Ey exp(Ei Aifi(x, y)).
The ME estimation becomes the maximization of
L(A). And it is equivalent to the maximization of the
log-likelihood: LL(A) = logHx,y pλ(y|x) ˜p(x,y).
</equation>
<bodyText confidence="0.996319222222222">
This optimization can be solved using algo-
rithms such as the GIS algorithm (Darroch and Rat-
cliff, 1972) and the IIS algorithm (Pietra et al.,
1997). In addition, gradient-based algorithms can
be applied since the objective function is concave.
Malouf (2002) compares several algorithms for the
ME estimation including GIS, IIS, and the limited-
memory variable metric (LMVM) method, which is
a gradient-based method, and shows that the LMVM
method requires much less time to converge for real
NLP datasets. We also observed that the LMVM
method converges very quickly for the text catego-
rization datasets with an improvement in accuracy.
Therefore, we use the LMVM method (and its vari-
ant for the inequality models) throughout the exper-
iments. Thus, we only show the gradient when men-
tioning the training. The gradient of the objective
function (8) is computed as:
</bodyText>
<equation confidence="0.985309">
∂L(λ) =
∂λi E˜p[fi] − Ep[fi]. (9)
</equation>
<sectionHeader confidence="0.934245" genericHeader="method">
3 The Inequality ME Model
</sectionHeader>
<bodyText confidence="0.9966885">
The maximum entropy model with the box-type in-
equality constraints (2) can be formulated as the fol-
</bodyText>
<equation confidence="0.99527">
i
lowing optimization problem:
�maximize
p
x
subject to E˜p[fi] − Ep[fi] − Ai ≤ 0, (10)
Ep[fi] − E˜p[fi] − Bi ≤ 0. (11)
</equation>
<bodyText confidence="0.999699">
By using the Lagrange method for optimization
problems with inequality constraints, the following
parametric form is derived.
The solution for the inequality ME model would
become sparse if the optimization determines many
features as inactive with given widths. The relation
between the widths and the sparseness of the solu-
tion is shown in the experiment.
The dual objective function becomes:
</bodyText>
<equation confidence="0.989591153846154">
L(α, β) = Ex˜p(x) Ey˜p(y|x) Ei(αi − βi)fi(x, y)
−Ex ˜p(x) log Ey exp(Ei(αi − βi)fi(x, y))
−Ei αiAi − Ei βiBi. (13)
˜p(x) � p(y|x) log p(y|x),
y
1 �
pα,β(y|x) = Z(x) exp( (αi − βi)fi(x, y)),
i
αi ≥ 0, βi ≥ 0, (12)
Thus, the estimation is formulated as:
maximize
αi&gt;0,βi&gt;0
L(α, β).
</equation>
<bodyText confidence="0.999742">
where parameters αi and βi are the Lagrange mul-
tipliers corresponding to constraints (10) and (11).
The Karush-Kuhn-Tucker conditions state that, at
the optimal point,
</bodyText>
<equation confidence="0.961629">
αi(E˜p[fi] − Ep[fi] − Ai) = 0,
βi(Ep[fi] − E˜p[fi] − Bi) = 0.
</equation>
<bodyText confidence="0.99960275">
These conditions mean that the equality constraint is
maximally violated when the parameter is non-zero,
and if the violation is strictly within the widths, the
parameter becomes zero. We call a feature upper
active when αi &gt; 0, and lower active when βi &gt; 0.
When αi −βi =~ 0, we call that feature active.2 Inac-
tive features can be removed from the model without
changing its behavior. Since Ai &gt; 0 and Bi &gt; 0, any
feature should not be upper active and lower active
at the same time.3
The inequality constraints together with the con-
straints Ey p(y|x) −1 = 0 define the feasible re-
gion in the original probability space, on which the
entropy varies and can be maximized. The larger
the widths, the more the feasible region is enlarged.
Therefore, it can be implied that the possibility of a
feature becoming inactive (the global maximal point
is strictly within the feasible region with respect
to that feature’s constraints) increases if the corre-
sponding widths become large.
</bodyText>
<footnote confidence="0.8505172">
2The term ’active’ may be confusing since in the ME re-
search, a feature is called active when fi(x, y) &gt; 0 for an
event. However, we follow the terminology in the constrained
optimization.
3This is only achieved with some tolerance in practice.
</footnote>
<bodyText confidence="0.998729666666667">
Unlike the optimization in the standard maximum
entropy estimation, we now have bound constraints
on parameters which state that parameters must be
non-negative. In addition, maximizing L(α, β) is no
longer equivalent to maximizing the log-likelihood
LL(α, β). Instead, we maximize:
</bodyText>
<equation confidence="0.644185">
LL(α, β) − Ei αiAi − Ei βiBi. (14)
</equation>
<bodyText confidence="0.99988825">
Although we can use many optimization algorithms
to solve this dual problem since the objective func-
tion is still concave, a method that supports bounded
parameters must be used. In this study, we use the
BLMVM algorithm (Benson and Mor´e, ), a variant
of the limited-memory variable metric (LMVM) al-
gorithm, which supports bound constraints.4
The gradient of the objective function is:
</bodyText>
<equation confidence="0.9675765">
∂L(α,β) = E˜[f.] − E [fi] − Ai,
∂αi p z p
∂L(α,β) = Ep[fi] − E˜p[fi] − Bi. (15)
∂βi
</equation>
<sectionHeader confidence="0.977542" genericHeader="method">
4 Soft Width Extension
</sectionHeader>
<bodyText confidence="0.994365380952381">
In this section, we present an extension of the in-
equality ME model, which we call soft width. The
soft width allows the widths to move as Ai + δi
and −Bi − γi using slack variables, but with some
penalties in the objective function. This soft width
extension is analogous to the soft margin extension
of the SVMs, and in fact, the mathematical discus-
sion is similar. If we penalize the slack variables
4Although we consider only the gradient-based method here
as noted earlier, an extension of GIS or IIS to support bounded
parameters would also be possible.
by their 2-norm, we obtain a natural combination of
the inequality ME model and the Gaussian MAP es-
timation. We refer to this extension using 2-norm
penalty as the 2-norm inequality ME model. As the
Gaussian MAP estimation has been shown to be suc-
cessful in several tasks, it should be interesting em-
pirically, as well as theoretically, to incorporate the
Gaussian MAP estimation into the inequality model.
We first review the Gaussian MAP estimation in the
following, and then we describe our extension.
</bodyText>
<subsectionHeader confidence="0.998264">
4.1 The Gaussian MAP estimation
</subsectionHeader>
<bodyText confidence="0.99982">
In the Gaussian MAP ME estimation (Chen and
Rosenfeld, 2000), the objective function is:
</bodyText>
<equation confidence="0.9927885">
LL(λ) − ~i( 1 (16)
2σ2 i )λ2 i ,
</equation>
<bodyText confidence="0.997081">
which is derived as a consequence of maximizing
the log-likelihood of the posterior probability, using
a Gaussian distribution centered around zero with
the variance σ2i as a prior on parameters. The gra-
dient becomes:
</bodyText>
<equation confidence="0.9855946">
∂a(,-) = E˜p[fi] − Ep[fi] − Q2. (17)
i
At the optimal point, E˜p[fi] − Ep[fi] − λi
σ2 = 0.
i
</equation>
<bodyText confidence="0.999968285714286">
Therefore, the Gaussian MAP estimation can also be
considered as relaxing the equality constraints. The
significant difference between the inequality ME
model and the Gaussian MAP estimation is that the
parameters are stabilized quadratically in the Gaus-
sian MAP estimation (16), while they are stabilized
linearly in the inequality ME model (14).
</bodyText>
<subsectionHeader confidence="0.997055">
4.2 2-norm penalty extension
</subsectionHeader>
<bodyText confidence="0.9996106">
Our 2-norm extension to the inequality ME model is
as follows.5
where C1 and C2 is the penalty constants. The para-
metric form is identical to the inequality ME model
(12). However, the dual objective function becomes:
</bodyText>
<equation confidence="0.9817003">
� � � �
i
LL(α, β) − � − �
αiAi + α2 i βiBi + β2 .
i 4C1 i 4C2
Accordingly, the gradient becomes:
∂ a(«zβ) = E˜p[fi] − Ep[fi] − (Ai +2αi
∂li(13,
) = Ep[fi] − E˜p[fi] − (Bi + 2&apos;Gz2). (20)
β
</equation>
<bodyText confidence="0.991552">
It can be seen that this model is a natural combina-
tion of the inequality ME model and the Gaussian
MAP estimation. It is important to note that the so-
lution sparseness is preserved in the above model.
</bodyText>
<sectionHeader confidence="0.621041" genericHeader="method">
5 Calculation of the Constraint Width
</sectionHeader>
<bodyText confidence="0.999943142857143">
The widths, Ai and Bi, in the inequality constraints
are desirably widened according to the unreliability
of the feature (i.e., the unreliability of the calculated
empirical expectation). In this paper, we examine
two methods to determine the widths.
The first is to use a common width for all features
fixed by the following formula.
</bodyText>
<equation confidence="0.960185">
Ai=Bi=W x 1L, (21)
</equation>
<bodyText confidence="0.999968">
where W is a constant, width factor, to control the
widths. This method can only capture the global re-
liability of all the features. That is, only the reli-
ability of the training examples as a whole can be
captured. We call this method single.
The second, which we call bayes, is a method that
determines the widths based on the Bayesian frame-
work to differentiate between the features depending
on their reliabilities.
For many NLP applications including text catego-
rization, we use the following type of features.
</bodyText>
<equation confidence="0.8681195">
maximize � � fj,i(x, y) = hi(x) if y = yj, 0 otherwise. (22)
p,δ,γ H(p) − C1 i δi2 − C2 i γ2i ,
subject to E˜p[fi] − Ep[fi] − Ai &lt; δi, (18)
Ep[fi] − E˜p[fi] − Bi &lt; γi, (19)
</equation>
<footnote confidence="0.891693">
5It is also possible to impose 1-norm penalties in the objec-
</footnote>
<bodyText confidence="0.775705625">
tive function. It yields an optimization problem which is iden-
tical to the inequality ME model except that the parameters are
upper-bounded as 0 &lt; αi &lt; C1 and 0 &lt; βi &lt; C2. We will not
investigate this 1-norm extension in this paper and leave it for
future research.
In this case, if we assume the approximation,
˜p(y|x) Pz� ˜p(y|hi(x) &gt; 0), the empirical expectation
can be interpreted as follows.6
</bodyText>
<equation confidence="0.992129">
E˜p[fj,i]= � ˜p(x)˜p(y = yj|hi(x)&gt;0)hi(x).
x: hi(x)&gt;0
∂
</equation>
<footnote confidence="0.978007">
6This is only for estimating the unreliability, and is not used
to calculate the actual empirical expectations in the constraints.
</footnote>
<bodyText confidence="0.999826272727273">
Here, a source of unreliability is ˜p(y|hi(x)&gt;0). We
consider ˜p(y|hi(x) &gt; 0) as the parameter θ of the
Bernoulli trials. That is, p(y|hi(x) &gt; 0) = θ and
p(¯y|hi(x)&gt;0) = 1 − θ. Then, we estimate the pos-
terior distribution of θ from the training examples
by Bayesian estimation and utilize the variance of
the distribution. With the uniform distribution as the
prior, k times out of n trials give the posterior distri-
bution: p(θ) = Be(1+k, 1+n−k), where Be(α, β)
is the beta distribution. The variance is calculated as
follows.
</bodyText>
<equation confidence="0.99942">
V [θ] = (1+k)(1+n−k)
(2+n)2(n+3) . (23)
</equation>
<bodyText confidence="0.954092">
Letting k = c(fj,i(x, y) &gt; 0) and n = c(hi(x) &gt; 0),
we obtain fine-grained variances narrowed accord-
ing to c(hi(x) &gt; 0) instead of a single value, which
just captures the global reliability. Assuming the in-
dependence of training examples, the variance of the
empirical expectation becomes:
V [E˜p[fj,i]] = [Ex: hi(x)&gt;0 {˜p(x)hi(x)}2I V [θj,i].
Then, we calculate the widths as follows:
</bodyText>
<equation confidence="0.9355075">
�V �E˜p[fj,i]]. (24)
Ai = Bi = W ×
</equation>
<sectionHeader confidence="0.998899" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.998796210526316">
For the evaluation, we use the “Reuters-21578, Dis-
tribution 1.0” dataset and the “OHSUMED” dataset.
The Reuters dataset developed by David D. Lewis
is a collection of labeled newswire articles.7 We
adopted “ModApte” split to split the collection,
and we obtained 7,048 documents for training, and
2,991 documents for testing. We used 112 “TOP-
ICS” that actually occurred in the training set as the
target categories.
The OHSUMED dataset (Hersh et al., 1994) is a
collection of clinical paper abstracts from the MED-
LINE database. Each abstract is manually assigned
MeSH terms. We simplified a MeSH term, like
“A/B/C H A”, and used the most frequent 100
simplified terms as the target categories. We ex-
tracted 9, 947 abstracts for training, and 9, 948 ab-
stracts for testing from the file “ohsumed.91.”
A documents is converted to a bag-of-words vec-
tor representation with TFIDF values, after the stop
</bodyText>
<footnote confidence="0.970095">
7Available from http://www.daviddlewis.com/resources/
</footnote>
<bodyText confidence="0.999506222222222">
words are removed and all the words are downcased.
Since the text categorization task requires that mul-
tiple categories are assigned if appropriate, we con-
structed a binary categorizer, pJ(y ∈ {+1, −1}|d),
for each category c. If the probability pJ(+1|d) is
greater than 0.5, the category is assigned. To con-
struct a conditional maximum entropy model, we
used the feature function of the form (22), where
hi(d) returns the TFIDF value of the i-th word of
the document vector.
We implemented the estimation algorithms as an
extension of an ME estimation tool, Amis,8 using
the Toolkit for Advanced Optimization (TAO) (Ben-
son et al., 2002), which provides the LMVM and the
BLMVM optimization modules. For the inequal-
ity ME estimation, we added a hook that checks the
KKT conditions after the normal convergence test.9
We compared the following models:
</bodyText>
<listItem confidence="0.994853666666667">
• ME models only with cut-off (cut-off),
• ME models with cut-off and the Gaussian MAP
estimation (gaussian),
• Inequality ME models (ineq),
• Inequality ME models with 2-norm extension
described in Section 4 (2-norm),10
</listItem>
<bodyText confidence="0.999940230769231">
For the inequality ME models, we compared the two
methods to determine the widths, single and bayes,
as described in Section 5. Although the Gaussian
MAP estimation can use different σi for each fea-
ture, we used a common variance σ for gaussian.
Thus, gaussian roughly corresponds to single in the
way of dealing with the unreliability of features.
Note that, for inequality models, we started with
all possible features and rely on their ability to re-
move unnecessary features automatically by solu-
tion sparseness. The average maximum number of
features in a categorizer is 63,150.0 for the Reuters
dataset and 116, 452.0 for the OHSUMED dataset.
</bodyText>
<footnote confidence="0.970821363636364">
8Developed by Yusuke Miyao so as to support various
ME estimations such as the efficient estimation with compli-
cated event structures (Miyao and Tsujii, 2002). Available at
http://www-tsujii.is.s.u-tokyo.ac.jp/
˜yusuke/amis
9The tolerance for the normal convergence test (relative im-
provement) and the KKT check is 10−4. We stop the training if
the KKT check has been failed many times and the ratio of the
bad (upper and lower active) features among the active features
is lower than 0.01.
10Here, we fix the penalty constants C1 = C2 = 1016.
</footnote>
<figure confidence="0.999527770833334">
Accuracy (F-score)
Accuracy (F-score)
Width Factor
(a) Reuters
Width Factor
(b) OHSUMED
1 0.01 1e-04 1e-06 1e-08 1e-10 1e-12 1e-14 1e-16
0.845
0.835
0.825
0.815
0.805
0.85
0.84
0.83
0.82
0.81
0.8
D
C
B
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
cut-off best
gaussian best
A
100 1 0.01 1e-04 1e-06 1e-08 1e-10 1e-12 1e-14 1e-16
0.62
0.61
0.59
0.58
0.57
0.56
0.55
0.54
0.6
B
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
cut-off best
gaussian best
C
A
</figure>
<figureCaption confidence="0.9999975">
Figure 1: Accuracies as a function of the width factor W for the development sets.
Figure 2: The average number of active features as a function of width factor W.
</figureCaption>
<figure confidence="0.99953980952381">
Width Factor Width Factor
(a) Reuters (b) OHSUMED
# of Active Features
60000
40000
20000
70000
50000
30000
10000
0
1
0.01 1e-04 1e-06 1e-08 1e-10 1e-12 1e-14
1e-16
# of Active Features
120000
100000
60000
40000
20000
80000
0
100
1
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
0.01 1e-04 1e-06 1e-08 1e-10 1e-12 1e-14
D
B
C
A
1e-16
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
D
C
B
A
</figure>
<subsectionHeader confidence="0.876689">
6.1 Results
</subsectionHeader>
<bodyText confidence="0.996901515151515">
We first found the best values for the control param-
eters of each model, W, Q, and the cut-off threshold,
by using the development set. We show that the in-
equality models outperform the other methods in the
development set. We then show that these values are
valid for the evaluation set. We used the first half of
the test set as the development set, and the second
half as the evaluation set.
Figure 1 shows the accuracies of the inequality
ME models for various width factors. The accura-
cies are presented by the “micro averaged” F-score.
The horizontal lines show the highest accuracies of
cut-off and gaussian models found by exhaustive
search. For cut-off, we varied the cut-off thresh-
old and found the best threshold. For gaussian, we
varied Q with each cut-off threshold, and found the
best Q and cut-off combination. We can see that
the inequality models outperform the cut-off method
and the Gaussian MAP estimation with an appro-
priate value for W in both datasets. Although the
OHSUMED dataset seems harder than the Reuters
dataset, the improvement in the OHSUMED dataset
is greater than that in the Reuters dataset. This may
be because the OHSUMED dataset is more sparse
than the Reuters dataset. The 2-norm extension
boosts the accuracies, especially for bayes, at the
moderate Ws (i.e., with the moderate numbers of
active features). However, we can not observe the
apparent advantage of the 2-norm extension in terms
of the highest accuracy here.
Figure 2 shows the average number of active fea-
tures of each inequality ME model for various width
factors. We can see that active features increase
</bodyText>
<figure confidence="0.999736222222222">
Accuracy (F-score)
0.85
0.84
0.83
0.82
0.81
0.79
0.8
D
F
B
B: 2-norm + single
D: 2-norm + bayes
E: cut-off
F: gaussian
E
Accuracy (F-score)
0.62
0.61
0.59
0.58
0.57
0.56
0.55
0.54
0.6
B
D
F E
B: 2-norm + single
D: 2-norm + bayes
E: cut-off
F: gaussian
100 1000 10000 1000 10000 100000
# of Active Features # of Active Features
(a) Reuters (b) OHSUMED
</figure>
<figureCaption confidence="0.9725645">
Figure 3: Accuracies as a function of the average number of active features for the development sets. For
gaussian, the accuracy with the best Q found by exhaustive search is shown for each cut-off threshold.
</figureCaption>
<bodyText confidence="0.999007782608696">
when the widths become small as expected.
Figure 3 shows the accuracy of each model as a
function of the number of active features. We can
see that the inequality ME models achieve the high-
est accuracy with a fairly small number of active fea-
tures, removing unnecessary features on their own.
Besides, they consistently achieve much higher ac-
curacies than the cut-off and the Gaussian MAP es-
timation with a small number of features.
Table 1 summarizes the above results including
the best control parameters for the development set,
and shows how well each method performs for the
evaluation set with these parameters. We can see that
the best parameters are valid for the evaluation sets,
and the inequality ME models outperform the other
methods in the evaluation set as well. This means
that the inequality ME model is generally superior
to the cut-off method and the Gaussian MAP estima-
tion. At this point, the 2-norm extension shows the
advantage of being robust, especially for the Reuters
dataset. That is, the 2-norm models outperform the
normal inequality models in the evaluation set. To
see the reason for this, we show the average cross
entropy of each inequality model as a function of
the width factor in Figure 4. The average cross en-
tropy was calculated as −c Ec i Ei log pc(yi|di),
where C is the number of categories. The cross en-
tropy of the 2-norm model is consistently more sta-
ble than that of the normal inequality model. Al-
though there is no simple relation between the abso-
lute accuracy and the cross entropy, this consistent
difference can be one explanation for the advantage
of the 2-norm extension. Besides, it is possible that
the effect of 2-norm extension appears more clearly
in the Reuters dataset because the robustness is more
important in the Reuters dataset since the develop-
ment set is rather small and easy to overfit.
Lastly, we could not observe the advantage of
bayes method in these experiments. However, since
our method is still in development, it is premature
to conclude that the idea of using different widths
according to its unreliability is not successful. It is
possible that the uncertainty of ˜p(x), which were not
concerned about, is needed to be modeled, or the
Bernoulli trial assumption is inappropriate. Further
investigation on these points must be done.
</bodyText>
<sectionHeader confidence="0.991261" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999867285714286">
We have shown that the inequality ME models
outperform the cut-off method and the Gaussian
MAP estimation, using the two text categoriza-
tion datasets. Besides, the inequality ME models
achieved high accuracies with a small number of
features due to the sparseness of the solution. How-
ever, it is an open question how the inequality ME
model differs from other sophisticated methods of
feature selection based on other criteria.
Future work will investigate the details of the in-
equality model including the effect of the penalty
constants of the 2-norm extension. Evaluations on
other NLP tasks are also planned. In addition, we
need to analyze the inequality ME model further to
</bodyText>
<tableCaption confidence="0.999649">
Table 1: The summary of the experiments.
</tableCaption>
<table confidence="0.97340375">
Reuters OHSUMED
best setting # active feats acc (dev) acc (eval) best setting # active feats acc (dev) acc (eval)
cut-off cthr=2 16, 961.9 83.24 86.38 cthr=0 116, 452.0 58.83 58.35
gaussian cthr=3, v=4.22E3 12, 326.6 84.01 87.04 cthr=8, v =2.55E3 10, 154.7 59.53 59.08
ineq+single W =1.78E−11 9, 479.9 84.47 87.41 W =4.22E−2 1,375.5 61.23 61.10
2-norm+single W =5.62E−11 6, 611.1 84.35 87.59 W =4.50E−2 1,316.5 61.26 61.23
ineq+bayes W =3.16E−15 63, 150.0 84.21 87.37 W =9.46 1,136.6 60.65 60.31
2-norm+bayes W =3.16E−9 10, 022.3 84.01 87.57 W =9.46 1,154.5 60.67 60.32
</table>
<figure confidence="0.997159553191489">
Width Factor
(a) Reuters
Width Factor
(b) OHSUMED
Avg. Entropy
0.18
0.16
0.14
0.12
0.08
0.06
0.04
0.02
0.1
0
1
0.01 1e-04 1e-06 1e-08 1e-10 1e-12 1e-14
D
C
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
B
A
1e-16
100 1 0.01 1e-04 1e-06 1e-08 1e-10 1e-12 1e-14 1e-16
Avg. Entropy
0.8
0.6
0.4
0.2
1.8
1.6
1.4
1.2
2
0
1
D
C
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
B
A
</figure>
<figureCaption confidence="0.999986">
Figure 4: W vs. the average cross entropy for the development sets.
</figureCaption>
<bodyText confidence="0.99895175">
clarify the reasons for its success.
Acknowledgments We would like to thank
Yusuke Miyao, Yoshimasa Tsuruoka, and the
anonymous reviewers for many helpful comments.
</bodyText>
<sectionHeader confidence="0.999244" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999803651162791">
S. J. Benson and J. J. Mor´e. A limited memory variable metric
method for bound constraint minimization. Technical Re-
port ANL/MCS-P909-0901, Argonne National Laboratory.
S. Benson, L. C. McInnes, J. J. Mor´e, and J. Sarich. 2002.
TAO users manual. Technical Report ANL/MCS-TM-242-
Revision 1.4, Argonne National Laboratory.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39–71.
A. Borthwick. 1999. A maximum entropy approach to named
entity recognition. Ph.D. Thesis. New York University.
S. F. Chen and R. Rosenfeld. 2000. A survey of smoothing
techniques for ME models. IEEE Trans. on Speech and Au-
dio Processing, 8(1):37–50.
J. N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathematical
Statistics, 43:1470–1480.
W. Hersh, C. Buckley, T.J. Leone, and D. Hickam. 1994.
OHSUMED: An interactive retrieval evaluation and new
large test collection for research. In Proc. of the 17th An-
nual ACM SIGIR Conference, pages 192–201.
S. Khudanpur. 1995. A method of ME estimation with re-
laxed constraints. In Johns Hopkins Univ. Language Model-
ing Workshop, pages 1–17.
R. Lau. 1994. Adaptive statistical language modeling. A Mas-
ter’s Thesis. MIT.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proc. of the sixth CoNLL.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estimation for
feature forests. In Proc. of HLT 2002.
W. Newman. 1977. Extension to the ME method. In IEEE
Trans. on Information Theory, volume IT-23, pages 89–93.
K. Nigam, J. Lafferty, and A. McCallum. 1999. Using maxi-
mum entropy for text classification. In IJCAI-99 Workshop
on Machine Learning for Information Filtering, pages 61–
67.
S. Pietra, V. Pietra, and J. Lafferty. 1997. Inducing features of
random fields. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 19(4):380–393.
A. Ratnaparkhi. 1996. A maximum entropy model for part-of-
speech tagging. In Proc. of the EMNLP, pages 133–142.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer Verlag.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.385112">
<title confidence="0.9565655">Evaluation and Extension of Maximum Entropy with Inequality Constraints</title>
<affiliation confidence="0.945611">of Computer University of</affiliation>
<address confidence="0.719889">Hongo 7-3-1, Tokyo 113-0033, Japan</address>
<affiliation confidence="0.960516">Japan Science and Technology</affiliation>
<address confidence="0.8978185">Honcho 4-1-8, Saitama 332-0012, Japan</address>
<abstract confidence="0.994386333333333">A maximum entropy (ME) model is usually estimated so that it conforms to equality constraints on feature expectations. However, the equality constraint is inappropriate for sparse and therefore unreliable features. This study explores an ME model with box-type inequality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>S J Benson</author>
<author>J J Mor´e</author>
</authors>
<title>A limited memory variable metric method for bound constraint minimization.</title>
<tech>Technical Report ANL/MCS-P909-0901,</tech>
<institution>Argonne National Laboratory.</institution>
<marker>Benson, Mor´e, </marker>
<rawString>S. J. Benson and J. J. Mor´e. A limited memory variable metric method for bound constraint minimization. Technical Report ANL/MCS-P909-0901, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Benson</author>
<author>L C McInnes</author>
<author>J J Mor´e</author>
<author>J Sarich</author>
</authors>
<title>TAO users manual.</title>
<date>2002</date>
<tech>Technical Report ANL/MCS-TM-242-Revision 1.4,</tech>
<institution>Argonne National Laboratory.</institution>
<marker>Benson, McInnes, Mor´e, Sarich, 2002</marker>
<rawString>S. Benson, L. C. McInnes, J. J. Mor´e, and J. Sarich. 2002. TAO users manual. Technical Report ANL/MCS-TM-242-Revision 1.4, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="1095" citStr="Berger et al., 1996" startWordPosition="151" endWordPosition="154">n feature expectations. However, the equality constraint is inappropriate for sparse and therefore unreliable features. This study explores an ME model with box-type inequality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension. 1 Introduction The maximum entropy model (Berger et al., 1996; Pietra et al., 1997) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks (Ratnaparkhi, 1996; Nigam et al., 1999; Borthwick, 1999). In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: E˜p[fi] = Ep[fi], (1) for each feature. E˜p[fi] represents the expectation of feature fi in the training data (empirical expectation), and Ep[fi] is the expectation with respect to the model being estimated. A power</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
</authors>
<title>A maximum entropy approach to named entity recognition.</title>
<date>1999</date>
<tech>Ph.D. Thesis.</tech>
<institution>New York University.</institution>
<contexts>
<context position="1299" citStr="Borthwick, 1999" startWordPosition="187" endWordPosition="188">y can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension. 1 Introduction The maximum entropy model (Berger et al., 1996; Pietra et al., 1997) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks (Ratnaparkhi, 1996; Nigam et al., 1999; Borthwick, 1999). In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: E˜p[fi] = Ep[fi], (1) for each feature. E˜p[fi] represents the expectation of feature fi in the training data (empirical expectation), and Ep[fi] is the expectation with respect to the model being estimated. A powerful and robust estimation is possible since the features can be as specific or general as required and does not need to be independent of each other, and since the most uniform model avoids overfitting th</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>A. Borthwick. 1999. A maximum entropy approach to named entity recognition. Ph.D. Thesis. New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for ME models.</title>
<date>2000</date>
<journal>IEEE Trans. on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2547" citStr="Chen and Rosenfeld, 2000" startWordPosition="394" endWordPosition="397">te of these advantages, the ME model still suffers from a lack of data as long as it imposes the equality constraint (1), since the empirical expectation calculated from the training data of limited size is inevitably unreliable. A careful treatment is required especially in NLP applications since the features are usually very sparse. In this study, text categorization is used as an example of such tasks with sparse features. Previous work on NLP proposed several solutions for this unreliability such as the cut-off, which simply omits rare features, the MAP estimation with the Gaussian prior (Chen and Rosenfeld, 2000), the fuzzy maximum entropy model (Lau, 1994), and fat constraints (Khudanpur, 1995; Newman, 1977). Currently, the Gaussian MAP estimation (combined with the cut-off) seems to be the most promising method from the empirical results. It succeeded in language modeling (Chen and Rosenfeld, 2000) and text categorization (Nigam et al., 1999). As described later, it relaxes constraints like E˜p[fi] − Ep[fi] = λi σ2, where λi is the model’s parameter. This study follows this line, but explores the following box-type inequality constraints: Ai ≥ E˜p[fi] − Ep[fi] ≥ −Bi, Ai, Bi &gt; 0. (2) Here, the equali</context>
<context position="11500" citStr="Chen and Rosenfeld, 2000" startWordPosition="1922" endWordPosition="1925">ers would also be possible. by their 2-norm, we obtain a natural combination of the inequality ME model and the Gaussian MAP estimation. We refer to this extension using 2-norm penalty as the 2-norm inequality ME model. As the Gaussian MAP estimation has been shown to be successful in several tasks, it should be interesting empirically, as well as theoretically, to incorporate the Gaussian MAP estimation into the inequality model. We first review the Gaussian MAP estimation in the following, and then we describe our extension. 4.1 The Gaussian MAP estimation In the Gaussian MAP ME estimation (Chen and Rosenfeld, 2000), the objective function is: LL(λ) − ~i( 1 (16) 2σ2 i )λ2 i , which is derived as a consequence of maximizing the log-likelihood of the posterior probability, using a Gaussian distribution centered around zero with the variance σ2i as a prior on parameters. The gradient becomes: ∂a(,-) = E˜p[fi] − Ep[fi] − Q2. (17) i At the optimal point, E˜p[fi] − Ep[fi] − λi σ2 = 0. i Therefore, the Gaussian MAP estimation can also be considered as relaxing the equality constraints. The significant difference between the inequality ME model and the Gaussian MAP estimation is that the parameters are stabilize</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>S. F. Chen and R. Rosenfeld. 2000. A survey of smoothing techniques for ME models. IEEE Trans. on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<booktitle>The Annals of Mathematical Statistics,</booktitle>
<pages>43--1470</pages>
<contexts>
<context position="6363" citStr="Darroch and Ratcliff, 1972" startWordPosition="1041" endWordPosition="1045">icates the number of times · occurred in the training data, and L is the number of training examples. By the Lagrange method, p(y|x) is found to have the following parametric form: 1 pλ(y|x) = Z(x) exp( Aifi(x, y)), (7) where Z(x) = Ey exp(Ei Aifi(x, y)). The dual objective function( becomes: L(A) = Ex˜p(x) Eyp(y |x) Ei Aifi(x, y) (8) − Ex ˜p(x) log Ey exp(Ei Aifi(x, y)). The ME estimation becomes the maximization of L(A). And it is equivalent to the maximization of the log-likelihood: LL(A) = logHx,y pλ(y|x) ˜p(x,y). This optimization can be solved using algorithms such as the GIS algorithm (Darroch and Ratcliff, 1972) and the IIS algorithm (Pietra et al., 1997). In addition, gradient-based algorithms can be applied since the objective function is concave. Malouf (2002) compares several algorithms for the ME estimation including GIS, IIS, and the limitedmemory variable metric (LMVM) method, which is a gradient-based method, and shows that the LMVM method requires much less time to converge for real NLP datasets. We also observed that the LMVM method converges very quickly for the text categorization datasets with an improvement in accuracy. Therefore, we use the LMVM method (and its variant for the inequali</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43:1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hersh</author>
<author>C Buckley</author>
<author>T J Leone</author>
<author>D Hickam</author>
</authors>
<title>OHSUMED: An interactive retrieval evaluation and new large test collection for research.</title>
<date>1994</date>
<booktitle>In Proc. of the 17th Annual ACM SIGIR Conference,</booktitle>
<pages>192--201</pages>
<contexts>
<context position="16044" citStr="Hersh et al., 1994" startWordPosition="2722" endWordPosition="2725">ectation becomes: V [E˜p[fj,i]] = [Ex: hi(x)&gt;0 {˜p(x)hi(x)}2I V [θj,i]. Then, we calculate the widths as follows: �V �E˜p[fj,i]]. (24) Ai = Bi = W × 6 Experiments For the evaluation, we use the “Reuters-21578, Distribution 1.0” dataset and the “OHSUMED” dataset. The Reuters dataset developed by David D. Lewis is a collection of labeled newswire articles.7 We adopted “ModApte” split to split the collection, and we obtained 7,048 documents for training, and 2,991 documents for testing. We used 112 “TOPICS” that actually occurred in the training set as the target categories. The OHSUMED dataset (Hersh et al., 1994) is a collection of clinical paper abstracts from the MEDLINE database. Each abstract is manually assigned MeSH terms. We simplified a MeSH term, like “A/B/C H A”, and used the most frequent 100 simplified terms as the target categories. We extracted 9, 947 abstracts for training, and 9, 948 abstracts for testing from the file “ohsumed.91.” A documents is converted to a bag-of-words vector representation with TFIDF values, after the stop 7Available from http://www.daviddlewis.com/resources/ words are removed and all the words are downcased. Since the text categorization task requires that mult</context>
</contexts>
<marker>Hersh, Buckley, Leone, Hickam, 1994</marker>
<rawString>W. Hersh, C. Buckley, T.J. Leone, and D. Hickam. 1994. OHSUMED: An interactive retrieval evaluation and new large test collection for research. In Proc. of the 17th Annual ACM SIGIR Conference, pages 192–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khudanpur</author>
</authors>
<title>A method of ME estimation with relaxed constraints.</title>
<date>1995</date>
<booktitle>In Johns Hopkins Univ. Language Modeling Workshop,</booktitle>
<pages>1--17</pages>
<contexts>
<context position="2630" citStr="Khudanpur, 1995" startWordPosition="408" endWordPosition="409">the equality constraint (1), since the empirical expectation calculated from the training data of limited size is inevitably unreliable. A careful treatment is required especially in NLP applications since the features are usually very sparse. In this study, text categorization is used as an example of such tasks with sparse features. Previous work on NLP proposed several solutions for this unreliability such as the cut-off, which simply omits rare features, the MAP estimation with the Gaussian prior (Chen and Rosenfeld, 2000), the fuzzy maximum entropy model (Lau, 1994), and fat constraints (Khudanpur, 1995; Newman, 1977). Currently, the Gaussian MAP estimation (combined with the cut-off) seems to be the most promising method from the empirical results. It succeeded in language modeling (Chen and Rosenfeld, 2000) and text categorization (Nigam et al., 1999). As described later, it relaxes constraints like E˜p[fi] − Ep[fi] = λi σ2, where λi is the model’s parameter. This study follows this line, but explores the following box-type inequality constraints: Ai ≥ E˜p[fi] − Ep[fi] ≥ −Bi, Ai, Bi &gt; 0. (2) Here, the equality can be violated by the widths Ai and Bi. We refer to the ME model with the above</context>
</contexts>
<marker>Khudanpur, 1995</marker>
<rawString>S. Khudanpur. 1995. A method of ME estimation with relaxed constraints. In Johns Hopkins Univ. Language Modeling Workshop, pages 1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lau</author>
</authors>
<title>Adaptive statistical language modeling. A Master’s Thesis.</title>
<date>1994</date>
<publisher>MIT.</publisher>
<contexts>
<context position="2592" citStr="Lau, 1994" startWordPosition="403" endWordPosition="404">ck of data as long as it imposes the equality constraint (1), since the empirical expectation calculated from the training data of limited size is inevitably unreliable. A careful treatment is required especially in NLP applications since the features are usually very sparse. In this study, text categorization is used as an example of such tasks with sparse features. Previous work on NLP proposed several solutions for this unreliability such as the cut-off, which simply omits rare features, the MAP estimation with the Gaussian prior (Chen and Rosenfeld, 2000), the fuzzy maximum entropy model (Lau, 1994), and fat constraints (Khudanpur, 1995; Newman, 1977). Currently, the Gaussian MAP estimation (combined with the cut-off) seems to be the most promising method from the empirical results. It succeeded in language modeling (Chen and Rosenfeld, 2000) and text categorization (Nigam et al., 1999). As described later, it relaxes constraints like E˜p[fi] − Ep[fi] = λi σ2, where λi is the model’s parameter. This study follows this line, but explores the following box-type inequality constraints: Ai ≥ E˜p[fi] − Ep[fi] ≥ −Bi, Ai, Bi &gt; 0. (2) Here, the equality can be violated by the widths Ai and Bi. W</context>
</contexts>
<marker>Lau, 1994</marker>
<rawString>R. Lau. 1994. Adaptive statistical language modeling. A Master’s Thesis. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proc. of the sixth CoNLL.</booktitle>
<contexts>
<context position="6517" citStr="Malouf (2002)" startWordPosition="1067" endWordPosition="1068">metric form: 1 pλ(y|x) = Z(x) exp( Aifi(x, y)), (7) where Z(x) = Ey exp(Ei Aifi(x, y)). The dual objective function( becomes: L(A) = Ex˜p(x) Eyp(y |x) Ei Aifi(x, y) (8) − Ex ˜p(x) log Ey exp(Ei Aifi(x, y)). The ME estimation becomes the maximization of L(A). And it is equivalent to the maximization of the log-likelihood: LL(A) = logHx,y pλ(y|x) ˜p(x,y). This optimization can be solved using algorithms such as the GIS algorithm (Darroch and Ratcliff, 1972) and the IIS algorithm (Pietra et al., 1997). In addition, gradient-based algorithms can be applied since the objective function is concave. Malouf (2002) compares several algorithms for the ME estimation including GIS, IIS, and the limitedmemory variable metric (LMVM) method, which is a gradient-based method, and shows that the LMVM method requires much less time to converge for real NLP datasets. We also observed that the LMVM method converges very quickly for the text categorization datasets with an improvement in accuracy. Therefore, we use the LMVM method (and its variant for the inequality models) throughout the experiments. Thus, we only show the gradient when mentioning the training. The gradient of the objective function (8) is compute</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>R. Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proc. of the sixth CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proc. of HLT</booktitle>
<contexts>
<context position="18417" citStr="Miyao and Tsujii, 2002" startWordPosition="3107" endWordPosition="3110">σi for each feature, we used a common variance σ for gaussian. Thus, gaussian roughly corresponds to single in the way of dealing with the unreliability of features. Note that, for inequality models, we started with all possible features and rely on their ability to remove unnecessary features automatically by solution sparseness. The average maximum number of features in a categorizer is 63,150.0 for the Reuters dataset and 116, 452.0 for the OHSUMED dataset. 8Developed by Yusuke Miyao so as to support various ME estimations such as the efficient estimation with complicated event structures (Miyao and Tsujii, 2002). Available at http://www-tsujii.is.s.u-tokyo.ac.jp/ ˜yusuke/amis 9The tolerance for the normal convergence test (relative improvement) and the KKT check is 10−4. We stop the training if the KKT check has been failed many times and the ratio of the bad (upper and lower active) features among the active features is lower than 0.01. 10Here, we fix the penalty constants C1 = C2 = 1016. Accuracy (F-score) Accuracy (F-score) Width Factor (a) Reuters Width Factor (b) OHSUMED 1 0.01 1e-04 1e-06 1e-08 1e-10 1e-12 1e-14 1e-16 0.845 0.835 0.825 0.815 0.805 0.85 0.84 0.83 0.82 0.81 0.8 D C B A: ineq + si</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Y. Miyao and J. Tsujii. 2002. Maximum entropy estimation for feature forests. In Proc. of HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Newman</author>
</authors>
<title>Extension to the ME method.</title>
<date>1977</date>
<booktitle>In IEEE Trans. on Information Theory,</booktitle>
<volume>23</volume>
<pages>89--93</pages>
<contexts>
<context position="2645" citStr="Newman, 1977" startWordPosition="410" endWordPosition="411">traint (1), since the empirical expectation calculated from the training data of limited size is inevitably unreliable. A careful treatment is required especially in NLP applications since the features are usually very sparse. In this study, text categorization is used as an example of such tasks with sparse features. Previous work on NLP proposed several solutions for this unreliability such as the cut-off, which simply omits rare features, the MAP estimation with the Gaussian prior (Chen and Rosenfeld, 2000), the fuzzy maximum entropy model (Lau, 1994), and fat constraints (Khudanpur, 1995; Newman, 1977). Currently, the Gaussian MAP estimation (combined with the cut-off) seems to be the most promising method from the empirical results. It succeeded in language modeling (Chen and Rosenfeld, 2000) and text categorization (Nigam et al., 1999). As described later, it relaxes constraints like E˜p[fi] − Ep[fi] = λi σ2, where λi is the model’s parameter. This study follows this line, but explores the following box-type inequality constraints: Ai ≥ E˜p[fi] − Ep[fi] ≥ −Bi, Ai, Bi &gt; 0. (2) Here, the equality can be violated by the widths Ai and Bi. We refer to the ME model with the above inequality con</context>
</contexts>
<marker>Newman, 1977</marker>
<rawString>W. Newman. 1977. Extension to the ME method. In IEEE Trans. on Information Theory, volume IT-23, pages 89–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>J Lafferty</author>
<author>A McCallum</author>
</authors>
<title>Using maximum entropy for text classification.</title>
<date>1999</date>
<booktitle>In IJCAI-99 Workshop on Machine Learning for Information Filtering,</booktitle>
<pages>61--67</pages>
<contexts>
<context position="1281" citStr="Nigam et al., 1999" startWordPosition="183" endWordPosition="186">s, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension. 1 Introduction The maximum entropy model (Berger et al., 1996; Pietra et al., 1997) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks (Ratnaparkhi, 1996; Nigam et al., 1999; Borthwick, 1999). In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: E˜p[fi] = Ep[fi], (1) for each feature. E˜p[fi] represents the expectation of feature fi in the training data (empirical expectation), and Ep[fi] is the expectation with respect to the model being estimated. A powerful and robust estimation is possible since the features can be as specific or general as required and does not need to be independent of each other, and since the most uniform model avo</context>
<context position="2885" citStr="Nigam et al., 1999" startWordPosition="446" endWordPosition="449"> study, text categorization is used as an example of such tasks with sparse features. Previous work on NLP proposed several solutions for this unreliability such as the cut-off, which simply omits rare features, the MAP estimation with the Gaussian prior (Chen and Rosenfeld, 2000), the fuzzy maximum entropy model (Lau, 1994), and fat constraints (Khudanpur, 1995; Newman, 1977). Currently, the Gaussian MAP estimation (combined with the cut-off) seems to be the most promising method from the empirical results. It succeeded in language modeling (Chen and Rosenfeld, 2000) and text categorization (Nigam et al., 1999). As described later, it relaxes constraints like E˜p[fi] − Ep[fi] = λi σ2, where λi is the model’s parameter. This study follows this line, but explores the following box-type inequality constraints: Ai ≥ E˜p[fi] − Ep[fi] ≥ −Bi, Ai, Bi &gt; 0. (2) Here, the equality can be violated by the widths Ai and Bi. We refer to the ME model with the above inequality constraints as the inequality ME model. This inequality constraint falls into a type of fat constraints, ai G Ep[fi] G bi, as suggested by (Khudanpur, 1995). However, as noted in (Chen and Rosenfeld, 2000), this type of constraint has not yet </context>
</contexts>
<marker>Nigam, Lafferty, McCallum, 1999</marker>
<rawString>K. Nigam, J. Lafferty, and A. McCallum. 1999. Using maximum entropy for text classification. In IJCAI-99 Workshop on Machine Learning for Information Filtering, pages 61– 67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pietra</author>
<author>V Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="1117" citStr="Pietra et al., 1997" startWordPosition="155" endWordPosition="158">s. However, the equality constraint is inappropriate for sparse and therefore unreliable features. This study explores an ME model with box-type inequality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension. 1 Introduction The maximum entropy model (Berger et al., 1996; Pietra et al., 1997) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks (Ratnaparkhi, 1996; Nigam et al., 1999; Borthwick, 1999). In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: E˜p[fi] = Ep[fi], (1) for each feature. E˜p[fi] represents the expectation of feature fi in the training data (empirical expectation), and Ep[fi] is the expectation with respect to the model being estimated. A powerful and robust estimat</context>
<context position="6407" citStr="Pietra et al., 1997" startWordPosition="1050" endWordPosition="1053">ng data, and L is the number of training examples. By the Lagrange method, p(y|x) is found to have the following parametric form: 1 pλ(y|x) = Z(x) exp( Aifi(x, y)), (7) where Z(x) = Ey exp(Ei Aifi(x, y)). The dual objective function( becomes: L(A) = Ex˜p(x) Eyp(y |x) Ei Aifi(x, y) (8) − Ex ˜p(x) log Ey exp(Ei Aifi(x, y)). The ME estimation becomes the maximization of L(A). And it is equivalent to the maximization of the log-likelihood: LL(A) = logHx,y pλ(y|x) ˜p(x,y). This optimization can be solved using algorithms such as the GIS algorithm (Darroch and Ratcliff, 1972) and the IIS algorithm (Pietra et al., 1997). In addition, gradient-based algorithms can be applied since the objective function is concave. Malouf (2002) compares several algorithms for the ME estimation including GIS, IIS, and the limitedmemory variable metric (LMVM) method, which is a gradient-based method, and shows that the LMVM method requires much less time to converge for real NLP datasets. We also observed that the LMVM method converges very quickly for the text categorization datasets with an improvement in accuracy. Therefore, we use the LMVM method (and its variant for the inequality models) throughout the experiments. Thus,</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. Pietra, V. Pietra, and J. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-ofspeech tagging.</title>
<date>1996</date>
<booktitle>In Proc. of the EMNLP,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="1261" citStr="Ratnaparkhi, 1996" startWordPosition="180" endWordPosition="182">equality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension. 1 Introduction The maximum entropy model (Berger et al., 1996; Pietra et al., 1997) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks (Ratnaparkhi, 1996; Nigam et al., 1999; Borthwick, 1999). In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: E˜p[fi] = Ep[fi], (1) for each feature. E˜p[fi] represents the expectation of feature fi in the training data (empirical expectation), and Ep[fi] is the expectation with respect to the model being estimated. A powerful and robust estimation is possible since the features can be as specific or general as required and does not need to be independent of each other, and since the mo</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-ofspeech tagging. In Proc. of the EMNLP, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="4070" citStr="Vapnik, 1995" startWordPosition="651" endWordPosition="652">e of constraint has not yet been applied nor evaluated for NLPs. The inequality ME model differs from the Gaussian MAP estimation in that its solution becomes sparse (i.e., many parameters become zero) as a result of optimization with inequality constraints. The features with a zero parameter can be removed from the model without changing its prediction behavior. Therefore, we can consider that the inequality ME model embeds feature selection in its estimation. Recently, the sparseness of the solution has been recognized as an important concept in constructing robust classifiers such as SVMs (Vapnik, 1995). We believe that the sparse solution improves the robustness of the ME model as well. We also extend the inequality ME model so that the constraint widths can move using slack variables. If we penalize the slack variables by their 2- norm, we obtain a natural integration of the inequality ME model and the Gaussian MAP estimation. While it incorporates the quadratic stabilization of the parameters as in the Gaussian MAP estimation, the sparseness of the solution is preserved. We evaluate the inequality ME models empirically, using two text categorization datasets. The results show that the ine</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>